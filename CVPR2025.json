{
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_Deterministic_Image-to-Image_Translation_via_Denoising_Brownian_Bridge_Models_with_Dual_CVPR_2025_paper.html": {
    "title": "Deterministic Image-to-Image Translation via Denoising Brownian Bridge Models with Dual Approximators",
    "volume": "main",
    "abstract": "Image-to-Image (I2I) translation involves converting an im- age from one domain to another. Deterministic I2I transla- tion, such as in image super-resolution, extends this con- cept by guaranteeing that each input generates a consistent and predictable output, closely matching the ground truth (GT) with high fidelity. In this paper, we propose a denois- ing Brownian bridge model with dual approximators (Dual- approx Bridge), a novel generative model that exploits the Brownian bridge dynamics and two neural network-based approximators (one for forward and one for reverse pro- cess) to produce faithful output with negligible variance and high image quality in I2I translations. Our extensive exper- iments on benchmark datasets including image generation and super-resolution demonstrate the consistent and supe- rior performance of Dual-approx Bridge in terms of im- age quality and faithfulness to GT when compared to both stochastic and deterministic baselines. Project page and code: https://github.com/bohan95/dual-app-bridge",
    "checked": true,
    "id": "76f1db1e7b31d2815a2318b64412fffdf993900f",
    "semantic_title": "deterministic image-to-image translation via denoising brownian bridge models with dual approximators",
    "citation_count": 0,
    "authors": [
      "Bohan Xiao",
      "Peiyong Wang",
      "Qisheng He",
      "Ming Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ahmed_Towards_Source-Free_Machine_Unlearning_CVPR_2025_paper.html": {
    "title": "Towards Source-Free Machine Unlearning",
    "volume": "main",
    "abstract": "As machine learning become more pervasive and data privacy regulations evolve, the ability to remove private or copyrighted information from trained models is becoming an increasingly critical requirement. Existing unlearning methods often rely on the assumption of having access to the entire training dataset during the forgetting process. However, this assumption may not hold true in practical scenarios where the original training data may not be accessible, i.e., the source-free setting. To address this challenge, we focus on the source-free unlearning scenario, where an unlearning algorithm must be capable of removing specific data from a trained model without requiring access to the original training dataset. Building on recent work, we present a method that can estimate the Hessian of the unknown remaining training data, a crucial component required for efficient unlearning. Leveraging this estimation technique, our method enables efficient zero-shot unlearning while providing robust theoretical guarantees on the unlearning performance, while maintaining performance on the remaining data. Extensive experiments over a wide range of datasets verify the efficacy of our method",
    "checked": true,
    "id": "e0a3f658a37c593fb18a3470eef78eecf5779d72",
    "semantic_title": "towards source-free machine unlearning",
    "citation_count": 0,
    "authors": [
      "Sk Miraj Ahmed",
      "Umit Yigit Basaran",
      "Dripta S. Raychaudhuri",
      "Arindam Dutta",
      "Rohit Kundu",
      "Fahim Faisal Niloy",
      "Basak Guler",
      "Amit K. Roy-Chowdhury"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a_CVPR_2025_paper.html": {
    "title": "Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single Video",
    "volume": "main",
    "abstract": "This paper presents a unified approach to understanding dynamic scenes from casual videos. Large pretrained vision foundation models, such as vision-language, video depth prediction, motion tracking, and segmentation models, offer promising capabilities. However, training a single model for comprehensive 4D understanding remains challenging. We introduce Uni4D, a multi-stage optimization framework that harnesses multiple pretrained models to advance dynamic 3D modeling, including static/dynamic reconstruction, camera pose estimation, and dense 3D motion tracking. Our results show state-of-the-art performance in dynamic 4D modeling with superior visual quality. Notably, Uni4D requires no retraining or fine-tuning, highlighting the effectiveness of repurposing visual foundation models for 4D understanding. Code and more results are available at: https://davidyao99.github.io/uni4d",
    "checked": true,
    "id": "56290d3eabfed76709a0d6e8fdae9704d3ab5e4a",
    "semantic_title": "uni4d: unifying visual foundation models for 4d modeling from a single video",
    "citation_count": 2,
    "authors": [
      "David Yifan Yao",
      "Albert J. Zhai",
      "Shenlong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_DynScene_Scalable_Generation_of_Dynamic_Robotic_Manipulation_Scenes_for_Embodied_CVPR_2025_paper.html": {
    "title": "DynScene: Scalable Generation of Dynamic Robotic Manipulation Scenes for Embodied AI",
    "volume": "main",
    "abstract": "Robotic manipulation in embodied AI critically depends on large-scale, high-quality datasets that reflect realistic object interactions and physical dynamics. However, existing data collection pipelines are often slow, expensive, and heavily reliant on manual efforts. We present DynScene, a diffusion-based framework for generating dynamic robotic manipulation scenes directly from textual instructions. Unlike prior methods that focus solely on static environments or isolated robot actions, DynScene decomposes the generation into two phases static scene synthesis and action trajectory generation allowing fine-grained control and diversity. Our model enhances realism and physical feasibility through scene refinement (layout sampling, quaternion quantization) and leverages residual action representation to enable action augmentation, generating multiple diverse trajectories from a single static configuration. Experiments show DynScene achieves 26.8x faster generation, 1.84x higher accuracy, and 28% greater action diversity than human-crafted data. Furthermore, agents trained with DynScene exhibit up to 19.4% higher success rates across complex manipulation tasks. Our approach paves the way for scalable, automated dataset generation in robot learning",
    "checked": true,
    "id": "a17138753a48b55966db7b087a83525eb1b0e723",
    "semantic_title": "dynscene: scalable generation of dynamic robotic manipulation scenes for embodied ai",
    "citation_count": 0,
    "authors": [
      "Sangmin Lee",
      "Sungyong Park",
      "Heewon Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rosu_DiffLocks_Generating_3D_Hair_from_a_Single_Image_using_Diffusion_CVPR_2025_paper.html": {
    "title": "DiffLocks: Generating 3D Hair from a Single Image using Diffusion Models",
    "volume": "main",
    "abstract": "We address the task of generating 3D hair geometry from a single image, which is challenging due to the diversity of hairstyles and the lack of paired image-to-3D hair data. Previous methods are primarily trained on synthetic data and cope with the limited amount of such data by using low-dimensional intermediate representations, such as guide strands and scalp-level embeddings, that require post-processing to decode, upsample, and add realism. These approaches fail to reconstruct detailed hair, struggle with curly hair, or are limited to handling only a few hairstyles. To overcome these limitations, we propose DiffLocks, a novel framework that enables detailed reconstruction of a wide variety of hairstyles directly from a single image. First, we address the lack of 3D hair data by automating the creation of the largest synthetic hair dataset to date, containing 40K hairstyles. Second, we leverage the synthetic hair dataset to learn an image-conditioned diffusion-transfomer model that generates accurate 3D strands from a single frontal image. By using a pretrained image backbone, our method generalizes to in-the-wild images despite being trained only on synthetic data. Our diffusion model predicts a scalp texture map in which any point in the map contains the latent code for an individual hair strand. These codes are directly decoded to 3D strands without post-processing techniques. Representing individual strands, instead of guide strands, enables the transformer to model the detailed spatial structure of complex hairstyles. With this, DiffLocks can recover highly curled hair, like afro hairstyles, from a single image for the first time. Data and code is available at https://radualexandru.github.io/difflocks",
    "checked": true,
    "id": "b0e2c758f302762a4ccaa45110b8e9ebcb51488b",
    "semantic_title": "difflocks: generating 3d hair from a single image using diffusion models",
    "citation_count": 0,
    "authors": [
      "Radu Alexandru Rosu",
      "Keyu Wu",
      "Yao Feng",
      "Youyi Zheng",
      "Michael J. Black"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Hyperbolic_Category_Discovery_CVPR_2025_paper.html": {
    "title": "Hyperbolic Category Discovery",
    "volume": "main",
    "abstract": "Generalized Category Discovery (GCD) is an intriguing open-world problem that has garnered increasing attention. Given a dataset that includes both labelled and unlabelled images, GCD aims to categorize all images in the unlabelled subset, regardless of whether they belong to known or unknown classes. In GCD, the common practice typically involves applying a spherical projection operator at the end of the self-supervised pretrained backbone, operating within Euclidean or spherical space. However, both of these spaces have been shown to be suboptimal for encoding samples that possess hierarchical structures. In contrast, hyperbolic space exhibits exponential volume growth relative to radius, making it inherently strong at capturing the hierarchical structure of samples from both seen and unseen categories. Therefore, we propose to tackle the category discovery challenge in the hyperbolic space. We introduce HypCD, a simple Hyperbolic framework for learning hierarchy-aware representations and classifiers for generalized Category Discovery. HypCD first transforms the Euclidean embedding space of the backbone network into hyperbolic space, facilitating subsequent representation and classification learning by considering both hyperbolic distance and the angle between samples. This approach is particularly helpful for knowledge transfer from known to unknown categories in GCD. We thoroughly evaluate HypCD on public GCD benchmarks, by applying it to various baseline and state-of-the-art methods, consistently achieving significant improvements",
    "checked": true,
    "id": "600b177b96d21126f7505d7915baf9e1f5be3412",
    "semantic_title": "hyperbolic category discovery",
    "citation_count": 1,
    "authors": [
      "Yuanpei Liu",
      "Zhenqi He",
      "Kai Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_The_Language_of_Motion_Unifying_Verbal_and_Non-verbal_Language_of_CVPR_2025_paper.html": {
    "title": "The Language of Motion: Unifying Verbal and Non-verbal Language of 3D Human Motion",
    "volume": "main",
    "abstract": "Human communication is inherently multimodal, involving a combination of verbal and non-verbal cues such as speech, facial expressions, and body gestures. Modeling these behaviors is essential for understanding human interaction and for creating virtual characters that can communicate naturally in applications like games, films, and virtual reality. However, existing motion generation models are typically limited to specific input modalities--either speech, text, or motion data--and cannot fully leverage the diversity of available data. In this paper, we propose a novel framework that unifies verbal and non-verbal language using multimodal language models for human motion understanding and generation. This model is flexible in taking text, speech, and motion or any combination of them as input. Coupled with our novel pre-training strategy, our model not only achieves state-of-the-art performance on co-speech gesture generation but also requires much less data for training. Our model also unlocks an array of novel tasks such as editable gesture generation and emotion prediction from motion. We believe unifying the verbal and non-verbal language of human motion is essential for real-world applications, and language models offer a powerful approach to achieving this goal",
    "checked": true,
    "id": "02b45d58fde7cba3465d2753bcab4f2082630c49",
    "semantic_title": "the language of motion: unifying verbal and non-verbal language of 3d human motion",
    "citation_count": 5,
    "authors": [
      "Changan Chen",
      "Juze Zhang",
      "Shrinidhi K. Lakshmikanth",
      "Yusu Fang",
      "Ruizhi Shao",
      "Gordon Wetzstein",
      "Li Fei-Fei",
      "Ehsan Adeli"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nguyen_CALICO_Part-Focused_Semantic_Co-Segmentation_with_Large_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language Models",
    "volume": "main",
    "abstract": "Recent advances in Large Vision-Language Models (LVLMs) have enabled general-purpose vision tasks through visual instruction tuning. While existing LVLMs can generate segmentation masks from text prompts for single images, they struggle with segmentation-grounded reasoning across images, especially at finer granularities such as object parts. In this paper, we introduce the new task of part-focused semantic co-segmentation, which involves identifying and segmenting common objects and their constituent common and unique parts across images. To address this task, we present CALICO, the first LVLM designed for multi-image part-level reasoning segmentation. CALICO features two key components, a novel Correspondence Extraction Module that identifies semantic part-level correspondences, and Correspondence Adaptation Modules that embed this information into the LVLM to facilitate multi-image understanding in a parameter-efficient manner. To support training and evaluation, we curate MixedParts, a large-scale multi-image segmentation dataset containing 2.4M samples across 44K images spanning diverse object and part categories. Experimental results demonstrate that CALICO, with just 0.3% of its parameters finetuned, achieves strong performance on this challenging task",
    "checked": true,
    "id": "c40c28f06427b4763a99fc98885a47b501802df5",
    "semantic_title": "calico: part-focused semantic co-segmentation with large vision-language models",
    "citation_count": 0,
    "authors": [
      "Kiet A. Nguyen",
      "Adheesh Juvekar",
      "Tianjiao Yu",
      "Muntasir Wahed",
      "Ismini Lourentzou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Task_Preference_Optimization_Improving_Multimodal_Large_Language_Models_with_Vision_CVPR_2025_paper.html": {
    "title": "Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment",
    "volume": "main",
    "abstract": "Current multimodal large language models (MLLMs) struggle with fine-grained or precise understanding of visuals although they give comprehensive perception and reasoning in a spectrum of vision applications. Recent studies either develop tool-using or unify specific visual tasks into the autoregressive framework, often at the expense of overall multimodal performance. To address this issue and enhance MLLMs with visual tasks in a scalable fashion, we propose Task Preference Optimization (TPO), a novel method that utilizes differentiable task preferences derived from typical fine-grained visual tasks. TPO introduces learnable task tokens that establish connections between multiple task-specific heads and the MLLM. By leveraging rich visual labels during training, TPO significantly enhances the MLLM's multimodal capabilities and task-specific performance. Through multi-task co-training within TPO, we observe synergistic benefits that elevate individual task performance beyond what is achievable through single-task training methodologies. Our instantiation of this approach with VideoChat and LLaVA demonstrates an overall 14.6% improvement in multimodal performance compared to baseline models. Additionally, MLLM-TPO demonstrates robust zero-shot capabilities across various tasks, performing comparably to state-of-the-art supervised models",
    "checked": true,
    "id": "6a3b8254a3b803fd0d4618a36fbdc4fe6d8c19db",
    "semantic_title": "task preference optimization: improving multimodal large language models with vision task alignment",
    "citation_count": 6,
    "authors": [
      "Ziang Yan",
      "Zhilin Li",
      "Yinan He",
      "Chenting Wang",
      "Kunchang Li",
      "Xinhao Li",
      "Xiangyu Zeng",
      "Zilei Wang",
      "Yali Wang",
      "Yu Qiao",
      "Limin Wang",
      "Yi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding_CVPR_2025_paper.html": {
    "title": "Cross-modal Causal Relation Alignment for Video Question Grounding",
    "volume": "main",
    "abstract": "Video question grounding (VideoQG) requires models to answer the questions and simultaneously infer the relevant video segments to support the answers. However, existing VideoQG methods usually suffer from spurious cross-modal correlations, leading to a failure to identify the dominant visual scenes that align with the intended question. Moreover, vision-language models exhibit unfaithful generalization performance and lack robustness on challenging downstream tasks such as VideoQG. In this work, we propose a novel VideoQG framework named Cross-modal Causal Relation Alignment (CRA), to eliminate spurious correlations and improve the causal consistency between question-answering and video temporal grounding. Our CRA involves three essential components: i) Gaussian Smoothing Grounding (GSG) module for estimating the time interval via cross-modal attention, which is de-noised by an adaptive Gaussian filter, ii) Cross-Modal Alignment (CMA) enhances the performance of weakly supervised VideoQG by leveraging bidirectional contrastive learning between estimated video segments and QA features, iii) Explicit Causal Intervention (ECI) module for multimodal deconfounding, which involves front-door intervention for vision and back-door intervention for language. Extensive experiments on two VideoQG datasets demonstrate the superiority of our CRA in discovering visually grounded content and achieving robust question reasoning. Codes are available at https://github.com/WissingChen/CRA-GQA",
    "checked": true,
    "id": "c6c4ea6548b0acfaba0b76863381aaf7a7646f85",
    "semantic_title": "cross-modal causal relation alignment for video question grounding",
    "citation_count": 2,
    "authors": [
      "Weixing Chen",
      "Yang Liu",
      "Binglin Chen",
      "Jiandong Su",
      "Yongsen Zheng",
      "Liang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_Words_or_Vision_Do_Vision-Language_Models_Have_Blind_Faith_in_CVPR_2025_paper.html": {
    "title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?",
    "volume": "main",
    "abstract": "Vision-Language Models (VLMs) excel in integrating visual and textual information for vision-centric tasks, but their handling of inconsistencies between modalities is underexplored. We investigate VLMs' modality preferences when faced with visual data and varied textual inputs in vision-centered settings.By introducing textual variations to four vision-centric tasks and evaluating ten Vision-Language Models (VLMs), we discover a \"blind faith in text\" phenomenon: VLMs disproportionately trust textual data over visual data when inconsistencies arise, leading to significant performance drops under corrupted text and raising safety concerns.We analyze factors influencing this text bias, including instruction prompts, language model size, text relevance, token order, and the interplay between visual and textual certainty. While certain factors, such as scaling up the language model size, slightly mitigate text bias, others like token order can exacerbate it due to positional biases inherited from language models. To address this issue, we explore supervised fine-tuning with text augmentation and demonstrate its effectiveness in reducing text bias. Additionally, we provide a theoretical analysis suggesting that the blind faith in text phenomenon may stem from an imbalance of pure text and multi-modal data during training.Our findings highlight the need for balanced training and careful consideration of modality interactions in VLMs to enhance their robustness and reliability in handling multi-modal data inconsistencies",
    "checked": true,
    "id": "15827d3552d7060a16a4cb6a5060650282d21983",
    "semantic_title": "words or vision: do vision-language models have blind faith in text?",
    "citation_count": 3,
    "authors": [
      "Ailin Deng",
      "Tri Cao",
      "Zhirui Chen",
      "Bryan Hooi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion_CVPR_2025_paper.html": {
    "title": "Diffusion Renderer: Neural Inverse and Forward Rendering with Video Diffusion Models",
    "volume": "main",
    "abstract": "Understanding and modeling lighting effects are fundamental tasks in computer vision and graphics. Classic physically-based rendering (PBR) accurately simulates the light transport, but relies on precise scene representations--explicit 3D geometry, high-quality material properties, and lighting conditions--that are often impractical to obtain in real-world scenarios. Therefore, we introduce Diffusion Renderer, a neural approach that addresses the dual problem of inverse and forward rendering within a holistic framework. Leveraging powerful video diffusion model priors, the inverse rendering model accurately estimates G-buffers from real-world videos, providing an interface for image editing tasks, and training data for the rendering model. Conversely, our rendering model generates photorealistic images from G-buffers without explicit light transport simulation. Specifically, we first train a video diffusion model for inverse rendering on synthetic data, which generalizes well to real-world videos and allows us to auto-label diverse real-world videos. We then co-train our rendering model using both synthetic and auto-labeled real-world data. Experiments demonstrate that Diffusion Renderer effectively approximates inverse and forwards rendering, consistently outperforming the state-of-the-art. Our model enables practical applications from a single video input--including relighting, material editing, and realistic object insertion",
    "checked": false,
    "id": "709c2aaa92be6ddb51a3cb608127946622e0e879",
    "semantic_title": "diffusionrenderer: neural inverse and forward rendering with video diffusion models",
    "citation_count": 10,
    "authors": [
      "Ruofan Liang",
      "Zan Gojcic",
      "Huan Ling",
      "Jacob Munkberg",
      "Jon Hasselgren",
      "Chih-Hao Lin",
      "Jun Gao",
      "Alexander Keller",
      "Nandita Vijaykumar",
      "Sanja Fidler",
      "Zian Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Harnessing_Frequency_Spectrum_Insights_for_Image_Copyright_Protection_Against_Diffusion_CVPR_2025_paper.html": {
    "title": "Harnessing Frequency Spectrum Insights for Image Copyright Protection Against Diffusion Models",
    "volume": "main",
    "abstract": "Diffusion models have achieved remarkable success in novel view synthesis, but their reliance on large, diverse, and often untraceable Web datasets has raised pressing concerns about image copyright protection. Current methods fall short in reliably identifying unauthorized image use, as they struggle to generalize across varied generation tasks and fail when the training dataset includes images from multiple sources with few identifiable (watermarked or poisoned) samples. In this paper, we present novel evidence that diffusion-generated images faithfully preserve the statistical properties of their training data, particularly reflected in their spectral features. Leveraging this insight, we introduce CoprGuard, a robust frequency domain watermarking framework to safeguard against unauthorized image usage in diffusion model training and fine-tuning. CoprGuard demonstrates remarkable effectiveness against a wide range of models, from naive diffusion models to sophisticated text-to-image models, and is robust even when watermarked images comprise a mere 1% of the training dataset. This robust and versatile approach empowers content owners to protect their intellectual property in the era of AI-driven image generation",
    "checked": true,
    "id": "b2898a80bd8236cfcfa995a0b57e323724617af1",
    "semantic_title": "harnessing frequency spectrum insights for image copyright protection against diffusion models",
    "citation_count": 0,
    "authors": [
      "Zhenguang Liu",
      "Chao Shuai",
      "Shaojing Fan",
      "Ziping Dong",
      "Jinwu Hu",
      "Zhongjie Ba",
      "Kui Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_Learning_to_Detect_Objects_from__Multi-Agent_LiDAR_Scans_without_CVPR_2025_paper.html": {
    "title": "Learning to Detect Objects from Multi-Agent LiDAR Scans without Manual Labels",
    "volume": "main",
    "abstract": "Unsupervised 3D object detection serves as an important solution for offline 3D object annotation. However, due to the data sparsity and limited views, the clustering-based label fitting in unsupervised object detection often generates low-quality pseudo-labels. Multi-agent collaborative dataset, which involves the sharing of complementary observations among agents, holds the potential to break through this bottleneck. In this paper, we introduce a novel unsupervised method that learns to Detect Objects from Multi-Agent LiDAR scans, termed DOtA, without using labels from external. DOtA first uses the internally shared ego-pose and ego-shape of collaborative agents to initialize the detector, leveraging the generalization performance of neural networks to infer preliminary labels. Subsequently, DOtA uses the complementary observations between agents to perform multi-scale encoding on preliminary labels, then decodes high-quality and low-quality labels. These labels are further used as prompts to guide a correct feature learning process, thereby enhancing the performance of the unsupervised object detection task. Extensive experiments on the V2V4Real and OPV2V datasets show that our DOtA outperforms state-of-the-art unsupervised 3D object detection methods. Additionally, we also validate the effectiveness of the DOtA labels under various collaborative perception frameworks. The code is available at https://github.com/xmuqimingxia/DOtA",
    "checked": true,
    "id": "327283869a1edfd41aed772d2d373f936a398c65",
    "semantic_title": "learning to detect objects from multi-agent lidar scans without manual labels",
    "citation_count": 0,
    "authors": [
      "Qiming Xia",
      "Wenkai Lin",
      "Haoen Xiang",
      "Xun Huang",
      "Siheng Chen",
      "Zhen Dong",
      "Cheng Wang",
      "Chenglu Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zeng_DeepLA-Net_Very_Deep_Local_Aggregation_Networks_for_Point_Cloud_Analysis_CVPR_2025_paper.html": {
    "title": "DeepLA-Net: Very Deep Local Aggregation Networks for Point Cloud Analysis",
    "volume": "main",
    "abstract": "Due to the irregular and disordered data structure in 3D point clouds, prior works have focused on designing more sophisticated local representation methods to capture these complex local patterns. However, the recognition performance has saturated over the past few years, indicating that increasingly complex and redundant designs no longer make improvements to local learning. This phenomenon prompts us to diverge from the trend in 3D vision and instead pursue an alternative and successful solution: deeper neural networks. In this paper, we propose DeepLA-Net, a series of very deep networks for point cloud analysis. The key insight of our approach is to exploit a small but mighty local learning block, which reduces 10xfewer FLOPs, enabling the construction of very deep networks. Furthermore, we design a training supervision strategy to ensure smooth gradient backpropagation and optimization in very deep networks. We construct the DeepLA-Net family with a depth of up to 120 blocks --- at least 5xdeeper than recent methods --- trained on a single RTX 3090. An ensemble of the DeepLA-Net achieves state-of-the-art performance on classification and segmentation tasks of S3DIS Area5 (+2.2% mIoU), ScanNet test set (+1.6% mIoU), ScanObjectNN (+2.1% OA), and ShapeNetPart (+0.9% cls.mIoU)",
    "checked": false,
    "id": "15178b9b558a7cf03453b34e50a860782ae1a586",
    "semantic_title": "semantic segmentation of point cloud scene via multi-scale feature aggregation and adaptive fusion",
    "citation_count": 0,
    "authors": [
      "Ziyin Zeng",
      "Mingyue Dong",
      "Jian Zhou",
      "Huan Qiu",
      "Zhen Dong",
      "Man Luo",
      "Bijun Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Multi-Layer_Visual_Feature_Fusion_in_Multimodal_LLMs_Methods_Analysis_and_CVPR_2025_paper.html": {
    "title": "Multi-Layer Visual Feature Fusion in Multimodal LLMs: Methods, Analysis, and Best Practices",
    "volume": "main",
    "abstract": "Multimodal Large Language Models (MLLMs) have made significant advancements in recent years, with visual features playing an increasingly critical role in enhancing model performance. However, the integration of multi-layer visual features in MLLMs remains underexplored, particularly with regard to optimal layer selection and fusion strategies. Existing methods often rely on arbitrary design choices, leading to suboptimal outcomes. In this paper, we systematically investigate two core aspects of multi-layer visual feature fusion: (1) selecting the most effective visual layers and (2) identifying the best fusion approach with the language model. Our experiments reveal that while combining visual features from multiple stages improves generalization, incorporating additional features from the same stage typically leads to diminished performance. Furthermore, we find that direct fusion of multi-layer visual features at the input stage consistently yields superior and more stable performance across various configurations",
    "checked": true,
    "id": "042e07d3f2ef78a85e8471c6451d91d099e772ba",
    "semantic_title": "multi-layer visual feature fusion in multimodal llms: methods, analysis, and best practices",
    "citation_count": 0,
    "authors": [
      "Junyan Lin",
      "Haoran Chen",
      "Yue Fan",
      "Yingqi Fan",
      "Xin Jin",
      "Hui Su",
      "Jinlan Fu",
      "Xiaoyu Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_APHQ-ViT_Post-Training_Quantization_with_Average_Perturbation_Hessian_Based_Reconstruction_for_CVPR_2025_paper.html": {
    "title": "APHQ-ViT: Post-Training Quantization with Average Perturbation Hessian Based Reconstruction for Vision Transformers",
    "volume": "main",
    "abstract": "Vision Transformers (ViTs) have become one of the most commonly used backbones for vision tasks. Despite their remarkable performance, they often suffer significant accuracy drop when quantized for practical deployment, particularly by post-training quantization (PTQ) under ultra-low bits. Recently, reconstruction-based PTQ methods have shown promising performance in quantizing Convolutional Neural Networks (CNNs). However, they fail when applied to ViTs, primarily due to the inaccurate estimation of output importance and the substantial accuracy degradation in quantizing post-GELU activations. To address these issues, we propose APHQ-ViT, a novel PTQ approach based on importance estimation with Average Perturbation Hessian (APH). Specifically, we first thoroughly analyze the current approximation approaches with Hessian loss, and propose an improved average perturbation Hessian loss. To deal with the quantization of the post-GELU activations, we design an MLP-Reconstruction (MR) method by replacing the GELU function in MLP with ReLU and reconstructing it by the APH loss on a small unlabeled calibration set. Extensive experiments demonstrate that APHQ-ViT using linear quantizers outperforms existing PTQ methods by substantial margins in 3-bit and 4-bit across different vision tasks. The source code is available at https://github.com/GoatWu/APHQ-ViT",
    "checked": true,
    "id": "24deec15e3b67dd2319137e8e12a0a1f6e4d77fa",
    "semantic_title": "aphq-vit: post-training quantization with average perturbation hessian based reconstruction for vision transformers",
    "citation_count": 1,
    "authors": [
      "Zhuguanyu Wu",
      "Jiayi Zhang",
      "Jiaxin Chen",
      "Jinyang Guo",
      "Di Huang",
      "Yunhong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_AdaptCMVC_Robust_Adaption_to_Incremental_Views_in_Continual_Multi-view_Clustering_CVPR_2025_paper.html": {
    "title": "AdaptCMVC: Robust Adaption to Incremental Views in Continual Multi-view Clustering",
    "volume": "main",
    "abstract": "Most Multi-view Clustering approaches assume that all views are available for clustering. However, this assumption is often unrealistic as views are incrementally accumulated over time, leading to a need for continual multi-view clustering (CMVC) methods. Current approaches to CMVC leverage late fusion-based approaches, where a new model is typically learned individually for each view to obtain the corresponding partition matrix, and then used to update a consensus matrix via a moving average. These approaches are prone to view-specific noise and struggle to adapt to large gaps between different views. To address these shortcomings, we reconsider CMVC from the perspective of domain adaptation and propose AdaptCMVC, which learns how to incrementally accumulate knowledge of new views as they become available and prevents catastrophic forgetting. Specifically, a self-training framework is introduced to extend the model to new views, particularly designed to be robust to view-specific noise. Further, to combat catastrophic forgetting, a structure alignment mechanism is proposed to enable the model to explore the global group structure across multiple views. Experiments on several multi-view benchmarks demonstrate the effectiveness of our proposed method on the CMVC task. The code is available at: AdaptCMVC",
    "checked": true,
    "id": "20eb73b11d6663e4fa38baa9e5370fbc12a44b0f",
    "semantic_title": "adaptcmvc: robust adaption to incremental views in continual multi-view clustering",
    "citation_count": 0,
    "authors": [
      "Jing Wang",
      "Songhe Feng",
      "Kristoffer Knutsen Wickstrøm",
      "Michael C. Kampffmeyer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wei_Omni-Scene_Omni-Gaussian_Representation_for_Ego-Centric_Sparse-View_Scene_Reconstruction_CVPR_2025_paper.html": {
    "title": "Omni-Scene: Omni-Gaussian Representation for Ego-Centric Sparse-View Scene Reconstruction",
    "volume": "main",
    "abstract": "Prior works employing pixel-based Gaussian representation have demonstrated efficacy in feed-forward sparse-view reconstruction. However, such representation necessitates cross-view overlap for accurate depth estimation, and is challenged by object occlusions and frustum truncations. As a result, these methods require scene-centric data acquisition to maintain cross-view overlap and complete scene visibility to circumvent occlusions and truncations, which limits their applicability to scene-centric reconstruction. In contrast, in autonomous driving scenarios, a more practical paradigm is ego-centric reconstruction, which is characterized by minimal cross-view overlap and frequent occlusions and truncations. The limitations of pixel-based representation thus hinder the utility of prior works in this task. In light of this, this paper conducts an in-depth analysis of different representations, and introduces Omni-Gaussian representation with tailored network design to complement their strengths and mitigate their drawbacks. Experiments show that our method significantly surpasses state-of-the-art methods, pixelSplat and MVSplat, in ego-centric reconstruction, and achieves comparable performance to prior works in scene-centric reconstruction. Our code is available at https://github.com/WU-CVGL/Omni-Scene",
    "checked": true,
    "id": "a3cf0b5ad5aa6682b4a8a00d1b3b615730022f1a",
    "semantic_title": "omni-scene: omni-gaussian representation for ego-centric sparse-view scene reconstruction",
    "citation_count": 2,
    "authors": [
      "Dongxu Wei",
      "Zhiqi Li",
      "Peidong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion_CVPR_2025_paper.html": {
    "title": "3DTopia-XL: Scaling High-quality 3D Asset Generation via Primitive Diffusion",
    "volume": "main",
    "abstract": "The increasing demand for high-quality 3D assets across various industries necessitates efficient and automated 3D content creation. Despite recent advancements in 3D generative models, existing methods still face challenges with optimization speed, geometric fidelity, and the lack of assets for physically based rendering (PBR). In this paper, we introduce 3DTopia-XL, a scalable native 3D generative model designed to overcome these limitations. 3DTopia-XL leverages a novel primitive-based 3D representation, PrimX, which encodes detailed shape, albedo, and material field into a compact tensorial format, facilitating the modeling of high-resolution geometry with PBR assets. On top of the novel representation, we propose a generative framework based on Diffusion Transformer (DiT), which comprises 1) Primitive Patch Compression, 2) and Latent Primitive Diffusion. 3DTopia-XL learns to generate high-quality 3D assets from textual or visual inputs. Extensive qualitative and quantitative experiments are conducted to demonstrate that 3DTopia-XL significantly outperforms existing methods in generating high-quality 3D assets with fine-grained textures and materials, efficiently bridging the quality gap between generative models and real-world applications",
    "checked": true,
    "id": "7ff2a57cb9c6bc65e87ebc1725f118fddf5629c4",
    "semantic_title": "3dtopia-xl: scaling high-quality 3d asset generation via primitive diffusion",
    "citation_count": 23,
    "authors": [
      "Zhaoxi Chen",
      "Jiaxiang Tang",
      "Yuhao Dong",
      "Ziang Cao",
      "Fangzhou Hong",
      "Yushi Lan",
      "Tengfei Wang",
      "Haozhe Xie",
      "Tong Wu",
      "Shunsuke Saito",
      "Liang Pan",
      "Dahua Lin",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_UA-Pose_Uncertainty-Aware_6D_Object_Pose_Estimation_and_Online_Object_Completion_CVPR_2025_paper.html": {
    "title": "UA-Pose: Uncertainty-Aware 6D Object Pose Estimation and Online Object Completion with Partial References",
    "volume": "main",
    "abstract": "6D object pose estimation has shown strong generalizability to novel objects. However, existing methods often require either a complete, well-reconstructed 3D model or numerous reference images that fully cover the object. Estimating 6D poses from partial references, which capture only fragments of an object's appearance and geometry, remains challenging. To address this, we propose UA-Pose, an uncertainty-aware approach for 6D object pose estimation and online object completion specifically designed for partial references. We assume access to either (1) a limited set of RGBD images with known poses or (2) a single 2D image. For the first case, we initialize a partial object 3D model based on the provided images and poses, while for the second, we use image-to-3D techniques to generate an initial object 3D model. Our method integrates uncertainty into the incomplete 3D model, distinguishing between seen and unseen regions. This uncertainty enables confidence assessment in pose estimation and guides an uncertainty-aware sampling strategy for online object completion, enhancing robustness in pose estimation accuracy and improving object completeness. We evaluate our method on the YCB-Video, YCBInEOAT, and HO3D datasets, including RGBD sequences of YCB objects manipulated by robots and human hands. Experimental results demonstrate significant performance improvements over existing methods, particularly when object observations are incomplete or partially captured",
    "checked": true,
    "id": "59d9cb6156b54cc02e69056132af69c80d777508",
    "semantic_title": "ua-pose: uncertainty-aware 6d object pose estimation and online object completion with partial references",
    "citation_count": 0,
    "authors": [
      "Ming-Feng Li",
      "Xin Yang",
      "Fu-En Wang",
      "Hritam Basak",
      "Yuyin Sun",
      "Shreekant Gayaka",
      "Min Sun",
      "Cheng-Hao Kuo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Missing_Target-Relevant_Information_Prediction_with_World_Model_for_Accurate_Zero-Shot_CVPR_2025_paper.html": {
    "title": "Missing Target-Relevant Information Prediction with World Model for Accurate Zero-Shot Composed Image Retrieval",
    "volume": "main",
    "abstract": "Zero-Shot Composed Image Retrieval (ZS-CIR) involves diverse tasks with a broad range of visual content manipulation intent across domain, scene, object, and attribute. The key challenge for ZS-CIR tasks is to modify a reference image according to manipulation text to accurately retrieve a target image, especially when the reference image is missing essential target content. In this paper, we propose a novel prediction-based mapping network, named PrediCIR, to adaptively predict the missing target visual content in reference images in the latent space before mapping for accurate ZS-CIR. Specifically, a world view generation module first constructs a source view by omitting certain visual content of a target view, coupled with an action that includes the manipulation intent derived from existing image-caption pairs. Then, a target content prediction module trains a world model as a predictor to adaptively predict the missing visual information guided by user intention in manipulating text at the latent space. The two modules map an image with the predicted relevant information to a pseudo-word token without extra supervision. Our model shows strong generalization ability on six ZS-CIR tasks. It obtains consistent and significant performance boosts ranging from 1.73% to 4.45% over the best methods and achieves new state-of-the-art results on ZS-CIR. Our code is available at https://github.com/Pter61/predicir",
    "checked": true,
    "id": "aa42b6b5757164a6fc8b1a9c765194796a2a447c",
    "semantic_title": "missing target-relevant information prediction with world model for accurate zero-shot composed image retrieval",
    "citation_count": 2,
    "authors": [
      "Yuanmin Tang",
      "Jing Yu",
      "Keke Gai",
      "Jiamin Zhuang",
      "Gang Xiong",
      "Gaopeng Gou",
      "Qi Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Binarized_Mamba-Transformer_for_Lightweight_Quad_Bayer_HybridEVS_Demosaicing_CVPR_2025_paper.html": {
    "title": "Binarized Mamba-Transformer for Lightweight Quad Bayer HybridEVS Demosaicing",
    "volume": "main",
    "abstract": "Quad Bayer demosaicing is the central challenge for enabling the widespread application of Hybrid Event-based Vision Sensors (HybridEVS). Although existing learning-based methods that leverage long-range dependency modeling have achieved promising results, their complexity severely limits deployment on mobile devices for real-world applications. To address these limitations, we propose a lightweight Mamba-based binary neural network designed for efficient and high-performing demosaicing of HybridEVS RAW images. First, to effectively capture both global and local dependencies, we introduce a hybrid Binarized Mamba-Transformer architecture that combines the strengths of the Mamba and Swin Transformer architectures. Next, to significantly reduce computational complexity, we propose a binarized Mamba (Bi-Mamba), which binarizes all projections while retaining the core Selective Scan in full precision. Bi-Mamba also incorporates additional global visual information to enhance global context and mitigate precision loss. We conduct quantitative and qualitative experiments to demonstrate the effectiveness of BMTNet in both performance and computational efficiency, providing a lightweight demosaicing solution suited for real-world edge devices. Our codes and models are available at https://github.com/Clausy9/BMTNet",
    "checked": true,
    "id": "f0cbb7ec8c2ed947d3e9e21cca1e92cb1d95cb77",
    "semantic_title": "binarized mamba-transformer for lightweight quad bayer hybridevs demosaicing",
    "citation_count": 1,
    "authors": [
      "Shiyang Zhou",
      "Haijin Zeng",
      "Yunfan Lu",
      "Tong Shao",
      "Ke Tang",
      "Yongyong Chen",
      "Jie Liu",
      "Jingyong Su"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_DiffSensei_Bridging_Multi-Modal_LLMs_and_Diffusion_Models_for_Customized_Manga_CVPR_2025_paper.html": {
    "title": "DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation",
    "volume": "main",
    "abstract": "Story visualization, the task of creating visual narratives from textual descriptions, has seen progress with text-to-image generation models. However, these models often lack effective control over character appearances and interactions, particularly in multi-character scenes. To address these limitations, we propose a new task: customized manga generation and introduce DiffSensei, an innovative framework specifically designed for generating manga with dynamic multi-character control. DiffSensei integrates a diffusion-based image generator with a multimodal large language model (MLLM) that acts as a text-compatible identity adapter. Our approach employs masked cross-attention to seamlessly incorporate character features, enabling precise layout control without direct pixel transfer. Additionally, the MLLM-based adapter adjusts character features to align with panel-specific text cues, allowing flexible adjustments in character expressions, poses, and actions. We also introduce MangaZero, a large-scale dataset tailored to this task, containing 43,264 manga pages and 427,147 annotated panels, supporting the visualization of varied character interactions and movements across sequential frames. Extensive experiments demonstrate that DiffSensei outperforms existing models, marking a significant advancement in manga generation by enabling text-adaptable character customization. The code, model, and dataset will be open-sourced to the community",
    "checked": true,
    "id": "ecbc2f63c0216c82fa5a22b2f33454b8f25e5a35",
    "semantic_title": "diffsensei: bridging multi-modal llms and diffusion models for customized manga generation",
    "citation_count": 5,
    "authors": [
      "Jianzong Wu",
      "Chao Tang",
      "Jingbo Wang",
      "Yanhong Zeng",
      "Xiangtai Li",
      "Yunhai Tong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hur_Narrating_the_Video_Boosting_Text-Video_Retrieval_via_Comprehensive_Utilization_of_CVPR_2025_paper.html": {
    "title": "Narrating the Video: Boosting Text-Video Retrieval via Comprehensive Utilization of Frame-Level Captions",
    "volume": "main",
    "abstract": "In recent text-video retrieval, the use of additional captions from vision-language models has shown promising effects on the performance. However, existing models using additional captions often have struggled to capture the rich semantics, including temporal changes, inherent in the video. In addition, incorrect information caused by generative models can lead to inaccurate retrieval. To address these issues, we propose a new framework, Narrating the Video (NarVid), which strategically leverages the comprehensive information available from frame-level captions, the narration. The proposed NarVid exploits narration in multiple ways: 1) feature enhancement through cross-modal interactions between narration and video, 2) query-aware adaptive filtering to suppress irrelevant or incorrect information, 3) dual-modal matching score by adding query-video similarity and query-narration similarity, and 4) hard-negative loss to learn discriminative features from multiple perspectives using the two similarities from different views. Experimental results demonstrate that NarVid achieves state-of-the-art performance on various benchmark datasets",
    "checked": true,
    "id": "73aeb825bffdd06a9d8580aba65b02de5a0a9b32",
    "semantic_title": "narrating the video: boosting text-video retrieval via comprehensive utilization of frame-level captions",
    "citation_count": 1,
    "authors": [
      "Chan Hur",
      "Jeong-hun Hong",
      "Dong-hun Lee",
      "Dabin Kang",
      "Semin Myeong",
      "Sang-hyo Park",
      "Hyeyoung Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_IDEA-Bench_How_Far_are_Generative_Models_from_Professional_Designing_CVPR_2025_paper.html": {
    "title": "IDEA-Bench: How Far are Generative Models from Professional Designing?",
    "volume": "main",
    "abstract": "Recent advancements in image generation models enable the creation of high-quality images and targeted modifications based on textual instructions. Some models even support multimodal complex guidance and demonstrate robust task generalization capabilities. However, they still fall short of meeting the nuanced, professional demands of designers. To bridge this gap, we introduce IDEA-Bench, a comprehensive benchmark designed to advance image generation models toward applications with robust task generalization. IDEA-Bench comprises 100 professional image generation tasks and 275 specific cases, categorized into five major types based on the current capabilities of existing models. Furthermore, we provide a representative subset of 18 tasks with enhanced evaluation criteria to facilitate more nuanced and reliable evaluations using Multimodal Large Language Models (MLLMs). By assessing models' ability to comprehend and execute novel, complex tasks, IDEA-Bench paves the way toward the development of generative models with autonomous and versatile visual generation capabilities",
    "checked": true,
    "id": "c3871abb91c8d0d97a5a0b46655a14392d2af91c",
    "semantic_title": "idea-bench: how far are generative models from professional designing?",
    "citation_count": 1,
    "authors": [
      "Chen Liang",
      "Lianghua Huang",
      "Jingwu Fang",
      "Huanzhang Dou",
      "Wei Wang",
      "Zhi-Fan Wu",
      "Yupeng Shi",
      "Junge Zhang",
      "Xin Zhao",
      "Yu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Interpretable_Image_Classification_via_Non-parametric_Part_Prototype_Learning_CVPR_2025_paper.html": {
    "title": "Interpretable Image Classification via Non-parametric Part Prototype Learning",
    "volume": "main",
    "abstract": "Classifying images with an interpretable decision-making process is a long-standing problem in computer vision. In recent years, Prototypical Part Networks has gained traction as an approach for self-explainable neural networks, due to their ability to mimic human visual reasoning by providing explanations based on prototypical object parts. However, the quality of the explanations generated by these methods leaves room for improvement, as the prototypes usually focus on repetitive and redundant concepts. Leveraging recent advances in prototype learning, we present a framework for part-based interpretable image classification that learns a set of semantically distinctive object parts for each class, and provides diverse and comprehensive explanations. The core of our method is to learn the part-prototypes in a non-parametric fashion, through clustering deep features extracted from foundation vision models that encode robust semantic information. To quantitatively evaluate the quality of explanations provided by ProtoPNets, we introduce Distinctiveness Score and Comprehensiveness Score. Through evaluation on CUB-200-2011, Stanford Cars and Stanford Dogs datasets, we show that our framework compares favourably against existing ProtoPNets while achieving better interpretability",
    "checked": true,
    "id": "4a6cd73feab653b8b878900d4546fa5abb30bd6a",
    "semantic_title": "interpretable image classification via non-parametric part prototype learning",
    "citation_count": 0,
    "authors": [
      "Zhijie Zhu",
      "Lei Fan",
      "Maurice Pagnucco",
      "Yang Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset_CVPR_2025_paper.html": {
    "title": "PhD: A ChatGPT-Prompted Visual Hallucination Evaluation Dataset",
    "volume": "main",
    "abstract": "Multimodal Large Language Models (MLLMs) hallucinate, resulting in an emerging topic of visual hallucination evaluation (VHE). This paper contributes a ChatGPT-Prompted visual hallucination evaluation Dataset (PhD) for objective VHE at a large scale. The essence of VHE is to ask an MLLM questions about specific images to assess its susceptibility to hallucination. Depending on what to ask (objects, attributes, sentiment, etc.) and how the questions are asked, we structure PhD along two dimensions, i.e. task and mode. Five visual recognition tasks, ranging from low-level (object / attribute recognition) to middle-level (sentiment / position recognition and counting), are considered. Besides a normal visual QA mode, which we term PhD-base, PhD also asks questions with specious context (PhD-sec) or with incorrect context (PhD-icc), or with AI-generated counter common sense images (PhD-ccs). We construct PhD by a ChatGPT-assisted semi-automated pipeline, encompassing four pivotal modules: task-specific hallucinatory item (hitem) selection, hitem-embedded question generation, specious / incorrect context generation, and counter-common-sense (CCS) image generation. With over 14k daily images, 750 CCS images and 102k VQA triplets in total, PhD reveals considerable variability in MLLMs' performance across various modes and tasks, offering valuable insights into the nature of hallucination. As such, PhD stands as a potent tool not only for VHE but may also play a significant role in the refinement of MLLMs",
    "checked": true,
    "id": "0daceeb8f42ad6ec846f80f7407500192d2046ee",
    "semantic_title": "phd: a chatgpt-prompted visual hallucination evaluation dataset",
    "citation_count": 10,
    "authors": [
      "Jiazhen Liu",
      "Yuhan Fu",
      "Ruobing Xie",
      "Runquan Xie",
      "Xingwu Sun",
      "Fengzong Lian",
      "Zhanhui Kang",
      "Xirong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Greer_CARL_A_Framework_for_Equivariant_Image_Registration_CVPR_2025_paper.html": {
    "title": "CARL: A Framework for Equivariant Image Registration",
    "volume": "main",
    "abstract": "Image registration estimates spatial correspondences between image pairs. These estimates are typically obtained via numerical optimization or regression by a deep network. A desirable property is that a correspondence estimate (e.g., the true oracle correspondence) for an image pair is maintained under deformations of the input images. Formally, the estimator should be equivariant to a desired class of image transformations. In this work, we present careful analyses of equivariance properties in the context of multi-step deep registration networks. Based on these analyses we 1) introduce the notions of [U,U] equivariance (network equivariance to the same deformations of the input images) and [W,U] equivariance (where input images can undergo different deformations); we 2) show that in a suitable multi-step registration setup it is sufficient for overall [W,U] equivariance if the first step has [W,U] equivariance and all others have [U,U] equivariance; we 3) show that common displacement-predicting networks only exhibit [U,U] equivariance to translations instead of the more powerful [W,U] equivariance; and we 4) show how to achieve multi-step [W,U] equivariance via a coordinate-attention mechanism combined with displacement-predicting networks. Our approach obtains excellent practical performance for 3D abdomen, lung, and brain medical image registration. We match or outperform state-of-the-art (SOTA) registration approaches on all the datasets with a particularly strong performance for the challening abdomen registration",
    "checked": true,
    "id": "4e98e0c61002a4b5e689cfa901afc143bdab53fd",
    "semantic_title": "carl: a framework for equivariant image registration",
    "citation_count": 0,
    "authors": [
      "Hastings Greer",
      "Lin Tian",
      "François-Xavier Vialard",
      "Roland Kwitt",
      "Raul San Jose Estepar",
      "Marc Niethammer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World_CVPR_2025_paper.html": {
    "title": "ClimbingCap: Multi-Modal Dataset and Method for Rock Climbing in World Coordinate",
    "volume": "main",
    "abstract": "Human Motion Recovery (HMR) research mainly focuses on ground-based motions such as running. The study on capturing climbing motion, an off-ground motion, is sparse. This is partly due to the limited availability of climbing motion datasets, especially large-scale and challenging 3D labeled datasets. To address the insufficiency of climbing motion datasets, we collect AscendMotion, a large-scale well-annotated, and challenging climbing motion dataset. It consists of 412k RGB, LiDAR frames, and IMU measurements, which includes the challenging climbing motions of 22 professional climbing coaches across 12 different rocks. Capturing the climbing motions is challenging as it requires precise recovery of not only the complex pose but also the global position of climbers. Although multiple global HMR methods have been proposed, they cannot faithfully capture climbing motions. To address the limitations of HMR methods for climbing, we propose ClimbingCap, a motion recovery method that reconstructs continuous 3D human climbing motion in a global coordinate system. One key insight is to use the RGB and the LiDAR modalities to separately reconstruct motions in camera coordinates and global coordinates and optimize them jointly. We demonstrate the quality of the AscendMotion dataset and present promising results from ClimbingCap. The AscendMotion dataset and the source code of ClimbingCap will be released publicly to the research community",
    "checked": true,
    "id": "8442504b07eb8c67600ff803c8e880df74742261",
    "semantic_title": "climbingcap: multi-modal dataset and method for rock climbing in world coordinate",
    "citation_count": 0,
    "authors": [
      "Ming Yan",
      "Xincheng Lin",
      "Yuhua Luo",
      "Shuqi Fan",
      "Yudi Dai",
      "Qixin Zhong",
      "Lincai Zhong",
      "Yuexin Ma",
      "Lan Xu",
      "Chenglu Wen",
      "Siqi Shen",
      "Cheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhuang_DAGSM_Disentangled_Avatar_Generation_with_GS-enhanced_Mesh_CVPR_2025_paper.html": {
    "title": "DAGSM: Disentangled Avatar Generation with GS-enhanced Mesh",
    "volume": "main",
    "abstract": "Text-driven avatar generation has gained significant attention owing to its convenience. However, existing methods typically model the human body with all garments as a single 3D model, limiting its usability, such as clothing replacement, and reducing user control over the generation process. To overcome the limitations above, we propose DAGSM, a novel pipeline that generates disentangled human bodies and garments from the given text prompts. Specifically, we model each part (e.g., body, upper/lower clothes) of the clothed human as one GS-enhanced mesh (GSM), which is a traditional mesh attached with 2D Gaussians to better handle complicated textures (e.g., woolen, translucent clothes) and produce realistic cloth animations. During the generation, we first create the unclothed body, followed by a sequence of individual cloth generation based on the body, where we introduce a semantic-based algorithm to achieve better human-cloth and garment-garment separation. To improve texture quality, we propose a view-consistent texture refinement module, including a cross-view attention mechanism for texture style consistency and an incident-angle-weighted denoising (IAW-DE) strategy to update the appearance. Extensive experiments have demonstrated that DAGSM generates high-quality disentangled avatars, supports clothing replacement and realistic animation, and outperforms the baselines in visual quality",
    "checked": true,
    "id": "7e7e4f090132dae0e753a8a38bdefbfeb89122e8",
    "semantic_title": "dagsm: disentangled avatar generation with gs-enhanced mesh",
    "citation_count": 1,
    "authors": [
      "Jingyu Zhuang",
      "Di Kang",
      "Linchao Bao",
      "Liang Lin",
      "Guanbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World_CVPR_2025_paper.html": {
    "title": "Estimating Body and Hand Motion in an Ego-sensed World",
    "volume": "main",
    "abstract": "We present EgoAllo, a system for human motion estimation from a head-mounted device. Using only egocentric SLAM poses and images, EgoAllo guides sampling from a conditional diffusion model to estimate 3D body pose, height, and hand parameters that capture a device wearer's actions in the allocentric coordinate frame of the scene. To achieve this, our key insight is in representation: we propose spatial and temporal invariance criteria for improving model performance, from which we derive a head motion conditioning parameterization that improves estimation by up to 18%. We also show how the bodies estimated by our system can improve hand estimation: the resulting kinematic and temporal constraints can reduce world-frame errors in single-frame estimates by 40%",
    "checked": true,
    "id": "96c7b6ce9952f935530cf2b55ad47629d8016fac",
    "semantic_title": "estimating body and hand motion in an ego-sensed world",
    "citation_count": 8,
    "authors": [
      "Brent Yi",
      "Vickie Ye",
      "Maya Zheng",
      "Yunqi Li",
      "Lea Müller",
      "Georgios Pavlakos",
      "Yi Ma",
      "Jitendra Malik",
      "Angjoo Kanazawa"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guillaro_A_Bias-Free_Training_Paradigm_for_More_General_AI-generated_Image_Detection_CVPR_2025_paper.html": {
    "title": "A Bias-Free Training Paradigm for More General AI-generated Image Detection",
    "volume": "main",
    "abstract": "Successful forensic detectors can produce excellent results in supervised learning benchmarks but struggle to transfer to real-world applications. We believe this limitation is largely due to inadequate training data quality. While most research focuses on developing new algorithms, less attention is given to training data selection, despite evidence that performance can be strongly impacted by spurious correlations such as content, format, or resolution. A well-designed forensic detector should detect generator specific artifacts rather than reflect data biases. To this end, we propose B-Free, a bias-free training paradigm, where fake images are generated from real ones using the conditioning procedure of stable diffusion models. This ensures semantic alignment between real and fake images, allowing any differences to stem solely from the subtle artifacts introduced by AI generation. Through content-based augmentation, we show significant improvements in both generalization and robustness over state-of-the-art detectors and more calibrated results across 27 different generative models, including recent releases, like FLUX and Stable Diffusion 3.5. Our findings emphasize the importance of a careful dataset design, highlighting the need for further research on this topic. Code and data are publicly available at https://grip-unina.github.io/B-Free/",
    "checked": true,
    "id": "7bd8343e28a1c6adab29c721a08aee878fbf3ada",
    "semantic_title": "a bias-free training paradigm for more general ai-generated image detection",
    "citation_count": 6,
    "authors": [
      "Fabrizio Guillaro",
      "Giada Zingarini",
      "Ben Usman",
      "Avneesh Sud",
      "Davide Cozzolino",
      "Luisa Verdoliva"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Truong_FALCON_Fairness_Learning_via_Contrastive_Attention_Approach_to_Continual_Semantic_CVPR_2025_paper.html": {
    "title": "FALCON: Fairness Learning via Contrastive Attention Approach to Continual Semantic Scene Understanding",
    "volume": "main",
    "abstract": "Continual Learning in semantic scene segmentation aims to continually learn new unseen classes in dynamic environments while maintaining previously learned knowledge. Prior studies focused on modeling the catastrophic forgetting and background shift challenges in continual learning. However, fairness, another major challenge that causes unfair predictions leading to low performance among major and minor classes, still needs to be well addressed. In addition, prior methods have yet to model the unknown classes well, thus resulting in producing non-discriminative features among unknown classes. This work presents a novel Fairness Learning via Contrastive Attention Approach to continual learning in semantic scene understanding. In particular, we first introduce a new Fairness Contrastive Clustering loss to address the problems of catastrophic forgetting and fairness. Then, we propose an attention-based visual grammar approach to effectively model the background shift problem and unknown classes, producing better feature representations for different unknown classes. Through our experiments, our proposed approach achieves State-of-the-Art (SoTA) performance on different continual learning benchmarks, i.e., ADE20K, Cityscapes, and Pascal VOC. It promotes the fairness of the continual semantic segmentation model",
    "checked": false,
    "id": "12a7afdf90675ed21a442292317f3bb8f23e7e7e",
    "semantic_title": "falcon: fairness learning via contrastive attention approach to continual semantic scene understanding in open world",
    "citation_count": 3,
    "authors": [
      "Thanh-Dat Truong",
      "Utsav Prabhu",
      "Bhiksha Raj",
      "Jackson Cothren",
      "Khoa Luu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bahari_Certified_Human_Trajectory_Prediction_CVPR_2025_paper.html": {
    "title": "Certified Human Trajectory Prediction",
    "volume": "main",
    "abstract": "Predicting human trajectories is essential for the safe operation of autonomous vehicles, yet current data-driven models often lack robustness in case of noisy inputs such as adversarial examples or imperfect observations. Although some trajectory prediction methods have been developed to provide empirical robustness, these methods are heuristic and do not offer guaranteed robustness. In this work, we propose a certification approach tailored for trajectory prediction that provides guaranteed robustness. To this end, we address the unique challenges associated with trajectory prediction, such as unbounded outputs and multi-modality. To mitigate the inherent performance drop through certification, we propose a diffusion-based trajectory denoiser and integrate it into our method. Moreover, we introduce new certified performance metrics to reliably measure the trajectory prediction performance. Through comprehensive experiments, we demonstrate the accuracy and robustness of the certified predictors and highlight their advantages over the non-certified ones. The code is available online: https://s-attack.github.io/",
    "checked": true,
    "id": "89c5394cacb127b15a38bca91e1d1b9a9dec9012",
    "semantic_title": "certified human trajectory prediction",
    "citation_count": 2,
    "authors": [
      "Mohammadhossein Bahari",
      "Saeed Saadatnejad",
      "Amirhossein Askari Farsangi",
      "Seyed-Mohsen Moosavi-Dezfooli",
      "Alexandre Alahi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Aghzal_Evaluating_Vision-Language_Models_as_Evaluators_in_Path_Planning_CVPR_2025_paper.html": {
    "title": "Evaluating Vision-Language Models as Evaluators in Path Planning",
    "volume": "main",
    "abstract": "Despite their promise to perform complex reasoning, large language models (LLMs) have been shown to have limited effectiveness in end-to-end planning. This has inspired an intriguing question: if these models cannot plan well, can they still contribute to the planning framework as a helpful plan evaluator? In this work, we generalize this question to consider LLMs augmented with visual understanding, i.e., Vision-Language Models (VLMs). We introduce **PathEval**, a novel benchmark evaluating VLMs as plan evaluators in complex path-planning scenarios. Succeeding in the benchmark requires a VLM to be able to abstract traits of optimal paths from the scenario description, demonstrate precise low-level perception on each path, and integrate this information to decide the better path. Our analysis of state-of-the-art VLMs reveals that these models face significant challenges on the benchmark. We observe that the VLMs can precisely abstract given scenarios to identify the desired traits and exhibit mixed performance in integrating the provided information. Yet, their vision component presents a critical bottleneck, with models struggling to perceive low-level details about a path. Our experimental results show that this issue cannot be trivially addressed via end-to-end fine-tuning; rather, task-specific discriminative adaptation of these vision encoders is needed for these VLMs to become effective path evaluators",
    "checked": true,
    "id": "2e62d76e381dd81ed2544e6a408a5ff15b1002b3",
    "semantic_title": "evaluating vision-language models as evaluators in path planning",
    "citation_count": 1,
    "authors": [
      "Mohamed Aghzal",
      "Xiang Yue",
      "Erion Plaku",
      "Ziyu Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dai_Free_on_the_Fly_Enhancing_Flexibility_in_Test-Time_Adaptation_with_CVPR_2025_paper.html": {
    "title": "Free on the Fly: Enhancing Flexibility in Test-Time Adaptation with Online EM",
    "volume": "main",
    "abstract": "Vision-Language Models (VLMs) have become prominent in open-world image recognition for their strong generalization abilities. Yet, their effectiveness in practical applications is compromised by domain shifts and distributional changes, especially when test data distributions diverge from training data. Therefore, the paradigm of test-time adaptation (TTA) has emerged, enabling the use of online off-the-shelf data at test time, supporting independent sample predictions, and eliminating reliance on test annotations. Traditional TTA methods, however, often rely on costly training or optimization processes, or make unrealistic assumptions about accessing or storing historical training and test data.Instead, this study proposes FreeTTA, a training-free and universally available method that makes no assumptions, to enhance the flexibility of TTA. More importantly, FreeTTA is the first to explicitly model the test data distribution, enabling the use of intrinsic relationships among test samples to enhance predictions of individual samples without simultaneous access--a direction not previously explored. FreeTTA achieves these advantages by introducing an online EM algorithm that utilizes zero-shot predictions from VLMs as priors to iteratively compute the posterior probabilities of each online test sample and update parameters. Experiments demonstrate that FreeTTA achieves stable and significant improvements compared to state-of-the-art methods across 15 datasets in both cross-domain and out-of-distribution settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyuan Dai",
      "Sibei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Transformers_without_Normalization_CVPR_2025_paper.html": {
    "title": "Transformers without Normalization",
    "volume": "main",
    "abstract": "Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation DyT(x)=tanh(ax), as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, S-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks",
    "checked": true,
    "id": "1e178f0c5bb9709ae5c7bdb60ecd76f00b0fcd86",
    "semantic_title": "transformers without normalization",
    "citation_count": 18,
    "authors": [
      "Jiachen Zhu",
      "Xinlei Chen",
      "Kaiming He",
      "Yann LeCun",
      "Zhuang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_SGC-Net_Stratified_Granular_Comparison_Network_for_Open-Vocabulary_HOI_Detection_CVPR_2025_paper.html": {
    "title": "SGC-Net: Stratified Granular Comparison Network for Open-Vocabulary HOI Detection",
    "volume": "main",
    "abstract": "Recent open-vocabulary human-object interaction (OV-HOI) detection methods primarily rely on large language model (LLM) for generating auxiliary descriptions and leverage knowledge distilled from CLIP to detect unseen interaction categories. Despite their effectiveness, these methods face two challenges: (1) feature granularity deficiency, due to reliance on last layer visual features for text alignment, leading to the neglect of crucial object-level details from intermediate layers; (2) semantic similarity confusion, resulting from CLIP's inherent biases toward certain classes, while LLM-generated descriptions based solely on labels fail to adequately capture inter-class similarities. To address these challenges, we propose a stratified granular comparison network. First, we introduce a granularity sensing alignment module that aggregates global semantic features with local details, refining interaction representations and ensuring robust alignment between intermediate visual features and text embeddings. Second, we develop a hierarchical group comparison module that recursively compares and groups classes using LLMs, generating fine-grained and discriminative descriptions for each interaction category. Experimental results on two widely-used benchmark datasets, SWIG-HOI and HICO-DET, demonstrate that our method achieves state-of-the-art results in OV-HOI detection. Codes is available at https://github.com/Phil0212/SGC-Net",
    "checked": true,
    "id": "d2cb8e0540566eafb2db20f96be3cf8ff020d74e",
    "semantic_title": "sgc-net: stratified granular comparison network for open-vocabulary hoi detection",
    "citation_count": 0,
    "authors": [
      "Xin Lin",
      "Chong Shi",
      "Zuopeng Yang",
      "Haojin Tang",
      "Zhili Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding_CVPR_2025_paper.html": {
    "title": "Galaxy Walker: Geometry-aware VLMs For Galaxy-scale Understanding",
    "volume": "main",
    "abstract": "Modern vision-language models (VLMs) develop patch embedding and convolution backbone within vector space, especially Euclidean ones, at the very founding. When expanding VLMs to a galaxy-scale for understanding astronomical phenomena, the integration of spherical space for planetary orbits and hyperbolic spaces for black holes raises two formidable challenges. a) The current pre-training model is confined to Euclidean space rather than a comprehensive geometric embedding. b) The predominant architecture lacks suitable backbones for anisotropic physical geometries. In this paper, we introduced Galaxy-Walker, a geometry-aware VLM, for the universe-level vision understanding tasks. We proposed the geometry prompt that generates geometry tokens by random walks across diverse spaces on a multi-scale physical graph, along with a geometry adapter that compresses and reshapes the space anisotropy in a mixture-of-experts manner. Extensive experiments demonstrate the effectiveness of our approach, with Galaxy-Walker achieving state-of-the-art performance in both galaxy property estimation (R2 scores up to 0.91) and morphology classification tasks (up to +0.17 F1 improvement in challenging features), significantly outperforming both domain-specific models and general-purpose VLMs",
    "checked": true,
    "id": "7e968919b47267df9bb238f7db3591d9933b1351",
    "semantic_title": "galaxy walker: geometry-aware vlms for galaxy-scale understanding",
    "citation_count": 0,
    "authors": [
      "Tianyu Chen",
      "Xingcheng Fu",
      "Yisen Gao",
      "Haodong Qian",
      "Yuecen Wei",
      "Kun Yan",
      "Haoyi Zhou",
      "Jianxin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_HiPART_Hierarchical_Pose_AutoRegressive_Transformer_for_Occluded_3D_Human_Pose_CVPR_2025_paper.html": {
    "title": "HiPART: Hierarchical Pose AutoRegressive Transformer for Occluded 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "Existing 2D-to-3D human pose estimation (HPE) methods struggle with the occlusion issue by enriching information like temporal and visual cues in the lifting stage. In this paper, we argue that these methods ignore the limitation of the sparse skeleton 2D input representation, which fundamentally restricts the 2D-to-3D lifting and worsens the occlusion issue. To address these, we propose a novel two-stage generative densification method, named Hierarchical Pose AutoRegressive Transformer (HiPART), to generate hierarchical 2D dense poses from the original sparse 2D pose. Specifically, we first develop a multi-scale skeleton tokenization module to quantize the highly dense 2D pose into hierarchical tokens and propose a skeleton-aware alignment to strengthen token connections. We then develop a hierarchical autoregressive modeling scheme for hierarchical 2D pose generation. With generated hierarchical poses as inputs for 2D-to-3D lifting, the proposed method shows strong robustness in occluded scenarios and achieves state-of-the-art performance on the single-frame-based 3D HPE. Moreover, it outperforms numerous multi-frame methods while reducing parameter and computational complexity and can also complement them to further enhance performance and robustness",
    "checked": true,
    "id": "451fdd5000033631fce439b4fe215eed185ed247",
    "semantic_title": "hipart: hierarchical pose autoregressive transformer for occluded 3d human pose estimation",
    "citation_count": 1,
    "authors": [
      "Hongwei Zheng",
      "Han Li",
      "Wenrui Dai",
      "Ziyang Zheng",
      "Chenglin Li",
      "Junni Zou",
      "Hongkai Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lai_SnowMaster_Comprehensive_Real-world_Image_Desnowing_via_MLLM_with_Multi-Model_Feedback_CVPR_2025_paper.html": {
    "title": "SnowMaster: Comprehensive Real-world Image Desnowing via MLLM with Multi-Model Feedback Optimization",
    "volume": "main",
    "abstract": "Snowfall presents significant challenges for visual data processing, necessitating specialized desnowing algorithms. However, existing models often fail to generalize effectively due to their heavy reliance on synthetic datasets. Furthermore, current real-world snowfall datasets are limited in scale and lack dedicated evaluation metrics designed specifically for snowfall degradation, thus hindering the effective integration of real snowy images into model training to reduce domain gaps. To address these challenges, we first introduce RealSnow10K, a large-scale, high-quality dataset consisting of over 10,000 annotated real-world snowy images. In addition, we curate a preference dataset comprising 36,000 expert-ranked image pairs, enabling the adaptation of multimodal large language models (MLLMs) to better perceive snowy image quality through our innovative Multi-Model Preference Optimization (MMPO). Finally, we propose the SnowMaster, which employs MMPO-enhanced MLLM to perform accurate snowy image evaluation and pseudo-label filtering for semi-supervised training. Experiments demonstrate that SnowMaster delivers superior desnowing performance under real-world conditions",
    "checked": true,
    "id": "954f41da6ec69f434c5be442c53d9d9122ab31e0",
    "semantic_title": "snowmaster: comprehensive real-world image desnowing via mllm with multi-model feedback optimization",
    "citation_count": 1,
    "authors": [
      "Jianyu Lai",
      "Sixiang Chen",
      "Yunlong Lin",
      "Tian Ye",
      "Yun Liu",
      "Song Fei",
      "Zhaohu Xing",
      "Hongtao Wu",
      "Weiming Wang",
      "Lei Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech_CVPR_2025_paper.html": {
    "title": "From Faces to Voices: Learning Hierarchical Representations for High-quality Video-to-Speech",
    "volume": "main",
    "abstract": "The objective of this study is to generate high-quality speech from silent talking face videos, a task also known as video-to-speech synthesis. A significant challenge in video-to-speech synthesis lies in the substantial modality gap between silent video and multi-faceted speech. In this paper, we propose a novel video-to-speech system that effectively bridges this modality gap, significantly enhancing the quality of synthesized speech. This is achieved by learning of hierarchical representations from video to speech. Specifically, we gradually transform silent video into acoustic feature spaces through three sequential stages -- content, timbre, and prosody modeling. In each stage, we align visual factors -- lip movements, face identity, and facial expressions -- with corresponding acoustic counterparts to ensure the seamless transformation. Additionally, to generate realistic and coherent speech from the visual representations, we employ a flow matching model that estimates direct trajectories from a simple prior distribution to the target speech distribution. Extensive experiments demonstrate that our method achieves exceptional generation quality comparable to real utterances, outperforming existing methods by a significant margin",
    "checked": true,
    "id": "525c82f446aa688bac1960d718000254ac450a68",
    "semantic_title": "from faces to voices: learning hierarchical representations for high-quality video-to-speech",
    "citation_count": 1,
    "authors": [
      "Ji-Hoon Kim",
      "Jeongsoo Choi",
      "Jaehun Kim",
      "Chaeyoung Jung",
      "Joon Son Chung"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_DFM_Differentiable_Feature_Matching_for_Anomaly_Detection_CVPR_2025_paper.html": {
    "title": "DFM: Differentiable Feature Matching for Anomaly Detection",
    "volume": "main",
    "abstract": "Feature matching methods for unsupervised anomaly detection have demonstrated impressive performance. Existing methods primarily rely on self-supervised training and handcrafted matching schemes for task adaptation. However, they can only achieve an inferior feature representation for anomaly detection because the feature extraction and matching modules are separately trained. To address these issues, we propose a Differentiable Feature Matching (DFM) framework for joint optimization of the feature extractor and the matching head. DFM transforms nearest-neighbor matching into a pooling-based module and embeds it within a Feature Matching Network (FMN). This design enables end-to-end feature extraction and feature matching module training, thus providing better feature representation for anomaly detection tasks. DFM is generic and can be incorporated into existing feature-matching methods. We implement DFM with various backbones and conduct extensive experiments across various tasks and datasets, demonstrating its effectiveness. Notably, we achieve state-of-the-art results in the continual anomaly detection task with instance-AUROC improvement of up to 3.9% and pixel-AP improvement of up to 5.5%",
    "checked": false,
    "id": "7bf8fa493e1f2f1c7dad51b9c6c746e9b4a462ba",
    "semantic_title": "representation-enhanced apt detection using contrastive learning",
    "citation_count": 1,
    "authors": [
      "Sheng Wu",
      "Yimi Wang",
      "Xudong Liu",
      "Yuguang Yang",
      "Runqi Wang",
      "Guodong Guo",
      "David Doermann",
      "Baochang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_FlashGS_Efficient_3D_Gaussian_Splatting_for_Large-scale_and_High-resolution_Rendering_CVPR_2025_paper.html": {
    "title": "FlashGS: Efficient 3D Gaussian Splatting for Large-scale and High-resolution Rendering",
    "volume": "main",
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated significant potential over traditional rendering techniques, attracting widespread attention from both industry and academia. However, real-time rendering with 3DGS remains a challenging problem, particularly in large-scale, high-resolution scenes due to the presence of numerous anisotropic Gaussian representations, and it has not been extensively explored. To address this challenge, we introduce FlashGS, an open-source CUDA library with Python bindings, featuring comprehensive algorithm design and optimizations, including redundancy elimination, adaptive scheduling, and efficient pipelining. First, we eliminate substantial redundant computations through precise Gaussian intersection tests, leveraging the intrinsic mechanism of the 3DGS rasterizer. During task partitioning, we propose an adaptive scheduling strategy that accounts for variations in Gaussian size and shape. Additionally, we design a multi-stage pipelining strategy for color computation in the rendering process, further accelerating performance. We conduct an extensive evaluation of FlashGS across a diverse range of synthetic and real-world 3D scenes, encompassing scene sizes of up to 2.7 km^2 cityscape and resolutions of up to over 4K. Our approach improves 3DGS rendering performance by an order of magnitude, achieving an average speedup of 7.2x, and rendering at a minimum of 125.9 FPS, setting a new state-of-the-art in real-time 3DGS rendering. https://github.com/InternLandMark/FlashGS",
    "checked": true,
    "id": "18e4b2a2781c13a0c79a6534f832487bb2d38257",
    "semantic_title": "flashgs: efficient 3d gaussian splatting for large-scale and high-resolution rendering",
    "citation_count": 32,
    "authors": [
      "Guofeng Feng",
      "Siyan Chen",
      "Rong Fu",
      "Zimu Liao",
      "Yi Wang",
      "Tao Liu",
      "Boni Hu",
      "Linning Xu",
      "Zhilin Pei",
      "Hengjie Li",
      "Xiuhong Li",
      "Ninghui Sun",
      "Xingcheng Zhang",
      "Bo Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_PointSR_Self-Regularized_Point_Supervision_for_Drone-View_Object_Detection_CVPR_2025_paper.html": {
    "title": "PointSR: Self-Regularized Point Supervision for Drone-View Object Detection",
    "volume": "main",
    "abstract": "Point-Supervised Object Detection (PSOD) in a discriminative style has recently gained significant attention for its impressive detection performance and cost-effectiveness. However, accurately predicting high-quality pseudo-box labels for drone-view images, which often feature densely packed small objects, remains a challenge. This difficulty arises primarily from the limitation of rigid sampling strategies, which hinder the pseudo-box optimization process. To address this, we propose PointSR, an effective and robust point-supervised object detection framework with self-regularized sampling that integrates temporal and informative constraints throughout the pseudo-box generation process. Specifically, the framework comprises three key components: Temporal-Ensembling Encoder (TE Encoder), Coarse Pseudo-box Prediction, and Pseudo-box Refinement. The TE Encoder builds an anchor prototype library by aggregating temporal information for dynamic anchor adjustment. In Coarse Pseudo-box Prediction, anchors are refined using the prototype library, and a set of informative samples is collected for subsequent refinement. During Pseudo-box Refinement, these informative negative samples are used to suppress low-confidence candidate positive samples, thereby improving the quality of the pseudo-boxes. Experimental results on benchmark datasets demonstrate that PointSR significantly outperforms state-of-the-art methods, achieving up to 2.6%~\\mathbf 7.2% higher AP_ 50 using only point supervision. Additionally, it exhibits strong robustness to perturbation in human-labeled points",
    "checked": true,
    "id": "8b8251c3b063a724c1bb46db0a0fa592a127ef2a",
    "semantic_title": "pointsr: self-regularized point supervision for drone-view object detection",
    "citation_count": 0,
    "authors": [
      "Weizhuo Li",
      "Yue Xi",
      "Wenjing Jia",
      "Zehao Zhang",
      "Fei Li",
      "Xiangzeng Liu",
      "Qiguang Miao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Exploring_Timeline_Control_for_Facial_Motion_Generation_CVPR_2025_paper.html": {
    "title": "Exploring Timeline Control for Facial Motion Generation",
    "volume": "main",
    "abstract": "This paper introduces a new control signal for facial motion generation: timeline control. Compared to audio and text signals, timelines provide more fine-grained control, such as generating specific facial motions with precise timing. Users can specify a multi-track timeline of facial actions arranged in temporal intervals, allowing precise control over the timing of each action. To model the timeline control capability, We first annotate the time intervals of facial actions in natural facial motion sequences at a frame-level granularity. This process is facilitated by Toeplitz Inverse Covariance-based Clustering to minimize human labor. Based on the annotations, we propose a diffusion-based generation model capable of generating facial motions that are natural and accurately aligned with input timelines. Our method supports text-guided motion generation by using ChatGPT to convert text into timelines. Experimental results show that our method can annotate facial action intervals with satisfactory accuracy, and produces natural facial motions accurately aligned with timelines",
    "checked": true,
    "id": "5cc306b099147d27699b10e045e5f342db9a9fb7",
    "semantic_title": "exploring timeline control for facial motion generation",
    "citation_count": 0,
    "authors": [
      "Yifeng Ma",
      "Jinwei Qi",
      "Chaonan Ji",
      "Peng Zhang",
      "Bang Zhang",
      "Zhidong Deng",
      "Liefeng Bo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation_CVPR_2025_paper.html": {
    "title": "v-CLR: View-Consistent Learning for Open-World Instance Segmentation",
    "volume": "main",
    "abstract": "In this paper, we address the challenging problem of open-world instance segmentation. Existing works have shown that vanilla visual networks are biased toward learning appearance information, e.g. texture, to recognize objects. This implicit bias causes the model to fail in detecting novel objects with unseen textures in the open-world setting. To address this challenge, we propose a learning framework, called view-Consistent LeaRning (v-CLR), which aims to enforce the model to learn appearance-invariant representations for robust instance segmentation. In v-CLR, we first introduce additional views for each image, where the texture undergoes significant alterations while preserving the image's underlying structure. We then encourage the model to learn the appearance-invariant representation by enforcing the consistency between object features across different views, for which we obtain class-agnostic object proposals using off-the-shelf unsupervised models that possess strong object-awareness. These proposals enable cross-view object feature matching, greatly reducing the appearance dependency while enhancing the object-awareness. We thoroughly evaluate our method on public benchmarks under both cross-class and cross-dataset settings, achieving state-of-the-art performance. Project page: https://visual-ai.github.io/vclr",
    "checked": true,
    "id": "f5c170a089c4e951cb1f7172008ceaa66d2ca420",
    "semantic_title": "v-clr: view-consistent learning for open-world instance segmentation",
    "citation_count": 0,
    "authors": [
      "Chang-Bin Zhang",
      "Jinhong Ni",
      "Yujie Zhong",
      "Kai Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Chat2SVG_Vector_Graphics_Generation_with_Large_Language_Models_and_Image_CVPR_2025_paper.html": {
    "title": "Chat2SVG: Vector Graphics Generation with Large Language Models and Image Diffusion Models",
    "volume": "main",
    "abstract": "Scalable Vector Graphics (SVG) has become the de facto standard for vector graphics in digital design, offering resolution independence and precise control over individual elements. Despite their advantages, creating high-quality SVG content remains challenging, as it demands technical expertise with professional editing software and a considerable time investment to craft complex shapes. Recent text-to-SVG generation methods aim to make vector graphics creation more accessible, but they still encounter limitations in shape regularity, generalization ability, and expressiveness. To address these challenges, we introduce Chat2SVG, a hybrid framework that combines the strengths of Large Language Models (LLMs) and image diffusion models for text-to-SVG generation. Our approach first uses an LLM to generate semantically meaningful SVG templates from basic geometric primitives. Guided by image diffusion models, a dual-stage optimization pipeline refines paths in latent space and adjusts point coordinates to enhance geometric complexity. Extensive experiments show that Chat2SVG outperforms existing methods in visual fidelity, path regularity, and semantic alignment. Additionally, our system enables intuitive editing through natural language instructions, making professional vector graphics creation accessible to all users. Our code is available at https://chat2svg.github.io/",
    "checked": true,
    "id": "1c774cb5348819f978c95627b3c4c583e4e6d4d2",
    "semantic_title": "chat2svg: vector graphics generation with large language models and image diffusion models",
    "citation_count": 4,
    "authors": [
      "Ronghuan Wu",
      "Wanchao Su",
      "Jing Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_GAF_Gaussian_Avatar_Reconstruction_from_Monocular_Videos_via_Multi-view_Diffusion_CVPR_2025_paper.html": {
    "title": "GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view Diffusion",
    "volume": "main",
    "abstract": "We propose a novel approach for reconstructing animatable 3D Gaussian avatars from monocular videos captured by commodity devices like smartphones. Photorealistic 3D head avatar reconstruction from such recordings is challenging due to limited observations, which leaves unobserved regions under-constrained and can lead to artifacts in novel views. To address this problem, we introduce a multi-view head diffusion model, leveraging its priors to fill in missing regions and ensure view consistency in Gaussian splatting renderings. To enable precise viewpoint control, we use normal maps rendered from FLAME-based head reconstruction, which provides pixel-aligned inductive biases. We also condition the diffusion model on VAE features extracted from the input image to preserve facial identity and appearance details. For Gaussian avatar reconstruction, we distill multi-view diffusion priors by using iteratively denoised images as pseudo-ground truths, effectively mitigating over-saturation issues. To further improve photorealism, we apply latent upsampling priors to refine the denoised latent before decoding it into an image. We evaluate our method on the NeRSemble dataset, showing that GAF outperforms previous state-of-the-art methods in novel view synthesis. Furthermore, we demonstrate higher-fidelity avatar reconstructions from monocular videos captured on commodity devices",
    "checked": true,
    "id": "2d28839ed74f2c576826b1b72c1cd5c477b3c63f",
    "semantic_title": "gaf: gaussian avatar reconstruction from monocular videos via multi-view diffusion",
    "citation_count": 5,
    "authors": [
      "Jiapeng Tang",
      "Davide Davoli",
      "Tobias Kirschstein",
      "Liam Schoneveld",
      "Matthias Nießner"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Reloc3r_Large-Scale_Training_of_Relative_Camera_Pose_Regression_for_Generalizable_CVPR_2025_paper.html": {
    "title": "Reloc3r: Large-Scale Training of Relative Camera Pose Regression for Generalizable, Fast, and Accurate Visual Localization",
    "volume": "main",
    "abstract": "Visual localization aims to determine the camera pose of a query image relative to a database of posed images. In recent years, deep neural networks that directly regress camera poses have gained popularity due to their fast inference capabilities. However, existing methods struggle to either generalize well to new scenes or provide accurate camera pose estimates. To address these issues, we present Reloc3r, a simple yet effective visual localization framework. It consists of an elegantly designed relative pose regression network, and a minimalist motion averaging module for absolute pose estimation. Trained on approximately eight million posed image pairs, Reloc3r achieves surprisingly good performance and generalization ability. We conduct extensive experiments on six public datasets, consistently demonstrating the effectiveness and efficiency of the proposed method. It provides high-quality camera pose estimates in real time and generalizes to novel scenes. Code: https://github.com/ffrivera0/reloc3r",
    "checked": true,
    "id": "1929d133e87ba19d0e071d32d4112855d82a49b3",
    "semantic_title": "reloc3r: large-scale training of relative camera pose regression for generalizable, fast, and accurate visual localization",
    "citation_count": 6,
    "authors": [
      "Siyan Dong",
      "Shuzhe Wang",
      "Shaohui Liu",
      "Lulu Cai",
      "Qingnan Fan",
      "Juho Kannala",
      "Yanchao Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_AI-Face_A_Million-Scale_Demographically_Annotated_AI-Generated_Face_Dataset_and_Fairness_CVPR_2025_paper.html": {
    "title": "AI-Face: A Million-Scale Demographically Annotated AI-Generated Face Dataset and Fairness Benchmark",
    "volume": "main",
    "abstract": "AI-generated faces have enriched human life, such as entertainment, education, and art. However, they also pose misuse risks. Therefore, detecting AI-generated faces becomes crucial, yet current detectors show biased performance across different demographic groups. Mitigating biases can be done by designing algorithmic fairness methods, which usually require demographically annotated face datasets for model training. However, no existing dataset encompasses both demographic attributes and diverse generative methods simultaneously, which hinders the development of fair detectors for AI-generated faces. In this work, we introduce the AI-Face dataset, the first million-scale demographically annotated AI-generated face image dataset, including real faces, faces from deepfake videos, and faces generated by Generative Adversarial Networks and Diffusion Models. Based on this dataset, we conduct the first comprehensive fairness benchmark to assess various AI face detectors and provide valuable insights and findings to promote the future fair design of AI face detectors. Our AI-Face dataset and benchmark code are publicly available at https://github.com/Purdue-M2/AI-Face-FairnessBench",
    "checked": true,
    "id": "76e623e687f8a2f2b52ef5e8dcdbcce120755a91",
    "semantic_title": "ai-face: a million-scale demographically annotated ai-generated face dataset and fairness benchmark",
    "citation_count": 12,
    "authors": [
      "Li Lin",
      "Santosh Santosh",
      "Mingyang Wu",
      "Xin Wang",
      "Shu Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bu_Inference-Scale_Complexity_in_ANN-SNN_Conversion_for_High-Performance_and_Low-Power_Applications_CVPR_2025_paper.html": {
    "title": "Inference-Scale Complexity in ANN-SNN Conversion for High-Performance and Low-Power Applications",
    "volume": "main",
    "abstract": "Spiking Neural Networks (SNNs) have emerged as a promising substitute for Artificial Neural Networks (ANNs) due to their advantages of fast inference and low power consumption. However, the lack of efficient training algorithms has hindered their widespread adoption. Even efficient ANN-SNN conversion methods necessitate quantized training of ANNs to enhance the effectiveness of the conversion, incurring additional training costs. To address these challenges, we propose an efficient ANN-SNN conversion framework with only inference scale complexity. The conversion framework includes a local threshold balancing algorithm, which enables efficient calculation of the optimal thresholds and fine-grained adjustment of the threshold value by channel-wise scaling. We also introduce an effective delayed evaluation strategy to mitigate the influence of the spike propagation delays. We demonstrate the scalability of our framework in typical computer vision tasks: image classification, semantic segmentation, object detection, and video classification. Our algorithm outperforms existing methods, highlighting its practical applicability and efficiency. Moreover, we have evaluated the energy consumption of the converted SNNs, demonstrating their superior low-power advantage compared to conventional ANNs. This approach simplifies the deployment of SNNs by leveraging open-source pre-trained ANN models, enabling fast, low-power inference with negligible performance reduction",
    "checked": true,
    "id": "5f61edb98e2dda8e611b7ec15265df4c4103bd32",
    "semantic_title": "inference-scale complexity in ann-snn conversion for high-performance and low-power applications",
    "citation_count": 0,
    "authors": [
      "Tong Bu",
      "Maohua Li",
      "Zhaofei Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Janus_Decoupling_Visual_Encoding_for_Unified_Multimodal_Understanding_and_Generation_CVPR_2025_paper.html": {
    "title": "Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation",
    "volume": "main",
    "abstract": "We introduce Janus, an autoregressive framework that unifies multimodal understanding and generation. Prior research often relies on a single visual encoder for both tasks, such as Chameleon. However, due to the differing levels of information granularity required by multimodal understanding and generation, this approach can lead to suboptimal performance, particularly in multimodal understanding. To address this issue, we decouple visual encoding into separate pathways, while still leveraging a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder's roles in understanding and generation, but also enhances the framework's flexibility. For instance, both the multimodal understanding and generation components can independently select their most suitable encoding methods. Experiments show that Janus surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models",
    "checked": true,
    "id": "b438a496045feccd1c028f1ce034062636a1ee04",
    "semantic_title": "janus: decoupling visual encoding for unified multimodal understanding and generation",
    "citation_count": 165,
    "authors": [
      "Chengyue Wu",
      "Xiaokang Chen",
      "Zhiyu Wu",
      "Yiyang Ma",
      "Xingchao Liu",
      "Zizheng Pan",
      "Wen Liu",
      "Zhenda Xie",
      "Xingkai Yu",
      "Chong Ruan",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Choi_MVDoppler-Pose_Multi-Modal_Multi-View_mmWave_Sensing_for_Long-Distance_Self-Occluded_Human_Walking_CVPR_2025_paper.html": {
    "title": "MVDoppler-Pose: Multi-Modal Multi-View mmWave Sensing for Long-Distance Self-Occluded Human Walking Pose Estimation",
    "volume": "main",
    "abstract": "One of the main challenges in reliable camera-based 3D pose estimation for walking subjects is to deal with self-occlusions, especially in the case of using low-resolution cameras or at longer distance scenarios. In recent years, millimeter-wave (mmWave) radar has emerged as a promising alternative, offering inherent resilience to the effect of occlusions and distance variations. However, mmWave-based human walking pose estimation (HWPE) is still in the nascent development stages, primarily due to its unique set of practical challenges including the quality of the observed radar signal dependent on the subject's motion direction. This paper introduces the first comprehensive study comparing mmWave radar to camera systems for HWPE, highlighting its utility for distance-agnostic and occlusion-resilient pose estimation. Building upon mmWave's unique advantages, we address its intrinsic directionality issue through a new approach--the synergetic integration of multi-modal, multi-view mmWave signals, achieving robust HWPE against variations both in distance and walking direction. Extensive experiments on a newly curated dataset not only demonstrate the superior potential of mmWave technology over traditional camera-based HWPE systems, but also validate the effectiveness of our approach in overcoming the core limitations of mmWave HWPE",
    "checked": true,
    "id": "1e5262cb19a21b4568f534d345c08b094589e857",
    "semantic_title": "mvdoppler-pose: multi-modal multi-view mmwave sensing for long-distance self-occluded human walking pose estimation",
    "citation_count": 0,
    "authors": [
      "Jaeho Choi",
      "Soheil Hor",
      "Shubo Yang",
      "Amin Arbabian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_TopNet_Transformer-Efficient_Occupancy_Prediction_Network_for_Octree-Structured_Point_Cloud_Geometry_CVPR_2025_paper.html": {
    "title": "TopNet: Transformer-Efficient Occupancy Prediction Network for Octree-Structured Point Cloud Geometry Compression",
    "volume": "main",
    "abstract": "Efficient Point Cloud Geometry Compression (PCGC) with a lower bits per point (BPP) and higher peak signal-to-noise ratio (PSNR) is essential for the transportation of large-scale 3D data. Although octree-based entropy models can reduce BPP without introducing geometry distortion, existing CNN-based models struggle with limited receptive fields to capture long-range dependencies, while Transformer-built architectures always neglect fine-grained details due to their reliance on global self-attention. In this paper, we propose a Transformer-efficient occupancy prediction Network, termed TopNet, to overcome these challenges by developing several novel components: Locally-enhanced Context Encoding (LeCE) for enhancing the translation-invariance of the octree nodes, Adaptive-Length Sliding Window Attention (AL-SWA) for capturing both global and local dependencies while adaptively adjusting attention weights based on the input window length, Spatial-Gated-enhanced Channel Mixer (SG-CM) for efficient feature aggregation from ancestors and siblings, and Latent-guided Node Occupancy Predictor (LNOP) for improving prediction accuracy of spatially adjacent octree nodes. Comprehensive experiments across both indoor and outdoor point cloud datasets demonstrate that our TopNet achieves state-of-the-art performance with fewer parameters, further advancing the reduction-efficiency boundaries of PCGC. The code is available at https://github.com/xinjiewang1995/TopNet",
    "checked": true,
    "id": "d8ed02d4c94336a466b0d2ec6f52d2f4c6360ebc",
    "semantic_title": "topnet: transformer-efficient occupancy prediction network for octree-structured point cloud geometry compression",
    "citation_count": 0,
    "authors": [
      "Xinjie Wang",
      "Yifan Zhang",
      "Ting Liu",
      "Xinpu Liu",
      "Ke Xu",
      "Jianwei Wan",
      "Yulan Guo",
      "Hanyun Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Song_MagicArticulate_Make_Your_3D_Models_Articulation-Ready_CVPR_2025_paper.html": {
    "title": "MagicArticulate: Make Your 3D Models Articulation-Ready",
    "volume": "main",
    "abstract": "With the explosive growth of 3D content creation, there is an increasing demand for automatically converting static 3D models into articulation-ready versions that support realistic animation. Traditional approaches rely heavily on manual annotation, which is both time-consuming and labor-intensive. Moreover, the lack of large-scale benchmarks has hindered the development of learning-based solutions. In this work, we present MagicArticulate, an effective framework that automatically transforms static 3D models into articulation-ready assets. Our key contributions are threefold. First, we introduce Articulation-XL, a large-scale benchmark containing over 33k 3D models with high-quality articulation annotations, carefully curated from Objaverse-XL. Second, we propose a novel skeleton generation method that formulates the task as a sequence modeling problem, leveraging an auto-regressive transformer to naturally handle varying numbers of bones or joints within skeletons and their inherent dependencies across different 3D models. Third, we predict skinning weights using a functional diffusion process that incorporates volumetric geodesic distance priors between vertices and joints. Extensive experiments demonstrate that MagicArticulate significantly outperforms existing methods across diverse object categories, achieving high-quality articulation that enables realistic animation. Project page: https://chaoyuesong.github.io/MagicArticulate",
    "checked": true,
    "id": "a46fcfaec5197c9941c0e99805742fcd2407ef97",
    "semantic_title": "magicarticulate: make your 3d models articulation-ready",
    "citation_count": 2,
    "authors": [
      "Chaoyue Song",
      "Jianfeng Zhang",
      "Xiu Li",
      "Fan Yang",
      "Yiwen Chen",
      "Zhongcong Xu",
      "Jun Hao Liew",
      "Xiaoyang Guo",
      "Fayao Liu",
      "Jiashi Feng",
      "Guosheng Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Gain_from_Neighbors_Boosting_Model_Robustness_in_the_Wild_via_CVPR_2025_paper.html": {
    "title": "Gain from Neighbors: Boosting Model Robustness in the Wild via Adversarial Perturbations Toward Neighboring Classes",
    "volume": "main",
    "abstract": "Recent approaches, such as data augmentation, adversarial training, and transfer learning, have shown potential in addressing the issue of performance degradation caused by distributional shifts. However, they typically demand careful design in terms of data or models and lack awareness of the impact of distributional shifts. In this paper, we observe that classification errors arising from distribution shifts tend to cluster near the true values, suggesting that misclassifications commonly occur in semantically similar, neighboring categories. Furthermore, robust advanced vision foundation models maintain larger inter-class distances while preserving semantic consistency, making them less vulnerable to such shifts. Building on these findings, we propose a new method called GFN (Gain From Neighbors), which uses gradient priors from neighboring classes to perturb input images and incorporates an inter-class distance-weighted loss to improve class separation. This approach encourages the model to learn more resilient features from data prone to errors, enhancing its robustness against shifts in diverse settings. In extensive experiments across various model architectures and benchmark datasets, GFN consistently demonstrated superior performance. For instance, compared to the current state-of-the-art TAPADL method, our approach achieved a higher corruption robustness of 41.4% on ImageNet-C (+2.3%), without requiring additional parameters and using only minimal data",
    "checked": true,
    "id": "40a5ece9835956a16e662fa6f8d148e050929e17",
    "semantic_title": "gain from neighbors: boosting model robustness in the wild via adversarial perturbations toward neighboring classes",
    "citation_count": 0,
    "authors": [
      "Zhou Yang",
      "Mingtao Feng",
      "Tao Huang",
      "Fangfang Wu",
      "Weisheng Dong",
      "Xin Li",
      "Guangming Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_Enhancing_Video-LLM_Reasoning_via_Agent-of-Thoughts_Distillation_CVPR_2025_paper.html": {
    "title": "Enhancing Video-LLM Reasoning via Agent-of-Thoughts Distillation",
    "volume": "main",
    "abstract": "This paper tackles the problem of video question answering (VideoQA), a task that often requires multi-step reasoning and a profound understanding of spatial-temporal dynamics. While large video-language models perform well on benchmarks, they often lack explainability and spatial-temporal grounding. In this paper, we propose **A**gent-**o**f-**T**houghts **D**istillation (**AoTD**), a method that enhances models by incorporating automatically generated Chain-of-Thoughts (CoTs) into the instruction-tuning process. Specifically, we leverage an agent-based system to decompose complex questions into sub-tasks, and address them with specialized vision models, the intermediate results are then treated as reasoning chains. We also introduce a verification mechanism using a large language model (LLM) to ensure the reliability of generated CoTs. Extensive experiments demonstrate that AoTD improves the performance on multiple-choice and open-ended benchmarks",
    "checked": true,
    "id": "6be9716bb42f7eb61e10db3fce3da58ea042b263",
    "semantic_title": "enhancing video-llm reasoning via agent-of-thoughts distillation",
    "citation_count": 1,
    "authors": [
      "Yudi Shi",
      "Shangzhe Di",
      "Qirui Chen",
      "Weidi Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_De2Gaze_Deformable_and_Decoupled_Representation_Learning_for_3D_Gaze_Estimation_CVPR_2025_paper.html": {
    "title": "De^2Gaze: Deformable and Decoupled Representation Learning for 3D Gaze Estimation",
    "volume": "main",
    "abstract": "3D Gaze estimation is a challenging task due to two main issues. First, existing methods focus on analyzing dense features (e.g., large pixel regions), which are sensitive to local noise (e.g., light spots, blurs) and result in increased computational complexity. Second, an eyeball model can correspond multiple gaze directions, and the entangled representation between gazes and models increases the learning difficulty. To address these issues, we propose De\\textsuperscript 2Gaze , a lightweight and accurate model-aware 3D gaze estimation method. In De\\textsuperscript 2 Gaze, we introduce two key innovations for deformable and decoupled representation learning. Specifically, first, we propose a deformable sparse attention mechanism that can adapt sparse sampling points to attention areas to avoid local noise influences. Second, we propose a spatial decoupling network with a dual-branch decoding architecture to disentangle invariant (e.g., eyeball radius, position) and variable (e.g., gaze, pupil, iris) features from the latent space. Compared to existing methods, De\\textsuperscript 2 Gaze requires fewer sparse features, and achieves faster convergence speed, lower computational complexity, and higher accuracy in 3D gaze estimation.Qualitative and quantitative experiments demonstrate that De\\textsuperscript 2 Gaze achieves state-of-the-art accuracy and high-quality semantic segmentation for 3D gaze estimation on the TEyeD dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunfeng Xiao",
      "Xiaowei Bai",
      "Baojun Chen",
      "Hao Su",
      "Hao He",
      "Liang Xie",
      "Erwei Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_ReCapture_Generative_Video_Camera_Controls_for_User-Provided_Videos_using_Masked_CVPR_2025_paper.html": {
    "title": "ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning",
    "volume": "main",
    "abstract": "Recently, breakthroughs in video modeling have allowed for controllable camera trajectories in generated videos. However, these methods cannot be directly applied to user-provided videos that are not generated by a video model. In this paper, we present ReCapture, a method for generating new videos with novel camera trajectories from a single user-provided video. Our method allows us to re-generate the reference video, with all its existing scene motion, from vastly different angles and with cinematic camera motion. Notably, using our method we can also plausibly hallucinate parts of the scene that were not observable in the reference video. Our method works by (1) generating a noisy anchor video with a new camera trajectory using multiview diffusion models or depth-based point cloud rendering and then (2) regenerating the anchor video into a clean and temporally consistent reangled video using our proposed masked video fine-tuning technique",
    "checked": true,
    "id": "b742dd8f4000acba68e9628affcd2426d4ec94a3",
    "semantic_title": "recapture: generative video camera controls for user-provided videos using masked video fine-tuning",
    "citation_count": 21,
    "authors": [
      "David Junhao Zhang",
      "Roni Paiss",
      "Shiran Zada",
      "Nikhil Karnad",
      "David E. Jacobs",
      "Yael Pritch",
      "Inbar Mosseri",
      "Mike Zheng Shou",
      "Neal Wadhwa",
      "Nataniel Ruiz"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_M3-VOS_Multi-Phase_Multi-Transition_and_Multi-Scenery_Video_Object_Segmentation_CVPR_2025_paper.html": {
    "title": "M^3-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object Segmentation",
    "volume": "main",
    "abstract": "Intelligent robots need to interact with diverse objects across various environments. The appearance and state of objects frequently undergo complex transformations depending on the object properties, e.g., phase transitions. However, in the vision community, segmenting dynamic objects with phase transitions is overlooked. In light of this, we introduce the concept of phase in segmentation, which categorizes real-world objects based on their visual characteristics and potential morphological and appearance changes. Then, we present a new benchmark, Multi-Phase, Multi-Transition, and Multi-Scenery Video Object Segmentation (M^3-VOS), to verify the ability of models to understand object phases, which consists of 471 high-resolution videos spanning over 10 distinct everyday scenarios. It provides dense instance mask annotations that capture both object phases and their transitions. We evaluate state-of-the-art methods on M^3-VOS, yielding several key insights. Notably, current appearance-based approaches show significant room for improvement when handling objects with phase transitions. The inherent changes in disorder suggest that the predictive performance of the forward entropy-increasing process can be improved through a reverse entropy-reducing process. These findings lead us to propose ReVOS, a new plug-and-play model that improves its performance by reversal refinement. Our data and code will be publicly available at https://zixuan-chen.github.io/M-cube-VOS.github.io/",
    "checked": false,
    "id": "9f3829a7f1bc5df28595e062cbf44c2e3ed6dba2",
    "semantic_title": "m3-vos: multi-phase, multi-transition, and multi-scenery video object segmentation",
    "citation_count": 0,
    "authors": [
      "Zixuan Chen",
      "Jiaxin Li",
      "Junxuan Liang",
      "Liming Tan",
      "Yejie Guo",
      "Cewu Lu",
      "Yong-Lu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Self-Expansion_of_Pre-trained_Models_with_Mixture_of_Adapters_for_Continual_CVPR_2025_paper.html": {
    "title": "Self-Expansion of Pre-trained Models with Mixture of Adapters for Continual Learning",
    "volume": "main",
    "abstract": "Continual learning (CL) aims to continually accumulate knowledge from a non-stationary data stream without catastrophic forgetting of learned knowledge, requiring a balance between stability and adaptability. Relying on the generalizable representation in pre-trained models (PTMs), PTM-based CL methods perform effective continual adaptation on downstream tasks by adding learnable adapters or prompts upon the frozen PTMs. However, many existing PTM-based CL methods use restricted adaptation on a fixed set of these modules to avoid forgetting, suffering from limited CL ability. Periodically adding task-specific modules results in linear model growth rate and impaired knowledge reuse. We propose Self-Expansion of pre-trained models with Modularized Adaptation (SEMA), a novel approach to enhance the control of stability-plasticity balance in PTM-based CL. SEMA automatically decides to reuse or add adapter modules on demand in CL, depending on whether significant distribution shift that cannot be handled is detected at different representation levels. We design modular adapter consisting of a functional adapter and a representation descriptor. The representation descriptors are trained as a distribution shift indicator and used to trigger self-expansion signals. For better composing the adapters, an expandable weighting router is learned jointly for mixture of adapter outputs. SEMA enables better knowledge reuse and sub-linear expansion rate. Extensive experiments demonstrate the effectiveness of the proposed self-expansion method, achieving state-of-the-art performance compared to PTM-based CL methods without memory rehearsal. Code is available at https://github.com/huiyiwang01/SEMA-CL",
    "checked": true,
    "id": "5fc2916a10545c79cf3fc25c17af6f8c4563639f",
    "semantic_title": "self-expansion of pre-trained models with mixture of adapters for continual learning",
    "citation_count": 11,
    "authors": [
      "Huiyi Wang",
      "Haodong Lu",
      "Lina Yao",
      "Dong Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kong_Dual_Prompting_Image_Restoration_with_Diffusion_Transformers_CVPR_2025_paper.html": {
    "title": "Dual Prompting Image Restoration with Diffusion Transformers",
    "volume": "main",
    "abstract": "Recent state-of-the-art image restoration methods mostly adopt latent diffusion models with U-Net backbones, yet still facing challenges in achieving high-quality restoration due to their limited capabilities. Diffusion transformers (DiTs), like SD3, are emerging as a promising alternative because of their better quality with scalability. However, previous conditional control methods for U-Net-based diffusion models, such as ControlNet, are not well-suited for DiTs. In this paper, we introduce DPIR (Dual Recent state-of-the-art image restoration methods mostly adopt latent diffusion models with U-Net backbones, yet still facing challenges in achieving high-quality restoration due to their limited capabilities. Diffusion transformers (DiTs), like SD3, are emerging as a promising alternative because of their better quality with scalability. In this paper, we introduce DPIR (Dual Prompting Image Restoration), a novel image restoration method that effectivly extracts conditional information of low-quality images from multiple perspectives. Specifically, DPIR consits of two branches: a low-quality image conditioning branch and a dual prompting control branch. The first branch utilizes a lightweight module to incorporate image priors into the DiT with high efficiency. More importantly, we believe that in image restoration, textual description alone cannot fully capture its rich visual characteristics. Therefore, a dual prompting module is designed to provide DiT with additional visual cues, capturing both global context and local appearance. The extracted global-local visual prompts as extra conditional control, alongside textual prompts to form dual prompts, greatly enhance the quality of the restoration. Extensive experimental results demonstrate that DPIR delivers superior image restoration performance",
    "checked": true,
    "id": "7d037f6bdb1204e2664683529ded24c5938a59aa",
    "semantic_title": "dual prompting image restoration with diffusion transformers",
    "citation_count": 1,
    "authors": [
      "Dehong Kong",
      "Fan Li",
      "Zhixin Wang",
      "Jiaqi Xu",
      "Renjing Pei",
      "Wenbo Li",
      "WenQi Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Brain-Inspired_Spiking_Neural_Networks_for_Energy-Efficient_Object_Detection_CVPR_2025_paper.html": {
    "title": "Brain-Inspired Spiking Neural Networks for Energy-Efficient Object Detection",
    "volume": "main",
    "abstract": "Brain-inspired spiking neural networks (SNNs) have the capability of energy-efficient processing of temporal information. However, leveraging the rich dynamic characteristics of SNNs and prior works in artificial neural networks (ANNs) to construct an effective object detection model for visual tasks remains an open question for further exploration. To develop a directly-trained , low energy consumption and high-performance multi-scale SNN model, we propose a novel interpretable object detection framework Multi-scale Spiking Detector (MSD). Initially, we propose a spiking convolutional neuron as a core component of the Optic Nerve Nucleus Block (ONNB), designed to significantly enhance the deep feature extraction capabilities of SNNs. ONNB enables direct training with improved energy efficiency, demonstrating superior performance compared to state-of-the-art ANN-to-SNN conversion and SNN techniques. In addition, we propose a Multi-scale Spiking Detection Framework to emulate the biological response and comprehension of stimuli from different objects. Wherein, spiking multi-scale fusion and the spiking detector are employed to integrate features across different depths and to detect response outcomes, respectively. Our method outperforms state-of-the-art ANN detectors, with only 7.8 M parameters and 6.43 mJ energy consumption. MSD obtains the mean average precision (mAP) of 62.0% and 66.3% on COCO and Gen1 datasets, respectively",
    "checked": false,
    "id": "dfeb8944b143b5d2e99be2efefde8045c51ef3a9",
    "semantic_title": "an fpga accelerator design of spiking neural network for energy-efficient object detection",
    "citation_count": 2,
    "authors": [
      "Ziqi Li",
      "Tao Gao",
      "Yisheng An",
      "Ting Chen",
      "Jing Zhang",
      "Yuanbo Wen",
      "Mengkun Liu",
      "Qianxi Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Medusa_A_Multi-Scale_High-order_Contrastive_Dual-Diffusion_Approach_for_Multi-View_Clustering_CVPR_2025_paper.html": {
    "title": "Medusa: A Multi-Scale High-order Contrastive Dual-Diffusion Approach for Multi-View Clustering",
    "volume": "main",
    "abstract": "Deep multi-view clustering methods utilize information from multiple views to achieve enhanced clustering results and have gained increasing popularity in recent years. Most existing methods typically focus on either inter-view or intra-view relationships, aiming to align information across views or analyze structural patterns within individual views. However, they often incorporate inter-view complementary information in a simplistic manner, while overlooking the complex, high-order relationships within multi-view data and the interactions among samples, resulting in an incomplete utilization of the rich information available. Instead, we propose a multi-scale approach that exploits all of the available information. We first introduce a dual graph diffusion module guided by a consensus graph. This module leverages inter-view information to enhance the representation of both nodes and edges within each view. Secondly, we propose a novel contrastive loss function based on hypergraphs to more effectively model and leverage complex intra-view data relationships. Finally, we propose to adaptively learn fusion weights at the sample level, which enables a more flexible and dynamic aggregation of multi-view information. Extensive experiments on eight datasets show favorable performance of the proposed method compared to state-of-the-art approaches, demonstrating its effectiveness across diverse scenarios",
    "checked": true,
    "id": "d0a24bf4a1a94823f976284c1fd2adeabec52b81",
    "semantic_title": "medusa: a multi-scale high-order contrastive dual-diffusion approach for multi-view clustering",
    "citation_count": 0,
    "authors": [
      "Liang Chen",
      "Zhe Xue",
      "Yawen Li",
      "Meiyu Liang",
      "Yan Wang",
      "Anton van den Hengel",
      "Yuankai Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_MambaOut_Do_We_Really_Need_Mamba_for_Vision_CVPR_2025_paper.html": {
    "title": "MambaOut: Do We Really Need Mamba for Vision?",
    "volume": "main",
    "abstract": "Mamba, an architecture with RNN-like token mixer of state space model (SSM), was recently introduced to address the quadratic complexity of the attention mechanism and subsequently applied to vision tasks. Nevertheless, the performance of Mamba for vision is often underwhelming when compared with convolutional and attention-based models. In this paper, we delve into the essence of Mamba, and conceptually conclude that Mamba is ideally suited for tasks with long-sequence and autoregressive characteristics. For vision tasks, as image classification on ImageNet does not align with either characteristic, we hypothesize that Mamba is not necessary for this task; Detection and segmentation tasks on COCO or ADE20K are also not autoregressive, yet they adhere to the long-sequence characteristic, so we believe it is still worthwhile to explore Mamba's potential for these tasks. To empirically verify our hypotheses, we construct a series of models named MambaOut through stacking Mamba blocks while removing their core token mixer, SSM. Experimental results strongly support our hypotheses. Specifically, our MambaOut model surpasses all visual Mamba models on ImageNet image classification, indicating that Mamba is indeed unnecessary for this task. As for detection and segmentation, MambaOut cannot match the performance of state-of-the-art visual Mamba models, demonstrating the potential of Mamba for long-sequence visual tasks",
    "checked": true,
    "id": "31fdba3a68f286894f025e734a277e2ce94dd84c",
    "semantic_title": "mambaout: do we really need mamba for vision?",
    "citation_count": 57,
    "authors": [
      "Weihao Yu",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Everything_to_the_Synthetic_Diffusion-driven_Test-time_Adaptation_via_Synthetic-Domain_Alignment_CVPR_2025_paper.html": {
    "title": "Everything to the Synthetic: Diffusion-driven Test-time Adaptation via Synthetic-Domain Alignment",
    "volume": "main",
    "abstract": "Test-time adaptation (TTA) aims to improve the performance of source-domain pre-trained models on previously unseen, shifted target domains. Traditional TTA methods primarily adapt model weights based on target data streams, making model performance sensitive to the amount and order of target data. The recently proposed diffusion-driven TTA methods mitigate this by adapting model inputs instead of weights, where an unconditional diffusion model, trained on the source domain, transforms target-domain data into a synthetic domain that is expected to approximate the source domain. However, in this paper, we reveal that although the synthetic data in diffusion-driven TTA seems indistinguishable from the source data, it is unaligned with, or even markedly different from the latter for deep networks. To address this issue, we propose a Synthetic-Domain Alignment (SDA) framework. Our key insight is to fine-tune the source model with synthetic data to ensure better alignment. Specifically, we first employ a conditional diffusion model to generate labeled samples, creating a synthetic dataset. Subsequently, we use the aforementioned unconditional diffusion model to add noise to and denoise each sample before fine-tuning. This Mix of Diffusion (MoD) process mitigates the potential domain misalignment between the conditional and unconditional models. Extensive experiments across classifiers, segmenters, and multimodal large language models (MLLMs, e.g., LLaVA) demonstrate that SDA achieves superior domain alignment and consistently outperforms existing diffusion-driven TTA methods. Our code is available at https://github.com/SHI-Labs/Diffusion-Driven-Test-Time-Adaptation-via-Synthetic-Domain-Alignment",
    "checked": true,
    "id": "0bcda9580066202cba741f0e07292ad1ea0fd7f0",
    "semantic_title": "everything to the synthetic: diffusion-driven test-time adaptation via synthetic-domain alignment",
    "citation_count": 6,
    "authors": [
      "Jiayi Guo",
      "Junhao Zhao",
      "Chaoqun Du",
      "Yulin Wang",
      "Chunjiang Ge",
      "Zanlin Ni",
      "Shiji Song",
      "Humphrey Shi",
      "Gao Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_Multi-Granularity_Class_Prototype_Topology_Distillation_for_Class-Incremental_Source-Free_Unsupervised_Domain_CVPR_2025_paper.html": {
    "title": "Multi-Granularity Class Prototype Topology Distillation for Class-Incremental Source-Free Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "This paper explores the Class-Incremental Source-Free Unsupervised Domain Adaptation (CI-SFUDA) problem, where the unlabeled target data come incrementally without access to labeled source instances. This problem poses two challenges, the interference of similar source-class knowledge in target-class representation learning and the shocks of new target knowledge to old ones. To address them, we propose the Multi-Granularity Class Prototype Topology Distillation (GROTO) algorithm, which effectively transfers the source knowledge to the class-incremental target domain. Concretely, we design the multi-granularity class prototype self-organization module and the prototype topology distillation module. First, we mine the positive classes by modeling accumulation distributions. Next, we introduce multi-granularity class prototypes to generate reliable pseudo-labels, and exploit them to promote the positive-class target feature self-organization. Second, the positive-class prototypes are leveraged to construct the topological structures of source and target feature spaces. Then, we perform the topology distillation to continually mitigate the shocks of new target knowledge to old ones. Extensive experiments demonstrate that our proposed method achieves state-of-the-art performance on three public datasets",
    "checked": true,
    "id": "45cea02f8f3285179041b8dac4aece126bf6f929",
    "semantic_title": "multi-granularity class prototype topology distillation for class-incremental source-free unsupervised domain adaptation",
    "citation_count": 1,
    "authors": [
      "Peihua Deng",
      "Jiehua Zhang",
      "Xichun Sheng",
      "Chenggang Yan",
      "Yaoqi Sun",
      "Ying Fu",
      "Liang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Danier_DepthCues_Evaluating_Monocular_Depth_Perception_in_Large_Vision_Models_CVPR_2025_paper.html": {
    "title": "DepthCues: Evaluating Monocular Depth Perception in Large Vision Models",
    "volume": "main",
    "abstract": "Large-scale pre-trained vision models are becoming increasingly prevalent, offering expressive and generalizable visual representations that benefit various downstream tasks. Recent studies on the emergent properties of these models have revealed their high-level geometric understanding, in particular in the context of depth perception. However, it remains unclear how depth perception arises in these models without explicit depth supervision provided during pre-training. To investigate this, we examine whether the monocular depth cues, similar to those used by the human visual system, emerge in these models. We introduce a new benchmark, DepthCues, designed to evaluate depth cue understanding, and present findings across 20 diverse and representative pre-trained vision models. Our analysis shows that human-like depth cues emerge in more recent larger models. We also explore enhancing depth perception in large vision models by fine-tuning on DepthCues, and find that even without dense depth supervision, this improves depth estimation. To support further research, our benchmark and evaluation code will be made publicly available for studying depth perception in vision models",
    "checked": true,
    "id": "29941d3d3b491d2a3bbba45014f5a8344707076f",
    "semantic_title": "depthcues: evaluating monocular depth perception in large vision models",
    "citation_count": 5,
    "authors": [
      "Duolikun Danier",
      "Mehmet Aygün",
      "Changjian Li",
      "Hakan Bilen",
      "Oisin Mac Aodha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition_CVPR_2025_paper.html": {
    "title": "A Polarization-Aided Transformer for Image Deblurring via Motion Vector Decomposition",
    "volume": "main",
    "abstract": "Effectively leveraging motion information is crucial for the image deblurring task. Existing methods typically build deep-learning models to restore a clean image by estimating blur patterns over the entire movement. This suggests that the blur caused by rotational motion components is processed together with the translational one. Exploring the movement without separation leads to limited performance for complex motion deblurring, especially rotational motion. In this paper, we propose Motion Decomposition Transformer (MDT), a transformer-based architecture augmented with polarized modules for deblurring via motion vector decomposition. MDT consists of a Motion Decomposition Module (MDM) for extracting hybrid rotation and translation features, and a Radial Stripe Attention Solver (RSAS) for sharp image reconstruction with enhanced rotational information. Specifically, the MDM uses a deformable Cartesian convolutional branch to capture translational motion, complemented by a polar-system branch to capture rotational motion. The RSAS employs radial stripe windows and angular relative positional encoding in the polar system to enhance rotational information. This design preserves translational details while keeping computational costs lower than dual-coordinate design. Experimental results on 6 image deblurring datasets show that MDT outperforms state-of-the-art methods, particularly in handling blur caused by complex motions with significant rotational components",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duosheng Chen",
      "Shihao Zhou",
      "Jinshan Pan",
      "Jinglei Shi",
      "Lishen Qu",
      "Jufeng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by_CVPR_2025_paper.html": {
    "title": "SpecTRe-GS: Modeling Highly Specular Surfaces with Reflected Nearby Objects by Tracing Rays in 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS), a recently emerged multi-view 3D reconstruction technique, has shown significant advantages in real-time rendering and explicit editing. However, 3DGS encounters challenges in the accurate modeling of both high-frequency view-dependent appearances and global illumination effects, including inter-reflection. This paper introduces SpecTRe-GS, which addresses these challenges and models highly Specular surfaces that reflect nearby objects through Tracing Rays in 3D Gaussian Splatting. SpecTRe-GS separately models reflections from highly specular and rough surfaces to leverage the distinctions between their reflective properties and integrates an efficient ray tracer within the 3DGS framework for querying secondary rays, thus achieving fast and accurate rendering. Also, it incorporates normal prior guidance and joint geometry optimization at various stages of the training process to enhance geometry reconstruction for undistorted reflections. Experiments on both synthetic and real-world scenes demonstrate the superiority of SpecTRe-GS compared to existing 3DGS-based methods in capturing highly specular inter-reflections and also showcase its editing applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajun Tang",
      "Fan Fei",
      "Zhihao Li",
      "Xiao Tang",
      "Shiyong Liu",
      "Youyu Chen",
      "Binxiao Huang",
      "Zhenyu Chen",
      "Xiaofei Wu",
      "Boxin Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cho_Seurat_From_Moving_Points_to_Depth_CVPR_2025_paper.html": {
    "title": "Seurat: From Moving Points to Depth",
    "volume": "main",
    "abstract": "Accurate depth estimation from monocular videos remains challenging due to ambiguities inherent in single-view geometry, as crucial depth cues like stereopsis are absent. However, humans often perceive relative depth intuitively by observing variations in the size and spacing of objects as they move. Inspired by this, we propose a novel method that infers relative depth by examining the spatial relationships and temporal evolution of a set of tracked 2D trajectories. Specifically, we use off-the-shelf point tracking models to capture 2D trajectories. Then, our approach employs spatial and temporal transformers to process these trajectories and directly infer depth changes over time. Evaluated on the TAPVid-3D benchmark, our method demonstrates robust zero-shot performance, generalizing effectively from synthetic to real-world datasets. Results indicate that our approach achieves temporally smooth, high-accuracy depth predictions across diverse domains",
    "checked": true,
    "id": "ec8fd9a83d6777d95c111a57e3c6118a0c0cb87f",
    "semantic_title": "seurat: from moving points to depth",
    "citation_count": 0,
    "authors": [
      "Seokju Cho",
      "Jiahui Huang",
      "Seungryong Kim",
      "Joon-Young Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_AuraFusion360_Augmented_Unseen_Region_Alignment_for_Reference-based_360deg_Unbounded_Scene_CVPR_2025_paper.html": {
    "title": "AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360deg Unbounded Scene Inpainting",
    "volume": "main",
    "abstract": "Three-dimensional scene inpainting is crucial for applications from virtual reality to architectural visualization, yet existing methods struggle with view consistency and geometric accuracy in 360deg unbounded scenes. We present AuraFusion360, a novel reference-based method that enables high-quality object removal and hole filling in 3D scenes represented by Gaussian Splatting. Our approach introduces (1) depth-aware unseen mask generation for accurate occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot method for accurate initial point placement without requiring additional training, and (3) SDEdit-based detail enhancement for multi-view coherence. We also introduce 360-USID, the first comprehensive dataset for 360deg unbounded scene inpainting with ground truth. Extensive experiments demonstrate that AuraFusion360 significantly outperforms existing methods, achieving superior perceptual quality while maintaining geometric accuracy across dramatic viewpoint changes",
    "checked": false,
    "id": "efe132be55a59dce48eb5b22d8e2b3a071cd0da3",
    "semantic_title": "aurafusion360: augmented unseen region alignment for reference-based 360° unbounded scene inpainting",
    "citation_count": 2,
    "authors": [
      "Chung-Ho Wu",
      "Yang-Jung Chen",
      "Ying-Huan Chen",
      "Jie-Ying Lee",
      "Bo-Hsu Ke",
      "Chun-Wei Tuan Mu",
      "Yi-Chuan Huang",
      "Chin-Yang Lin",
      "Min-Hung Chen",
      "Yen-Yu Lin",
      "Yu-Lun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zha_Language-Guided_Image_Tokenization_for_Generation_CVPR_2025_paper.html": {
    "title": "Language-Guided Image Tokenization for Generation",
    "volume": "main",
    "abstract": "Image tokenization, the process of transforming raw image pixels into a compact low-dimensional latent representation, has proven crucial for scalable and efficient image generation. However, mainstream image tokenization methods generally have limited compression rates, making high-resolution image generation computationally expensive. To address this challenge, we propose to leverage language for efficient image tokenization, and we call our method Text-Conditioned Image Tokenization (TexTok). TexTok is a simple yet effective tokenization framework that leverages language to provide a compact, high-level semantic representation. By conditioning the tokenization process on descriptive text captions, TexTok simplifies semantic learning, allowing more learning capacity and token space to be allocated to capture fine-grained visual details, leading to enhanced reconstruction quality and higher compression rates. Compared to the conventional tokenizer without text conditioning, TexTok achieves average reconstruction FID improvements of 29.2% and 48.1% on ImageNet-256 and -512 benchmarks respectively, across varying numbers of tokens. These tokenization improvements consistently translate to 16.3% and 34.3% average improvements in generation FID. By simply replacing the tokenizer in Diffusion Transformer (DiT) with TexTok, our system can achieve a 93.5x inference speedup while still outperforming the original DiT using only 32 tokens on ImageNet-512. TexTok with a vanilla DiT generator achieves state-of-the-art FID scores of 1.46 and 1.62 on ImageNet-256 and -512 respectively. Furthermore, we demonstrate TexTok's superiority on the text-to-image generation task, effectively utilizing the off-the-shelf text captions in tokenization",
    "checked": true,
    "id": "1407d10be41d6d1de444173ec88fd622a5f31566",
    "semantic_title": "language-guided image tokenization for generation",
    "citation_count": 8,
    "authors": [
      "Kaiwen Zha",
      "Lijun Yu",
      "Alireza Fathi",
      "David A. Ross",
      "Cordelia Schmid",
      "Dina Katabi",
      "Xiuye Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiao_Img-Diff_Contrastive_Data_Synthesis_for_Multimodal_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models",
    "volume": "main",
    "abstract": "High-performance Multimodal Large Language Models (MLLMs) rely heavily on data quality. This study introduces a novel data synthesis method, leveraging insights from contrastive learning and image difference captioning to enhance fine-grained image recognition in MLLMs. By analyzing object differences in detailed regions between similar images, we challenge the model to identify both matching and distinct components. Specifically, our method initially create pairs of similar images that highlight object variations. After that, we introduce a Difference Area Generator for object differences identifying, followed by a Difference Captions Generator for differences describing. The outcome is a high-quality dataset of \"object replacement\" samples, named Img-Diff, which can be expanded as needed due to its automation. We use the generated dataset to finetune state-of-the-art (SOTA) MLLMs such as InternVL2, yielding comprehensive improvements across numerous image difference and Visual Question Answering tasks. For instance, the trained models notably surpass the SOTA models GPT-4V and Gemini on the MMVP benchmark. Additionally, we conduct thorough evaluations to confirm the dataset's diversity, quality, and robustness, presenting several insights on the synthesis of such a contrastive dataset. We release our codes and dataset to encourage further research on multimodal data synthesis and MLLMs' fundamental capabilities for image understanding",
    "checked": true,
    "id": "6b8850e5607f4d877fe512befcc368a1c62678dc",
    "semantic_title": "img-diff: contrastive data synthesis for multimodal large language models",
    "citation_count": 8,
    "authors": [
      "Qirui Jiao",
      "Daoyuan Chen",
      "Yilun Huang",
      "Bolin Ding",
      "Yaliang Li",
      "Ying Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_CocoER_Aligning_Multi-Level_Feature_by__Competition_and_Coordination_for_CVPR_2025_paper.html": {
    "title": "CocoER: Aligning Multi-Level Feature by Competition and Coordination for Emotion Recognition",
    "volume": "main",
    "abstract": "With the explosion of human-machine interaction, emotion recognition has reignited attention. Previous works focus on improving visual feature fusion and reasoning from multiple image levels. Although it is non-trivial to deduce a person's emotion by integrating multi-level feature (head, body and context), the emotion recognition results of each level is usually different from one another, which creates inconsistency in the prevailing feature alignment method and decrease recognition performance. In this work, we propose a multi-level image feature refinement method for emotion recognition (CocoER) to mitigate the impact caused by conflicting results from multi-level recognition. First, we leverage cross-level attention to improve visual feature consistency between hierarchically cropped head, body and context windows. Then, vocabulary informed alignment is incorporated into the recognition framework to produce pseudo label and guide hierarchical visual feature refinement. To effectively fuse multi-level feature, we elaborate on a competition process of eliminating irrelevant image level predictions and a coordination process to enhance the feature across all levels. Extensive experiments are executed on two popular datasets, and our method achieves state-of-the-art performance with multi-level interpretation results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuli Shen",
      "Hua Cai",
      "Weilin Shen",
      "Qing Xu",
      "Dingding Yu",
      "Weifeng Ge",
      "Xiangyang Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sur_Hyperbolic_Uncertainty-Aware_Few-Shot_Incremental_Point_Cloud_Segmentation_CVPR_2025_paper.html": {
    "title": "Hyperbolic Uncertainty-Aware Few-Shot Incremental Point Cloud Segmentation",
    "volume": "main",
    "abstract": "3D point cloud segmentation is essential across a range of applications; however, conventional methods often struggle in evolving environments, particularly when tasked with identifying novel categories under limited supervision. Few-Shot Learning (FSL) and Class Incremental Learning (CIL) have been adapted previously to address these challenges in isolation, yet the combined paradigm of Few-Shot Class Incremental Learning (FSCIL) remains largely unexplored for point cloud segmentation. To address this gap, we introduce Hyperbolic Ideal Prototypes Optimization (HIPO), a novel framework that harnesses hyperbolic embeddings for FSCIL in 3D point clouds. HIPO employs the Poincare Hyperbolic Sphere as its embedding space, integrating Ideal Prototypes enriched by CLIP-derived class semantics, to capture the hierarchical structure of 3D data. By enforcing orthogonality among prototypes and maximizing representational margins, HIPO constructs a resilient embedding space that mitigates forgetting and enables the seamless integration of new classes, thereby effectively countering overfitting. Extensive evaluations on S3DIS, ScanNetv2, and cross-dataset scenarios demonstrate HIPO's strong performance, significantly surpassing existing approaches in both in-domain and cross-dataset FSCIL tasks for 3D point cloud segmentation",
    "checked": false,
    "id": "db0f0d6564217d794cedde7c937ab0a90522185e",
    "semantic_title": "probabilistic interactive 3d segmentation with hierarchical neural processes",
    "citation_count": 0,
    "authors": [
      "Tanuj Sur",
      "Samrat Mukherjee",
      "Kaizer Rahaman",
      "Subhasis Chaudhuri",
      "Muhammad Haris Khan",
      "Biplab Banerjee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_Enhancing_Creative_Generation_on_Stable_Diffusion-based_Models_CVPR_2025_paper.html": {
    "title": "Enhancing Creative Generation on Stable Diffusion-based Models",
    "volume": "main",
    "abstract": "Recent text-to-image generative models, particularly Stable Diffusion and its distilled variants, have achieved impressive fidelity and strong text-image alignment. However, their creative generation capacity remains limited, as simply adding the term \"creative\" to prompts often fails to yield genuinely creative results. In this paper, we introduce C3 (Creative Concept Catalyst), a training-free approach designed to enhance creativity in Stable Diffusion-based models. C3 selectively amplifies features during the denoising process to foster more creative outputs. We offer practical guidelines for choosing amplification factors based on two main aspects of creativity. C3 allows user-friendly creativity control in image generation and is the first study to enhance creativity in diffusion models without extensive computational costs. We demonstrate its effectiveness across various Stable Diffusion-based models. Source codes will be publicly available",
    "checked": true,
    "id": "e546c997b12e37d84a3eda7c5f86c0d447720206",
    "semantic_title": "enhancing creative generation on stable diffusion-based models",
    "citation_count": 2,
    "authors": [
      "Jiyeon Han",
      "Dahee Kwon",
      "Gayoung Lee",
      "Junho Kim",
      "Jaesik Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_The_Devil_is_in_the_Prompts_Retrieval-Augmented_Prompt_Optimization_for_CVPR_2025_paper.html": {
    "title": "The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation",
    "volume": "main",
    "abstract": "The evolution of Text-to-video (T2V) generative models, trained on large-scale datasets, has been marked by significant progress. However, the sensitivity of T2V generative models to input prompts highlights the critical role of prompt design in influencing generative outcomes. Prior research has predominantly relied on Large Language Models (LLMs) to align user-provided prompts with the distribution of training prompts, albeit without tailored guidance encompassing prompt vocabulary and sentence structure nuances. To this end, we introduce RAPO, a novel Retrieval-Augmented Prompt Optimization framework. In order to address potential inaccuracies and ambiguous details generated by LLM-generated prompts. RAPO refines the naive prompts through dual optimization branches, selecting the superior prompt for T2V generation. The first branch augments user prompts with diverse modifiers extracted from a learned relational graph, refining them to align with the format of training prompts via a fine-tuned LLM. Conversely, the second branch rewrites the naive prompt using a pre-trained LLM following a well-defined instruction set. Extensive experiments demonstrate that RAPO can effectively enhance both the static and dynamic dimensions of generated videos, demonstrating the significance of prompt optimization for user-provided prompts. Project website: \\href https://whynothaha.github.io/Prompt_optimizer/RAPO.html GitHub",
    "checked": true,
    "id": "c5eb46e5872db5fe4f868eb5d6669f667d94e146",
    "semantic_title": "the devil is in the prompts: retrieval-augmented prompt optimization for text-to-video generation",
    "citation_count": 1,
    "authors": [
      "Bingjie Gao",
      "Xinyu Gao",
      "Xiaoxue Wu",
      "Yujie Zhou",
      "Yu Qiao",
      "Li Niu",
      "Xinyuan Chen",
      "Yaohui Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhuravlev_Denoising_Functional_Maps_Diffusion_Models_for_Shape_Correspondence_CVPR_2025_paper.html": {
    "title": "Denoising Functional Maps: Diffusion Models for Shape Correspondence",
    "volume": "main",
    "abstract": "Estimating correspondences between pairs of deformable shapes remains a challenging problem. Despite substantial progress, existing methods lack broad generalization capabilities and require category-specific training data. To address these limitations, we propose a fundamentally new approach to shape correspondence based on denoising diffusion models. In our method, a diffusion model learns to directly predict the functional map, a low-dimensional representation of a point-wise map between shapes. We use a large dataset of synthetic human meshes for training and employ two steps to reduce the number of functional maps that need to be learned. First, the maps refer to a template rather than shape pairs. Second, the functional map is defined in a basis of eigenvectors of the Laplacian, which is not unique due to sign ambiguity. Therefore, we introduce an unsupervised approach to select a specific basis by correcting the signs of eigenvectors based on surface features. Our model achieves competitive performance on standard human datasets, meshes with anisotropic connectivity, non-isometric humanoid shapes, as well as animals compared to existing descriptor-based and large-scale shape deformation methods",
    "checked": true,
    "id": "b45c5f9c082e69d9369ae3d30c30259883785552",
    "semantic_title": "denoising functional maps: diffusion models for shape correspondence",
    "citation_count": 1,
    "authors": [
      "Aleksei Zhuravlev",
      "Zorah Lähner",
      "Vladislav Golyanik"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ke_ProReflow_Progressive_Reflow_with_Decomposed_Velocity_CVPR_2025_paper.html": {
    "title": "ProReflow: Progressive Reflow with Decomposed Velocity",
    "volume": "main",
    "abstract": "Diffusion models have achieved significant progress in both image and video generation while still suffering from huge computation costs. As an effective solution, rectified flow aims to rectify the diffusion process of diffusion models into a straight line for few-step and even one-step generation. However, in this paper, we suggest that the original training pipeline of reflow is not optimal and introduce two techniques to improve it. Firstly, we introduce progressive reflow, which progressively reflows the diffusion models in local timesteps until the whole diffusion progresses, reducing the difficulty of flow matching. Second, we introduce aligned v-prediction, which highlights the importance of direction matching in flow matching over magnitude matching. Experimental results on SDv1.5 and SDXL demonstrate the effectiveness of our method, for example, conducting on SDv1.5 achieves an FID of 10.70 on MSCOCO2014 validation set with only 4 sampling steps, close to our teacher model (32 DDIM steps, FID = 10.05). Our codes will be released at Github",
    "checked": true,
    "id": "12661cef7f57067e991e3984636aa4c934c0f221",
    "semantic_title": "proreflow: progressive reflow with decomposed velocity",
    "citation_count": 1,
    "authors": [
      "Lei Ke",
      "Haohang Xu",
      "Xuefei Ning",
      "Yu Li",
      "Jiajun Li",
      "Haoling Li",
      "Yuxuan Lin",
      "Dongsheng Jiang",
      "Yujiu Yang",
      "Linfeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_DnLUT_Ultra-Efficient_Color_Image_Denoising_via_Channel-Aware_Lookup_Tables_CVPR_2025_paper.html": {
    "title": "DnLUT: Ultra-Efficient Color Image Denoising via Channel-Aware Lookup Tables",
    "volume": "main",
    "abstract": "While deep neural networks have revolutionized image denoising capabilities, their deployment on edge devices remains challenging due to substantial computational and memory requirements. To this end, we present DnLUT, an ultra-efficient lookup table-based framework that achieves high-quality color image denoising with minimal resource consumption. Our key innovation lies in two complementary components: a Pairwise Channel Mixer (PCM) that effectively captures inter-channel correlations and spatial dependencies in parallel, and a novel L-shaped convolution design that maximizes receptive field coverage while minimizing storage overhead. By converting these components into optimized lookup tables post-training, DnLUT achieves remarkable efficiency - requiring only 500KB storage and 0.1% energy consumption compared to its CNN contestant DnCNN, while delivering 20x faster inference. Extensive experiments demonstrate that DnLUT outperforms all existing LUT-based methods by over 1dB in PSNR, establishing a new state-of-the-art in resource-efficient color image denoising",
    "checked": true,
    "id": "02ebf6954f203c1ce5058701d69afed73a633ca8",
    "semantic_title": "dnlut: ultra-efficient color image denoising via channel-aware lookup tables",
    "citation_count": 0,
    "authors": [
      "Sidi Yang",
      "Binxiao Huang",
      "Yulun Zhang",
      "Dahai Yu",
      "Yujiu Yang",
      "Ngai Wong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jo_Devil_is_in_the_Detail_Towards_Injecting_Fine_Details_of_CVPR_2025_paper.html": {
    "title": "Devil is in the Detail: Towards Injecting Fine Details of Image Prompt in Image Generation via Conflict-free Guidance and Stratified Attention",
    "volume": "main",
    "abstract": "While large-scale text-to-image diffusion models enable the generation of high-quality, diverse images from text prompts, these prompts struggle to capture intricate details, such as textures, preventing the user intent from being reflected. This limitation has led to efforts to generate images conditioned on user-provided images, referred to as image prompts. Recent work modifies the self-attention mechanism to impose image conditions in generated images by replacing or concatenating the keys and values from the image prompt. This enables the self-attention layer to work like a cross-attention layer, generally used to incorporate text prompts.In this paper, we identify two common issues in existing methods of modifying self-attention that hinder diffusion models from reflecting the image prompt. By addressing these issues, we propose a novel method that generates images that properly reflect the details of image prompts. First, existing approaches often neglect the importance of image prompts in classifier-free guidance, which directs the model towards the intended conditions and away from those undesirable. Specifically, current methods use image prompts as both desired and undesired conditions, causing conflicting signals. To resolve this, we propose conflict-free guidance by using image prompts only as desired conditions, ensuring that the generated image faithfully reflects the image prompt.In addition, we observe that the two most common self-attention modifications involve a trade-off between the realism of the generated image and alignment with the image prompt, achieved by selectively using keys and values from both images. Specifically, selecting more keys and values from the image prompt improves alignment, while selecting more from the generated image enhances realism. To balance both, we propose an alternative self-attention modification method, Stratified Attention, which jointly uses keys and values from both images rather than selecting between them.Through extensive experiments across three distinct image generation tasks, we demonstrate that the proposed method outperforms existing image-prompting models in faithfully reflecting the image prompt",
    "checked": true,
    "id": "dbcbbff9887b8e2f0dbf140d17a0a2047a6f8c1c",
    "semantic_title": "devil is in the detail: towards injecting fine details of image prompt in image generation via conflict-free guidance and stratified attention",
    "citation_count": 0,
    "authors": [
      "Kyungmin Jo",
      "Jooyeol Yun",
      "Jaegul Choo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_D3-Human_Dynamic_Disentangled_Digital_Human_from_Monocular_Video_CVPR_2025_paper.html": {
    "title": "D^3-Human: Dynamic Disentangled Digital Human from Monocular Video",
    "volume": "main",
    "abstract": "We introduce \\text D ^3\\text -Human , a method for reconstructing Dynamic Disentangled Digital Human geometry from monocular videos. Past monocular video human reconstruction primarily focuses on reconstructing undecoupled clothed human bodies or only reconstructing clothing, making it difficult to apply directly in applications such as animation production. The challenge in reconstructing decoupled clothing and body lies in the occlusion caused by clothing over the body. To this end, the details of the visible area and the plausibility of the invisible area must be ensured during the reconstruction process. Our proposed method combines explicit and implicit representations to model the decoupled clothed human body, leveraging the robustness of explicit representations and the flexibility of implicit representations. Specifically, we reconstruct the visible region as SDF and propose a novel human manifold signed distance field (hmSDF) to segment the visible clothing and visible body, and then merge the visible and invisible body. Extensive experimental results demonstrate that, compared with existing reconstruction schemes, \\text D ^3\\text -Human can achieve high-quality decoupled reconstruction of the human body wearing different clothing, and can be directly applied to clothing transfer and animation production",
    "checked": false,
    "id": "72caf8d5c0ec597882590c596e8d46677b7bccbf",
    "semantic_title": "d3-human: dynamic disentangled digital human from monocular video",
    "citation_count": 0,
    "authors": [
      "Honghu Chen",
      "Bo Peng",
      "Yunfan Tao",
      "Juyong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Seo_BiM-VFI_Bidirectional_Motion_Field-Guided_Frame_Interpolation_for_Video_with_Non-uniform_CVPR_2025_paper.html": {
    "title": "BiM-VFI: Bidirectional Motion Field-Guided Frame Interpolation for Video with Non-uniform Motions",
    "volume": "main",
    "abstract": "Existing Video Frame interpolation (VFI) models tend to suffer from time-to-location ambiguity when trained with video of non-uniform motions, such as accelerating, decelerating, and changing directions, which often yield blurred interpolated frames.In this paper, we propose (i) a novel motion description map, Bidirectional Motion field (BiM), to effectively describe non-uniform motions; (ii) a BiM-guided Flow Net (BiMFN) with Content-Aware Upsampling Network (CAUN) for precise optical flow estimation; and (iii) Knowledge Distillation for VFI-centric Flow supervision (KDVCF) to supervise the motion estimation of VFI model with VFI-centric teacher flows.The proposed VFI is called a Bidirectional Motion field-guided VFI (BiM-VFI) model.Extensive experiments show that our BiM-VFI model significantly surpasses the recent state-of-the-art VFI methods by 26% and 45% improvements in LPIPS and STLPIPS respectively, yielding interpolated frames with much fewer blurs at arbitrary time instances",
    "checked": true,
    "id": "cca4c5c44f1e3c064160fef5b5061f2bac0728a7",
    "semantic_title": "bim-vfi: bidirectional motion field-guided frame interpolation for video with non-uniform motions",
    "citation_count": 0,
    "authors": [
      "Wonyong Seo",
      "Jihyong Oh",
      "Munchurl Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Curriculum_Coarse-to-Fine_Selection_for_High-IPC_Dataset_Distillation_CVPR_2025_paper.html": {
    "title": "Curriculum Coarse-to-Fine Selection for High-IPC Dataset Distillation",
    "volume": "main",
    "abstract": "Dataset distillation (DD) excels in synthesizing a small number of images per class (IPC) but struggles to maintain its effectiveness in high-IPC settings. Recent works on dataset distillation demonstrate that combining distilled and real data can mitigate the effectiveness decay. However, our analysis of the combination paradigm reveals that the current one-shot and independent selection mechanism induces an incompatibility issue between distilled and real images. To address this issue, we introduce a novel curriculum coarse-to-fine selection (CCFS) method for efficient high-IPC dataset distillation. CCFS employs a curriculum selection framework for real data selection, where we leverage a coarse-to-fine strategy to select appropriate real data based on the current synthetic dataset in each curriculum. Extensive experiments validate CCFS, surpassing the state-of-the-art by +6.6% on CIFAR-10, +5.8% on CIFAR-100, and +3.4% on Tiny-ImageNet under high-IPC settings. Notably, CCFS achieves 60.2% test accuracy on ResNet-18 with a 20% compression ratio of Tiny-ImageNet, closely matching full-dataset training with only 0.3% degradation. Code: https://github.com/CYDaaa30/CCFS",
    "checked": true,
    "id": "c2a4d826bfdda42899980203e6b87b161512ccc3",
    "semantic_title": "curriculum coarse-to-fine selection for high-ipc dataset distillation",
    "citation_count": 3,
    "authors": [
      "Yanda Chen",
      "Gongwei Chen",
      "Miao Zhang",
      "Weili Guan",
      "Liqiang Nie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor_CVPR_2025_paper.html": {
    "title": "BADGR: Bundle Adjustment Diffusion Conditioned by Gradients for Wide-Baseline Floor Plan Reconstruction",
    "volume": "main",
    "abstract": "Reconstructing precise camera poses and floor plan layouts from wide-baseline RGB panoramas is a difficult and unsolved problem. We introduce BADGR, a novel diffusion model that jointly performs reconstruction and bundle adjustment (BA) to refine poses and layouts from a coarse state, using 1D floor boundary predictions from dozens of sparsely captured images. Unlike guided diffusion models, BADGR is conditioned on dense per-column outputs from a single-step Levenberg Marquardt (LM) optimizer and is trained to predict camera and wall positions, while minimizing reprojection errors for view consistency. The objective of layout generation from denoising diffusion process complements BA optimization by providing additional learned layout-structural constraints on top of the co-visible features across images. These constraints help BADGR make plausible guesses about spatial relationships, which constrain the pose graph, such as wall adjacency and collinearity, while also learning to mitigate errors from dense boundary observations using global context. BADGR trains exclusively on 2D floor plans, simplifying data acquisition, enabling robust augmentation, and supporting a variety of input densities. Our experiments validate our method, which significantly outperforms the state-of-the-art pose and floor plan layout reconstruction with different input densities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuguang Li",
      "Ivaylo Boyadzhiev",
      "Zixuan Liu",
      "Linda Shapiro",
      "Alex Colburn"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bae_Three_Cars_Approaching_within_100m_Enhancing_Distant_Geometry_by_Tri-Axis_CVPR_2025_paper.html": {
    "title": "Three Cars Approaching within 100m! Enhancing Distant Geometry by Tri-Axis Voxel Scanning for Camera-based Semantic Scene Completion",
    "volume": "main",
    "abstract": "Camera-based Semantic Scene Completion (SSC) is gaining attentions in the 3D perception field. However, properties such as perspective and occlusion lead to the underestimation of the geometry in distant regions, posing a critical issue for safety-focused autonomous driving systems. To tackle this, we propose ScanSSC, a novel camera-based SSC model composed of a Scan Module and Scan Loss, both designed to enhance distant scenes by leveraging context from near-viewpoint scenes. The Scan Module uses axis-wise masked attention, where each axis employing a near-to-far cascade masking that enables distant voxels to capture relationships with preceding voxels. In addition, the Scan Loss computes the cross-entropy along each axis between cumulative logits and corresponding class distributions in a near-to-far direction, thereby propagating rich context-aware signals to distant voxels. Leveraging the synergy between these components, ScanSSC achieves state-of-the-art performance, with IoUs of 44.54 and 48.29, and mIoUs of 17.40 and 20.14 on the SemanticKITTI and SSCBench-KITTI-360 benchmarks",
    "checked": true,
    "id": "432adb636a421a7cd38d7aecdf9937e8e848e005",
    "semantic_title": "three cars approaching within 100m! enhancing distant geometry by tri-axis voxel scanning for camera-based semantic scene completion",
    "citation_count": 1,
    "authors": [
      "Jongseong Bae",
      "Junwoo Ha",
      "Ha Young Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MetaShadow_Object-Centered_Shadow_Detection_Removal_and_Synthesis_CVPR_2025_paper.html": {
    "title": "MetaShadow: Object-Centered Shadow Detection, Removal, and Synthesis",
    "volume": "main",
    "abstract": "Shadows are often underconsidered or even ignored in image editing applications, limiting the realism of the edited results. In this paper, we introduce MetaShadow, a three-in-one versatile framework that enables detection, removal, and controllable synthesis of shadows in natural images in an object-centered fashion. MetaShadow combines the strengths of two cooperative components: Shadow Analyzer, for object-centered shadow detection and removal, and Shadow Synthesizer, for reference-based controllable shadow synthesis. Notably, we optimize the learning of the intermediate features from Shadow Analyzer to guide Shadow Synthesizer to generate more realistic shadows that blend seamlessly with the scene. Extensive evaluations on multiple shadow benchmark datasets show significant improvements of MetaShadow over the existing state-of-the-art methods on object-centered shadow detection, removal, and synthesis. MetaShadow excels in supporting imageediting tasks such as object removal, relocation, and insertion, pushing the boundaries of object-centered image editing",
    "checked": true,
    "id": "8e615af86d62801c68ef777dd5ac21d2a8586737",
    "semantic_title": "metashadow: object-centered shadow detection, removal, and synthesis",
    "citation_count": 2,
    "authors": [
      "Tianyu Wang",
      "Jianming Zhang",
      "Haitian Zheng",
      "Zhihong Ding",
      "Scott Cohen",
      "Zhe Lin",
      "Wei Xiong",
      "Chi-Wing Fu",
      "Luis Figueroa",
      "Soo Ye Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ziliotto_TANGO_Training-free_Embodied_AI_Agents_for_Open-world_Tasks_CVPR_2025_paper.html": {
    "title": "TANGO: Training-free Embodied AI Agents for Open-world Tasks",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated excellent capabilities in composing various modules together to create programs that can perform complex reasoning tasks on images. In this paper, we propose TANGO, an approach that extends the program composition via LLMs already observed for images, aiming to integrate those capabilities into embodied agents capable of observing and acting in the world. Specifically, by employing a simple PointGoal Navigation model combined with a memory-based exploration policy as a foundational primitive for guiding an agent through the world, we show how a single model can address diverse tasks without additional training. We task an LLM with composing the provided primitives to solve a specific task, using only a few in-context examples in the prompt. We evaluate our approach on three key Embodied AI tasks: Open-Set ObjectGoal Navigation, Multi-Modal Lifelong Navigation, and Open Embodied Question Answering, achieving state-of-the-art results without any specific fine-tuning in challenging zero-shot scenarios",
    "checked": true,
    "id": "2fef87c86ea8b318a81031befe5e09fe3706bccd",
    "semantic_title": "tango: training-free embodied ai agents for open-world tasks",
    "citation_count": 5,
    "authors": [
      "Filippo Ziliotto",
      "Tommaso Campari",
      "Luciano Serafini",
      "Lamberto Ballan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nikzad_SATA_Spatial_Autocorrelation_Token_Analysis_for_Enhancing_the_Robustness_of_CVPR_2025_paper.html": {
    "title": "SATA: Spatial Autocorrelation Token Analysis for Enhancing the Robustness of Vision Transformers",
    "volume": "main",
    "abstract": "Over the past few years, vision transformers (ViTs) have consistently demonstrated remarkable performance across various visual recognition tasks. However, attempts to enhance their robustness have yielded limited success, mainly focusing on different training strategies, input patch augmentation, or network structural enhancements. These approaches often involve extensive training and fine-tuning, which are time-consuming and resource-intensive. To tackle these obstacles, we introduce a novel approach named Spatial Autocorrelation Token Analysis (SATA). By harnessing spatial relationships between token features, SATA enhances both the representational capacity and robustness of ViT models. This is achieved through the analysis and grouping of tokens according to their spatial autocorrelation scores prior to their input into the Feed-Forward Network (FFN) block of the self-attention mechanism. Importantly, SATA seamlessly integrates into existing pre-trained ViT baselines without requiring retraining or additional fine-tuning, while concurrently improving efficiency by reducing the computational load of the FFN units. Experimental results show that the baseline ViTs enhanced with SATA not only achieve a new state-of-the-art top-1 accuracy on ImageNet-1K image classification (94.9%) but also establish new state-of-the-art performance across multiple robustness benchmarks, including ImageNet-A (top-1=63.6%), ImageNet-R (top-1=79.2%), and ImageNet-C (mCE=13.6%), all without requiring additional training or fine-tuning of baseline models. Availability: https://github.com/nick-nikzad/SATA",
    "checked": true,
    "id": "ea6aebdeecc68286d6d743c73cb46f0b26e794da",
    "semantic_title": "sata: spatial autocorrelation token analysis for enhancing the robustness of vision transformers",
    "citation_count": 0,
    "authors": [
      "Nick Nikzad",
      "Yi Liao",
      "Yongsheng Gao",
      "Jun Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_DViN_Dynamic_Visual_Routing_Network_for_Weakly_Supervised_Referring_Expression_CVPR_2025_paper.html": {
    "title": "DViN: Dynamic Visual Routing Network for Weakly Supervised Referring Expression Comprehension",
    "volume": "main",
    "abstract": "In this paper, we focus on weakly supervised referring expression comprehension (REC), and identify that the lack of fine-grained visual capability greatly limits the upper performance bound of existing methods. To address this issue, we propose a novel framework for weakly supervised REC, namely Dynamic Visual routing Network (DViN), which overcomes the visual shortcomings from the perspective of feature combination and alignment. In particular, DViN is equipped with a novel sparse routing mechanism to efficiently combine features of multiple visual encoders in a dynamic manner, thus improving the visual descriptive power. Besides, we further propose an innovative weakly supervised objective, namely Routing-based Feature Alignment (RFA), which facilitates the visual understanding of routed features through the intra-modal and inter-modal alignment. To validate DViN, we conduct extensive experiments on four REC benchmark datasets. Experiments demonstrate that DViN achieves state-of-the-art results on four benchmarks while maintaining competitive inference efficiency. Besides, the strong generalization ability of DViN is also validated on weakly supervised referring expression segmentation. Source codes are anonymously released at: https://anonymous.4open.science/r/DViN-7736",
    "checked": false,
    "id": "a4c65265f1f05966fdbb11c7706c84e63da5ae4b",
    "semantic_title": "weakmcn: multi-task collaborative network for weakly supervised referring expression comprehension and segmentation",
    "citation_count": 0,
    "authors": [
      "Xiaofu Chen",
      "Yaxin Luo",
      "Gen Luo",
      "Jiayi Ji",
      "Henghui Ding",
      "Yiyi Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Nested_Diffusion_Models_Using_Hierarchical_Latent_Priors_CVPR_2025_paper.html": {
    "title": "Nested Diffusion Models Using Hierarchical Latent Priors",
    "volume": "main",
    "abstract": "We introduce nested diffusion models, an efficient and powerful hierarchical generative framework that substantially enhances the generation quality of diffusion models, particularly for images of complex scenes. Our approach employs a series of diffusion models to progressively generate latent variables at different semantic levels. Each model in this series is conditioned on the output of the preceding higher-level models, culminating in image generation. Hierarchical latent variables guide the generation process along predefined semantic pathways, allowing our approach to capture intricate structural details. To construct these latent variables, we leverage a pre-trained visual encoder, which learns strong semantic visual representations, and modulate its capacity via dimensionality reduction and noise injection. Across multiple datasets, our system demonstrates significant enhancements in image quality for both unconditional and class/text conditional generation. Moreover, our unconditional generation system substantially outperforms the baseline conditional system. These advancements incur minimal computational overhead as the more abstract levels of our hierarchy work with lower-dimensional representations",
    "checked": true,
    "id": "5a613652d700f9a271b6d01c7d9e4223e9883300",
    "semantic_title": "nested diffusion models using hierarchical latent priors",
    "citation_count": 1,
    "authors": [
      "Xiao Zhang",
      "Ruoxi Jiang",
      "Rebecca Willett",
      "Michael Maire"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_A_Theory_of_Learning_Unified_Model_via_Knowledge_Integration_from_CVPR_2025_paper.html": {
    "title": "A Theory of Learning Unified Model via Knowledge Integration from Label Space Varying Domains",
    "volume": "main",
    "abstract": "Existing domain adaptation systems can hardly be applied to real-world problems with new classes presenting at deployment time, especially regarding source-free scenarios where multiple source domains do not share the label space despite being given a few labeled target data. To address this, we consider a challenging problem: multi-source semi-supervised open-set domain adaptation and propose a learning theory via joint error, effectively tackling strong domain shift. To generalize the algorithm into source-free cases, we introdcue a computationally efficient and architecture-flexible attention-based feature generation module. Extensive experiments on various data sets demonstrate the significant improvement of our proposed algorithm over baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dexuan Zhang",
      "Thomas Westfechtel",
      "Tatsuya Harada"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_HiLoTs_High-Low_Temporal_Sensitive_Representation_Learning_for_Semi-Supervised_LiDAR_Segmentation_CVPR_2025_paper.html": {
    "title": "HiLoTs: High-Low Temporal Sensitive Representation Learning for Semi-Supervised LiDAR Segmentation in Autonomous Driving",
    "volume": "main",
    "abstract": "LiDAR point cloud semantic segmentation plays a crucial role in autonomous driving. In recent years, semi-supervised methods have gained popularity due to their significant reduction in annotation labor and time costs. Current semi-supervised methods typically focus on point cloud spatial distribution or consider short-term temporal representations, e.g., only two adjacent frames, often overlooking the rich long-term temporal properties inherent in autonomous driving scenarios. In driving experience, we observe that nearby objects, such as roads and vehicles, remain stable while driving, whereas distant objects exhibit greater variability in category and shape. This natural phenomenon is also captured by LiDAR, which reflects lower temporal sensitivity for nearby objects and higher sensitivity for distant ones. To leverage these characteristics, we propose HiLoTs, which learns high-temporal sensitivity and low-temporal sensitivity representations from continuous LiDAR frames. These representations are further enhanced and fused using a cross-attention mechanism. Additionally, we employ a teacher-student framework to align the representations learned by the labeled and unlabeled branches, effectively utilizing the large amounts of unlabeled data. Experimental results on the SemanticKITTI and nuScenes datasets demonstrate that our proposed HiLoTs outperforms state-of-the-art semi-supervised methods, and achieves performance close to LiDAR+Camera multimodal approaches",
    "checked": true,
    "id": "5e59c75d3409557747abc43c43f3676b7377a64d",
    "semantic_title": "hilots: high-low temporal sensitive representation learning for semi-supervised lidar segmentation in autonomous driving",
    "citation_count": 0,
    "authors": [
      "R.D. Lin",
      "Pengcheng Weng",
      "Yinqiao Wang",
      "Han Ding",
      "Jinsong Han",
      "Fei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Spiking_Transformer_with_Spatial-Temporal_Attention_CVPR_2025_paper.html": {
    "title": "Spiking Transformer with Spatial-Temporal Attention",
    "volume": "main",
    "abstract": "Spike-based Transformer presents a compelling and energy-efficient alternative to traditional Artificial Neural Network (ANN)-based Transformers, achieving impressive results through sparse binary computations. However, existing spike-based transformers predominantly focus on spatial attention while neglecting crucial temporal dependencies inherent in spike-based processing, leading to suboptimal feature representation and limited performance. To address this limitation, we propose Spiking Transformer with Spatial-Temporal Attention (STAtten), a simple and straightforward architecture that efficiently integrates both spatial and temporal information in the self-attention mechanism. STAtten introduces a block-wise computation strategy that processes information in spatial-temporal chunks, enabling comprehensive feature capture while maintaining the same computational complexity as previous spatial-only approaches. Our method can be seamlessly integrated into existing spike-based transformers without architectural overhaul. Extensive experiments demonstrate that STAtten significantly improves the performance of existing spike-based transformers across both static and neuromorphic datasets, including CIFAR10/100, ImageNet, CIFAR10-DVS, and N-Caltech101",
    "checked": true,
    "id": "38492494f81d6cc96d9d233a3262b4cd0b691fe5",
    "semantic_title": "spiking transformer with spatial-temporal attention",
    "citation_count": 3,
    "authors": [
      "Donghyun Lee",
      "Yuhang Li",
      "Youngeun Kim",
      "Shiting Xiao",
      "Priyadarshini Panda"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Khan_Perceptual_Video_Compression_with_Neural_Wrapping_CVPR_2025_paper.html": {
    "title": "Perceptual Video Compression with Neural Wrapping",
    "volume": "main",
    "abstract": "Standard video codecs are rate-distortion optimization machines, where distortion is typically quantified using PSNR versus the source. However, it is now widely accepted that increasing PSNR does not necessarily translate to better visual quality. In this paper, a better balance between perception and fidelity is targeted, in order to provide for significant rate savings over state-of-the-art standards-based video codecs. Specifically, pre- and postprocessing neural networks are proposed that enhance the coding efficiency of standard video codecs when benchmarked with an array of well-established perceptual quality scores. These \"neural wrapper\" elements are end-to-end trained with a neural codec module serving as a differentiable proxy for standard video codecs. The codec proxy is jointly optimized with the pre- and post components, via a novel two-phase pretraining strategy and end-to-end iterative refinement with stop-gradient. This allows the neural pre- and postprocessor to learn to embed, remove and recover information in a codec-aware manner, thus improving its rate-quality performance. A single neural-wrapper model is thereby established and used for the entire rate-quality curve without needing any downscaling or upscaling. The trained model is tested with the AV1 and VVC standard codecs via an array of well-established objective quality scores (SSIM, MS-SSIM, VMAF, AVQT), as well as mean opinion scores (MOS) derived from ITU-T P.910 subjective testing. Experimental results show that the proposed approach improves all quality scores, with -18.5% average Bjontegaard Delta-rate (BD-rate) saving over all objective scores and MOS improvement over both standard codecs. This illustrates the significant potential of neural wrapper components over standards-based video coding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Umar Karim Khan",
      "Aaron Chadha",
      "Mohammad Ashraful Anam",
      "Yiannis Andreopoulos"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_ViKIENet_Towards_Efficient_3D_Object_Detection_with_Virtual_Key_Instance_CVPR_2025_paper.html": {
    "title": "ViKIENet: Towards Efficient 3D Object Detection with Virtual Key Instance Enhanced Network",
    "volume": "main",
    "abstract": "The sparsity of point clouds and inadequacy of semantic information pose challenges to current LiDAR-only 3D object detection methods. Recent methods alleviate these challenges by converting RGB images into virtual points via depth completion to be fused with LiDAR points. Although these methods have shown outstanding results, they often introduce significant computation overhead due to the high density of virtual points and noise due to inaccurate depth completion. Besides, they do not thoroughly leverage semantic information from images. In this work, we propose the virtual key instance enhanced network (ViKIENet), a highly efficient and effective multi-modal feature fusion framework that fuses the features of virtual key instances (VKIs) and LiDAR points through multiple stages. Our contributions include three main components: semantic key instance selection (SKIS), virtual-instance-focused fusion (VIFF), and virtual-instance-to-real attention (VIRA). We also propose the extended version ViKIENet-R with VIFF-R which includes rotationally equivariant features. Experiment results show that ViKIENet and ViKIENet-R achieve significant improvements in detection performance on the KITTI, JRDB, and nuScenes datasets compared to existing works. On the KITTI dataset, ViKIENet and ViKIENet-R operate at 22.7 and 15.0 FPS, respectively. As of CVPR submission (Nov. 15th, 2024), ViKIENet ranks first on the car detection and orientation estimation leaderboard, while ViKIENet-R ranks second (compared with officially published papers) on the 3D car detection leaderboard",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuochen Yu",
      "Bijie Qiu",
      "Andy W. H. Khong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiang_DKDM_Data-Free_Knowledge_Distillation_for_Diffusion_Models_with_Any_Architecture_CVPR_2025_paper.html": {
    "title": "DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any Architecture",
    "volume": "main",
    "abstract": "Diffusion models (DMs) have demonstrated exceptional generative capabilities across various domains, including image, video, and so on. A key factor contributing to their effectiveness is the high quantity and quality of data used during training. However, mainstream DMs now consume increasingly large amounts of data. For example, training a Stable Diffusion model requires billions of image-text pairs. This enormous data requirement poses significant challenges for training large DMs due to high data acquisition costs and storage expenses. To alleviate this data burden, we propose a novel scenario: using existing DMs as data sources to train new DMs with any architecture. We refer to this scenario as Data-Free Knowledge Distillation for Diffusion Models (DKDM), where the generative ability of DMs is transferred to new ones in a data-free manner. To tackle this challenge, we make two main contributions. First, we introduce a DKDM objective that enables the training of new DMs via distillation, without requiring access to the data. Second, we develop a dynamic iterative distillation method that efficiently extracts time-domain knowledge from existing DMs, enabling direct retrieval of training data without the need for a prolonged generative process. To the best of our knowledge, we are the first to explore this scenario. Experimental results demonstrate that our data-free approach not only achieves competitive generative performance but also, in some instances, outperforms models trained with the entire dataset",
    "checked": true,
    "id": "5aa19b8158accc50c7feda1bc1827b9c3f3fd64f",
    "semantic_title": "dkdm: data-free knowledge distillation for diffusion models with any architecture",
    "citation_count": 10,
    "authors": [
      "Qianlong Xiang",
      "Miao Zhang",
      "Yuzhang Shang",
      "Jianlong Wu",
      "Yan Yan",
      "Liqiang Nie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jia_SymDPO_Boosting_In-Context_Learning_of_Large_Multimodal_Models_with_Symbol_CVPR_2025_paper.html": {
    "title": "SymDPO: Boosting In-Context Learning of Large Multimodal Models with Symbol Demonstration Direct Preference Optimization",
    "volume": "main",
    "abstract": "As language models continue to scale, Large Language Models (LLMs) have exhibited emerging capabilities in In-Context Learning (ICL), enabling them to solve language tasks by prefixing a few in-context demonstrations (ICDs) as context. Inspired by these advancements, researchers have extended these techniques to develop Large Multimodal Models (LMMs) with ICL capabilities. However, existing LMMs face a critical issue: they often fail to effectively leverage the visual context in multimodal demonstrations and instead simply follow textual patterns. This indicates that LMMs do not achieve effective alignment between multimodal demonstrations and model outputs. To address this problem, we propose Symbol Demonstration Direct Preference Optimization (SymDPO). Specifically, SymDPO aims to break the traditional paradigm of constructing multimodal demonstrations by using random symbols to replace text answers within instances. This forces the model to carefully understand the demonstration images and establish a relationship between the images and the symbols to answer questions correctly. We validate the effectiveness of this method on multiple benchmarks, demonstrating that with SymDPO, LMMs can more effectively understand the multimodal context within examples and utilize this knowledge to answer questions better. Code is available at https://github.com/APiaoG/SymDPO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongrui Jia",
      "Chaoya Jiang",
      "Haiyang Xu",
      "Wei Ye",
      "Mengfan Dong",
      "Ming Yan",
      "Ji Zhang",
      "Fei Huang",
      "Shikun Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Stealthy_Backdoor_Attack_in_Self-Supervised_Learning_Vision_Encoders_for_Large_CVPR_2025_paper.html": {
    "title": "Stealthy Backdoor Attack in Self-Supervised Learning Vision Encoders for Large Vision Language Models",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) vision encoders learn high-quality image representations and thus have become a vital part of developing vision modality of large vision language models (LVLMs). Due to the high cost of training such encoders, pre-trained encoders are widely shared and deployed into many LVLMs, which are security-critical or bear societal significance. Under this practical scenario, we reveal a new backdoor threat that significant visual hallucinations can be induced into these LVLMs by merely compromising vision encoders. Because of the sharing and reuse of these encoders, many downstream LVLMs may inherit backdoor behaviors from encoders, leading to widespread backdoors. In this work, we propose BadVision, the first method to exploit this vulnerability in SSL vision encoders for LVLMs with novel trigger optimization and backdoor learning techniques. We evaluate BadVision on two types of SSL encoders and LVLMs across eight benchmarks. We show that BadVision effectively drives the LVLMs to attacker-chosen hallucination with over 99% attack success rate, causing a 77.6% relative visual understanding error while maintaining the stealthiness. SoTA backdoor detection methods cannot detect our attack effectively",
    "checked": true,
    "id": "0a7081dc6dcae9516231e102932a1dc6095b445b",
    "semantic_title": "stealthy backdoor attack in self-supervised learning vision encoders for large vision language models",
    "citation_count": 2,
    "authors": [
      "Zhaoyi Liu",
      "Huan Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Data-free_Universal_Adversarial_Perturbation_with_Pseudo-semantic_Prior_CVPR_2025_paper.html": {
    "title": "Data-free Universal Adversarial Perturbation with Pseudo-semantic Prior",
    "volume": "main",
    "abstract": "Data-free Universal Adversarial Perturbation (UAP) is an image-agnostic adversarial attack that deceives deep neural networks using a single perturbation generated solely from random noise without relying on data priors. However, traditional data-free UAP methods often suffer from limited transferability due to the absence of semantic content in random noise. To address this issue, we propose a novel data-free universal attack method that recursively extracts pseudo-semantic priors directly from the UAPs during training to enrich the semantic content within the data-free UAP framework. Our approach effectively leverages latent semantic information within UAPs via region sampling, enabling successful input transformations--typically ineffective in traditional data-free UAP methods due to the lack of semantic cues--and significantly enhancing black-box transferability. Furthermore, we introduce a sample reweighting technique to mitigate potential imbalances from random sampling and transformations, emphasizing hard examples less affected by the UAPs. Comprehensive experiments on ImageNet show that our method achieves state-of-the-art performance in average fooling rate by a substantial margin, notably improves attack transferability across various CNN architectures compared to existing data-free UAP methods, and even surpasses data-dependent UAP methods. Code is available at: https://github.com/ChnanChan/PSP-UAP",
    "checked": true,
    "id": "256601384b896f6e0abc96f8ce74cdf479de6642",
    "semantic_title": "data-free universal adversarial perturbation with pseudo-semantic prior",
    "citation_count": 0,
    "authors": [
      "Chanhui Lee",
      "Yeonghwan Song",
      "Jeany Son"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Debiasing_Multimodal_Large_Language_Models_via_Noise-Aware_Preference_Optimization_CVPR_2025_paper.html": {
    "title": "Debiasing Multimodal Large Language Models via Noise-Aware Preference Optimization",
    "volume": "main",
    "abstract": "Multimodal Large Language Models (MLLMs) excel in various tasks, yet often struggle with modality bias, tending to rely heavily on a single modality or prior knowledge when generating responses. In this paper, we propose a debiased preference optimization dataset, RLAIF-V-Bias, and introduce a Noise-Aware Preference Optimization (NAPO) algorithm. Specifically, we first construct the dataset by introducing perturbations to reduce the informational content of certain modalities, prompting the model to overly rely on a specific modality when generating responses. To address the inevitable noise in automatically constructed data, we combine the noise-robust Mean Absolute Error (MAE) with the Binary Cross-Entropy (BCE) in Direct Preference Optimization (DPO) using a negative Box-Cox transformation and dynamically adjust the algorithm's noise robustness based on the evaluated noise levels in the data.Extensive experiments validate our approach, demonstrating not only its effectiveness in mitigating modality bias but also its significant role in minimizing hallucinations",
    "checked": true,
    "id": "c1b2e20202fbe9989cbe02f0acde78559bb38d7b",
    "semantic_title": "debiasing multimodal large language models via noise-aware preference optimization",
    "citation_count": 1,
    "authors": [
      "Zefeng Zhang",
      "Hengzhu Tang",
      "Jiawei Sheng",
      "Zhenyu Zhang",
      "Yiming Ren",
      "Zhenyang Li",
      "Dawei Yin",
      "Duohe Ma",
      "Tingwen Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SAM2-LOVE_Segment_Anything_Model_2_in_Language-aided_Audio-Visual_Scenes_CVPR_2025_paper.html": {
    "title": "SAM2-LOVE: Segment Anything Model 2 in Language-aided Audio-Visual Scenes",
    "volume": "main",
    "abstract": "Reference Audio-Visual Segmentation (Ref-AVS) aims to provide a pixel-wise scene understanding in Language-aided Audio-Visual Scenes (LAVS). This task requires the model to continuously segment objects referred to by text and audio from a video. Previous dual-modality methods always fail due to the lack of a third modality and the existing triple-modality method struggles with spatio-temporal consistency, leading to the target shift of different frames. In this work, we introduce a novel framework, termed SAM2-LOVE, which integrates textual, audio, and visual representations into a learnable token to prompt and align SAM2 for achieving Ref-AVS in the LAVS. Technically, our approach includes a multimodal fusion module aimed at improving multimodal understanding of SAM2, as well as token propagation and accumulation strategies designed to enhance spatio-temporal consistency without forgetting historical information. We conducted extensive experiments to demonstrate that SAM2-LOVE outperforms the SOTA by 8.5% in J&F on the Ref-AVS benchmark and showcase the simplicity and effectiveness of the components. Our code will be available here",
    "checked": true,
    "id": "1f87b567685c15823bd39bd8990030f99a1e5c9a",
    "semantic_title": "sam2-love: segment anything model 2 in language-aided audio-visual scenes",
    "citation_count": 0,
    "authors": [
      "Yuji Wang",
      "Haoran Xu",
      "Yong Liu",
      "Jiaze Li",
      "Yansong Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_GIVEPose_Gradual_Intra-class_Variation_Elimination_for_RGB-based_Category-Level_Object_Pose_CVPR_2025_paper.html": {
    "title": "GIVEPose: Gradual Intra-class Variation Elimination for RGB-based Category-Level Object Pose Estimation",
    "volume": "main",
    "abstract": "Recent advances in RGBD-based category-level object pose estimation have been limited by their reliance on precise depth information, restricting their broader applicability. In response, RGB-based methods have been developed. Among these methods, geometry-guided pose regression that originated from instance-level tasks has demonstrated strong performance. However, we argue that the NOCS map is an inadequate intermediate representation for geometry-guided pose regression method, as its many-to-one correspondence with category-level pose introduces redundant instance-specific information, resulting in suboptimal results. This paper identifies the intra-class variation problem inherent in pose regression based solely on the NOCS map and proposes the Intra-class Variation-Free Consensus (IVFC) map, a novel coordinate representation generated from the category-level consensus model. By leveraging the complementary strengths of the NOCS map and the IVFC map, we introduce GIVEPose, a framework that implements Gradual Intra-class Variation Elimination for category-level object pose estimation. Extensive evaluations on both synthetic and real-world datasets demonstrate that GIVEPose significantly outperforms existing state-of-the-art RGB-based approaches, achieving substantial improvements in category-level object pose estimation. Our code is available at https://github.com/ziqin-h/GIVEPose",
    "checked": true,
    "id": "3b6eb37c314c1814b70a0e30268ac5bf92449ed3",
    "semantic_title": "givepose: gradual intra-class variation elimination for rgb-based category-level object pose estimation",
    "citation_count": 1,
    "authors": [
      "Ziqin Huang",
      "Gu Wang",
      "Chenyangguang Zhang",
      "Ruida Zhang",
      "Xiu Li",
      "Xiangyang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video_CVPR_2025_paper.html": {
    "title": "FRAME: Floor-aligned Representation for Avatar Motion from Egocentric Video",
    "volume": "main",
    "abstract": "Egocentric motion capture with a head-mounted body-facing stereo camera is crucial for VR and AR applications but presents significant challenges such as heavy occlusions and limited annotated real-world data. Existing methods rely on synthetic pretraining and struggle to generate smooth and accurate predictions in real-world settings, particularly for lower limbs. Our work addresses these limitations by introducing a lightweight VR-based data collection setup with on-board, real-time 6D pose tracking. Using this setup, we collected the most extensive real-world dataset for ego-facing ego-mounted cameras to date in size and motion variability. Effectively integrating this multimodal input -- device pose and camera feeds -- is challenging due to the differing characteristics of each data source. To address this, we propose FRAME, a simple yet effective architecture that combines device pose and camera feeds for state-of-the-art body pose prediction through geometrically sound multimodal integration and can run at 300 FPS on modern hardware. Lastly, we showcase a novel training strategy to enhance the model's generalization capabilities. Our approach exploits the problem's geometric properties, yielding high-quality motion capture free from common artifacts in prior works. Qualitative and quantitative evaluations, along with extensive comparisons, demonstrate the effectiveness of our method. Data, code, and CAD designs will be available at https://vcai.mpi-inf.mpg.de/projects/FRAME/",
    "checked": true,
    "id": "03a4cab525ef61922be3212f4e887593f8f9f561",
    "semantic_title": "frame: floor-aligned representation for avatar motion from egocentric video",
    "citation_count": 0,
    "authors": [
      "Andrea Boscolo Camiletto",
      "Jian Wang",
      "Eduardo Alvarado",
      "Rishabh Dabral",
      "Thabo Beeler",
      "Marc Habermann",
      "Christian Theobalt"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sain_Sketch_Down_the_FLOPs_Towards_Efficient_Networks_for_Human_Sketch_CVPR_2025_paper.html": {
    "title": "Sketch Down the FLOPs: Towards Efficient Networks for Human Sketch",
    "volume": "main",
    "abstract": "As sketch research has collectively matured over time, its adaptation for at-mass commercialisation emerges on the immediate horizon. Despite an already mature research endeavour for photos, there is no research on the efficient inference specifically designed for sketch data. In this paper, we first demonstrate existing state-of-the-art efficient light-weight models designed for photos do not work on sketches. We then propose two sketch-specific components which work in a plug-n-play manner on any photo efficient network to adapt them to work on sketch data. We specifically chose fine-grained sketch-based image retrieval (FG-SBIR) as a demonstrator as the most recognised sketch problem with immediate commercial value. Technically speaking, we first propose a cross-modal knowledge distillation network to transfer existing photo efficient networks to be compatible with sketch, which brings down number of FLOPs and model parameters by 97.96% percent and 84.89% respectively. We then exploit the abstract trait of sketch to introduce a RL-based canvas selector that dynamically adjusts to the abstraction level which further cuts down number of FLOPs by two thirds. The end result is an overall reduction of 99.37% of FLOPs (from 40.18G to 0.254G) when compared with a full network, while retaining the accuracy (33.03% vs 32.77%) -- finally making an efficient network for the sparse sketch data that exhibit even fewer FLOPs than the best photo counterpart",
    "checked": true,
    "id": "b75057d59be562deb1dbd48490afaa9a58764df0",
    "semantic_title": "sketch down the flops: towards efficient networks for human sketch",
    "citation_count": 0,
    "authors": [
      "Aneeshan Sain",
      "Subhajit Maity",
      "Pinaki Nath Chowdhury",
      "Shubhadeep Koley",
      "Ayan Kumar Bhunia",
      "Yi-Zhe Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Generalized_Zero-Shot_Classification_via_Semantics-Free_Inter-Class_Feature_Generation_CVPR_2025_paper.html": {
    "title": "Generalized Zero-Shot Classification via Semantics-Free Inter-Class Feature Generation",
    "volume": "main",
    "abstract": "Generalized Zero-Shot Learning (GZSL) addresses the challenge of classifying unseen classes in the presence of seen classes by leveraging semantic attributes to bridge the gap for unseen classes. However, in image based disease classification, such as glioma sub-typing, distinguishing between classes using image semantic attributes can be challenging. To address this challenge, we introduce a novel GZSL method that eliminates the dependency on semantic information. Specifically, we propose that the primary of most classification in clinic is risk stratification, and classes are inherently ordered rather than purely categorical. Based on this insight, we present an inter-class feature augmentation (IFA) module, where distributions of different classes are ordered by their risk levels in a learned feature space using pre-defined joint conditional Gaussian distribution model. This ordering enables the generation of unseen class features through feature mixing of adjacent seen classes, effectively transforming the zero-shot learning problem into a supervised learning task. Our method eliminates the need for explicit semantic information, avoiding the cross-modal alignment between visual and semantic features. Moreover, the IFA module for GZSL requires no structural modifications to the existing classification models. In the experiment, both in-house and public datasets are used to evaluate our method across different tasks, including glioma subtyping, Alzheimer's disease (AD) classification and diabetic retinopathy classification. Experimental results demonstrate that our method outperforms the state-of-the-art GZSL methods with statistical significance",
    "checked": false,
    "id": "bfe41c89aa91358126823d22cca98f111ba42fec",
    "semantic_title": "data-free generalized zero-shot learning",
    "citation_count": 11,
    "authors": [
      "Libiao Chen",
      "Dong Nie",
      "Junjun Pan",
      "Jing Yan",
      "Zhenyu Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Feat2GS_Probing_Visual_Foundation_Models_with_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "Feat2GS: Probing Visual Foundation Models with Gaussian Splatting",
    "volume": "main",
    "abstract": "Given that visual foundation models (VFMs) are trained on extensive datasets but often limited to 2D images, a natural question arises: how well do they understand the 3D world? With the differences in architecture and training protocols (i.e., objectives, proxy tasks), a unified framework to fairly and comprehensively probe their 3D awareness is urgently needed. Existing works on 3D probing suggest single-view 2.5D estimation (e.g., depth and normal) or two-view sparse 2D correspondence (e.g., matching and tracking). Unfortunately, these tasks ignore texture awareness, and require 3D data as ground-truth, which limits the scale and diversity of their evaluation set. To address these issues, we introduce Feat2GS, which readout 3D Gaussians attributes from VFM features extracted from unposed images. This allows us to probe 3D awareness for geometry and texture via novel view synthesis, without requiring 3D data. Additionally, the disentanglement of 3DGS parameters - geometry (x, a, S) and texture (c) - enables separate analysis of texture and geometry awareness. Under Feat2GS, we conduct extensive experiments to probe the 3D awareness of several VFMs, and investigate the ingredients that lead to a 3D aware VFM. Building on these findings, we develop several variants that achieve state-of-the-art across diverse datasets. This makes Feat2GS useful for probing VFMs, and as a simple-yet-effective baseline for novel-view synthesis. Code and data will be made available at fanegg.github.io/Feat2GS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Chen",
      "Xingyu Chen",
      "Anpei Chen",
      "Gerard Pons-Moll",
      "Yuliang Xiu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Multi-Modal_Aerial-Ground_Cross-View_Place_Recognition_with_Neural_ODEs_CVPR_2025_paper.html": {
    "title": "Multi-Modal Aerial-Ground Cross-View Place Recognition with Neural ODEs",
    "volume": "main",
    "abstract": "Place recognition (PR) aims at retrieving the query place from a database and plays a crucial role in various applications, including navigation, autonomous driving, and augmented reality. While previous multi-modal PR works have mainly focused on the same-view scenario in which ground-view descriptors are matched with a database of ground-view descriptors during inference, the multi-modal cross-view scenario, in which ground-view descriptors are matched with aerial-view descriptors in a database, remains under-explored. We propose AGPlace, a model that effectively integrates information from multi-modal ground sensors (cameras and LiDARs) to achieve accurate aerial-ground PR. AGPlace achieves effective aerial-ground cross-view PR by leveraging a manifold-based neural ordinary differential equation (ODE) framework with a multi-domain alignment loss. It outperforms existing state-of-the-art cross-view PR models on large-scale datasets. As most existing PR models are designed for ground-ground PR, we adapt these baselines into our cross-view pipeline. Experiments demonstrate that this direct adaptation performs worse than our overall model architecture AGPlace. AGPlace represents a significant advancement in multi-modal aerial-ground PR, with promising implications for real-world applications",
    "checked": false,
    "id": "e0dcca0175d7b8a1fe5a290aed84255ee48d49dd",
    "semantic_title": "visual, spatial, geometric-preserved place recognition for cross-view and cross-modal collaborative perception",
    "citation_count": 3,
    "authors": [
      "Sijie Wang",
      "Rui She",
      "Qiyu Kang",
      "Siqi Li",
      "Disheng Li",
      "Tianyu Geng",
      "Shangshu Yu",
      "Wee Peng Tay"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wazir_Rethinking_Decoder_Design_Improving_Biomarker_Segmentation_Using_Depth-to-Space_Restoration_and_CVPR_2025_paper.html": {
    "title": "Rethinking Decoder Design: Improving Biomarker Segmentation Using Depth-to-Space Restoration and Residual Linear Attention",
    "volume": "main",
    "abstract": "Segmenting biomarkers in medical images is crucial for various biotech applications. Despite advances, Transformer and CNN based methods often struggle with variations in staining and morphology, limiting feature extraction. In medical image segmentation, where datasets often have limited sample availability, recent state-of-the-art (SOTA) methods achieve higher accuracy by leveraging pre-trained encoders, whereas end-to-end methods tend to underperform. This is due to challenges in effectively transferring rich multiscale features from encoders to decoders, as well as limitations in decoder efficiency. To address these issues, we propose an architecture that captures multi-scale local and global contextual information and a novel decoder design, which effectively integrates features from the encoder, emphasizes important channels and regions, and reconstructs spatial dimensions to enhance segmentation accuracy. Our method, compatible with various encoders, outperforms SOTA methods, as demonstrated by experiments on four datasets and ablation studies. Specifically, our method achieves absolute performance gains of 2.76% on MoNuSeg, 3.12% on DSB, 2.87% on Electron Microscopy, and 4.03% on TNBC datasets compared to existing SOTA methods. Code: https://github.com/saadwazir/MCADS-Decoder",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saad Wazir",
      "Daeyoung Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_MaDCoW_Marginal_Distortion_Correction_for_Wide-Angle_Photography_with_Arbitrary_Objects_CVPR_2025_paper.html": {
    "title": "MaDCoW: Marginal Distortion Correction for Wide-Angle Photography with Arbitrary Objects",
    "volume": "main",
    "abstract": "We introduce MaDCoW, a method for correcting marginal distortion of arbitrary objects in wide-angle photography. People often use wide-angle photography to convey natural scenes--smartphones typically default to wide-angle photography--but depicting very wide-field-of-view scenes produces distorted object appearance, particularly marginal distortion in linear projections. With MaDCoW, a user annotates regions-of-interest to correct, along with straight lines. For each region, MaDCoW solves for a local-linear perspective projection and then jointly solves for a projection for the whole photograph that minimizes distortion. We show that our method can produce good results in cases where previous methods yield visible distortions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Zhang",
      "Jia-Bin Huang",
      "Jose Echevarria",
      "Stephen DiVerdi",
      "Aaron Hertzmann"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_SynTab-LLaVA_Enhancing_Multimodal_Table_Understanding_with_Decoupled_Synthesis_CVPR_2025_paper.html": {
    "title": "SynTab-LLaVA: Enhancing Multimodal Table Understanding with Decoupled Synthesis",
    "volume": "main",
    "abstract": "Due to the limited scale of multimodal table understanding (MTU) data, model performance is constrained. A straightforward approach is to use multimodal large language models to obtain more samples, but this may cause hallucinations, generate incorrect sample pairs, and cost significantly.To address the above issues, we design a simple yet effective synthesis framework that consists of two independent steps: table image rendering and table question and answer (Q&A) pairs generation.We use table codes (HTML, LaTeX, Markdown) to synthesize images and generate Q&A pairs with large language model (LLM).This approach leverages LLM's high concurrency and low cost to boost annotation efficiency and reduce expenses. By inputting code instead of images, LLMs can directly access the content and structure of the table, reducing hallucinations in table understanding and improving the accuracy of generated Q&A pairs. Finally, we synthesize a large-scale MTU dataset, SynTab, containing 636K images and 1.8M samples costing within \\200 in US dollars. We further introduce a generalist tabular multimodal model, SynTab-LLaVA. This model not only effectively extracts local textual content within the table but also enables global modeling of relationships between cells.SynTab-LLaVA achieves SOTA performance on 21 out of 24 in-domain and out-of-domain benchmarks, demonstrating the effectiveness and generalization of our method. The Code is available at \\href https://github.com/bang123-box/SynTab-LLaVA SynTab-LLaVA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bangbang Zhou",
      "Zuan Gao",
      "Zixiao Wang",
      "Boqiang Zhang",
      "Yuxin Wang",
      "Zhineng Chen",
      "Hongtao Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Edit_Away_and_My_Face_Will_not_Stay_Personal_Biometric_CVPR_2025_paper.html": {
    "title": "Edit Away and My Face Will not Stay: Personal Biometric Defense against Malicious Generative Editing",
    "volume": "main",
    "abstract": "Recent advancements in diffusion models have made generative image editing more accessible than ever. While these developments allow users to generate creative edits with ease, they also raise significant ethical concerns, particularly regarding malicious edits to human portraits that threaten individuals' privacy and identity security. Existing general-purpose image protection methods primarily focus on generating adversarial perturbations to nullify edit effects. However, these approaches often exhibit instability to protect against diverse editing requests. In this work, we introduce a novel perspective to personal human portrait protection against malicious editing. Unlike traditional methods aiming to prevent edits from taking effect, our method, FaceLock, optimizes adversarial perturbations to ensure that original biometric information---such as facial features---is either destroyed or substantially altered post-editing, rendering the subject in the edited output biometrically unrecognizable. Our approach innovatively integrates facial recognition and visual perception factors into the perturbation optimization process, ensuring robust protection against a variety of editing attempts. Besides, we shed light on several critical issues with commonly used evaluation metrics in image editing and reveal cheating methods by which they can be easily manipulated, leading to deceptive assessments of protection. Through extensive experiments, we demonstrate that FaceLock significantly outperforms all baselines in defense performance against a wide range of malicious edits. Moreover, our method also exhibits strong robustness against purification techniques. Comprehensive ablation studies confirm the stability and broad applicability of our method across diverse diffusion-based editing algorithms. Our work not only advances the state-of-the-art in biometric defense but also sets the foundation for more secure and privacy-preserving practices in image editing",
    "checked": true,
    "id": "66167d826908a02bc675f49fba5165ccb05690a0",
    "semantic_title": "edit away and my face will not stay: personal biometric defense against malicious generative editing",
    "citation_count": 2,
    "authors": [
      "Hanhui Wang",
      "Yihua Zhang",
      "Ruizheng Bai",
      "Yue Zhao",
      "Sijia Liu",
      "Zhengzhong Tu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Any6D_Model-free_6D_Pose_Estimation_of_Novel_Objects_CVPR_2025_paper.html": {
    "title": "Any6D: Model-free 6D Pose Estimation of Novel Objects",
    "volume": "main",
    "abstract": "We introduce Any6D, a model-free framework for 6D object pose estimation that requires only a single RGB-D anchor image to estimate both the 6D pose and size of unknown objects in novel scenes. Unlike existing methods that rely on textured 3D models or multiple viewpoints, Any6D leverages a joint object alignment process to enhance 2D-3D alignment and metric scale estimation for improved pose accuracy. Our approach integrates a render-and-compare strategy to generate and refine pose hypotheses, enabling robust performance in scenarios with occlusions, non-overlapping views, diverse lighting conditions, and large cross-environment variations. We evaluate our method on five challenging datasets: REAL275, Toyota-Light, HO3D, YCBINEOAT, and LM-O, demonstrating its effectiveness in significantly outperforming state-of-the-art methods for novel object pose estimation. Project page: https://taeyeop.com/any6d",
    "checked": true,
    "id": "a08ea515555253c7791a2b00f358c7caf6dec8d0",
    "semantic_title": "any6d: model-free 6d pose estimation of novel objects",
    "citation_count": 2,
    "authors": [
      "Taeyeop Lee",
      "Bowen Wen",
      "Minjun Kang",
      "Gyuree Kang",
      "In So Kweon",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Improving_Accuracy_and_Calibration_via_Differentiated_Deep_Mutual_Learning_CVPR_2025_paper.html": {
    "title": "Improving Accuracy and Calibration via Differentiated Deep Mutual Learning",
    "volume": "main",
    "abstract": "Deep Neural Networks (DNNs) have achieved remarkable success in a variety of tasks, particularly in terms of prediction accuracy. However, in real-world scenarios, especially in safety-critical applications, accuracy alone is insufficient; reliable uncertainty estimates are essential. Modern DNNs, often trained with cross-entropy loss, tend to exhibit overconfidence, especially on ambiguous samples. Many techniques aim to improve uncertainty calibration, yet they often come at the cost of reduced accuracy or increased computational demands. To address this challenge, we propose Differentiated Deep Mutual Learning (Diff-DML), an efficient ensemble approach that simultaneously enhances accuracy and uncertainty calibration. Diff-DML draws inspiration from Deep Mutual Learning (DML) while introducing two strategies to maintain prediction diversity: (1) Differentiated Training Strategy (DTS) and (2) Diversity-Preserving Learning Objective (DPLO). Our theoretical analysis shows that Diff-DML's diversified learning framework not only leverages ensemble benefits but also avoids the loss of prediction diversity observed in traditional DML setups, which is crucial for improved calibration. Extensive evaluations on various benchmarks confirm the effectiveness of Diff-DML. For instance, on the CIFAR-100 dataset, Diff-DML on ResNet34 model achieved substantial improvements over the previous state-of-the-art method, MDCA, with absolute accuracy gains of 1.3%/3.1%, relative ECE reductions of 49.6%/43.8%, and relative classwise-ECE reductions of 7.7%/13.0%",
    "checked": false,
    "id": "b0472ef75cd1a81b6d4e83f0b8224b1cef43c244",
    "semantic_title": "mri-based deep learning model for differentiation of hepatic hemangioma and hepatoblastoma in early infancy",
    "citation_count": 3,
    "authors": [
      "Han Liu",
      "Peng Cui",
      "Bingning Wang",
      "Weipeng Chen",
      "Yupeng Zhang",
      "Jun Zhu",
      "Xiaolin Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_DrVideo_Document_Retrieval_Based_Long_Video_Understanding_CVPR_2025_paper.html": {
    "title": "DrVideo: Document Retrieval Based Long Video Understanding",
    "volume": "main",
    "abstract": "Most of the existing methods for video understanding primarily focus on videos only lasting tens of seconds, with limited exploration of techniques for handling long videos. The increased number of frames in long videos poses two main challenges: difficulty in locating key information and performing long-range reasoning. Thus, we propose DrVideo, a document-retrieval-based system designed for long video understanding. Our key idea is to convert the long-video understanding problem into a long-document understanding task so as to effectively leverage the power of large language models. Specifically, DrVideo first transforms a long video into a coarse text-based long document to initially retrieve key frames and then updates the documents with the augmented key frame information. It then employs an agent-based iterative loop to continuously search for missing information and augment the document until sufficient question-related information is gathered for making the final predictions in a chain-of-thought manner. Extensive experiments on long video benchmarks confirm the effectiveness of our method. DrVideo significantly outperforms existing LLM-based state-of-the-art methods on EgoSchema benchmark (3 minutes), MovieChat-1K benchmark (10 minutes), and the long split of Video-MME benchmark (average of 44 minutes). Code is available at https://github.com/Upper9527/DrVideo",
    "checked": true,
    "id": "61a521d47d2ea646a447d50f6a6a64ada8ea62ea",
    "semantic_title": "drvideo: document retrieval based long video understanding",
    "citation_count": 15,
    "authors": [
      "Ziyu Ma",
      "Chenhui Gou",
      "Hengcan Shi",
      "Bin Sun",
      "Shutao Li",
      "Hamid Rezatofighi",
      "Jianfei Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Infighting_in_the_Dark_Multi-Label_Backdoor_Attack_in_Federated_Learning_CVPR_2025_paper.html": {
    "title": "Infighting in the Dark: Multi-Label Backdoor Attack in Federated Learning",
    "volume": "main",
    "abstract": "Federated Learning (FL), a privacy-preserving decentralized machine learning framework, has been shown to be vulnerable to backdoor attacks. Current research primarily focuses on the Single-Label Backdoor Attack (SBA), wherein adversaries share a consistent target. However, a critical fact is overlooked: adversaries may be non-cooperative, have distinct targets, and operate independently, which exhibits a more practical scenario called Multi-Label Backdoor Attack (MBA). Unfortunately, prior works are ineffective in the MBA scenario since non-cooperative attackers exclude each other. In this work, we conduct an in-depth investigation to uncover the inherent constraints of the exclusion: similar backdoor mappings are constructed for different targets, resulting in conflicts among backdoor functions. To address this limitation, we propose Mirage, the first non-cooperative MBA strategy in FL that allows attackers to inject effective and persistent backdoors into the global model without collusion by constructing in-distribution (ID) backdoor mapping. Specifically, we introduce an adversarial adaptation method to bridge the backdoor features and the target distribution in an ID manner. Additionally, we further leverage a constrained optimization method to ensure the ID mapping survives in the global training dynamics. Extensive evaluations demonstrate that Mirage outperforms various state-of-the-art attacks and bypasses existing defenses, achieving an average ASR greater than 97% and maintaining over 90% after 900 rounds. This work aims to alert researchers to this potential threat and inspire the design of effective defense mechanisms. Code has been made open-source",
    "checked": false,
    "id": "9efcff730248c2fb843bb966fe8f07923eaec018",
    "semantic_title": "infighting in the dark: multi-labels backdoor attack in federated learning",
    "citation_count": 0,
    "authors": [
      "Ye Li",
      "Yanchao Zhao",
      "Chengcheng Zhu",
      "Jiale Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kuang_Buffer_Anytime_Zero-Shot_Video_Depth_and_Normal_from_Image_Priors_CVPR_2025_paper.html": {
    "title": "Buffer Anytime: Zero-Shot Video Depth and Normal from Image Priors",
    "volume": "main",
    "abstract": "We present Buffer Anytime, a framework for estimation of depth and normal maps (which we call geometric buffers) from video that eliminates the need for paired video--depth and video--normal training data. Instead of relying on large-scale annotated video datasets, we demonstrate high-quality video buffer estimation by leveraging single-image priors with temporal consistency constraints. Our zero-shot training strategy combines state-of-the-art image estimation models based on optical flow smoothness through a hybrid loss function, implemented via a lightweight temporal attention architecture. Applied to leading image models like Depth Anything V2 and Marigold-E2E-FT, our approach significantly improves temporal consistency while maintaining accuracy. Experiments show that our method not only outperforms image-based approaches but also achieves results comparable to state-of-the-art video models trained on large-scale paired video datasets, despite using no such paired video data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengfei Kuang",
      "Tianyuan Zhang",
      "Kai Zhang",
      "Hao Tan",
      "Sai Bi",
      "Yiwei Hu",
      "Zexiang Xu",
      "Milos Hasan",
      "Gordon Wetzstein",
      "Fujun Luan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_PSHuman_Photorealistic_Single-image_3D_Human_Reconstruction_using_Cross-Scale_Multiview_Diffusion_CVPR_2025_paper.html": {
    "title": "PSHuman: Photorealistic Single-image 3D Human Reconstruction using Cross-Scale Multiview Diffusion and Explicit Remeshing",
    "volume": "main",
    "abstract": "Photorealistic 3D human modeling is essential for various applications and has seen tremendous progress. However, existing methods for monocular full-body reconstruction, typically relying on front and/or predicted back view, still struggle with satisfactory performance due to the ill-posed nature of the problem and sophisticated self-occlusions. In this paper, we propose PSHuman, a novel framework that explicitly reconstructs human meshes utilizing priors from the multiview diffusion model. It is found that directly applying multiview diffusion on single-view human images leads to severe geometric distortions, especially on generated faces. To address it, we propose a cross-scale diffusion that models the joint probability distribution of global full-body shape and local facial characteristics, enabling identity-preserved novel-view generation without geometric distortion. Moreover, to enhance cross-view body shape consistency of varied human poses, we condition the generative model on parametric models (SMPL-X), which provide body priors and prevent unnatural views inconsistent with human anatomy. Leveraging the generated multiview normal and color images, we present SMPLX-initialized explicit human carving to recover realistic textured human meshes efficiently. Extensive experiments on CAPE and THuman2.1 demonstrate PSHuman's superiority in geometry details, texture fidelity, and generalization capability",
    "checked": true,
    "id": "28b9c8cfd17159f925fb30acd8fff6269c76f60e",
    "semantic_title": "pshuman: photorealistic single-image 3d human reconstruction using cross-scale multiview diffusion and explicit remeshing",
    "citation_count": 5,
    "authors": [
      "Peng Li",
      "Wangguandong Zheng",
      "Yuan Liu",
      "Tao Yu",
      "Yangguang Li",
      "Xingqun Qi",
      "Xiaowei Chi",
      "Siyu Xia",
      "Yan-Pei Cao",
      "Wei Xue",
      "Wenhan Luo",
      "Yike Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_LSNet_See_Large_Focus_Small_CVPR_2025_paper.html": {
    "title": "LSNet: See Large, Focus Small",
    "volume": "main",
    "abstract": "Vision network designs, including Convolutional Neural Networks and Vision Transformers, have significantly advanced the field of computer vision. Yet, their complex computations pose challenges for practical deployments, particularly in real-time applications. To tackle this issue, researchers have explored various lightweight and efficient network designs. However, existing lightweight models predominantly leverage self-attention mechanisms and convolutions for token mixing. This dependence brings limitations in effectiveness and efficiency in the perception and aggregation processes of lightweight networks, hindering the balance between performance and efficiency under limited computational budgets. In this paper, we draw inspiration from the dynamic heteroscale vision ability inherent in the efficient human vision system and propose a \"See Large, Focus Small\" strategy for lightweight vision network design. We introduce LS (Large-Small) convolution, which combines large-kernel perception and small-kernel aggregation. It can efficiently capture a wide range of perceptual information and achieve precise feature aggregation for dynamic and complex visual representations, thus enabling proficient processing of visual information. Based on LS convolution, we present LSNet, a new family of lightweight models. Extensive experiments demonstrate that LSNet achieves superior performance and efficiency over existing lightweight networks in various vision tasks. Codes and models are available at https://github.com/jameslahm/lsnet",
    "checked": true,
    "id": "64bdc3bf4cd9a7f89274d1c6a1bde74cf14ed5bc",
    "semantic_title": "lsnet: see large, focus small",
    "citation_count": 0,
    "authors": [
      "Ao Wang",
      "Hui Chen",
      "Zijia Lin",
      "Jungong Han",
      "Guiguang Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_DynamicScaler_Seamless_and_Scalable_Video_Generation_for_Panoramic_Scenes_CVPR_2025_paper.html": {
    "title": "DynamicScaler: Seamless and Scalable Video Generation for Panoramic Scenes",
    "volume": "main",
    "abstract": "The increasing demand for immersive AR/VR applications and spatial intelligence has heightened the need to generate high-quality scene-level and 360deg panoramic video. However, most video diffusion models are constrained by limited resolution and aspect ratio, which restricts their applicability to scene-level dynamic content synthesis. In this work, we propose DynamicScaler, addressing these challenges by enabling spatially scalable and panoramic dynamic scene synthesis that preserves coherence across panoramic scenes of arbitrary size. Specifically, we introduce a Offset Shifting Denoiser, facilitating efficient, synchronous, and coherent denoising panoramic dynamic scenes via a diffusion model with fixed resolution through a seamless rotating Window, which ensures seamless boundary transitions and consistency across the entire panoramic space, accommodating varying resolutions and aspect ratios. Additionally, we employ a Global Motion Guidance mechanism to ensure both local detail fidelity and global motion continuity. Extensive experiments demonstrate our method achieves superior content and motion quality in panoramic scene-level video generation, offering a training-free, efficient, and scalable solution for immersive dynamic scene creation with constant VRAM consumption regardless of the output video resolution. Project page is available at https://dynamic-scaler.pages.dev/new",
    "checked": true,
    "id": "ef6201bef3feffada2b2fa2bc1f0a465429582d6",
    "semantic_title": "dynamicscaler: seamless and scalable video generation for panoramic scenes",
    "citation_count": 4,
    "authors": [
      "Jinxiu Liu",
      "Shaoheng Lin",
      "Yinxiao Li",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Tartan_IMU_A_Light_Foundation_Model_for_Inertial_Positioning_in_CVPR_2025_paper.html": {
    "title": "Tartan IMU: A Light Foundation Model for Inertial Positioning in Robotics",
    "volume": "main",
    "abstract": "Despite recent advances in deep learning, most existing learning IMU odometry methods are trained on specific datasets, lack generalization, and are prone to overfitting, which limits their real-world application. To address these challenges, we present Tartan IMU, a foundation model designed for generalizable, IMU-based state estimation across diverse robotic platforms. Our approach consists of three-stage: First, a pre-trained foundation model leverages over 100 hours of multi-platform data to establish general motion knowledge, achieving 36% improvement in ATE over specialized models. Second, to adapt to previously unseen tasks, we employ the Low-Rank Adaptation (LoRA), allowing positive transfer with only 1.1 M trainable parameters. Finally, to support robotics deployment, we introduce online test-time adaptation, which eliminates the boundary between training and testing, allowing the model to continuously \"learn as it operates\" at 200 FPS in real-time",
    "checked": true,
    "id": "cfda737f0a7de84151a0d7d1e30d6b29be91cc3c",
    "semantic_title": "tartan imu: a light foundation model for inertial positioning in robotics",
    "citation_count": 2,
    "authors": [
      "Shibo Zhao",
      "Sifan Zhou",
      "Raphael Blanchard",
      "Yuheng Qiu",
      "Wenshan Wang",
      "Sebastian Scherer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging_CVPR_2025_paper.html": {
    "title": "Event Ellipsometer: Event-based Mueller-Matrix Video Imaging",
    "volume": "main",
    "abstract": "Light-matter interactions modify both the intensity and polarization state of light. Changes in polarization, represented by a Mueller matrix, encode detailed scene information. Existing optical ellipsometers capture Mueller-matrix images; however, they are often limited to static scenes due to long acquisition times. Here, we introduce Event Ellipsometer, a method for acquiring Mueller-matrix images of dynamic scenes. Our imaging system employs fast-rotating quarter-wave plates (QWPs) in front of a light source and an event camera that asynchronously captures intensity changes induced by the rotating QWPs. We develop an ellipsometric-event image formation model, a calibration method, and an ellipsometric-event reconstruction method. We experimentally demonstrate that Event Ellipsometer enables Mueller-matrix imaging at 30fps, extending ellipsometry to dynamic scenes",
    "checked": true,
    "id": "c2219496f43b4ced443c11f827000170bf989e5b",
    "semantic_title": "event ellipsometer: event-based mueller-matrix video imaging",
    "citation_count": 0,
    "authors": [
      "Ryota Maeda",
      "Yunseong Moon",
      "Seung-Hwan Baek"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liao_DocLayLLM_An_Efficient_Multi-modal_Extension_of_Large_Language_Models_for_CVPR_2025_paper.html": {
    "title": "DocLayLLM: An Efficient Multi-modal Extension of Large Language Models for Text-rich Document Understanding",
    "volume": "main",
    "abstract": "Text-rich document understanding (TDU) requires comprehensive analysis of documents containing substantial textual content and complex layouts. While Multimodal Large Language Models (MLLMs) have achieved fast progress in this domain, existing approaches either demand significant computational resources or struggle with effective multi-modal integration. In this paper, we introduce DocLayLLM, an efficient multi-modal extension of LLMs specifically designed for TDU. By lightly integrating visual patch tokens and 2D positional tokens into LLMs' input and encoding the document content using the LLMs themselves, we fully take advantage of the document comprehension capability of LLMs and enhance their perception of OCR information. We have also deeply considered the role of chain-of-thought (CoT) and innovatively proposed the techniques of CoT Pre-training and CoT Annealing. Our DocLayLLM can achieve remarkable performances with lightweight training settings, showcasing its efficiency and effectiveness. Experimental results demonstrate that our DocLayLLM outperforms existing OCR-dependent methods and OCR-free competitors. Code and model are available at https://github.com/whlscut/DocLayLLM",
    "checked": true,
    "id": "7cc132254e9d6bd2049383be4890b58be36954a8",
    "semantic_title": "doclayllm: an efficient multi-modal extension of large language models for text-rich document understanding",
    "citation_count": 0,
    "authors": [
      "Wenhui Liao",
      "Jiapeng Wang",
      "Hongliang Li",
      "Chengyu Wang",
      "Jun Huang",
      "Lianwen Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_EDEN_Enhanced_Diffusion_for_High-quality_Large-motion_Video_Frame_Interpolation_CVPR_2025_paper.html": {
    "title": "EDEN: Enhanced Diffusion for High-quality Large-motion Video Frame Interpolation",
    "volume": "main",
    "abstract": "Handling complex or nonlinear motion patterns has long posed challenges for video frame interpolation. Although recent advances in diffusion-based methods offer improvements over traditional optical flow-based approaches, they still struggle to generate sharp, temporally consistent frames in scenarios with large motion. To address this limitation, we introduce EDEN, an Enhanced Diffusion for high-quality large-motion vidEo frame iNterpolation. Our approach first utilizes a transformer-based tokenizer to produce refined latent representations of the intermediate frames for diffusion models. We then enhance the diffusion transformer with temporal attention across the process and incorporate a start-end frame difference embedding to guide the generation of dynamic motion. Extensive experiments demonstrate that EDEN achieves state-of-the-art results across popular benchmarks, including nearly a 10% LPIPS reduction on DAVIS and SNU-FILM, and an 8% improvement on DAIN-HD",
    "checked": true,
    "id": "8d1e463df564ebeaef915d35e7c927384b437f56",
    "semantic_title": "eden: enhanced diffusion for high-quality large-motion video frame interpolation",
    "citation_count": 2,
    "authors": [
      "Zihao Zhang",
      "Haoran Chen",
      "Haoyu Zhao",
      "Guansong Lu",
      "Yanwei Fu",
      "Hang Xu",
      "Zuxuan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Handling_Spatial-Temporal_Data_Heterogeneity_for_Federated_Continual_Learning_via_Tail_CVPR_2025_paper.html": {
    "title": "Handling Spatial-Temporal Data Heterogeneity for Federated Continual Learning via Tail Anchor",
    "volume": "main",
    "abstract": "Federated Continual Learning (FCL) allows each client to continually update its knowledge from task streams, enhancing the applicability of federated learning in real-world scenarios. However, FCL needs to address not only spatial data heterogeneity between clients but also temporal data heterogeneity between tasks. In this paper, empirical experiments demonstrate that such input-level heterogeneity significantly affects the model's internal parameters and outputs, leading to severe spatial-temporal catastrophic forgetting of local and previous knowledge. To this end, we propose Federated Tail Anchor (FedTA) to mix trainable Tail Anchor with the frozen output features to adjust their position in the feature space, thereby overcoming parameter-forgetting and output-forgetting. Three novel components are also included: Input Enhancement for improving the performance of pre-trained models on downstream tasks; Selective Input Knowledge Fusion for fusion of heterogeneous local knowledge on the server; and Best Global Prototype Selection for finding the best anchor point for each class in the feature space. Extensive experiments demonstrate that FedTA not only outperforms existing FCL methods but also effectively preserves the relative positions of features",
    "checked": true,
    "id": "2e30d26595cea3248beeb2a7210c563f6cb8d94a",
    "semantic_title": "handling spatial-temporal data heterogeneity for federated continual learning via tail anchor",
    "citation_count": 1,
    "authors": [
      "Hao Yu",
      "Xin Yang",
      "Le Zhang",
      "Hanlin Gu",
      "Tianrui Li",
      "Lixin Fan",
      "Qiang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peng_DeSiRe-GS_4D_Street_Gaussians_for_Static-Dynamic_Decomposition_and_Surface_Reconstruction_CVPR_2025_paper.html": {
    "title": "DeSiRe-GS: 4D Street Gaussians for Static-Dynamic Decomposition and Surface Reconstruction for Urban Driving Scenes",
    "volume": "main",
    "abstract": "We present DeSiRe-GS, a self-supervised gaussian splatting representation, enabling effective static-dynamic decomposition and high-fidelity surface reconstruction in complex driving scenarios. Our approach employs a two-stage optimization pipeline of dynamic street Gaussians. In the first stage, we extract 2D motion masks based on the observation that 3D Gaussian Splatting inherently can reconstruct only the static regions in dynamic environments. These extracted 2D motion priors are then mapped into the Gaussian space in a differentiable manner, leveraging an efficient formulation of dynamic Gaussians in the second stage. Combined with the introduced geometric regularizations, our method are able to address the over-fitting issues caused by data sparsity in autonomous driving, reconstructing physically plausible Gaussians that align with object surfaces rather than floating in air. Furthermore, we introduce temporal cross-view consistency to ensure coherence across time and viewpoints, resulting in high-quality surface reconstruction. Comprehensive experiments demonstrate the efficiency and effectiveness of DeSiRe-GS, surpassing prior self-supervised arts and achieving accuracy comparable to methods relying on external 3D bounding box annotations",
    "checked": true,
    "id": "82d74ad33892c0dd61dcc94b815cb6eb20cbf48e",
    "semantic_title": "desire-gs: 4d street gaussians for static-dynamic decomposition and surface reconstruction for urban driving scenes",
    "citation_count": 6,
    "authors": [
      "Chensheng Peng",
      "Chengwei Zhang",
      "Yixiao Wang",
      "Chenfeng Xu",
      "Yichen Xie",
      "Wenzhao Zheng",
      "Kurt Keutzer",
      "Masayoshi Tomizuka",
      "Wei Zhan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding_CVPR_2025_paper.html": {
    "title": "End-to-End HOI Reconstruction Transformer with Graph-based Encoding",
    "volume": "main",
    "abstract": "Human-object interaction (HOI) reconstruction has garnered significant attention due to its diverse applications and the success of capturing human meshes. Existing HOI reconstruction methods often rely on explicitly modeling interactions between humans and objects. However, such a way leads to a natural conflict between 3D mesh reconstruction, which emphasizes global structure, and fine-grained contact reconstruction, which focuses on local details. To address the limitations of explicit modeling, we propose the End-to-End HOI Reconstruction Transformer with Graph-based Encoding (HOI-TG). It implicitly learns the interaction between humans and objects by leveraging self-attention mechanisms. Within the transformer architecture, we devise graph residual blocks to aggregate the topology among vertices of different spatial structures. This dual focus effectively balances global and local representations. Without bells and whistles, HOI-TG achieves state-of-the-art performance on BEHAVE and InterCap datasets. Particularly on the challenging InterCap dataset, our method improves the reconstruction results for human and object meshes by 8.9% and 8.6%, respectively",
    "checked": true,
    "id": "07f18a8165615d8b72fde83ab914d3bb2d978a7a",
    "semantic_title": "end-to-end hoi reconstruction transformer with graph-based encoding",
    "citation_count": 0,
    "authors": [
      "Zhenrong Wang",
      "Qi Zheng",
      "Sihan Ma",
      "Maosheng Ye",
      "Yibing Zhan",
      "Dongjiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_REWIND_Real-Time_Egocentric_Whole-Body_Motion_Diffusion_with_Exemplar-Based_Identity_Conditioning_CVPR_2025_paper.html": {
    "title": "REWIND: Real-Time Egocentric Whole-Body Motion Diffusion with Exemplar-Based Identity Conditioning",
    "volume": "main",
    "abstract": "We present REWIND (Real-Time Egocentric Whole-Body Motion Diffusion), a one-step diffusion model for real-time, high-fidelity human motion estimation from egocentric image inputs. While an existing method for egocentric whole-body (i.e., body and hands) motion estimation is non-real-time and acausal due to diffusion-based iterative motion refinement to capture correlations between body and hand poses, REWIND operates in a fully causal and real-time manner. To enable real-time inference, we introduce (1) cascaded body-hand denoising diffusion, which effectively models the correlation between egocentric body and hand motions in a fast, feed-forward manner, and (2) diffusion distillation, which enables high-quality motion estimation with a single denoising step. Our denoising diffusion model is based on a modified Transformer architecture, designed to causally model output motions while enhancing generalizability to unseen motion lengths. Additionally, REWIND optionally supports identity-conditioned motion estimation when identity prior is available. To this end, we propose a novel identity conditioning method based on a small set of pose exemplars of the target identity, which further enhances motion estimation quality. Through extensive experiments, we demonstrate that REWIND significantly outperforms the existing baselines both with and without exemplar-based identity conditioning",
    "checked": true,
    "id": "56c639ab6894573e7adcfe5f7a1b36c0ea9c812c",
    "semantic_title": "rewind: real-time egocentric whole-body motion diffusion with exemplar-based identity conditioning",
    "citation_count": 0,
    "authors": [
      "Jihyun Lee",
      "Weipeng Xu",
      "Alexander Richard",
      "Shih-En Wei",
      "Shunsuke Saito",
      "Shaojie Bai",
      "Te-Li Wang",
      "Minhyuk Sung",
      "Tae-Kyun Kim",
      "Jason Saragih"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Hiding_Images_in_Diffusion_Models_by_Editing_Learned_Score_Functions_CVPR_2025_paper.html": {
    "title": "Hiding Images in Diffusion Models by Editing Learned Score Functions",
    "volume": "main",
    "abstract": "Hiding data using neural networks (i.e., neural steganography) has achieved remarkable success across both discriminative classifiers and generative adversarial networks. However, the potential of data hiding in diffusion models remains relatively unexplored. Current methods exhibit limitations in achieving high extraction accuracy, model fidelity, and hiding efficiency due primarily to the entanglement of the hiding and extraction processes with multiple denoising diffusion steps. To address these, we describe a simple yet effective approach that embeds images at specific timesteps in the reverse diffusion process by editing the learned score functions. Additionally, we introduce a parameter-efficient fine-tuning method that combines gradient-based parameter selection with low-rank adaptation to enhance model fidelity and hiding efficiency. Comprehensive experiments demonstrate that our method extracts high-quality images at human-indistinguishable levels, replicates the original model behaviors at both sample and population levels, and embeds images orders of magnitude faster than prior methods. Besides, our method naturally supports multi-recipient scenarios through independent extraction channels",
    "checked": true,
    "id": "0488570d7d30f43ac4d4ce88ce8fc3887e20fd43",
    "semantic_title": "hiding images in diffusion models by editing learned score functions",
    "citation_count": 0,
    "authors": [
      "Haoyu Chen",
      "Yunqiao Yang",
      "Nan Zhong",
      "Kede Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pang_Disco4D_Disentangled_4D_Human_Generation_and_Animation_from_a_Single_CVPR_2025_paper.html": {
    "title": "Disco4D: Disentangled 4D Human Generation and Animation from a Single Image",
    "volume": "main",
    "abstract": "We present Disco4D, a novel Gaussian Splatting framework for 4D human generation and animation from a single image. Different from existing methods, Disco4D distinctively disentangles clothings (with Gaussian models) from the human body (with SMPL-X model), significantly enhancing the generation details and flexibility. It has the following technical innovations. (1) Disco4D learns to efficiently fit the clothing Gaussians over the SMPL-X Gaussians. (2) It adopts diffusion models to enhance the 3D generation process, e.g. modeling occluded parts not visible in the input image. (3) It learns an identity encoding for each clothing Gaussian to facilitate the separation and extraction of clothing assets. Furthermore, Disco4D naturally supports 4D human animation with vivid dynamics. Extensive experiments demonstrate the superiority of Disco4D on 4D human generation and animation tasks",
    "checked": true,
    "id": "2e7a0bf4dfe68ca87a0795ed5e8e34ff4601aca5",
    "semantic_title": "disco4d: disentangled 4d human generation and animation from a single image",
    "citation_count": 3,
    "authors": [
      "Hui En Pang",
      "Shuai Liu",
      "Zhongang Cai",
      "Lei Yang",
      "Tianwei Zhang",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_DoraCycle_Domain-Oriented_Adaptation_of_Unified_Generative_Model_in_Multimodal_Cycles_CVPR_2025_paper.html": {
    "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles",
    "volume": "main",
    "abstract": "Adapting generative models to specific domains presents an effective solution for satisfying specialized requirements. However, adapting to some complex domains remains challenging, especially when these domains require substantial paired data to capture the targeted distributions. Since unpaired data from a single modality, such as vision or language, is more readily available, we utilize the bidirectional mappings between vision and language learned by the unified generative model to enable training on unpaired data for domain adaptation. Specifically, we propose DoraCycle, which integrates two multimodal cycles: text-to-image-to-text and image-to-text-to-image. The model is optimized through cross-entropy loss computed at the cycle endpoints, where both endpoints share the same modality. This facilitates self-evolution of the model without reliance on annotated text-image pairs. Experimental results demonstrate that for tasks independent of paired knowledge, such as stylization, DoraCycle can effectively adapt the unified model using only unpaired data. For tasks involving new paired knowledge, such as specific identities, a combination of a small set of paired image-text examples and larger-scale unpaired data is sufficient for effective domain-oriented adaptation. The code will be released at https://github.com/showlab/DoraCycle",
    "checked": true,
    "id": "86d181c0a4f2163573af5fd1eff23dfd4dcf822d",
    "semantic_title": "doracycle: domain-oriented adaptation of unified generative model in multimodal cycles",
    "citation_count": 1,
    "authors": [
      "Rui Zhao",
      "Weijia Mao",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_WeatherGen_A_Unified_Diverse_Weather_Generator_for_LiDAR_Point_Clouds_CVPR_2025_paper.html": {
    "title": "WeatherGen: A Unified Diverse Weather Generator for LiDAR Point Clouds via Spider Mamba Diffusion",
    "volume": "main",
    "abstract": "3D scene perception demands a large amount of adverse-weather LiDAR data, yet the cost of LiDAR data collection presents a significant scaling-up challenge. To this end, a series of LiDAR simulators have been proposed. Yet, they can only simulate a single adverse weather with a single physical model, and the fidelity is quite limited. This paper presents **WeatherGen**, the first unified diverse-weather LiDAR data diffusion generation framework, significantly improving fidelity. Specifically, we first design a map-based data producer, which is capable of providing a vast amount of high-quality diverse-weather data for training purposes. Then, we utilize the diffusion-denoising paradigm to construct a diffusion model. Among them, we propose a spider mamba generator with the spider mamba scan to restore the disturbed diverse weather data gradually. The spider mamba models the feature interactions by scanning the LiDAR beam circle and central ray, excellently maintaining the physical structure of the LiDAR point cloud. Subsequently, we design a latent domain aligner following the generator to transfer real-world knowledge. Afterward, we devise a contrastive learning-based controller, which equips weather control signals with compact semantic knowledge through language supervision from CLIP, guiding the diffusion model in generating more discriminative data. Finally, we fine-tune WeatherGen with small-scale real-world data to further enhance its performance. Extensive evaluations on KITTI-360 and Seeing Through Fog demonstrate the high generation quality of WeatherGen. Through WeatherGen, we construct the mini-weather dataset, promoting the performance of the downstream task under adverse weather conditions",
    "checked": true,
    "id": "4b0264fc66e3c2ad162fa039eb24e691857ba77c",
    "semantic_title": "weathergen: a unified diverse weather generator for lidar point clouds via spider mamba diffusion",
    "citation_count": 0,
    "authors": [
      "Yang Wu",
      "Yun Zhu",
      "Kaihua Zhang",
      "Jianjun Qian",
      "Jin Xie",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qin_MUST_The_First_Dataset_and_Unified_Framework_for_Multispectral_UAV_CVPR_2025_paper.html": {
    "title": "MUST: The First Dataset and Unified Framework for Multispectral UAV Single Object Tracking",
    "volume": "main",
    "abstract": "UAV tracking faces significant challenges in real-world scenarios, such as small-size targets and occlusions, which limit the performance of RGB-based trackers. Multispectral images (MSI), which capture additional spectral information, offer a promising solution to these challenges. However, progress in this field has been hindered by the lack of relevant datasets. To address this gap, we introduce the first large-scale Multispectral UAV Single Object Tracking dataset (MUST), which includes 250 video sequences spanning diverse environments and challenges, providing a comprehensive data foundation for multispectral UAV tracking. We also propose a novel tracking framework, UNTrack, which encodes unified spectral, spatial, and temporal features from spectrum prompts, initial templates, and sequential searches. UNTrack employs an asymmetric transformer with a spectral background eliminate mechanism for optimal relationship modeling and an encoder that continuously updates the spectrum prompt to refine tracking, improving both accuracy and efficiency. Extensive experiments show that our proposed UNTrack outperforms state-of-the-art UAV trackers. We believe our dataset and framework will drive future research in this area. The dataset is available on https://github.com/q2479036243/MUST-Multispectral-UAV-Single-Object-Tracking",
    "checked": true,
    "id": "7c09fce77f682e392bae1eb02d1b81a2c14b381e",
    "semantic_title": "must: the first dataset and unified framework for multispectral uav single object tracking",
    "citation_count": 0,
    "authors": [
      "Haolin Qin",
      "Tingfa Xu",
      "Tianhao Li",
      "Zhenxiang Chen",
      "Tao Feng",
      "Jianan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhuang_IDOL_Instant_Photorealistic_3D_Human_Creation_from_a_Single_Image_CVPR_2025_paper.html": {
    "title": "IDOL: Instant Photorealistic 3D Human Creation from a Single Image",
    "volume": "main",
    "abstract": "Creating a high-fidelity, animatable 3D full-body avatar from a single image is a challenging task due to the diverse appearance and poses of humans and the limited availability of high-quality training data. To achieve fast and high-quality human reconstruction, this work rethinks the task from the perspectives of dataset, model, and representation. First, we introduce a large-scale HUman GEnerated training dataset, HuGe100K, consisting of 100K diverse, photorealistic human images with corresponding 24-view in a static pose or dynamic pose frames generated via a pose-controllable image-to-video model. Next, leveraging the diversity in views, poses, and appearances within HuGe100K, we develop a scalable feed-forward transformer model to predict a 3D human Gaussian representation in a uniform space of a given human image. This model is trained to disentangle human pose, shape, clothing geometry, and texture. Accordingly, the estimated Gaussians can be animated robustly without post-processing. We conduct comprehensive experiments to validate the effectiveness of the proposed dataset and method. Our model demonstrates the generalizable ability to efficiently reconstruct photorealistic humans in under 1 second using a single GPU. Additionally, it seamlessly supports various applications, including animation, shape, and texture editing tasks",
    "checked": true,
    "id": "97399c7dfb530190dd73da37ec3f4ea30bf177e2",
    "semantic_title": "idol: instant photorealistic 3d human creation from a single image",
    "citation_count": 11,
    "authors": [
      "Yiyu Zhuang",
      "Jiaxi Lv",
      "Hao Wen",
      "Qing Shuai",
      "Ailing Zeng",
      "Hao Zhu",
      "Shifeng Chen",
      "Yujiu Yang",
      "Xun Cao",
      "Wei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_Tightening_Robustness_Verification_of_MaxPool-based_Neural_Networks_via_Minimizing_the_CVPR_2025_paper.html": {
    "title": "Tightening Robustness Verification of MaxPool-based Neural Networks via Minimizing the Over-Approximation Zone",
    "volume": "main",
    "abstract": "The robustness of neural network classifiers is important in the safety-critical domain and can be quantified by robustness verification. At present, efficient and scalable verification techniques are always sound but incomplete, and thus, the improvement of verified robustness results is the key criterion to evaluate the performance of incomplete verification approaches. The multi-variate function MaxPool is widely adopted yet challenging to verify. In this paper, we present Ti-Lin, a robustness verifier for MaxPool-based CNNs with Tight Linear Approximation. Following the sequel of minimizing the over-approximation zone of the non-linear function of CNNs, we are the first to propose the provably neuron-wise tightest linear bounds for the MaxPool function. By our proposed linear bounds, we can certify larger robustness results for CNNs. We evaluate the effectiveness of Ti-Lin on different verification frameworks with open-sourced benchmarks, including LeNet, PointNet, and networks trained on the MNIST, CIFAR-10, Tiny ImageNet and ModelNet40 datasets. Experimental results show that Ti-Lin significantly outperforms the state-of-the-art methods across all networks with up to 78.6% improvement in terms of the certified accuracy with almost the same time consumption as the fastest tool. Our code is available at https://anonymous.4open.science/r/Ti-Lin-cvpr-72EE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Xiao",
      "Yuchen Chen",
      "Shiqing Ma",
      "Chunrong Fang",
      "Tongtong Bai",
      "Mingzheng Gu",
      "Yuxin Cheng",
      "Yanwei Chen",
      "Zhenyu Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_SketchVideo_Sketch-based_Video_Generation_and_Editing_CVPR_2025_paper.html": {
    "title": "SketchVideo: Sketch-based Video Generation and Editing",
    "volume": "main",
    "abstract": "Video generation and editing conditioned on text prompts or images have undergone significant advancements. However, challenges remain in accurately controlling global layout and geometry details solely by texts, and supporting motion control and local modification through images. In this paper, we aim to achieve sketch-based spatial and motion control for video generation and support fine-grained editing of real or synthetic videos. Based on the DiT video generation model, we propose a memory-efficient control structure with sketch control blocks that predict residual features of skipped DiT blocks. Sketches are drawn on one or two keyframes (at arbitrary time points) for easy interaction. To propagate such temporally sparse sketch conditions across all frames, we propose an inter-frame attention mechanism to analyze the relationship between the keyframes and each video frame. For sketch-based video editing, we design an additional video insertion module that maintains consistency between the newly edited content and the original video's spatial feature and dynamic motion. During inference, we use latent fusion for the accurate preservation of unedited regions. Extensive experiments demonstrate that our SketchVideo achieves superior performance in controllable video generation and editing",
    "checked": true,
    "id": "2edbb260e2fcc3a46bd01d48adeef18a11674b26",
    "semantic_title": "sketchvideo: sketch-based video generation and editing",
    "citation_count": 0,
    "authors": [
      "Feng-Lin Liu",
      "Hongbo Fu",
      "Xintao Wang",
      "Weicai Ye",
      "Pengfei Wan",
      "Di Zhang",
      "Lin Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Spitznagel_PhysicsGen_Can_Generative_Models_Learn_from_Images_to_Predict_Complex_CVPR_2025_paper.html": {
    "title": "PhysicsGen: Can Generative Models Learn from Images to Predict Complex Physical Relations?",
    "volume": "main",
    "abstract": "The image-to-image translation abilities of generative learning models have recently made significant progress in the estimation of complex (steered) mappings between image distributions. While appearance based tasks like image in-painting or style transfer have been studied at length, we propose to investigate the potential of generative models in the context of physical simulations. Providing a dataset of 300k image-pairs and baseline evaluations for three different physical simulation tasks, we propose a benchmark to investigate the following research questions: i) are generative models able to learn complex physical relations from input-output image pairs? ii) what speedups can be achieved by replacing differential equation based simulations? While baseline evaluations of different current models show the potential for high speedups (ii), these results also show strong limitations toward the physical correctness (i). This underlines the need for new methods to enforce physical correctness",
    "checked": true,
    "id": "adba20dcc4509ba2dab1ff4ae23acb57f8fce746",
    "semantic_title": "physicsgen: can generative models learn from images to predict complex physical relations?",
    "citation_count": 0,
    "authors": [
      "Martin Spitznagel",
      "Jan Vaillant",
      "Janis Keuper"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Taste_More_Taste_Better_Diverse_Data_and_Strong_Model_Boost_CVPR_2025_paper.html": {
    "title": "Taste More, Taste Better: Diverse Data and Strong Model Boost Semi-Supervised Crowd Counting",
    "volume": "main",
    "abstract": "Semi-supervised crowd counting is crucial for addressing the high annotation costs of densely populated scenes. Although several methods based on pseudo-labeling have been proposed, it remains challenging to effectively and accurately utilize unlabeled data. In this paper, we propose a novel framework called Taste More Taste Better (TMTB), which emphasizes both data and model aspects. Firstly, we explore a data augmentation technique well-suited for the crowd counting task. By inpainting the background regions, this technique can effectively enhance data diversity while preserving the fidelity of the entire scenes. Secondly, we introduce the Visual State Space Model as backbone to capture the global context information from crowd scenes, which is crucial for extremely crowded, low-light, and adverse weather scenarios. In addition to the traditional regression head for exact prediction, we employ an Anti-Noise classification head to provide less exact but more accurate supervision, since the regression head is sensitive to noise in manual annotations. We conduct extensive experiments on four benchmark datasets and show that our method outperforms state-of-the-art methods by a large margin. Code is publicly available on https://github.com/syhien/taste_more_taste_better",
    "checked": true,
    "id": "398a1f99435f912fcc6589815498bb37280d5bc1",
    "semantic_title": "taste more, taste better: diverse data and strong model boost semi-supervised crowd counting",
    "citation_count": 0,
    "authors": [
      "Maochen Yang",
      "Zekun Li",
      "Jian Zhang",
      "Lei Qi",
      "Yinghuan Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_Gaussian_Splashing_Unified_Particles_for_Versatile_Motion_Synthesis_and_Rendering_CVPR_2025_paper.html": {
    "title": "Gaussian Splashing: Unified Particles for Versatile Motion Synthesis and Rendering",
    "volume": "main",
    "abstract": "We demonstrate the feasibility of integrating physics-based animations of solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in virtual scenes reconstructed using 3DGS. Leveraging the coherence of the Gaussian Splatting and Position-Based Dynamics (PBD) in the underlying representation, we manage rendering, view synthesis, and the dynamics of solids and fluids in a cohesive manner. Similar to GaussianShader, we enhance each Gaussian kernel with an added normal, aligning the kernel's orientation with the surface normal to refine the PBD simulation. This approach effectively eliminates spiky noises that arise from rotational deformation in solids. It also allows us to integrate physically based rendering to augment the dynamic surface reflections on fluids. Consequently, our framework is capable of realistically reproducing surface highlights on dynamic fluids and facilitating interactions between scene objects and fluids from new views",
    "checked": true,
    "id": "647cd7fa76183b31175ff63c305ef2ed67c8866b",
    "semantic_title": "gaussian splashing: unified particles for versatile motion synthesis and rendering",
    "citation_count": 8,
    "authors": [
      "Yutao Feng",
      "Xiang Feng",
      "Yintong Shang",
      "Ying Jiang",
      "Chang Yu",
      "Zeshun Zong",
      "Tianjia Shao",
      "Hongzhi Wu",
      "Kun Zhou",
      "Chenfanfu Jiang",
      "Yin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Improve_Representation_for_Imbalanced_Regression_through_Geometric_Constraints_CVPR_2025_paper.html": {
    "title": "Improve Representation for Imbalanced Regression through Geometric Constraints",
    "volume": "main",
    "abstract": "In representation learning, uniformity refers to the uniform feature distribution in the latent space (i.e., unit hypersphere). Previous work has shown that improving uniformity contributes to the learning of under-represented classes. However, most of the previous work focused on classification; the representation space of imbalanced regression remains unexplored. Classification-based methods are not suitable for regression tasks because they cluster features into distinct groups without considering the continuous and ordered nature essential for regression. In a geometric aspect, we uniquely focus on ensuring uniformity in the latent space for imbalanced regression through two key losses: enveloping and homogeneity. The enveloping loss encourages the induced trace to uniformly occupy the surface of a hypersphere, while the homogeneity loss ensures smoothness, with representations evenly spaced at consistent intervals. Our method integrates these geometric principles into the data representations via a Surrogate-driven Representation Learning (SRL) framework. Experiments with real-world regression and operator learning tasks highlight the importance of uniformity in imbalanced regression and validate the efficacy of our geometry-based loss functions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijian Dong",
      "Yilei Wu",
      "Chongyao Chen",
      "Yingtian Zou",
      "Yichi Zhang",
      "Juan Helen Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_AnyDressing_Customizable_Multi-Garment_Virtual_Dressing_via_Latent_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent Diffusion Models",
    "volume": "main",
    "abstract": "Recent advances in garment-centric image generation from text and image prompts based on diffusion models are impressive. However, existing methods lack support for various combinations of attire, and struggle to preserve the garment details while maintaining faithfulness to the text prompts, limiting their performance across diverse scenarios. In this paper, we focus on a new task, i.e., Multi-Garment Virtual Dressing, and we propose a novel AnyDressing method for customizing characters conditioned on any combination of garments and any personalized text prompts. AnyDressing primarily comprises two primary networks named GarmentsNet and DressingNet, which are respectively dedicated to extracting detailed clothing features and generating customized images. Specifically, we propose an efficient and scalable module called Garment-Specific Feature Extractor in GarmentsNet to individually encode garment textures in parallel. This design prevents garment confusion while ensuring network efficiency. Meanwhile, we design an adaptive Dressing-Attention mechanism and a novel Instance-Level Garment Localization Learning strategy in DressingNet to accurately inject multi-garment features into their corresponding regions. This approach efficiently integrates multi-garment texture cues into generated images and further enhances text-image consistency. Additionally, we introduce a Garment-Enhanced Texture Learning strategy to improve the fine-grained texture details of garments. Thanks to our well-craft design, AnyDressing can serve as a plug-in module to easily integrate with any community control extensions for diffusion models, improving the diversity and controllability of synthesized images. Extensive experiments show that AnyDressing achieves state-of-the-art results",
    "checked": true,
    "id": "085a905534925eaddcdafd7c36ad3439f52126b2",
    "semantic_title": "anydressing: customizable multi-garment virtual dressing via latent diffusion models",
    "citation_count": 4,
    "authors": [
      "Xinghui Li",
      "Qichao Sun",
      "Pengze Zhang",
      "Fulong Ye",
      "Zhichao Liao",
      "Wanquan Feng",
      "Songtao Zhao",
      "Qian He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bahri_Spectral_Informed_Mamba_for_Robust_Point_Cloud_Processing_CVPR_2025_paper.html": {
    "title": "Spectral Informed Mamba for Robust Point Cloud Processing",
    "volume": "main",
    "abstract": "State Space Models (SSMs) have shown significant promise in Natural Language Processing (NLP) and, more recently, computer vision. This paper introduces a new methodology leveraging Mamba and Masked Autoencoder (MAE) networks for point cloud data in both supervised and self-supervised learning. We propose three key contributions to enhance Mamba's capability in processing complex point cloud structures. First, we exploit the spectrum of a graph Laplacian to capture patch connectivity, defining an isometry-invariant traversal order that is robust to viewpoints and better captures shape manifolds than traditional 3D grid-based traversals. Second, we adapt segmentation via a recursive patch partitioning strategy informed by Laplacian spectral components, allowing finer integration and segment analysis. Third, we address token placement in MAE for Mamba by restoring tokens to their original positions, which preserves essential order and improves learning. Extensive experiments demonstrate our approach's improvements in classification, segmentation, and few-shot tasks over state-of-the-art (SOTA) baselines",
    "checked": true,
    "id": "0f4f1eb47ec009543d9cb6f295ec6c86eda3f195",
    "semantic_title": "spectral informed mamba for robust point cloud processing",
    "citation_count": 3,
    "authors": [
      "Ali Bahri",
      "Moslem Yazdanpanah",
      "Mehrdad Noori",
      "Sahar Dastani",
      "Milad Cheraghalikhani",
      "Gustavo Adolfo Vargas Hakim",
      "David Osowiechi",
      "Farzad Beizaee",
      "Ismail Ben Ayed",
      "Christian Desrosiers"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Souza_Latent_Space_Imaging_CVPR_2025_paper.html": {
    "title": "Latent Space Imaging",
    "volume": "main",
    "abstract": "Digital imaging systems have traditionally relied on brute-force measurement and processing of pixels arranged on regular grids. In contrast, the human visual system performs significant data reduction from the large number of photoreceptors to the optic nerve, effectively encoding visual information into a low-bandwidth latent space representation optimized for brain processing. Inspired by this, we propose a similar approach to advance artificial vision systems. Latent Space Imaging introduces a new paradigm that combines optics and software to encode image information directly into the semantically rich latent space of a generative model. This approach substantially reduces bandwidth and memory demands during image capture and enables a range of downstream tasks focused on the latent space.We validate this principle through an initial hardware prototype based on a single-pixel camera. By implementing an amplitude modulation scheme that encodes into the generative model's latent space, we achieve compression ratios ranging from 1:100 to 1:1000 during imaging, and up to 1:16384 for downstream applications. This approach leverages the model's intrinsic linear boundaries, demonstrating the potential of latent space imaging for highly efficient imaging hardware, adaptable future applications in high-speed imaging, and task-specific cameras with significantly reduced hardware complexity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matheus Souza",
      "Yidan Zheng",
      "Kaizhang Kang",
      "Yogeshwar Nath Mishra",
      "Qiang Fu",
      "Wolfgang Heidrich"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Balanced_Direction_from_Multifarious_Choices_Arithmetic_Meta-Learning_for_Domain_Generalization_CVPR_2025_paper.html": {
    "title": "Balanced Direction from Multifarious Choices: Arithmetic Meta-Learning for Domain Generalization",
    "volume": "main",
    "abstract": "Domain generalization is proposed to address distribution shift, arising from statistical disparities between training source and unseen target domains. The widely used first-order meta-learning algorithms demonstrate strong performance for domain generalization by leveraging the gradient matching theory, which aims to establish balanced parameters across source domains to reduce overfitting to any particular domain. However, our analysis reveals that there are actually numerous directions to achieve gradient matching, with current methods representing just one possible path. These methods actually overlook another critical factor that the balanced parameters should be close to the centroid of optimal parameters of each source domain. To address this, we propose a simple yet effective arithmetic meta-learning with arithmetic-weighted gradients. This approach, while adhering to the principles of gradient matching, promotes a more precise balance by estimating the centroid between domain-specific optimal parameters. Experimental results validate the effectiveness of our strategy. Our code is available at https://github.com/zzwdx/ARITH",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiran Wang",
      "Jian Zhang",
      "Lei Qi",
      "Yinghuan Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shin_Anatomical_Consistency_and_Adaptive_Prior-informed_Transformation_for_Multi-contrast_MR_Image_CVPR_2025_paper.html": {
    "title": "Anatomical Consistency and Adaptive Prior-informed Transformation for Multi-contrast MR Image Synthesis via Diffusion Model",
    "volume": "main",
    "abstract": "Multi-contrast magnetic resonance (MR) images offer critical diagnostic information but are limited by long scan times and high cost. While diffusion models (DMs) excel in medical image synthesis, they often struggle to maintain anatomical consistency and utilize the diverse characteristics of multi-contrast MR images effectively. We propose APT, a unified diffusion model designed to generate accurate and anatomically consistent multi-contrast MR images. APT introduces a mutual information fusion module and an anatomical consistency loss to preserve critical anatomical structures across multiple contrast inputs. To enhance synthesis, APT incorporates a two-stage inference process: in the first stage, a prior codebook provides coarse anatomical structures by selecting appropriate guidance based on precomputed similarity mappings and Bezier curve transformations. The second stage applies iterative unrolling with weighted averaging to refine the initial output, enhancing fine anatomical details and ensuring structural consistency. This approach enables the preservation of both global structures and local details, resulting in realistic and diagnostically valuable synthesized images. Extensive experiments on public multi-contrast MR brain images demonstrate that our approach significantly outperforms state-of-the-art methods. The source codes are available at https://github.com/yejees/APT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yejee Shin",
      "Yeeun Lee",
      "Hanbyol Jang",
      "Geonhui Son",
      "Hyeongyu Kim",
      "Dosik Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_BlobGEN-Vid_Compositional_Text-to-Video_Generation_with_Blob_Video_Representations_CVPR_2025_paper.html": {
    "title": "BlobGEN-Vid: Compositional Text-to-Video Generation with Blob Video Representations",
    "volume": "main",
    "abstract": "Existing video generation models struggle to follow complex text prompts and synthesize multiple objects, raising the need for additional grounding input for improved controllability. In this work, we propose to decompose videos into visual primitives -- blob video representation, a general representation for controllable video generation. Based on blob conditions, we develop a blob-grounded video diffusion model named BlobGEN-Vid that allows users to control object motions and fine-grained object appearance. In particular, we introduce a masked 3D attention module that effectively improves regional consistency across frames. In addition, we introduce a learnable module to interpolate text embeddings so that users can control semantics in specific frames and obtain smooth object transitions. We show that our framework is model-agnostic and build BlobGEN-Vid based on both U-Net and DiT-based video diffusion models. Extensive experimental results show that BlobGEN-Vid achieves superior zero-shot video generation ability and state-of-the-art layout controllability on multiple benchmarks. When combined with a Large Language Model for layout planning, our framework even outperforms proprietary text-to-video generators regarding compositional accuracy",
    "checked": true,
    "id": "5e096c6fdef7b26fe9227e4f1ddf41be8d87acc3",
    "semantic_title": "blobgen-vid: compositional text-to-video generation with blob video representations",
    "citation_count": 5,
    "authors": [
      "Weixi Feng",
      "Chao Liu",
      "Sifei Liu",
      "William Yang Wang",
      "Arash Vahdat",
      "Weili Nie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_D2SP_Dynamic_Dual-Stage_Purification_Framework_for_Dual_Noise_Mitigation_in_CVPR_2025_paper.html": {
    "title": "D2SP: Dynamic Dual-Stage Purification Framework for Dual Noise Mitigation in Vision-based Affective Recognition",
    "volume": "main",
    "abstract": "The current advancements in Dynamic Facial Expression Recognition (DFER) methods mainly focus on better capturing the spatial and temporal features of facial expressions. However, DFER datasets contain a substantial amount of noisy samples, and few have addressed the issue of handling this noise. We identified two types of noise: one is caused by low-quality data resulting from factors such as occlusion, dim lighting, and blurriness; the other arises from mislabeled data due to annotation bias by annotators. Addressing the two types of noise, we have meticulously crafted a Dynamic Dual-Stage Purification (D2SP) Framework. This initiative aims to dynamically purify the DFER datasets of these two types of noise, ensuring that only high-quality and correctly labeled data is used in the training process. To mitigate low-quality samples, we introduce the Coarse-Grained Pruning (CGP) stage, which computes sample weights and prunes those low-weight samples. After CGP, the Fine-Grained Correction (FGC) stage evaluates prediction stability to correct mislabeled data. Moreover, D2SP is conceived as a general and plug-and-play framework, tailored to integrate seamlessly with prevailing DFER methods. Extensive experiments covering prevalent DFER datasets and deploying multiple benchmark methods have substantiated D2SP's ability to significantly enhance performance metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Wang",
      "Xinji Mai",
      "Zeng Tao",
      "Xuan Tong",
      "Junxiong Lin",
      "Yan Wang",
      "Jiawen Yu",
      "Shaoqi Yan",
      "Ziheng Zhou",
      "Wenqiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_PartRM_Modeling_Part-Level_Dynamics_with_Large_Cross-State_Reconstruction_Model_CVPR_2025_paper.html": {
    "title": "PartRM: Modeling Part-Level Dynamics with Large Cross-State Reconstruction Model",
    "volume": "main",
    "abstract": "As interest grows in world models that predict future states from current observations and actions, accurately modeling part-level dynamics has become increasingly relevant for various applications. Existing approaches, such as Puppet-Master, rely on fine-tuning large-scale pre-trained video diffusion models, which are impractical for real-world use due to the limitations of 2D video representation and slow processing times. To overcome these challenges, we present PartRM, a novel 4D reconstruction framework that simultaneously models appearance, geometry, and part-level motion from multi-view images of a static object. PartRM builds upon large 3D Gaussian reconstruction models, leveraging their extensive knowledge of appearance and geometry in static objects. To address data scarcity in 4D, we introduce the PartDrag-4D dataset, providing multi-view observations of part-level dynamics across over 20,000 states. We enhance the model's understanding of interaction conditions with a multi-scale drag embedding module that captures dynamics at varying granularities. To prevent catastrophic forgetting during fine-tuning, we implement a two-stage training process that focuses sequentially on motion and appearance learning. Experimental results show that PartRM establishes a new state-of-the-art in part-level motion learning and can be applied in manipulation tasks in robotics. Our code, data, and models are publicly available to facilitate future research",
    "checked": true,
    "id": "bb8fc303bfce1d61f2a3f703a0ac65cfed6be370",
    "semantic_title": "partrm: modeling part-level dynamics with large cross-state reconstruction model",
    "citation_count": 2,
    "authors": [
      "Mingju Gao",
      "Yike Pan",
      "Huan-ang Gao",
      "Zongzheng Zhang",
      "Wenyi Li",
      "Hao Dong",
      "Hao Tang",
      "Li Yi",
      "Hao Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_LaVin-DiT_Large_Vision_Diffusion_Transformer_CVPR_2025_paper.html": {
    "title": "LaVin-DiT: Large Vision Diffusion Transformer",
    "volume": "main",
    "abstract": "This paper presents the Large Vision Diffusion Transformer (LaVin-DiT), a scalable and unified foundation model designed to tackle over 20 computer vision tasks in a generative framework. Unlike existing large vision models directly adapted from natural language processing architectures, which rely on less efficient autoregressive techniques and disrupt spatial relationships essential for vision data, LaVin-DiT introduces key innovations to optimize generative performance for vision tasks. First, to address the high dimensionality of visual data, we incorporate a spatial-temporal variational autoencoder that encodes data into a continuous latent space. Second, for generative modeling, we develop a joint diffusion transformer that progressively produces vision outputs. Third, for unified multi-task training, in-context learning is implemented. Input-target pairs serve as task context, which guides the diffusion transformer to align outputs with specific tasks within the latent space. During inference, a task-specific context set and test data as queries allow LaVin-DiT to generalize across tasks without fine-tuning. Trained on extensive vision datasets, the model is scaled from 0.1B to 3.4B parameters, demonstrating substantial scalability and state-of-the-art performance across diverse vision tasks. This work introduces a novel pathway for large vision foundation models, underscoring the promising potential of diffusion transformers. The code and models are available at https://derrickwang005.github.io/LaVin-DiT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoqing Wang",
      "Xiaobo Xia",
      "Runnan Chen",
      "Dongdong Yu",
      "Changhu Wang",
      "Mingming Gong",
      "Tongliang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_DiffFNO_Diffusion_Fourier_Neural_Operator_CVPR_2025_paper.html": {
    "title": "DiffFNO: Diffusion Fourier Neural Operator",
    "volume": "main",
    "abstract": "We introduce DiffFNO, a novel diffusion framework for arbitrary-scale super-resolution strengthened by a Weighted Fourier Neural Operator (WFNO). Mode Rebalancing in WFNO effectively captures critical frequency components, significantly improving the reconstruction of high-frequency image details that are crucial for super-resolution tasks. Gated Fusion Mechanism (GFM) adaptively complements WFNO's spectral features with spatial features from an Attention-based Neural Operator (AttnNO). This enhances the network's capability to capture both global structures and local details. Adaptive Time-Step (ATS) ODE solver, a deterministic sampling strategy, accelerates inference without sacrificing output quality by dynamically adjusting integration step sizes ATS. Extensive experiments demonstrate that DiffFNO achieves state-of-the-art (SOTA) results, outperforming existing methods across various scaling factors by a margin of 2-4 dB in PSNR, including those beyond the training distribution. It also achieves this at lower inference time. Our approach sets a new standard in super-resolution, delivering both superior accuracy and computational efficiency",
    "checked": true,
    "id": "f7b5af8dffa609418b5335b48d4f6b1d4e792e8e",
    "semantic_title": "difffno: diffusion fourier neural operator",
    "citation_count": 2,
    "authors": [
      "Xiaoyi Liu",
      "Hao Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation_CVPR_2025_paper.html": {
    "title": "CAP-Net: A Unified Network for 6D Pose and Size Estimation of Categorical Articulated Parts from a Single RGB-D Image",
    "volume": "main",
    "abstract": "This paper tackles category-level pose estimation of ar- ticulated objects in robotic manipulation tasks and intro- duces a new benchmark dataset. While recent methods es- timate part poses and sizes at the category level, they often rely on geometric cues and complex multi-stage pipelines that first segment parts from the point cloud, followed by Normalized Part Coordinate Space (NPCS) estimation for 6D poses. These approaches overlook dense semantic cues from RGB images, leading to suboptimal accuracy, partic- ularly for objects with small parts. To address these limita- tions, we propose a single-stage Network, CAP-Net, for es- timating the 6D poses and sizes of Categorical Articulated Parts. This method combines RGB-D features to generate instance segmentation and NPCS representations for each part in an end-to-end manner. CAP-Net uses a unified net- work to simultaneously predict point-wise class labels, cen- troid offsets, and NPCS maps. A clustering algorithm then groups points of the same predicted class based on their es- timated centroid distances to isolate each part. Finally, the NPCS region of each part is aligned with the point cloud to recover its final pose and size. To bridge the sim-to-real do- main gap, we introduce the RGBD-Art dataset, the largest RGB-D articulated dataset to date, featuring photorealistic RGB images and depth noise simulated from real sensors. Experimental evaluations on the RGBD-Art dataset demon- strate that our method significantly outperforms the state- of-the-art approach. Real-world deployments of our model in robotic tasks underscore its robustness and exceptional sim-to-real transfer capabilities, confirming its substantial practical utility",
    "checked": true,
    "id": "aa5e10598b70c286e656076d802953074ac3c1cb",
    "semantic_title": "cap-net: a unified network for 6d pose and size estimation of categorical articulated parts from a single rgb-d image",
    "citation_count": 0,
    "authors": [
      "Jingshun Huang",
      "Haitao Lin",
      "Tianyu Wang",
      "Yanwei Fu",
      "Xiangyang Xue",
      "Yi Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in_CVPR_2025_paper.html": {
    "title": "SeCap: Self-Calibrating and Adaptive Prompts for Cross-view Person Re-Identification in Aerial-Ground Networks",
    "volume": "main",
    "abstract": "When discussing the Aerial-Ground Person Re-identification (AGPReID) task, we face the main challenge of the significant appearance variations caused by different viewpoints, making identity matching difficult. To address this issue, previous methods attempt to reduce the differences between viewpoints by critical attributes and decoupling the viewpoints. While these methods can mitigate viewpoint differences to some extent, they still face two main issues: (1) difficulty in handling viewpoint diversity and (2) neglect of the contribution of local features. To effectively address these challenges, we design and implement the Self-Calibrating and Adaptive Prompt (SeCap) method for the AGPReID task. The core of this framework relies on the Prompt Re-calibration Module (PRM), which adaptively re-calibrates prompts based on the input. Combined with the Local Feature Refinement Module (LFRM), SeCap can extract view-invariant features from local features for AGPReID. Meanwhile, given the current scarcity of datasets in the AGPReID field, we further contribute two real-world Large-scale Aerial-Ground Person Re-Identification datasets, LAGPeR and G2APS-ReID. The former is collected and annotated by us independently, covering 4,231 unique identities and containing 63,841 high-quality images; the latter is reconstructed from the person search dataset G2APS. Through extensive experiments on AGPReID datasets, we demonstrate that SeCap is a feasible and effective solution for the AGPReID task",
    "checked": true,
    "id": "d239da3be29454a6addcce17fd62a3bba1c9ee63",
    "semantic_title": "secap: self-calibrating and adaptive prompts for cross-view person re-identification in aerial-ground networks",
    "citation_count": 2,
    "authors": [
      "Shining Wang",
      "Yunlong Wang",
      "Ruiqi Wu",
      "Bingliang Jiao",
      "Wenxuan Wang",
      "Peng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pippi_Zero-Shot_Styled_Text_Image_Generation_but_Make_It_Autoregressive_CVPR_2025_paper.html": {
    "title": "Zero-Shot Styled Text Image Generation, but Make It Autoregressive",
    "volume": "main",
    "abstract": "Styled Handwritten Text Generation (HTG) has recently received attention from the computer vision and document analysis communities, which have developed several solutions, either GAN- or diffusion-based, that achieved promising results. Nonetheless, these strategies fail to generalize to novel styles and have technical constraints, particularly in terms of maximum output length and training efficiency. To overcome these limitations, in this work, we propose a novel framework for text-image generation, dubbed Emuru. Our approach leverages a powerful text-image representation model (a variational autoencoder) combined with an autoregressive Transformer. Our approach enables the generation of styled text images conditioned on textual content and style examples, such as specific fonts or handwriting styles. We train our model solely on a diverse, synthetic dataset of English text rendered in over 100,000 typewritten and calligraphy fonts, which gives it the capability to reproduce unseen styles (both fonts and users' handwriting) in zero-shot. To the best of our knowledge, Emuru is the first autoregressive model for HTG, and the first designed specifically for generalization to novel styles. Moreover, our model generates images without background artifacts, which are easier to use for downstream applications. Extensive evaluation on both typewritten and handwritten, any-length text image generation scenarios demonstrates the effectiveness of our approach",
    "checked": true,
    "id": "5e3fadc38814422591cc02e65a94927479ad3040",
    "semantic_title": "zero-shot styled text image generation, but make it autoregressive",
    "citation_count": 3,
    "authors": [
      "Vittorio Pippi",
      "Fabio Quattrini",
      "Silvia Cascianelli",
      "Alessio Tonioni",
      "Rita Cucchiara"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Song_Dont_Shake_the_Wheel_Momentum-Aware_Planning_in_End-to-End_Autonomous_Driving_CVPR_2025_paper.html": {
    "title": "Don't Shake the Wheel: Momentum-Aware Planning in End-to-End Autonomous Driving",
    "volume": "main",
    "abstract": "End-to-end autonomous driving frameworks enable seamless integration of perception and planning but often rely on one-shot trajectory prediction, which may lead to unstable control and vulnerability to occlusions in single-frame perception. To address this, we propose the Momentum-Aware Driving (MomAD) framework, which introduces trajectory momentum and perception momentum to stabilize and refine trajectory predictions. MomAD comprises two core components: (1) Topological Trajectory Matching (TTM) employs Hausdorff Distance to select the optimal planning query that aligns with prior paths to ensure coherence; (2) Momentum Planning Interactor (MPI) cross-attends the selected planning query with historical queries to expand static and dynamic perception files. This enriched query, in turn, helps regenerate long-horizon trajectory and reduce collision risks. To mitigate noise arising from dynamic environments and detection errors, we introduce robust instance denoising during training, enabling the planning model to focus on critical signals and improve its robustness. We also propose a novel Trajectory Prediction Consistency (TPC) metric to quantitatively assess planning stability. Experiments on the nuScenes dataset demonstrate that MomAD achieves superior long-term consistency (>3s) compared to SOTA methods. Moreover, evaluations on the curated Turning-nuScenes shows that MomAD reduces the collision rate by 26% and improves TPC by 0.97m (33.45%) over a 6s prediction horizon, while closed-loop on Bench2Drive demonstrates an up to 16.3% improvement in success rate",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziying Song",
      "Caiyan Jia",
      "Lin Liu",
      "Hongyu Pan",
      "Yongchang Zhang",
      "Junming Wang",
      "Xingyu Zhang",
      "Shaoqing Xu",
      "Lei Yang",
      "Yadan Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Leveraging_Perturbation_Robustness_to_Enhance_Out-of-Distribution_Detection_CVPR_2025_paper.html": {
    "title": "Leveraging Perturbation Robustness to Enhance Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "Out-of-distribution (OOD) detection is the task of identifying inputs that deviate from the training data distribution. This capability is essential for the safe deployment of deep computer vision models in open-world environments. In this work, we propose a post-hoc method, Perturbation-Rectified OOD detection (PRO), based on the insight that prediction confidence for OOD inputs is more susceptible to reduction under perturbation than IND inputs. From this observation, we proposed a meta-score function that searches for local minimum scores near original inputs by applying gradient descent. This procedure enhances the separability between in-distribution (IND) and OOD samples. Importantly, the approach improves OOD detection performance without complex modifications to the underlying model architectures or training protocol. To validate our approach, we conduct extensive experiments using the OpenOOD benchmark. Our approach further pushes the limit of softmax-based OOD detection and is the leading post-hoc method for small-scale models. On a CIFAR-10 model with adversarial training, PRO effectively detects near-OOD inputs, achieving a reduction of more than 10% on FPR@95 compared to state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxi Chen",
      "Raymond A. Yeh",
      "Shaoshuai Mou",
      "Yan Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hao_Neural_Motion_Simulator_Pushing_the_Limit_of_World_Models_in_CVPR_2025_paper.html": {
    "title": "Neural Motion Simulator Pushing the Limit of World Models in Reinforcement Learning",
    "volume": "main",
    "abstract": "An embodied system must not only model the patterns of the external world but also understand its own motion dynamics. A motion dynamic model is essential for efficient skill acquisition and effective planning. In this work, we introduce the neural motion simulator (MoSim), a world model that predicts the future physical state of an embodied system based on current observations and actions. MoSim achieves state-of-the-art performance in physical state prediction and provides competitive performance across a range of downstream tasks. This works shows that when a world model is accurate enough and performs precise long-horizon predictions, it can facilitate efficient skill acquisition in imagined worlds and even enable zero-shot reinforcement learning. Furthermore, MoSim can transform any model-free reinforcement learning (RL) algorithm into a model-based approach, effectively decoupling physical environment modeling from RL algorithm development. This separation allows for independent advancements in RL algorithms and world modeling, significantly improving sample efficiency and enhancing generalization capabilities. Our findings highlight that world models for motion dynamics is a promising direction for developing more versatile and capable embodied systems",
    "checked": true,
    "id": "63df5f97fbfb22f438de2cc36f08967bdd73fd78",
    "semantic_title": "neural motion simulator pushing the limit of world models in reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Chenjie Hao",
      "Weyl Lu",
      "Yifan Xu",
      "Yubei Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Aesthetic_Post-Training_Diffusion_Models_from_Generic_Preferences_with_Step-by-step_Preference_CVPR_2025_paper.html": {
    "title": "Aesthetic Post-Training Diffusion Models from Generic Preferences with Step-by-step Preference Optimization",
    "volume": "main",
    "abstract": "Generating visually appealing images is fundamental to modern text-to-image generation models. A potential solution to better aesthetics is direct preference optimization (DPO), which has been applied to diffusion models to improve general image quality including prompt alignment and aesthetics. Popular DPO methods propagate preference labels from clean image pairs to all the intermediate steps along the two generation trajectories. However, preference labels provided in existing datasets are blended with layout and aesthetic opinions, which would disagree with aesthetic preference. Even if aesthetic labels were provided (at substantial cost), it would be hard for the two-trajectory methods to capture nuanced visual differences at different steps. To improve aesthetics economically, this paper uses existing generic preference data and introduces step-by-step preference optimization (SPO) that discards the propagation strategy and allows fine-grained image details to be assessed. Specifically, at each denoising step, we 1) sample a pool of candidates by denoising from a shared noise latent, 2) use a step-aware preference model to find a suitable win-lose pair to supervise the diffusion model, and 3) randomly select one from the pool to initialize the next denoising step. This strategy ensures that diffusion models focus on the subtle, fine-grained visual differences instead of layout aspect. We find that aesthetics can be significantly enhanced by accumulating these improved minor differences. When fine-tuning Stable Diffusion v1.5 and SDXL, SPO yields significant improvements in aesthetics compared with existing DPO methods while not sacrificing image-text alignment compared with vanilla models. Moreover, SPO converges much faster than DPO methods due to the use of more correct preference labels provided by the step-aware preference model. Code and models are available at https://github.com/RockeyCoss/SPO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanhao Liang",
      "Yuhui Yuan",
      "Shuyang Gu",
      "Bohan Chen",
      "Tiankai Hang",
      "Mingxi Cheng",
      "Ji Li",
      "Liang Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Adversarial_Diffusion_Compression_for_Real-World_Image_Super-Resolution_CVPR_2025_paper.html": {
    "title": "Adversarial Diffusion Compression for Real-World Image Super-Resolution",
    "volume": "main",
    "abstract": "Real-world image super-resolution (Real-ISR) aims to reconstruct high-resolution images from low-resolution inputs degraded by complex, unknown processes. While many Stable Diffusion (SD)-based Real-ISR methods have achieved remarkable success, their slow, multi-step inference hinders practical deployment. Recent SD-based one-step networks like OSEDiff and S3Diff alleviate this issue but still incur high computational costs due to their reliance on large pretrained SD models. This paper proposes a novel Real-ISR method, AdcSR, by distilling the one-step diffusion network OSEDiff into a streamlined diffusion-GAN model under our Adversarial Diffusion Compression (ADC) framework. We meticulously examine the modules of OSEDiff, categorizing them into two types: (1) Removable (VAE encoder, prompt extractor, text encoder, etc.) and (2) Prunable (denoising UNet and VAE decoder). Since direct removal and pruning can degrade the model's generation capability, we pretrain our pruned VAE decoder to restore its ability to decode images and employ adversarial distillation to compensate for performance loss. This ADC-based diffusion-GAN hybrid design effectively reduces complexity by 73% in inference time, 78% in computation, and 74% in parameters, while preserving the model's generation capability. Experiments manifest that our proposed AdcSR achieves competitive recovery quality on both synthetic and real-world datasets, offering up to 9.3x speedup over previous one-step diffusion-based methods. Code and models are available at https://github.com/Guaishou74851/AdcSR",
    "checked": true,
    "id": "dfc694571b55ea8b8908cfec125056c6fbf04cfc",
    "semantic_title": "adversarial diffusion compression for real-world image super-resolution",
    "citation_count": 12,
    "authors": [
      "Bin Chen",
      "Gehui Li",
      "Rongyuan Wu",
      "Xindong Zhang",
      "Jie Chen",
      "Jian Zhang",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mall_DiSciPLE_Learning_Interpretable_Programs_for_Scientific_Visual_Discovery_CVPR_2025_paper.html": {
    "title": "DiSciPLE: Learning Interpretable Programs for Scientific Visual Discovery",
    "volume": "main",
    "abstract": "Visual data is used in numerous different scientific workflows ranging from remote sensing to ecology. As the amount of observation data increases, the challenge is not just to make accurate predictions but also to understand the underlying mechanisms for those predictions. Good interpretation is important in scientific workflows, as it allows for better decision-making by providing insights into the data. This paper introduces an automatic way of obtaining such interpretable-by-design models, by learning programs that interleave neural networks. We propose DiSciPLE (Discovering Scientific Programs using LLMs and Evolution) an evolutionary algorithm that leverages common sense and prior knowledge of large language models (LLMs) to create Python programs explaining visual data. Additionally, we propose two improvements: a program critic and a program simplifier to improve our method further to synthesize good programs. On three different real world problems, DiSciPLE learns state-of-the-art programs on novel tasks with no prior literature. For example, we can learn programs with 35% lower error than the closest non-interpretable baseline for population density estimation",
    "checked": true,
    "id": "7436dca1d767f13595a1d76c9e4b84a0023dd6ec",
    "semantic_title": "disciple: learning interpretable programs for scientific visual discovery",
    "citation_count": 1,
    "authors": [
      "Utkarsh Mall",
      "Cheng Perng Phoo",
      "Mia Chiquier",
      "Bharath Hariharan",
      "Kavita Bala",
      "Carl Vondrick"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_SOLAMI_Social_Vision-Language-Action_Modeling_for_Immersive_Interaction_with_3D_Autonomous_CVPR_2025_paper.html": {
    "title": "SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters",
    "volume": "main",
    "abstract": "Human beings are social animals. How to equip 3D autonomous characters with similar social intelligence that can perceive, understand and interact with humans remains an open yet foundamental problem. In this paper, we introduce SOLAMI, the first end-to-end Social vision-Language-Action (VLA) Modeling framework for Immersive interaction with 3D autonomous characters. Specifically, SOLAMI builds 3D autonomous characters from three aspects: 1) Social VLA Architecture: We propose a unified social VLA framework to generate multimodal response (speech and motion) based on the user's multimodal input to drive the character for social interaction. 2) Interactive Multimodal Data: We present SynMSI, a synthetic multimodal social interaction dataset generated by an automatic pipeline using only existing motion datasets to address the issue of data scarcity. 3) Immersive VR Interface: We develop a VR interface that enables users to immersively interact with these characters driven by various architectures. Extensive quantitative experiments and user studies demonstrate that our framework leads to more precise and natural character responses (in both speech and motion) that align with user expectations with lower latency",
    "checked": true,
    "id": "8ce9302cf87ca20aba04bc33ca8f37e59bbd5554",
    "semantic_title": "solami: social vision-language-action modeling for immersive interaction with 3d autonomous characters",
    "citation_count": 6,
    "authors": [
      "Jianping Jiang",
      "Weiye Xiao",
      "Zhengyu Lin",
      "Huaizhong Zhang",
      "Tianxiang Ren",
      "Yang Gao",
      "Zhiqian Lin",
      "Zhongang Cai",
      "Lei Yang",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_EntropyMark_Towards_More_Harmless_Backdoor_Watermark_via_Entropy-based_Constraint_for_CVPR_2025_paper.html": {
    "title": "EntropyMark: Towards More Harmless Backdoor Watermark via Entropy-based Constraint for Open-source Dataset Copyright Protection",
    "volume": "main",
    "abstract": "High-quality open-source datasets are essential for advancing deep neural networks. However, the unauthorized commercial use of these datasets has raised significant concerns about copyright protection. One promising approach is backdoor watermark-based dataset ownership verification (BW-DOV), in which dataset protectors implant specific backdoors into illicit models through dataset watermarking, enabling the tracing of these models through abnormal prediction behaviors. Unfortunately, the targeted nature of these BW-DOV methods can be maliciously exploited, potentially leading to harmful side effects. While existing harmless methods attempt to mitigate these risks, watermarked datasets can still negatively affect prediction results, partially compromising dataset functionality. In this paper, we propose a more harmless backdoor watermark, called EntropyMark, which improves prediction confidence without altering the final prediction results. For this purpose, an entropy-based constraint is introduced to regulate the probability distribution. Specifically, we design an iterative clean-label dataset watermarking framework. Our framework employs gradient matching and adaptive data selection to optimize backdoor injection. In parallel, we introduce a hypothesis test method grounded in entropy inconsistency to verify dataset ownership. Extensive experiments on benchmark datasets demonstrate the effectiveness, transferability, and defense resistance of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Sun",
      "Rui Wang",
      "Zixuan Zhu",
      "Lihua Jing",
      "Yuanfang Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_Adaptive_Markup_Language_Generation_for_Contextually-Grounded_Visual_Document_Understanding_CVPR_2025_paper.html": {
    "title": "Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding",
    "volume": "main",
    "abstract": "Visual Document Understanding has become essential with the increase of text-rich visual content. This field poses significant challenges due to the need for effective integration of visual perception and textual comprehension, particularly across diverse document types with complex layouts. Moreover, existing fine-tuning datasets for this domain often fall short in providing the detailed contextual information for robust understanding, leading to hallucinations and limited comprehension of spatial relationships among visual elements. To address these challenges, we propose an innovative pipeline that utilizes adaptive generation of markup languages, such as Markdown, JSON, HTML, and TiKZ, to build highly structured document representations and deliver contextually-grounded responses. We introduce two fine-grained structured datasets: DocMark-Pile, comprising approximately 3.8M pretraining data pairs for document parsing, and DocMark-Instruct, featuring 624k fine-tuning data annotations for grounded instruction following.Extensive experiments demonstrate that our proposed model significantly outperforms existing state-of-the-art MLLMs across a range of visual document understanding benchmarks, facilitating advanced reasoning and comprehension capabilities in complex visual scenarios",
    "checked": true,
    "id": "859d187ca4985c42fdea4909d614e2be0785a270",
    "semantic_title": "adaptive markup language generation for contextually-grounded visual document understanding",
    "citation_count": 1,
    "authors": [
      "Han Xiao",
      "Yina Xie",
      "Guanxin Tan",
      "Yinghao Chen",
      "Rui Hu",
      "Ke Wang",
      "Aojun Zhou",
      "Hao Li",
      "Hao Shao",
      "Xudong Lu",
      "Peng Gao",
      "Yafei Wen",
      "Xiaoxin Chen",
      "Shuai Ren",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_BARD-GS_Blur-Aware_Reconstruction_of_Dynamic_Scenes_via_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian Splatting",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) has shown remarkable potential for static scene reconstruction, and recent advancements have extended its application to dynamic scenes. However, the quality of reconstructions depends heavily on high-quality input images and precise camera poses, which is not that trivial to fulfill in the real-world scenarios. Capturing dynamic scenes with handheld monocular cameras, for instance, typically involves simultaneous movement of both the camera and objects within a single exposure. This combined motion frequently results in image blur that existing methods cannot adequately handle. To address these challenges, we introduce BARD-GS, a novel approach for robust dynamic scene reconstruction that effectively handles blurry inputs and imprecise camera poses. Our method comprises two main components: 1) camera motion deblurring and 2) object motion deblurring. By explicitly decomposing motion blur into camera motion blur and object motion blur and modeling them separately, we achieve significantly improved rendering results in dynamic regions. In addition, we collect a real-world motion blur dataset of dynamic scenes to evaluate our approach. Extensive experiments demonstrate that BARD-GS effectively reconstructs high-quality dynamic scenes under realistic conditions, significantly outperforming existing methods",
    "checked": true,
    "id": "62543038d0e7b399bb39bf85c8ab11c149330dc4",
    "semantic_title": "bard-gs: blur-aware reconstruction of dynamic scenes via gaussian splatting",
    "citation_count": 3,
    "authors": [
      "Yiren Lu",
      "Yunlai Zhou",
      "Disheng Liu",
      "Tuo Liang",
      "Yu Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hong_SALAD_Skeleton-aware_Latent_Diffusion_for_Text-driven_Motion_Generation_and_Editing_CVPR_2025_paper.html": {
    "title": "SALAD: Skeleton-aware Latent Diffusion for Text-driven Motion Generation and Editing",
    "volume": "main",
    "abstract": "Text-driven motion generation has advanced significantly with the rise of denoising diffusion models. However, previous methods often oversimplify representations for the skeletal joints, temporal frames, and textual words, limiting their ability to fully capture the information within each modality and their interactions. Moreover, when using pre-trained models for downstream tasks, such as editing, they typically require additional efforts, including manual interventions, optimization, or fine-tuning. In this paper, we introduce a skeleton-aware latent diffusion (SALAD), a model that explicitly captures the intricate inter-relationships between joints, frames, and words. Furthermore, by leveraging cross-attention maps produced during the generation process, we enable the attention-based zero-shot text-driven motion editing using a pre-trained SALAD model, requiring no additional user input beyond text prompts. Our approach significantly outperforms previous methods in terms of text-motion alignment without compromising generation quality, and demonstrates practical versatility by providing diverse editing capabilities beyond generation. Code is available at project page",
    "checked": true,
    "id": "387a3698b591ce32a21604c05d1977848f70911c",
    "semantic_title": "salad: skeleton-aware latent diffusion for text-driven motion generation and editing",
    "citation_count": 4,
    "authors": [
      "Seokhyeon Hong",
      "Chaelin Kim",
      "Serin Yoon",
      "Junghyun Nam",
      "Sihun Cha",
      "Junyong Noh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Towards_Universal_AI-Generated_Image_Detection_by_Variational_Information_Bottleneck_Network_CVPR_2025_paper.html": {
    "title": "Towards Universal AI-Generated Image Detection by Variational Information Bottleneck Network",
    "volume": "main",
    "abstract": "The rapid advancement of generative models has significantly improved the quality of generated images. Meanwhile, it challenges information authenticity and credibility. Current generated image detection methods based on large-scale pre-trained multimodal models have achieved impressive results. Although these models provide abundant features, the authentication task-related features are often submerged. Consequently, those authentication task-irrelated features cause models to learn superficial biases, thereby harming their generalization performance across different model genera (e.g., GANs and Diffusion Models). To this end, we proposed VIB-Net, which uses Variational Information Bottlenecks to enforce authentication task-related feature learning. We tested and analyzed the proposed method and existing methods on samples generated by 17 different generative models. Compared to SOTA methods, VIB-Net achieved a 5.55% improvement in mAP and a 9.33% increase in accuracy. Notably, in generalization tests on unseen generative models from different series, VIB-Net improved mAP by 12.48% and accuracy by 23.59% over SOTA methods. The code is available at https://github.com/oceanzhf/VIBAIGCDetect",
    "checked": true,
    "id": "e9317ec76a0df353928f014a3ca727eb502e97e6",
    "semantic_title": "towards universal ai-generated image detection by variational information bottleneck network",
    "citation_count": 2,
    "authors": [
      "Haifeng Zhang",
      "Qinghui He",
      "Xiuli Bi",
      "Weisheng Li",
      "Bo Liu",
      "Bin Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_HSI_A_Holistic_Style_Injector_for_Arbitrary_Style_Transfer_CVPR_2025_paper.html": {
    "title": "HSI: A Holistic Style Injector for Arbitrary Style Transfer",
    "volume": "main",
    "abstract": "Attention-based arbitrary style transfer methods have gained significant attention recently due to their impressive ability to synthesize style details. However, the point-wise matching within the attention mechanism may overly focus on local patterns such that neglect the remarkable global features of style images. Additionally, when processing large images, the quadratic complexity of the attention mechanism will bring high computational load. To alleviate above problems, we propose Holistic Style Injector (HSI), a novel attention-style transformation module to deliver artistic expression of target style. Specifically, HSI performs stylization only based on global style representation that is more in line with the characteristics of style transfer, to avoid generating local disharmonious patterns in stylized images. Moreover, we propose a dual relation learning mechanism inside the HSI to dynamically render images by leveraging semantic similarity in content and style, ensuring the stylized images preserve the original content and improve style fidelity. Note that the proposed HSI achieves linear computational complexity because it establishes feature mapping through element-wise multiplication rather than matrix multiplication. Qualitative and quantitative results demonstrate that our method outperforms state-of-the-art approaches in both effectiveness and efficiency",
    "checked": true,
    "id": "caa67b35ce6854389957793e16533df75b11c9af",
    "semantic_title": "hsi: a holistic style injector for arbitrary style transfer",
    "citation_count": 0,
    "authors": [
      "Shuhao Zhang",
      "Hui Kang",
      "Yang Liu",
      "Fang Mei",
      "Hongjuan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping_CVPR_2025_paper.html": {
    "title": "LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping",
    "volume": "main",
    "abstract": "Anamorphosis refers to a category of images that are intentionally distorted, making them unrecognizable when viewed directly. Their true form only reveals itself when seen from a specific viewpoint, which can be through some catadioptric device like a mirror or a lens. While the construction of these mathematical devices can be traced back to as early as the 17th century, they are only interpretable when viewed from a specific vantage point and tend to lose meaning when seen normally. In this paper, we revisit these famous optical illusions with a generative twist. With the help of latent rectified flow models, we propose a method to create anamorphic images that still retain a valid interpretation when viewed directly. To this end, we introduce Laplacian Pyramid Warping, a frequency-aware image warping technique key to generating high-quality visuals. Our work extends Visual Anagrams (Geng et al. 2024) to latent space models and to a wider range of spatial transforms, enabling the creation of novel generative perceptual illusions",
    "checked": true,
    "id": "1086671036364274970c7617135442ac9a4058ad",
    "semantic_title": "lookingglass: generative anamorphoses via laplacian pyramid warping",
    "citation_count": 0,
    "authors": [
      "Pascal Chang",
      "Sergio Sancho",
      "Jingwei Tang",
      "Markus Gross",
      "Vinicius Azevedo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_V2V3D_View-to-View_Denoised_3D_Reconstruction_for_Light_Field_Microscopy_CVPR_2025_paper.html": {
    "title": "V2V3D: View-to-View Denoised 3D Reconstruction for Light Field Microscopy",
    "volume": "main",
    "abstract": "Light field microscopy (LFM) has gained significant attention due to its ability to capture snapshot-based, large-scale 3D fluorescence images. However, existing LFM reconstruction algorithms are highly sensitive to sensor noise or require hard-to-get ground-truth annotated data for training. To address these challenges, this paper introduces V2V3D, an unsupervised view2view-based framework that establishes a new paradigm for joint optimization of image denoising and 3D reconstruction in a unified architecture. We assume that the LF images are derived from a consistent 3D signal, with the noise in each view being independent. This enables V2V3D to incorporate the principle of noise2noise for effective denoising. To enhance the recovery of high-frequency details, we propose a novel wave-optics-based feature alignment technique, which transforms the point spread function, used for forward propagation in wave optics, into convolution kernels specifically designed for feature alignment. Moreover, we introduce an LFM dataset containing LF images and their corresponding 3D intensity volumes. Extensive experiments demonstrate that our approach achieves high computational efficiency and outperforms the other state-of-the-art methods. These advancements position V2V3D as a promising solution for 3D imaging under challenging conditions. Our code and dataset will be publicly accessible at https://joey1998hub.github.io/V2V3D/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayin Zhao",
      "Zhenqi Fu",
      "Tao Yu",
      "Hui Qiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_DiN_Diffusion_Model_for_Robust_Medical_VQA_with_Semantic_Noisy_CVPR_2025_paper.html": {
    "title": "DiN: Diffusion Model for Robust Medical VQA with Semantic Noisy Labels",
    "volume": "main",
    "abstract": "Medical Visual Question Answering (Med-VQA) systems benefit the interpretation of medical images containing critical clinical information. However, the challenge of noisy labels and limited high-quality datasets remains underexplored. To address this, we establish the first benchmark for noisy labels in Med-VQA by simulating human mislabeling with semantically designed noise types. More importantly, we introduce the DiN framework, which leverages a diffusion model to handle noisy labels in Med-VQA. Unlike the dominant classification-based VQA approaches that directly predict answers, our Answer Diffuser (AD) module employs a coarse-to-fine process, refining answer candidates with a diffusion model for improved accuracy. The Answer Condition Generator (ACG) further enhances this process by generating task-specific conditional information via integrating answer embeddings with fused image-question features. To address label noise, our Noisy Label Refinement(NLR) module introduces a robust loss function and dynamic answer adjustment to further boost the performance of the AD module. Our DiN framework consistently outperforms existing methods across multiple benchmarks with varying noise levels",
    "checked": true,
    "id": "144783f08621cee07e222ed4ca9b18ba50e786d9",
    "semantic_title": "din: diffusion model for robust medical vqa with semantic noisy labels",
    "citation_count": 1,
    "authors": [
      "Erjian Guo",
      "Zhen Zhao",
      "Zicheng Wang",
      "Tong Chen",
      "Yunyi Liu",
      "Luping Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Splatter-360_Generalizable_360_Gaussian_Splatting_for_Wide-baseline_Panoramic_Images_CVPR_2025_paper.html": {
    "title": "Splatter-360: Generalizable 360 Gaussian Splatting for Wide-baseline Panoramic Images",
    "volume": "main",
    "abstract": "Wide-baseline panoramic images are frequently used in applications like VR and simulations to minimize capturing labor costs and storage needs. However, synthesizing novel views from these panoramic images in real time remains a significant challenge, especially due to panoramic imagery's high resolution and inherent distortions. Although existing 3D Gaussian splatting (3DGS) methods can produce photo-realistic views under narrow baselines, they often overfit the training views when dealing with wide-baseline panoramic images due to the difficulty in learning precise geometry from sparse 360^ \\circ views. This paper presents Splatter-360, a novel end-to-end generalizable 3DGS framework designed to handle wide-baseline panoramic images. Unlike previous approaches, Splatter-360 performs multi-view matching directly in the spherical domain by constructing a spherical cost volume through a spherical sweep algorithm, enhancing the network's depth perception and geometry estimation. Additionally, we introduce a 3D-aware bi-projection encoder to mitigate the distortions inherent in panoramic images and integrate cross-view attention to improve feature interactions across multiple viewpoints. This enables robust 3D-aware feature representations and real-time rendering capabilities. Experimental results on the HM3D and Replica demonstrate that Splatter-360 significantly outperforms state-of-the-art NeRF and 3DGS methods (e.g., PanoGRF, MVSplat, DepthSplat, and HiSplat) in both synthesis quality and generalization performance for wide-baseline panoramic images. Code and trained models are available at https://3d-aigc.github.io/Splatter-360/",
    "checked": false,
    "id": "1af2084341a26fb321bebddcccf9493e43c9e207",
    "semantic_title": "splatter-360: generalizable 360° gaussian splatting for wide-baseline panoramic images",
    "citation_count": 7,
    "authors": [
      "Zheng Chen",
      "Chenming Wu",
      "Zhelun Shen",
      "Chen Zhao",
      "Weicai Ye",
      "Haocheng Feng",
      "Errui Ding",
      "Song-Hai Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_ShowMak3r_Compositional_TV_Show_Reconstruction_CVPR_2025_paper.html": {
    "title": "ShowMak3r: Compositional TV Show Reconstruction",
    "volume": "main",
    "abstract": "Reconstructing dynamic radiance fields from video clips is challenging, especially when entertainment videos like TV shows are given. Many challenges make the reconstruction difficult due to (1) actors occluding with each other and having diverse facial expressions, (2) cluttered stages, and (3) small baseline views or sudden shot changes. To address these issues, we present ShowMak3r, a comprehensive reconstruction pipeline that allows the editing of scenes like how video clips are made in a production control room. In ShowMak3r, a 3DLocator module locates recovered actors on the stage using depth prior, and estimates unseen human poses via interpolation. The proposed ShotMatcher module then tracks the actors under shot changes. Furthermore, ShowMak3r introduces a face-fitting network that dynamically recovers the actors' expressions. Experiments on Sitcoms3D dataset show that our pipeline can reassemble TV show scenes with new cameras at different timestamps. We also demonstrate that ShowMak3r enables interesting applications such as synthetic shot-making, actor relocation, insertion, deletion, and pose manipulation. Project page : https://nstar1125.github.io/showmak3r",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangmin Kim",
      "Seunguk Do",
      "Jaesik Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ling_CADRef_Robust_Out-of-Distribution_Detection_via_Class-Aware_Decoupled_Relative_Feature_Leveraging_CVPR_2025_paper.html": {
    "title": "CADRef: Robust Out-of-Distribution Detection via Class-Aware Decoupled Relative Feature Leveraging",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) have been widely criticized for their overconfidence when dealing with out-of-distribution (OOD) samples, highlighting the critical need for effective OOD detection to ensure the safe deployment of DNNs in real-world settings. Existing post-hoc OOD detection methods primarily enhance the discriminative power of logit-based approaches by reshaping sample features, yet they often neglect critical information inherent in the features themselves. In this paper, we propose the \\underline C lass-\\underline A ware \\underline Re lative \\underline F eature-based method (CARef), which utilizes the error between a sample's feature and its class-aware average feature as a discriminative criterion. To further refine this approach, we introduce the \\underline C lass-\\underline A ware \\underline D ecoupled \\underline Re lative \\underline F eature-based method (CADRef), which decouples sample features based on the alignment of signs between the relative feature and corresponding model weights, enhancing the discriminative capabilities of CARef.Extensive experimental results across multiple datasets and models demonstrate that both proposed methods exhibit effectiveness and robustness in OOD detection compared to state-of-the-art methods. Specifically, our two methods outperform the best baseline by 2.82% and 3.27% in AUROC, with improvements of 4.03% and 6.32% in FPR95, respectively",
    "checked": true,
    "id": "fe67d07f5224d9d0d00da5fe7a9f8bd8e2710ac6",
    "semantic_title": "cadref: robust out-of-distribution detection via class-aware decoupled relative feature leveraging",
    "citation_count": 1,
    "authors": [
      "Zhiwei Ling",
      "Yachen Chang",
      "Hailiang Zhao",
      "Xinkui Zhao",
      "Kingsum Chow",
      "Shuiguang Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_S3-Face_SSS-Compliant_Facial_Reflectance_Estimation_via_Diffusion_Priors_CVPR_2025_paper.html": {
    "title": "S^3-Face: SSS-Compliant Facial Reflectance Estimation via Diffusion Priors",
    "volume": "main",
    "abstract": "Recent 3D face reconstruction methods have made remarkable advancements, yet achieving high-quality facial reflectance from monocular input remains challenging. Existing methods rely on the light-stage captured data to learn facial reflectance models. However, limited subject diversity in these datasets poses challenges in achieving good generalization and broad applicability. This motivates us to explore whether the extensive priors captured in recent generative diffusion models (e.g., Stable Diffusion) can enable more generalizable facial reflectance estimation as these models have been pre-trained on large-scale internet image collections containing rich visual patterns. In this paper, we introduce the use of Stable Diffusion as a prior for facial reflectance estimation, achieving robust results with minimal captured data for fine-tuning. We present S^3-Face, a comprehensive framework capable of producing SSS-compliant skin reflectance from in-the-wild images. Our method adopts a two-stage training approach: in the first stage, DSN-Net is trained to predict diffuse albedo, specular albedo, and normal maps from in-the-wild images using a novel joint reflectance attention module. In the second stage, HM-Net is trained to generate hemoglobin and melanin maps based on the diffuse albedo predicted in the first stage, yielding SSS-compliant and detailed reflectance maps. Extensive experiments demonstrate that our method achieves strong generalization and produces high-fidelity, SSS-compliant facial reflectance reconstructions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Ren",
      "Jiankang Deng",
      "Yuhao Cheng",
      "Wenhan Zhu",
      "Yichao Yan",
      "Xiaokang Yang",
      "Stefanos Zafeiriou",
      "Chao Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_FSBench_A_Figure_Skating_Benchmark_for_Advancing_Artistic_Sports_Understanding_CVPR_2025_paper.html": {
    "title": "FSBench: A Figure Skating Benchmark for Advancing Artistic Sports Understanding",
    "volume": "main",
    "abstract": "Figure skating, known as the \"Art on Ice,\" is among the most artistic sports, challenging to understand due to its blend of technical elements (like jumps and spins) and overall artistic expression. Existing figure skating datasets mainly focus on single tasks, such as action recognition or scoring, lacking comprehensive annotations for both technical and artistic evaluation. Current sports research is largely centered on ball games, with limited relevance to artistic sports like figure skating. To address this, we introduce FSAnno, a large-scale dataset advancing artistic sports understanding through figure skating. FSAnno includes an open-access training and test dataset, alongside a benchmark dataset, FSBench, for fair model evaluation. FSBench consists of FSBench-Text, with multiple-choice questions and explanations, and FSBench-Motion, containing multimodal data and Question and Answer (QA) pairs, supporting tasks from technical analysis to performance commentary. Initial tests on FSBench reveal significant limitations in existing models' understanding of artistic sports. We hope FSBench will become a key tool for evaluating and enhancing model comprehension of figure skating. All data, models, and more details are available at: https://github.com/Moomin-Fin/Ano",
    "checked": true,
    "id": "fd85290f9c79790b21c976eb88e1cb695258e78a",
    "semantic_title": "fsbench: a figure skating benchmark for advancing artistic sports understanding",
    "citation_count": 0,
    "authors": [
      "Rong Gao",
      "Xin Liu",
      "Zhuozhao Hu",
      "Bohao Xing",
      "Baiqiang Xia",
      "Zitong Yu",
      "Heikki Kälviäinen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic_CVPR_2025_paper.html": {
    "title": "Keep the Balance: A Parameter-Efficient Symmetrical Framework for RGB+X Semantic Segmentation",
    "volume": "main",
    "abstract": "Multimodal semantic segmentation is a critical challenge in computer vision, with early methods suffering from high computational costs and limited transferability due to full fine-tuning of RGB-based pre-trained parameters. Recent studies, while leveraging additional modalities as supplementary prompts to RGB, still predominantly rely on RGB, which restricts the full potential of other modalities. To address these issues, we propose a novel symmetric parameter-efficient fine-tuning framework for multimodal segmentation, featuring with a modality-aware prompting and adaptation scheme, to simultaneously adapt the capabilities of a powerful pre-trained model to both RGB and X modalities. Furthermore, prevalent approaches use the global cross-modality correlations of attention mechanism for modality fusion, which inadvertently introduces noise across modalities. To mitigate this noise, we propose a dynamic sparse cross-modality fusion module to facilitate effective and efficient cross-modality fusion. To further strengthen the above two modules, we propose a training strategy that leverages accurately predicted dual-modality results to self-teach the single-modality outcomes. In comprehensive experiments, we demonstrate that our method outperforms previous state-of-the-art approaches across six multimodal segmentation scenarios with minimal computation cost",
    "checked": true,
    "id": "8ff0764099bb3931d41e2919c0c2d918e107eb3c",
    "semantic_title": "keep the balance: a parameter-efficient symmetrical framework for rgb+x semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Jiaxin Cai",
      "Jingze Su",
      "Qi Li",
      "Wenjie Yang",
      "Shu Wang",
      "Tiesong Zhao",
      "Shengfeng He",
      "Wenxi Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VideoDirector_Precise_Video_Editing_via_Text-to-Video_Models_CVPR_2025_paper.html": {
    "title": "VideoDirector: Precise Video Editing via Text-to-Video Models",
    "volume": "main",
    "abstract": "Despite the typical inversion-then-editing paradigm using text-to-image (T2I) models has demonstrated promising results, directly extending it to text-to-video (T2V) models still suffers severe artifacts such as color flickering and content distortion. Consequently, current video editing methods primarily rely on T2I models, which inherently lack temporal-coherence generative ability, often resulting in inferior editing results. In this paper, we attribute the failure of the typical editing paradigm to: 1) Tightly Spatial-temporal Coupling. The vanilla pivotal-based inversion strategy struggles to disentangle spatial-temporal information in the video diffusion model; 2) Complicated Spatial-temporal Layout. The vanilla cross-attention control strategy is deficient in preserving the unedited content. To address these limitations, we propose a spatial-temporal decoupled guidance (STDG) and multi-frame null-text optimization strategy to provide pivotal temporal cues for more precise pivotal inversion. Furthermore, we introduce a self-attention control strategy to maintain higher fidelity for precise partial content editing. Experimental results demonstrate that our method (termed VideoDirector) effectively harnesses the powerful temporal generation capabilities of T2V models, producing edited videos with state-of-the-art performance in accuracy, motion smoothness, realism, and fidelity to unedited content",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yukun Wang",
      "Longguang Wang",
      "Zhiyuan Ma",
      "Qibin Hu",
      "Kai Xu",
      "Yulan Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lai_LLM-driven_Multimodal_and_Multi-Identity_Listening_Head_Generation_CVPR_2025_paper.html": {
    "title": "LLM-driven Multimodal and Multi-Identity Listening Head Generation",
    "volume": "main",
    "abstract": "Generating natural listener responses in conversational scenarios is crucial for creating engaging digital humans and avatars. Recent work has shown that large language models (LLMs) can be effectively leveraged for this task, demonstrating remarkable capabilities in generating contextually appropriate listener behaviors. However, current LLM-based methods face two critical limitations: they rely solely on speech content, overlooking other crucial communication signals, and they entangle listener identity with response generation, compromising output fidelity and generalization. In this work, we present a novel framework that addresses these limitations while maintaining the advantages of LLMs. Our approach introduces a Multimodal-LM architecture that jointly processes speech content, acoustics, and speaker emotion, capturing the full spectrum of communication cues. Additionally, we propose an identity disentanglement strategy using instance normalization and adaptive instance normalization in a VQ-VAE framework, enabling high-fidelity listening head synthesis with flexible identity control. Extensive experiments demonstrate that our method significantly outperforms existing approaches in terms of response naturalness and fidelity, while enabling effective identity control without retraining",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiwen Lai",
      "Weizhi Zhong",
      "Yipeng Qin",
      "Xiaohang Ren",
      "Baoyuan Wang",
      "Guanbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Towards_Understanding_How_Knowledge_Evolves_in_Large_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "Towards Understanding How Knowledge Evolves in Large Vision-Language Models",
    "volume": "main",
    "abstract": "Large Vision-Language Models (LVLMs) are gradually becoming the foundation for many artificial intelligence applications. However, understanding their internal working mechanisms has continued to puzzle researchers, which in turn limits the further enhancement of their capabilities. In this paper, we seek to investigate how multimodal knowledge evolves and eventually induces natural languages in LVLMs. We design a series of novel strategies for analyzing internal knowledge within LVLMs, and delve into the evolution of multimodal knowledge from three levels, including single token probabilities, token probability distributions, and feature encodings. In this process, we identify two key nodes in knowledge evolution: the critical layers and the mutation layers, dividing the evolution process into three stages: rapid evolution, stabilization, and mutation. Our research is the first to reveal the trajectory of knowledge evolution in LVLMs, providing a fresh perspective for understanding their underlying mechanisms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sudong Wang",
      "Yunjian Zhang",
      "Yao Zhu",
      "Jianing Li",
      "Zizhe Wang",
      "Yanwei Liu",
      "Xiangyang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kumar_A_Unified_Resilient_and_Explainable_Adversarial_Patch_Detector_CVPR_2025_paper.html": {
    "title": "A Unified, Resilient, and Explainable Adversarial Patch Detector",
    "volume": "main",
    "abstract": "Deep Neural Networks (DNNs), backbone architecture in `almost' every computer vision task, are vulnerable to adversarial attacks, particularly physical out-of-distribution (OOD) adversarial patches. Existing defense models often struggle with interpreting these attacks in ways that align with human visual perception. Our proposed AdvPatchXAI approach introduces a generalized, robust, and explainable defense algorithm designed to defend DNNs against physical adversarial threats. AdvPatchXAI employs a novel patch decorrelation loss that reduces feature redundancy and enhances the distinctiveness of patch representations, enabling better generalization across unseen adversarial scenarios. It learns prototypical parts self-supervised, enhancing interpretability and correlation with human vision. The model utilizes a sparse linear layer for classification, making the decision process globally interpretable through a set of learned prototypes and locally explainable by pinpointing relevant prototypes within an image. Our comprehensive evaluation shows that AdvPatchXAI closes the \"semantic\" gap between latent space and pixel space and effectively handles unseen adversarial patches even perturbed with unseen corruptions, thereby significantly advancing DNN robustness in practical settings(https://github.com/tbvl22/Unified-resilient-and-Explainable-Adversarial-Patch-detector)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vishesh Kumar",
      "Akshay Agarwal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_VISTA_Enhancing_Long-Duration_and_High-Resolution_Video_Understanding_by_Video_Spatiotemporal_CVPR_2025_paper.html": {
    "title": "VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation",
    "volume": "main",
    "abstract": "Current large multimodal models (LMMs) face significant challenges in processing and comprehending long-duration or high-resolution videos, which is mainly due to the lack of high-quality datasets. To address this issue from a data-centric perspective, we propose VISTA, a simple yet effective video spatiotemporal augmentation framework that synthesizes long-duration and high-resolution video instruction-following pairs from existing video-caption datasets. VISTA spatially and temporally combines videos to create new synthetic videos with extended durations and enhanced resolutions, and subsequently produces question-answer pairs pertaining to these newly synthesized videos. Based on this paradigm, we develop seven video augmentation methods and curate VISTA-400K, a video instruction-following dataset aimed at enhancing long-duration and high-resolution video understanding. Finetuning various video LMMs on our data resulted in an average improvement of 3.3% across four challenging benchmarks for long-video understanding. Furthermore, we introduce the first comprehensive high-resolution video understanding benchmark HRVideoBench, on which our finetuned models achieve a 6.5% performance gain. These results highlight the effectiveness of our framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiming Ren",
      "Huan Yang",
      "Jie Min",
      "Cong Wei",
      "Wenhu Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation_CVPR_2025_paper.html": {
    "title": "Structured 3D Latents for Scalable and Versatile 3D Generation",
    "volume": "main",
    "abstract": "We introduce a novel 3D generation method for versatile and high-quality 3D asset creation.The cornerstone is a unified Structured LATent (SLAT) representation which allows decoding to different output formats, such as Radiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a sparsely-populated 3D grid with dense multiview visual features extracted from a powerful vision foundation model, comprehensively capturing both structural (geometry) and textural (appearance) information while maintaining flexibility during decoding.We employ rectified flow transformers tailored for SLAT as our 3D generation models and train models with up to 2 billion parameters on a large 3D asset dataset of 500K diverse objects. Our model generates high-quality results with text or image conditions, significantly surpassing existing methods, including recent ones at similar scales. We showcase flexible output format selection and local 3D editing capabilities which were not offered by previous models. Code, model, and data will be released",
    "checked": true,
    "id": "5666d551bc9f86e2f379ede8b8ffddbe4d1d53a8",
    "semantic_title": "structured 3d latents for scalable and versatile 3d generation",
    "citation_count": 211,
    "authors": [
      "Jianfeng Xiang",
      "Zelong Lv",
      "Sicheng Xu",
      "Yu Deng",
      "Ruicheng Wang",
      "Bowen Zhang",
      "Dong Chen",
      "Xin Tong",
      "Jiaolong Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kawana_GA3CE_Unconstrained_3D_Gaze_Estimation_with_Gaze-Aware_3D_Context_Encoding_CVPR_2025_paper.html": {
    "title": "GA3CE: Unconstrained 3D Gaze Estimation with Gaze-Aware 3D Context Encoding",
    "volume": "main",
    "abstract": "We propose a novel 3D gaze estimation approach that learns spatial relationships between the subject and objects in the scene, and outputs 3D gaze direction. Our method targets unconstrained settings, including cases where close-up views of the subject's eyes are unavailable, such as when the subject is distant or facing away. Previous approaches typically rely on either 2D appearance alone or incorporate limited spatial cues using depth maps in the non-learnable post-processing step. Estimating 3D gaze direction from 2D observations in these scenarios is challenging; variations in subject pose, scene layout, and gaze direction, combined with differing camera poses, yield diverse 2D appearances and 3D gaze directions even when targeting the same 3D scene. To address this issue, we propose GA3CE: Gaze-Aware 3D Context Encoding. Our method represents subject and scene using 3D poses and object positions, treating them as 3D context to learn spatial relationships in 3D space. Inspired by human vision, we align this context in an egocentric space, significantly reducing spatial complexity. Furthermore, we propose D^3 (direction-distance-decomposed) positional encoding to better capture the spatial relationship between 3D context and gaze direction in direction and distance space. Experiments demonstrate substantial improvements, reducing mean angle error by 13%-37% compared to leading baselines on benchmark datasets in single-frame settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuki Kawana",
      "Shintaro Shiba",
      "Quan Kong",
      "Norimasa Kobori"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qiu_Self-Cross_Diffusion_Guidance_for_Text-to-Image_Synthesis_of_Similar_Subjects_CVPR_2025_paper.html": {
    "title": "Self-Cross Diffusion Guidance for Text-to-Image Synthesis of Similar Subjects",
    "volume": "main",
    "abstract": "Diffusion models achieved unprecedented fidelity and diversity for synthesizing image, video, 3D assets, etc. However, subject mixing is an unresolved issue for diffusion-based image synthesis, particularly for synthesizing multiple similar-looking subjects. We propose Self-Cross Diffusion Guidance to penalize the overlap between cross-attention maps and the aggregated self-attention map. Compared to previous methods based on self-attention or cross-attention alone, our guidance is more effective in eliminating subject mixing. What's more, our guidance addresses subject mixing for all relevant patches beyond the most discriminant one, e.g., the beak of a bird. For each subject, we aggregate self-attention maps of patches with higher cross-attention values. Thus, the aggregated self-attention map forms a region that the whole subject attends to. Our training-free method boosts the performance of both Unet-based and Transformer-based diffusion models such as the Stable Diffusion series. We also release a similar subjects dataset (SSD), a challenging benchmark, and utilize GPT-4o for automatic and reliable evaluation. Extensive qualitative and quantitative results demonstrate the effectiveness of our self-cross diffusion guidance",
    "checked": true,
    "id": "917cfbdb225f9f6f45ae5c7be1fc4ca5741a0886",
    "semantic_title": "self-cross diffusion guidance for text-to-image synthesis of similar subjects",
    "citation_count": 3,
    "authors": [
      "Weimin Qiu",
      "Jieke Wang",
      "Meng Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yao_RigGS_Rigging_of_3D_Gaussians_for_Modeling_Articulated_Objects_in_CVPR_2025_paper.html": {
    "title": "RigGS: Rigging of 3D Gaussians for Modeling Articulated Objects in Videos",
    "volume": "main",
    "abstract": "This paper considers the problem of modeling articulated objects captured in 2D videos to enable novel view synthesis, while also being easily editable, drivable, and reposable. To tackle this challenging problem, we propose RigGS, a new paradigm that leverages 3D Gaussian representation and skeleton-based motion representation to model dynamic objects without utilizing additional template priors. Specifically, we first propose skeleton-aware node-controlled deformation, which deforms a canonical 3D Gaussian representation over time to initialize the modeling process, producing candidate skeleton nodes that are further simplified into a sparse 3D skeleton according to their motion and semantic information. Subsequently, based on the resulting skeleton, we design learnable skin deformations and pose-dependent detailed deformations, thereby easily deforming the 3D Gaussian representation to generate new actions and render further high-quality images from novel views. Extensive experiments demonstrate that our method can generate realistic new actions easily for objects and achieve high-quality rendering",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Yao",
      "Zhi Deng",
      "Junhui Hou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Noise_Modeling_in_One_Hour_Minimizing_Preparation_Efforts_for_Self-supervised_CVPR_2025_paper.html": {
    "title": "Noise Modeling in One Hour: Minimizing Preparation Efforts for Self-supervised Low-Light RAW Image Denoising",
    "volume": "main",
    "abstract": "Noise synthesis is a promising solution for addressing the data shortage problem in data-driven low-light RAW image denoising. However, accurate noise synthesis methods often necessitate labor-intensive calibration and profiling procedures during preparation, preventing them from landing to practice at scale. This work introduces a practically simple noise synthesis pipeline based on detailed analyses of noise properties and extensive justification of widespread techniques. Compared to other approaches, our proposed pipeline eliminates the cumbersome system gain calibration and signal-independent noise profiling steps, reducing the preparation time for noise synthesis from days to hours. Meanwhile, our method exhibits strong denoising performance, showing an up to 0.54dB PSNR improvement over the current state-of-the-art noise synthesis technique",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feiran Li",
      "Haiyang Jiang",
      "Daisuke Iso"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks_CVPR_2025_paper.html": {
    "title": "Adv-CPG: A Customized Portrait Generation Framework with Facial Adversarial Attacks",
    "volume": "main",
    "abstract": "Recent Customized Portrait Generation (CPG) methods, taking a facial image and a textual prompt as inputs, have attracted substantial attention. Although these methods generate high-fidelity portraits, they fail to prevent the generated portraits from being tracked and misused by malicious face recognition systems. To address this, this paper proposes a Customized Portrait Generation framework with facial Adversarial attacks (Adv-CPG). Specifically, to achieve facial privacy protection, we devise a lightweight local ID encryptor and an encryption enhancer. They implement progressive double-layer encryption protection by directly injecting the target identity and adding additional identity guidance, respectively. Furthermore, to accomplish fine-grained and personalized portrait generation, we develop a multi-modal image customizer capable of generating controlled fine-grained facial features. To the best of our knowledge, Adv-CPG is the first study that introduces facial adversarial attacks into CPG. Extensive experiments demonstrate the superiority of Adv-CPG, e.g., the average attack success rate of the proposed Adv-CPG is 28.1% and 2.86% higher compared to the SOTA noise-based attack methods and unconstrained attack methods, respectively",
    "checked": true,
    "id": "432428e09747b490ec2bd6a8bc5aecc160f06c0c",
    "semantic_title": "adv-cpg: a customized portrait generation framework with facial adversarial attacks",
    "citation_count": 9,
    "authors": [
      "Junying Wang",
      "Hongyuan Zhang",
      "Yuan Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mehrab_Fish-Vista_A_Multi-Purpose_Dataset_for_Understanding__Identification_of_Traits_CVPR_2025_paper.html": {
    "title": "Fish-Vista: A Multi-Purpose Dataset for Understanding & Identification of Traits from Images",
    "volume": "main",
    "abstract": "We introduce Fish-Visual Trait Analysis (Fish-Vista), the first organismal image dataset designed for the analysis of visual traits of aquatic species directly from images using machine learning and computer vision methods. Fish-Vista contains 69,269 annotated images spanning 4,316 fish species, curated and organized to serve three downstream tasks: species classification, trait identification, and trait segmentation. Our work makes two key contributions. First, we provide a fully reproducible data processing pipeline to process fish images sourced from various museum collections, contributing to the advancement of AI in biodiversity science. We annotate the images with carefully curated labels from biological databases and manual annotations to create an AI-ready dataset of visual traits. Second, our work offers fertile grounds for researchers to develop novel methods for a variety of problems in computer vision such as handling long-tailed distributions, out-of-distribution generalization, learning with weak labels, explainable AI, and segmenting small objects. Dataset and code for Fish-Vista are available at https://github.com/Imageomics/Fish-Vista",
    "checked": true,
    "id": "b427d30b218c3325cb801f9ba6b3a5336cf5d953",
    "semantic_title": "fish-vista: a multi-purpose dataset for understanding & identification of traits from images",
    "citation_count": 6,
    "authors": [
      "Kazi Sajeed Mehrab",
      "M. Maruf",
      "Arka Daw",
      "Abhilash Neog",
      "Harish Babu Manogaran",
      "Mridul Khurana",
      "Zhenyang Feng",
      "Bahadir Altintas",
      "Yasin Bakis",
      "Elizabeth G Campolongo",
      "Matthew J Thompson",
      "Xiaojun Wang",
      "Hilmar Lapp",
      "Tanya Berger-Wolf",
      "Paula Mabee",
      "Henry Bart",
      "Wei-Lun Chao",
      "Wasila M Dahdul",
      "Anuj Karpatne"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_High_Dynamic_Range_Video_Compression_A_Large-Scale_Benchmark_Dataset_and_CVPR_2025_paper.html": {
    "title": "High Dynamic Range Video Compression: A Large-Scale Benchmark Dataset and A Learned Bit-depth Scalable Compression Algorithm",
    "volume": "main",
    "abstract": "Recently, learned video compression (LVC) is undergoing a period of rapid development. However, due to absence of large and high-quality high dynamic range (HDR) video training data, LVC on HDR video is still unexplored. In this paper, we are the first to collect a large-scale HDR video benchmark dataset, named HDRVD2K, featuring huge quantity, diverse scenes and multiple motion types. HDRVD2K fills gaps of video training data and facilitate the development of LVC on HDR videos. Based on HDRVD2K, we further propose the first learned bit-depth scalable video compression (LBSVC) network for HDR videos by effectively exploiting bit-depth redundancy between videos of multiple dynamic ranges. To achieve this, we first propose a compression-friendly bit-depth enhancement module (BEM) to effectively predict original HDR videos based on compressed tone-mapped low dynamic range (LDR) videos and dynamic range prior, instead of reducing redundancy only through spatio-temporal predictions. Our method greatly improves the reconstruction quality and compression performance on HDR videos. Extensive experiments demonstrate the effectiveness of HDRVD2K on learned HDR video compression and great compression performance of our proposed LBSVC network",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyi Tian",
      "Feifeng Wang",
      "Shiwei Wang",
      "Zihao Zhou",
      "Yao Zhu",
      "Liquan Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lei_OffsetOPT_Explicit_Surface_Reconstruction_without_Normals_CVPR_2025_paper.html": {
    "title": "OffsetOPT: Explicit Surface Reconstruction without Normals",
    "volume": "main",
    "abstract": "Neural surface reconstruction has been dominated by implicit representations with marching cubes for explicit surface extraction. However, those methods typically require high-quality normals for accurate reconstruction. We propose OffsetOPT, a method that reconstructs explicit surfaces directly from 3D point clouds and eliminates the need for point normals. The approach comprises two stages: first, we train a neural network to predict surface triangles based on local point geometry, given uniformly distributed training point clouds. Next, we apply the frozen network to reconstruct surfaces from unseen point clouds by optimizing a per-point offset to maximize the accuracy of triangle predictions. Compared to state-of-the-art methods, OffsetOPT not only excels at reconstructing overall surfaces but also significantly preserves sharp surface features. We demonstrate its accuracy on popular benchmarks, including small-scale shapes and large-scale open surfaces",
    "checked": true,
    "id": "de057de3da9b4d471a7f3bcc3667315bfb876d36",
    "semantic_title": "offsetopt: explicit surface reconstruction without normals",
    "citation_count": 0,
    "authors": [
      "Huan Lei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/So_PCM__Picard_Consistency_Model_for_Fast_Parallel_Sampling_of_CVPR_2025_paper.html": {
    "title": "PCM : Picard Consistency Model for Fast Parallel Sampling of Diffusion Models",
    "volume": "main",
    "abstract": "Recently, diffusion models have achieved significant advances in vision, text, and robotics. However, they still face slow generation speeds due to sequential denoising processes. To address this, a parallel sampling method based on Picard iteration was introduced, effectively reducing sequential steps while ensuring exact convergence to the original output. Nonetheless, Picard iteration does not guarantee faster convergence, which can still result in slow generation in practice. In this work, we propose a new parallelization scheme, the Picard Consistency Model (PCM), which significantly reduces the number of generation steps in Picard iteration. Inspired by the consistency model, PCM is directly trained to predict the fixed-point solution, or the final output, at any stage of the convergence trajectory. Additionally, we introduce a new concept called model switching, which addresses PCM's limitations and ensures exact convergence. Extensive experiments demonstrate that PCM achieves up to a 2.71x speedup over sequential sampling and a 1.77x speedup over Picard iteration across various tasks, including image generation and robotic control",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhyuk So",
      "Jiwoong Shin",
      "Chaeyeon Jang",
      "Eunhyeok Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jang_CoMapGS_Covisibility_Map-based_Gaussian_Splatting_for_Sparse_Novel_View_Synthesis_CVPR_2025_paper.html": {
    "title": "CoMapGS: Covisibility Map-based Gaussian Splatting for Sparse Novel View Synthesis",
    "volume": "main",
    "abstract": "We propose Covisibility Map-based Gaussian Splatting (CoMapGS), designed to recover underrepresented sparse regions in sparse novel view synthesis. CoMapGS addresses both high- and low-uncertainty regions by constructing covisibility maps, enhancing initial point clouds, and applying uncertainty-aware weighted supervision using a proximity classifier. Our contributions are threefold: (1) CoMapGS reframes novel view synthesis by leveraging covisibility maps as a core component to address region-specific uncertainty; (2) Enhanced initial point clouds for both low- and high-uncertainty regions compensate for sparse COLMAP-derived point clouds, improving reconstruction quality and benefiting few-shot 3DGS methods; (3) Adaptive supervision with covisibility-score-based weighting and proximity classification achieves consistent performance gains across scenes with varying sparsity scores derived from covisibility maps. Experimental results demonstrate that CoMapGS outperforms state-of-the-art methods on datasets including Mip-NeRF 360 and LLFF",
    "checked": true,
    "id": "6c05a56b3cef3286eabd6061da922278c40b44a0",
    "semantic_title": "comapgs: covisibility map-based gaussian splatting for sparse novel view synthesis",
    "citation_count": 2,
    "authors": [
      "Youngkyoon Jang",
      "Eduardo Pérez-Pellitero"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Karageorgiou_Any-Resolution_AI-Generated_Image_Detection_by_Spectral_Learning_CVPR_2025_paper.html": {
    "title": "Any-Resolution AI-Generated Image Detection by Spectral Learning",
    "volume": "main",
    "abstract": "Recent works have established that AI models introduce spectral artifacts into generated images and propose approaches for learning to capture them using labeled data. However, the significant differences in such artifacts among different generative models hinder these approaches from generalizing to generators not seen during training. In this work, we build upon the key idea that the spectral distribution of real images constitutes both an invariant and highly discriminative pattern for AI-generated image detection. To model this under a self-supervised setup, we employ masked spectral learning using the pretext task of frequency reconstruction. Since generated images constitute out-of-distribution samples for this model, we propose spectral reconstruction similarity to capture this divergence. Moreover, we introduce spectral context attention, which enables our approach to efficiently capture subtle spectral inconsistencies in images of any resolution. Our spectral AI-generated image detection approach (SPAI) achieves a 5.5% absolute improvement in AUC over the previous state-of-the-art across 13 recent generative approaches, while exhibiting robustness against common online perturbations. Code is available on https://mever-team.github.io/spai",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimitrios Karageorgiou",
      "Symeon Papadopoulos",
      "Ioannis Kompatsiaris",
      "Efstratios Gavves"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Alvar_DivPrune_Diversity-based_Visual_Token_Pruning_for_Large_Multimodal_Models_CVPR_2025_paper.html": {
    "title": "DivPrune: Diversity-based Visual Token Pruning for Large Multimodal Models",
    "volume": "main",
    "abstract": "Large Multimodal Models (LMMs) have emerged as powerful models capable of understanding various data modalities, including text, images, and videos. LMMs encode both text and visual data into tokens that are then combined and processed by an integrated Large Language Model (LLM). Including visual tokens substantially increases the total token count, often by thousands. The increased input length for LLM significantly raises the complexity of inference, resulting in high latency in LMMs. To address this issue, token pruning methods, which remove part of the visual tokens, are proposed. The existing token pruning methods either require extensive calibration and fine-tuning or rely on suboptimal importance metrics which results in increased redundancy among the retained tokens. In this paper, we first formulate token pruning as Max-Min Diversity Problem (MMDP) where the goal is to select a subset such that the diversity among the selected tokens is maximized. Then, we solve the MMDP to obtain the selected subset and prune the rest. The proposed method, DivPrune, reduces redundancy and achieves the highest diversity of the selected tokens. By ensuring high diversity, the selected tokens better represent the original tokens, enabling effective performance even at high pruning ratios without requiring fine-tuning. Extensive experiments with various LMMs show that DivPrune achieves state-of-the-art accuracy over 16 image- and video-language datasets. Additionally, DivPrune reduces both the end-to-end latency and GPU memory usage for the tested models. The code is available here",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saeed Ranjbar Alvar",
      "Gursimran Singh",
      "Mohammad Akbari",
      "Yong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Training_Data_Provenance_Verification_Did_Your_Model_Use_Synthetic_Data_CVPR_2025_paper.html": {
    "title": "Training Data Provenance Verification: Did Your Model Use Synthetic Data from My Generative Model for Training?",
    "volume": "main",
    "abstract": "High-quality open-source text-to-image models have lowered the threshold for obtaining photorealistic images significantly, but also face potential risks of misuse. Specifically, suspects may use synthetic data generated by these generative models to train models for specific tasks without permission, when lacking real data resources especially. Protecting these generative models is crucial for the well-being of their owners. In this work, we propose the first method to this important yet unresolved issue, called Training data Provenance Verification (TrainProVe). The rationale behind TrainProVe is grounded in the principle of generalization error bound, which suggests that, for two models with the same task, if the distance between their training data distributions is smaller, their generalization ability will be closer. We validate the efficacy of TrainProVe across four text-to-image models (Stable Diffusion v1.4, latent consistency model, PixArt-\\alpha, and Stable Cascade). The results show that TrainProVe achieves a verification accuracy of over 99% in determining the provenance of suspicious model training data, surpassing all previous methods. Code is available at https://github.com/xieyc99/TrainProVe",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuechen Xie",
      "Jie Song",
      "Huiqiong Wang",
      "Mingli Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wei_3D-AVS_LiDAR-based_3D_Auto-Vocabulary_Segmentation_CVPR_2025_paper.html": {
    "title": "3D-AVS: LiDAR-based 3D Auto-Vocabulary Segmentation",
    "volume": "main",
    "abstract": "Open-vocabulary segmentation methods offer promising capabilities in detecting unseen object categories, but the category must be aware and needs to be provided by a human, either via a text prompt or pre-labeled datasets, thus limiting their scalability. We propose 3D-AVS, a method for Auto-Vocabulary Segmentation of 3D point clouds for which the vocabulary is unknown and auto-generated for each input at runtime, thus eliminating the human in the loop and typically providing a substantially larger vocabulary for richer annotations. 3D-AVS first recognizes semantic entities from image or point cloud data and then segments all points with the automatically generated vocabulary. Our method incorporates both image-based and point-based recognition, enhancing robustness under challenging lighting conditions where geometric information from LiDAR is especially valuable. Our point-based recognition features a Sparse Masked Attention Pooling (SMAP) module to enrich the diversity of recognized objects. To address the challenges of evaluating unknown vocabularies and avoid annotation biases from label synonyms, hierarchies, or semantic overlaps, we introduce the annotation-free Text-Point Semantic Similarity (TPSS) metric for assessing generated vocabulary quality. Our evaluations on nuScenes and ScanNet200 demonstrate 3D-AVS's ability to generate semantic classes with accurate point-wise segmentations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijie Wei",
      "Osman Ülger",
      "Fatemeh Karimi Nejadasl",
      "Theo Gevers",
      "Martin R. Oswald"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_STOP_Integrated_Spatial-Temporal_Dynamic_Prompting_for_Video_Understanding_CVPR_2025_paper.html": {
    "title": "STOP: Integrated Spatial-Temporal Dynamic Prompting for Video Understanding",
    "volume": "main",
    "abstract": "Pre-trained on tremendous image-text pairs, vision-language models like CLIP have demonstrated promising zero-shot generalization across numerous image-based tasks. However, extending these capabilities to video tasks remains challenging due to limited labeled video data and high training costs. Recent video prompting methods attempt to adapt CLIP for video tasks by introducing learnable prompts, but they typically rely on a single static prompt for all video sequences, overlooking the diverse temporal dynamics and spatial variations that exist across frames. This limitation significantly hinders the model's ability to capture essential temporal information for effective video understanding. To address this, we propose an integrated Spatial-TempOral dynamic Prompting (STOP) model which consists of two complementary modules, the intra-frame spatial prompting and inter-frame temporal prompting. Our intra-frame spatial prompts are designed to adaptively highlight discriminative regions within each frame by leveraging intra-frame attention and temporal variation, allowing the model to focus on areas with substantial temporal dynamics and capture fine-grained spatial details. Additionally, to highlight the varying importance of frames for video understanding, we further introduce inter-frame temporal prompts, dynamically inserting prompts between frames with high temporal variance as measured by frame similarity. This enables the model to prioritize key frames and enhances its capacity to understand temporal dependencies across sequences. Extensive experiments on various video benchmarks demonstrate that STOP consistently achieves superior performance against state-of-the-art methods. The code is available at https://github.com/zhoujiahuan1991/CVPR2025-STOP",
    "checked": true,
    "id": "a51f8f0642b869f6bfb45bae6b873874e0c91209",
    "semantic_title": "stop: integrated spatial-temporal dynamic prompting for video understanding",
    "citation_count": 5,
    "authors": [
      "Zichen Liu",
      "Kunlun Xu",
      "Bing Su",
      "Xu Zou",
      "Yuxin Peng",
      "Jiahuan Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_TimeTracker_Event-based_Continuous_Point_Tracking_for_Video_Frame_Interpolation_with_CVPR_2025_paper.html": {
    "title": "TimeTracker: Event-based Continuous Point Tracking for Video Frame Interpolation with Non-linear Motion",
    "volume": "main",
    "abstract": "Video frame interpolation (VFI) that leverages the bio-inspired event cameras as guidance has recently shown better performance and memory efficiency than the frame-based methods, thanks to the event cameras' advantages, such as high temporal resolution. A hurdle for event-based VFI is how to effectively deal with non-linear motion, caused by the dynamic changes in motion direction and speed within the scene. Existing methods either use events to estimate sparse optical flow or fuse events with image features to estimate dense optical flow. Unfortunately, motion errors often degrade the VFI quality as the continuous motion cues from events do not align with the dense spatial information of images in the temporal dimension. In this paper, we find that object motion is continuous in space, tracking local regions over continuous time enables more accurate identification of spatiotemporal feature correlations. In light of this, we propose a novel continuous point tracking-based VFI framework, named TimeTracker. Specifically, we first design a Scene-Aware Region Segmentation (SARS) module to divide the scene into similar patches. Then, a Continuous Trajectory guided Motion Estimation (CTME) module is proposed to track the continuous motion trajectory of each patch through events. Finally, intermediate frames at any given time are generated through global motion optimization and frame refinement. Moreover, we collect a real-world dataset that features fast non-linear motion. Extensive experiments show that our method outperforms prior arts in both motion estimation and frame interpolation quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyue Liu",
      "Jinghan Xu",
      "Yi Chang",
      "Hanyu Zhou",
      "Haozhi Zhao",
      "Lin Wang",
      "Luxin Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Improving_the_Training_of_Data-Efficient_GANs_via_Quality_Aware_Dynamic_CVPR_2025_paper.html": {
    "title": "Improving the Training of Data-Efficient GANs via Quality Aware Dynamic Discriminator Rejection Sampling",
    "volume": "main",
    "abstract": "Data-Efficient Generative Adversarial Nets (DE-GANs) have become more and more popular in recent years. Existing methods apply data augmentation, noise injection and pre-trained models to maximumly increase the number of training samples thus improving the training of DE-GANs. However, none of these methods considers the sample quality during training, which can also significantly influence the training of DE-GANs. Focusing on sample quality during training, in this paper, we are the first to incorporate discriminator rejection sampling (DRS) into the training process and introduce a novel method, called quality aware dynamic discriminator rejection sampling (QADDRS). Specifically, QADDRS consists of two steps: (1) the sample quality aware step, which aims to obtain the sorted critic scores, i.e., the ordered discriminator outputs, on real/fake samples in the current training stage; (2) the dynamic rejection step that obtains dynamic rejection number N, where N is controlled by the overfitting degree of discriminator (D) during training. When updating the parameters of D, the N high critic score real samples and the N low critic score fake samples in the minibatch are rejected dynamically based on the overfitting degree of D. As a result, QADDRS can avoid D becoming overly confident in distinguishing both real and fake samples, thereby alleviating the overfitting of D issue during training. Extensive experiments on several datasets demonstrate that integrating QADDRS into different DE-GANs can achieve better performance and deliver state-of-the-art results. Codes are available at https://github.com/zzhang05/QADDRS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyu Zhang",
      "Yang Hua",
      "Guanxiong Sun",
      "Hui Wang",
      "Seán McLoone"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Shading_Meets_Motion_Self-supervised_Indoor_3D_Reconstruction_Via_Simultaneous_Shape-from-Shading_CVPR_2025_paper.html": {
    "title": "Shading Meets Motion: Self-supervised Indoor 3D Reconstruction Via Simultaneous Shape-from-Shading and Structure-from-Motion",
    "volume": "main",
    "abstract": "Scene reconstruction has a wide range of applications in computer vision and robotics. To build practical constraints and feature Scene reconstruction has a wide range of applications in computer vision and robotics. To build practical constraints and feature correspondences, rich textures and distinguished gradient variations are particularly required in classic and learning-based SfM. When building low-texture regions with repeated patterns, especially mostly-white indoor rooms, there is a significant drop in performance. In this work, we propose Shading-SfM-Net, a novel framework for simultaneously learning a shape-from-shading network based on the inverse rendering constraint and a structure-from-motion framework based on warped keypoint and geometric consistency, to improve structure-from-motion and surface reconstruction for low-texture indoor scenes. Shading-SfM-Net tightly incorporates the surface shape consistency and 3D geometric registration loss in order to dig into their mutual information and further overcome the instability on flat regions. We evaluate the proposed framework on texture-less indoor scenes (NYUv2 and ScanNet), and show that by simultaneously learning shading, motion and shape, our pipeline is able to achieve state-of-the-art performance with superior generalization capability for unseen texture-less datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoyu Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bhattacharjee_Believing_is_Seeing_Unobserved_Object_Detection_using_Generative_Models_CVPR_2025_paper.html": {
    "title": "Believing is Seeing: Unobserved Object Detection using Generative Models",
    "volume": "main",
    "abstract": "Can objects that are not visible in an image---but are in the vicinity of the camera---be detected? This study introduces the novel tasks of 2D, 2.5D and 3D unobserved object detection for predicting the location of nearby objects that are occluded or lie outside the image frame. We adapt several state-of-the-art pre-trained generative models to address this task, including 2D and 3D diffusion models and vision-language models, and show that they can be used to infer the presence of objects that are not directly observed. To benchmark this task, we propose a suite of metrics that capture different aspects of performance. Our empirical evaluation on indoor scenes from the RealEstate10k and NYU Depth v2 datasets demonstrate results that motivate the use of generative models for the unobserved object detection task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subhransu S. Bhattacharjee",
      "Dylan Campbell",
      "Rahul Shome"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_MotionStone_Decoupled_Motion_Intensity_Modulation_with_Diffusion_Transformer_for_Image-to-Video_CVPR_2025_paper.html": {
    "title": "MotionStone: Decoupled Motion Intensity Modulation with Diffusion Transformer for Image-to-Video Generation",
    "volume": "main",
    "abstract": "The image-to-video (I2V) generation is conditioned on the static image, which has been enhanced recently by the motion intensity as an additional control signal. These motion-aware models are appealing to generate diverse motion patterns, yet there lacks a reliable motion estimator for training such models on large-scale video set in the wild. Traditional metrics, e.g., SSIM or optical flow, are hard to generalize to arbitrary videos, while, it is very tough for human annotators to label the abstract motion intensity neither. Furthermore, the motion intensity shall reveal both local object motion and global camera movement, which has not been studied before. This paper addresses the challenge with a new motion estimator, capable of measuring the decoupled motion intensities of objects and cameras in video. We leverage the contrastive learning on randomly paired videos and distinguish the video with greater motion intensity. Such a paradigm is friendly for annotation and easy to scale up to achieve stable performance on motion estimation. We then present a new I2V model, named MotionStone, developed with the decoupled motion estimator. Experimental results demonstrate the stability of the proposed motion estimator and the state-of-the-art performance of MotionStone on I2V generation. These advantages warrant the decoupled motion estimator to serve as a general plug-in enhancer for both data processing and video generation training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuwei Shi",
      "Biao Gong",
      "Xi Chen",
      "Dandan Zheng",
      "Shuai Tan",
      "Zizheng Yang",
      "Yuyuan Li",
      "Jingwen He",
      "Kecheng Zheng",
      "Jingdong Chen",
      "Ming Yang",
      "Yinqiang Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "NLPrompt: Noise-Label Prompt Learning for Vision-Language Models",
    "volume": "main",
    "abstract": "The emergence of vision-language foundation models, such as CLIP, has revolutionized image-text representation, enabling a broad range of applications via prompt learning. Despite its promise, real-world datasets often contain noisy labels that can degrade prompt learning performance. In this paper, we demonstrate that using mean absolute error (MAE) loss in prompt learning, named PromptMAE, significantly enhances robustness against noisy labels while maintaining high accuracy. Though MAE is straightforward and recognized for its robustness, it is rarely used in noisy-label learning due to its slow convergence and poor performance outside prompt learning scenarios. To elucidate the robustness of PromptMAE, we leverage feature learning theory to show that MAE can suppress the influence of noisy samples, thereby improving the signal-to-noise ratio and enhancing overall robustness. Additionally, we introduce PromptOT, a prompt-based optimal transport data purification method to enhance the robustness further. PromptOT employs text encoder representations in vision-language models as prototypes to construct an optimal transportation matrix. This matrix effectively partitions datasets into clean and noisy subsets, allowing for the application of cross-entropy loss to the clean subset and MAE loss to the noisy subset. Our Noise-Label Prompt Learning method, named NLPrompt, offers a simple and efficient approach that leverages the expressive representation and precise alignment capabilities of vision-language models for robust prompt learning. We validate NLPrompt through extensive experiments across various noise settings, demonstrating significant performance improvements",
    "checked": true,
    "id": "136bafd5c49715f8c4644e664805103132eb8503",
    "semantic_title": "nlprompt: noise-label prompt learning for vision-language models",
    "citation_count": 2,
    "authors": [
      "Bikang Pan",
      "Qun Li",
      "Xiaoying Tang",
      "Wei Huang",
      "Zhen Fang",
      "Feng Liu",
      "Jingya Wang",
      "Jingyi Yu",
      "Ye Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery_CVPR_2025_paper.html": {
    "title": "MEGA: Masked Generative Autoencoder for Human Mesh Recovery",
    "volume": "main",
    "abstract": "Human Mesh Recovery (HMR) from a single RGB image is a highly ambiguous problem, as an infinite set of 3D interpretations can explain the 2D observation equally well. Nevertheless, most HMR methods overlook this issue and make a single prediction without accounting for this ambiguity. A few approaches generate a distribution of human meshes, enabling the sampling of multiple predictions; however, none of them is competitive with the latest single-output model when making a single prediction. This work proposes a new approach based on masked generative modeling. By tokenizing the human pose and shape, we formulate the HMR task as generating a sequence of discrete tokens conditioned on an input image. We introduce MEGA, a MaskEd Generative Autoencoder trained to recover human meshes from images and partial human mesh token sequences. Given an image, our flexible generation scheme allows us to predict a single human mesh in deterministic mode or to generate multiple human meshes in stochastic mode. Experiments on in-the-wild benchmarks show that MEGA achieves state-of-the-art performance in deterministic and stochastic modes, outperforming single-output and multi-output approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guénolé Fiche",
      "Simon Leglaive",
      "Xavier Alameda-Pineda",
      "Francesc Moreno-Noguer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_PBR-NeRF_Inverse_Rendering_with_Physics-Based_Neural_Fields_CVPR_2025_paper.html": {
    "title": "PBR-NeRF: Inverse Rendering with Physics-Based Neural Fields",
    "volume": "main",
    "abstract": "We tackle the ill-posed inverse rendering problem in 3D reconstruction with a Neural Radiance Field (NeRF) approach informed by Physics-Based Rendering (PBR) theory, named PBR-NeRF. Our method addresses a key limitation in most NeRF and 3D Gaussian Splatting approaches: they estimate view-dependent appearance without modeling scene materials and illumination. To address this limitation, we present an inverse rendering (IR) model capable of jointly estimating scene geometry, materials, and illumination. Our model builds upon recent NeRF-based IR approaches, but crucially introduces two novel physics-based priors that better constrain the IR estimation. Our priors are rigorously formulated as intuitive loss terms and achieve state-of-the-art material estimation without compromising novel view synthesis quality. Our method is easily adaptable to other inverse rendering and 3D reconstruction frameworks that require material estimation. We demonstrate the importance of extending current neural rendering approaches to fully model scene properties beyond geometry and view-dependent appearance. Code is publicly available at: https://github.com/s3anwu/pbrnerf",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sean Wu",
      "Shamik Basu",
      "Tim Broedermann",
      "Luc Van Gool",
      "Christos Sakaridis"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Muthukumar_Disentangling_Safe_and_Unsafe_Image_Corruptions_via_Anisotropy_and_Locality_CVPR_2025_paper.html": {
    "title": "Disentangling Safe and Unsafe Image Corruptions via Anisotropy and Locality",
    "volume": "main",
    "abstract": "State-of-the-art machine learning systems are vulnerable to small perturbations to their input, where _small_ is defined according to a threat model that assigns a positive threat to each perturbation. Most prior works define a task-agnostic, isotropic, and global threat, like the l_p norm, where the magnitude of the perturbation fully determines the degree of the threat and neither the direction of the attack nor its position in space matter. However, common corruptions in computer vision, such as blur, compression, or occlusions, are not well captured by such treat models. This paper proposes a novel threat model called \\texttt Projected Displacement (PD) to study robustness beyond existing isotropic and global threat models. The proposed threat model measures the threat of a perturbation via its alignment with _unsafe directions_, defined as directions in the input space along which a perturbation of sufficient magnitude changes the ground truth class label. Unsafe directions are identified locally for each input based on observed training data. In this way, the PD-threat model exhibits anisotropy and locality. The PD-threat model is computationally efficient and can be easily integrated into existing robustness pipelines. Experiments on Imagenet-1k data indicate that, for any input, the set of perturbations with small PD threat includes _safe_ perturbations of large l_p norm that preserve the true label, such as noise, blur and compression, while simultaneously excluding _unsafe_ perturbations that alter the true label. Unlike perceptual threat models based on embeddings of large-vision models, the PD-threat model can be readily computed for arbitrary classification tasks without pre-training or finetuning. Further additional task information such as sensitivity to image regions or concept hierarchies can be easily integrated into the assessment of threat and thus the PD threat model presents practitioners a flexible, task-driven threat specification that alleviates the limitations of l_p-threat models",
    "checked": true,
    "id": "04aa16b41d39af8c5298246ed31c75d42dd0ff8d",
    "semantic_title": "disentangling safe and unsafe image corruptions via anisotropy and locality",
    "citation_count": 0,
    "authors": [
      "Ramchandran Muthukumar",
      "Ambar Pal",
      "Jeremias Sulam",
      "Rene Vidal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Prometheus_3D-Aware_Latent_Diffusion_Models_for_Feed-Forward_Text-to-3D_Scene_Generation_CVPR_2025_paper.html": {
    "title": "Prometheus: 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D Scene Generation",
    "volume": "main",
    "abstract": "In this work, we introduce Prometheus, a 3D-aware latent diffusion model for text-to-3D generation at both object and scene levels in seconds. We formulate 3D scene generation as multi-view, feed-forward, pixel-aligned 3D Gaussian generation within the latent diffusion paradigm. To ensure generalizability, we build our model upon pre-trained text-to-image generation model with only minimal adjustments and further train it using a large number of images from both single-view and multi-view datasets. Furthermore, we introduce an RGB-D latent space into 3D Gaussian generation to disentangle appearance and geometry information, enabling efficient feed-forward generation of 3D Gaussians with better fidelity and geometry. Extensive experimental results demonstrate the effectiveness of our method in both feed-forward 3D Gaussian reconstruction and text-to-3D generation",
    "checked": true,
    "id": "699aeed32ca75d19ded2e2b6d23e32ea63578c87",
    "semantic_title": "prometheus: 3d-aware latent diffusion models for feed-forward text-to-3d scene generation",
    "citation_count": 8,
    "authors": [
      "Yuanbo Yang",
      "Jiahao Shao",
      "Xinyang Li",
      "Yujun Shen",
      "Andreas Geiger",
      "Yiyi Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution_CVPR_2025_paper.html": {
    "title": "No Pains, More Gains: Recycling Sub-Salient Patches for Efficient High-Resolution Image Recognition",
    "volume": "main",
    "abstract": "Over the last decade, many notable methods have emerged to tackle the computational resource challenge of the high resolution image recognition (HRIR). They typically focus on identifying and aggregating a few salient regions for classification, discarding sub-salient areas for low training consumption. Nevertheless, many HRIR tasks necessitate the exploration of wider regions to model objects and contexts, which limits their performance in such scenarios. To address this issue, we present a DBPS strategy to enable training with more patches at low consumption. Specifically, in addition to a fundamental buffer that stores the embeddings of most salient patches, DBPS further employs an auxiliary buffer to recycle those sub-salient ones. To reduce the computational cost associated with gradients of sub-salient patches, these patches are primarily used in the forward pass to provide sufficient information for classification. Meanwhile, only the gradients of the salient patches are back-propagated to update the entire network. Moreover, we design a Multiple Instance Learning (MIL) architecture that leverages aggregated information from salient patches to filter out uninformative background within sub-salient patches for better accuracy. Besides, we introduce the random patch drop to accelerate training process and uncover informative regions. Experiment results demonstrate the superiority of our method in terms of both accuracy and training consumption against other advanced methods. The code is available in the https://github.com/Qinrong-NKU/DBPS",
    "checked": true,
    "id": "b456bac2fd24f79f2656a6ae8d10d001e9c5c524",
    "semantic_title": "no pains, more gains: recycling sub-salient patches for efficient high-resolution image recognition",
    "citation_count": 0,
    "authors": [
      "Rong Qin",
      "Xin Liu",
      "Xingyu Liu",
      "Jiaxuan Liu",
      "Jinglei Shi",
      "Liang Lin",
      "Jufeng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Benny_SphereUFormer_A_U-Shaped_Transformer_for_Spherical_360_Perception_CVPR_2025_paper.html": {
    "title": "SphereUFormer: A U-Shaped Transformer for Spherical 360 Perception",
    "volume": "main",
    "abstract": "This paper proposes a novel method for omnidirectional 360\\degree perception. Most common previous methods relied on equirectangular projection. This representation is easily applicable to 2D operation layers but introduces distortions into the image. Other methods attempted to remove the distortions by maintaining a sphere representation but relied on complicated convolution kernels that failed to show competitive results. In this work, we introduce a transformer-based architecture that, by incorporating a novel \"Spherical Local Self-Attention\" and other spherically-oriented modules, successfully operates in the spherical domain and outperforms the state-of-the-art in 360\\degree perception benchmarks for depth estimation and semantic segmentation. Our code is available at https://github.com/yanivbenny/sphere_uformer",
    "checked": true,
    "id": "59f67e77949a4cf72993ceecd355b435e1c15b2b",
    "semantic_title": "sphereuformer: a u-shaped transformer for spherical 360 perception",
    "citation_count": 1,
    "authors": [
      "Yaniv Benny",
      "Lior Wolf"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Advancing_Generalizable_Tumor_Segmentation_with_Anomaly-Aware_Open-Vocabulary_Attention_Maps_and_CVPR_2025_paper.html": {
    "title": "Advancing Generalizable Tumor Segmentation with Anomaly-Aware Open-Vocabulary Attention Maps and Frozen Foundation Diffusion Models",
    "volume": "main",
    "abstract": "We explore Generalizable Tumor Segmentation, aiming to train a single model for zero-shot tumor segmentation across diverse anatomical regions. Existing methods face limitations related to segmentation quality, scalability, and the range of applicable imaging modalities. In this paper, we uncover the potential of the internal representations within frozen medical foundation diffusion models as highly efficient zero-shot learners for tumor segmentation by introducing a novel framework named DiffuGTS. DiffuGTS creates anomaly-aware open-vocabulary attention maps based on text prompts to enable generalizable anomaly segmentation without being restricted by a predefined training category list. To further improve and refine anomaly segmentation masks, DiffuGTS leverages the diffusion model, transforming pathological regions into high-quality pseudo-healthy counterparts through latent space inpainting, and applies a novel pixel-level and feature-level residual learning approach, resulting in segmentation masks with significantly enhanced quality and generalization. Comprehensive experiments on four datasets and seven tumor categories demonstrate the superior performance of our method, surpassing current state-of-the-art models across multiple zero-shot settings. Codes are available at https://github.com/Yankai96/DiffuGTS",
    "checked": true,
    "id": "6c9e7e8374023b8743388235ebf61eb37ab72004",
    "semantic_title": "advancing generalizable tumor segmentation with anomaly-aware open-vocabulary attention maps and frozen foundation diffusion models",
    "citation_count": 0,
    "authors": [
      "Yankai Jiang",
      "Peng Zhang",
      "Donglin Yang",
      "Yuan Tian",
      "Hai Lin",
      "Xiaosong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Towards_Generalizable_Scene_Change_Detection_CVPR_2025_paper.html": {
    "title": "Towards Generalizable Scene Change Detection",
    "volume": "main",
    "abstract": "While current state-of-the-art Scene Change Detection (SCD) approaches achieve impressive results in well-trained research data, they become unreliable under unseen environments and different temporal conditions; in-domain performance drops from 77.6% to 8.0% in a previously unseen environment and to 4.6% under a different temporal condition---calling for generalizable SCD and benchmark. In this work, we propose the Generalizable Scene Change Detection Framework (GeSCF), which addresses unseen domain performance and temporal consistency---to meet the growing demand for anything SCD. Our method leverages the pre-trained Segment Anything Model (SAM) in a zero-shot manner. For this, we design Initial Pseudo-mask Generation and Geometric-Semantic Mask Matching---seamlessly turning user-guided prompt and single-image based segmentation into scene change detection for a pair of inputs without guidance. Furthermore, we define the Generalizable Scene Change Detection (GeSCD) benchmark along with novel metrics and an evaluation protocol to facilitate SCD research in generalizability. In the process, we introduce the ChangeVPR dataset, a collection of challenging image pairs with diverse environmental scenarios---including urban, suburban, and rural settings. Extensive experiments across various datasets demonstrate that GeSCF achieves an average performance gain of 19.2% on existing SCD datasets and 30.0% on the ChangeVPR dataset, nearly doubling the prior art performance. We believe our work can lay a solid foundation for robust and generalizable SCD research",
    "checked": true,
    "id": "67671edfb0eb14f5eff33ae902ca44c105c22471",
    "semantic_title": "towards generalizable scene change detection",
    "citation_count": 2,
    "authors": [
      "Jae-Woo Kim",
      "Ue-Hwan Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Beyond_Clean_Training_Data_A_Versatile_and_Model-Agnostic_Framework_for_CVPR_2025_paper.html": {
    "title": "Beyond Clean Training Data: A Versatile and Model-Agnostic Framework for Out-of-Distribution Detection with Contaminated Training Data",
    "volume": "main",
    "abstract": "In real-world AI applications, training datasets are often contaminated, containing a mix of in-distribution (ID) and out-of-distribution (OOD) samples without labels. This contamination poses a significant challenge for developing and training OOD detection models, as nearly all existing methods assume access to a clean training dataset of only ID samples--a condition rarely met in real-world scenarios. Customizing each existing OOD detection method to handle such contamination is impractical, given the vast number of diverse methods designed for clean data. To address this issue, we propose a universal, model-agnostic framework that integrates with nearly all existing OOD detection methods, enabling training on contaminated datasets while achieving high OOD detection accuracy on test datasets. Additionally, our framework provides an accurate estimation of the unknown proportion of OOD samples within the training dataset--an important and distinct challenge in its own right. Our approach introduces a novel dynamic weighting function and transition mechanism within an iterative training structure, enabling both reliable estimation of the OOD sample proportion of the training data and precise OOD detection on test data. Extensive evaluations across diverse datasets, including ImageNet-1k, demonstrate that our framework accurately estimates OOD sample proportions of training data and substantially enhances OOD detection accuracy on test data",
    "checked": true,
    "id": "05947f46d794b1c160bf0c2e4ee01bc401b39788",
    "semantic_title": "beyond clean training data: a versatile and model-agnostic framework for out-of-distribution detection with contaminated training data",
    "citation_count": 0,
    "authors": [
      "Yuchuan Li",
      "Jae-Mo Kang",
      "Il-Min Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Incomplete_Multi-modal_Brain_Tumor_Segmentation_via_Learnable_Sorting_State_Space_CVPR_2025_paper.html": {
    "title": "Incomplete Multi-modal Brain Tumor Segmentation via Learnable Sorting State Space Model",
    "volume": "main",
    "abstract": "Brain tumor segmentation plays a crucial role in clinical diagnosis, yet the frequent unavailability of certain MRI modalities poses a significant challenge. In this paper, we introduce the Learnable Sorting State Space Model (LS3M), a novel framework designed to maximize the utilization of available modalities for brain tumor segmentation. LS3M excels at efficiently modeling long-range dependencies based on the Mamba design, while incorporating differentiable permutation matrices that reorder input sequences based on modality-specific characteristics. This dynamic reordering ensures that critical spatial inductive biases and long-range semantic correlations inherent in 3D brain MRI are preserved, which is crucial for imcomplete multi-modal brain tumor segmentation.Once the input sequences are reordered using the generated permutation matrix, the Series State Space Model (S3M) block models the relationships between them, capturing both local and long-range dependencies. This enables effective representation of intra-modal and inter-modal relationships, significantly improving segmentation accuracy. Extensive experiments on the BraTS2018 and BraTS2020 datasets demonstrate that LS3M outperforms existing methods, offering a robust solution for brain tumor segmentation, particularly in scenarios with missing modalities",
    "checked": true,
    "id": "b8d978f72167f62a03ff3e12c982fb4ff5638b53",
    "semantic_title": "incomplete multi-modal brain tumor segmentation via learnable sorting state space model",
    "citation_count": 0,
    "authors": [
      "Zheyu Zhang",
      "Yayuan Lu",
      "Feipeng Ma",
      "Yueyi Zhang",
      "Huanjing Yue",
      "Xiaoyan Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_FedAWA_Adaptive_Optimization_of_Aggregation_Weights_in_Federated_Learning_Using_CVPR_2025_paper.html": {
    "title": "FedAWA: Adaptive Optimization of Aggregation Weights in Federated Learning Using Client Vectors",
    "volume": "main",
    "abstract": "Federated Learning (FL) has emerged as a promising framework for distributed machine learning, enabling collaborative model training without sharing local data, thereby preserving privacy and enhancing security. However, data heterogeneity resulting from differences across user behaviors, preferences, and device characteristics poses a significant challenge for federated learning. Most previous works overlook the adjustment of aggregation weights, relying solely on dataset size for weight assignment, which often leads to unstable convergence and reduced model performance. Recently, several studies have sought to refine aggregation strategies by incorporating dataset characteristics and model alignment. However, adaptively adjusting aggregation weights while ensuring data security--without requiring additional proxy data--remains a significant challenge. In this work, we propose Federated learning with Adaptive Weight Aggregation (FedAWA), a novel method that adaptively adjusts aggregation weights based on client vectors during the learning process. The client vector captures the direction of model updates, reflecting local data variations, and is used to optimize the aggregation weight without requiring additional datasets or violating privacy. By assigning higher aggregation weights to local models whose updates align closely with the global optimization direction, FedAWA enhances the stability and generalization of the global model. Extensive experiments under diverse scenarios demonstrate the superiority of our method, providing a promising solution to the challenges of data heterogeneity in federated learning",
    "checked": true,
    "id": "16c0620ed99a84f715fd128f9640f2d5157fce0b",
    "semantic_title": "fedawa: adaptive optimization of aggregation weights in federated learning using client vectors",
    "citation_count": 3,
    "authors": [
      "Changlong Shi",
      "He Zhao",
      "Bingjie Zhang",
      "Mingyuan Zhou",
      "Dandan Guo",
      "Yi Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_FreeUV_Ground-Truth-Free_Realistic_Facial_UV_Texture_Recovery_via_Cross-Assembly_Inference_CVPR_2025_paper.html": {
    "title": "FreeUV: Ground-Truth-Free Realistic Facial UV Texture Recovery via Cross-Assembly Inference Strategy",
    "volume": "main",
    "abstract": "Recovering high-quality 3D facial textures from single-view 2D images is a challenging task, especially under constraints of limited data and complex facial details such as makeup, wrinkles, and occlusions. In this paper, we introduce FreeUV, a novel ground-truth-free UV texture recovery framework that eliminates the need for annotated or synthetic UV data. FreeUV leverages pre-trained stable diffusion model alongside a Cross-Assembly inference strategy to fulfill this objective. In FreeUV, separate networks are trained independently to focus on realistic appearance and structural consistency, and these networks are combined during inference to generate coherent textures. Our approach accurately captures intricate facial features and demonstrates robust performance across diverse poses and occlusions. Extensive experiments validate FreeUV's effectiveness, with results surpassing state-of-the-art methods in both quantitative and qualitative metrics. Additionally, FreeUV enables new applications, including local editing, facial feature interpolation, and multi-view texture recovery. By reducing data requirements, FreeUV offers a scalable solution for generating high-fidelity 3D facial textures suitable for real-world scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingchao Yang",
      "Takafumi Taketomi",
      "Yuki Endo",
      "Yoshihiro Kanamori"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_HarmonySet_A_Comprehensive_Dataset_for_Understanding_Video-Music_Semantic_Alignment_and_CVPR_2025_paper.html": {
    "title": "HarmonySet: A Comprehensive Dataset for Understanding Video-Music Semantic Alignment and Temporal Synchronization",
    "volume": "main",
    "abstract": "This paper introduces HarmonySet, a comprehensive dataset designed to advance video-music understanding. HarmonySet consists of 48,328 diverse video-music pairs, annotated with detailed information on rhythmic synchronization, emotional alignment, thematic coherence, and cultural relevance. We propose a multi-step human-machine collaborative framework for efficient annotation, combining human insights with machine-generated descriptions to identify key transitions and assess alignment across multiple dimensions. Additionally, we introduce a novel evaluation framework with tasks and metrics to assess the multi-dimensional alignment of video and music, including rhythm, emotion, theme, and cultural context. Our extensive experiments demonstrate that HarmonySet, along with the proposed evaluation framework, significantly improves the ability of multimodal models to capture and analyze the intricate relationships between video and music. Project page: https://harmonyset.github.io/",
    "checked": true,
    "id": "34288ad84ff79159fb32c0bb5fb56ed6da5f0ce4",
    "semantic_title": "harmonyset: a comprehensive dataset for understanding video-music semantic alignment and temporal synchronization",
    "citation_count": 2,
    "authors": [
      "Zitang Zhou",
      "Ke Mei",
      "Yu Lu",
      "Tianyi Wang",
      "Fengyun Rao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Meng_Rethinking_Diffusion_for_Text-Driven_Human_Motion_Generation_Redundant_Representations_Evaluation_CVPR_2025_paper.html": {
    "title": "Rethinking Diffusion for Text-Driven Human Motion Generation: Redundant Representations, Evaluation, and Masked Autoregression",
    "volume": "main",
    "abstract": "Since 2023, Vector Quantization (VQ)-based discrete generation methods have rapidly dominated human motion generation, primarily surpassing diffusion-based continuous generation methods in standard performance metrics. However, VQ-based methods have inherent limitations. Representing continuous motion data as limited discrete tokens leads to inevitable information loss, reduces the diversity of generated motions, and restricts their ability to function effectively as motion priors or generation guidance. In contrast, the continuous space generation nature of diffusion-based methods makes them well-suited to address these limitations and with even potential for model scalability. In this work, we systematically investigate why current VQ-based methods perform well and explore the limitations of existing diffusion-based methods from the perspective of motion data representation and distribution. Drawing on these insights, we preserve the inherent strengths of a diffusion-based human motion generation model and gradually optimize it with inspiration from VQ-based approaches. Our approach introduces a human motion diffusion model enabled to perform masked autoregression, optimized with a reformed data representation and distribution. Additionally, we propose a more robust evaluation method to assess different approaches. Extensive experiments on various datasets demonstrate our method outperforms previous methods and achieves state-of-the-art performances",
    "checked": true,
    "id": "2283ebef7384996b31e112735996aa76e54c4890",
    "semantic_title": "rethinking diffusion for text-driven human motion generation: redundant representations, evaluation, and masked autoregression",
    "citation_count": 13,
    "authors": [
      "Zichong Meng",
      "Yiming Xie",
      "Xiaogang Peng",
      "Zeyu Han",
      "Huaizu Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_StyleMaster_Stylize_Your_Video_with_Artistic_Generation_and_Translation_CVPR_2025_paper.html": {
    "title": "StyleMaster: Stylize Your Video with Artistic Generation and Translation",
    "volume": "main",
    "abstract": "Style control has been popular in video generation models. Existing methods often generate videos far from the given style, cause content leakage, and struggle to transfer one video to the desired style. Our first observation is that the style extraction stage matters, whereas existing methods emphasize global style but ignore local textures. In order to bring texture features while preventing content leakage, we filter content-related patches while retaining style ones based on prompt-patch similarity; for global style extraction, we generate a paired style dataset through model illusion to facilitate contrastive learning, which greatly enhances the absolute style consistency. Moreover, to fill in the image-to-video gap, we train a lightweight motion adapter on still videos, which implicitly enhances stylization extent, and enables our image-trained model to be seamlessly applied to videos. Benefited from these efforts, our approach, StyleMaster, not only achieves significant improvement in both style resemblance and temporal coherence, but also can easily generalize to video style transfer with a gray tile ControlNet. Extensive experiments and visualizations demonstrate that StyleMaster significantly outperforms competitors, effectively generating high-quality stylized videos that align with textual content and closely resemble the style of reference images",
    "checked": true,
    "id": "0b0847aeb0eca4a26f838b0969b35c26ff8d8cc2",
    "semantic_title": "stylemaster: stylize your video with artistic generation and translation",
    "citation_count": 10,
    "authors": [
      "Zixuan Ye",
      "Huijuan Huang",
      "Xintao Wang",
      "Pengfei Wan",
      "Di Zhang",
      "Wenhan Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling_CVPR_2025_paper.html": {
    "title": "Unsupervised Continual Domain Shift Learning with Multi-Prototype Modeling",
    "volume": "main",
    "abstract": "In real-world applications, deep neural networks may encounter constantly changing environments, where the test data originates from continually shifting unlabeled target domains. This problem, known as Unsupervised Continual Domain Shift Learning (UCDSL), poses practical difficulties. Existing methods for UCDSL aim to learn domain-invariant representations for all target domains. However, due to the existence of adaptivity gap, the invariant representation may theoretically lead to large joint errors. To overcome the limitation, we propose a novel UCDSL method, called Multi-Prototype Modeling (MPM). Our model comprises two key components: (1) Multi-Prototype Learning (MPL) for acquiring domain-specific representations using multiple domain-specific prototypes. MPL achieves domain-specific error minimization instead of enforcing feature alignment across different domains. (2) Bi-Level Graph Enhancer (BiGE) for enhancing domain-level and category-level representations, resulting in more accurate predictions. We provide theoretical and empirical analysis to demonstrate the effectiveness of our proposed method. We evaluate our approach on multiple benchmark datasets and show that our model surpasses state-of-the-art methods across all datasets, highlighting its effectiveness and robustness in handling unsupervised continual domain shift learning. Codes will be publicly accessible",
    "checked": true,
    "id": "b44a9cd2c563db0b1925d7c8c6690b24763f5bc6",
    "semantic_title": "unsupervised continual domain shift learning with multi-prototype modeling",
    "citation_count": 1,
    "authors": [
      "Haopeng Sun",
      "Yingwei Zhang",
      "Lumin Xu",
      "Sheng Jin",
      "Ping Luo",
      "Chen Qian",
      "Wentao Liu",
      "Yiqiang Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_OmniGuard_Hybrid_Manipulation_Localization_via_Augmented_Versatile_Deep_Image_Watermarking_CVPR_2025_paper.html": {
    "title": "OmniGuard: Hybrid Manipulation Localization via Augmented Versatile Deep Image Watermarking",
    "volume": "main",
    "abstract": "With the rapid growth of generative AI and its widespread application in image editing, new risks have emerged regarding the authenticity and integrity of digital content. Existing versatile watermarking approaches suffer from trade-offs between tamper localization precision and visual quality. Constrained by the limited flexibility of previous framework, their localized watermark must remain fixed across all images. Under AIGC-editing, their copyright extraction accuracy is also unsatisfactory. To address these challenges, we propose OmniGuard, a novel augmented versatile watermarking approach that integrates proactive embedding with passive, blind extraction for robust copyright protection and tamper localization. OmniGuard employs a hybrid forensic framework that enables flexible localization watermark selection and introduces a degradation-aware tamper extraction network for precise localization under challenging conditions. Additionally, a lightweight AIGC-editing simulation layer is designed to enhance robustness across global and local editing. Extensive experiments show that OmniGuard achieves superior fidelity, robustness, and flexibility. Compared to the recent state-of-the-art approach EditGuard, our method outperforms it by 4.25dB in PSNR of the container image, 20.7% in F1-Score under noisy conditions, and 14.8% in average bit accuracy",
    "checked": true,
    "id": "0dd9e2400c0e6b77de42c671516a1d77c5b2e78c",
    "semantic_title": "omniguard: hybrid manipulation localization via augmented versatile deep image watermarking",
    "citation_count": 8,
    "authors": [
      "Xuanyu Zhang",
      "Zecheng Tang",
      "Zhipei Xu",
      "Runyi Li",
      "Youmin Xu",
      "Bin Chen",
      "Feng Gao",
      "Jian Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring_CVPR_2025_paper.html": {
    "title": "Open-Canopy: Towards Very High Resolution Forest Monitoring",
    "volume": "main",
    "abstract": "Estimating canopy height and its changes at meter resolution from satellite imagery is a significant challenge in computer vision with critical environmental applications. However, the lack of open-access datasets at this resolution hinders the reproducibility and evaluation of models. We introduce Open-Canopy, the first open-access, country-scale benchmark for very high-resolution (1.5 m) canopy height estimation, covering over 87,000 km2 across France with 1.5 m resolution satellite imagery and aerial LiDAR data. Additionally, we present Open-Canopy-, a benchmark for canopy height change detection between images from different years at tree level--a challenging task for current computer vision models. We evaluate state-of-the-art architectures on these benchmarks, highlighting significant challenges and opportunities for improvement. Our datasets and code are publicly available at https://github.com/fajwel/Open-Canopy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fajwel Fogel",
      "Yohann Perron",
      "Nikola Besic",
      "Laurent Saint-André",
      "Agnès Pellissier-Tanon",
      "Martin Schwartz",
      "Thomas Boudras",
      "Ibrahim Fayad",
      "Alexandre d'Aspremont",
      "Loic Landrieu",
      "Philippe Ciais"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_ClearSight_Visual_Signal_Enhancement_for_Object_Hallucination_Mitigation_in_Multimodal_CVPR_2025_paper.html": {
    "title": "ClearSight: Visual Signal Enhancement for Object Hallucination Mitigation in Multimodal Large Language Models",
    "volume": "main",
    "abstract": "Contrastive decoding strategies are widely used to mitigate object hallucinations in multimodal large language models (MLLMs). By reducing over-reliance on language priors, these strategies ensure that generated content remains closely grounded in visual inputs, producing contextually accurate outputs. Since contrastive decoding requires no additional training or external tools, it offers both computational efficiency and versatility, making it highly attractive. However, these methods present two main limitations: (1) bluntly suppressing language priors can compromise coherence and accuracy of generated content, and (2) processing contrastive inputs adds computational load, significantly slowing inference speed. To address these challenges, we propose Visual Amplification Fusion (VAF), a plug-and-play technique that enhances attention to visual signals within the model's middle layers, where modality fusion predominantly occurs. This approach enables more effective capture of visual features, reducing the model's bias toward language modality. Experimental results demonstrate that VAF significantly reduces hallucinations across various MLLMs without affecting inference speed, while maintaining coherence and accuracy in generated outputs",
    "checked": true,
    "id": "ecc51ce52ca524be17616a9c0dc8a051a2996ad7",
    "semantic_title": "clearsight: visual signal enhancement for object hallucination mitigation in multimodal large language models",
    "citation_count": 2,
    "authors": [
      "Hao Yin",
      "Guangzong Si",
      "Zilei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sehwag_Stretching_Each_Dollar_Diffusion_Training_from_Scratch_on_a_Micro-Budget_CVPR_2025_paper.html": {
    "title": "Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget",
    "volume": "main",
    "abstract": "As scaling laws in generative AI push performance, they simultaneously concentrate the development of these models among actors with large computational resources. With a focus on text-to-image (T2I) generative models, we aim to unlock this bottleneck by demonstrating very low-cost training of large-scale T2I diffusion transformer models. As the computational cost of transformers increases with the number of patches in each image, we propose randomly masking up to 75% of the image patches during training. We propose a deferred masking strategy that preprocesses all patches using a patch-mixer before masking, thus significantly reducing the performance degradation with masking, making it superior to model downscaling in reducing computational cost. We also incorporate the latest improvements in transformer architecture, such as the use of mixture-of-experts layers, to improve performance and further identify the critical benefit of using synthetic images in micro-budget training. Finally, using only 37M publicly available real and synthetic images, we train a 1.16 billion parameter sparse transformer with only 1,890 USD economical cost and achieve a 12.7 FID in zero-shot generation on the COCO dataset. Notably, our model achieves competitive performance across both automated and human-centric evaluations, as well as high-quality generations, while incurring 118xlower costs than Stable Diffusion models and 14xlower costs than the current state-of-the-art approach, which costs \\28,400. We also further investigate the influence of synthetic images on performance and demonstrate that micro-budget training on only synthetic images is sufficient for achieving high-quality data generation. Our end-to-end training pipeline and model checkpoints are available at https://github.com/SonyResearch/micro_diffusion",
    "checked": true,
    "id": "fbc91231aab3327f2ff271e84178e6ee4c38e48a",
    "semantic_title": "stretching each dollar: diffusion training from scratch on a micro-budget",
    "citation_count": 13,
    "authors": [
      "Vikash Sehwag",
      "Xianghao Kong",
      "Jingtao Li",
      "Michael Spranger",
      "Lingjuan Lyu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xue_Guiding_Human-Object_Interactions_with_Rich_Geometry_and_Relations_CVPR_2025_paper.html": {
    "title": "Guiding Human-Object Interactions with Rich Geometry and Relations",
    "volume": "main",
    "abstract": "Human-object interaction (HOI) synthesis is crucial for creating immersive and realistic experiences for applications such as virtual reality. Existing methods often rely on simplified object representations, such as the object's centroid or the nearest point to a human, to achieve physically plausible motions. However, these approaches may overlook geometric complexity, resulting in suboptimal interaction fidelity. To address this limitation, we introduce ROG, a novel diffusion-based framework that models the spatiotemporal relationships inherent in HOIs with rich geometric detail. For efficient object representation, we select boundary-focused and fine-detail key points from the object mesh, ensuring a comprehensive depiction of the object's geometry. This representation is used to construct an interactive distance field (IDF), capturing the robust HOI dynamics. Furthermore, we develop a diffusion-based relation model that integrates spatial and temporal attention mechanisms, enabling a better understanding of intricate HOI relationships. This relation model refines the generated motion's IDF, guiding the motion generation process to produce relation-aware and semantically aligned movements. Experimental evaluations demonstrate that ROG significantly outperforms state-of-the-art methods in the realism and semantic accuracy of synthesized HOIs. This paper's code will be released",
    "checked": true,
    "id": "409d41cc91d10a19c08652f7348e8d586879c8cf",
    "semantic_title": "guiding human-object interactions with rich geometry and relations",
    "citation_count": 4,
    "authors": [
      "Mengqing Xue",
      "Yifei Liu",
      "Ling Guo",
      "Shaoli Huang",
      "Changxing Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion_CVPR_2025_paper.html": {
    "title": "TacoDepth: Towards Efficient Radar-Camera Depth Estimation with One-stage Fusion",
    "volume": "main",
    "abstract": "Radar-Camera depth estimation aims to predict dense and accurate metric depth by fusing input images and Radar data. Model efficiency is crucial for this task in pursuit of real-time processing on autonomous vehicles and robotic platforms. However, due to the sparsity of Radar returns, the prevailing methods adopt multi-stage frameworks with intermediate quasi-dense depth, which are time-consuming and not robust. To address these challenges, we propose TacoDepth, an efficient and accurate Radar-Camera depth estimation model with one-stage fusion. Specifically, the graph-based Radar structure extractor and the pyramid-based Radar fusion module are designed to capture and integrate the graph structures of Radar point clouds, delivering superior model efficiency and robustness without relying on the intermediate depth results. Moreover, TacoDepth can be flexible for different inference modes, providing a better balance of speed and accuracy. Extensive experiments are conducted to demonstrate the efficacy of our method. Compared with the previous state-of-the-art approach, TacoDepth improves depth accuracy and processing speed by 12.8% and 91.8%. Our work provides a new perspective on efficient Radar-Camera depth estimation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiran Wang",
      "Jiaqi Li",
      "Chaoyi Hong",
      "Ruibo Li",
      "Liusheng Sun",
      "Xiao Song",
      "Zhe Wang",
      "Zhiguo Cao",
      "Guosheng Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Taketsugu_Physical_Plausibility-aware_Trajectory_Prediction_via_Locomotion_Embodiment_CVPR_2025_paper.html": {
    "title": "Physical Plausibility-aware Trajectory Prediction via Locomotion Embodiment",
    "volume": "main",
    "abstract": "Humans can predict future human trajectories even from momentary observations by using human pose-related cues. However, previous Human Trajectory Prediction (HTP) methods leverage the pose cues implicitly, resulting in implausible predictions. To address this, we propose Locomotion Embodiment, a framework that explicitly evaluates the physical plausibility of the predicted trajectory by locomotion generation under the laws of physics. While the plausibility of locomotion is learned with an indifferentiable physics simulator, it is replaced by our differentiable Locomotion Value function to train an HTP network in a data-driven manner. In particular, our proposed Embodied Locomotion loss is beneficial for efficiently training a stochastic HTP network using multiple heads. Furthermore, the Locomotion Value filter is proposed to filter out implausible trajectories at inference. Experiments demonstrate that our method enhances even the state-of-the-art HTP methods across diverse datasets and problem settings. Our code is available at: https://github.com/ImIntheMiddle/EmLoco",
    "checked": true,
    "id": "d66a698fc38c2a5dcfbcfb250d9099323c53eba8",
    "semantic_title": "physical plausibility-aware trajectory prediction via locomotion embodiment",
    "citation_count": 5,
    "authors": [
      "Hiromu Taketsugu",
      "Takeru Oba",
      "Takahiro Maeda",
      "Shohei Nobuhara",
      "Norimichi Ukita"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images_CVPR_2025_paper.html": {
    "title": "CADDreamer: CAD Object Generation from Single-view Images",
    "volume": "main",
    "abstract": "The field of diffusion-based 3D generation has experienced tremendous progress in recent times. However, existing 3D generative models often produce overly dense and unstructured meshes, which are in stark contrast to the compact, structured and clear-edged CAD models created by human modelers. We introduce CADDreamer, a novel method for generating CAD objects from a single image. This method proposes a primitive-aware multi-view diffusion model, which perceives both local geometry and high-level structural semantics during the generation process. We encode primitive semantics into the color domain, and enforce the strong priors in pre-trained diffusion models to align with the well-defined primitives. As a result, we can infer multi-view normal maps and semantic maps from a single image, thereby reconstructing a mesh with primitive labels. Correspondingly, we propose a set of fitting and optimization methods to deal with the inevitable noise and distortion in generated primitives, ultimately producing a complete and seamless Boundary Representation (B-rep) of a Computer-Aided Design (CAD) model. Experimental results demonstrate that our method can effectively recover high-quality CAD objects from single-view images. Compared to existing 3D generation methods, the models produced by CADDreamer are compact in representation, clear in structure, sharp in boundaries, and watertight in topology",
    "checked": true,
    "id": "d89bc61f4521308340ffc34cd0e48c4d91b685f1",
    "semantic_title": "caddreamer: cad object generation from single-view images",
    "citation_count": 6,
    "authors": [
      "Yuan Li",
      "Cheng Lin",
      "Yuan Liu",
      "Xiaoxiao Long",
      "Chenxu Zhang",
      "Ningna Wang",
      "Xin Li",
      "Wenping Wang",
      "Xiaohu Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Vision-Language_Model_IP_Protection_via_Prompt-based_Learning_CVPR_2025_paper.html": {
    "title": "Vision-Language Model IP Protection via Prompt-based Learning",
    "volume": "main",
    "abstract": "Vision-language models (VLMs) like CLIP (Contrastive Language-Image Pre-Training) have seen remarkable success in visual recognition, highlighting the increasing need to safeguard the intellectual property (IP) of well-trained models. Effective IP protection extends beyond ensuring authorized usage; it also necessitates restricting model deployment to authorized data domains, particularly when the model is fine-tuned for specific target domains. However, current IP protection methods often rely solely on the visual backbone, which may lack sufficient semantic richness. To bridge this gap, we introduce IP-CLIP, a lightweight IP protection strategy tailored to CLIP, employing a prompt-based learning approach. By leveraging the frozen visual backbone of CLIP, we extract both image style and content information, incorporating them into the learning of IP prompt. This strategy acts as a robust barrier, effectively preventing the unauthorized transfer of features from authorized domains to unauthorized ones. Additionally, we propose a style-enhancement branch that constructs feature banks for both authorized and unauthorized domains. This branch integrates self-enhanced and cross-domain features, further strengthening IP-CLIP's capability to block features from unauthorized domains. Finally, we present new three metrics designed to better balance the performance degradation of authorized and unauthorized domains. Comprehensive experiments in various scenarios demonstrate its promising potential for application in IP protection tasks for VLMs",
    "checked": true,
    "id": "e8bd21bada5a93a05ad7f90b54b1875068c1f63a",
    "semantic_title": "vision-language model ip protection via prompt-based learning",
    "citation_count": 0,
    "authors": [
      "Lianyu Wang",
      "Meng Wang",
      "Huazhu Fu",
      "Daoqiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bai_Wheres_the_Liability_in_the_Generative_Era_Recovery-based_Black-Box_Detection_CVPR_2025_paper.html": {
    "title": "Where's the Liability in the Generative Era? Recovery-based Black-Box Detection of AI-Generated Content",
    "volume": "main",
    "abstract": "The recent proliferation of photorealistic images created by generative models has sparked both excitement and concern, as these images are increasingly indistinguishable from real ones to the human eye. While offering new creative and commercial possibilities, the potential for misuse, such as in misinformation and fraud, highlights the need for effective detection methods. Current detection approaches often rely on access to model weights or require extensive collections of real image datasets, limiting their scalability and practical application in real-world scenarios. In this work, we introduce a novel black-box detection framework that requires only API access, sidestepping the need for model weights or large auxiliary datasets. Our approach leverages a corrupt-and-recover strategy: by masking part of an image and assessing the model's ability to reconstruct it, we measure the likelihood that the image was generated by the model itself. For black-box models that do not support masked-image inputs, we incorporate a cost-efficient surrogate model trained to align with the target model's distribution, enhancing detection capability. Our framework demonstrates strong performance, outperforming baseline methods by 4.31% in mean average precision across eight diffusion model variant datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyue Bai",
      "Yiyou Sun",
      "Wei Cheng",
      "Haifeng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Kiss3DGen_Repurposing_Image_Diffusion_Models_for_3D_Asset_Generation_CVPR_2025_paper.html": {
    "title": "Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation",
    "volume": "main",
    "abstract": "Diffusion models have achieved great success in generating 2D images. However, the quality and generalizability of 3D content generation remain limited. State-of-the-art methods often require large-scale 3D assets for training, which are challenging to collect. In this work, we introduce Kiss3DGen (Keep It Simple and Straightforward in 3D Generation), an efficient framework for generating, editing, and enhancing 3D objects by repurposing a well-trained 2D image diffusion model for 3D generation. Specifically, we fine-tune a diffusion model to generate \"3D Bundle Image\", a tiled representation composed of multi-view images and their corresponding normal maps. The normal maps are then used to reconstruct a 3D mesh, and the multi-view images provide texture mapping, resulting in a complete 3D model. This simple method effectively transforms the 3D generation problem into a 2D image generation task, maximizing the utilization of knowledge in pretrained diffusion models. Furthermore, we demonstrate that our Kiss3DGen model is compatible with various diffusion model techniques, enabling advanced features such as 3D editing, mesh and texture enhancement, etc. Through extensive experiments, we demonstrate the effectiveness of our approach, showcasing its ability to produce high-quality 3D models efficiently",
    "checked": true,
    "id": "cc6c64b4fec00e5371b38d7c4dfa10a1b23c76fe",
    "semantic_title": "kiss3dgen: repurposing image diffusion models for 3d asset generation",
    "citation_count": 5,
    "authors": [
      "Jiantao Lin",
      "Xin Yang",
      "Meixi Chen",
      "Yingjie Xu",
      "Dongyu Yan",
      "Leyi Wu",
      "Xinli Xu",
      "Lie Xu",
      "Shunsi Zhang",
      "Ying-Cong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mantri_DiTASK_Multi-Task_Fine-Tuning_with_Diffeomorphic_Transformations_CVPR_2025_paper.html": {
    "title": "DiTASK: Multi-Task Fine-Tuning with Diffeomorphic Transformations",
    "volume": "main",
    "abstract": "Pre-trained Vision Transformers now serve as powerful tools for computer vision. Yet, efficiently adapting them for multiple tasks remains a challenge that arises from the need to modify the rich hidden representations encoded by the learned weight matrices, without inducing interference between tasks. Current parameter-efficient methods like LoRA, which apply low-rank updates, force tasks to compete within constrained subspaces, ultimately degrading performance. We introduce DiTASK, a novel Diffeomorphic Multi-Task Fine-Tuning approach that maintains pre-trained representations by preserving weight matrix singular vectors, while enabling task-specific adaptations through neural diffeomorphic transformations of the singular values. By following this approach, DiTASK enables both shared and task-specific feature modulations with minimal added parameters. Our theoretical analysis shows that DiTASK achieves full-rank updates during optimization, preserving the geometric structure of pre-trained features, and establishing a new paradigm for efficient multi-task learning (MTL). Our experiments on PASCAL MTL and NYUD show that DiTASK achieves state-of-the-art performance across four dense prediction tasks, using 75% fewer parameters than existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krishna Sri Ipsit Mantri",
      "Carola-Bibiane Schönlieb",
      "Bruno Ribeiro",
      "Chaim Baskin",
      "Moshe Eliasof"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xi_OW-OVD_Unified_Open_World_and_Open_Vocabulary_Object_Detection_CVPR_2025_paper.html": {
    "title": "OW-OVD: Unified Open World and Open Vocabulary Object Detection",
    "volume": "main",
    "abstract": "Open world perception expands traditional closed-set frameworks, which assume a predefined set of known categories, to encompass dynamic real-world environments. Open World Object Detection (OWOD) and Open Vocabulary Object Detection (OVD) are two main research directions, each addressing unique challenges in dynamic environments. However, existing studies often focus on only one of these tasks, leaving the combined challenges of OWOD and OVD largely underexplored. In this paper, we propose a novel detector, OW-OVD, which inherits the zero-shot generalization capability of OVD detectors while incorporating the ability to actively detect unknown objects and progressively optimize performance through incremental learning, as seen in OWOD detectors. To achieve this, we start with a standard OVD detector and adapt it for OWOD tasks. For attribute selection, we propose the Visual Similarity Attribute Selection (VSAS) method, which identifies the most generalizable attributes by computing similarity distributions across annotated and unannotated regions. Additionally, to ensure the diversity of attributes, we incorporate a similarity constraint in the iterative process. Finally, to preserve the standard inference process of OVD, we propose the Hybrid Attribute-Uncertainty Fusion (HAUF) method. This method combines attribute similarity with known class uncertainty to infer the likelihood of an object belonging to an unknown class. We validated the effectiveness of OW-OVD through evaluations on two OWOD benchmarks, M-OWODB and S-OWODB. The results demonstrate that OW-OVD outperforms existing state-of-the-art models, achieving a +15.3 improvement in unknown object recall (U-Recall) and a +15.5 increase in unknown class average precision (U-mAP). Our code is available at: https://github.com/xxyzll/OW_OVD",
    "checked": true,
    "id": "15c53a098da6e6d2cfe5d74ac3a18f866183393a",
    "semantic_title": "ow-ovd: unified open world and open vocabulary object detection",
    "citation_count": 1,
    "authors": [
      "Xing Xi",
      "Yangyang Huang",
      "Ronghua Luo",
      "Yu Qiu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing_CVPR_2025_paper.html": {
    "title": "Improving Diffusion Inverse Problem Solving with Decoupled Noise Annealing",
    "volume": "main",
    "abstract": "Diffusion models have recently achieved success in solving Bayesian inverse problems with learned data priors. Current methods build on top of the diffusion sampling process, where each denoising step makes small modifications to samples from the previous step. However, this process struggles to correct errors from earlier sampling steps, leading to worse performance in complicated nonlinear inverse problems, such as phase retrieval. To address this challenge, we propose a new method called Decoupled Annealing Posterior Sampling (DAPS) that relies on a novel noise annealing process. Specifically, we decouple consecutive steps in a diffusion sampling trajectory, allowing them to vary considerably from one another while ensuring their time-marginals anneal to the true posterior as we reduce noise levels. This approach enables the exploration of a larger solution space, improving the success rate for accurate reconstructions. We demonstrate that DAPS significantly improves sample quality and stability across multiple image restoration tasks, particularly in complicated nonlinear inverse problems",
    "checked": true,
    "id": "26820c6371feb72c6487d3131e64152dee99614a",
    "semantic_title": "improving diffusion inverse problem solving with decoupled noise annealing",
    "citation_count": 51,
    "authors": [
      "Bingliang Zhang",
      "Wenda Chu",
      "Julius Berner",
      "Chenlin Meng",
      "Anima Anandkumar",
      "Yang Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_AvatarArtist_Open-Domain_4D_Avatarization_CVPR_2025_paper.html": {
    "title": "AvatarArtist: Open-Domain 4D Avatarization",
    "volume": "main",
    "abstract": "This work focuses on open-domain 4D avatarization, with the purpose of creating a 4D avatar from a portrait image in an arbitrary style. We select parametric triplanes as the intermediate 4D representation, and propose a practical training paradigm that takes advantage of both generative adversarial networks (GANs) and diffusion models. Our design stems from the observation that 4D GANs excel at bridging images and triplanes without supervision yet usually face challenges in handling diverse data distributions. A robust 2D diffusion prior emerges as the solution, assisting the GAN in transferring its expertise across various domains. The synergy between these experts permits the construction of a multi-domain image-triplane dataset, which drives the development of a general 4D avatar creator. Extensive experiments suggest that our model, termed AvatarArtist, is capable of producing high-quality 4D avatars with strong robustness to various source image domains. The code, the data, and the models will be made publicly available to facilitate future studies",
    "checked": true,
    "id": "23e637f6076535fd266ecc4048338cf058a4a8f8",
    "semantic_title": "avatarartist: open-domain 4d avatarization",
    "citation_count": 7,
    "authors": [
      "Hongyu Liu",
      "Xuan Wang",
      "Ziyu Wan",
      "Yue Ma",
      "Jingye Chen",
      "Yanbo Fan",
      "Yujun Shen",
      "Yibing Song",
      "Qifeng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "DesignDiffusion: High-Quality Text-to-Design Image Generation with Diffusion Models",
    "volume": "main",
    "abstract": "In this paper, we present DesignDiffusion, a simple yet effective framework for the novel task of synthesizing design images from textual descriptions. A primary challenge lies in generating accurate and style-consistent textual and visual content. Existing works in a related task of visual text generation often focus on generating text within given specific regions, which limits the creativity of generation models, resulting in style or color inconsistencies between textual and visual elements if applied to design image generation. To address this issue, we propose an end-to-end, one-stage diffusion-based framework that avoids intricate components like position and layout modeling. Specifically, the proposed framework directly synthesizes textual and visual design elements from user prompts. It utilizes a distinctive character embedding derived from the visual text to enhance the input prompt, along with a character localization loss for enhanced supervision during text generation. Furthermore, we employ a self-play Direct Preference Optimization fine-tuning strategy to improve the quality and accuracy of the synthesized visual text. Extensive experiments demonstrate that DesignDiffusion achieves state-of-the-art performance in design image generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhendong Wang",
      "Jianmin Bao",
      "Shuyang Gu",
      "Dong Chen",
      "Wengang Zhou",
      "Houqiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liao_Using_Powerful_Prior_Knowledge_of_Diffusion_Model_in_Deep_Unfolding_CVPR_2025_paper.html": {
    "title": "Using Powerful Prior Knowledge of Diffusion Model in Deep Unfolding Networks for Image Compressive Sensing",
    "volume": "main",
    "abstract": "Recently, Deep Unfolding Networks (DUNs) have achieved impressive reconstruction quality in the field of image Compressive Sensing (CS) by unfolding iterative optimization algorithms into neural networks. The reconstruction quality of DUNs depends on the learned prior knowledge, so introducing stronger prior knowledge can further improve reconstruction quality. On the other hand, pre-trained diffusion models contain powerful prior knowledge and have a solid theoretical foundation and strong scalability, but it requires a large number of iterative steps to achieve reconstruction. In this paper, we propose to use the powerful prior knowledge of pre-trained diffusion model in DUNs to achieve high-quality reconstruction with less steps for image CS. Specifically, we first design an iterative optimization algorithm named Diffusion Message Passing (DMP), which embeds a pre-trained diffusion model into each iteration process of DMP. Then, we deeply unfold the DMP algorithm into a neural network named DMP-DUN. The proposed DMP-DUN can use lightweight neural networks to achieve mapping from measurement data to the intermediate steps of the reverse diffusion process and directly approximate the divergence of the diffusion model, thereby further improving reconstruction efficiency. Extensive experiments show that our proposed DMP-DUN achieves state-of-the-art performance and requires at least only 2 steps to reconstruct the image. Codes are available at https://github.com/FengodChen/DMP-DUN-CVPR2025",
    "checked": true,
    "id": "9ccd10109958ed52731b1d5c05abc4d9ec92ab93",
    "semantic_title": "using powerful prior knowledge of diffusion model in deep unfolding networks for image compressive sensing",
    "citation_count": 1,
    "authors": [
      "Chen Liao",
      "Yan Shen",
      "Dan Li",
      "Zhongli Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Koala-36M_A_Large-scale_Video_Dataset_Improving_Consistency_between_Fine-grained_Conditions_CVPR_2025_paper.html": {
    "title": "Koala-36M: A Large-scale Video Dataset Improving Consistency between Fine-grained Conditions and Video Content",
    "volume": "main",
    "abstract": "With the continuous progress of visual generation technologies, the scale of video datasets has grown exponentially. The quality of these datasets plays a pivotal role in the performance of video generation models. We assert that temporal splitting, detailed captions, and video quality filtering are three crucial determinants of dataset quality. However, existing datasets exhibit various limitations in these areas. To address these challenges, we introduce Koala-36M, a large-scale, high-quality video dataset featuring accurate temporal splitting, detailed captions, and superior video quality. The essence of our approach lies in improving the consistency between fine-grained conditions and video content. Specifically, we employ a linear classifier on probability distributions to enhance the accuracy of transition detection, ensuring better temporal consistency. We then provide structured captions for the splitted videos, with an average length of 200 words, to improve text-video alignment. Additionally, we develop a Video Training Suitability Score (VTSS) that integrates multiple sub-metrics, allowing us to filter high-quality videos from the original corpus. Finally, we incorporate several metrics into the training process of the generation model, further refining the fine-grained conditions. Our experiments demonstrate the effectiveness of our data processing pipeline and the quality of the proposed Koala-36M dataset. Our dataset and code have been released at https://koala36m.github.io/",
    "checked": false,
    "id": "6f004592aa9d3baa772f75a347720c1f2c6b5774",
    "semantic_title": "koala-36m : a large-scale video dataset improving consistency between fine-grained conditions and video content",
    "citation_count": 50,
    "authors": [
      "Qiuheng Wang",
      "Yukai Shi",
      "Jiarong Ou",
      "Rui Chen",
      "Ke Lin",
      "Jiahao Wang",
      "Boyuan Jiang",
      "Haotian Yang",
      "Mingwu Zheng",
      "Xin Tao",
      "Fei Yang",
      "Pengfei Wan",
      "Di Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhuang_VASparse_Towards_Efficient_Visual_Hallucination_Mitigation_via_Visual-Aware_Token_Sparsification_CVPR_2025_paper.html": {
    "title": "VASparse: Towards Efficient Visual Hallucination Mitigation via Visual-Aware Token Sparsification",
    "volume": "main",
    "abstract": "Large Vision-Language Models (LVLMs) may produce outputs that are unfaithful to reality, also known as visual hallucinations (VH), which significantly impedes their real-world usage. To alleviate VH, various decoding strategies have been proposed to enhance visual information. However, many of these methods may require secondary decoding and rollback, which significantly reduces inference speed. In this work, we propose an efficient plug-and-play decoding algorithm via Visual-Aware Sparsification (VASparse) from the perspective of token sparsity for mitigating VH. VASparse is inspired by empirical observations: (1) the sparse activation of attention in LVLMs, and (2) visual-agnostic tokens sparsification exacerbates VH. Based on these insights, we propose a novel token sparsification strategy that balances efficiency and trustworthiness. Specifically, VASparse implements a visual-aware token selection strategy during decoding to reduce redundant tokens while preserving visual context effectively. Additionally, we innovatively introduce a sparse-based visual contrastive decoding method to recalibrate the distribution of hallucinated outputs without the time overhead associated with secondary decoding. Subsequently, VASparse recalibrates attention scores to penalize attention sinking of LVLMs towards text tokens. Extensive experiments across four popular benchmarks confirm the effectiveness of VASparse in mitigating VH across different LVLM families without requiring additional training or post-processing. Impressively, VASparse achieves state-of-the-art performance for mitigating VH while maintaining competitive decoding speed. Code is available at https://github.com/mengchuang123/VASparse-github",
    "checked": true,
    "id": "90de34816d7b5aec17f8bf048b75de7d257cbaac",
    "semantic_title": "vasparse: towards efficient visual hallucination mitigation via visual-aware token sparsification",
    "citation_count": 2,
    "authors": [
      "Xianwei Zhuang",
      "Zhihong Zhu",
      "Yuxin Xie",
      "Liming Liang",
      "Yuexian Zou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Miller_SPARC_Score_Prompting_and_Adaptive_Fusion_for_Zero-Shot_Multi-Label_Recognition_CVPR_2025_paper.html": {
    "title": "SPARC: Score Prompting and Adaptive Fusion for Zero-Shot Multi-Label Recognition in Vision-Language Models",
    "volume": "main",
    "abstract": "Zero-shot multi-label recognition (MLR) with Vision-Language Models (VLMs) faces significant challenges without training data, model tuning, or architectural modifications. Existing approaches require prompt tuning or architectural adaptations, limiting zero-shot applicability. Our work proposes a novel solution treating VLMs as black boxes, leveraging scores without training data or ground truth. We make two contributions. First, we find that VLM scores suffer from image- and prompt-specific biases, and that simple standardization is surprisingly effective at removing these and boosting MLR performance. And second, we introduce compound prompts grounded in realistic object combinations. Our analysis reveals \"AND\"/\"OR\" signal ambiguities that cause maximum compound scores to be surprisingly suboptimal compared to second-highest scores. We introduce an adaptive fusion method to address this issue. Our method enhances other zero-shot approaches, consistently improving their results. Experiments show superior mean Average Precision (mAP) compared to methods requiring training data, achieved through refined object ranking for robust zero-shot MLR. Code can be found at https://github.com/kjmillerCURIS/SPARC",
    "checked": true,
    "id": "6bf0fe65df87abeac6e1b774643d27e6e85af959",
    "semantic_title": "sparc: score prompting and adaptive fusion for zero-shot multi-label recognition in vision-language models",
    "citation_count": 1,
    "authors": [
      "Kevin Miller",
      "Aditya Gangrade",
      "Samarth Mishra",
      "Kate Saenko",
      "Venkatesh Saligrama"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_UniGoal_Towards_Universal_Zero-shot_Goal-oriented_Navigation_CVPR_2025_paper.html": {
    "title": "UniGoal: Towards Universal Zero-shot Goal-oriented Navigation",
    "volume": "main",
    "abstract": "In this paper, we propose a general framework for universal zero-shot goal-oriented navigation. Existing zero-shot methods build inference framework upon large language models (LLM) for specific tasks, which differs a lot in overall pipeline and fails to generalize across different types of goal. Towards the aim of universal zero-shot navigation, we propose a uniform graph representation to unify different goals, including object category, instance image and text description. We also convert the observation of agent into an online maintained scene graph. With this consistent scene and goal representation, we preserve most structural information compared with pure text and are able to leverage LLM for explicit graph-based reasoning.Specifically, we conduct graph matching between the scene graph and goal graph at each time instant and propose different strategies to generate long-term goal of exploration according to different matching states. The agent first iteratively searches subgraph of goal when zero-matched. With partial matching, the agent then utilizes coordinate projection and anchor pair alignment to infer the goal location. Finally scene graph correction and goal verification are applied for perfect matching. We also present a blacklist mechanism to enable robust switch between stages.Extensive experiments on several benchmarks show that our UniGoal achieves state-of-the-art zero-shot performance on three studied navigation tasks with a single model, even outperforming task-specific zero-shot methods and supervised universal methods",
    "checked": true,
    "id": "01bde49e75eaa46baa2619fb239fdcde9dc47a31",
    "semantic_title": "unigoal: towards universal zero-shot goal-oriented navigation",
    "citation_count": 15,
    "authors": [
      "Hang Yin",
      "Xiuwei Xu",
      "Linqing Zhao",
      "Ziwei Wang",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qiu_Noise-Consistent_Siamese-Diffusion_for_Medical_Image_Synthesis_and_Segmentation_CVPR_2025_paper.html": {
    "title": "Noise-Consistent Siamese-Diffusion for Medical Image Synthesis and Segmentation",
    "volume": "main",
    "abstract": "Deep learning has revolutionized medical image segmentation, yet its full potential remains constrained by the paucity of annotated datasets. While diffusion models have emerged as a promising approach for generating synthetic image-mask pairs to augment these datasets, they paradoxically suffer from the same data scarcity challenges they aim to mitigate. Traditional mask-only models frequently yield low-fidelity images due to their inability to adequately capture morphological intricacies, which can critically compromise the robustness and reliability of segmentation models. To alleviate this limitation, we introduce Siamese-Diffusion, a novel dual-component model comprising Mask-Diffusion and Image-Diffusion. During training, a Noise Consistency Loss is introduced between these components to enhance the morphological fidelity of Mask-Diffusion in the parameter space. During sampling, only Mask-Diffusion is used, ensuring diversity and scalability. Comprehensive experiments demonstrate the superiority of our method. Siamese-Diffusion boosts SANet's mDice and mIoU by 3.6% and 4.4% on the Polyps, while UNet improves by 1.52% and 1.64% on the ISIC2018",
    "checked": true,
    "id": "88ecda31867c681925d4fe52d01ded7c13a9f6be",
    "semantic_title": "noise-consistent siamese-diffusion for medical image synthesis and segmentation",
    "citation_count": 10,
    "authors": [
      "Kunpeng Qiu",
      "Zhiqiang Gao",
      "Zhiying Zhou",
      "Mingjie Sun",
      "Yongxin Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual_CVPR_2025_paper.html": {
    "title": "DefectFill: Realistic Defect Generation with Inpainting Diffusion Model for Visual Inspection",
    "volume": "main",
    "abstract": "Developing effective visual inspection models remains challenging due to the scarcity of defect data. While image generation models have been used to synthesize defect images, producing highly realistic defects remains difficult. We propose DefectFill, a novel method for realistic defect generation that requires only a few reference defect images. It leverages a fine-tuned inpainting diffusion model, optimized with our custom loss functions incorporating defect, object, and attention terms. It enables precise capture of detailed, localized defect features and their seamless integration into defect-free objects. Additionally, our Low-Fidelity Selection method further enhances the defect sample quality. Experiments show that DefectFill generates high-quality defect images, enabling visual inspection models to achieve state-of-the-art performance on the MVTec AD dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaewoo Song",
      "Daemin Park",
      "Kanghyun Baek",
      "Sangyub Lee",
      "Jooyoung Choi",
      "Eunji Kim",
      "Sungroh Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Less_is_More_Efficient_Image_Vectorization_with_Adaptive_Parameterization_CVPR_2025_paper.html": {
    "title": "Less is More: Efficient Image Vectorization with Adaptive Parameterization",
    "volume": "main",
    "abstract": "Image vectorization aims to convert raster images to vector ones, allowing for easy scaling and editing.Existing works mainly rely on preset parameters (i.e., a fixed number of paths and control points), ignoring the complexity of the image and posing significant challenges to practical applications.We demonstrate that such an assumption is often incorrect, as the preset paths or control points may be neither essential nor enough to achieve accurate and editable vectorization results.Based on this key insight, in this paper, we propose AdaVec, an efficient image vectorization method with adaptive parametrization, where the paths and control points can be adjusted dynamically based on the complexity of the input raster image.In particular, we first decompose the input raster image into a set of pure-colored layers that are aligned with human perception.For each layer with varying shape complexity, we propose a novel allocation mechanism to adaptively adjust the control point distribution.We further adopt a differentiable rendering process to compose and optimize the shape and color parameters of each layer iteratively.Extensive experiments demonstrate that AdaVec outperforms the baselines qualitatively and quantitatively, in terms of computational efficiency, vectorization accuracy, and editing flexibility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaibo Zhao",
      "Liang Bao",
      "Yufei Li",
      "Xu Su",
      "Ke Zhang",
      "Xiaotian Qiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_FedMIA_An_Effective_Membership_Inference_Attack_Exploiting_All_for_One_CVPR_2025_paper.html": {
    "title": "FedMIA: An Effective Membership Inference Attack Exploiting \"All for One\" Principle in Federated Learning",
    "volume": "main",
    "abstract": "Federated Learning (FL) is a promising approach for training machine learning models on decentralized data while preserving privacy. However, privacy risks, particularly Membership Inference Attacks (MIAs), which aim to determine whether a specific data point belongs to a target client's training set, remain a significant concern. Existing methods for implementing MIAs in FL primarily analyze updates from the target client, focusing on metrics such as loss, gradient norm, and gradient difference. However, these methods fail to leverage updates from non-target clients, potentially underutilizing available information.In this paper, we first formulate a one-tailed likelihood-ratio hypothesis test based on the likelihood of updates from non-target clients. Building upon this formulation, we introduce a three-stage Membership Inference Attack (MIA) method, called FedMIA, which follows the \"all for one\"--leveraging updates from all clients across multiple communication rounds to enhance MIA effectiveness. Both theoretical analysis and extensive experimental results demonstrate that FedMIA outperforms existing MIAs in both classification and generative tasks. Additionally, it can be integrated as an extension to existing methods and is robust against various defense strategies, Non-IID data, and different federated structures. Our code is available in https://github.com/Liar-Mask/FedMIA",
    "checked": true,
    "id": "3256c83b21f8feb344667711a25b8b596553f8de",
    "semantic_title": "fedmia: an effective membership inference attack exploiting \"all for one\" principle in federated learning",
    "citation_count": 2,
    "authors": [
      "Gongxi Zhu",
      "Donghao Li",
      "Hanlin Gu",
      "Yuan Yao",
      "Lixin Fan",
      "Yuxing Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways_CVPR_2025_paper.html": {
    "title": "Erase Diffusion: Empowering Object Removal Through Calibrating Diffusion Pathways",
    "volume": "main",
    "abstract": "Erase inpainting, or object removal, aims to precisely remove target objects within masked regions while preserving the overall consistency of the surrounding content. Despite diffusion-based methods have made significant strides in the field of image inpainting, challenges remain regarding the emergence of unexpected objects or artifacts. We assert that the inexact diffusion pathways established by existing standard optimization paradigms constrain the efficacy of object removal. To tackle these challenges, we propose a novel Erase Diffusion, termed EraDiff, aimed at unleashing the potential power of standard diffusion in the context of object removal. In contrast to standard diffusion, the EraDiff adapts both the optimization paradigm and the network to improve the coherence and elimination of the erasure results.We first introduce a Chain-Rectifying Optimization (CRO) paradigm, a sophisticated diffusion process specifically designed to align with the objectives of erasure. This paradigm establishes innovative diffusion transition pathways that simulate the gradual elimination of objects during optimization, allowing the model to accurately capture the intent of object removal. Furthermore, to mitigate deviations caused by artifacts during the sampling pathways, we develop a simple yet effective Self-Rectifying Attention (SRA) mechanism. The SRA calibrates the sampling pathways by altering self-attention activation, allowing the model to effectively bypass artifacts while further enhancing the coherence of the generated content. With this design, our proposed EraDiff achieves state-of-the-art performance on the OpenImages V5 dataset and demonstrates significant superiority in real-world scenarios",
    "checked": true,
    "id": "b49dd3d9f6578aef778a3f082725965c5b5d81e7",
    "semantic_title": "erase diffusion: empowering object removal through calibrating diffusion pathways",
    "citation_count": 2,
    "authors": [
      "Yi Liu",
      "Hao Zhou",
      "Benlei Cui",
      "Wenxiang Shang",
      "Ran Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chowdhury_Prompt-CAM_Making_Vision_Transformers_Interpretable_for_Fine-Grained_Analysis_CVPR_2025_paper.html": {
    "title": "Prompt-CAM: Making Vision Transformers Interpretable for Fine-Grained Analysis",
    "volume": "main",
    "abstract": "We present a simple approach to make pre-trained Vision Transformers (ViTs) interpretable for fine-grained analysis, aiming to identify and localize the traits that distinguish visually similar categories, such as bird species. Pre-trained ViTs, such as DINO, have demonstrated remarkable capabilities in extracting localized, discriminative features. However, saliency maps like Grad-CAM often fail to identify these traits, producing blurred, coarse heatmaps that highlight entire objects instead. We propose a novel approach, Prompt Class Attention Map (Prompt-CAM), to address this limitation. Prompt-CAM learns class-specific prompts for a pre-trained ViT and uses the corresponding outputs for classification. To correctly classify an image, the true-class prompt must attend to unique image patches not present in other classes' images (i.e., traits). As a result, the true class's multi-head attention maps reveal traits and their locations. Implementation-wise, Prompt-CAM is almost a \"free lunch,\" requiring only a modification to the prediction head of Visual Prompt Tuning (VPT). This makes Prompt-CAM easy to train and apply, in stark contrast to other interpretable methods that require designing specific models and training processes. Extensive empirical studies on a dozen datasets from various domains (e.g., birds, fishes, insects, fungi, flowers, food, and cars) validate the superior interpretation capability of Prompt-CAM. The source code and demo are available at https://github.com/Imageomics/Prompt_CAM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arpita Chowdhury",
      "Dipanjyoti Paul",
      "Zheda Mai",
      "Jianyang Gu",
      "Ziheng Zhang",
      "Kazi Sajeed Mehrab",
      "Elizabeth G. Campolongo",
      "Daniel Rubenstein",
      "Charles V. Stewart",
      "Anuj Karpatne",
      "Tanya Berger-Wolf",
      "Yu Su",
      "Wei-Lun Chao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move_CVPR_2025_paper.html": {
    "title": "Instruction-based Image Manipulation by Watching How Things Move",
    "volume": "main",
    "abstract": "This paper introduces a novel dataset construction pipeline that samples pairs of frames from videos and uses multimodal large language models (MLLMs) to generate editing instructions for training instruction-based image manipulation models. Video frames inherently preserve the identity of subjects and scenes, ensuring consistent content preservation during editing. Additionally, video data captures diverse, natural dynamics--such as non-rigid subject motion and complex camera movements--that are difficult to model otherwise, making it an ideal source for scalable dataset construction. Using this approach, we create a new dataset to train InstructMove, a model capable of instruction-based complex manipulations that are difficult to achieve with synthetically generated datasets. Our model demonstrates state-of-the-art performance in tasks such as adjusting subject poses, rearranging elements, and altering camera perspectives",
    "checked": true,
    "id": "32edab3b925a8f5f9ba41418b19af246704d9610",
    "semantic_title": "instruction-based image manipulation by watching how things move",
    "citation_count": 5,
    "authors": [
      "Mingdeng Cao",
      "Xuaner Zhang",
      "Yinqiang Zheng",
      "Zhihao Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Morimitsu_DPFlow_Adaptive_Optical_Flow_Estimation_with_a_Dual-Pyramid_Framework_CVPR_2025_paper.html": {
    "title": "DPFlow: Adaptive Optical Flow Estimation with a Dual-Pyramid Framework",
    "volume": "main",
    "abstract": "Optical flow estimation is essential for video processing tasks, such as restoration and action recognition. The quality of videos is constantly increasing, with current standards reaching 8K resolution. However, optical flow methods are usually designed for low resolution and do not generalize to large inputs due to their rigid architectures. They adopt downscaling or input tiling to reduce the input size, causing a loss of details and global information. There is also a lack of optical flow benchmarks to judge the actual performance of existing methods on high-resolution samples. Previous works only conducted qualitative high-resolution evaluations on hand-picked samples. This paper fills this gap in optical flow estimation in two ways. We propose DPFlow, an adaptive optical flow architecture capable of generalizing up to 8K resolution inputs while trained with only low-resolution samples. We also introduce Kubric-NK, a new benchmark for evaluating optical flow methods with input resolutions ranging from 1K to 8K. Our high-resolution evaluation pushes the boundaries of existing methods and reveals new insights about their generalization capabilities. Extensive experimental results show that DPFlow achieves state-of-the-art results on the MPI-Sintel, KITTI 2015, Spring, and other high-resolution benchmarks. The code and dataset are available at https://github.com/hmorimitsu/ptlflow/tree/main/ptlflow/models/dpflow",
    "checked": true,
    "id": "4cef76d5b81830de17d13e419d14e9f8ad454002",
    "semantic_title": "dpflow: adaptive optical flow estimation with a dual-pyramid framework",
    "citation_count": 4,
    "authors": [
      "Henrique Morimitsu",
      "Xiaobin Zhu",
      "Roberto M. Cesar",
      "Xiangyang Ji",
      "Xu-Cheng Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_DocSAM_Unified_Document_Image_Segmentation_via_Query_Decomposition_and_Heterogeneous_CVPR_2025_paper.html": {
    "title": "DocSAM: Unified Document Image Segmentation via Query Decomposition and Heterogeneous Mixed Learning",
    "volume": "main",
    "abstract": "Document image segmentation is crucial in document analysis and recognition but remains challenging due to the heterogeneity of document formats and diverse segmentation tasks. Existing methods often treat these tasks separately, leading to limited generalization and resource wastage.This paper introduces DocSAM, a transformer-based unified framework for various document image segmentation tasks, including document layout analysis, multi-granularity text segmentation, and table structure recognition by modelling these tasks as a combination of instance and semantic segmentation.Specifically, DocSAM uses a Sentence BERT to map category names from each dataset into semantic queries of the same dimension as instance queries. These queries interact through attention mechanisms and are cross-attended with image features to predict instance and semantic segmentation masks. To predict instance categories, instance queries are dot-producted with semantic queries, and scores are normalized using softmax.As a result, DocSAM can be jointly trained on heterogeneous datasets, enhancing robustness and generalization while reducing computing and storage resources. Comprehensive evaluations show that DocSAM outperforms existing methods in accuracy, efficiency, and adaptability, highlighting its potential for advancing document image understanding and segmentation in various applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao-Hui Li",
      "Fei Yin",
      "Cheng-Lin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Ferret_An_Efficient_Online_Continual_Learning_Framework_under_Varying_Memory_CVPR_2025_paper.html": {
    "title": "Ferret: An Efficient Online Continual Learning Framework under Varying Memory Constraints",
    "volume": "main",
    "abstract": "In the realm of high-frequency data streams, achieving real-time learning within varying memory constraints is paramount. This paper presents Ferret, a comprehensive framework designed to enhance online accuracy of Online Continual Learning (OCL) algorithms while dynamically adapting to varying memory budgets.Ferret employs a fine-grained pipeline parallelism strategy combined with an iterative gradient compensation algorithm, ensuring seamless handling of high-frequency data with minimal latency, and effectively counteracting the challenge of stale gradients in parallel training. To adapt to varying memory budgets, its automated model partitioning and pipeline planning optimizes performance regardless of memory limitations. Extensive experiments across 20 benchmarks and 5 integrated OCL algorithms show Ferret's remarkable efficiency, achieving up to 3.7x lower memory overhead to reach the same online accuracy compared to competing methods.Furthermore, Ferret consistently outperforms these methods across diverse memory budgets, underscoring its superior adaptability. These findings position Ferret as a premier solution for efficient and adaptive OCL framework in real-time environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Zhou",
      "Yuxin Tian",
      "Jindi Lv",
      "Mingjia Shi",
      "Yuanxi Li",
      "Qing Ye",
      "Shuhao Zhang",
      "Jiancheng Lv"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hyung_Spatiotemporal_Skip_Guidance_for_Enhanced_Video_Diffusion_Sampling_CVPR_2025_paper.html": {
    "title": "Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sampling",
    "volume": "main",
    "abstract": "Diffusion models have emerged as a powerful tool for generating high-quality images, videos, and 3D content. While sampling guidance techniques like CFG improve quality, they reduce diversity and motion. Autoguidance mitigates these issues but demands extra weak model training, limiting its practicality for large-scale models. In this work, we introduce Spatiotemporal Skip Guidance (STG), a simple training-free sampling guidance method for enhancing transformer-based video diffusion models. STG employs an implicit weak model via self-perturbation, avoiding the need for external models or additional training. By selectively skipping spatiotemporal layers, STG produces an aligned, degraded version of the original model to boost sample quality without compromising diversity or dynamic degree. Our contributions include: (1) introducing STG as an efficient, high-performing guidance technique for video diffusion models, (2) eliminating the need for auxiliary models by simulating a weak model through layer skipping, and (3) ensuring quality-enhanced guidance without compromising sample diversity or dynamics unlike CFG. For additional results, visit https://junhahyung.github.io/STGuidance",
    "checked": true,
    "id": "7e73f27507f41486a8d6d499783582d827f40008",
    "semantic_title": "spatiotemporal skip guidance for enhanced video diffusion sampling",
    "citation_count": 7,
    "authors": [
      "Junha Hyung",
      "Kinam Kim",
      "Susung Hong",
      "Min-Jung Kim",
      "Jaegul Choo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_VidComposition_Can_MLLMs_Analyze_Compositions_in_Compiled_Videos_CVPR_2025_paper.html": {
    "title": "VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?",
    "volume": "main",
    "abstract": "The advancement of Multimodal Large Language Models (MLLMs) has enabled significant progress in multimodal understanding, expanding their capacity to analyze video content. However, existing evaluation benchmarks for MLLMs primarily focus on abstract video comprehension, lacking a detailed assessment of their ability to understand video compositions, the nuanced interpretation of how visual elements combine and interact within highly compiled video contexts. We introduce VidComposition, a new benchmark specifically designed to evaluate the video composition understanding capabilities of MLLMs using carefully curated compiled videos and cinematic-level annotations.VidComposition includes 982 videos with 1706 multiple-choice questions, covering various compositional aspects such as camera movement, angle, shot size, narrative structure, character actions and emotions, etc. Our comprehensive evaluation of 33 open-source and proprietary MLLMs reveals a significant performance gap between human and model capabilities. This highlights the limitations of current MLLMs in understanding complex, compiled video compositions and offers insights into areas for further improvement. Our benchmark is publicly available at https://yunlong10.github.io/VidComposition/",
    "checked": true,
    "id": "50b900f9d4cf428c97253a1f2289d5628d798bba",
    "semantic_title": "vidcomposition: can mllms analyze compositions in compiled videos?",
    "citation_count": 12,
    "authors": [
      "Yunlong Tang",
      "Junjia Guo",
      "Hang Hua",
      "Susan Liang",
      "Mingqian Feng",
      "Xinyang Li",
      "Rui Mao",
      "Chao Huang",
      "Jing Bi",
      "Zeliang Zhang",
      "Pooyan Fazli",
      "Chenliang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_SAR3D_Autoregressive_3D_Object_Generation_and_Understanding_via_Multi-scale_3D_CVPR_2025_paper.html": {
    "title": "SAR3D: Autoregressive 3D Object Generation and Understanding via Multi-scale 3D VQVAE",
    "volume": "main",
    "abstract": "Autoregressive models have demonstrated remarkable success across various fields, from large language models (LLMs) to large multimodal models (LMMs) and 2D content generation, moving closer to artificial general intelligence (AGI). Despite these advances, applying autoregressive approaches to 3D object generation and understanding remains largely unexplored. This paper introduces Scale AutoRegressive 3D (SAR3D), a novel framework that leverages a multi-scale 3D vector-quantized variational autoencoder (VQVAE) to tokenize 3D objects for efficient autoregressive generation and detailed understanding. By predicting the next scale in a multi-scale latent representation instead of the next single token, SAR3D reduces generation time significantly, achieving fast 3D object generation in just 0.82 seconds on an A6000 GPU. Additionally, given the tokens enriched with hierarchical 3D-aware information, we finetune a pretrained LLM on them, enabling multimodal comprehension of 3D content.Our experiments show that SAR3D surpasses current 3D generation methods in both speed and quality and allows LLMs to interpret and caption 3D models comprehensively",
    "checked": true,
    "id": "c298686fcac0262bb3bb46f1a6932a9b3e4c95cd",
    "semantic_title": "sar3d: autoregressive 3d object generation and understanding via multi-scale 3d vqvae",
    "citation_count": 7,
    "authors": [
      "Yongwei Chen",
      "Yushi Lan",
      "Shangchen Zhou",
      "Tengfei Wang",
      "Xingang Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jin_Dual-Interrelated_Diffusion_Model_for_Few-Shot_Anomaly_Image_Generation_CVPR_2025_paper.html": {
    "title": "Dual-Interrelated Diffusion Model for Few-Shot Anomaly Image Generation",
    "volume": "main",
    "abstract": "The performance of anomaly inspection in industrial manufacturing is constrained by the scarcity of anomaly data. To overcome this challenge, researchers have started employing anomaly generation approaches to augment the anomaly dataset. However, existing anomaly generation methods suffer from limited diversity in the generated anomalies and struggle to achieve a seamless blending of this anomaly with the original image. Moreover, the generated mask is usually not aligned with the generated anomaly. In this paper, we overcome these challenges from a new perspective, simultaneously generating a pair of the overall image and the corresponding anomaly part. We propose DualAnoDiff, a novel diffusion-based few-shot anomaly image generation model, which can generate diverse and realistic anomaly images by using a dual-interrelated diffusion model, where one of them is employed to generate the whole image while the other one generates the anomaly part. Moreover, we extract background and shape information to mitigate the distortion and blurriness phenomenon in few-shot image generation. Extensive experiments demonstrate the superiority of our proposed model over state-of-the-art methods in terms of diversity, realism and the accuracy of mask. Overall, our approach significantly improves the performance of downstream anomaly inspection tasks, including anomaly detection, anomaly localization, and anomaly classification tasks. Code will be made available",
    "checked": true,
    "id": "ffae095d0aff2a712638d12c646aee5fdc51e911",
    "semantic_title": "dual-interrelated diffusion model for few-shot anomaly image generation",
    "citation_count": 12,
    "authors": [
      "Ying Jin",
      "Jinlong Peng",
      "Qingdong He",
      "Teng Hu",
      "Jiafu Wu",
      "Hao Chen",
      "Haoxuan Wang",
      "Wenbing Zhu",
      "Mingmin Chi",
      "Jun Liu",
      "Yabiao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tu_ODE_Open-Set_Evaluation_of_Hallucinations_in_Multimodal_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models",
    "volume": "main",
    "abstract": "Hallucination poses a persistent challenge for multimodal large language models (MLLMs). However, existing benchmarks for evaluating hallucinations are generally static, which may overlook the potential risk of data contamination. To address this issue, we propose ODE, an open-set, dynamic protocol designed to evaluate object hallucinations in MLLMs at both the existence and attribute levels. ODE employs a graph-based structure to represent real-world object concepts, their attributes, and the distributional associations between them. This structure facilitates the extraction of concept combinations based on diverse distributional criteria, generating varied samples for structured queries that evaluate hallucinations in both generative and discriminative tasks. Through the generation of new samples, dynamic concept combinations, and varied distribution frequencies, ODE mitigates the risk of data contamination and broadens the scope of evaluation. This protocol is applicable to both general and specialized scenarios, including those with limited data. Experimental results demonstrate the effectiveness of our protocol, revealing that MLLMs exhibit higher hallucination rates when evaluated with ODE-generated samples, which indicates potential data contamination. Furthermore, these generated samples aid in analyzing hallucination patterns and fine-tuning models, offering an effective approach to mitigating hallucinations in MLLMs",
    "checked": true,
    "id": "ac7cc880f897626ee2d2d5a5c40180d551f4e0f8",
    "semantic_title": "ode: open-set evaluation of hallucinations in multimodal large language models",
    "citation_count": 3,
    "authors": [
      "Yahan Tu",
      "Rui Hu",
      "Jitao Sang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Self-Supervised_Learning_for_Color_Spike_Camera_Reconstruction_CVPR_2025_paper.html": {
    "title": "Self-Supervised Learning for Color Spike Camera Reconstruction",
    "volume": "main",
    "abstract": "Spike camera is a kind of neuromorphic camera with ultra-high temporal resolution, which can capture dynamic scenes by continuously firing spike signals. To capture color information, a color filter array (CFA) is employed on the sensor of the spike camera, resulting in Bayer-pattern spike streams. How to restore high-quality color images from the binary spike signals remains challenging. In this paper, we propose a motion-guided reconstruction method for spike cameras with CFA, utilizing color layout and estimated motion information. Specifically, we develop a joint motion estimation pipeline for the Bayer-pattern spike stream, exploiting the motion consistency of channels. We propose to estimate the missing pixels of each color channel according to temporally neighboring pixels of the corresponding color along the motion trajectory. As the spike signals are read out at discrete time points, there is quantization noise that impacts the image quality. Thus, we analyze the correlation of the noise in spatial and temporal domains and propose a self-supervised network utilizing a masked spike encoder to handle the noise. Experiments on real-world captured Bayer-pattern spike streams show that our method can restore color images with better visual quality, compared with state-of-the-art methods. The source codes are available at https://github.com/csycdong/SSL-CSC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanchen Dong",
      "Ruiqin Xiong",
      "Xiaopeng Fan",
      "Zhaofei Yu",
      "Yonghong Tian",
      "Tiejun Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huy_Interactive_Medical_Image_Analysis_with_Concept-based_Similarity_Reasoning_CVPR_2025_paper.html": {
    "title": "Interactive Medical Image Analysis with Concept-based Similarity Reasoning",
    "volume": "main",
    "abstract": "The ability to interpret and intervene model decisions is important for the adoption of computer-aided diagnosis methods in clinical workflows. Recent concept-based methods link the model predictions with interpretable concepts and modify their activation scores to interact with the model. However, these concepts are at the image level, which hinders the model from pinpointing the exact patches the concepts are activated. Alternatively, prototype-based methods learn representations from training image patches and compare these with test image patches, using the similarity scores for final class prediction. However, interpreting the underlying concepts of these patches can be challenging and often necessitates post-hoc guesswork. To address this issue, this paper introduces the novel Concept-based Similarity Reasoning network (CSR), which offers (i) patch-level prototype with intrinsic concept interpretation, and (ii) spatial interactivity. First, the proposed CSR provides localized explanation by grounding prototypes of each concept on image regions. Second, our model introduces novel spatial-level interaction, allowing doctors to engage directly with specific image areas, making it an intuitive and transparent tool for medical imaging. CSR improves upon prior state-of-the-art interpretable methods by up to 4.% across three biomedical datasets",
    "checked": true,
    "id": "6a95909e870ea3ba0e6739182f75455f50f30ea1",
    "semantic_title": "interactive medical image analysis with concept-based similarity reasoning",
    "citation_count": 4,
    "authors": [
      "Ta Duc Huy",
      "Sen Kim Tran",
      "Phan Nguyen",
      "Nguyen Hoang Tran",
      "Tran Bao Sam",
      "Anton van den Hengel",
      "Zhibin Liao",
      "Johan W. Verjans",
      "Minh-Son To",
      "Vu Minh Hieu Phan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_From_Elements_to_Design_A_Layered_Approach_for_Automatic_Graphic_CVPR_2025_paper.html": {
    "title": "From Elements to Design: A Layered Approach for Automatic Graphic Design Composition",
    "volume": "main",
    "abstract": "In this work, we investigate automatic design composition from multimodal graphic elements. Although recent studies have developed various generative models for graphic design, they usually face the following limitations: they only focus on certain subtasks and are far from achieving the design composition task; they do not consider the hierarchical information of graphic designs during the generation process. To tackle these issues, we introduce the layered design principle into Large Multimodal Models (LMMs) and propose a novel approach, called LaDeCo, to accomplish this challenging task. Specifically, LaDeCo first performs layer planning for a given element set, dividing the input elements into different semantic layers according to their contents. Based on the planning results, it subsequently predicts element attributes that control the design composition in a layer-wise manner, and includes the rendered image of previously generated layers into the context. With this insightful design, LaDeCo decomposes the difficult task into smaller manageable steps, making the generation process smoother and clearer. The experimental results demonstrate the effectiveness of LaDeCo in design composition. Furthermore, we show that LaDeCo enables some interesting applications in graphic design, such as resolution adjustment, design decoration, design variation, etc. In addition, it even outperforms the specialized models in some design subtasks without any task-specific training",
    "checked": true,
    "id": "aeb6bd1ce4d53dd789801a60d5740300e420da56",
    "semantic_title": "from elements to design: a layered approach for automatic graphic design composition",
    "citation_count": 0,
    "authors": [
      "Jiawei Lin",
      "Shizhao Sun",
      "Danqing Huang",
      "Ting Liu",
      "Ji Li",
      "Jiang Bian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nguyen_h-Edit_Effective_and_Flexible_Diffusion-Based_Editing_via_Doobs_h-Transform_CVPR_2025_paper.html": {
    "title": "h-Edit: Effective and Flexible Diffusion-Based Editing via Doob's h-Transform",
    "volume": "main",
    "abstract": "We introduce a theoretical framework for diffusion-based image editing by formulating it as a reverse-time bridge modeling problem. This approach modifies the backward process of a pretrained diffusion model to construct a bridge that converges to an implicit distribution associated with the editing target at time 0. Building on this framework, we propose h-Edit, a novel editing method that utilizes Doob's h-transform and Langevin Monte Carlo to decompose the update of an intermediate edited sample into two components: a \"reconstruction\" term and an \"editing\" term. This decomposition provides flexibility, allowing the reconstruction term to be computed via existing inversion techniques and enabling the combination of multiple editing terms to handle complex editing tasks. To our knowledge, h-Edit is the first training-free method capable of performing simultaneous text-guided and reward-model-based editing. Extensive experiments, both quantitative and qualitative, show that h-Edit outperforms state-of-the-art baselines in terms of editing effectiveness and faithfulness",
    "checked": true,
    "id": "e96e619638eb6444618c0721dce2af43f330c24e",
    "semantic_title": "h-edit: effective and flexible diffusion-based editing via doob's h-transform",
    "citation_count": 2,
    "authors": [
      "Toan Nguyen",
      "Kien Do",
      "Duc Kieu",
      "Thin Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Heo_Masking_meets_Supervision_A_Strong_Learning_Alliance_CVPR_2025_paper.html": {
    "title": "Masking meets Supervision: A Strong Learning Alliance",
    "volume": "main",
    "abstract": "Pre-training with random masked inputs has emerged as a novel trend in self-supervised training. However, supervised learning still faces a challenge in adopting masking augmentations, primarily due to unstable training. In this paper, we propose a novel way to involve masking augmentations dubbed Masked Sub-branch (MaskSub). MaskSub consists of the main-branch and sub-branch, the latter being a part of the former. The main-branch undergoes conventional training recipes, while the sub-branch merits intensive masking augmentations, during training. MaskSub tackles the challenge by mitigating adverse effects through a relaxed loss function similar to a self-distillation loss. Our analysis shows that MaskSub improves performance, with the training loss converging faster than in standard training, which suggests our method stabilizes the training process. We further validate MaskSub across diverse training scenarios and models, including DeiT-III training, MAE finetuning, CLIP finetuning, BERT training, and hierarchical architectures (ResNet and Swin Transformer). Our results show that MaskSub consistently achieves impressive performance gains across all the cases. MaskSub provides a practical and effective solution for introducing additional regularization under various training recipes. Code available at https://github.com/naver-ai/augsub",
    "checked": true,
    "id": "886d6ce09ad3804c29d0754c02265f00fb5ff26b",
    "semantic_title": "masking meets supervision: a strong learning alliance",
    "citation_count": 3,
    "authors": [
      "Byeongho Heo",
      "Taekyung Kim",
      "Sangdoo Yun",
      "Dongyoon Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_DI-PCG_Diffusion-based_Efficient_Inverse_Procedural_Content_Generation_for_High-quality_3D_CVPR_2025_paper.html": {
    "title": "DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation",
    "volume": "main",
    "abstract": "Procedural Content Generation (PCG) is powerful in creating high-quality 3D contents, yet controlling it to produce desired shapes is difficult and often requires extensive parameter tuning. Inverse Procedural Content Generation aims to automatically find the best parameters under the input condition. However, existing sampling-based and neural network-based methods still suffer from numerous sample iterations or limited controllability. In this work, we present DI-PCG, a novel and efficient method for Inverse PCG from general image conditions. At its core is a lightweight diffusion transformer model, where PCG parameters are directly treated as the denoising target and the observed images as conditions to control parameter generation. DI-PCG is efficient and effective. With only 7.6M network parameters and 30 GPU hours to train, it demonstrates superior performance in recovering parameters accurately, and generalizing well to in-the-wild images. Quantitative and qualitative experiment results validate the effectiveness of DI-PCG in inverse PCG and image-to-3D generation tasks. DI-PCG offers a promising approach for efficient inverse PCG and represents a valuable exploration step towards a 3D generation path that models how to construct a 3D asset using parametric models",
    "checked": true,
    "id": "951b4df132215c39486f98b107feeb0afb9829d6",
    "semantic_title": "di-pcg: diffusion-based efficient inverse procedural content generation for high-quality 3d asset creation",
    "citation_count": 3,
    "authors": [
      "Wang Zhao",
      "Yan-Pei Cao",
      "Jiale Xu",
      "Yuejiang Dong",
      "Ying Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_SALOVA_Segment-Augmented_Long_Video_Assistant_for_Targeted_Retrieval_and_Routing_CVPR_2025_paper.html": {
    "title": "SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval and Routing in Long-Form Video Analysis",
    "volume": "main",
    "abstract": "Despite advances in Large Multi-modal Models, applying them to long and untrimmed video content remains challenging due to limitations in context length and substantial memory overhead. These constraints often lead to significant information loss and reduced relevance in the model responses. With the exponential growth of video data across web platforms, understanding long-form video is crucial for advancing generalized intelligence. In this paper, we introduce SALOVA: Segment-Augmented LOng Video Assistant, a novel video-LLM framework designed to enhance the comprehension of lengthy video content through targeted retrieval process. We address two main challenges to achieve it: (i) We present the SceneWalk dataset, a high-quality collection of 87.8K long videos, each densely captioned at the segment level to enable models to capture scene continuity and maintain rich descriptive context. (ii) We develop robust architectural designs integrating dynamic routing mechanism and spatio-temporal projector to efficiently retrieve and process relevant video segments based on user queries. Our framework mitigates the limitations of current video-LMMs by allowing for precise identification and retrieval of relevant video segments in response to queries, thereby improving the contextual relevance of the generated responses. Through extensive experiments, SALOVA demonstrates enhanced capability in processing complex long-form videos, showing significant capability to maintain contextual integrity across extended sequences",
    "checked": true,
    "id": "cf5898e70519a4823249ccec2e0509d8136a30eb",
    "semantic_title": "salova: segment-augmented long video assistant for targeted retrieval and routing in long-form video analysis",
    "citation_count": 4,
    "authors": [
      "Junho Kim",
      "Hyunjun Kim",
      "Hosu Lee",
      "Yong Man Ro"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_Notes-guided_MLLM_Reasoning_Enhancing_MLLM_with_Knowledge_and_Visual_Notes_CVPR_2025_paper.html": {
    "title": "Notes-guided MLLM Reasoning: Enhancing MLLM with Knowledge and Visual Notes for Visual Question Answering",
    "volume": "main",
    "abstract": "The knowledge-based visual question answering (KB-VQA) task involves using external knowledge about the image to assist reasoning. Building on the impressive performance of multimodal large language model (MLLM), recent methods have commenced leveraging MLLM as an implicit knowledge base for reasoning. However, the direct employment of MLLM with raw external knowledge might result in reasoning errors due to misdirected knowledge information. Additionally, MLLM may lack fine-grained perception of visual features, which can result in hallucinations during reasoning. To address these challenges, we propose Notes-guided MLLM Reasoning (NoteMR), a novel framework that guides MLLM in better reasoning by utilizing knowledge notes and visual notes. Specifically, we initially obtain explicit knowledge from an external knowledge base. Then, this explicit knowledge, combined with images, is used to assist the MLLM in generating knowledge notes. These notes are designed to filter explicit knowledge and identify relevant internal implicit knowledge within the MLLM. We then identify highly correlated regions between the images and knowledge notes, retaining them as image notes to enhance the model's fine-grained perception, thereby mitigating MLLM induced hallucinations. Finally, both notes are fed into the MLLM, enabling a more comprehensive understanding of the image-question pair and enhancing the model's reasoning capabilities. Our method achieves state-of-the-art performance on the OK-VQA and A-OKVQA datasets, demonstrating its robustness and effectiveness across diverse VQA scenarios",
    "checked": true,
    "id": "9a0225c78ed7f4ffecdbd5762860959d57479262",
    "semantic_title": "notes-guided mllm reasoning: enhancing mllm with knowledge and visual notes for visual question answering",
    "citation_count": 0,
    "authors": [
      "Wenlong Fang",
      "Qiaofeng Wu",
      "Jing Chen",
      "Yun Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Are_Spatial-Temporal_Graph_Convolution_Networks_for_Human_Action_Recognition_Over-Parameterized_CVPR_2025_paper.html": {
    "title": "Are Spatial-Temporal Graph Convolution Networks for Human Action Recognition Over-Parameterized?",
    "volume": "main",
    "abstract": "Spatial-temporal graph convolutional networks (ST-GCNs) showcase impressive performance in skeleton-based human action recognition (HAR). However, despite the development of numerous models, their recognition performance does not differ significantly after aligning the input settings. With this observation, we hypothesize that ST-GCNs are over-parameterized for HAR, a conjecture subsequently confirmed through experiments employing the lottery ticket hypothesis. Additionally, a novel sparse ST-GCNs generator is proposed, which trains a sparse architecture from a randomly initialized dense network while maintaining comparable performance levels to the dense components. Moreover, we generate multi-level sparsity ST-GCNs by integrating sparse structures at various sparsity levels and demonstrate that the assembled model yields a significant enhancement in HAR performance. Thorough experiments on four datasets, including NTU-RGB+D 60(120), Kinetics-400, and FineGYM, demonstrate that the proposed sparse ST-GCNs can achieve comparable performance to their dense components. Even with 95% fewer parameters, the sparse ST-GCNs exhibit a degradation of <1% in top-1 accuracy. Meanwhile, the multi-level sparsity ST-GCNs, which require only 66% of the parameters of the dense ST-GCNs, demonstrate an improvement of >1% in top-1 accuracy. The code is available at https://github.com/davelailai/Sparse-ST-GCN",
    "checked": true,
    "id": "8a909479fcc0a3e05f8f18e14f2563cca87ad2e3",
    "semantic_title": "are spatial-temporal graph convolution networks for human action recognition over-parameterized?",
    "citation_count": 0,
    "authors": [
      "Jianyang Xie",
      "Yitian Zhao",
      "Yanda Meng",
      "He Zhao",
      "Anh Nguyen",
      "Yalin Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_DA-VPT_Semantic-Guided_Visual_Prompt_Tuning_for_Vision_Transformers_CVPR_2025_paper.html": {
    "title": "DA-VPT: Semantic-Guided Visual Prompt Tuning for Vision Transformers",
    "volume": "main",
    "abstract": "Visual Prompt Tuning (VPT) has become a promising solution for Parameter-Efficient Fine-Tuning (PEFT) approach for Vision Transformer (ViT) models by partially fine-tuning learnable tokens while keeping most model parameters frozen. Recent research has explored modifying the connection structures of the prompts. However, the fundamental correlation and distribution between the prompts and image tokens remain unexplored. In this paper, we leverage metric learning techniques to investigate how the distribution of prompts affects fine-tuning performance. Specifically, we propose a novel framework, Distribution Aware Visual Prompt Tuning (DA-VPT), to guide the distributions of the prompts by learning the distance metric from their class-related semantic data. Our method demonstrates that the prompts can serve as an effective bridge to share semantic information between image patches and the class token. We extensively evaluated our approach on popular benchmarks in both recognition and segmentation tasks. The results demonstrate that our approach enables more effective and efficient fine-tuning of ViT models by leveraging semantic information to guide the learning of the prompts, leading to improved performance on various downstream vision tasks",
    "checked": true,
    "id": "3cf4f7bb9b08b18ab8118616f36c92c5d62c9ead",
    "semantic_title": "da-vpt: semantic-guided visual prompt tuning for vision transformers",
    "citation_count": 1,
    "authors": [
      "Li Ren",
      "Chen Chen",
      "Liqiang Wang",
      "Kien Hua"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_Towards_Lossless_Implicit_Neural_Representation_via_Bit_Plane_Decomposition_CVPR_2025_paper.html": {
    "title": "Towards Lossless Implicit Neural Representation via Bit Plane Decomposition",
    "volume": "main",
    "abstract": "We quantify the upper bound on the size of the implicit neural representation (INR) model from a digital perspective. The upper bound of the model size increases exponentially as the required bit-precision increases. To this end, we present a bit-plane decomposition method that makes INR predict bit-planes, producing the same effect as reducing the upper bound of the model size. We validate our hypothesis that reducing the upper bound leads to faster convergence with constant model size. Our method achieves lossless representation in 2D image and audio fitting, even for high bit-depth signals, such as 16-bit, which was previously unachievable. We pioneered the presence of bit bias, which INR prioritizes as the most significant bit (MSB). We expand the application of the INR task to bit depth expansion, lossless image compression, and extreme network quantization. Our source code is available at https://github.com/WooKyoungHan/LosslessINR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woo Kyoung Han",
      "Byeonghun Lee",
      "Hyunmin Cho",
      "Sunghoon Im",
      "Kyong Hwan Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dastani_Spectral_State_Space_Model_for_Rotation-Invariant_Visual_Representation_Learning_CVPR_2025_paper.html": {
    "title": "Spectral State Space Model for Rotation-Invariant Visual Representation Learning",
    "volume": "main",
    "abstract": "State Space Models (SSMs) have recently emerged as an alternative to Vision Transformers (ViTs) due to their unique ability of modeling global relationships with linear complexity. SSMs are specifically designed to capture spatially proximate relationships of image patches. However, they fail to identify relationships between conceptually related yet not adjacent patches. This limitation arises from the non-causal nature of image data, which lacks inherent directional relationships. Additionally, current vision-based SSMs are highly sensitive to transformations such as rotation. Their predefined scanning directions depend on the original image orientation, which can cause the model to produce inconsistent patch-processing sequences after rotation.To address these limitations, we introduce Spectral VMamba, a novel approach that effectively captures the global structure within an image by leveraging spectral information derived from the graph Laplacian of image patches. Through spectral decomposition, our approach encodes patch relationships independently of image orientation, achieving rotation invariance with the aid of our Rotational Feature Normalizer (RFN) module. Our experiments on classification tasks show that Spectral VMamba outperforms the leading SSM models in vision, such as VMamba, while maintaining invariance to rotations and a providing a similar runtime efficiency",
    "checked": true,
    "id": "e702e1470c8590c74867594d6dced833b3edb186",
    "semantic_title": "spectral state space model for rotation-invariant visual representation learning",
    "citation_count": 1,
    "authors": [
      "Sahar Dastani",
      "Ali Bahri",
      "Moslem Yazdanpanah",
      "Mehrdad Noori",
      "David Osowiechi",
      "Gustavo Adolfo Vargas Hakim",
      "Farzad Beizaee",
      "Milad Cheraghalikhani",
      "Arnab Kumar Mondal",
      "Herve Lombaert",
      "Christian Desrosiers"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_iSegMan_Interactive_Segment-and-Manipulate_3D_Gaussians_CVPR_2025_paper.html": {
    "title": "iSegMan: Interactive Segment-and-Manipulate 3D Gaussians",
    "volume": "main",
    "abstract": "The efficient rendering and explicit nature of 3DGS promote the advancement of 3D scene manipulation.However, existing methods typically encounter challenges in controlling the manipulation region and are unable to furnish the user with interactive feedback, which inevitably leads to unexpected results.Intuitively, incorporating interactive 3D segmentation tools can compensate for this deficiency. Nevertheless, existing segmentation frameworks impose a pre-processing step of scene-specific parameter training, which limits the efficiency and flexibility of scene manipulation.To deliver a 3D region control module that is well-suited for scene manipulation with reliable efficiency, we propose **i**nteractive **Seg**ment-and-**Man**ipulate 3D Gaussians (**iSegMan**), an interactive segmentation and manipulation framework that only requires simple 2D user interactions in any view.To propagate user interactions to other views, we propose Epipolar-guided Interaction Propagation (**EIP**), which innovatively exploits epipolar constraint for efficient and robust interaction matching.To avoid scene-specific training to maintain efficiency, we further propose the novel Visibility-based Gaussian Voting (**VGV**), which obtains 2D segmentations from SAM and models the region extraction as a voting game between 2D Pixels and 3D Gaussians based on Gaussian visibility.Taking advantage of the efficient and precise region control of EIP and VGV, we put forth a **Manipulation Toolbox** to implement various functions on selected regions, enhancing the controllability, flexibility and practicality of scene manipulation.Extensive results on 3D scene manipulation and segmentation tasks fully demonstrate the significant advantages of iSegMan",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yian Zhao",
      "Wanshi Xu",
      "Ruochong Zheng",
      "Pengchong Qiao",
      "Chang Liu",
      "Jie Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_BlueLM-V-3B_Algorithm_and_System_Co-Design_for_Multimodal_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices",
    "volume": "main",
    "abstract": "The emergence and growing popularity of multimodal large language models (MLLMs) have significant potential to enhance various aspects of daily life, from improving communication to facilitating learning and problem-solving. Mobile phones, as essential daily companions, represent the most effective and accessible deployment platform for MLLMs, enabling seamless integration into everyday tasks. However, deploying MLLMs on mobile phones presents challenges due to limitations in memory size and computational capability, making it difficult to achieve smooth and real-time processing without extensive optimization. In this paper, we present BlueLM-V-3B, an algorithm and system co-design approach specifically tailored for the efficient deployment of MLLMs on mobile platforms. To be specific, we redesign the dynamic resolution scheme adopted by mainstream MLLMs and implement system optimization for hardware-aware deployment to optimize model inference on mobile phones. BlueLM-V-3B boasts the following key highlights: (1) Small Size: BlueLM-V-3B features a language model with 2.7B parameters and a vision encoder with 400M parameters. (2) Fast Speed: BlueLM-V-3B achieves a generation speed of 24.4 token/s on the MediaTek Dimensity 9300 processor with 4-bit LLM weight quantization. (3) Strong Performance: BlueLM-V-3B has attained the highest average score of 66.1 on the OpenCompass benchmark among models with <= 4B parameters and surpassed a series of models with much larger parameter sizes (e.g., MiniCPM-V-2.6, InternVL2-8B)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xudong Lu",
      "Yinghao Chen",
      "Cheng Chen",
      "Hui Tan",
      "Boheng Chen",
      "Yina Xie",
      "Rui Hu",
      "Guanxin Tan",
      "Renshou Wu",
      "Yan Hu",
      "Yi Zeng",
      "Lei Wu",
      "Liuyang Bian",
      "Zhaoxiong Wang",
      "Long Liu",
      "Yanzhou Yang",
      "Han Xiao",
      "Aojun Zhou",
      "Yafei Wen",
      "Xiaoxin Chen",
      "Shuai Ren",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Unraveling_Normal_Anatomy_via_Fluid-Driven_Anomaly_Randomization_CVPR_2025_paper.html": {
    "title": "Unraveling Normal Anatomy via Fluid-Driven Anomaly Randomization",
    "volume": "main",
    "abstract": "Data-driven machine learning has made significant strides in medical image analysis. However, most existing methods are tailored to specific modalities and assume a particular resolution (often isotropic). This limits their generalizability in clinical settings, where variations in scan appearance arise from differences in sequence parameters, resolution, and orientation. Furthermore, most general-purpose models are designed for healthy subjects and suffer from performance degradation when pathology is present. We introduce UNA (Unraveling Normal Anatomy), the first modality-agnostic learning approach for normal brain anatomy reconstruction that can handle both healthy scans and cases with pathology. We propose a fluid-driven anomaly randomization method that generates an unlimited number of realistic pathology profiles on-the-fly. UNA is trained on a combination of synthetic and real data, and can be applied directly to real images with potential pathology without the need for fine-tuning. We demonstrate UNA's effectiveness in reconstructing healthy brain anatomy and showcase its direct application to anomaly detection, using both simulated and real images from 3D healthy and stroke datasets, including CT and MRI scans. By bridging the gap between healthy and diseased images, UNA enables the use of general-purpose models on diseased images, opening up new opportunities for large-scale analysis of uncurated clinical images in the presence of pathology",
    "checked": true,
    "id": "cdae5828062e06b3d560f0dde0b465336cd1e650",
    "semantic_title": "unraveling normal anatomy via fluid-driven anomaly randomization",
    "citation_count": 3,
    "authors": [
      "Peirong Liu",
      "Ana Lawry Aguila",
      "Juan E. Iglesias"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Taming_Teacher_Forcing_for_Masked_Autoregressive_Video_Generation_CVPR_2025_paper.html": {
    "title": "Taming Teacher Forcing for Masked Autoregressive Video Generation",
    "volume": "main",
    "abstract": "We introduce MAGI, a hybrid video generation framework that combines masked modeling for intra-frame generation with causal modeling for next-frame generation. Our key innovation, Complete Teacher Forcing (CTF), conditions masked frames on complete observation frames rather than masked ones (namely Masked Teacher Forcing, MTF), enabling a smooth transition from token-level (patch-level) to frame-level autoregressive generation. CTF significantly outperforms MTF, achieving a 23% improvement in FVD scores on first-frame conditioned video prediction. To address issues like exposure bias, we employ targeted training strategies, setting a new benchmark in autoregressive video generation. Experiments show that MAGI can generate long, coherent video sequences exceeding 100 frames, even when trained on as few as 16 frames, highlighting its potential for scalable, high-quality video generation",
    "checked": true,
    "id": "ae66295a839e6e4dc71c9cde6da9bc28061ddc8a",
    "semantic_title": "taming teacher forcing for masked autoregressive video generation",
    "citation_count": 12,
    "authors": [
      "Deyu Zhou",
      "Quan Sun",
      "Yuang Peng",
      "Kun Yan",
      "Runpei Dong",
      "Duomin Wang",
      "Zheng Ge",
      "Nan Duan",
      "Xiangyu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion_CVPR_2025_paper.html": {
    "title": "UniRestore: Unified Perceptual and Task-Oriented Image Restoration Model Using Diffusion Prior",
    "volume": "main",
    "abstract": "Image restoration aims to recover content from inputs degraded by various factors, such as adverse weather, blur, and noise. Perceptual Image Restoration (PIR) methods improve visual quality but often do not support downstream tasks effectively. On the other hand, Task-oriented Image Restoration (TIR) methods focus on enhancing image utility for high-level vision tasks, sometimes compromising visual quality. This paper introduces UniRestore, a unified image restoration model that bridges the gap between PIR and TIR by using a diffusion prior. The diffusion prior is designed to generate images that align with human visual quality preferences, but these images are often unsuitable for TIR scenarios. To solve this limitation, UniRestore utilizes encoder features from an autoencoder to adapt the diffusion prior to specific tasks. We propose a Complementary Feature Restoration Module (CFRM) to reconstruct degraded encoder features and a Task Feature Adapter (TFA) module to facilitate adaptive feature fusion in the decoder. This design allows UniRestore to optimize images for both human perception and downstream task requirements, addressing discrepancies between visual quality and functional needs. Integrating these modules also enhances UniRestore's adapability and efficiency across diverse tasks. Extensive expertments demonstrate the superior performance of UniRestore in both PIR and TIR scenarios",
    "checked": true,
    "id": "f4498685e25c1d8da85a3dd110a94054a1db71f9",
    "semantic_title": "unirestore: unified perceptual and task-oriented image restoration model using diffusion prior",
    "citation_count": 5,
    "authors": [
      "I-Hsiang Chen",
      "Wei-Ting Chen",
      "Yu-Wei Liu",
      "Yuan-Chun Chiang",
      "Sy-Yen Kuo",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Edelstein_Sharp-It_A_Multi-view_to_Multi-view_Diffusion_Model_for_3D_Synthesis_CVPR_2025_paper.html": {
    "title": "Sharp-It: A Multi-view to Multi-view Diffusion Model for 3D Synthesis and Manipulation",
    "volume": "main",
    "abstract": "Advancements in text-to-image diffusion models have led to significant progress in fast 3D content creation. One common approach is to generate a set of multi-view images of an object, and then reconstruct it into a 3D model. However, this approach bypasses the use of a native 3D representation of the object and is hence prone to geometric artifacts and limited in controllability and manipulation capabilities. An alternative approach involves native 3D generative models that directly produce 3D representations. These models, however, are typically limited in their resolution, resulting in lower quality 3D objects. In this work, we bridge the quality gap between methods that directly generate 3D representations and ones that reconstruct 3D objects from multi-view images. We introduce a multi-view to multi-view diffusion model called Sharp-It, which takes a 3D consistent set of multi-view images rendered from a low-quality object and enriches its geometric details and texture. The diffusion model operates on the multi-view set in parallel, in the sense that it shares features across the generated views. A high-quality 3D model can then be reconstructed from the enriched multi-view set. By leveraging the advantages of both 2D and 3D approaches, our method offers an efficient and controllable method for high-quality 3D content creation. We demonstrate that Sharp-It enables various 3D applications, such as fast synthesis, editing, and controlled generation, while attaining high-quality assets",
    "checked": true,
    "id": "be5b7cb14ae8dd3f248eac09927396371f6fce4a",
    "semantic_title": "sharp-it: a multi-view to multi-view diffusion model for 3d synthesis and manipulation",
    "citation_count": 1,
    "authors": [
      "Yiftach Edelstein",
      "Or Patashnik",
      "Dana Cohen-Bar",
      "Lihi Zelnik-Manor"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_URWKV_Unified_RWKV_Model_with_Multi-state_Perspective_for_Low-light_Image_CVPR_2025_paper.html": {
    "title": "URWKV: Unified RWKV Model with Multi-state Perspective for Low-light Image Restoration",
    "volume": "main",
    "abstract": "Existing low-light image enhancement (LLIE) and joint LLIE and deblurring (LLIE-deblur) models have made strides in addressing predefined degradations, yet they are often constrained by dynamically coupled degradations. To address these challenges, we introduce a Unified Receptance Weighted Key Value (URWKV) model with multi-state perspective, enabling flexible and effective degradation restoration for low-light images. Specifically, we customize the core URWKV block to perceive and analyze complex degradations by leveraging multiple intra- and inter-stage states. First, inspired by the pupil mechanism in the human visual system, we propose Luminance-adaptive Normalization (LAN) that adjusts normalization parameters based on rich inter-stage states, allowing for adaptive, scene-aware luminance modulation. Second, we aggregate multiple intra-stage states through exponential moving average approach, effectively capturing subtle variations while mitigating information loss inherent in the single-state mechanism. To reduce the degradation effects commonly associated with conventional skip connections, we propose the State-aware Selective Fusion (SSF) module, which dynamically aligns and integrates multi-state features across encoder stages, selectively fusing contextual information. In comparison to state-of-the-art models, our URWKV model achieves superior performance on various benchmarks, while requiring significantly fewer parameters and computational resources. Code is available at: https://github.com/FZU-N/URWKV",
    "checked": true,
    "id": "2dccf575dfd8b6298e41b9cddb923f302dfde6ac",
    "semantic_title": "urwkv: unified rwkv model with multi-state perspective for low-light image restoration",
    "citation_count": 2,
    "authors": [
      "Rui Xu",
      "Yuzhen Niu",
      "Yuezhou Li",
      "Huangbiao Xu",
      "Wenxi Liu",
      "Yuzhong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Revisiting_Backdoor_Attacks_against_Large_Vision-Language_Models_from_Domain_Shift_CVPR_2025_paper.html": {
    "title": "Revisiting Backdoor Attacks against Large Vision-Language Models from Domain Shift",
    "volume": "main",
    "abstract": "Instruction tuning enhances large vision-language models (LVLMs) but increases their vulnerability to backdoor attacks due to their open design. Unlike prior studies in static settings, this paper explores backdoor attacks in LVLM instruction tuning across mismatched training and testing domains. We introduce a new evaluation dimension, backdoor domain generalization, to assess attack robustness under visual and text domain shifts. Our findings reveal two insights: (1) backdoor generalizability improves when distinctive trigger patterns are independent of specific data domains or model architectures, and (2) the competitive interaction between trigger patterns and clean semantic regions, where guiding the model to predict triggers enhances attack generalizability. Based on these insights, we propose a multimodal attribution backdoor attack (MABA) that injects domain-agnostic triggers into critical areas using attributional interpretation. Experiments with OpenFlamingo, Blip-2, and Otter show that MABA significantly boosts the attack success rate of generalization by 36.4% over the unimodal attack, achieving a 97% success rate at a 0.2% poisoning rate. This study reveals limitations in current evaluations and highlights how enhanced backdoor generalizability poses a security threat to LVLMs, even without test data access",
    "checked": true,
    "id": "2a76b2797949fda3102a83213c6446e6716e002e",
    "semantic_title": "revisiting backdoor attacks against large vision-language models from domain shift",
    "citation_count": 11,
    "authors": [
      "Siyuan Liang",
      "Jiawei Liang",
      "Tianyu Pang",
      "Chao Du",
      "Aishan Liu",
      "Mingli Zhu",
      "Xiaochun Cao",
      "Dacheng Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ding_Condensing_Action_Segmentation_Datasets_via_Generative_Network_Inversion_CVPR_2025_paper.html": {
    "title": "Condensing Action Segmentation Datasets via Generative Network Inversion",
    "volume": "main",
    "abstract": "This work presents the first condensation approach for procedural video datasets used in temporal action segmentation. We propose a condensation framework that leverages generative prior learned from the dataset and network inversion to condense data into compact latent codes with significant storage reduced across temporal and channel aspects. Orthogonally, we propose sampling diverse and representative action sequences to minimize video-wise redundancy. Our evaluation on standard benchmarks demonstrates consistent effectiveness in condensing TAS datasets and achieving competitive performances. Specifically, on the Breakfast dataset, our approach reduces storage by over 500xwhile retaining 83% of the performance compared to training with the full dataset. Furthermore, when applied to a downstream incremental learning task, it yields superior performance compared to the state-of-the-art",
    "checked": true,
    "id": "6eea6a64f59f0faee5a6ea2b2635e3a99c65c7d6",
    "semantic_title": "condensing action segmentation datasets via generative network inversion",
    "citation_count": 1,
    "authors": [
      "Guodong Ding",
      "Rongyu Chen",
      "Angela Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kwon_TCFG_Tangential_Damping_Classifier-free_Guidance_CVPR_2025_paper.html": {
    "title": "TCFG: Tangential Damping Classifier-free Guidance",
    "volume": "main",
    "abstract": "Diffusion models have achieved remarkable success in text-to-image synthesis, largely attributed to the use of classifier-free guidance (CFG), which enables high-quality, condition-aligned image generation. CFG combines the conditional score (e.g., text-conditioned) with the unconditional score to control the output. However, the unconditional score is in charge of estimating the transition between manifolds of adjacent timesteps from x_t to x_ t-1 , which may inadvertently interfere with the trajectory toward the specific condition. In this work, we introduce a novel approach that leverages a geometric perspective on the unconditional score to enhance CFG performance when conditional scores are available. Specifically, we propose a method that filters the singular vectors of both conditional and unconditional scores using singular value decomposition. This filtering process aligns the unconditional score with the conditional score, thereby refining the sampling trajectory to stay closer to the manifold. Our approach improves image quality with negligible additional computation. We provide deeper insights into the score function behavior in diffusion models and present a practical technique for achieving more accurate and contextually coherent image synthesis",
    "checked": true,
    "id": "996b79731a0ccfab2f03f5401312741ed678a310",
    "semantic_title": "tcfg: tangential damping classifier-free guidance",
    "citation_count": 1,
    "authors": [
      "Mingi Kwon",
      "Shin seong Kim",
      "Jaeseok Jeong",
      "Yi Ting Hsiao",
      "Youngjung Uh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_MatAnyone_Stable_Video_Matting_with_Consistent_Memory_Propagation_CVPR_2025_paper.html": {
    "title": "MatAnyone: Stable Video Matting with Consistent Memory Propagation",
    "volume": "main",
    "abstract": "Auxiliary-free human video matting methods, which rely solely on input frames, often struggle with complex or ambiguous backgrounds. To tackle this, we propose MatAnyone, a practical framework designed for target-assigned video matting. Specifically, building on a memory-based framework, we introduce a consistent memory propagation module via region-adaptive memory fusion, which adaptively combines memory from the previous frame. This ensures stable semantic consistency in core regions while maintaining fine details along object boundaries. For robust training, we present a larger, high-quality, and diverse dataset for video matting. Additionally, we incorporate a novel training strategy that efficiently leverages large-scale segmentation data, further improving matting stability. With this new network design, dataset, and training strategy, MatAnyone delivers robust, accurate video matting in diverse real-world scenarios, outperforming existing methods. The code and model will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiqing Yang",
      "Shangchen Zhou",
      "Jixin Zhao",
      "Qingyi Tao",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cai_Can_Generative_Video_Models_Help_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "Can Generative Video Models Help Pose Estimation?",
    "volume": "main",
    "abstract": "Pairwise pose estimation from images with little or no overlap is an open challenge in computer vision. Existing methods, even those trained on large-scale datasets, struggle in these scenarios due to the lack of identifiable correspondences or visual overlap. Inspired by the human ability to infer spatial relationships from diverse scenes, we propose a novel approach, InterPose, that leverages the rich priors encoded within pre-trained generative video models. We propose to use a video model to hallucinate intermediate frames between two input images, effectively creating a dense, visual transition, which significantly simplifies the problem of pose estimation. Since current video models can still produce implausible motion or inconsistent geometry, we introduce a self-consistency score that evaluates the consistency of pose predictions from sampled videos. We demonstrate that our approach generalizes among three state-of-the-art video models and show consistent improvements over the state-of-the-art DUSt3R baseline on four diverse datasets encompassing indoor, outdoor, and object-centric scenes. Our findings suggest a promising avenue for improving pose estimation models by leveraging large generative models trained on vast amounts of video data, which is more readily available than 3D data. See our project page for results: Inter-Pose.github.io",
    "checked": true,
    "id": "eba6a54df251ec0a818f02d8570eb25b3a146f1a",
    "semantic_title": "can generative video models help pose estimation?",
    "citation_count": 1,
    "authors": [
      "Ruojin Cai",
      "Jason Y. Zhang",
      "Philipp Henzler",
      "Zhengqi Li",
      "Noah Snavely",
      "Ricardo Martin-Brualla"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art_CVPR_2025_paper.html": {
    "title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models",
    "volume": "main",
    "abstract": "Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matt Deitke",
      "Christopher Clark",
      "Sangho Lee",
      "Rohun Tripathi",
      "Yue Yang",
      "Jae Sung Park",
      "Mohammadreza Salehi",
      "Niklas Muennighoff",
      "Kyle Lo",
      "Luca Soldaini",
      "Jiasen Lu",
      "Taira Anderson",
      "Erin Bransom",
      "Kiana Ehsani",
      "Huong Ngo",
      "YenSung Chen",
      "Ajay Patel",
      "Mark Yatskar",
      "Chris Callison-Burch",
      "Andrew Head",
      "Rose Hendrix",
      "Favyen Bastani",
      "Eli VanderBilt",
      "Nathan Lambert",
      "Yvonne Chou",
      "Arnavi Chheda",
      "Jenna Sparks",
      "Sam Skjonsberg",
      "Michael Schmitz",
      "Aaron Sarnat",
      "Byron Bischoff",
      "Pete Walsh",
      "Chris Newell",
      "Piper Wolters",
      "Tanmay Gupta",
      "Kuo-Hao Zeng",
      "Jon Borchardt",
      "Dirk Groeneveld",
      "Crystal Nam",
      "Sophie Lebrecht",
      "Caitlin Wittlif",
      "Carissa Schoenick",
      "Oscar Michel",
      "Ranjay Krishna",
      "Luca Weihs",
      "Noah A. Smith",
      "Hannaneh Hajishirzi",
      "Ross Girshick",
      "Ali Farhadi",
      "Aniruddha Kembhavi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous_CVPR_2025_paper.html": {
    "title": "DriveGPT4-V2: Harnessing Large Language Model Capabilities for Enhanced Closed-Loop Autonomous Driving",
    "volume": "main",
    "abstract": "Multimodal large language models (MLLMs) possess the ability to comprehend visual images or videos, and show impressive reasoning ability thanks to the vast amounts of pretrained knowledge, making them highly suitable for autonomous driving applications. Unlike the previous work, DriveGPT4-V1, which focused on open-loop tasks, this study explores the capabilities of LLMs in enhancing closed-loop autonomous driving. DriveGPT4-V2 processes camera images and vehicle states as input to generate low-level control signals for end-to-end vehicle operation. A multi-view visual tokenizer (MV-VT) is employed enabling DriveGPT4-V2 to perceive the environment with an extensive range while maintaining critical details. The model architecture has been refined to improve decision prediction and inference speed. To further enhance the performance, an additional expert LLM is trained for online imitation learning. The expert LLM, sharing a similar structure with DriveGPT4-V2, can access privileged information about surrounding objects for more robust and reliable predictions. Experimental results show that DriveGPT4-V2 outperforms all baselines on the challenging CARLA Longest6 benchmark. The code and data of DriveGPT4-V2 will be publicly available",
    "checked": true,
    "id": "2e53d676a3ed157f964e76b7b5ee2a1d69e7dcc6",
    "semantic_title": "drivegpt4-v2: harnessing large language model capabilities for enhanced closed-loop autonomous driving",
    "citation_count": 2,
    "authors": [
      "Zhenhua Xu",
      "Yan Bai",
      "Yujia Zhang",
      "Zhuoling Li",
      "Fei Xia",
      "Kwan-Yee K. Wong",
      "Jianqiang Wang",
      "Hengshuang Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds_CVPR_2025_paper.html": {
    "title": "High-Fidelity Lightweight Mesh Reconstruction from Point Clouds",
    "volume": "main",
    "abstract": "Recently, learning signed distance functions (SDFs) from point clouds has become popular for reconstruction. To ensure accuracy, most methods require using high-resolution Marching Cubes for surface extraction. However, this results in redundant mesh elements, making the mesh inconvenient to use. To solve the problem, we propose an adaptive meshing method to extract resolution-adaptive meshes based on surface curvature, enabling the recovery of high-fidelity lightweight meshes. Specifically, we first use point-based representation to perceive implicit surfaces and calculate surface curvature. A vertex generator is designed to produce curvature-adaptive vertices with any specified number on the implicit surface, preserving the overall structure and high-curvature features. Then we develop a Delaunay meshing algorithm to generate meshes from vertices, ensuring geometric fidelity and correct topology. In addition, to obtain accurate SDFs for adaptive meshing and achieve better lightweight reconstruction, we design a hybrid representation combining feature grid and feature tri-plane for better detail capture. Experiments demonstrate that our method can generate high-quality lightweight meshes from point clouds. Compared with methods from various categories, our approach achieves superior results, especially in capturing more details with fewer elements",
    "checked": true,
    "id": "41e0aa355cf58187b2c66ae8691f029a20c1cd33",
    "semantic_title": "high-fidelity lightweight mesh reconstruction from point clouds",
    "citation_count": 0,
    "authors": [
      "Chen Zhang",
      "Wentao Wang",
      "Ximeng Li",
      "Xinyao Liao",
      "Wanjuan Su",
      "Wenbing Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_MDP_Multidimensional_Vision_Model_Pruning_with_Latency_Constraint_CVPR_2025_paper.html": {
    "title": "MDP: Multidimensional Vision Model Pruning with Latency Constraint",
    "volume": "main",
    "abstract": "Current structural pruning methods face two significant limitations: (i) they often limit pruning to finer-grained levels like channels, making aggressive parameter reduction challenging, and (ii) they focus heavily on parameter and FLOP reduction, with existing latency-aware methods frequently relying on simplistic, suboptimal linear models that fail to generalize well to transformers, where multiple interacting dimensions impact latency. In this paper, we address both limitations by introducing Multi-Dimensional Pruning(MDP), a novel paradigm that jointly optimizes across a variety of pruning granularities--including channels, query/key, heads, embeddings, and blocks. MDP employs an advanced latency modeling technique to accurately capture latency variations across all prunable dimensions, achieving an optimal balance between latency and accuracy. By reformulating pruning as a Mixed-Integer Nonlinear Program (MINLP), MDP efficiently identifies the optimal pruned structure across all prunable dimensions while respecting latency constraints. This versatile framework supports both CNNs and transformers. Extensive experiments demonstrate that MDP significantly outperforms previous methods, especially at high pruning ratios. On ImageNet, MDP achieves a 28% speed increase with a +1.4 Top-1 accuracy improvement over prior work like HALP for ResNet50 pruning. Against the latest transformer pruning method, Isomorphic, MDP delivers an additional 37% acceleration with a +0.7 Top-1 accuracy improvement",
    "checked": true,
    "id": "e499768e171c5d9e3794927ad171c37c60f99b90",
    "semantic_title": "mdp: multidimensional vision model pruning with latency constraint",
    "citation_count": 0,
    "authors": [
      "Xinglong Sun",
      "Barath Lakshmanan",
      "Maying Shen",
      "Shiyi Lan",
      "Jingde Chen",
      "Jose M. Alvarez"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_OSDFace_One-Step_Diffusion_Model_for_Face_Restoration_CVPR_2025_paper.html": {
    "title": "OSDFace: One-Step Diffusion Model for Face Restoration",
    "volume": "main",
    "abstract": "Diffusion models have demonstrated impressive performance in face restoration. Yet, their multi-step inference process remains computationally intensive, limiting their applicability in real-world scenarios. Moreover, existing methods often struggle to generate face images that are harmonious, realistic, and consistent with the subject's identity. In this work, we propose OSDFace, a novel one-step diffusion model for face restoration. Specifically, we propose a visual representation embedder (VRE) to better capture prior information and understand the input face. In VRE, low-quality faces are processed by a visual tokenizer and subsequently embedded with a vector-quantized dictionary to generate visual prompts. Additionally, we incorporate a facial identity loss derived from face recognition to further ensure identity consistency. We further employ a generative adversarial network (GAN) as a guidance model to encourage distribution alignment between the restored face and the ground truth. Experimental results demonstrate that OSDFace surpasses current state-of-the-art (SOTA) methods in both visual quality and quantitative metrics, generating high-fidelity, natural face images with high identity consistency. The code and model will be released at https://github.com/jkwang28/OSDFace",
    "checked": true,
    "id": "e0548d4b0facfc925eceb95e57a23e9356137737",
    "semantic_title": "osdface: one-step diffusion model for face restoration",
    "citation_count": 8,
    "authors": [
      "Jingkai Wang",
      "Jue Gong",
      "Lin Zhang",
      "Zheng Chen",
      "Xing Liu",
      "Hong Gu",
      "Yutong Liu",
      "Yulun Zhang",
      "Xiaokang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gargiulo_Task_Singular_Vectors_Reducing_Task_Interference_in_Model_Merging_CVPR_2025_paper.html": {
    "title": "Task Singular Vectors: Reducing Task Interference in Model Merging",
    "volume": "main",
    "abstract": "Task Arithmetic has emerged as a simple yet effective method to merge models without additional training. However, by treating entire networks as flat parameter vectors, it overlooks key structural information and is susceptible to task interference. In this paper, we study task vectors at the layer level, focusing on task layer matrices and their singular value decomposition. In particular, we concentrate on the resulting singular vectors, which we refer to as Task Singular Vectors (TSV). Recognizing that layer task matrices are often low-rank, we propose TSV-Compress, a simple procedure that compresses them to 10% of their original size while retaining 99% of accuracy. We further leverage this low-rank space to define a new measure of task interference based on the interaction of singular vectors from different tasks. Building on these findings, we introduce TSV-Merge, a novel model merging approach that combines compression with interference reduction, significantly outperforming existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antonio Andrea Gargiulo",
      "Donato Crisostomi",
      "Maria Sofia Bucarelli",
      "Simone Scardapane",
      "Fabrizio Silvestri",
      "Emanuele Rodolà"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes_CVPR_2025_paper.html": {
    "title": "Functionality Understanding and Segmentation in 3D Scenes",
    "volume": "main",
    "abstract": "Understanding functionalities in 3D scenes involves interpreting natural language descriptions to locate functional interactive objects, such as handles and buttons, in a 3D environment. Functionality understanding is highly challenging, as it requires both world knowledge to interpret language and spatial perception to identify fine-grained objects. For example, given a task like \"turn on the ceiling light\", an embodied AI agent must infer that it needs to locate the light switch, even though the switch is not explicitly mentioned in the task description.To date, no dedicated methods have been developed for this problem. In this paper, we introduce Fun3DU, the first approach designed for functionality understanding in 3D scenes. Fun3DU uses a language model to parse the task description through Chain-of-Thought reasoning in order to identify the object of interest. The identified object is segmented across multiple views of the captured scene by using a VLM. The segmentation results from each view are lifted in 3D and aggregated into the point cloud using geometric information. Fun3DU is training-free, relying entirely on pre-trained models. We evaluate Fun3DU on SceneFun3D, the most recent and only dataset to benchmark this task, which comprises over 3000 task descriptions on 230 scenes. Our method significantly outperforms state-of-the-art open-vocabulary 3D segmentation approaches. Project page: https://tev-fbk.github.io/fun3du",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaime Corsetti",
      "Francesco Giuliari",
      "Alice Fasoli",
      "Davide Boscaini",
      "Fabio Poiesi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guang_Dragin3D_Image_Editing_by_Dragging_in_3D_Space_CVPR_2025_paper.html": {
    "title": "Dragin3D: Image Editing by Dragging in 3D Space",
    "volume": "main",
    "abstract": "Interactive drag editing of images is a valuable task that has gained considerable attention for its precision and controllability. However, existing approaches have primarily focused on manipulating the shape or movement of objects in 2D plane. We propose to extend this drag-based editing task to 3D space. Firstly, we utilize the trajectory of two points to represent the rotational trajectory of the object. Gaussian maps of a circle and a square are centered at these two points, respectively. We use distinct shapes to ensure that symmetric views produce different object representations. Secondly, we introduce a lightweight mapping network to embed the object features into two Gaussian maps to obtain a continuous control condition that guides the model in learning the correspondence between the trajectory and the object. Finally, to overcome the limitations of current 3D object reconstruction datasets, which typically consist of object maps with transparent backgrounds, we affix random backgrounds to them. This modification helps improve the model's ability to ignore background interference when editing real images with complex backgrounds. Experiments demonstrate that our approach successfully achieves object rotation within the drag framework and demonstrates strong generalization to real-world images",
    "checked": true,
    "id": "08f91a5bd6abd59f676888d5dabbd672a1f3e7e8",
    "semantic_title": "dragin3d: image editing by dragging in 3d space",
    "citation_count": 2,
    "authors": [
      "Weiran Guang",
      "Xiaoguang Gu",
      "Mengqi Huang",
      "Zhendong Mao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MMTL-UniAD_A_Unified_Framework_for_Multimodal_and_Multi-Task_Learning_in_CVPR_2025_paper.html": {
    "title": "MMTL-UniAD: A Unified Framework for Multimodal and Multi-Task Learning in Assistive Driving Perception",
    "volume": "main",
    "abstract": "Advanced driver assistance systems require a comprehensive understanding of the driver's mental/physical state and traffic context but existing works often neglect the potential benefits of joint learning between these tasks. This paper proposes MMTL-UniAD, a unified multi-modal multi-task learning framework that simultaneously recognizes driver behavior (e.g., looking around, talking), driver emotion (e.g., anxiety, happiness), vehicle behavior (e.g., parking, turning), and traffic context (e.g., traffic jam, traffic smooth). A key challenge is avoiding negative transfer between tasks, which can impair learning performance. To address this, we introduce two key components into the framework: one is the multi-axis region attention network to extract global context-sensitive features, and the other is the dual-branch multimodal embedding to learn multimodal embeddings from both task-shared and task-specific features. The former uses a multi-attention mechanism to extract task-relevant features, mitigating negative transfer caused by task-unrelated features. The latter employs a dual-branch structure to adaptively adjust task-shared and task-specific parameters, enhancing cross-task knowledge transfer while reducing task conflicts. We assess MMTL-UniAD on the AIDE dataset, using a series of ablation studies, and show that it outperforms state-of-the-art methods across all four tasks. The code is available on https://github.com/Wenzhuo-Liu/MMTL-UniAD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenzhuo Liu",
      "Wenshuo Wang",
      "Yicheng Qiao",
      "Qiannan Guo",
      "Jiayin Zhu",
      "Pengfei Li",
      "Zilong Chen",
      "Huiming Yang",
      "Zhiwei Li",
      "Lening Wang",
      "Tiao Tan",
      "Huaping Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_T2V-CompBench_A_Comprehensive_Benchmark_for_Compositional_Text-to-video_Generation_CVPR_2025_paper.html": {
    "title": "T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation",
    "volume": "main",
    "abstract": "Text-to-video (T2V) generative models have advanced significantly, yet their ability to compose different objects, attributes, actions, and motions into a video remains unexplored. Previous text-to-video benchmarks also neglect this important ability for evaluation. In this work, we conduct the first systematic study on compositional text-to-video generation. We propose T2V-CompBench, the first benchmark tailored for compositional text-to-video generation. T2V-CompBench encompasses diverse aspects of compositionality, including consistent attribute binding, dynamic attribute binding, spatial relationships, motion binding, action binding, object interactions, and generative numeracy. We further carefully design evaluation metrics of multimodal large language model (MLLM)-based, detection-based, and tracking-based metrics, which can better reflect the compositional text-to-video generation quality of seven proposed categories with 1400 text prompts. The effectiveness of the proposed metrics is verified by correlation with human evaluations. We also benchmark various text-to-video generative models and conduct in-depth analysis across different models and various compositional categories. We find that compositional text-to-video generation is highly challenging for current models, and we hope our attempt could shed light on future research in this direction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiyue Sun",
      "Kaiyi Huang",
      "Xian Liu",
      "Yue Wu",
      "Zihan Xu",
      "Zhenguo Li",
      "Xihui Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sehgal_Self-Evolving_Visual_Concept_Library_using_Vision-Language_Critics_CVPR_2025_paper.html": {
    "title": "Self-Evolving Visual Concept Library using Vision-Language Critics",
    "volume": "main",
    "abstract": "We study the problem of building a visual concept library for visual recognition. Building effective visual concept libraries is challenging, as manual definition is labor-intensive, while relying solely on LLMs for concept generation can result in concepts that lack discriminative power or fail to account for the complex interactions between them. Our approach, ESCHER, takes a library learning perspective to iteratively discover and improve visual concepts. ESCHER uses a vision-language model (VLM) as a critic to iteratively refine the concept library, including accounting for interactions between concepts and how they affect downstream classifiers. By leveraging the in-context learning abilities of LLMs and the history of performance using various concepts, ESCHER dynamically improves its concept generation strategy based on the VLM critic's feedback. Finally, ESCHER does not require any human annotations, and is thus an automated plug-and-play framework. We empirically demonstrate the ability of ESCHER to learn a concept library for zero-shot, few-shot, and fine-tuning visual classification tasks. This work represents, to our knowledge, the first application of concept library learning to real-world visual tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atharva Sehgal",
      "Patrick Yuan",
      "Ziniu Hu",
      "Yisong Yue",
      "Jennifer J. Sun",
      "Swarat Chaudhuri"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders_CVPR_2025_paper.html": {
    "title": "Multimodal Autoregressive Pre-training of Large Vision Encoders",
    "volume": "main",
    "abstract": "We introduce a novel method for pre-training of large-scale vision encoders. Building on recent advancements in autoregressive pre-training of vision models, we extend this framework to a multimodal setting, i.e., images and text. In this paper, we present AIMV2, a family of generalist vision encoders characterized by a straightforward pre-training process, scalability, and remarkable performance across a range of downstream tasks. This is achieved by pairing the vision encoder with a multimodal decoder that autoregressively generates raw image patches and text tokens. Our encoders excel not only in multimodal evaluations but also in vision benchmarks such as localization, grounding, and classification. Notably, our AIMV2-3B encoder achieves 89.5% accuracy on ImageNet-1k with a frozen trunk. Fur- thermore, AIMV2 consistently outperforms state-of-the-art contrastive models (e.g., CLIP, SigLIP) in multimodal im- age understanding across diverse settings",
    "checked": true,
    "id": "f6aeb1921d39ebe47750b9fe3e77a4c264b8fb91",
    "semantic_title": "multimodal autoregressive pre-training of large vision encoders",
    "citation_count": 43,
    "authors": [
      "Enrico Fini",
      "Mustafa Shukor",
      "Xiujun Li",
      "Philipp Dufter",
      "Michal Klein",
      "David Haldimann",
      "Sai Aitharaju",
      "Victor G. Turrisi da Costa",
      "Louis Béthune",
      "Zhe Gan",
      "Alexander Toshev",
      "Marcin Eichner",
      "Moin Nabi",
      "Yinfei Yang",
      "Joshua Susskind",
      "Alaaeldin El-Nouby"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_AKiRa_Augmentation_Kit_on_Rays_for_Optical_Video_Generation_CVPR_2025_paper.html": {
    "title": "AKiRa: Augmentation Kit on Rays for Optical Video Generation",
    "volume": "main",
    "abstract": "Recent advances in text-conditioned video diffusion have greatly improved video quality. However, these methods offer limited or sometimes no control to users on camera aspects, including dynamic camera motion, zoom, distorted lens and focus shifts. These motion and optical aspects are crucial for adding controllability and cinematic elements to generation frameworks, ultimately resulting in visual content that draws focus, enhances mood, and guides emotions according to filmmakers' controls. In this paper, we aim to close the gap between controllable video generation and camera optics. To achieve this, we propose AKiRa (Augmentation Kit on Rays), a novel augmentation framework that builds and trains a camera adapter with a complex camera model over an existing video generation backbone. It enables fine-tuned control over camera motion as well as complex optical parameters (focal length, distortion, aperture) to achieve cinematic effects such as zoom, fisheye effect, and bokeh. Extensive experiments demonstrate AKiRa's effectiveness in combining and composing camera optics while outperforming all state-of-the-art methods. This work sets a new landmark in controlled and optically enhanced video generation, paving the way for future camera diffusion methods",
    "checked": true,
    "id": "797c7dd6bba79c65ccf7b524ed86c92485a58b65",
    "semantic_title": "akira: augmentation kit on rays for optical video generation",
    "citation_count": 7,
    "authors": [
      "Xi Wang",
      "Robin Courant",
      "Marc Christie",
      "Vicky Kalogeiton"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhong_Towards_Stable_and_Storage-efficient_Dataset_Distillation_Matching_Convexified_Trajectory_CVPR_2025_paper.html": {
    "title": "Towards Stable and Storage-efficient Dataset Distillation: Matching Convexified Trajectory",
    "volume": "main",
    "abstract": "The rapid evolution of deep learning and large language models has led to an exponential growth in the demand for training data, prompting the development of Dataset Distillation methods to address the challenges of managing large datasets. Among these, Matching Training Trajectories (MTT) has been a prominent approach, which replicates the training trajectory of an expert network on real data with a synthetic dataset. However, our investigation found that this method suffers from three significant limitations: 1. Instability of expert trajectory generated by Stochastic Gradient Descent (SGD); 2. Low convergence speed of the distillation process; 3. High storage consumption of the expert trajectory. To address these issues, we offer a new perspective on understanding the essence of Dataset Distillation and MTT through a simple transformation of the objective function, and introduce a novel method called Matching Convexified Trajectory (MCT), which aims to provide better guidance for the student trajectory. MCT creates convex combinations of expert trajectories by selecting a few expert models, guiding student networks to converge quickly and stably. This trajectory is not only easier to store, but also enables continuous sampling strategies during the distillation process, ensuring thorough learning and fitting of the entire expert trajectory. The comprehensive experiment of three public datasets verified that MCT is superior to the traditional MTT method",
    "checked": true,
    "id": "5ca96c678e27c14b4e636458c6b069b4ec2b8dc0",
    "semantic_title": "towards stable and storage-efficient dataset distillation: matching convexified trajectory",
    "citation_count": 2,
    "authors": [
      "Wenliang Zhong",
      "Haoyu Tang",
      "Qinghai Zheng",
      "Mingzhu Xu",
      "Yupeng Hu",
      "Weili Guan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Radman_TSAM_Temporal_SAM_Augmented_with_Multimodal_Prompts_for_Referring_Audio-Visual_CVPR_2025_paper.html": {
    "title": "TSAM: Temporal SAM Augmented with Multimodal Prompts for Referring Audio-Visual Segmentation",
    "volume": "main",
    "abstract": "Referring audio-visual segmentation (Ref-AVS) aims to segment objects within audio-visual scenes using multimodal cues embedded in text expressions. While the Segment Anything Model (SAM) has revolutionized visual segmentation, its applicability to Ref-AVS, where multimodal cues act as novel prompts, remains unexplored. SAM's limitation to single-frame segmentation also hinders its ability to capture essential temporal context needed for multi-frame audio-visual segmentation. To address this gap, we propose TSAM, a novel extension of SAM designed to leverage multimodal cues for precise segmentation in dynamic audio-visual scenes. TSAM enhances SAM's image encoder with a temporal modeling branch, enabling spatio-temporal learning and deep multimodal fusion across video frames, while retaining SAM's pre-trained knowledge. Additionally, TSAM replaces SAM's user-interactive prompting mechanism with sparse and dense data-driven prompts, enabling more effective integration of audio-visual inputs and reference text expressions. Extensive experiments on the Ref-AVS dataset demonstrate TSAM's superiority over state-of-the-art methods. The results illustrate its effectiveness in segmenting objects in dynamic audio-visual scenes using text-based multimodal cues and its strong generalization to unseen objects",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abduljalil Radman",
      "Jorma Laaksonen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance_CVPR_2025_paper.html": {
    "title": "TFCustom: Customized Image Generation with Time-Aware Frequency Feature Guidance",
    "volume": "main",
    "abstract": "Subject-driven image personalization has seen notable advancements, especially with the advent of the ReferenceNet paradigm. ReferenceNet excels in integrating image reference features, making it highly applicable in creative and commercial settings. However, current implementations of ReferenceNet primarily operate as latent-level feature extractors, which limit their potential. This constraint hinders the provision of appropriate features to the denoising backbone across different timesteps, leading to suboptimal image consistency. In this paper, we revisit the extraction of reference features and propose TFCustom, a model framework designed to focus on reference image features at different temporal steps and frequency levels. Specifically, we firstly propose synchronized ReferenceNet to extract reference image features while simultaneously optimizing noise injection and denoising for the reference image. We also propose a time-aware frequency feature refinement module that leverages high- and low-frequency filters, combined with time embeddings, to adaptively select the degree of reference feature injection. Additionally, to enhance the similarity between reference objects and the generated image, we introduce a novel reward-based loss that encourages greater alignment between the reference and generated images. Experimental results demonstrate state-of-the-art performance in both multi-object and single-object reference generation, with significant improvements in texture and textual detail generation over existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mushui Liu",
      "Dong She",
      "Jingxuan Pang",
      "Qihan Huang",
      "Jiacheng Ying",
      "Wanggui He",
      "Yuanlei Hou",
      "Siming Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Boosting_Point-Supervised_Temporal_Action_Localization_through_Integrating_Query_Reformation_and_CVPR_2025_paper.html": {
    "title": "Boosting Point-Supervised Temporal Action Localization through Integrating Query Reformation and Optimal Transport",
    "volume": "main",
    "abstract": "Point-supervised Temporal Action Localization poses significant challenges due to the difficulty of identifying complete actions with a single-point annotation per action. Existing methods typically employ Multiple Instance Learning, which struggles to capture global temporal context and requires heuristic post-processing. In research on fully-supervised tasks, DETR-based structures have effectively addressed these limitations. However, it is nontrivial to merely adapt DETR to this task, encountering two major bottlenecks. (1) How to integrate point label information into the model and (2) How to select optimal decoder proposals for training in the absence of complete action segment annotations. To address this issue, we introduce an end-to-end framework by integrating Query Reformation and Optimal Transport (QROT). Specifically, we encode point labels through a set of semantic consensus queries, enabling effective focus on action-relevant snippets. Furthermore, we integrate an optimal transport mechanism to generate high-quality pseudo labels. These pseudo-labels facilitate precise proposals selection based on Hungarian algorithm, significantly enhancing localization accuracy in point-supervised settings. Extensive experiments on the THUMOS14 and ActivityNet-v1.3 datasets demonstrate that our method outperforms existing MIL-based approaches, offering more stable and accurate temporal action localization in point-level supervision",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengnan Liu",
      "Le Wang",
      "Sanping Zhou",
      "Kun Xia",
      "Xiaolong Sun",
      "Gang Hua"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Koley_SketchFusion_Learning_Universal_Sketch_Features_through_Fusing_Foundation_Models_CVPR_2025_paper.html": {
    "title": "SketchFusion: Learning Universal Sketch Features through Fusing Foundation Models",
    "volume": "main",
    "abstract": "While foundation models have revolutionised computer vision, their effectiveness for sketch understanding remains limited by the unique challenges of abstract, sparse visual inputs. Through systematic analysis, we uncover two fundamental limitations: Stable Diffusion (SD) struggles to extract meaningful features from abstract sketches (unlike its success with photos), and exhibits a pronounced frequency-domain bias that suppresses essential low-frequency components needed for sketch understanding. Rather than costly retraining, we address these limitations by strategically combining SD with CLIP, whose strong semantic understanding naturally compensates for SD's spatial-frequency biases. By dynamically injecting CLIP features into SD's denoising process and adaptively aggregating features across semantic levels, our method achieves state-of-the-art performance in sketch retrieval (+3.35%), recognition (+1.06%), segmentation (+29.42%), and correspondence learning (+21.22%), demonstrating the first truly universal sketch feature representation in the era of foundation models",
    "checked": true,
    "id": "bcac8bfc09d088bde8ad37f631fc2270ef26a396",
    "semantic_title": "sketchfusion: learning universal sketch features through fusing foundation models",
    "citation_count": 0,
    "authors": [
      "Subhadeep Koley",
      "Tapas Kumar Dutta",
      "Aneeshan Sain",
      "Pinaki Nath Chowdhury",
      "Ayan Kumar Bhunia",
      "Yi-Zhe Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Bridging_the_Vision-Brain_Gap_with_an_Uncertainty-Aware_Blur_Prior_CVPR_2025_paper.html": {
    "title": "Bridging the Vision-Brain Gap with an Uncertainty-Aware Blur Prior",
    "volume": "main",
    "abstract": "Can our brain signals faithfully reflect the original visual stimuli, even including high-frequency details? Although human perceptual and cognitive capacities enable us to process and remember visual information, these abilities are constrained by several factors, such as limited attentional resources and finite capacity of visual memory. When visual stimuli are processed by human visual system into brain signals, some information is inevitably lost, leading to a discrepancy known as the System GAP. Additionally, perceptual and cognitive dynamics, along with technical noise in signal acquisition, degrade the fidelity of brain signals relative to the visual stimuli, known as the Random GAP. When encoded brain representations are directly aligned with the corresponding pretrained image features, the System GAP and Random GAP between paired data challenge the model, requiring it to bridge these gaps. However, due to limited paired data, these gaps are difficult for the model to learn, leading to overfitting and poor generalization to new data. To address these GAPs, we propose a simple yet effective approach called the Uncertainty-aware Blur Prior (UBP). It estimates the uncertainty within the paired data, reflecting the mismatch between brain signals and visual stimuli. Based on uncertainty, UBP dynamically blurs the high-frequency details of the original images, reducing the impact of mismatch and improving alignment. Our method achieves a top-1 accuracy of 50.9% and a top-5 accuracy of 79.7% on the zero-shot brain-to-image retrieval task, surpassing previous state-of-the-art methods. Code is available at https://github.com/HaitaoWuTJU/Uncertainty-aware-Blur-Prior",
    "checked": true,
    "id": "fe4a99fdfb8c7d2d26fda5d5b19b9b8e949928f9",
    "semantic_title": "bridging the vision-brain gap with an uncertainty-aware blur prior",
    "citation_count": 2,
    "authors": [
      "Haitao Wu",
      "Qing Li",
      "Changqing Zhang",
      "Zhen He",
      "Xiaomin Ying"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Invisible_Backdoor_Attack_against_Self-supervised_Learning_CVPR_2025_paper.html": {
    "title": "Invisible Backdoor Attack against Self-supervised Learning",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) models are vulnerable to backdoor attacks. Existing backdoor attacks that are effective in SSL often involve noticeable triggers, like colored patches or visible noise, which are vulnerable to human inspection. This paper proposes an imperceptible and effective backdoor attack against self-supervised models. We first find that existing imperceptible triggers designed for supervised learning are less effective in compromising self-supervised models. We then identify this ineffectiveness is attributed to the overlap in distributions between the backdoor and augmented samples used in SSL. Building on this insight, we design an attack using optimized triggers disentangled with the augmented transformation in the SSL, while remaining imperceptible to human vision. Experiments on five datasets and six SSL algorithms demonstrate our attack is highly effective and stealthy. It also has strong resistance to existing backdoor defenses. Our code can be found at https://github.com/Zhang-Henry/INACTIVE",
    "checked": true,
    "id": "cd6f250135a7db133cb6647fe0327d0b400e260e",
    "semantic_title": "invisible backdoor attack against self-supervised learning",
    "citation_count": 3,
    "authors": [
      "Hanrong Zhang",
      "Zhenting Wang",
      "Boheng Li",
      "Fulin Lin",
      "Tingxu Han",
      "Mingyu Jin",
      "Chenlu Zhan",
      "Mengnan Du",
      "Hongwei Wang",
      "Shiqing Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation_CVPR_2025_paper.html": {
    "title": "Perceptually Accurate 3D Talking Head Generation: New Definitions, Speech-Mesh Representation, and Evaluation Metrics",
    "volume": "main",
    "abstract": "Recent advancements in speech-driven 3D talking head generation have made significant progress in lip synchronization. However, existing models still struggle to capture the perceptual alignment between varying speech characteristics and corresponding lip movements. In this work, we claim that three criteria--Temporal Synchronization, Lip Readability, and Expressiveness--are crucial for achieving perceptually accurate lip movements. Motivated by our hypothesis that a desirable representation space exists to meet these three criteria, we introduce a speech-mesh synchronized representation that captures intricate correspondences between speech signals and 3D face meshes. We found that our learned representation exhibits desirable characteristics, and we plug it into existing models as a perceptual loss to better align lip movements to the given speech. In addition, we utilize this representation as a perceptual metric and introduce two other physically grounded lip synchronization metrics to assess how well the generated 3D talking heads align with these three criteria. Experiments show that training 3D talking head generation models with our perceptual loss significantly improve all three aspects of perceptually accurate lip synchronization. Codes and datasets are available at https://perceptual-3d-talking-head.github.io/",
    "checked": true,
    "id": "79ac360f4014001847a687d4c424a13727e93628",
    "semantic_title": "perceptually accurate 3d talking head generation: new definitions, speech-mesh representation, and evaluation metrics",
    "citation_count": 3,
    "authors": [
      "Lee Chae-Yeon",
      "Oh Hyun-Bin",
      "Han EunGi",
      "Kim Sung-Bin",
      "Suekyeong Nam",
      "Tae-Hyun Oh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with_CVPR_2025_paper.html": {
    "title": "BWFormer: Building Wireframe Reconstruction from Airborne LiDAR Point Cloud with Transformer",
    "volume": "main",
    "abstract": "In this paper, we present BWFormer, a novel Transformer-based model for building wireframe reconstruction from airborne LiDAR point cloud. The problem is solved in a ground-up manner here by detecting the building corners in 2D, lifting and connecting them in 3D space afterwards with additional data augmentation.Due to the 2.5D characteristic of the airborne LiDAR point cloud, we simplify the problem by projecting the points on the ground plane to produce a 2D height map. With the height map, a heat map is first generated with pixel-wise corner likelihood to predict the possible 2D corners.Then, 3D corners are predicted by a Transformer-based network with extra height embedding initialization. This 2D-to-3D corner detection strategy reduces the search space significantly. To recover the topological connections among the corners, edges are finally predicted from the height map with the proposed edge attention mechanism, which extracts holistic features and preserves local details simultaneously. In addition, due to the limited datasets in the field and the irregularity of the point clouds, a conditional latent diffusion model for LiDAR scanning simulation is utilized for data augmentation. BWFormer surpasses other state-of-the-art methods, especially in reconstruction completeness. Our code is available at: https://github.com/3dv-casia/BWformer/",
    "checked": true,
    "id": "305c6147d3ae0b4dad4f7d4740d2a03c6a17d6ac",
    "semantic_title": "bwformer: building wireframe reconstruction from airborne lidar point cloud with transformer",
    "citation_count": 0,
    "authors": [
      "Yuzhou Liu",
      "Lingjie Zhu",
      "Hanqiao Ye",
      "Shangfeng Huang",
      "Xiang Gao",
      "Xianwei Zheng",
      "Shuhan Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Diffusion-4K_Ultra-High-Resolution_Image_Synthesis_with_Latent_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent Diffusion Models",
    "volume": "main",
    "abstract": "In this paper, we present Diffusion-4K, a novel framework for direct ultra-high-resolution image synthesis using text-to-image diffusion models. The core advancements include: (1) Aesthetic-4K Benchmark: addressing the absence of a publicly available 4K image synthesis dataset, we construct Aesthetic-4K, a comprehensive benchmark for ultra-high-resolution image generation. We curated a high-quality 4K dataset with carefully selected images and captions generated by GPT-4o. Additionally, we introduce GLCM Score and compression ratio metrics to evaluate fine details, combined with holistic measures such as FID, Aesthetics and CLIPScore for a comprehensive assessment of ultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose a wavelet-based fine-tuning approach for direct training with photorealistic 4K images, applicable to various latent diffusion models, demonstrating its effectiveness in synthesizing highly detailed 4K images. Consequently, Diffusion-4K achieves impressive performance in high-quality image synthesis and text prompt adherence, especially when powered by modern large-scale diffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental results from our benchmark demonstrate the superiority of Diffusion-4K in ultra-high-resolution image synthesis. Code is available at https://github.com/zhang0jhon/diffusion-4k",
    "checked": true,
    "id": "bf232b405cad758317ddf0114f4822c5cec23efb",
    "semantic_title": "diffusion-4k: ultra-high-resolution image synthesis with latent diffusion models",
    "citation_count": 8,
    "authors": [
      "Jinjin Zhang",
      "Qiuyu Huang",
      "Junjie Liu",
      "Xiefan Guo",
      "Di Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_AffordDP_Generalizable_Diffusion_Policy_with_Transferable_Affordance_CVPR_2025_paper.html": {
    "title": "AffordDP: Generalizable Diffusion Policy with Transferable Affordance",
    "volume": "main",
    "abstract": "Diffusion-based policies have shown impressive performance in robotic manipulation tasks while struggling with out-of-domain distributions. Recent efforts attempted to enhance generalization by improving the visual feature encoding for diffusion policy. However, their generalization is typically limited to the same category with similar appearances. Our key insight is that leveraging affordances--manipulation priors that define \"where\" and \"how\" an agent interacts with an object--can substantially enhance generalization to entirely unseen object instances and categories. We introduce the Diffusion Policy with transferable Affordance (AffordDP), designed for generalizable manipulation across novel categories. AffordDP models affordances through 3D contact points and post-contact trajectories, capturing the essential static and dynamic information for complex tasks. The transferable affordance from in-domain data to unseen objects is achieved by estimating a 6D transformation matrix using foundational vision models and point cloud registration techniques. More importantly, we incorporate affordance guidance during diffusion sampling that can refine action sequence generation. This guidance directs the generated action to gradually move towards the desired manipulation for unseen objects while keeping the generated action within the manifold of action space. Experimental results from both simulated and real-world environments demonstrate that AffordDP consistently outperforms previous diffusion-based methods, successfully generalizing to unseen instances and categories where others fail",
    "checked": true,
    "id": "d1d94cc5fc86a52e4b1afd3e4038ba3cdbccf57e",
    "semantic_title": "afforddp: generalizable diffusion policy with transferable affordance",
    "citation_count": 10,
    "authors": [
      "Shijie Wu",
      "Yihang Zhu",
      "Yunao Huang",
      "Kaizhen Zhu",
      "Jiayuan Gu",
      "Jingyi Yu",
      "Ye Shi",
      "Jingya Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kumbong_HMAR_Efficient_Hierarchical_Masked_Auto-Regressive_Image_Generation_CVPR_2025_paper.html": {
    "title": "HMAR: Efficient Hierarchical Masked Auto-Regressive Image Generation",
    "volume": "main",
    "abstract": "Visual AutoRegressive modeling (VAR) shows promise in bridging the speed and quality gap between autoregressive image models and diffusion models. VAR reformulates autoregressive modeling by decomposing an image into successive resolution scales. During inference, an image is generated by predicting all the tokens in the next (higher-resolution) scale, conditioned on all tokens in all previous (lower-resolutions) scales. However, this formulation suffers from reduced image quality due to parallel generation of all tokens in a resolution scale; has sequence lengths scaling superlinearly in image resolution; and requires retraining to change the resolution sampling schedule.We introduce \\underline H ierarchical \\underline M asked \\underline A uto\\underline R egressive modeling (HMAR), a new image generation algorithm that alleviates these issues using next-scale prediction and masked prediction to generate high-quality images with fast sampling. HMAR reformulates next-scale prediction as a Markovian process, wherein prediction of each resolution scale is conditioned only on tokens in its immediate predecessor instead of the tokens in all predecessor resolutions. When predicting a resolution scale, HMAR uses a controllable multi-step masked generation procedure to generate a subset of the tokens in each step. On ImageNet 256 x 256 and 512 x 512 benchmarks, HMAR models match or outperform parameter-matched VAR, diffusion, and autoregressive baselines. We develop efficient IO-aware block-sparse attention kernels that allow HMAR to achieve faster training and inference times over VAR by over 2.5xand 1.75 xrespectively, as well as over 3 xlower inference memory footprint. Finally, the Markovian formulation of HMAR yields additional flexibility over VAR; we show that its sampling schedule can be changed without further training, and it can be applied to image editing tasks in a zero-shot manner",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hermann Kumbong",
      "Xian Liu",
      "Tsung-Yi Lin",
      "Ming-Yu Liu",
      "Xihui Liu",
      "Ziwei Liu",
      "Daniel Y. Fu",
      "Christopher Re",
      "David W. Romero"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_OmniDrive_A_Holistic_Vision-Language_Dataset_for_Autonomous_Driving_with_Counterfactual_CVPR_2025_paper.html": {
    "title": "OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving with Counterfactual Reasoning",
    "volume": "main",
    "abstract": "The advances in vision-language models (VLMs) have led to a growing interest in autonomous driving to leverage their strong reasoning capabilities. However, extending these capabilities from 2D to full 3D understanding is crucial for real-world applications. To address this challenge, we propose OmniDrive, a holistic vision-language dataset that aligns agent models with 3D driving tasks through counterfactual reasoning. This approach enhances decision-making by evaluating potential scenarios and their outcomes, similar to human drivers considering alternative actions. Our counterfactual-based synthetic data annotation process generates large-scale, high-quality datasets, providing denser supervision signals that bridge planning trajectories and language-based reasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely Omni-L and Omni-Q, to assess the importance of vision-language alignment versus 3D perception, revealing critical insights into designing effective LLM-agents. Significant improvements on the DriveLM Q&A benchmark and nuScenes open-loop planning demonstrate the effectiveness of our dataset and methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shihao Wang",
      "Zhiding Yu",
      "Xiaohui Jiang",
      "Shiyi Lan",
      "Min Shi",
      "Nadine Chang",
      "Jan Kautz",
      "Ying Li",
      "Jose M. Alvarez"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cui_DKC_Differentiated_Knowledge_Consolidation_for_Cloth-Hybrid_Lifelong_Person_Re-identification_CVPR_2025_paper.html": {
    "title": "DKC: Differentiated Knowledge Consolidation for Cloth-Hybrid Lifelong Person Re-identification",
    "volume": "main",
    "abstract": "Lifelong person re-identification (LReID) aims to match the same person using sequentially collected data. However, due to the long-term nature of lifelong learning, the inevitable changes in human clothes prevent the model from relying on unified discriminative information (e.g., clothing style) to match the same person in the streaming data, demanding differentiated cloth-irrelevant information. Unfortunately, existing LReID methods typically fail to leverage such knowledge resulting in the exacerbation of catastrophic forgetting issues. Therefore, in this paper, we focus on a challenging practical task called Cloth-Hybrid Lifelong Person Re-identification (CH-LReID), which requires matching the same person wearing different clothes using sequentially collected data. A Differentiated Knowledge Consolidation (DKC) framework is designed to unify and balance distinct knowledge across streaming data. The core idea is to adaptively balance differentiated knowledge and compatibly consolidate cloth-relevant and cloth-irrelevant information. To this end, a Differentiated Knowledge Transfer (DKT) module and a Latent Knowledge Consolidation (LKC) module are designed to adaptively discover differentiated new knowledge, while eliminating the derived domain shift of old knowledge via reconstructing the old latent feature space, respectively. Then, to further alleviate the catastrophic conflict between differentiated new and old knowledge, we further propose a Dual-level Distribution Alignment (DDA) module to align the distribution of discriminative knowledge at both the instance level and the fine-grained level. Extensive experiments on multiple benchmarks demonstrate the superiority of our method against existing methods in both CH-LReID and traditional LReID tasks. The source code of this paper is available at https://github.com/PKU-ICST-MIPL/DKC-CVPR2025",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Cui",
      "Jiahuan Zhou",
      "Yuxin Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Salar_Enhancing_Facial_Privacy_Protection_via_Weakening_Diffusion_Purification_CVPR_2025_paper.html": {
    "title": "Enhancing Facial Privacy Protection via Weakening Diffusion Purification",
    "volume": "main",
    "abstract": "The rapid growth of social media has led to the widespread sharing of individual portrait images, which pose serious privacy risks due to the capabilities of automatic face recognition (AFR) systems for mass surveillance. Hence, protecting facial privacy against unauthorized AFR systems is essential. Inspired by the generation capability of the emerging diffusion models, recent methods employ diffusion models to generate adversarial face images for privacy protection. However, they suffer from the diffusion purification effect, leading to a low protection success rate (PSR). In this paper, we first propose learning unconditional embeddings to increase the learning capacity for adversarial modifications and then use them to guide the modification of the adversarial latent code to weaken the diffusion purification effect. Moreover, we integrate an identity-preserving structure to maintain structural consistency between the original and generated images, allowing human observers to recognize the generated image as having the same identity as the original. Extensive experiments conducted on two public datasets, i.e., CelebA-HQ and LADN, demonstrate the superiority of our approach. The protected faces generated by our method outperform those produced by existing facial privacy protection approaches in terms of transferability and natural appearance. The code is available at https://github.com/parham1998/Facial-Privacy-Protection",
    "checked": true,
    "id": "4e8de21560ed183d351150555c03cab12280d1c0",
    "semantic_title": "enhancing facial privacy protection via weakening diffusion purification",
    "citation_count": 3,
    "authors": [
      "Ali Salar",
      "Qing Liu",
      "Yingli Tian",
      "Guoying Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_ORIDa_Object-centric_Real-world_Image_Composition_Dataset_CVPR_2025_paper.html": {
    "title": "ORIDa: Object-centric Real-world Image Composition Dataset",
    "volume": "main",
    "abstract": "Object compositing, the task of placing and harmonizing objects in images of diverse visual scenes, has become an important task in computer vision with the rise of generative models.However, existing datasets lack the diversity and scale required to comprehensively explore real-world scenarios comprehensively. We introduce ORIDa (Object-centric Real-world Image Composition Dataset), a large-scale, real-captured dataset containing over 30,000 images featuring 200 unique objects, each of which is presented across varied positions and scenes. ORIDa has two types of data: factual-counterfactual sets and factual-only scenes. The factual-counterfactual sets consist of four factual images showing an object in different positions within a scene and a single counterfactual (or background) image of the scene without the object, resulting in five images per scene. The factual-only scenes include a single image containing an object in a specific context, expanding the variety of environments. To our knowledge, ORIDa is the first publicly available dataset with its scale and complexity for real-world image composition. Extensive analysis and experiments highlight the value of ORIDa as a resource for advancing further research in object compositing",
    "checked": true,
    "id": "02e57ba127c626f46d626a9f963727e9ad4768a8",
    "semantic_title": "orida: object-centric real-world image composition dataset",
    "citation_count": 1,
    "authors": [
      "Jinwoo Kim",
      "Sangmin Han",
      "Jinho Jeong",
      "Jiwoo Choi",
      "Dongyeoung Kim",
      "Seon Joo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MeGA_Hybrid_Mesh-Gaussian_Head_Avatar_for_High-Fidelity_Rendering_and_Head_CVPR_2025_paper.html": {
    "title": "MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and Head Editing",
    "volume": "main",
    "abstract": "Creating high-fidelity head avatars from multi-view videos is essential for many AR/VR applications. However, current methods often struggle to achieve high-quality renderings across all head components (e.g., skin vs. hair) due to the limitations of using one single representation for elements with varying characteristics. In this paper, we introduce a Hybrid Mesh-Gaussian Head Avatar (MeGA) that models different head components with more suitable representations. Specifically, we employ an enhanced FLAME mesh for the facial representation and predict a UV displacement map to provide per-vertex offsets for improved personalized geometric details. To achieve photorealistic rendering, we use deferred neural rendering to obtain facial colors and decompose neural textures into three meaningful parts. For hair modeling, we first build a static canonical hair using 3D Gaussian Splatting. A rigid transformation and an MLP-based deformation field are further applied to handle complex dynamic expressions. Combined with our occlusion-aware blending, MeGA generates higher-fidelity renderings for the whole head and naturally supports diverse downstream tasks. Experiments on the NeRSemble dataset validate the effectiveness of our designs, outperforming previous state-of-the-art methods and enabling versatile editing capabilities, including hairstyle alteration and texture editing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Wang",
      "Di Kang",
      "Heyi Sun",
      "Shenhan Qian",
      "Zixuan Wang",
      "Linchao Bao",
      "Song-Hai Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dombrowski_Image_Generation_Diversity_Issues_and_How_to_Tame_Them_CVPR_2025_paper.html": {
    "title": "Image Generation Diversity Issues and How to Tame Them",
    "volume": "main",
    "abstract": "Generative methods have reached a level of quality that is almost indistinguishable from real data. However, while individual samples may appear unique, generative models often exhibit limitations in covering the full data distribution. Unlike quality issues, diversity problems within generative models are not easily detected by simply observing single images or generated datasets, which means we need a specific measure to assess the diversity of these models. In this paper, we draw attention to the current lack of diversity in generative models and the inability of common metrics to measure this. We achieve this by framing diversity as an image retrieval problem, where we measure how many real images can be retrieved using synthetic data as queries. This yields the Image Retrieval Score (IRS), an interpretable, hyperparameter-free metric that quantifies the diversity of a generative model's output. IRS requires only a subset of synthetic samples and provides a statistical measure of confidence. Our experiments indicate that current feature extractors commonly used in generative model assessment are inadequate for evaluating diversity effectively. Consequently, we perform an extensive search for the best feature extractors to assess diversity. Evaluation reveals that current diffusion models converge to limited subsets of the real distribution, with no current state-of-the-art models superpassing 77% of the diversity of the training data. To address this limitation, we introduce Diversity-Aware Diffusion Models (DiADM), a novel approach that improves diversity of unconditional diffusion models without loss of image quality. We do this by disentangling diversity from image quality by using a diversity aware module that uses pseudo-unconditional features as input. We provide a Python package offering unified feature extraction and metric computation to further facilitate the evaluation of generative models https://github.com/MischaD/beyondfid",
    "checked": true,
    "id": "361b8ae37d90f110fcc744ba842e7e645c5f8fed",
    "semantic_title": "image generation diversity issues and how to tame them",
    "citation_count": 3,
    "authors": [
      "Mischa Dombrowski",
      "Weitong Zhang",
      "Sarah Cechnicka",
      "Hadrien Reynaud",
      "Bernhard Kainz"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation_CVPR_2025_paper.html": {
    "title": "Annotation Ambiguity Aware Semi-Supervised Medical Image Segmentation",
    "volume": "main",
    "abstract": "Despite the remarkable progress of deep learning-based methods in medical image segmentation, their use in clinical practice remains limited for two main reasons. First, obtaining a large medical dataset with precise annotations to train segmentation models is challenging. Secondly, most current segmentation techniques generate a single deterministic segmentation mask for each image. However, in real-world scenarios, there is often significant uncertainty regarding what defines the \"correct\" segmentation, and various expert annotators might provide different segmentations for the same image. To tackle both of these problems, we propose Annotation Ambiguity Aware Semi-Supervised Medical Image Segmentation (AmbiSSL). AmbiSSL combines a small amount of multi-annotator labeled data and a large set of unlabeled data to generate diverse and plausible segmentation maps. Our method consists of three key components: (1) The Diverse Pseudo-Label Generation (DPG) module utilizes multiple decoders, created by performing randomized pruning on the original backbone decoder. These pruned decoders enable the generation of a diverse pseudo-label set; (2) a Semi-Supervised Latent Distribution Learning (SSLDL) module constructs a common latent space by utilizing both ground truth annotations and pseudo-label set; and (3) a Cross-Decoder Supervision (CDS) module, which enables pruned decoders to guide each other's learning. We evaluated the proposed method on two publicly available datasets. Extensive experiments demonstrate that AmbiSSL can generate diverse segmentation maps using only a small amount of labeled data and abundant unlabeled data, offering a more practical solution for medical image segmentation by reducing reliance on large labeled datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suruchi Kumari",
      "Pravendra Singh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Effective_Cloud_Removal_for_Remote_Sensing_Images_by_an_Improved_CVPR_2025_paper.html": {
    "title": "Effective Cloud Removal for Remote Sensing Images by an Improved Mean-Reverting Denoising Model with Elucidated Design Space",
    "volume": "main",
    "abstract": "Cloud removal (CR) remains a challenging task in remote sensing image processing. Although diffusion models (DM) exhibit strong generative capabilities, their direct applications to CR are suboptimal, as they generate cloudless images from random noise, ignoring inherent information in cloudy inputs. To overcome this drawback, we develop a new CR model EMRDM based on mean-reverting diffusion models (MRDMs) to establish a direct diffusion process between cloudy and cloudless images. Compared to current MRDMs, EMRDM offers a modular framework with updatable modules and an elucidated design space, based on a reformulated forward process and a new ordinary differential equation (ODE)-based backward process. Leveraging our framework, we redesign key MRDM modules to boost CR performance, including restructuring the denoiser via a preconditioning technique, reorganizing the training process, and improving the sampling process by introducing deterministic and stochastic samplers. To achieve multi-temporal CR, we further develop a denoising network for simultaneously denoising sequential images. Experiments on mono-temporal and multi-temporal datasets demonstrate the superior performance of EMRDM. Our code is available at https://github.com/Ly403/EMRDM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Liu",
      "Wengen Li",
      "Jihong Guan",
      "Shuigeng Zhou",
      "Yichao Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion_CVPR_2025_paper.html": {
    "title": "CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models",
    "volume": "main",
    "abstract": "Reconstructing photorealistic and dynamic portrait avatars from images is essential to many applications including advertising, visual effects, and virtual reality. Depending on the application, avatar reconstruction involves different capture setups and constraints -- for example, visual effects studios use camera arrays to capture hundreds of reference images, while content creators may seek to animate a single portrait image downloaded from the internet. As such, there is a large and heterogeneous ecosystem of methods for avatar reconstruction. Techniques based on multi-view stereo or neural rendering achieve the highest quality results, but require hundreds of reference images. Recent generative models produce convincing avatars from a single reference image, but visual fidelity lags behind multi-view techniques. Here, we present CAP4D: an approach that uses a morphable multi-view diffusion model to reconstruct photoreal 4D (dynamic 3D) portrait avatars from any number of reference images (i.e., one to 100) and animate and render them in real time. Our approach demonstrates state-of-the-art performance for single-, few-, and multi-image 4D portrait avatar reconstruction, and takes steps to bridge the gap in visual fidelity between single-image and multi-view reconstruction techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Taubner",
      "Ruihang Zhang",
      "Mathieu Tuli",
      "David B. Lindell"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision_CVPR_2025_paper.html": {
    "title": "Comprehensive Information Bottleneck for Unveiling Universal Attribution to Interpret Vision Transformers",
    "volume": "main",
    "abstract": "The feature attribution method reveals the contribution of input variables to the decision-making process to provide an attribution map for explanation. Existing methods grounded on the information bottleneck principle compute information in a specific layer to obtain attributions, compressing the features by injecting noise via a parametric damping ratio. However, the attribution obtained in a specific layer neglects evidence of the decision-making process distributed across layers. In this paper, we introduce a comprehensive information bottleneck (CoIBA), which discovers the relevant information in each targeted layer to explain the decision-making process. Our core idea is applying information bottleneck in multiple targeted layers to estimate the comprehensive information by sharing a parametric damping ratio across the layers. Leveraging this shared ratio complements the over-compressed information to discover the omitted clues of the decision by sharing the relevant information across the targeted layers. We suggest the variational approach to fairly reflect the relevant information of each layer by upper bounding layer-wise information. Therefore, CoIBA guarantees that the discarded activation is unnecessary in every targeted layer to make a decision. The extensive experimental results demonstrate the enhancement in faithfulness of the feature attributions provided by CoIBA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jung-Ho Hong",
      "Ho-Joong Kim",
      "Kyu-Sung Jeon",
      "Seong-Whan Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction_CVPR_2025_paper.html": {
    "title": "OpticalNet: An Optical Imaging Dataset and Benchmark Beyond the Diffraction Limit",
    "volume": "main",
    "abstract": "Optical imaging capable of resolving nanoscale features would revolutionize scientific research and engineering applications across biomedicine, smart manufacturing, and semiconductor quality control. However, due to the physical phenomenon of diffraction, the optical resolution is limited to approximately half the wavelength of light, which impedes the observation of subwavelength objects such as the native state coronavirus, typically smaller than 200 nm. Fortunately, deep learning methods have shown remarkable potential in uncovering underlying patterns within data, promising to overcome the diffraction limit by revealing the mapping pattern between diffraction images and their corresponding ground truth object images. However, the absence of suitable datasets has hindered progress in this field--collecting high-quality optical data of subwavelength objects is highly difficult as these objects are inherently invisible under conventional microscopy, making it impossible to perform standard visual calibration and drift correction. Therefore, we provide the first general optical imaging dataset based on the \"building block\" concept for challenging the diffraction limit. Drawing an analogy to modular construction principles, we construct a comprehensive optical imaging dataset comprising subwavelength fundamental elements, i.e., small square units that can be assembled into larger and more complex objects. We then frame the task as an image-to-image translation task and evaluate various vision methods. Experimental results validate our \"building block\" concept, demonstrating that models trained on basic square units can effectively generalize to realistic, more complex unseen objects. Most importantly, by highlighting this underexplored AI-for-science area and its potential, we aspire to advance optical science by fostering collaboration with the vision and machine learning communities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benquan Wang",
      "Ruyi An",
      "Jin-Kyu So",
      "Sergei Kurdiumov",
      "Eng Aik Chan",
      "Giorgio Adamo",
      "Yuhan Peng",
      "Yewen Li",
      "Bo An"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective_CVPR_2025_paper.html": {
    "title": "Dataset Distillation with Neural Characteristic Function: A Minmax Perspective",
    "volume": "main",
    "abstract": "Dataset distillation has emerged as a powerful approach for reducing data requirements in deep learning. Among various methods, distribution matching-based approaches stand out for their balance of computational efficiency and strong performance. However, existing distance metrics used in distribution matching often fail to accurately capture distributional differences, leading to unreliable measures of discrepancy. In this paper, we reformulate dataset distillation as a minmax optimization problem and introduce Neural Characteristic Function Discrepancy (NCFD), a comprehensive and theoretically grounded metric for measuring distributional differences. NCFD leverages the Characteristic Function (CF) to encapsulate full distributional information, employing a neural network to optimize the sampling strategy for the CF's frequency arguments, thereby maximizing the discrepancy to enhance distance estimation. Simultaneously, we minimize the difference between real and synthetic data under this optimized NCFD measure. Our approach, termed Neural Characteristic Function Matching (NCFM), inherently aligns the phase and amplitude of neural features in the complex plane for both real and synthetic data, achieving a balance between realism and diversity in synthetic samples. Experiments demonstrate that our method achieves significant performance gains over state-of-the-art methods on both low- and high-resolution datasets. Notably, we achieve a 22.9% accuracy boost on ImageSquawk. Our method also reduces GPU memory usage by over 300x and achieves 20x faster processing speeds compared to state-of-the-art methods. To the best of our knowledge, this is the first work to achieve lossless compression of CIFAR-100 on a single NVIDIA 2080 Ti GPU using only 2.3 GB of memory. The code for this work is publicly available at: https://github.com/gszfwsb/NCFM",
    "checked": true,
    "id": "ed343a29ef8e1152cbde39147f4af93c8c31c885",
    "semantic_title": "dataset distillation with neural characteristic function: a minmax perspective",
    "citation_count": 15,
    "authors": [
      "Shaobo Wang",
      "Yicun Yang",
      "Zhiyuan Liu",
      "Chenghao Sun",
      "Xuming Hu",
      "Conghui He",
      "Linfeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection_CVPR_2025_paper.html": {
    "title": "Free-viewpoint Human Animation with Pose-correlated Reference Selection",
    "volume": "main",
    "abstract": "Diffusion-based human animation aims to animate a human character based on a source human image as well as driving signals such as a sequence of poses. Leveraging the generative capacity of diffusion model, existing approaches are able to generate high-fidelity poses, but struggle with significant viewpoint changes, especially in zoom-in/zoom-out scenarios where camera-character distance varies. This limits the applications such as cinematic shot type plan or camera control. We propose a pose-correlated reference selection diffusion network, supporting substantial viewpoint variations in human animation. Our key idea is to enable the network to utilize multiple reference images as input, since significant viewpoint changes often lead to missing appearance details on the human body. To eliminate the computational cost, we first introduce a novel pose correlation module to compute similarities between non-aligned target and source poses, and then propose an adaptive reference selection strategy, utilizing the attention map to identify key regions for animation generation. To train our model, we curated a large dataset from public TED talks featuring varied shots of the same character, helping the model learn synthesis for different perspectives. Our experimental results show that with the same number of reference images, our model performs favorably compared to the current SOTA methods under large viewpoint change. We further show that the adaptive reference selection is able to choose the most relevant reference regions to generate humans under free viewpoints",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fa-Ting Hong",
      "Zhan Xu",
      "Haiyang Liu",
      "Qinjie Lin",
      "Luchuan Song",
      "Zhixin Shu",
      "Yang Zhou",
      "Duygu Ceylan",
      "Dan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_CORE4D_A_4D_Human-Object-Human_Interaction_Dataset_for_Collaborative_Object_REarrangement_CVPR_2025_paper.html": {
    "title": "CORE4D: A 4D Human-Object-Human Interaction Dataset for Collaborative Object REarrangement",
    "volume": "main",
    "abstract": "Understanding how humans cooperatively rearrange household objects is critical for VR/AR and human-robot interaction. However, in-depth studies on modeling these behaviors are under-researched due to the lack of relevant datasets. We fill this gap by presenting CORE4D, a novel large-scale 4D human-object-human interaction dataset focusing on collaborative object rearrangement, which encompasses diverse compositions of various object geometries, collaboration modes, and 3D scenes. With 1K human-object-human motion sequences captured in the real world, we enrich CORE4D by contributing an iterative collaboration retargeting strategy to augment motions to a variety of novel objects. Leveraging this approach, CORE4D comprises a total of 11K collaboration sequences spanning 3K real and virtual object shapes. Benefiting from extensive motion patterns provided by CORE4D, we benchmark two tasks aiming at generating human-object interaction: human-object motion forecasting and interaction synthesis. Extensive experiments demonstrate the effectiveness of our collaboration retargeting strategy and indicate that CORE4D has posed new challenges to existing human-object interaction generation methodologies",
    "checked": true,
    "id": "eb885c05e45a1aa0be7bafb32732c03f3c6030e6",
    "semantic_title": "core4d: a 4d human-object-human interaction dataset for collaborative object rearrangement",
    "citation_count": 18,
    "authors": [
      "Yun Liu",
      "Chengwen Zhang",
      "Ruofan Xing",
      "Bingda Tang",
      "Bowen Yang",
      "Li Yi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_PillarHist_A_Quantization-aware_Pillar_Feature_Encoder_based_on_Height-aware_Histogram_CVPR_2025_paper.html": {
    "title": "PillarHist: A Quantization-aware Pillar Feature Encoder based on Height-aware Histogram",
    "volume": "main",
    "abstract": "Real-time and high-performance 3D object detection plays a critical role in autonomous driving and robotics. Recent pillar-based 3D object detectors have gained significant attention due to their compact representation and low computational overhead, making them suitable for onboard deployment and quantization. However, existing pillar-based detectors still suffer from information loss along height dimension and large numerical distribution difference during pillar feature encoding (PFE), which severely limits their performance and quantization potential. To address above issue, we first unveil the importance of different input information during PFE and identify the height dimension as a key factor in enhancing 3D detection performance. Motivated by this observation, we propose a height-aware pillar feature encoder, called PillarHist. Specifically, PillarHist statistics the discrete distribution of points at different heights within one pillar. This simple yet effective design greatly preserves the information along the height dimension while significantly reducing the computation overhead of the PFE. Meanwhile, PillarHist also constrains the arithmetic distribution of PFE input to a stable range, making it quantization-friendly. Notably, PillarHist operates exclusively within the PFE stage to enhance performance, enabling seamless integration into existing pillar-based methods without introducing complex operations. Extensive experiments show the effectiveness of PillarHist in terms of both efficiency and performance",
    "checked": true,
    "id": "b61c0af51e38b789da964d271f9bc692f1e8471e",
    "semantic_title": "pillarhist: a quantization-aware pillar feature encoder based on height-aware histogram",
    "citation_count": 5,
    "authors": [
      "Sifan Zhou",
      "Zhihang Yuan",
      "Dawei Yang",
      "Xing Hu",
      "Jian Qian",
      "Ziyu Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wilson_POp-GS_Next_Best_View_in_3D-Gaussian_Splatting_with_P-Optimality_CVPR_2025_paper.html": {
    "title": "POp-GS: Next Best View in 3D-Gaussian Splatting with P-Optimality",
    "volume": "main",
    "abstract": "In this paper, we present a novel algorithm for quantifying uncertainty and information gained within 3D Gaussian Splatting (3D-GS) through P-Optimality. While 3D-GS has proven to be a useful world model with high-quality rasterizations, it does not natively quantify uncertainty or information, posing a challenge for real-world applications such as 3D-GS SLAM. We propose to quantify information gain in 3D-GS by reformulating the problem through the lens of optimal experimental design, which is a classical solution widely used in literature. By restructuring information quantification of 3D-GS through optimal experimental design, we arrive at multiple solutions, of which T-Optimality and D-Optimality perform the best quantitatively and qualitatively as measured on two popular datasets. Additionally, we propose a block diagonal covariance approximation which provides a measure of correlation at the expense of a greater computation cost",
    "checked": true,
    "id": "c20a975e390ee5320bfe2b613913b923456805a5",
    "semantic_title": "pop-gs: next best view in 3d-gaussian splatting with p-optimality",
    "citation_count": 0,
    "authors": [
      "Joey Wilson",
      "Marcelino Almeida",
      "Sachit Mahajan",
      "Martin Labrie",
      "Maani Ghaffari",
      "Omid Ghasemalizadeh",
      "Min Sun",
      "Cheng-Hao Kuo",
      "Arnab Sen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility_CVPR_2025_paper.html": {
    "title": "Empowering Vector Graphics with Consistently Arbitrary Viewing and View-dependent Visibility",
    "volume": "main",
    "abstract": "This work presents a novel text-to-vector graphics generation approach, Dream3DVG, allowing for arbitrary viewpoint viewing, progressive detail optimization, and view-dependent occlusion awareness. Our approach is a dual-branch optimization framework, consisting of an auxiliary 3D Gaussian Splatting optimization branch and a 3D vector graphics optimization branch. The introduced 3DGS branch can bridge the domain gaps between text prompts and vector graphics with more consistent guidance. Moreover, 3DGS allows for progressive detail control by scheduling classifier-free guidance, facilitating guiding vector graphics with coarse shapes at the initial stages and finer details at later stages. We also improve the view-dependent occlusions by devising a visibility-awareness rendering module. Extensive results on 3D sketches and 3D iconographies, demonstrate the superiority of the method on different abstraction levels of details, cross-view consistency, and occlusion-aware stroke culling",
    "checked": true,
    "id": "01939c04d3e872c28226422f1eb15afdaa7ba969",
    "semantic_title": "empowering vector graphics with consistently arbitrary viewing and view-dependent visibility",
    "citation_count": 1,
    "authors": [
      "Yidi Li",
      "Jun Xiao",
      "Zhengda Lu",
      "Yiqun Wang",
      "Haiyong Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_Semantic_and_Expressive_Variations_in_Image_Captions_Across_Languages_CVPR_2025_paper.html": {
    "title": "Semantic and Expressive Variations in Image Captions Across Languages",
    "volume": "main",
    "abstract": "Most vision-language models today are primarily trained on English image-text pairs, with non-English pairs often filtered out. Evidence from cross-cultural psychology suggests that this approach will bias models against perceptual modes exhibited by people who speak other (non-English) languages. We investigate semantic and expressive variation in image captions across different languages; we analyze both human-annotated datasets and model-produced captions. By analyzing captions across seven languages (English, French, German, Russian, Chinese, Japanese, Korean) in high-quality image captioning datasets (Crossmodal and Visual Genome), we find that multilingual caption sets tend to provide richer visual descriptions than monolingual (including English-only) ones; multilingual sets contain 46.0% more objects, 66.1% more relationships, and 66.8% more attributes. We observe the same results with multilingual captions produced by LLaVA and the Google Vertex API: for example, compared to monolingual captions, they cover 21.9% more objects,18.8% more relations, and 20.1% more attributes. These suggest that, across a large number of samples, different languages bias people and models to focus on different visual concepts. Finally, we show that models trained on image-text data in one language perform distinctly better on that language's test set. Our work points towards the potential value of training vision models on multilingual data sources to widen the range/variation of descriptive information those models are exposed to",
    "checked": false,
    "id": "64e16687f2356973dad1f2b4d0b56a40d86a8345",
    "semantic_title": "semantic and expressive variation in image captions across languages",
    "citation_count": 4,
    "authors": [
      "Andre Ye",
      "Sebastin Santy",
      "Jena D. Hwang",
      "Amy X. Zhang",
      "Ranjay Krishna"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_ATP-LLaVA_Adaptive_Token_Pruning_for_Large_Vision_Language_Models_CVPR_2025_paper.html": {
    "title": "ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models",
    "volume": "main",
    "abstract": "Large Vision Language Models (LVLMs) have achieved significant success across multi-modal tasks. However, the computational cost of processing long visual tokens can be prohibitively expensive on resource-limited devices. Previous methods have identified redundancy in visual tokens within the Large Language Model (LLM) decoder layers and have mitigated this by pruning tokens using a pre-defined or fixed ratio, thereby reducing computational overhead. Nonetheless, we observe that the impact of pruning ratio varies across different LLM layers and instances (image-prompt pairs). Therefore, it is essential to develop a layer-wise and instance-wise vision token pruning strategy to balance computational cost and model performance effectively. We propose ATP-LLaVA, a novel approach that adaptively determines instance-specific token pruning ratios for each LLM layer. Specifically, we introduce an Adaptive Token Pruning (ATP) module, which computes the importance score and pruning threshold based on input instance adaptively. The ATP module can be seamlessly integrated between any two LLM layers with negligible computational overhead. Additionally, we develop a Spatial Augmented Pruning (SAP) strategy that prunes visual tokens with both token redundancy and spatial modeling perspectives. Our approach reduces the average token count by 75% while maintaining performance, with only a minimal 1.9% degradation across seven widely used benchmarks",
    "checked": true,
    "id": "134a66cc64344b9ef913e0f25c2fab6f4ff92c94",
    "semantic_title": "atp-llava: adaptive token pruning for large vision language models",
    "citation_count": 19,
    "authors": [
      "Xubing Ye",
      "Yukang Gan",
      "Yixiao Ge",
      "Xiao-Ping Zhang",
      "Yansong Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mi_ADD_Attribution-Driven_Data_Augmentation_Framework_for_Boosting_Image_Super-Resolution_CVPR_2025_paper.html": {
    "title": "ADD: Attribution-Driven Data Augmentation Framework for Boosting Image Super-Resolution",
    "volume": "main",
    "abstract": "Data augmentation (DA) stands out as a powerful technique to enhance the generalization capabilities of deep neural networks across diverse tasks. However, in low-level vision tasks, DA remains rudimentary (i.e., vanilla DA), facing a critical bottleneck due to information loss. In this paper, we introduce a novel Calibrated Attribution Maps (CAM) to generate saliency masks, followed by two saliency-based DA methods-- Attribution-Driven Data augmentation (ADD) and ADD+--designed to address this issue. CAM leverages integrated gradients and incorporates two key innovations: a global feature detector and calibrated integrated gradients. Based on CAM and the proposed methods, we have two new insights for low-level vision tasks: (1) increasing pixel diversity, as seen in vanilla DA, can improve performance, and (2) focusing on salient features while minimizing the impact of irrelevant pixels, as seen in saliency-based DA, more effectively enhances model performance. Additionally, we find and highlight the key guiding principle for designing saliency-based DA: a wider spectrum of degradation patterns. Extensive experiments demonstrate the compatibility and consistency of our method, as well as the significant performance improvement across various SR tasks and networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ze-Yu Mi",
      "Yu-Bin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_DIFFER_Disentangling_Identity_Features_via_Semantic_Cues_for_Clothes-Changing_Person_CVPR_2025_paper.html": {
    "title": "DIFFER: Disentangling Identity Features via Semantic Cues for Clothes-Changing Person Re-ID",
    "volume": "main",
    "abstract": "Clothes-changing person re-identification (CC-ReID) aims to recognize individuals under different clothing scenarios. Current CC-ReID approaches either concentrate on modeling body shape using additional modalities including silhouette, pose, and body mesh, potentially causing the model to overlook other critical biometric traits such as gender, age, and style, or they incorporate supervision through additional labels that the model tries to disregard or emphasize, such as clothing or personal attributes. However, these annotations are discrete in nature and do not capture comprehensive descriptions. In this work, we propose DIFFER: Disentangle Identity Features From Entangled Representations, a novel adversarial learning method that leverages textual descriptions to disentangle identity features. Recognizing that image features inherently mix inseparable information, DIFFER introduces NBDetach, a mechanism designed for feature disentanglement by leveraging the separable nature of text descriptions as supervision. It partitions the feature space into distinct subspaces and, through gradient reversal layers, effectively separates identity-related features from non-biometric features. We evaluate DIFFER on 4 different benchmark datasets (LTCC, PRCC, CelebreID-Light, and CCVID) to demonstrate its effectiveness and provide state-of-the-art performance across all the benchmarks. DIFFER consistently outperforms the baseline method, with improvements in top-1 accuracy of 3.6% on LTCC, 3.4% on PRCC, 2.5% on CelebReID-Light, and 1% on CCVID. Our code can be found at https://github.com/xliangp/DIFFER.git",
    "checked": true,
    "id": "cf53c21d47b9c7719e90e81159048c5240fb5f31",
    "semantic_title": "differ: disentangling identity features via semantic cues for clothes-changing person re-id",
    "citation_count": 4,
    "authors": [
      "Xin Liang",
      "Yogesh S Rawat"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ferens_HyperPose_Hypernetwork-Infused_Camera_Pose_Localization_and_an_Extended_Cambridge_Landmarks_CVPR_2025_paper.html": {
    "title": "HyperPose: Hypernetwork-Infused Camera Pose Localization and an Extended Cambridge Landmarks Dataset",
    "volume": "main",
    "abstract": "In this work, we propose HyperPose, which utilizes hypernetworks in absolute camera pose regressors. The inherent appearance variations in natural scenes, attributable to environmental conditions, perspective, and lighting, induce a significant domain disparity between the training and test datasets. This disparity degrades the precision of contemporary localization networks. To mitigate this, we advocate for incorporating hypernetworks into single-scene and multiscene camera pose regression models. During inference, the hypernetwork dynamically computes adaptive weights for the localization regression heads based on the particular input image, effectively narrowing the domain gap. Using indoor and outdoor datasets, we evaluate the HyperPose methodology across multiple established absolute pose regression architectures. In particular, we introduce and share the Extended Cambridge Landmarks (ECL), which is a novel localization dataset, based on the Cambridge Landmarks dataset, showing it in multiple seasons with significantly varying appearance conditions. Our empirical experiments demonstrate that HyperPose yields notable performance enhancements for both single- and multi-scene architectures. We have made our source code, pre-trained models, and ECL dataset openly available",
    "checked": true,
    "id": "6d28405111d348e10340ec7157e57eca030b7bf5",
    "semantic_title": "hyperpose: hypernetwork-infused camera pose localization and an extended cambridge landmarks dataset",
    "citation_count": 0,
    "authors": [
      "Ron Ferens",
      "Yosi Keller"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Critic-V_VLM_Critics_Help_Catch_VLM_Errors_in_Multimodal_Reasoning_CVPR_2025_paper.html": {
    "title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning",
    "volume": "main",
    "abstract": "Vision-language models (VLMs) have shown remarkable advancements in multimodal reasoning tasks. However, they still often generate inaccurate or irrelevant responses due to issues like hallucinated image understandings or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a novel framework inspired by the Actor-Critic paradigm to boost the reasoning capability of VLMs. This framework decouples the reasoning process and critic process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critique to refine these paths. In this approach, the Reasoner generates reasoning responses according to text prompts, which can evolve iteratively as a policy based on feedback from the Critic. This interaction process was theoretically driven by a reinforcement learning framework where the Critic offers natural language critiques instead of scalar rewards, enabling more nuanced feedback to boost the Reasoner's capability on complex reasoning tasks. The Critic model is trained using Direct Preference Optimization (DPO), leveraging a preference dataset of critiques ranked by Rule-based Reward (RBR) to enhance its critic capabilities. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner and constructive feedback from the preference-optimized Critic enables a more reliable and context-sensitive multimodal reasoning process. Our approach provides a promising solution to enhance the reliability of VLMs, improving their performance in real-world reasoning-heavy multimodal applications such as autonomous driving and embodied intelligence",
    "checked": true,
    "id": "8690c869ab7e18cdbd4f17e38a41727c16b28221",
    "semantic_title": "critic-v: vlm critics help catch vlm errors in multimodal reasoning",
    "citation_count": 12,
    "authors": [
      "Di Zhang",
      "Jingdi Lei",
      "Junxian Li",
      "Xunzhi Wang",
      "Yujie Liu",
      "Zonglin Yang",
      "Jiatong Li",
      "Weida Wang",
      "Suorong Yang",
      "Jianbo Wu",
      "Peng Ye",
      "Wanli Ouyang",
      "Dongzhan Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wei_Mono3DVLT_Monocular-Video-Based_3D_Visual_Language_Tracking_CVPR_2025_paper.html": {
    "title": "Mono3DVLT: Monocular-Video-Based 3D Visual Language Tracking",
    "volume": "main",
    "abstract": "Visual-Language Tracking (VLT) is emerging as a promising paradigm to bridge the human-machine performance gap. For single objects, VLT broadens the problem scope to text-driven video comprehension. Yet, this direction is still confined to 2D spatial extents, currently lacking the ability to deal with 3D tracking in the confines of monocular video. Unfortunately, advances in 3D tracking mainly rely on expensive sensor inputs, e.g., point clouds, depth measurements, radar. Absence of language counterpart for the outputs of these mildly democratized sensors in the literature also hinders VLT expansion to 3D tracking. Addressing that, we make the first attempt towards extending VLT to 3D tracking based on monocular video. We present a comprehensive framework, introducing (i) the Monocular-Video-based 3D Visual Language Tracking (Mono3DVLT) task, (ii) a large-scale dataset for the task, called Mono3DVLT-V2X, and (iii) a customized neural model for the task. Our dataset is carefully curated, leveraging a Large Langauge Model (LLM) followed by human verification, composing natural language descriptions for 79,158 video sequences aiming at single object tracking, providing 2D and 3D bounding box annotations. Our neural model, termed Mono3DVLT-MT, is the first targeted approach for the Mono3DVLT task. Comprising the pipeline of multi-modal feature extractor, visual-language encoder, tracking decoder and a tracking head, our model sets a strong baseline for the task on Mono3DVLT-V2X. Experimental results show that our method significantly outperforms existing techniques on the Mono3DVLT-V2X dataset. Our dataset and code are available in https://github.com/hongkai-wei/Mono3DVLT",
    "checked": true,
    "id": "320f902fd301f97f01c6aa36f443420892d5da83",
    "semantic_title": "mono3dvlt: monocular-video-based 3d visual language tracking",
    "citation_count": 0,
    "authors": [
      "Hongkai Wei",
      "Yang Yang",
      "Shijie Sun",
      "Mingtao Feng",
      "Xiangyu Song",
      "Qi Lei",
      "Hongli Hu",
      "Rong Wang",
      "Huansheng Song",
      "Naveed Akhtar",
      "Ajmal Saeed Mian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion_CVPR_2025_paper.html": {
    "title": "Towards Universal Dataset Distillation via Task-Driven Diffusion",
    "volume": "main",
    "abstract": "Dataset distillation (DD) condenses key information from large-scale datasets into smaller synthetic datasets, reducing storage and computational costs for training networks. However, recent research has primarily focused on image classification tasks, with limited expansion to detection and segmentation. Two key challenges remain: (i) Task Optimization Heterogeneity, where existing methods focus on class-level information and fail to address the diverse needs of detection and segmentation and (ii) Inflexible Image Generation, where current generation methods rely on global updates for single-class targets and lack localized optimization for specific object regions.To address these challenges, we propose a universal dataset distillation framework, named UniDD, a task-driven diffusion model for diverse DD tasks, as illustrated in Fig.1. Our approach operates in two stages: Universal Task Knowledge Mining, which captures task-relevant information through task-specific proxy model training, and Universal Task-Driven Diffusion, where these proxies guide the diffusion process to generate task-specific synthetic images.Extensive experiments across ImageNet-1K, Pascal VOC, and MS COCO demonstrate that UniDD consistently outperforms state-of-the-art methods. In particular, on ImageNet-1K with IPC-10, UniDD surpasses previous diffusion-based methods by 6.1%, while also reducing deployment costs",
    "checked": true,
    "id": "cbd137bc868da7005da1fdbb71dd9776bc7c4232",
    "semantic_title": "towards universal dataset distillation via task-driven diffusion",
    "citation_count": 1,
    "authors": [
      "Ding Qi",
      "Jian Li",
      "Junyao Gao",
      "Shuguang Dou",
      "Ying Tai",
      "Jianlong Hu",
      "Bo Zhao",
      "Yabiao Wang",
      "Chengjie Wang",
      "Cairong Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Parametric_Point_Cloud_Completion_for_Polygonal_Surface_Reconstruction_CVPR_2025_paper.html": {
    "title": "Parametric Point Cloud Completion for Polygonal Surface Reconstruction",
    "volume": "main",
    "abstract": "Existing polygonal surface reconstruction methods heavily depend on input completeness and struggle with incomplete point clouds. We argue that while current point cloud completion techniques may recover missing points, they are not optimized for polygonal surface reconstruction, where the parametric representation of underlying surfaces remains overlooked. To address this gap, we introduce parametric completion, a novel paradigm for point cloud completion, which recovers parametric primitives instead of individual points to convey high-level geometric structures. Our presented approach, PaCo, enables high-quality polygonal surface reconstruction by leveraging plane proxies that encapsulate both plane parameters and inlier points, proving particularly effective in challenging scenarios with highly incomplete data. Comprehensive evaluations of our approach on the ABC dataset establish its effectiveness with superior performance and set a new standard for polygonal surface reconstruction from incomplete data. Project page: https://parametric-completion.github.io",
    "checked": true,
    "id": "61a1e263d3250c1fc0675091added8931eaea555",
    "semantic_title": "parametric point cloud completion for polygonal surface reconstruction",
    "citation_count": 0,
    "authors": [
      "Zhaiyu Chen",
      "Yuqing Wang",
      "Liangliang Nan",
      "Xiao Xiang Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_SyncSDE_A_Probabilistic_Framework_for_Diffusion_Synchronization_CVPR_2025_paper.html": {
    "title": "SyncSDE: A Probabilistic Framework for Diffusion Synchronization",
    "volume": "main",
    "abstract": "There have been many attempts to leverage multiple diffusion models for collaborative generation, extending beyond the original domain. A prominent approach involves synchronizing multiple diffusion trajectories by mixing the estimated scores to artificially correlate the generation processes. However, existing methods rely on naive heuristics, such as averaging, without considering task specificity. These approaches do not clarify why such methods work and often fail when a heuristic suitable for one task is blindly applied to others. In this paper, we present a probabilistic framework for analyzing why diffusion synchronization works and reveal where heuristics should be focused--modeling correlations between multiple trajectories and adapting them to each specific task. We further identify optimal correlation models per task, achieving better results than previous approaches that apply a single heuristic across all tasks without justification",
    "checked": true,
    "id": "425429e059532a7a5876d387cae1a7bedfa6a00e",
    "semantic_title": "syncsde: a probabilistic framework for diffusion synchronization",
    "citation_count": 0,
    "authors": [
      "Hyunjun Lee",
      "Hyunsoo Lee",
      "Sookwan Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MaRI_Material_Retrieval_Integration_across_Domains_CVPR_2025_paper.html": {
    "title": "MaRI: Material Retrieval Integration across Domains",
    "volume": "main",
    "abstract": "Accurate material retrieval is critical for creating realistic 3D assets. Existing methods rely on datasets that capture shape-invariant and lighting-varied representations of materials, which are scarce and face challenges due to limited diversity and inadequate real-world generalization. Most current approaches adopt traditional image search techniques. They fall short in capturing the unique properties of material spaces, leading to suboptimal performance in retrieval tasks. Addressing these challenges, we introduce MaRI, a framework designed to bridge the feature space gap between synthetic and real-world materials. MaRI constructs a shared embedding space that harmonizes visual and material attributes through a contrastive learning strategy by jointly training a image and a material encoder, bringing similar materials and images closer while separating dissimilar pairs within the feature space. To support this, we construct a comprehensive dataset comprising high-quality synthetic materials rendered with controlled shape variations and diverse lighting conditions, along with real-world materials processed and standardized using material transfer techniques. Extensive experiments demonstrate the superior performance, accuracy, and generalization capabilities of MaRI across diverse and complex material retrieval tasks, outperforming existing methods",
    "checked": true,
    "id": "0b890e0260ce01c4dce1ca5c9f8b795af32e400c",
    "semantic_title": "mari: material retrieval integration across domains",
    "citation_count": 3,
    "authors": [
      "Jianhui Wang",
      "Zhifei Yang",
      "Yangfan He",
      "Huixiong Zhang",
      "Yuxuan Chen",
      "Jingwei Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_MCCD_Multi-Agent_Collaboration-based_Compositional_Diffusion_for_Complex_Text-to-Image_Generation_CVPR_2025_paper.html": {
    "title": "MCCD: Multi-Agent Collaboration-based Compositional Diffusion for Complex Text-to-Image Generation",
    "volume": "main",
    "abstract": "Diffusion models have shown excellent performance in text-to-image generation. However, existing methods often suffer from performance bottlenecks when dealing with complex prompts involving multiple objects, characteristics, and relations. Therefore, we propose a Multi-agent Collaboration-based Compositional Diffusion (MCCD) for text-to-image generation for complex scenes. Specifically, we design a multi-agent collaboration based scene parsing module that generates an agent system containing multiple agents with different tasks using MLLMs to adequately extract multiple scene elements. In addition, Hierarchical Compositional diffusion utilizes Gaussian mask and filtering to achieve the refinement of bounding box regions and highlights objects through region enhancement for accurate and high-fidelity generation of complex scenes. Comprehensive experiments demonstrate that our MCCD significantly improves the performance of the baseline models in a training-free manner, which has a large advantage in complex scene generation. The code will be open-source on github",
    "checked": true,
    "id": "f8b50a476e3e1ed2499f853bcc3e16acf93bca85",
    "semantic_title": "mccd: multi-agent collaboration-based compositional diffusion for complex text-to-image generation",
    "citation_count": 3,
    "authors": [
      "Mingcheng Li",
      "Xiaolu Hou",
      "Ziyang Liu",
      "Dingkang Yang",
      "Ziyun Qian",
      "Jiawei Chen",
      "Jinjie Wei",
      "Yue Jiang",
      "Qingyao Xu",
      "Lihua Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Dual_Semantic_Guidance_for_Open_Vocabulary_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Dual Semantic Guidance for Open Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyang Wang",
      "Tingliang Feng",
      "Fan Lyu",
      "Fanhua Shang",
      "Wei Feng",
      "Liang Wan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Blum_CroCoDL_Cross-device_Collaborative_Dataset_for_Localization_CVPR_2025_paper.html": {
    "title": "CroCoDL: Cross-device Collaborative Dataset for Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hermann Blum",
      "Alessandro Mercurio",
      "Joshua O'Reilly",
      "Tim Engelbracht",
      "Mihai Dusmanu",
      "Marc Pollefeys",
      "Zuria Bauer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Q-Bench-Video_Benchmark_the_Video_Quality_Understanding_of_LMMs_CVPR_2025_paper.html": {
    "title": "Q-Bench-Video: Benchmark the Video Quality Understanding of LMMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zicheng Zhang",
      "Ziheng Jia",
      "Haoning Wu",
      "Chunyi Li",
      "Zijian Chen",
      "Yingjie Zhou",
      "Wei Sun",
      "Xiaohong Liu",
      "Xiongkuo Min",
      "Weisi Lin",
      "Guangtao Zhai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition_CVPR_2025_paper.html": {
    "title": "Glossy Object Reconstruction with Cost-effective Polarized Acquisition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bojian Wu",
      "Yifan Peng",
      "Ruizhen Hu",
      "Xiaowei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Generalizable_Object_Keypoint_Localization_from_Generative_Priors_CVPR_2025_paper.html": {
    "title": "Generalizable Object Keypoint Localization from Generative Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongkai Wang",
      "Jiang Duan",
      "Liangjian Wen",
      "Shiyu Xuan",
      "Hao Chen",
      "Shiliang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qin_CLIP_is_Almost_All_You_Need_Towards_Parameter-Efficient_Scene_Text_CVPR_2025_paper.html": {
    "title": "CLIP is Almost All You Need: Towards Parameter-Efficient Scene Text Retrieval without OCR",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xugong Qin",
      "Peng Zhang",
      "Jun Jie Ou Yang",
      "Gangyan Zeng",
      "Yubo Li",
      "Yuanyuan Wang",
      "Wanqian Zhang",
      "Pengwen Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Casarin_L-SWAG_Layer-Sample_Wise_Activation_with_Gradients_Information_for_Zero-Shot_NAS_CVPR_2025_paper.html": {
    "title": "L-SWAG: Layer-Sample Wise Activation with Gradients Information for Zero-Shot NAS on Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sofia Casarin",
      "Sergio Escalera",
      "Oswald Lanz"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Commonsense_Video_Question_Answering_through_Video-Grounded_Entailment_Tree_Reasoning_CVPR_2025_paper.html": {
    "title": "Commonsense Video Question Answering through Video-Grounded Entailment Tree Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huabin Liu",
      "Filip Ilievski",
      "Cees G. M. Snoek"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Frank_What_Makes_a_Good_Dataset_for_Knowledge_Distillation_CVPR_2025_paper.html": {
    "title": "What Makes a Good Dataset for Knowledge Distillation?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Logan Frank",
      "Jim Davis"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Lifelong_Knowledge_Editing_for_Vision_Language_Models_with_Low-Rank_Mixture-of-Experts_CVPR_2025_paper.html": {
    "title": "Lifelong Knowledge Editing for Vision Language Models with Low-Rank Mixture-of-Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qizhou Chen",
      "Chengyu Wang",
      "Dakan Wang",
      "Taolin Zhang",
      "Wangyue Li",
      "Xiaofeng He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gong_Rectification-specific_Supervision_and_Constrained_Estimator_for_Online_Stereo_Rectification_CVPR_2025_paper.html": {
    "title": "Rectification-specific Supervision and Constrained Estimator for Online Stereo Rectification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Gong",
      "Kim-Hui Yap",
      "Weide Liu",
      "Xulei Yang",
      "Jun Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Long_Shape_and_Texture_What_Influences_Reliable_Optical_Flow_Estimation_CVPR_2025_paper.html": {
    "title": "Shape and Texture: What Influences Reliable Optical Flow Estimation?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Libo Long",
      "Xiao Hu",
      "Jochen Lang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "PartGen: Part-level 3D Generation and Reconstruction with Multi-view Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghao Chen",
      "Roman Shapovalov",
      "Iro Laina",
      "Tom Monnier",
      "Jianyuan Wang",
      "David Novotny",
      "Andrea Vedaldi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_FedCALM_Conflict-aware_Layer-wise_Mitigation_for_Selective_Aggregation_in_Deeper_Personalized_CVPR_2025_paper.html": {
    "title": "FedCALM: Conflict-aware Layer-wise Mitigation for Selective Aggregation in Deeper Personalized Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zheng",
      "Zhigang Hu",
      "Liu Yang",
      "Meiguang Zheng",
      "Aikun Xu",
      "Boyu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jayasundara_SINR_Sparsity_Driven_Compressed_Implicit_Neural_Representations_CVPR_2025_paper.html": {
    "title": "SINR: Sparsity Driven Compressed Implicit Neural Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dhananjaya Jayasundara",
      "Sudarshan Rajagopalan",
      "Yasiru Ranasinghe",
      "Trac D. Tran",
      "Vishal M. Patel"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qu_CaricatureBooth_Data-Free_Interactive_Caricature_Generation_in_a_Photo_Booth_CVPR_2025_paper.html": {
    "title": "CaricatureBooth: Data-Free Interactive Caricature Generation in a Photo Booth",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyu Qu",
      "Yunqi Miao",
      "Zhensong Zhang",
      "Jifei Song",
      "Jiankang Deng",
      "Yi-Zhe Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_FlexGS_Train_Once_Deploy_Everywhere_with_Many-in-One_Flexible_3D_Gaussian_CVPR_2025_paper.html": {
    "title": "FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengyu Liu",
      "Yuehao Wang",
      "Chenxin Li",
      "Ruisi Cai",
      "Kevin Wang",
      "Wuyang Li",
      "Pavlo Molchanov",
      "Peihao Wang",
      "Zhangyang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Generalizing_Deepfake_Video_Detection_with_Plug-and-Play_Video-Level_Blending_and_Spatiotemporal_CVPR_2025_paper.html": {
    "title": "Generalizing Deepfake Video Detection with Plug-and-Play: Video-Level Blending and Spatiotemporal Adapter Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Yan",
      "Yandan Zhao",
      "Shen Chen",
      "Mingyi Guo",
      "Xinghe Fu",
      "Taiping Yao",
      "Shouhong Ding",
      "Yunsheng Wu",
      "Li Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_ManipTrans_Efficient_Dexterous_Bimanual_Manipulation_Transfer_via_Residual_Learning_CVPR_2025_paper.html": {
    "title": "ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kailin Li",
      "Puhao Li",
      "Tengyu Liu",
      "Yuyang Li",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Precise_Fast_and_Low-cost_Concept_Erasure_in_Value_Space__CVPR_2025_paper.html": {
    "title": "Precise, Fast, and Low-cost Concept Erasure in Value Space: Orthogonal Complement Matters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Wang",
      "Ouxiang Li",
      "Tingting Mu",
      "Yanbin Hao",
      "Kuien Liu",
      "Xiang Wang",
      "Xiangnan He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_HOIGen-1M_A_Large-scale_Dataset_for_Human-Object_Interaction_Video_Generation_CVPR_2025_paper.html": {
    "title": "HOIGen-1M: A Large-scale Dataset for Human-Object Interaction Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Liu",
      "Qi Liu",
      "Xinchen Liu",
      "Jie Li",
      "Yongdong Zhang",
      "Jiebo Luo",
      "Xiaodong He",
      "Wu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_T2ISafety_Benchmark_for_Assessing_Fairness_Toxicity_and_Privacy_in_Image_CVPR_2025_paper.html": {
    "title": "T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lijun Li",
      "Zhelun Shi",
      "Xuhao Hu",
      "Bowen Dong",
      "Yiran Qin",
      "Xihui Liu",
      "Lu Sheng",
      "Jing Shao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hahn_Order-One_Rolling_Shutter_Cameras_CVPR_2025_paper.html": {
    "title": "Order-One Rolling Shutter Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marvin Anas Hahn",
      "Kathlén Kohn",
      "Orlando Marigliano",
      "Tomas Pajdla"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Animate_and_Sound_an_Image_CVPR_2025_paper.html": {
    "title": "Animate and Sound an Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xihua Wang",
      "Ruihua Song",
      "Chongxuan Li",
      "Xin Cheng",
      "Boyuan Li",
      "Yihan Wu",
      "Yuyue Wang",
      "Hongteng Xu",
      "Yunfeng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Miao_Shining_Yourself_High-Fidelity_Ornaments_Virtual_Try-on_with_Diffusion_Model_CVPR_2025_paper.html": {
    "title": "Shining Yourself: High-Fidelity Ornaments Virtual Try-on with Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingmao Miao",
      "Zhanpeng Huang",
      "Rui Han",
      "Zibin Wang",
      "Chenhao Lin",
      "Chao Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zeng_Foveated_Instance_Segmentation_CVPR_2025_paper.html": {
    "title": "Foveated Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyi Zeng",
      "Wenxuan Liu",
      "Tianhua Xia",
      "Jinhui Chen",
      "Ziyun Li",
      "Sai Qian Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Binyamin_Make_It_Count_Text-to-Image_Generation_with_an_Accurate_Number_of_CVPR_2025_paper.html": {
    "title": "Make It Count: Text-to-Image Generation with an Accurate Number of Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lital Binyamin",
      "Yoad Tewel",
      "Hilit Segev",
      "Eran Hirsch",
      "Royi Rassin",
      "Gal Chechik"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Choe_Universal_Domain_Adaptation_for_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Universal Domain Adaptation for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seun-An Choe",
      "Keon-Hee Park",
      "Jinwoo Choi",
      "Gyeong-Moon Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Thirgood_HyperGS_Hyperspectral_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "HyperGS: Hyperspectral 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Thirgood",
      "Oscar Mendez",
      "Erin Ling",
      "Jon Storey",
      "Simon Hadfield"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Emphasizing_Discriminative_Features_for_Dataset_Distillation_in_Complex_Scenarios_CVPR_2025_paper.html": {
    "title": "Emphasizing Discriminative Features for Dataset Distillation in Complex Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Wang",
      "Zekai Li",
      "Zhi-Qi Cheng",
      "Samir Khaki",
      "Ahmad Sajedi",
      "Ramakrishna Vedantam",
      "Konstantinos N Plataniotis",
      "Alexander Hauptmann",
      "Yang You"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_LMO_Linear_Mamba_Operator_for_MRI_Reconstruction_CVPR_2025_paper.html": {
    "title": "LMO: Linear Mamba Operator for MRI Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Li",
      "Jiawei Jiang",
      "Jie Wu",
      "Kaihao Yu",
      "Jianwei Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_AnomalyNCD_Towards_Novel_Anomaly_Class_Discovery_in_Industrial_Scenarios_CVPR_2025_paper.html": {
    "title": "AnomalyNCD: Towards Novel Anomaly Class Discovery in Industrial Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziming Huang",
      "Xurui Li",
      "Haotian Liu",
      "Feng Xue",
      "Yuzhe Wang",
      "Yu Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Schmidt_Segment_This_Thing_Foveated_Tokenization_for_Efficient_Point-Prompted_Segmentation_CVPR_2025_paper.html": {
    "title": "Segment This Thing: Foveated Tokenization for Efficient Point-Prompted Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanner Schmidt",
      "Richard Newcombe"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Task-Specific_Gradient_Adaptation_for_Few-Shot_One-Class_Classification_CVPR_2025_paper.html": {
    "title": "Task-Specific Gradient Adaptation for Few-Shot One-Class Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunlong Li",
      "Xiabi Liu",
      "Liyuan Pan",
      "Yuchen Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Song_TraF-Align_Trajectory-aware_Feature_Alignment_for_Asynchronous_Multi-agent_Perception_CVPR_2025_paper.html": {
    "title": "TraF-Align: Trajectory-aware Feature Alignment for Asynchronous Multi-agent Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiying Song",
      "Lei Yang",
      "Fuxi Wen",
      "Jun Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Aiello_DreamCache_Finetuning-Free_Lightweight_Personalized_Image_Generation_via_Feature_Caching_CVPR_2025_paper.html": {
    "title": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation via Feature Caching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emanuele Aiello",
      "Umberto Michieli",
      "Diego Valsesia",
      "Mete Ozay",
      "Enrico Magli"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_3D_Gaussian_Inpainting_with_Depth-Guided_Cross-View_Consistency_CVPR_2025_paper.html": {
    "title": "3D Gaussian Inpainting with Depth-Guided Cross-View Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng-Yu Huang",
      "Zi-Ting Chou",
      "Yu-Chiang Frank Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads_CVPR_2025_paper.html": {
    "title": "Your Large Vision-Language Model Only Needs A Few Attention Heads For Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seil Kang",
      "Jinyeong Kim",
      "Junhyeok Kim",
      "Seong Jae Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_FlexUOD_The_Answer_to_Real-world_Unsupervised_Image_Outlier_Detection_CVPR_2025_paper.html": {
    "title": "FlexUOD: The Answer to Real-world Unsupervised Image Outlier Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhonghang Liu",
      "Kun Zhou",
      "Changshuo Wang",
      "Wen-Yan Lin",
      "Jiangbo Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mane_Ges3ViG__Incorporating_Pointing_Gestures_into_Language-Based_3D_Visual_Grounding_CVPR_2025_paper.html": {
    "title": "Ges3ViG : Incorporating Pointing Gestures into Language-Based 3D Visual Grounding for Embodied Reference Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atharv Mahesh Mane",
      "Dulanga Weerakoon",
      "Vigneshwaran Subbaraju",
      "Sougata Sen",
      "Sanjay E. Sarma",
      "Archan Misra"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shim_Focusing_on_Tracks_for_Online_Multi-Object_Tracking_CVPR_2025_paper.html": {
    "title": "Focusing on Tracks for Online Multi-Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyujin Shim",
      "Kangwook Ko",
      "Yujin Yang",
      "Changick Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hoffmann_Floxels_Fast_Unsupervised_Voxel_Based_Scene_Flow_Estimation_CVPR_2025_paper.html": {
    "title": "Floxels: Fast Unsupervised Voxel Based Scene Flow Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David T. Hoffmann",
      "Syed Haseeb Raza",
      "Hanqiu Jiang",
      "Denis Tananaev",
      "Steffen Klingenhoefer",
      "Martin Meinke"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_LiveCC_Learning_Video_LLM_with_Streaming_Speech_Transcription_at_Scale_CVPR_2025_paper.html": {
    "title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joya Chen",
      "Ziyun Zeng",
      "Yiqi Lin",
      "Wei Li",
      "Zejun Ma",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Identity-preserving_Distillation_Sampling_by_Fixed-Point_Iterator_CVPR_2025_paper.html": {
    "title": "Identity-preserving Distillation Sampling by Fixed-Point Iterator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SeonHwa Kim",
      "Jiwon Kim",
      "Soobin Park",
      "Donghoon Ahn",
      "Jiwon Kang",
      "Seungryong Kim",
      "Kyong Hwan Jin",
      "Eunju Cha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Long_Progressive_Focused_Transformer_for_Single_Image_Super-Resolution_CVPR_2025_paper.html": {
    "title": "Progressive Focused Transformer for Single Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Long",
      "Xingyu Zhou",
      "Leheng Zhang",
      "Shuhang Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ouali_VladVA_Discriminative_Fine-tuning_of_LVLMs_CVPR_2025_paper.html": {
    "title": "VladVA: Discriminative Fine-tuning of LVLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yassine Ouali",
      "Adrian Bulat",
      "Alexandros Xenos",
      "Anestis Zaganidis",
      "Ioannis Maniadis Metaxas",
      "Brais Martinez",
      "Georgios Tzimiropoulos"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with_CVPR_2025_paper.html": {
    "title": "FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sotiris Anagnostidis",
      "Gregor Bachmann",
      "Yeongmin Kim",
      "Jonas Kohler",
      "Markos Georgopoulos",
      "Artsiom Sanakoyeu",
      "Yuming Du",
      "Albert Pumarola",
      "Ali Thabet",
      "Edgar Schönfeld"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Potamias_WiLoR_End-to-end_3D_Hand_Localization_and_Reconstruction_in-the-wild_CVPR_2025_paper.html": {
    "title": "WiLoR: End-to-end 3D Hand Localization and Reconstruction in-the-wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rolandos Alexandros Potamias",
      "Jinglei Zhang",
      "Jiankang Deng",
      "Stefanos Zafeiriou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_HumanMM_Global_Human_Motion_Recovery_from_Multi-shot_Videos_CVPR_2025_paper.html": {
    "title": "HumanMM: Global Human Motion Recovery from Multi-shot Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhong Zhang",
      "Guanlin Wu",
      "Ling-Hao Chen",
      "Zhuokai Zhao",
      "Jing Lin",
      "Xiaoke Jiang",
      "Jiamin Wu",
      "Zhuoheng Li",
      "Hao Frank Yang",
      "Haoqian Wang",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kee_Removing_Reflections_from_RAW_Photos_CVPR_2025_paper.html": {
    "title": "Removing Reflections from RAW Photos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Kee",
      "Adam Pikielny",
      "Kevin Blackburn-Matzen",
      "Marc Levoy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Koleilat_BiomedCoOp_Learning_to_Prompt_for_Biomedical_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taha Koleilat",
      "Hojat Asgariandehkordi",
      "Hassan Rivaz",
      "Yiming Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_AMR-Transformer_Enabling_Efficient_Long-range_Interaction_for_Complex_Neural_Fluid_Simulation_CVPR_2025_paper.html": {
    "title": "AMR-Transformer: Enabling Efficient Long-range Interaction for Complex Neural Fluid Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyi Xu",
      "Jinfan Liu",
      "Kuangxu Chen",
      "Ye Chen",
      "Zhangli Hu",
      "Bingbing Ni"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chharia_MV-SSM_Multi-View_State_Space_Modeling_for_3D_Human_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "MV-SSM: Multi-View State Space Modeling for 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aviral Chharia",
      "Wenbo Gou",
      "Haoye Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nguyen_HyperGLM_HyperGraph_for_Video_Scene_Graph_Generation_and_Anticipation_CVPR_2025_paper.html": {
    "title": "HyperGLM: HyperGraph for Video Scene Graph Generation and Anticipation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trong-Thuan Nguyen",
      "Pha Nguyen",
      "Jackson Cothren",
      "Alper Yilmaz",
      "Khoa Luu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and_CVPR_2025_paper.html": {
    "title": "AnySat: One Earth Observation Model for Many Resolutions, Scales, and Modalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillaume Astruc",
      "Nicolas Gonthier",
      "Clément Mallet",
      "Loic Landrieu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_FSFM_A_Generalizable_Face_Security_Foundation_Model_via_Self-Supervised_Facial_CVPR_2025_paper.html": {
    "title": "FSFM: A Generalizable Face Security Foundation Model via Self-Supervised Facial Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaojian Wang",
      "Feng Lin",
      "Tong Wu",
      "Zhenguang Liu",
      "Zhongjie Ba",
      "Kui Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Niu_OVO-Bench_How_Far_is_Your_Video-LLMs_from_Real-World_Online_Video_CVPR_2025_paper.html": {
    "title": "OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junbo Niu",
      "Yifei Li",
      "Ziyang Miao",
      "Chunjiang Ge",
      "Yuanhang Zhou",
      "Qihao He",
      "Xiaoyi Dong",
      "Haodong Duan",
      "Shuangrui Ding",
      "Rui Qian",
      "Pan Zhang",
      "Yuhang Zang",
      "Yuhang Cao",
      "Conghui He",
      "Jiaqi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_AlignMamba_Enhancing_Multimodal_Mamba_with_Local_and_Global_Cross-modal_Alignment_CVPR_2025_paper.html": {
    "title": "AlignMamba: Enhancing Multimodal Mamba with Local and Global Cross-modal Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Li",
      "Yifei Xing",
      "Xiangyuan Lan",
      "Xin Li",
      "Haifeng Chen",
      "Dongmei Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Blurry-Edges_Photon-Limited_Depth_Estimation_from_Defocused_Boundaries_CVPR_2025_paper.html": {
    "title": "Blurry-Edges: Photon-Limited Depth Estimation from Defocused Boundaries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Xu",
      "Charles James Wagner",
      "Junjie Luo",
      "Qi Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_VideoComp_Advancing_Fine-Grained_Compositional_and_Temporal_Alignment_in_Video-Text_Models_CVPR_2025_paper.html": {
    "title": "VideoComp: Advancing Fine-Grained Compositional and Temporal Alignment in Video-Text Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dahun Kim",
      "AJ Piergiovanni",
      "Ganesh Mallya",
      "Anelia Angelova"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_One_Model_for_ALL_Low-Level_Task_Interaction_Is_a_Key_CVPR_2025_paper.html": {
    "title": "One Model for ALL: Low-Level Task Interaction Is a Key to Task-Agnostic Image Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunyang Cheng",
      "Tianyang Xu",
      "Zhenhua Feng",
      "Xiaojun Wu",
      "Zhangyong Tang",
      "Hui Li",
      "Zeyang Zhang",
      "Sara Atito",
      "Muhammad Awais",
      "Josef Kittler"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shao_MICAS_Multi-grained_In-Context_Adaptive_Sampling_for_3D_Point_Cloud_Processing_CVPR_2025_paper.html": {
    "title": "MICAS: Multi-grained In-Context Adaptive Sampling for 3D Point Cloud Processing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feifei Shao",
      "Ping Liu",
      "Zhao Wang",
      "Yawei Luo",
      "Hongwei Wang",
      "Jun Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zanella_Can_Text-to-Video_Generation_help_Video-Language_Alignment_CVPR_2025_paper.html": {
    "title": "Can Text-to-Video Generation help Video-Language Alignment?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Zanella",
      "Massimiliano Mancini",
      "Willi Menapace",
      "Sergey Tulyakov",
      "Yiming Wang",
      "Elisa Ricci"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xing_GoalFlow_Goal-Driven_Flow_Matching_for_Multimodal_Trajectories_Generation_in_End-to-End_CVPR_2025_paper.html": {
    "title": "GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories Generation in End-to-End Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zebin Xing",
      "Xingyu Zhang",
      "Yang Hu",
      "Bo Jiang",
      "Tong He",
      "Qian Zhang",
      "Xiaoxiao Long",
      "Wei Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_GuardSplat_Efficient_and_Robust_Watermarking_for_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "GuardSplat: Efficient and Robust Watermarking for 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Chen",
      "Guangcong Wang",
      "Jiahao Zhu",
      "Jianhuang Lai",
      "Xiaohua Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Weakly_Supervised_Contrastive_Adversarial_Training_for_Learning_Robust_Features_from_CVPR_2025_paper.html": {
    "title": "Weakly Supervised Contrastive Adversarial Training for Learning Robust Features from Semi-supervised Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lilin Zhang",
      "Chengpei Wu",
      "Ning Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yuan_From_Poses_to_Identity_Training-Free_Person_Re-Identification_via_Feature_Centralization_CVPR_2025_paper.html": {
    "title": "From Poses to Identity: Training-Free Person Re-Identification via Feature Centralization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Yuan",
      "Guiwei Zhang",
      "Changxiao Ma",
      "Tianyi Zhang",
      "Guanglin Niu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Edstedt_ColabSfM_Collaborative_Structure-from-Motion_by_Point_Cloud_Registration_CVPR_2025_paper.html": {
    "title": "ColabSfM: Collaborative Structure-from-Motion by Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johan Edstedt",
      "André Mateus",
      "Alberto Jaenal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Parikh_RoadSocial_A_Diverse_VideoQA_Dataset_and_Benchmark_for_Road_Event_CVPR_2025_paper.html": {
    "title": "RoadSocial: A Diverse VideoQA Dataset and Benchmark for Road Event Understanding from Social Video Narratives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chirag Parikh",
      "Deepti Rawat",
      "Rakshitha R. T.",
      "Tathagata Ghosh",
      "Ravi Kiran Sarvadevabhatla"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following_CVPR_2025_paper.html": {
    "title": "MangaNinja: Line Art Colorization with Precise Reference Following",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiheng Liu",
      "Ka Leong Cheng",
      "Xi Chen",
      "Jie Xiao",
      "Hao Ouyang",
      "Kai Zhu",
      "Yu Liu",
      "Yujun Shen",
      "Qifeng Chen",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Curreli_Nonisotropic_Gaussian_Diffusion_for_Realistic_3D_Human_Motion_Prediction_CVPR_2025_paper.html": {
    "title": "Nonisotropic Gaussian Diffusion for Realistic 3D Human Motion Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cecilia Curreli",
      "Dominik Muhle",
      "Abhishek Saroha",
      "Zhenzhang Ye",
      "Riccardo Marin",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Is_Your_World_Simulator_a_Good_Story_Presenter_A_Consecutive_CVPR_2025_paper.html": {
    "title": "Is Your World Simulator a Good Story Presenter? A Consecutive Events-Based Benchmark for Future Long Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiping Wang",
      "Xuehai He",
      "Kuan Wang",
      "Luyao Ma",
      "Jianwei Yang",
      "Shuohang Wang",
      "Simon Shaolei Du",
      "Yelong Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_LookCloser_Frequency-aware_Radiance_Field_for_Tiny-Detail_Scene_CVPR_2025_paper.html": {
    "title": "LookCloser: Frequency-aware Radiance Field for Tiny-Detail Scene",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyu Zhang",
      "Weihong Pan",
      "Chong Bao",
      "Xiyu Zhang",
      "Xiaojun Xiang",
      "Hanqing Jiang",
      "Hujun Bao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cseke_PICO_Reconstructing_3D_People_In_Contact_with_Objects_CVPR_2025_paper.html": {
    "title": "PICO: Reconstructing 3D People In Contact with Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alpár Cseke",
      "Shashank Tripathi",
      "Sai Kumar Dwivedi",
      "Arjun S. Lakshmipathy",
      "Agniv Chatterjee",
      "Michael J. Black",
      "Dimitrios Tzionas"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World_CVPR_2025_paper.html": {
    "title": "Convex Relaxation for Robust Vanishing Point Estimation in Manhattan World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bangyan Liao",
      "Zhenjun Zhao",
      "Haoang Li",
      "Yi Zhou",
      "Yingping Zeng",
      "Hao Li",
      "Peidong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Linguistics-aware_Masked_Image_Modeling_for_Self-supervised_Scene_Text_Recognition_CVPR_2025_paper.html": {
    "title": "Linguistics-aware Masked Image Modeling for Self-supervised Scene Text Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Zhang",
      "Chang Liu",
      "Jin Wei",
      "Xiaomeng Yang",
      "Yu Zhou",
      "Can Ma",
      "Xiangyang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_FruitNinja_3D_Object_Interior_Texture_Generation_with_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "FruitNinja: 3D Object Interior Texture Generation with Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangyu Wu",
      "Yuhao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Scaling_up_Image_Segmentation_across_Data_and_Tasks_CVPR_2025_paper.html": {
    "title": "Scaling up Image Segmentation across Data and Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pei Wang",
      "Zhaowei Cai",
      "Hao Yang",
      "Ashwin Swaminathan",
      "R. Manmatha",
      "Stefano Soatto"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Take_the_Bull_by_the_Horns_Learning_to_Segment_Hard_CVPR_2025_paper.html": {
    "title": "Take the Bull by the Horns: Learning to Segment Hard Samples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Guo",
      "Jingyu Kong",
      "Yu Wang",
      "Yuping Duan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_MIMO_A_Medical_Vision_Language_Model_with_Visual_Referring_Multimodal_CVPR_2025_paper.html": {
    "title": "MIMO: A Medical Vision Language Model with Visual Referring Multimodal Input and Pixel Grounding Multimodal Output",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanyuan Chen",
      "Dexuan Xu",
      "Yu Huang",
      "Songkun Zhan",
      "Hanpin Wang",
      "Dongxue Chen",
      "Xueping Wang",
      "Meikang Qiu",
      "Hang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kayabasi_Bias_for_Action_Video_Implicit_Neural_Representations_with_Bias_Modulation_CVPR_2025_paper.html": {
    "title": "Bias for Action: Video Implicit Neural Representations with Bias Modulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alper Kayabasi",
      "Anil Kumar Vadathya",
      "Guha Balakrishnan",
      "Vishwanath Saragadam"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Bridging_Past_and_Future_End-to-End_Autonomous_Driving_with_Historical_Prediction_CVPR_2025_paper.html": {
    "title": "Bridging Past and Future: End-to-End Autonomous Driving with Historical Prediction and Planning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bozhou Zhang",
      "Nan Song",
      "Xin Jin",
      "Li Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_Blood_Flow_Speed_Estimation_with_Optical_Coherence_Tomography_Angiography_Images_CVPR_2025_paper.html": {
    "title": "Blood Flow Speed Estimation with Optical Coherence Tomography Angiography Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wensheng Cheng",
      "Zhenghong Li",
      "Jiaxiang Ren",
      "Hyomin Jeong",
      "Congwu Du",
      "Yingtian Pan",
      "Haibin Ling"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_DreamTrack_Dreaming_the_Future_for_Multimodal_Visual_Object_Tracking_CVPR_2025_paper.html": {
    "title": "DreamTrack: Dreaming the Future for Multimodal Visual Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingzhe Guo",
      "Weiping Tan",
      "Wenyu Ran",
      "Liping Jing",
      "Zhipeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_OmniStyle_Filtering_High_Quality_Style_Transfer_Data_at_Scale_CVPR_2025_paper.html": {
    "title": "OmniStyle: Filtering High Quality Style Transfer Data at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Wang",
      "Ruiqi Liu",
      "Jiang Lin",
      "Fei Liu",
      "Zili Yi",
      "Yilin Wang",
      "Rui Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jagpal_EIDT-V_Exploiting_Intersections_in_Diffusion_Trajectories_for_Model-Agnostic_Zero-Shot_Training-Free_CVPR_2025_paper.html": {
    "title": "EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diljeet Jagpal",
      "Xi Chen",
      "Vinay P. Namboodiri"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators_CVPR_2025_paper.html": {
    "title": "Cross-View Completion Models are Zero-shot Correspondence Estimators",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Honggyu An",
      "Jin Hyeon Kim",
      "Seonghoon Park",
      "Jaewoo Jung",
      "Jisang Han",
      "Sunghwan Hong",
      "Seungryong Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Multi-party_Collaborative_Attention_Control_for_Image_Customization_CVPR_2025_paper.html": {
    "title": "Multi-party Collaborative Attention Control for Image Customization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Yang",
      "Chuanguang Yang",
      "Qiuli Wang",
      "Zhulin An",
      "Weilun Feng",
      "Libo Huang",
      "Yongjun Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Reproducible_Vision-Language_Models_Meet_Concepts_Out_of_Pre-Training_CVPR_2025_paper.html": {
    "title": "Reproducible Vision-Language Models Meet Concepts Out of Pre-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziliang Chen",
      "Xin Huang",
      "Xiaoxuan Fan",
      "Keze Wang",
      "Yuyu Zhou",
      "Quanlong Guan",
      "Liang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yariv_Through-The-Mask_Mask-based_Motion_Trajectories_for_Image-to-Video_Generation_CVPR_2025_paper.html": {
    "title": "Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guy Yariv",
      "Yuval Kirstain",
      "Amit Zohar",
      "Shelly Sheynin",
      "Yaniv Taigman",
      "Yossi Adi",
      "Sagie Benaim",
      "Adam Polyak"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MAGE__Single_Image_to_Material-Aware_3D_via_the_Multi-View_CVPR_2025_paper.html": {
    "title": "MAGE : Single Image to Material-Aware 3D via the Multi-View G-Buffer Estimation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyuan Wang",
      "Zhenwei Wang",
      "Xiaoxiao Long",
      "Cheng Lin",
      "Gerhard Hancke",
      "Rynson W.H. Lau"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tai_Segment_Anything_Even_Occluded_CVPR_2025_paper.html": {
    "title": "Segment Anything, Even Occluded",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei-En Tai",
      "Yu-Lin Shih",
      "Cheng Sun",
      "Yu-Chiang Frank Wang",
      "Hwann-Tzong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View_CVPR_2025_paper.html": {
    "title": "HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Prithviraj Banerjee",
      "Sindi Shkodrani",
      "Pierre Moulon",
      "Shreyas Hampali",
      "Shangchen Han",
      "Fan Zhang",
      "Linguang Zhang",
      "Jade Fountain",
      "Edward Miller",
      "Selen Basol",
      "Richard Newcombe",
      "Robert Wang",
      "Jakob Julian Engel",
      "Tomas Hodan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_DELT_A_Simple_Diversity-driven_EarlyLate_Training_for_Dataset_Distillation_CVPR_2025_paper.html": {
    "title": "DELT: A Simple Diversity-driven EarlyLate Training for Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqiang Shen",
      "Ammar Sherif",
      "Zeyuan Yin",
      "Shitong Shao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_MESC-3DMining_Effective_Semantic_Cues_for_3D_Reconstruction_from_a_Single_CVPR_2025_paper.html": {
    "title": "MESC-3D:Mining Effective Semantic Cues for 3D Reconstruction from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaoming Li",
      "Qing Cai",
      "Songqi Kong",
      "Runqing Tan",
      "Heng Tong",
      "Shiji Qiu",
      "Yongguo Jiang",
      "Zhi Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ji_RoboBrain_A_Unified_Brain_Model_for_Robotic_Manipulation_from_Abstract_CVPR_2025_paper.html": {
    "title": "RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuheng Ji",
      "Huajie Tan",
      "Jiayu Shi",
      "Xiaoshuai Hao",
      "Yuan Zhang",
      "Hengyuan Zhang",
      "Pengwei Wang",
      "Mengdi Zhao",
      "Yao Mu",
      "Pengju An",
      "Xinda Xue",
      "Qinghang Su",
      "Huaihai Lyu",
      "Xiaolong Zheng",
      "Jiaming Liu",
      "Zhongyuan Wang",
      "Shanghang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide_CVPR_2025_paper.html": {
    "title": "Advancing Multiple Instance Learning with Continual Learning for Whole Slide Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianrui Li",
      "Yufei Cui",
      "Jun Li",
      "Antoni B. Chan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Beyond_Image_Classification_A_Video_Benchmark_and_Dual-Branch_Hybrid_Discrimination_CVPR_2025_paper.html": {
    "title": "Beyond Image Classification: A Video Benchmark and Dual-Branch Hybrid Discrimination Framework for Compositional Zero-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyao Jiang",
      "Haodong Jing",
      "Yongqiang Ma",
      "Nanning Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_ABBSPO_Adaptive_Bounding_Box_Scaling_and_Symmetric_Prior_based_Orientation_CVPR_2025_paper.html": {
    "title": "ABBSPO: Adaptive Bounding Box Scaling and Symmetric Prior based Orientation Prediction for Detecting Aerial Image Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woojin Lee",
      "Hyugjae Chang",
      "Jaeho Moon",
      "Jaehyup Lee",
      "Munchurl Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any_CVPR_2025_paper.html": {
    "title": "Decoupled Distillation to Erase: A General Unlearning Method for Any Class-centric Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Zhou",
      "Dian Zheng",
      "Qijie Mo",
      "Renjie Lu",
      "Kun-Yu Lin",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu-Hang_TAET_Two-Stage_Adversarial_Equalization_Training_on_Long-Tailed__Distributions_CVPR_2025_paper.html": {
    "title": "TAET: Two-Stage Adversarial Equalization Training on Long-Tailed Distributions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wang Yu-Hang",
      "Junkang Guo",
      "Aolei Liu",
      "Kaihao Wang",
      "Zaitong Wu",
      "Zhenyu Liu",
      "Wenfei Yin",
      "Jian Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xue_Few-shot_Personalized_Scanpath_Prediction_CVPR_2025_paper.html": {
    "title": "Few-shot Personalized Scanpath Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruoyu Xue",
      "Jingyi Xu",
      "Sounak Mondal",
      "Hieu Le",
      "Greg Zelinsky",
      "Minh Hoai",
      "Dimitris Samaras"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kang_Do_Your_Best_and_Get_Enough_Rest_for_Continual_Learning_CVPR_2025_paper.html": {
    "title": "Do Your Best and Get Enough Rest for Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hankyul Kang",
      "Gregor Seifer",
      "Donghyun Lee",
      "Jongbin Ryu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Enhancing_Few-Shot_Class-Incremental_Learning_via_Training-Free_Bi-Level_Modality_Calibration_CVPR_2025_paper.html": {
    "title": "Enhancing Few-Shot Class-Incremental Learning via Training-Free Bi-Level Modality Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Chen",
      "Tianyu Ding",
      "Lei Wang",
      "Jing Huo",
      "Yang Gao",
      "Wenbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction_CVPR_2025_paper.html": {
    "title": "MUSt3R: Multi-view Network for Stereo 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yohann Cabon",
      "Lucas Stoffl",
      "Leonid Antsfeld",
      "Gabriela Csurka",
      "Boris Chidlovskii",
      "Jerome Revaud",
      "Vincent Leroy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Hybrid-Level_Instruction_Injection_for_Video_Token_Compression_in_Multi-modal_Large_CVPR_2025_paper.html": {
    "title": "Hybrid-Level Instruction Injection for Video Token Compression in Multi-modal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihang Liu",
      "Chen-Wei Xie",
      "Pandeng Li",
      "Liming Zhao",
      "Longxiang Tang",
      "Yun Zheng",
      "Chuanbin Liu",
      "Hongtao Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Mamba4D_Efficient_4D_Point_Cloud_Video_Understanding_with_Disentangled_Spatial-Temporal_CVPR_2025_paper.html": {
    "title": "Mamba4D: Efficient 4D Point Cloud Video Understanding with Disentangled Spatial-Temporal State Space Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiuming Liu",
      "Jinru Han",
      "Lihao Liu",
      "Angelica I. Aviles-Rivero",
      "Chaokang Jiang",
      "Zhe Liu",
      "Hesheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision_CVPR_2025_paper.html": {
    "title": "Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Zhang",
      "Robby T. Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Vision-Guided_Action_Enhancing_3D_Human_Motion_Prediction_with_Gaze-informed_Affordance_CVPR_2025_paper.html": {
    "title": "Vision-Guided Action: Enhancing 3D Human Motion Prediction with Gaze-informed Affordance in 3D Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting Yu",
      "Yi Lin",
      "Jun Yu",
      "Zhenyu Lou",
      "Qiongjie Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_LOGICZSL_Exploring_Logic-induced_Representation_for_Compositional_Zero-shot_Learning_CVPR_2025_paper.html": {
    "title": "LOGICZSL: Exploring Logic-induced Representation for Compositional Zero-shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Wu",
      "Xiankai Lu",
      "Hao Hu",
      "Yongqin Xian",
      "Jianbing Shen",
      "Wenguan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zeng_ChainHOI_Joint-based_Kinematic_Chain_Modeling_for_Human-Object_Interaction_Generation_CVPR_2025_paper.html": {
    "title": "ChainHOI: Joint-based Kinematic Chain Modeling for Human-Object Interaction Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ling-An Zeng",
      "Guohong Huang",
      "Yi-Lin Wei",
      "Shengbo Gu",
      "Yu-Ming Tang",
      "Jingke Meng",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pitawela_CLOC_Contrastive_Learning_for_Ordinal_Classification_with_Multi-Margin_N-pair_Loss_CVPR_2025_paper.html": {
    "title": "CLOC: Contrastive Learning for Ordinal Classification with Multi-Margin N-pair Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dileepa Pitawela",
      "Gustavo Carneiro",
      "Hsiang-Ting Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_Universal_Actions_for_Enhanced_Embodied_Foundation_Models_CVPR_2025_paper.html": {
    "title": "Universal Actions for Enhanced Embodied Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinliang Zheng",
      "Jianxiong Li",
      "Dongxiu Liu",
      "Yinan Zheng",
      "Zhihao Wang",
      "Zhonghong Ou",
      "Yu Liu",
      "Jingjing Liu",
      "Ya-Qin Zhang",
      "Xianyuan Zhan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_ObjectMover_Generative_Object_Movement_with_Video_Prior_CVPR_2025_paper.html": {
    "title": "ObjectMover: Generative Object Movement with Video Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Yu",
      "Tianyu Wang",
      "Soo Ye Kim",
      "Paul Guerrero",
      "Xi Chen",
      "Qing Liu",
      "Zhe Lin",
      "Xiaojuan Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_FaithDiff_Unleashing_Diffusion_Priors_for_Faithful_Image_Super-resolution_CVPR_2025_paper.html": {
    "title": "FaithDiff: Unleashing Diffusion Priors for Faithful Image Super-resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyang Chen",
      "Jinshan Pan",
      "Jiangxin Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling_CVPR_2025_paper.html": {
    "title": "MLLM-as-a-Judge for Image Safety without Human Labeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenting Wang",
      "Shuming Hu",
      "Shiyu Zhao",
      "Xiaowen Lin",
      "Felix Juefei-Xu",
      "Zhuowei Li",
      "Ligong Han",
      "Harihar Subramanyam",
      "Li Chen",
      "Jianfa Chen",
      "Nan Jiang",
      "Lingjuan Lyu",
      "Shiqing Ma",
      "Dimitris N. Metaxas",
      "Ankit Jain"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bodrito_A_New_Statistical_Model_of_Star_Speckles_for_Learning_to_CVPR_2025_paper.html": {
    "title": "A New Statistical Model of Star Speckles for Learning to Detect and Characterize Exoplanets in Direct Imaging Observations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Théo Bodrito",
      "Olivier Flasseur",
      "Julien Mairal",
      "Jean Ponce",
      "Maud Langlois",
      "Anne-Marie Lagrange"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_Scene-agnostic_Pose_Regression_for_Visual_Localization_CVPR_2025_paper.html": {
    "title": "Scene-agnostic Pose Regression for Visual Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junwei Zheng",
      "Ruiping Liu",
      "Yufan Chen",
      "Zhenfang Chen",
      "Kailun Yang",
      "Jiaming Zhang",
      "Rainer Stiefelhagen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM_CVPR_2025_paper.html": {
    "title": "Learning to Filter Outlier Edges in Global SfM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicole Damblon",
      "Marc Pollefeys",
      "Daniel Barath"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pei_Divide_and_Conquer_Heterogeneous_Noise_Integration_for_Diffusion-based_Adversarial_Purification_CVPR_2025_paper.html": {
    "title": "Divide and Conquer: Heterogeneous Noise Integration for Diffusion-based Adversarial Purification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaozheng Pei",
      "Shaojie Lyu",
      "Gong Chen",
      "Ke Ma",
      "Qianqian Xu",
      "Yingfei Sun",
      "Qingming Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_SEC-PromptSEmantic_Complementary_Prompting_for_Few-Shot_Class-Incremental_Learning_CVPR_2025_paper.html": {
    "title": "SEC-Prompt:SEmantic Complementary Prompting for Few-Shot Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Liu",
      "Meng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_LiMoE_Mixture_of_LiDAR_Representation_Learners_from_Automotive_Scenes_CVPR_2025_paper.html": {
    "title": "LiMoE: Mixture of LiDAR Representation Learners from Automotive Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Xu",
      "Lingdong Kong",
      "Hui Shuai",
      "Liang Pan",
      "Ziwei Liu",
      "Qingshan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_CoT-VLA_Visual_Chain-of-Thought_Reasoning_for_Vision-Language-Action_Models_CVPR_2025_paper.html": {
    "title": "CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingqing Zhao",
      "Yao Lu",
      "Moo Jin Kim",
      "Zipeng Fu",
      "Zhuoyang Zhang",
      "Yecheng Wu",
      "Zhaoshuo Li",
      "Qianli Ma",
      "Song Han",
      "Chelsea Finn",
      "Ankur Handa",
      "Tsung-Yi Lin",
      "Gordon Wetzstein",
      "Ming-Yu Liu",
      "Donglai Xiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_WAVE_Weight_Templates_for_Adaptive_Initialization_of_Variable-sized_Models_CVPR_2025_paper.html": {
    "title": "WAVE: Weight Templates for Adaptive Initialization of Variable-sized Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fu Feng",
      "Yucheng Xie",
      "Jing Wang",
      "Xin Geng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cui_Forensics_Adapter_Adapting_CLIP_for_Generalizable_Face_Forgery_Detection_CVPR_2025_paper.html": {
    "title": "Forensics Adapter: Adapting CLIP for Generalizable Face Forgery Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinjie Cui",
      "Yuezun Li",
      "Ao Luo",
      "Jiaran Zhou",
      "Junyu Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning_CVPR_2025_paper.html": {
    "title": "KAC: Kolmogorov-Arnold Classifier for Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusong Hu",
      "Zichen Liang",
      "Fei Yang",
      "Qibin Hou",
      "Xialei Liu",
      "Ming-Ming Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_PI-HMR_Towards_Robust_In-bed_Temporal_Human_Shape_Reconstruction_with_Contact_CVPR_2025_paper.html": {
    "title": "PI-HMR: Towards Robust In-bed Temporal Human Shape Reconstruction with Contact Pressure Sensing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyu Wu",
      "Yufan Xiong",
      "Mengting Niu",
      "Fangting Xie",
      "Quan Wan",
      "Qijun Ying",
      "Boyan Liu",
      "Xiaohui Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_BOOTPLACE_Bootstrapped_Object_Placement_with_Detection_Transformers_CVPR_2025_paper.html": {
    "title": "BOOTPLACE: Bootstrapped Object Placement with Detection Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Zhou",
      "Xinxin Zuo",
      "Rui Ma",
      "Li Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation_CVPR_2025_paper.html": {
    "title": "CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxing Long",
      "Jiyao Zhang",
      "Mingjie Pan",
      "Tianshu Wu",
      "Taewhan Kim",
      "Hao Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_CXPMRG-Bench_Pre-training_and_Benchmarking_for_X-ray_Medical_Report_Generation_on_CVPR_2025_paper.html": {
    "title": "CXPMRG-Bench: Pre-training and Benchmarking for X-ray Medical Report Generation on CheXpert Plus Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Wang",
      "Fuling Wang",
      "Yuehang Li",
      "Qingchuan Ma",
      "Shiao Wang",
      "Bo Jiang",
      "Jin Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dang_FASTer_Focal_token_Acquiring-and-Scaling_Transformer_for_Long-term_3D_Objection_Detection_CVPR_2025_paper.html": {
    "title": "FASTer: Focal token Acquiring-and-Scaling Transformer for Long-term 3D Objection Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxu Dang",
      "ZaiPeng Duan",
      "Pei An",
      "Xinmin Zhang",
      "Xuzhong Hu",
      "Jie Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_SEEN-DA_SEmantic_ENtropy_guided_Domain-aware_Attention_for_Domain_Adaptive_Object_CVPR_2025_paper.html": {
    "title": "SEEN-DA: SEmantic ENtropy guided Domain-aware Attention for Domain Adaptive Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haochen Li",
      "Rui Zhang",
      "Hantao Yao",
      "Xin Zhang",
      "Yifan Hao",
      "Xinkai Song",
      "Shaohui Peng",
      "Yongwei Zhao",
      "Chen Zhao",
      "Yanjun Wu",
      "Ling Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Event-Equalized_Dense_Video_Captioning_CVPR_2025_paper.html": {
    "title": "Event-Equalized Dense Video Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangyi Wu",
      "Pengna Li",
      "Jingwen Fu",
      "Yizhe Li",
      "Yang Wu",
      "Yuhan Liu",
      "Jinjun Wang",
      "Sanping Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ha_Geometry-guided_Online_3D_Video_Synthesis_with_Multi-View_Temporal_Consistency_CVPR_2025_paper.html": {
    "title": "Geometry-guided Online 3D Video Synthesis with Multi-View Temporal Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunho Ha",
      "Lei Xiao",
      "Christian Richardt",
      "Thu Nguyen-Phuoc",
      "Changil Kim",
      "Min H. Kim",
      "Douglas Lanman",
      "Numair Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_EDCFlow_Exploring_Temporally_Dense_Difference_Maps_for_Event-based_Optical_Flow_CVPR_2025_paper.html": {
    "title": "EDCFlow: Exploring Temporally Dense Difference Maps for Event-based Optical Flow Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daikun Liu",
      "Lei Cheng",
      "Teng Wang",
      "Changyin Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Point2RBox-v2_Rethinking_Point-supervised_Oriented_Object_Detection_with_Spatial_Layout_Among_CVPR_2025_paper.html": {
    "title": "Point2RBox-v2: Rethinking Point-supervised Oriented Object Detection with Spatial Layout Among Instances",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Yu",
      "Botao Ren",
      "Peiyuan Zhang",
      "Mingxin Liu",
      "Junwei Luo",
      "Shaofeng Zhang",
      "Feipeng Da",
      "Junchi Yan",
      "Xue Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions_CVPR_2025_paper.html": {
    "title": "LibraGrad: Balancing Gradient Flow for Universally Better Vision Transformer Attributions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Faridoun Mehri",
      "Mahdieh Soleymani Baghshah",
      "Mohammad Taher Pilehvar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Blind_Bitstream-corrupted_Video_Recovery_via_Metadata-guided_Diffusion_Model_CVPR_2025_paper.html": {
    "title": "Blind Bitstream-corrupted Video Recovery via Metadata-guided Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyun Wang",
      "Hu Zhang",
      "Xin Shen",
      "Dadong Wang",
      "Xin Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and_CVPR_2025_paper.html": {
    "title": "Mind the Trojan Horse: Image Prompt Adapter Enabling Scalable and Deceptive Jailbreaking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junxi Chen",
      "Junhao Dong",
      "Xiaohua Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jang_Lost_in_Translation_Found_in_Context_Sign_Language_Translation_with_CVPR_2025_paper.html": {
    "title": "Lost in Translation, Found in Context: Sign Language Translation with Contextual Cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngjoon Jang",
      "Haran Raajesh",
      "Liliane Momeni",
      "Gül Varol",
      "Andrew Zisserman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_CoCoGaussian_Leveraging_Circle_of_Confusion_for_Gaussian_Splatting_from_Defocused_CVPR_2025_paper.html": {
    "title": "CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from Defocused Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungho Lee",
      "Suhwan Cho",
      "Taeoh Kim",
      "Ho-Deok Jang",
      "Minhyeok Lee",
      "Geonho Cha",
      "Dongyoon Wee",
      "Dogyoon Lee",
      "Sangyoun Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pan_Semantic_and_Sequential_Alignment_for_Referring_Video_Object_Segmentation_CVPR_2025_paper.html": {
    "title": "Semantic and Sequential Alignment for Referring Video Object Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feiyu Pan",
      "Hao Fang",
      "Fangkai Li",
      "Yanyu Xu",
      "Yawei Li",
      "Luca Benini",
      "Xiankai Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Synchronized_Video-to-Audio_Generation_via_Mel_Quantization-Continuum_Decomposition_CVPR_2025_paper.html": {
    "title": "Synchronized Video-to-Audio Generation via Mel Quantization-Continuum Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juncheng Wang",
      "Chao Xu",
      "Cheng Yu",
      "Lei Shang",
      "Zhe Hu",
      "Shujun Wang",
      "Liefeng Bo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Continual_SFT_Matches_Multimodal_RLHF_with_Negative_Supervision_CVPR_2025_paper.html": {
    "title": "Continual SFT Matches Multimodal RLHF with Negative Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Zhu",
      "Yu Wang",
      "Yanpeng Sun",
      "Qiang Chen",
      "Jiangjiang Liu",
      "Gang Zhang",
      "Jingdong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Semantic-guided_Cross-Modal_Prompt_Learning_for_Skeleton-based_Zero-shot_Action_Recognition_CVPR_2025_paper.html": {
    "title": "Semantic-guided Cross-Modal Prompt Learning for Skeleton-based Zero-shot Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anqi Zhu",
      "Jingmin Zhu",
      "James Bailey",
      "Mingming Gong",
      "Qiuhong Ke"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_FATE_Full-head_Gaussian_Avatar_with_Textural_Editing_from_Monocular_Video_CVPR_2025_paper.html": {
    "title": "FATE: Full-head Gaussian Avatar with Textural Editing from Monocular Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Zhang",
      "Zijian Wu",
      "Zhiyang Liang",
      "Yicheng Gong",
      "Dongfang Hu",
      "Yao Yao",
      "Xun Cao",
      "Hao Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jia_ChatGen_Automatic_Text-to-Image_Generation_From_FreeStyle_Chatting_CVPR_2025_paper.html": {
    "title": "ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengyou Jia",
      "Changliang Xia",
      "Zhuohang Dang",
      "Weijia Wu",
      "Hangwei Qian",
      "Minnan Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hassan_GEM_A_Generalizable_Ego-Vision_Multimodal_World_Model_for_Fine-Grained_Ego-Motion_CVPR_2025_paper.html": {
    "title": "GEM: A Generalizable Ego-Vision Multimodal World Model for Fine-Grained Ego-Motion, Object Dynamics, and Scene Composition Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mariam Hassan",
      "Sebastian Stapf",
      "Ahmad Rahimi",
      "Pedro M B Rezende",
      "Yasaman Haghighi",
      "David Brüggemann",
      "Isinsu Katircioglu",
      "Lin Zhang",
      "Xiaoran Chen",
      "Suman Saha",
      "Marco Cannici",
      "Elie Aljalbout",
      "Botao Ye",
      "Xi Wang",
      "Aram Davtyan",
      "Mathieu Salzmann",
      "Davide Scaramuzza",
      "Marc Pollefeys",
      "Paolo Favaro",
      "Alexandre Alahi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing_CVPR_2025_paper.html": {
    "title": "VEU-Bench: Towards Comprehensive Understanding of Video Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bozheng Li",
      "Yongliang Wu",
      "Yi Lu",
      "Jiashuo Yu",
      "Licheng Tang",
      "Jiawang Cao",
      "Wenqing Zhu",
      "Yuyang Sun",
      "Jay Wu",
      "Wenbo Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Decouple_Distortion_from_Perception_Region_Adaptive_Diffusion_for_Extreme-low_Bitrate_CVPR_2025_paper.html": {
    "title": "Decouple Distortion from Perception: Region Adaptive Diffusion for Extreme-low Bitrate Perception Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinchang Xu",
      "Shaokang Wang",
      "Jintao Chen",
      "Zhe Li",
      "Peidong Jia",
      "Fei Zhao",
      "Guoqing Xiang",
      "Zhijian Hao",
      "Shanghang Zhang",
      "Xiaodong Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nguyen_YoChameleon_Personalized_Vision_and_Language_Generation_CVPR_2025_paper.html": {
    "title": "Yo'Chameleon: Personalized Vision and Language Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thao Nguyen",
      "Krishna Kumar Singh",
      "Jing Shi",
      "Trung Bui",
      "Yong Jae Lee",
      "Yuheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Du_PatchVSR_Breaking_Video_Diffusion_Resolution_Limits_with_Patch-wise_Video_Super-Resolution_CVPR_2025_paper.html": {
    "title": "PatchVSR: Breaking Video Diffusion Resolution Limits with Patch-wise Video Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shian Du",
      "Menghan Xia",
      "Chang Liu",
      "Xintao Wang",
      "Jing Wang",
      "Pengfei Wan",
      "Di Zhang",
      "Xiangyang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dalva_FluxSpace_Disentangled_Semantic_Editing_in_Rectified_Flow_Models_CVPR_2025_paper.html": {
    "title": "FluxSpace: Disentangled Semantic Editing in Rectified Flow Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusuf Dalva",
      "Kavana Venkatesh",
      "Pinar Yanardag"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation_CVPR_2025_paper.html": {
    "title": "Scene-Centric Unsupervised Panoptic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oliver Hahn",
      "Christoph Reich",
      "Nikita Araslanov",
      "Daniel Cremers",
      "Christian Rupprecht",
      "Stefan Roth"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Touch2Shape_Touch-Conditioned_3D_Diffusion_for_Shape_Exploration_and_Reconstruction_CVPR_2025_paper.html": {
    "title": "Touch2Shape: Touch-Conditioned 3D Diffusion for Shape Exploration and Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanbo Wang",
      "Zhaoxuan Zhang",
      "Jiajin Qiu",
      "Dilong Sun",
      "Zhengyu Meng",
      "Xiaopeng Wei",
      "Xin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_VITED_Video_Temporal_Evidence_Distillation_CVPR_2025_paper.html": {
    "title": "VITED: Video Temporal Evidence Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Lu",
      "Yale Song",
      "William Wang",
      "Lorenzo Torresani",
      "Tushar Nagarajan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Adversarial_Domain_Prompt_Tuning_and_Generation_for_Single_Domain_Generalization_CVPR_2025_paper.html": {
    "title": "Adversarial Domain Prompt Tuning and Generation for Single Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhipeng Xu",
      "De Cheng",
      "Xinyang Jiang",
      "Nannan Wang",
      "Dongsheng Li",
      "Xinbo Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Garcia_Learning_Physics_From_Video_Unsupervised_Physical_Parameter_Estimation_for_Continuous_CVPR_2025_paper.html": {
    "title": "Learning Physics From Video: Unsupervised Physical Parameter Estimation for Continuous Dynamical Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alejandro Castañeda Garcia",
      "Jan Warchocki",
      "Jan van Gemert",
      "Daan Brinks",
      "Nergis Tomen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cao_Temporal_Score_Analysis_for_Understanding_and_Correcting_Diffusion_Artifacts_CVPR_2025_paper.html": {
    "title": "Temporal Score Analysis for Understanding and Correcting Diffusion Artifacts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Cao",
      "Zengqun Zhao",
      "Ioannis Patras",
      "Shaogang Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qu_ProAPO_Progressively_Automatic_Prompt_Optimization_for_Visual_Classification_CVPR_2025_paper.html": {
    "title": "ProAPO: Progressively Automatic Prompt Optimization for Visual Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyan Qu",
      "Gaopeng Gou",
      "Jiamin Zhuang",
      "Jing Yu",
      "Kun Song",
      "Qihao Wang",
      "Yili Li",
      "Gang Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Petrov_ShapeWords_Guiding_Text-to-Image_Synthesis_with_3D_Shape-Aware_Prompts_CVPR_2025_paper.html": {
    "title": "ShapeWords: Guiding Text-to-Image Synthesis with 3D Shape-Aware Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dmitry Petrov",
      "Pradyumn Goyal",
      "Divyansh Shivashok",
      "Yuanming Tao",
      "Melinos Averkiou",
      "Evangelos Kalogerakis"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Auto-Encoded_Supervision_for_Perceptual_Image_Super-Resolution_CVPR_2025_paper.html": {
    "title": "Auto-Encoded Supervision for Perceptual Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MinKyu Lee",
      "Sangeek Hyun",
      "Woojin Jun",
      "Jae-Pil Heo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chinchure_Black_Swan_Abductive_and_Defeasible_Video_Reasoning_in_Unpredictable_Events_CVPR_2025_paper.html": {
    "title": "Black Swan: Abductive and Defeasible Video Reasoning in Unpredictable Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Chinchure",
      "Sahithya Ravi",
      "Raymond Ng",
      "Vered Shwartz",
      "Boyang Li",
      "Leonid Sigal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise_CVPR_2025_paper.html": {
    "title": "Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Burgert",
      "Yuancheng Xu",
      "Wenqi Xian",
      "Oliver Pilarski",
      "Pascal Clausen",
      "Mingming He",
      "Li Ma",
      "Yitong Deng",
      "Lingxiao Li",
      "Mohsen Mousavi",
      "Michael Ryoo",
      "Paul Debevec",
      "Ning Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gan_Silence_is_Golden_Leveraging_Adversarial_Examples_to_Nullify_Audio_Control_CVPR_2025_paper.html": {
    "title": "Silence is Golden: Leveraging Adversarial Examples to Nullify Audio Control in LDM-based Talking-Head Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Gan",
      "Jiaxu Miao",
      "Yunze Wang",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_Iterative_Predictor-Critic_Code_Decoding_for_Real-World_Image_Dehazing_CVPR_2025_paper.html": {
    "title": "Iterative Predictor-Critic Code Decoding for Real-World Image Dehazing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Fu",
      "Siyu Liu",
      "Zikun Liu",
      "Chun-Le Guo",
      "Hyunhee Park",
      "Ruiqi Wu",
      "Guoqing Wang",
      "Chongyi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_RNG_Relightable_Neural_Gaussians_CVPR_2025_paper.html": {
    "title": "RNG: Relightable Neural Gaussians",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahui Fan",
      "Fujun Luan",
      "Jian Yang",
      "Milos Hasan",
      "Beibei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_Towards_Realistic_Example-based_Modeling_via_3D_Gaussian_Stitching_CVPR_2025_paper.html": {
    "title": "Towards Realistic Example-based Modeling via 3D Gaussian Stitching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Gao",
      "Ziyi Yang",
      "Bingchen Gong",
      "Xiaoguang Han",
      "Sipeng Yang",
      "Xiaogang Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for_CVPR_2025_paper.html": {
    "title": "Filter Images First, Generate Instructions Later: Pre-Instruction Data Selection for Visual Instruction Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bardia Safaei",
      "Faizan Siddiqui",
      "Jiacong Xu",
      "Vishal M. Patel",
      "Shao-Yuan Lo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ballas_Gradient-Guided_Annealing_for_Domain_Generalization_CVPR_2025_paper.html": {
    "title": "Gradient-Guided Annealing for Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aristotelis Ballas",
      "Christos Diou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kong_Generative_Sparse-View_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "Generative Sparse-View Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyang Kong",
      "Xingyi Yang",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Burgess_MicroVQA_A_Multimodal_Reasoning_Benchmark_for_Microscopy-Based_Scientific_Research_CVPR_2025_paper.html": {
    "title": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Burgess",
      "Jeffrey J Nirschl",
      "Laura Bravo-Sánchez",
      "Alejandro Lozano",
      "Sanket Rajan Gupte",
      "Jesus G. Galaz-Montoya",
      "Yuhui Zhang",
      "Yuchang Su",
      "Disha Bhowmik",
      "Zachary Coman",
      "Sarina M Hasan",
      "Alexandra Johannesson",
      "William D. Leineweber",
      "Malvika G Nair",
      "Ridhi Yarlagadda",
      "Connor Zuraski",
      "Wah Chiu",
      "Sarah Cohen",
      "Jan N. Hansen",
      "Manuel D Leonetti",
      "Chad Liu",
      "Emma Lundberg",
      "Serena Yeung-Levy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Generative_Inbetweening_through_Frame-wise_Conditions-Driven_Video_Generation_CVPR_2025_paper.html": {
    "title": "Generative Inbetweening through Frame-wise Conditions-Driven Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Zhu",
      "Dongwei Ren",
      "Qilong Wang",
      "Xiaohe Wu",
      "Wangmeng Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness_CVPR_2025_paper.html": {
    "title": "DexGrasp Anything: Towards Universal Robotic Dexterous Grasping with Physics Awareness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Zhong",
      "Qi Jiang",
      "Jingyi Yu",
      "Yuexin Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kong_CustAny_Customizing_Anything_from_A_Single_Example_CVPR_2025_paper.html": {
    "title": "CustAny: Customizing Anything from A Single Example",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingjie Kong",
      "Kai Wu",
      "Chengming Xu",
      "Xiaobin Hu",
      "Wenhui Han",
      "Jinlong Peng",
      "Donghao Luo",
      "Mengtian Li",
      "Jiangning Zhang",
      "Chengjie Wang",
      "Yanwei Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zeng_Vision-Language_Gradient_Descent-driven_All-in-One_Deep_Unfolding_Networks_CVPR_2025_paper.html": {
    "title": "Vision-Language Gradient Descent-driven All-in-One Deep Unfolding Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haijin Zeng",
      "Xiangming Wang",
      "Yongyong Chen",
      "Jingyong Su",
      "Jie Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_3D-LLaVA_Towards_Generalist_3D_LMMs_with_Omni_Superpoint_Transformer_CVPR_2025_paper.html": {
    "title": "3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajun Deng",
      "Tianyu He",
      "Li Jiang",
      "Tianyu Wang",
      "Feras Dayoub",
      "Ian Reid"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_Event-based_Video_Super-Resolution_via_State_Space_Models_CVPR_2025_paper.html": {
    "title": "Event-based Video Super-Resolution via State Space Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyu Xiao",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ji_PoseTraj_Pose-Aware_Trajectory_Control_in_Video_Diffusion_CVPR_2025_paper.html": {
    "title": "PoseTraj: Pose-Aware Trajectory Control in Video Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longbin Ji",
      "Lei Zhong",
      "Pengfei Wei",
      "Changjian Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hermosilla_Masked_Scene_Modeling_Narrowing_the_Gap_Between_Supervised_and_Self-Supervised_CVPR_2025_paper.html": {
    "title": "Masked Scene Modeling: Narrowing the Gap Between Supervised and Self-Supervised Learning in 3D Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pedro Hermosilla",
      "Christian Stippel",
      "Leon Sick"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jang_VL2Lite_Task-Specific_Knowledge_Distillation_from_Large_Vision-Language_Models_to_Lightweight_CVPR_2025_paper.html": {
    "title": "VL2Lite: Task-Specific Knowledge Distillation from Large Vision-Language Models to Lightweight Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinseong Jang",
      "Chunfei Ma",
      "Byeongwon Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Boost_the_Inference_with_Co-training_A_Depth-guided_Mutual_Learning_Framework_CVPR_2025_paper.html": {
    "title": "Boost the Inference with Co-training: A Depth-guided Mutual Learning Framework for Semi-supervised Medical Polyp Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Li",
      "Zihao Zhu",
      "Yuxiang Zhang",
      "Yifan Chen",
      "Zhibin Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_VidHalluc_Evaluating_Temporal_Hallucinations_in_Multimodal_Large_Language_Models_for_CVPR_2025_paper.html": {
    "title": "VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoyu Li",
      "Eun Woo Im",
      "Pooyan Fazli"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gan_StageDesigner_Artistic_Stage_Generation_for_Scenography_via_Theater_Scripts_CVPR_2025_paper.html": {
    "title": "StageDesigner: Artistic Stage Generation for Scenography via Theater Scripts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoxing Gan",
      "Mengtian Li",
      "Ruhua Chen",
      "Zhongxia Ji",
      "Sichen Guo",
      "Huanling Hu",
      "Guangnan Ye",
      "Zuo Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_From_Laboratory_to_Real_World_A_New_Benchmark_Towards_Privacy-Preserved_CVPR_2025_paper.html": {
    "title": "From Laboratory to Real World: A New Benchmark Towards Privacy-Preserved Visible-Infrared Person Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Jiang",
      "Hao Yu",
      "Xu Cheng",
      "Haoyu Chen",
      "Zhaodong Sun",
      "Guoying Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sang_4Deform_Neural_Surface_Deformation_for_Robust_Shape_Interpolation_CVPR_2025_paper.html": {
    "title": "4Deform: Neural Surface Deformation for Robust Shape Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Sang",
      "Zehranaz Canfes",
      "Dongliang Cao",
      "Riccardo Marin",
      "Florian Bernard",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Astermark_Dense_Match_Summarization_for_Faster_Two-view_Estimation_CVPR_2025_paper.html": {
    "title": "Dense Match Summarization for Faster Two-view Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Astermark",
      "Anders Heyden",
      "Viktor Larsson"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Align-A-Video_Deterministic_Reward_Tuning_of_Image_Diffusion_Models_for_Consistent_CVPR_2025_paper.html": {
    "title": "Align-A-Video: Deterministic Reward Tuning of Image Diffusion Models for Consistent Video Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengzhi Wang",
      "Yingkang Zhong",
      "Jiangchuan Mu",
      "Kai Wu",
      "Mingliang Xiong",
      "Wen Fang",
      "Mingqing Liu",
      "Hao Deng",
      "Bin He",
      "Gang Li",
      "Qingwen Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search_CVPR_2025_paper.html": {
    "title": "Interpreting Object-level Foundation Models via Visual Precision Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruoyu Chen",
      "Siyuan Liang",
      "Jingzhi Li",
      "Shiming Liu",
      "Maosen Li",
      "Zhen Huang",
      "Hua Zhang",
      "Xiaochun Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mo_Foley-Flow_Coordinated_Video-to-Audio_Generation_with_Masked_Audio-Visual_Alignment_and_Dynamic_CVPR_2025_paper.html": {
    "title": "Foley-Flow: Coordinated Video-to-Audio Generation with Masked Audio-Visual Alignment and Dynamic Conditional Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shentong Mo",
      "Yibing Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_LION-FS_Fast__Slow_Video-Language_Thinker_as_Online_Video_Assistant_CVPR_2025_paper.html": {
    "title": "LION-FS: Fast & Slow Video-Language Thinker as Online Video Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Li",
      "Bing Hu",
      "Rui Shao",
      "Leyang Shen",
      "Liqiang Nie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction_CVPR_2025_paper.html": {
    "title": "CARE Transformer: Mobile-Friendly Linear Visual Transformer via Decoupled Dual Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Zhou",
      "Qingshan Xu",
      "Jiequan Cui",
      "Junbao Zhou",
      "Jing Zhang",
      "Richang Hong",
      "Hanwang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wasserman_Paint_by_Inpaint_Learning_to_Add_Image_Objects_by_Removing_CVPR_2025_paper.html": {
    "title": "Paint by Inpaint: Learning to Add Image Objects by Removing Them First",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Navve Wasserman",
      "Noam Rotstein",
      "Roy Ganz",
      "Ron Kimmel"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_Motion-Grounded_Video_Reasoning_Understanding_and_Perceiving_Motion_at_Pixel_Level_CVPR_2025_paper.html": {
    "title": "Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andong Deng",
      "Tongjia Chen",
      "Shoubin Yu",
      "Taojiannan Yang",
      "Lincoln Spencer",
      "Yapeng Tian",
      "Ajmal Saeed Mian",
      "Mohit Bansal",
      "Chen Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zha_PMA_Towards_Parameter-Efficient_Point_Cloud_Understanding_via_Point_Mamba_Adapter_CVPR_2025_paper.html": {
    "title": "PMA: Towards Parameter-Efficient Point Cloud Understanding via Point Mamba Adapter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaohua Zha",
      "Yanzi Wang",
      "Hang Guo",
      "Jinpeng Wang",
      "Tao Dai",
      "Bin Chen",
      "Zhihao Ouyang",
      "Xue Yuerong",
      "Ke Chen",
      "Shu-Tao Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images_CVPR_2025_paper.html": {
    "title": "All-directional Disparity Estimation for Real-world QPD Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongtao Yu",
      "Shaohui Song",
      "Lihu Sun",
      "Wenkai Su",
      "Xiaodong Yang",
      "Chengming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jeong_LC-Mamba_Local_and_Continuous_Mamba_with_Shifted_Windows_for_Frame_CVPR_2025_paper.html": {
    "title": "LC-Mamba: Local and Continuous Mamba with Shifted Windows for Frame Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min Wu Jeong",
      "Chae Eun Rhee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kang_Zero-Shot_Head_Swapping_in_Real-World_Scenarios_CVPR_2025_paper.html": {
    "title": "Zero-Shot Head Swapping in Real-World Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taewoong Kang",
      "Sohyun Jeong",
      "Hyojin Jang",
      "Jaegul Choo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ouasfi_Toward_Robust_Neural_Reconstruction_from_Sparse_Point_Sets_CVPR_2025_paper.html": {
    "title": "Toward Robust Neural Reconstruction from Sparse Point Sets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amine Ouasfi",
      "Shubhendu Jena",
      "Eric Marchand",
      "Adnane Boukhayma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_GPAvatar_High-fidelity_Head_Avatars_by_Learning_Efficient_Gaussian_Projections_CVPR_2025_paper.html": {
    "title": "GPAvatar: High-fidelity Head Avatars by Learning Efficient Gaussian Projections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei-Qi Feng",
      "Dong Han",
      "Ze-Kang Zhou",
      "Shunkai Li",
      "Xiaoqiang Liu",
      "Pengfei Wan",
      "Di Zhang",
      "Miao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_PIAD_Pose_and_Illumination_agnostic_Anomaly_Detection_CVPR_2025_paper.html": {
    "title": "PIAD: Pose and Illumination agnostic Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaichen Yang",
      "Junjie Cao",
      "Zeyu Bai",
      "Zhixun Su",
      "Andrea Tagliasacchi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Araujo_CAV-MAE_Sync_Improving_Contrastive_Audio-Visual_Mask_Autoencoders_via_Fine-Grained_Alignment_CVPR_2025_paper.html": {
    "title": "CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via Fine-Grained Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edson Araujo",
      "Andrew Rouditchenko",
      "Yuan Gong",
      "Saurabhchand Bhati",
      "Samuel Thomas",
      "Brian Kingsbury",
      "Leonid Karlinsky",
      "Rogerio Feris",
      "James R. Glass",
      "Hilde Kuehne"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jung_Two_is_Better_than_One__Efficient_Ensemble_Defense_for_CVPR_2025_paper.html": {
    "title": "Two is Better than One: Efficient Ensemble Defense for Robust and Compact Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoojin Jung",
      "Byung Cheol Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Madar_Tiled_Diffusion_CVPR_2025_paper.html": {
    "title": "Tiled Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Or Madar",
      "Ohad Fried"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Using_Diffusion_Priors_for_Video_Amodal_Segmentation_CVPR_2025_paper.html": {
    "title": "Using Diffusion Priors for Video Amodal Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaihua Chen",
      "Deva Ramanan",
      "Tarasha Khurana"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Das_COBRA_COmBinatorial_Retrieval_Augmentation_for_Few-Shot_Adaptation_CVPR_2025_paper.html": {
    "title": "COBRA: COmBinatorial Retrieval Augmentation for Few-Shot Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arnav M. Das",
      "Gantavya Bhatt",
      "Lilly Kumari",
      "Sahil Verma",
      "Jeff Bilmes"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera_CVPR_2025_paper.html": {
    "title": "Dyn-HaMR: Recovering 4D Interacting Hand Motion from a Dynamic Camera",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengdi Yu",
      "Stefanos Zafeiriou",
      "Tolga Birdal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings_CVPR_2025_paper.html": {
    "title": "The Scene Language: Representing Scenes with Programs, Words, and Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunzhi Zhang",
      "Zizhang Li",
      "Matt Zhou",
      "Shangzhe Wu",
      "Jiajun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Toussaint_ProbeSDF_Light_Field_Probes_For_Neural_Surface_Reconstruction_CVPR_2025_paper.html": {
    "title": "ProbeSDF: Light Field Probes For Neural Surface Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Briac Toussaint",
      "Diego Thomas",
      "Jean-Sébastien Franco"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays_CVPR_2025_paper.html": {
    "title": "Descriptor-In-Pixel : Point-Feature Tracking For Pixel Processor Arrays",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Laurie Bose",
      "Jianing Chen",
      "Piotr Dudek"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Hybrid_Concept_Bottleneck_Models_CVPR_2025_paper.html": {
    "title": "Hybrid Concept Bottleneck Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Liu",
      "Tianwei Zhang",
      "Shi Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rai_UVGS_Reimagining_Unstructured_3D_Gaussian_Splatting_using_UV_Mapping_CVPR_2025_paper.html": {
    "title": "UVGS: Reimagining Unstructured 3D Gaussian Splatting using UV Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aashish Rai",
      "Dilin Wang",
      "Mihir Jain",
      "Nikolaos Sarafianos",
      "Kefan Chen",
      "Srinath Sridhar",
      "Aayush Prakash"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Dual_Consolidation_for_Pre-Trained_Model-Based_Domain-Incremental_Learning_CVPR_2025_paper.html": {
    "title": "Dual Consolidation for Pre-Trained Model-Based Domain-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Da-Wei Zhou",
      "Zi-Wen Cai",
      "Han-Jia Ye",
      "Lijun Zhang",
      "De-Chuan Zhan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Learning_Physics-Based_Full-Body_Human_Reaching_and_Grasping_from_Brief_Walking_CVPR_2025_paper.html": {
    "title": "Learning Physics-Based Full-Body Human Reaching and Grasping from Brief Walking References",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yitang Li",
      "Mingxian Lin",
      "Zhuo Lin",
      "Yipeng Deng",
      "Yue Cao",
      "Li Yi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_EmoEdit_Evoking_Emotions_through_Image_Manipulation_CVPR_2025_paper.html": {
    "title": "EmoEdit: Evoking Emotions through Image Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyuan Yang",
      "Jiawei Feng",
      "Weibin Luo",
      "Dani Lischinski",
      "Daniel Cohen-Or",
      "Hui Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_RORem_Training_a_Robust_Object_Remover_with_Human-in-the-Loop_CVPR_2025_paper.html": {
    "title": "RORem: Training a Robust Object Remover with Human-in-the-Loop",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruibin Li",
      "Tao Yang",
      "Song Guo",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages_CVPR_2025_paper.html": {
    "title": "All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashmal Vayani",
      "Dinura Dissanayake",
      "Hasindri Watawana",
      "Noor Ahsan",
      "Nevasini Sasikumar",
      "Omkar Thawakar",
      "Henok Biadglign Ademtew",
      "Yahya Hmaiti",
      "Amandeep Kumar",
      "Kartik Kukreja",
      "Mykola Maslych",
      "Wafa Al Ghallabi",
      "Mihail Minkov Mihaylov",
      "Chao Qin",
      "Abdelrahman M. Shaker",
      "Mike Zhang",
      "Mahardika Krisna Ihsani",
      "Amiel Gian Esplana",
      "Monil Gokani",
      "Shachar Mirkin",
      "Harsh Singh",
      "Ashay Srivastava",
      "Endre Hamerlik",
      "Fathinah Asma Izzati",
      "Fadillah Adamsyah Maani",
      "Sebastian Cavada",
      "Jenny Chim",
      "Rohit Gupta",
      "Sanjay Manjunath",
      "Kamila Zhumakhanova",
      "Feno Heriniaina Rabevohitra",
      "Azril Hafizi Amirudin",
      "Muhammad Ridzuan",
      "Daniya Najiha Abdul Kareem",
      "Ketan Pravin More",
      "Kunyang Li",
      "Pramesh Shakya",
      "Muhammad Saad",
      "Amirpouya Ghasemaghaei",
      "Amirbek Djanibekov",
      "Dilshod Azizov",
      "Branislava Jankovic",
      "Naman Bhatia",
      "Alvaro Cabrera",
      "Johan Obando-Ceron",
      "Olympiah Otieno",
      "Febian Farestam",
      "Muztoba Rabbani",
      "Sanoojan Ballah",
      "Santosh Sanjeev",
      "Abduragim Shtanchaev",
      "Maheen Fatima",
      "Thao Nguyen",
      "Amrin Kareem",
      "Toluwani Aremu",
      "Nathan Augusto Zacarias Xavier",
      "Amit Bhatkal",
      "Hawau Olamide Toyin",
      "Aman Chadha",
      "Hisham Cholakkal",
      "Rao Muhammad Anwer",
      "Michael Felsberg",
      "Jorma Laaksonen",
      "Thamar Solorio",
      "Monojit Choudhury",
      "Ivan Laptev",
      "Mubarak Shah",
      "Salman Khan",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yuan_SparseAlign_a_Fully_Sparse_Framework_for_Cooperative_Object_Detection_CVPR_2025_paper.html": {
    "title": "SparseAlign: a Fully Sparse Framework for Cooperative Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunshuang Yuan",
      "Yan Xia",
      "Daniel Cremers",
      "Monika Sester"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_Video-Bench_Human-Aligned_Video_Generation_Benchmark_CVPR_2025_paper.html": {
    "title": "Video-Bench: Human-Aligned Video Generation Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Han",
      "Siyuan Li",
      "Jiaqi Chen",
      "Yiwen Yuan",
      "Yuling Wu",
      "Yufan Deng",
      "Chak Tou Leong",
      "Hanwen Du",
      "Junchen Fu",
      "Youhua Li",
      "Jie Zhang",
      "Chi Zhang",
      "Li-jia Li",
      "Yongxin Ni"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/del_Rio_Data_Distributional_Properties_As_Inductive_Bias_for_Systematic_Generalization_CVPR_2025_paper.html": {
    "title": "Data Distributional Properties As Inductive Bias for Systematic Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felipe del Rio",
      "Alain Raymond-Saez",
      "Daniel Florea",
      "Rodrigo Toro Icarte",
      "Julio Hurtado",
      "Cristian B. Calderon",
      "Alvaro Soto"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_MergeVQ_A_Unified_Framework_for_Visual_Generation_and_Representation_with_CVPR_2025_paper.html": {
    "title": "MergeVQ: A Unified Framework for Visual Generation and Representation with Disentangled Token Merging and Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Li",
      "Luyuan Zhang",
      "Zedong Wang",
      "Juanxi Tian",
      "Cheng Tan",
      "Zicheng Liu",
      "Chang Yu",
      "Qingsong Xie",
      "Haonan Lu",
      "Haoqian Wang",
      "Zhen Lei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_InterAct_Advancing_Large-Scale_Versatile_3D_Human-Object_Interaction_Generation_CVPR_2025_paper.html": {
    "title": "InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sirui Xu",
      "Dongting Li",
      "Yucheng Zhang",
      "Xiyan Xu",
      "Qi Long",
      "Ziyin Wang",
      "Yunzhi Lu",
      "Shuchang Dong",
      "Hezi Jiang",
      "Akshat Gupta",
      "Yu-Xiong Wang",
      "Liang-Yan Gui"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model_CVPR_2025_paper.html": {
    "title": "TopoCellGen: Generating Histopathology Cell Topology with a Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meilong Xu",
      "Saumya Gupta",
      "Xiaoling Hu",
      "Chen Li",
      "Shahira Abousamra",
      "Dimitris Samaras",
      "Prateek Prasanna",
      "Chao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Anyattack_Towards_Large-scale_Self-supervised_Adversarial_Attacks_on_Vision-language_Models_CVPR_2025_paper.html": {
    "title": "Anyattack: Towards Large-scale Self-supervised Adversarial Attacks on Vision-language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Zhang",
      "Junhong Ye",
      "Xingjun Ma",
      "Yige Li",
      "Yunfan Yang",
      "Yunhao Chen",
      "Jitao Sang",
      "Dit-Yan Yeung"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nguyen_Joint_Optimization_of_Neural_Radiance_Fields_and_Continuous_Camera_Motion_CVPR_2025_paper.html": {
    "title": "Joint Optimization of Neural Radiance Fields and Continuous Camera Motion from a Monocular Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoang Chuong Nguyen",
      "Wei Mao",
      "Jose M. Alvarez",
      "Miaomiao Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gu_IRGS_Inter-Reflective_Gaussian_Splatting_with_2D_Gaussian_Ray_Tracing_CVPR_2025_paper.html": {
    "title": "IRGS: Inter-Reflective Gaussian Splatting with 2D Gaussian Ray Tracing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun Gu",
      "Xiaofei Wei",
      "Zixuan Zeng",
      "Yuxuan Yao",
      "Li Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions_CVPR_2025_paper.html": {
    "title": "InterMimic: Towards Universal Whole-Body Control for Physics-Based Human-Object Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sirui Xu",
      "Hung Yu Ling",
      "Yu-Xiong Wang",
      "Liang-Yan Gui"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning_CVPR_2025_paper.html": {
    "title": "Meta-Learning Hyperparameters for Parameter Efficient Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichen Tian",
      "Yaoyao Liu",
      "Qianru Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cohen-Bar_TriTex_Learning_Texture_from_a_Single_Mesh_via_Triplane_Semantic_CVPR_2025_paper.html": {
    "title": "TriTex: Learning Texture from a Single Mesh via Triplane Semantic Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dana Cohen-Bar",
      "Daniel Cohen-Or",
      "Gal Chechik",
      "Yoni Kasten"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning_CVPR_2025_paper.html": {
    "title": "Efficient Test-time Adaptive Object Detection via Sensitivity-Guided Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kunyu Wang",
      "Xueyang Fu",
      "Xin Lu",
      "Chengjie Ge",
      "Chengzhi Cao",
      "Wei Zhai",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wen_A_Data-Centric_Revisit_of_Pre-Trained_Vision_Models_for_Robot_Learning_CVPR_2025_paper.html": {
    "title": "A Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Wen",
      "Bingchen Zhao",
      "Yilun Chen",
      "Jiangmiao Pang",
      "Xiaojuan Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Marsili_Visual_Agentic_AI_for_Spatial_Reasoning_with_a_Dynamic_API_CVPR_2025_paper.html": {
    "title": "Visual Agentic AI for Spatial Reasoning with a Dynamic API",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Damiano Marsili",
      "Rohun Agrawal",
      "Yisong Yue",
      "Georgia Gkioxari"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_TAMT_Temporal-Aware_Model_Tuning_for_Cross-Domain_Few-Shot_Action_Recognition_CVPR_2025_paper.html": {
    "title": "TAMT: Temporal-Aware Model Tuning for Cross-Domain Few-Shot Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilong Wang",
      "Zilin Gao",
      "Qilong Wang",
      "Zhaofeng Chen",
      "Peihua Li",
      "Qinghua Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zang_Feature_Spectrum_Learning_for_Remote_Sensing_Change_Detection_CVPR_2025_paper.html": {
    "title": "Feature Spectrum Learning for Remote Sensing Change Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zang",
      "Dong Zhao",
      "Shuang Wang",
      "Dou Quan",
      "Zhun Zhong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gallagher-Syed_BioX-CPath_Biologically-driven_Explainable_Diagnostics_for_Multistain_IHC_Computational_Pathology_CVPR_2025_paper.html": {
    "title": "BioX-CPath: Biologically-driven Explainable Diagnostics for Multistain IHC Computational Pathology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amaya Gallagher-Syed",
      "Henry Senior",
      "Omnia Alwazzan",
      "Elena Pontarini",
      "Michele Bombardieri",
      "Costantino Pitzalis",
      "Myles J. Lewis",
      "Michael R. Barnes",
      "Luca Rossi",
      "Gregory Slabaugh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_DriveDreamer4D_World_Models_Are_Effective_Data_Machines_for_4D_Driving_CVPR_2025_paper.html": {
    "title": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guosheng Zhao",
      "Chaojun Ni",
      "Xiaofeng Wang",
      "Zheng Zhu",
      "Xueyang Zhang",
      "Yida Wang",
      "Guan Huang",
      "Xinze Chen",
      "Boyuan Wang",
      "Youyi Zhang",
      "Wenjun Mei",
      "Xingang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cai_LoKi_Low-dimensional_KAN_for_Efficient_Fine-tuning_Image_Models_CVPR_2025_paper.html": {
    "title": "LoKi: Low-dimensional KAN for Efficient Fine-tuning Image Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Cai",
      "Renjie Pan",
      "Hua Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language_CVPR_2025_paper.html": {
    "title": "Dr. Splat: Directly Referring 3D Gaussian Splatting via Direct Language Embedding Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kim Jun-Seong",
      "GeonU Kim",
      "Kim Yu-Ji",
      "Yu-Chiang Frank Wang",
      "Jaesung Choe",
      "Tae-Hyun Oh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Wavelet_and_Prototype_Augmented_Query-based_Transformer_for_Pixel-level_Surface_Defect_CVPR_2025_paper.html": {
    "title": "Wavelet and Prototype Augmented Query-based Transformer for Pixel-level Surface Defect Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Yan",
      "Xiaoheng Jiang",
      "Yang Lu",
      "Jiale Cao",
      "Dong Chen",
      "Mingliang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Marques_GauCho_Gaussian_Distributions_with_Cholesky_Decomposition_for_Oriented_Object_Detection_CVPR_2025_paper.html": {
    "title": "GauCho: Gaussian Distributions with Cholesky Decomposition for Oriented Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "José Henrique Lima Marques",
      "Jeffri Murrugarra-Llerena",
      "Claudio R. Jung"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zou_Alignment_Mining_and_Fusion_Representation_Alignment_with_Hard_Negative_Mining_CVPR_2025_paper.html": {
    "title": "Alignment, Mining and Fusion: Representation Alignment with Hard Negative Mining and Selective Knowledge Fusion for Medical Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhao Zou",
      "Zhaozheng Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_No_Thing_Nothing_Highlighting_Safety-Critical_Classes_for_Robust_LiDAR_Semantic_CVPR_2025_paper.html": {
    "title": "No Thing, Nothing: Highlighting Safety-Critical Classes for Robust LiDAR Semantic Segmentation in Adverse Weather",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junsung Park",
      "Hwijeong Lee",
      "Inha Kang",
      "Hyunjung Shim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_Mind_the_Gap_Detecting_Black-box_Adversarial_Attacks_in_the_Making_CVPR_2025_paper.html": {
    "title": "Mind the Gap: Detecting Black-box Adversarial Attacks in the Making through Query Update Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeonghwan Park",
      "Niall McLaughlin",
      "Ihsen Alouani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_Consistent_Normal_Orientation_for_3D_Point_Clouds_via_Least_Squares_CVPR_2025_paper.html": {
    "title": "Consistent Normal Orientation for 3D Point Clouds via Least Squares on Delaunay Graph",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rao Fu",
      "Jianmin Zheng",
      "Liang Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zuo_GaussianWorld_Gaussian_World_Model_for_Streaming_3D_Occupancy_Prediction_CVPR_2025_paper.html": {
    "title": "GaussianWorld: Gaussian World Model for Streaming 3D Occupancy Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sicheng Zuo",
      "Wenzhao Zheng",
      "Yuanhui Huang",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity_CVPR_2025_paper.html": {
    "title": "ICP: Immediate Compensation Pruning for Mid-to-high Sparsity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Luo",
      "Xueming Fu",
      "Zihang Jiang",
      "S. Kevin Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_VinaBench_Benchmark_for_Faithful_and_Consistent_Visual_Narratives_CVPR_2025_paper.html": {
    "title": "VinaBench: Benchmark for Faithful and Consistent Visual Narratives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Silin Gao",
      "Sheryl Mathew",
      "Li Mi",
      "Sepideh Mamooler",
      "Mengjie Zhao",
      "Hiromi Wakaki",
      "Yuki Mitsufuji",
      "Syrielle Montariol",
      "Antoine Bosselut"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_ATA_Adaptive_Transformation_Agent_for_Text-Guided_Subject-Position_Variable_Background_Inpainting_CVPR_2025_paper.html": {
    "title": "ATA: Adaptive Transformation Agent for Text-Guided Subject-Position Variable Background Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizhe Tang",
      "Zhimin Sun",
      "Yuzhen Du",
      "Ran Yi",
      "Guangben Lu",
      "Teng Hu",
      "Luying Li",
      "Lizhuang Ma",
      "Fangyuan Zou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model_CVPR_2025_paper.html": {
    "title": "Optimizing for the Shortest Path in Denoising Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ping Chen",
      "Xingpeng Zhang",
      "Zhaoxiang Liu",
      "Huan Hu",
      "Xiang Liu",
      "Kai Wang",
      "Min Wang",
      "Yanlin Qian",
      "Shiguo Lian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Antidote_A_Unified_Framework_for_Mitigating_LVLM_Hallucinations_in_Counterfactual_CVPR_2025_paper.html": {
    "title": "Antidote: A Unified Framework for Mitigating LVLM Hallucinations in Counterfactual Presupposition and Object Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanchen Wu",
      "Lu Zhang",
      "Hang Yao",
      "Junlong Du",
      "Ke Yan",
      "Shouhong Ding",
      "Yunsheng Wu",
      "Xiaoqiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Language-Guided_Audio-Visual_Learning_for_Long-Term_Sports_Assessment_CVPR_2025_paper.html": {
    "title": "Language-Guided Audio-Visual Learning for Long-Term Sports Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huangbiao Xu",
      "Xiao Ke",
      "Huanqi Wu",
      "Rui Xu",
      "Yuezhou Li",
      "Wenzhong Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_Dynamic_Pseudo_Labeling_via_Gradient_Cutting_for_High-Low_Entropy_Exploration_CVPR_2025_paper.html": {
    "title": "Dynamic Pseudo Labeling via Gradient Cutting for High-Low Entropy Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jae Hyeon Park",
      "Joo Hyeon Jeon",
      "Jae Yun Lee",
      "Sangyeon Ahn",
      "Min Hee Cha",
      "Min Geol Kim",
      "Hyeok Nam",
      "Sung In Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_VODiff_Controlling_Object_Visibility_Order_in_Text-to-Image_Generation_CVPR_2025_paper.html": {
    "title": "VODiff: Controlling Object Visibility Order in Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Liang",
      "Jinyuan Jia",
      "Yuhao Liu",
      "Zhanghan Ke",
      "Hongbo Fu",
      "Rynson W. H. Lau"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Dual_Diffusion_for_Unified_Image_Generation_and_Understanding_CVPR_2025_paper.html": {
    "title": "Dual Diffusion for Unified Image Generation and Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijie Li",
      "Henry Li",
      "Yichun Shi",
      "Amir Barati Farimani",
      "Yuval Kluger",
      "Linjie Yang",
      "Peng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_WeakMCN_Multi-task_Collaborative_Network_for_Weakly_Supervised_Referring_Expression_Comprehension_CVPR_2025_paper.html": {
    "title": "WeakMCN: Multi-task Collaborative Network for Weakly Supervised Referring Expression Comprehension and Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Silin Cheng",
      "Yang Liu",
      "Xinwei He",
      "Sebastien Ourselin",
      "Lei Tan",
      "Gen Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_CAD-Llama_Leveraging_Large_Language_Models_for_Computer-Aided_Design_Parametric_3D_CVPR_2025_paper.html": {
    "title": "CAD-Llama: Leveraging Large Language Models for Computer-Aided Design Parametric 3D Model Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Li",
      "Weijian Ma",
      "Xueyang Li",
      "Yunzhong Lou",
      "Guichun Zhou",
      "Xiangdong Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Leveraging_SD_Map_to_Augment_HD_Map-based_Trajectory_Prediction_CVPR_2025_paper.html": {
    "title": "Leveraging SD Map to Augment HD Map-based Trajectory Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwei Dong",
      "Ran Ding",
      "Wei Li",
      "Peng Zhang",
      "Guobin Tang",
      "Jia Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_4DGC_Rate-Aware_4D_Gaussian_Compression_for_Efficient_Streamable_Free-Viewpoint_Video_CVPR_2025_paper.html": {
    "title": "4DGC: Rate-Aware 4D Gaussian Compression for Efficient Streamable Free-Viewpoint Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiang Hu",
      "Zihan Zheng",
      "Houqiang Zhong",
      "Sihua Fu",
      "Li Song",
      "Xiaoyun Zhang",
      "Guangtao Zhai",
      "Yanfeng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tan_ONDA-Pose_Occlusion-Aware_Neural_Domain_Adaptation_for_Self-Supervised_6D_Object_Pose_CVPR_2025_paper.html": {
    "title": "ONDA-Pose: Occlusion-Aware Neural Domain Adaptation for Self-Supervised 6D Object Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Tan",
      "Qiulei Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhen_Teller_Real-Time_Streaming_Audio-Driven_Portrait_Animation_with_Autoregressive_Motion_Generation_CVPR_2025_paper.html": {
    "title": "Teller: Real-Time Streaming Audio-Driven Portrait Animation with Autoregressive Motion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingcheng Zhen",
      "Shunshun Yin",
      "Shiyang Qin",
      "Hou Yi",
      "Ziwei Zhang",
      "Siyuan Liu",
      "Gan Qi",
      "Ming Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Saunders_GASP_Gaussian_Avatars_with_Synthetic_Priors_CVPR_2025_paper.html": {
    "title": "GASP: Gaussian Avatars with Synthetic Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jack Saunders",
      "Charlie Hewitt",
      "Yanan Jian",
      "Marek Kowalski",
      "Tadas Baltrusaitis",
      "Yiye Chen",
      "Darren Cosker",
      "Virginia Estellers",
      "Nicholas Gydé",
      "Vinay P. Namboodiri",
      "Benjamin E. Lundell"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Q-PART_Quasi-Periodic_Adaptive_Regression_with_Test-time_Training_for_Pediatric_Left_CVPR_2025_paper.html": {
    "title": "Q-PART: Quasi-Periodic Adaptive Regression with Test-time Training for Pediatric Left Ventricular Ejection Fraction Regression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Liu",
      "Tiexin Qin",
      "Hui Liu",
      "Yilei Shi",
      "Lichao Mou",
      "Xiao Xiang Zhu",
      "Shiqi Wang",
      "Haoliang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rangwani_Composing_Parts_for_Expressive_Object_Generation_CVPR_2025_paper.html": {
    "title": "Composing Parts for Expressive Object Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Harsh Rangwani",
      "Aishwarya Agarwal",
      "Kuldeep Kulkarni",
      "R. Venkatesh Babu",
      "Srikrishna Karanam"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_CarPlanner_Consistent_Auto-regressive_Trajectory_Planning_for_Large-Scale_Reinforcement_Learning_in_CVPR_2025_paper.html": {
    "title": "CarPlanner: Consistent Auto-regressive Trajectory Planning for Large-Scale Reinforcement Learning in Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongkun Zhang",
      "Jiaming Liang",
      "Ke Guo",
      "Sha Lu",
      "Qi Wang",
      "Rong Xiong",
      "Zhenwei Miao",
      "Yue Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qin_Apply_Hierarchical-Chain-of-Generation_to_Complex_Attributes_Text-to-3D_Generation_CVPR_2025_paper.html": {
    "title": "Apply Hierarchical-Chain-of-Generation to Complex Attributes Text-to-3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Qin",
      "Zhu Xu",
      "Yang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_PersonaHOI_Effortlessly_Improving_Face_Personalization_in_Human-Object_Interaction_Generation_CVPR_2025_paper.html": {
    "title": "PersonaHOI: Effortlessly Improving Face Personalization in Human-Object Interaction Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinting Hu",
      "Haoran Wang",
      "Jan Eric Lenssen",
      "Bernt Schiele"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yi_Video-Panda_Parameter-efficient_Alignment_for_Encoder-free_Video-Language_Models_CVPR_2025_paper.html": {
    "title": "Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhui Yi",
      "Syed Talal Wasim",
      "Yanan Luo",
      "Muzammal Naseer",
      "Juergen Gall"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_COSMIC_Clique-Oriented_Semantic_Multi-space_Integration_for_Robust_CLIP_Test-Time_Adaptation_CVPR_2025_paper.html": {
    "title": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanding Huang",
      "Jingyan Jiang",
      "Qinting Jiang",
      "Hebei Li",
      "Faisal Nadeem Khan",
      "Zhi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MonoSplat_Generalizable_3D_Gaussian_Splatting_from_Monocular_Depth_Foundation_Models_CVPR_2025_paper.html": {
    "title": "MonoSplat: Generalizable 3D Gaussian Splatting from Monocular Depth Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Liu",
      "Keyu Fan",
      "Weihao Yu",
      "Chenxin Li",
      "Hao Lu",
      "Yixuan Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Hybrid_Global-Local_Representation_with_Augmented_Spatial_Guidance_for_Zero-Shot_Referring_CVPR_2025_paper.html": {
    "title": "Hybrid Global-Local Representation with Augmented Spatial Guidance for Zero-Shot Referring Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting Liu",
      "Siyuan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_SOLVE_Synergy_of_Language-Vision_and_End-to-End_Networks_for_Autonomous_Driving_CVPR_2025_paper.html": {
    "title": "SOLVE: Synergy of Language-Vision and End-to-End Networks for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuesong Chen",
      "Linjiang Huang",
      "Tao Ma",
      "Rongyao Fang",
      "Shaoshuai Shi",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Du_Shift_the_Lens_Environment-Aware_Unsupervised_Camouflaged_Object_Detection_CVPR_2025_paper.html": {
    "title": "Shift the Lens: Environment-Aware Unsupervised Camouflaged Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ji Du",
      "Fangwei Hao",
      "Mingyang Yu",
      "Desheng Kong",
      "Jiesheng Wu",
      "Bin Wang",
      "Jing Xu",
      "Ping Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Probability_Density_Geodesics_in_Image_Diffusion_Latent_Space_CVPR_2025_paper.html": {
    "title": "Probability Density Geodesics in Image Diffusion Latent Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingtao Yu",
      "Jaskirat Singh",
      "Zhaoyuan Yang",
      "Peter Henry Tu",
      "Jing Zhang",
      "Hongdong Li",
      "Richard Hartley",
      "Dylan Campbell"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_High-quality_Point_Cloud_Oriented_Normal_Estimation_via_Hybrid_Angular_and_CVPR_2025_paper.html": {
    "title": "High-quality Point Cloud Oriented Normal Estimation via Hybrid Angular and Euclidean Distance Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanqi Li",
      "Jingcheng Huang",
      "Hongshen Wang",
      "Peiyuan Lv",
      "Yansong Liu",
      "Jiuming Zheng",
      "Jie Guo",
      "Yanwen Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_DriveScape_High-Resolution_Driving_Video_Generation_by_Multi-View_Feature_Fusion_CVPR_2025_paper.html": {
    "title": "DriveScape: High-Resolution Driving Video Generation by Multi-View Feature Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Wu",
      "Xi Guo",
      "Weixuan Tang",
      "Tingxuan Huang",
      "Chiyu Wang",
      "Chenjing Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tybl_Training-free_Neural_Architecture_Search_through_Variance_of_Knowledge_of_Deep_CVPR_2025_paper.html": {
    "title": "Training-free Neural Architecture Search through Variance of Knowledge of Deep Network Weights",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ondrej Tybl",
      "Lukas Neumann"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Every_SAM_Drop_Counts_Embracing_Semantic_Priors_for_Multi-Modality_Image_CVPR_2025_paper.html": {
    "title": "Every SAM Drop Counts: Embracing Semantic Priors for Multi-Modality Image Fusion and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanyao Wu",
      "Haoyu Liu",
      "Hongming Fu",
      "Yichuan Peng",
      "Jinyuan Liu",
      "Xin Fan",
      "Risheng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_EgoLife_Towards_Egocentric_Life_Assistant_CVPR_2025_paper.html": {
    "title": "EgoLife: Towards Egocentric Life Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingkang Yang",
      "Shuai Liu",
      "Hongming Guo",
      "Yuhao Dong",
      "Xiamengwei Zhang",
      "Sicheng Zhang",
      "Pengyun Wang",
      "Zitang Zhou",
      "Binzhu Xie",
      "Ziyue Wang",
      "Bei Ouyang",
      "Zhengyu Lin",
      "Marco Cominelli",
      "Zhongang Cai",
      "Bo Li",
      "Yuanhan Zhang",
      "Peiyuan Zhang",
      "Fangzhou Hong",
      "Joerg Widmer",
      "Francesco Gringoli",
      "Lei Yang",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xing_RAEncoder_A_Label-Free_Reversible_Adversarial_Examples_Encoder_for_Dataset_Intellectual_CVPR_2025_paper.html": {
    "title": "RAEncoder: A Label-Free Reversible Adversarial Examples Encoder for Dataset Intellectual Property Protection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Xing",
      "Zhuo Tian",
      "Xuefeng Fan",
      "Xiaoyi Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_BrepGiff_Lightweight_Generation_of_Complex_B-rep_with_3D_GAT_Diffusion_CVPR_2025_paper.html": {
    "title": "BrepGiff: Lightweight Generation of Complex B-rep with 3D GAT Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Guo",
      "Xiaoshui Huang",
      "Hao jiacheng",
      "Yunpeng Bai",
      "Hongping Gan",
      "Yilei Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Towards_Fine-Grained_Interpretability_Counterfactual_Explanations_for_Misclassification_with_Saliency_Partition_CVPR_2025_paper.html": {
    "title": "Towards Fine-Grained Interpretability: Counterfactual Explanations for Misclassification with Saliency Partition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lintong Zhang",
      "Kang Yin",
      "Seong-Whan Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Song_Prior-free_3D_Object_Tracking_CVPR_2025_paper.html": {
    "title": "Prior-free 3D Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiuqiang Song",
      "Li Jin",
      "Zhengxian Zhang",
      "Jiachen Li",
      "Fan Zhong",
      "Guofeng Zhang",
      "Xueying Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_LatentHOI_On_the_Generalizable_Hand_Object_Motion_Generation_with_Latent_CVPR_2025_paper.html": {
    "title": "LatentHOI: On the Generalizable Hand Object Motion Generation with Latent Hand Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muchen Li",
      "Sammy Christen",
      "Chengde Wan",
      "Yujun Cai",
      "Renjie Liao",
      "Leonid Sigal",
      "Shugao Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Progressive_Correspondence_Regenerator_for_Robust_3D_Registration_CVPR_2025_paper.html": {
    "title": "Progressive Correspondence Regenerator for Robust 3D Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guiyu Zhao",
      "Sheng Ao",
      "Ye Zhang",
      "Kai Xu",
      "Yulan Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Cross-Modal_3D_Representation_with_Multi-View_Images_and_Point_Clouds_CVPR_2025_paper.html": {
    "title": "Cross-Modal 3D Representation with Multi-View Images and Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Zhou",
      "Pinghui Wang",
      "Zi Liang",
      "Haitao Bai",
      "Ruofei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ventura_Chapter-Llama_Efficient_Chaptering_in_Hour-Long_Videos_with_LLMs_CVPR_2025_paper.html": {
    "title": "Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Ventura",
      "Antoine Yang",
      "Cordelia Schmid",
      "Gül Varol"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ni_Decompositional_Neural_Scene_Reconstruction_with_Generative_Diffusion_Prior_CVPR_2025_paper.html": {
    "title": "Decompositional Neural Scene Reconstruction with Generative Diffusion Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junfeng Ni",
      "Yu Liu",
      "Ruijie Lu",
      "Zirui Zhou",
      "Song-Chun Zhu",
      "Yixin Chen",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Distribution_Prototype_Diffusion_Learning_for_Open-set_Supervised_Anomaly_Detection_CVPR_2025_paper.html": {
    "title": "Distribution Prototype Diffusion Learning for Open-set Supervised Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fuyun Wang",
      "Tong Zhang",
      "Yuanzhi Wang",
      "Yide Qiu",
      "Xin Liu",
      "Xu Guo",
      "Zhen Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Learning_Visual_Generative_Priors_without_Text_CVPR_2025_paper.html": {
    "title": "Learning Visual Generative Priors without Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuailei Ma",
      "Kecheng Zheng",
      "Ying Wei",
      "Wei Wu",
      "Fan Lu",
      "Yifei Zhang",
      "Chen-Wei Xie",
      "Biao Gong",
      "Jiapeng Zhu",
      "Yujun Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Joint_Scheduling_of_Causal_Prompts_and_Tasks_for_Multi-Task_Learning_CVPR_2025_paper.html": {
    "title": "Joint Scheduling of Causal Prompts and Tasks for Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoyang Li",
      "Jianyang Qin",
      "Jinhao Cui",
      "Zeyu Liu",
      "Ning Hu",
      "Qing Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers_CVPR_2025_paper.html": {
    "title": "Full-DoF Egomotion Estimation for Event Cameras Using Geometric Solvers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ji Zhao",
      "Banglei Guan",
      "Zibin Liu",
      "Laurent Kneip"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Wu",
      "Janick Martinez Esturo",
      "Ashkan Mirzaei",
      "Nicolas Moënne-Loccoz",
      "Zan Gojcic"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/You_Teaching_Large_Language_Models_to_Regress_Accurate_Image_Quality_Scores_CVPR_2025_paper.html": {
    "title": "Teaching Large Language Models to Regress Accurate Image Quality Scores Using Score Distribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan You",
      "Xin Cai",
      "Jinjin Gu",
      "Tianfan Xue",
      "Chao Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_Lifting_the_Veil_on_Visual_Information_Flow_in_MLLMs_Unlocking_CVPR_2025_paper.html": {
    "title": "Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Yin",
      "Guangzong Si",
      "Zilei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Your_Scale_Factors_are_My_Weapon_Targeted_Bit-Flip_Attacks_on_CVPR_2025_paper.html": {
    "title": "Your Scale Factors are My Weapon: Targeted Bit-Flip Attacks on Vision Transformers via Scale Factor Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialai Wang",
      "Yuxiao Wu",
      "Weiye Xu",
      "Yating Huang",
      "Chao Zhang",
      "Zongpeng Li",
      "Mingwei Xu",
      "Zhenkai Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Marten_Visual_Question_Answering_with_Mask_Generation_for_Multi-modal_Document_CVPR_2025_paper.html": {
    "title": "Marten: Visual Question Answering with Mask Generation for Multi-modal Document Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zining Wang",
      "Tongkun Guan",
      "Pei Fu",
      "Chen Duan",
      "Qianyi Jiang",
      "Zhentao Guo",
      "Shan Guo",
      "Junfeng Luo",
      "Wei Shen",
      "Xiaokang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Mamba-Reg_Vision_Mamba_Also_Needs_Registers_CVPR_2025_paper.html": {
    "title": "Mamba-Reg: Vision Mamba Also Needs Registers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Wang",
      "Jiahao Wang",
      "Sucheng Ren",
      "Guoyizhe Wei",
      "Jieru Mei",
      "Wei Shao",
      "Yuyin Zhou",
      "Alan Yuille",
      "Cihang Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Schnaus_Its_a_Blind_Match_Towards_Vision-Language_Correspondence_without_Parallel_Data_CVPR_2025_paper.html": {
    "title": "It's a (Blind) Match! Towards Vision-Language Correspondence without Parallel Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominik Schnaus",
      "Nikita Araslanov",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_Open_Set_Label_Shift_with_Test_Time_Out-of-Distribution_Reference_CVPR_2025_paper.html": {
    "title": "Open Set Label Shift with Test Time Out-of-Distribution Reference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changkun Ye",
      "Russell Tsuchida",
      "Lars Petersson",
      "Nick Barnes"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nam_Visual_Persona_Foundation_Model_for_Full-Body_Human_Customization_CVPR_2025_paper.html": {
    "title": "Visual Persona: Foundation Model for Full-Body Human Customization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jisu Nam",
      "Soowon Son",
      "Zhan Xu",
      "Jing Shi",
      "Difan Liu",
      "Feng Liu",
      "Seungryong Kim",
      "Yang Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_SOGS_Second-Order_Anchor_for_Advanced_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "SOGS: Second-Order Anchor for Advanced 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahui Zhang",
      "Fangneng Zhan",
      "Ling Shao",
      "Shijian Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_GaussianFormer-2_Probabilistic_Gaussian_Superposition_for_Efficient_3D_Occupancy_Prediction_CVPR_2025_paper.html": {
    "title": "GaussianFormer-2: Probabilistic Gaussian Superposition for Efficient 3D Occupancy Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhui Huang",
      "Amonnut Thammatadatrakoon",
      "Wenzhao Zheng",
      "Yunpeng Zhang",
      "Dalong Du",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_MExD_An_Expert-Infused_Diffusion_Model_for_Whole-Slide_Image_Classification_CVPR_2025_paper.html": {
    "title": "MExD: An Expert-Infused Diffusion Model for Whole-Slide Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianwei Zhao",
      "Xin Li",
      "Fan Yang",
      "Qiang Zhai",
      "Ao Luo",
      "Yang Zhao",
      "Hong Cheng",
      "Huazhu Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Buch_Flexible_Frame_Selection_for_Efficient_Video_Reasoning_CVPR_2025_paper.html": {
    "title": "Flexible Frame Selection for Efficient Video Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shyamal Buch",
      "Arsha Nagrani",
      "Anurag Arnab",
      "Cordelia Schmid"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_EventGPT_Event_Stream_Understanding_with_Multimodal_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "EventGPT: Event Stream Understanding with Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaoyu Liu",
      "Jianing Li",
      "Guanghui Zhao",
      "Yunjian Zhang",
      "Xin Meng",
      "Fei Richard Yu",
      "Xiangyang Ji",
      "Ming Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Pixel-level_and_Semantic-level_Adjustable_Super-resolution_A_Dual-LoRA_Approach_CVPR_2025_paper.html": {
    "title": "Pixel-level and Semantic-level Adjustable Super-resolution: A Dual-LoRA Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingchen Sun",
      "Rongyuan Wu",
      "Zhiyuan Ma",
      "Shuaizheng Liu",
      "Qiaosi Yi",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Let_Samples_Speak_Mitigating_Spurious_Correlation_by_Exploiting_the_Clusterness_CVPR_2025_paper.html": {
    "title": "Let Samples Speak: Mitigating Spurious Correlation by Exploiting the Clusterness of Samples",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiwei Li",
      "Junzhuo Liu",
      "Yuanyuan Ren",
      "Yuchen Zheng",
      "Yahao Liu",
      "Wen Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Mr._DETR_Instructive_Multi-Route_Training_for_Detection_Transformers_CVPR_2025_paper.html": {
    "title": "Mr. DETR: Instructive Multi-Route Training for Detection Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang-Bin Zhang",
      "Yujie Zhong",
      "Kai Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking_CVPR_2025_paper.html": {
    "title": "MITracker: Multi-View Integration for Visual Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengjie Xu",
      "Yitao Zhu",
      "Haotian Jiang",
      "Jiaming Li",
      "Zhenrong Shen",
      "Sheng Wang",
      "Haolin Huang",
      "Xinyu Wang",
      "Han Zhang",
      "Qing Yang",
      "Qian Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dou_Hearing_Hands_Generating_Sounds_from_Physical_Interactions_in_3D_Scenes_CVPR_2025_paper.html": {
    "title": "Hearing Hands: Generating Sounds from Physical Interactions in 3D Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Dou",
      "Wonseok Oh",
      "Yuqing Luo",
      "Antonio Loquercio",
      "Andrew Owens"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yao_AirRoom_Objects_Matter_in_Room_Reidentification_CVPR_2025_paper.html": {
    "title": "AirRoom: Objects Matter in Room Reidentification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runmao Yao",
      "Yi Du",
      "Zhuoqun Chen",
      "Haoze Zheng",
      "Chen Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language_CVPR_2025_paper.html": {
    "title": "Not Only Text: Exploring Compositionality of Visual Representations in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davide Berasi",
      "Matteo Farina",
      "Massimiliano Mancini",
      "Elisa Ricci",
      "Nicola Strisciuglio"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_DefMamba_Deformable_Visual_State_Space_Model_CVPR_2025_paper.html": {
    "title": "DefMamba: Deformable Visual State Space Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leiye Liu",
      "Miao Zhang",
      "Jihao Yin",
      "Tingwei Liu",
      "Wei Ji",
      "Yongri Piao",
      "Huchuan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_HOP_Heterogeneous_Topology-based_Multimodal_Entanglement_for_Co-Speech_Gesture_Generation_CVPR_2025_paper.html": {
    "title": "HOP: Heterogeneous Topology-based Multimodal Entanglement for Co-Speech Gesture Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongye Cheng",
      "Tianyu Wang",
      "Guangsi Shi",
      "Zexing Zhao",
      "Yanwei Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_VoxelSplat_Dynamic_Gaussian_Splatting_as_an_Effective_Loss_for_Occupancy_CVPR_2025_paper.html": {
    "title": "VoxelSplat: Dynamic Gaussian Splatting as an Effective Loss for Occupancy and Flow Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyue Zhu",
      "Shenlong Wang",
      "Jin Xie",
      "Jiang-jiang Liu",
      "Jingdong Wang",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Exploring_Scene_Affinity_for_Semi-Supervised_LiDAR_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Exploring Scene Affinity for Semi-Supervised LiDAR Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuandong Liu",
      "Xingxing Weng",
      "Shuguo Jiang",
      "Pengcheng Li",
      "Lei Yu",
      "Gui-Song Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jang_ControlFace_Harnessing_Facial_Parametric_Control_for_Face_Rigging_CVPR_2025_paper.html": {
    "title": "ControlFace: Harnessing Facial Parametric Control for Face Rigging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wooseok Jang",
      "Youngjun Hong",
      "Geonho Cha",
      "Seungryong Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization_CVPR_2025_paper.html": {
    "title": "Minority-Focused Text-to-Image Generation via Prompt Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soobin Um",
      "Jong Chul Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dai_Imputation-free_and_Alignment-free_Incomplete_Multi-view_Clustering_Driven_by_Consensus_Semantic_CVPR_2025_paper.html": {
    "title": "Imputation-free and Alignment-free: Incomplete Multi-view Clustering Driven by Consensus Semantic Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhuo Dai",
      "Jiaqi Jin",
      "Zhibin Dong",
      "Siwei Wang",
      "Xinwang Liu",
      "En Zhu",
      "Xihong Yang",
      "Xinbiao Gan",
      "Yu Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Sensitivity-Aware_Efficient_Fine-Tuning_via_Compact_Dynamic-Rank_Adaptation_CVPR_2025_paper.html": {
    "title": "Sensitivity-Aware Efficient Fine-Tuning via Compact Dynamic-Rank Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianran Chen",
      "Jiarui Chen",
      "Baoquan Zhang",
      "Zhehao Yu",
      "Shidong Chen",
      "Rui Ye",
      "Xutao Li",
      "Yunming Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_MANTA_A_Large-Scale_Multi-View_and_Visual-Text_Anomaly_Detection_Dataset_for_CVPR_2025_paper.html": {
    "title": "MANTA: A Large-Scale Multi-View and Visual-Text Anomaly Detection Dataset for Tiny Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Fan",
      "Dongdong Fan",
      "Zhiguang Hu",
      "Yiwen Ding",
      "Donglin Di",
      "Kai Yi",
      "Maurice Pagnucco",
      "Yang Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kwak_MoDec-GS_Global-to-Local_Motion_Decomposition_and_Temporal_Interval_Adjustment_for_Compact_CVPR_2025_paper.html": {
    "title": "MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangwoon Kwak",
      "Joonsoo Kim",
      "Jun Young Jeong",
      "Won-Sik Cheong",
      "Jihyong Oh",
      "Munchurl Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_DaCapo_Score_Distillation_as_Stacked_Bridge_for_Fast_and_High-quality_CVPR_2025_paper.html": {
    "title": "DaCapo: Score Distillation as Stacked Bridge for Fast and High-quality 3D Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Huang",
      "Bangyan Liao",
      "Yuqi Hu",
      "Haitao Lin",
      "Lirong Wu",
      "Siyuan Li",
      "Cheng Tan",
      "Zicheng Liu",
      "Yunfan Liu",
      "Zelin Zang",
      "Chang Yu",
      "Zhen Lei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_A_Selective_Re-learning_Mechanism_for_Hyperspectral_Fusion_Imaging_CVPR_2025_paper.html": {
    "title": "A Selective Re-learning Mechanism for Hyperspectral Fusion Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanye Liu",
      "Jinyang Liu",
      "Renwei Dian",
      "Shutao Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_SCSegamba_Lightweight_Structure-Aware_Vision_Mamba_for_Crack_Segmentation_in_Structures_CVPR_2025_paper.html": {
    "title": "SCSegamba: Lightweight Structure-Aware Vision Mamba for Crack Segmentation in Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Liu",
      "Chen Jia",
      "Fan Shi",
      "Xu Cheng",
      "Shengyong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Autoregressive_Sequential_Pretraining_for_Visual_Tracking_CVPR_2025_paper.html": {
    "title": "Autoregressive Sequential Pretraining for Visual Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyi Liang",
      "Yifan Bai",
      "Yihong Gong",
      "Xing Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Number_it_Temporal_Grounding_Videos_like_Flipping_Manga_CVPR_2025_paper.html": {
    "title": "Number it: Temporal Grounding Videos like Flipping Manga",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongliang Wu",
      "Xinting Hu",
      "Yuyang Sun",
      "Yizhou Zhou",
      "Wenbo Zhu",
      "Fengyun Rao",
      "Bernt Schiele",
      "Xu Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zu_Collaborative_Tree_Search_for_Enhancing_Embodied_Multi-Agent_Collaboration_CVPR_2025_paper.html": {
    "title": "Collaborative Tree Search for Enhancing Embodied Multi-Agent Collaboration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lizheng Zu",
      "Lin Lin",
      "Song Fu",
      "Na Zhao",
      "Pan Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_PromptHMR_Promptable_Human_Mesh_Recovery_CVPR_2025_paper.html": {
    "title": "PromptHMR: Promptable Human Mesh Recovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufu Wang",
      "Yu Sun",
      "Priyanka Patel",
      "Kostas Daniilidis",
      "Michael J. Black",
      "Muhammed Kocabas"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pallotta_SyncVP_Joint_Diffusion_for_Synchronous_Multi-Modal_Video_Prediction_CVPR_2025_paper.html": {
    "title": "SyncVP: Joint Diffusion for Synchronous Multi-Modal Video Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enrico Pallotta",
      "Sina Mokhtarzadeh Azar",
      "Shuai Li",
      "Olga Zatsarynna",
      "Juergen Gall"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_HUSH_Holistic_Panoramic_3D_Scene_Understanding_using_Spherical_Harmonics_CVPR_2025_paper.html": {
    "title": "HUSH: Holistic Panoramic 3D Scene Understanding using Spherical Harmonics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongsung Lee",
      "Harin Park",
      "Byeong-Uk Lee",
      "Kyungdon Joo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations_CVPR_2025_paper.html": {
    "title": "SkillMimic: Learning Basketball Interaction Skills from Demonstrations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinhuai Wang",
      "Qihan Zhao",
      "Runyi Yu",
      "Hok Wai Tsui",
      "Ailing Zeng",
      "Jing Lin",
      "Zhengyi Luo",
      "Jiwen Yu",
      "Xiu Li",
      "Qifeng Chen",
      "Jian Zhang",
      "Lei Zhang",
      "Ping Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/You_VISTREAM_Improving_Computation_Efficiency_of_Visual_Streaming_Perception_via_Law-of-Charge-Conservation_CVPR_2025_paper.html": {
    "title": "VISTREAM: Improving Computation Efficiency of Visual Streaming Perception via Law-of-Charge-Conservation Inspired Spiking Neural Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kang You",
      "Ziling Wei",
      "Jing Yan",
      "Boning Zhang",
      "Qinghai Guo",
      "Yaoyu Zhang",
      "Zhezhi He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Garg_STPro_Spatial_and_Temporal_Progressive_Learning_for_Weakly_Supervised_Spatio-Temporal_CVPR_2025_paper.html": {
    "title": "STPro: Spatial and Temporal Progressive Learning for Weakly Supervised Spatio-Temporal Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aaryan Garg",
      "Akash Kumar",
      "Yogesh S Rawat"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars_CVPR_2025_paper.html": {
    "title": "RGBAvatar: Reduced Gaussian Blendshapes for Online Modeling of Head Avatars",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linzhou Li",
      "Yumeng Li",
      "Yanlin Weng",
      "Youyi Zheng",
      "Kun Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Donnelly_Rashomon_Sets_for_Prototypical-Part_Networks_Editing_Interpretable_Models_in_Real-Time_CVPR_2025_paper.html": {
    "title": "Rashomon Sets for Prototypical-Part Networks: Editing Interpretable Models in Real-Time",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jon Donnelly",
      "Zhicheng Guo",
      "Alina Jade Barnett",
      "Hayden McTavish",
      "Chaofan Chen",
      "Cynthia Rudin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_EEE-Bench_A_Comprehensive_Multimodal_Electrical_And_Electronics_Engineering_Benchmark_CVPR_2025_paper.html": {
    "title": "EEE-Bench: A Comprehensive Multimodal Electrical And Electronics Engineering Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Li",
      "Jike Zhong",
      "Tianle Chen",
      "Yuxiang Lai",
      "Konstantinos Psounis"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Text-Driven_Fashion_Image_Editing_with_Compositional_Concept_Learning_and_Counterfactual_CVPR_2025_paper.html": {
    "title": "Text-Driven Fashion Image Editing with Compositional Concept Learning and Counterfactual Abduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanshan Huang",
      "Haoxuan Li",
      "Chunyuan Zheng",
      "Mingyuan Ge",
      "Wei Gao",
      "Lei Wang",
      "Li Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_EnvPoser_Environment-aware_Realistic_Human_Motion_Estimation_from_Sparse_Observations_with_CVPR_2025_paper.html": {
    "title": "EnvPoser: Environment-aware Realistic Human Motion Estimation from Sparse Observations with Uncertainty Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songpengcheng Xia",
      "Yu Zhang",
      "Zhuo Su",
      "Xiaozheng Zheng",
      "Zheng Lv",
      "Guidong Wang",
      "Yongjie Zhang",
      "Qi Wu",
      "Lei Chu",
      "Ling Pei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Heidari_A_Unified_Framework_for_Heterogeneous_Semi-supervised_Learning_CVPR_2025_paper.html": {
    "title": "A Unified Framework for Heterogeneous Semi-supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marzi Heidari",
      "Abdullah Alchihabi",
      "Hao Yan",
      "Yuhong Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Adapting_Text-to-Image_Generation_with_Feature_Difference_Instruction_for_Generic_Image_CVPR_2025_paper.html": {
    "title": "Adapting Text-to-Image Generation with Feature Difference Instruction for Generic Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Wang",
      "Hehe Fan",
      "Huichen Yang",
      "Sarvnaz Karimi",
      "Lina Yao",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zha_ReCon_Enhancing_True_Correspondence_Discrimination_through_Relation_Consistency_for_Robust_CVPR_2025_paper.html": {
    "title": "ReCon: Enhancing True Correspondence Discrimination through Relation Consistency for Robust Noisy Correspondence Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quanxing Zha",
      "Xin Liu",
      "Shu-Juan Peng",
      "Yiu-ming Cheung",
      "Xing Xu",
      "Nannan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bao_Free360_Layered_Gaussian_Splatting_for_Unbounded_360-Degree_View_Synthesis_from_CVPR_2025_paper.html": {
    "title": "Free360: Layered Gaussian Splatting for Unbounded 360-Degree View Synthesis from Extremely Sparse and Unposed Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chong Bao",
      "Xiyu Zhang",
      "Zehao Yu",
      "Jiale Shi",
      "Guofeng Zhang",
      "Songyou Peng",
      "Zhaopeng Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks_CVPR_2025_paper.html": {
    "title": "Tuning the Frequencies: Robust Training for Sinusoidal Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiago Novello",
      "Diana Aldana",
      "Andre Araujo",
      "Luiz Velho"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chng_Preconditioners_for_the_Stochastic_Training_of_Neural_Fields_CVPR_2025_paper.html": {
    "title": "Preconditioners for the Stochastic Training of Neural Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shin-Fang Chng",
      "Hemanth Saratchandran",
      "Simon Lucey"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Open_Ad-hoc_Categorization_with_Contextualized_Feature_Learning_CVPR_2025_paper.html": {
    "title": "Open Ad-hoc Categorization with Contextualized Feature Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zilin Wang",
      "Sangwoo Mo",
      "Stella X. Yu",
      "Sima Behpour",
      "Liu Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double_CVPR_2025_paper.html": {
    "title": "Real-time Free-view Human Rendering from Sparse-view RGB Videos using Double Unprojected Textures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoxing Sun",
      "Rishabh Dabral",
      "Heming Zhu",
      "Pascal Fua",
      "Christian Theobalt",
      "Marc Habermann"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dang_ECBench_Can_Multi-modal_Foundation_Models_Understand_the_Egocentric_World_A_CVPR_2025_paper.html": {
    "title": "ECBench: Can Multi-modal Foundation Models Understand the Egocentric World? A Holistic Embodied Cognition Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ronghao Dang",
      "Yuqian Yuan",
      "Wenqi Zhang",
      "Yifei Xin",
      "Boqiang Zhang",
      "Long Li",
      "Liuyi Wang",
      "Qinyang Zeng",
      "Xin Li",
      "Lidong Bing"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ji_SfM-Free_3D_Gaussian_Splatting_via_Hierarchical_Training_CVPR_2025_paper.html": {
    "title": "SfM-Free 3D Gaussian Splatting via Hierarchical Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Ji",
      "Angela Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lavoie_Large_Self-Supervised_Models_Bridge_the_Gap_in_Domain_Adaptive_Object_CVPR_2025_paper.html": {
    "title": "Large Self-Supervised Models Bridge the Gap in Domain Adaptive Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marc-Antoine Lavoie",
      "Anas Mahmoud",
      "Steven L. Waslander"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Dynamic_Updates_for_Language_Adaptation_in_Visual-Language_Tracking_CVPR_2025_paper.html": {
    "title": "Dynamic Updates for Language Adaptation in Visual-Language Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohai Li",
      "Bineng Zhong",
      "Qihua Liang",
      "Zhiyi Mo",
      "Jian Nong",
      "Shuxiang Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Multi-focal_Conditioned_Latent_Diffusion_for_Person_Image_Synthesis_CVPR_2025_paper.html": {
    "title": "Multi-focal Conditioned Latent Diffusion for Person Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Liu",
      "Jichao Zhang",
      "Paolo Rota",
      "Nicu Sebe"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Uncertainty_Meets_Diversity_A_Comprehensive_Active_Learning_Framework_for_Indoor_CVPR_2025_paper.html": {
    "title": "Uncertainty Meets Diversity: A Comprehensive Active Learning Framework for Indoor 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangyi Wang",
      "Na Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design_CVPR_2025_paper.html": {
    "title": "CASAGPT: Cuboid Arrangement and Scene Assembly for Interior Design",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weitao Feng",
      "Hang Zhou",
      "Jing Liao",
      "Li Cheng",
      "Wenbo Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pang_Identity-Clothing_Similarity_Modeling_for_Unsupervised_Clothing_Change_Person_Re-Identification_CVPR_2025_paper.html": {
    "title": "Identity-Clothing Similarity Modeling for Unsupervised Clothing Change Person Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqi Pang",
      "Junjie Wang",
      "Lingling Zhao",
      "Chunyu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mao_Evaluating_Model_Perception_of_Color_Illusions_in_Photorealistic_Scenes_CVPR_2025_paper.html": {
    "title": "Evaluating Model Perception of Color Illusions in Photorealistic Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingjun Mao",
      "Zineng Tang",
      "Alane Suhr"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_MINIMA_Modality_Invariant_Image_Matching_CVPR_2025_paper.html": {
    "title": "MINIMA: Modality Invariant Image Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangwei Ren",
      "Xingyu Jiang",
      "Zizhuo Li",
      "Dingkang Liang",
      "Xin Zhou",
      "Xiang Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_OccMamba_Semantic_Occupancy_Prediction_with_State_Space_Models_CVPR_2025_paper.html": {
    "title": "OccMamba: Semantic Occupancy Prediction with State Space Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heng Li",
      "Yuenan Hou",
      "Xiaohan Xing",
      "Yuexin Ma",
      "Xiao Sun",
      "Yanyong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes_CVPR_2025_paper.html": {
    "title": "3D Convex Splatting: Radiance Field Rendering with 3D Smooth Convexes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Held",
      "Renaud Vandeghen",
      "Abdullah Hamdi",
      "Adrien Deliege",
      "Anthony Cioppa",
      "Silvio Giancola",
      "Andrea Vedaldi",
      "Bernard Ghanem",
      "Marc Van Droogenbroeck"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_3D_Prior_Is_All_You_Need_Cross-Task_Few-shot_2D_Gaze_CVPR_2025_paper.html": {
    "title": "3D Prior Is All You Need: Cross-Task Few-shot 2D Gaze Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihua Cheng",
      "Hengfei Wang",
      "Zhongqun Zhang",
      "Yang Yue",
      "Boeun Kim",
      "Feng Lu",
      "Hyung Jin Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Cheb-GR_Rethinking_K-nearest_Neighbor_Search_in_Re-ranking_for_Person_Re-identification_CVPR_2025_paper.html": {
    "title": "Cheb-GR: Rethinking K-nearest Neighbor Search in Re-ranking for Person Re-identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinxi Yang",
      "He Li",
      "Bo Du",
      "Mang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nekrasov_Spotting_the_Unexpected_STU_A_3D_LiDAR_Dataset_for_Anomaly_CVPR_2025_paper.html": {
    "title": "Spotting the Unexpected (STU): A 3D LiDAR Dataset for Anomaly Segmentation in Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexey Nekrasov",
      "Malcolm Burdorf",
      "Stewart Worrall",
      "Bastian Leibe",
      "Julie Stephany Berrio Perez"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jung_Is_Right_Right_Enhancing_Object_Orientation_Understanding_in_Multimodal_Large_CVPR_2025_paper.html": {
    "title": "Is `Right' Right? Enhancing Object Orientation Understanding in Multimodal Large Language Models through Egocentric Instruction Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ji Hyeok Jung",
      "Eun Tae Kim",
      "Seoyeon Kim",
      "Joo Ho Lee",
      "Bumsoo Kim",
      "Buru Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chang_GCC_Generative_Color_Constancy_via_Diffusing_a_Color_Checker_CVPR_2025_paper.html": {
    "title": "GCC: Generative Color Constancy via Diffusing a Color Checker",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen-Wei Chang",
      "Cheng-De Fan",
      "Chia-Che Chang",
      "Yi-Chen Lo",
      "Yu-Chee Tseng",
      "Jiun-Long Huang",
      "Yu-Lun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Perincherry_Do_Visual_Imaginations_Improve_Vision-and-Language_Navigation_Agents_CVPR_2025_paper.html": {
    "title": "Do Visual Imaginations Improve Vision-and-Language Navigation Agents?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akhil Perincherry",
      "Jacob Krantz",
      "Stefan Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jin_On_Denoising_Walking_Videos_for_Gait_Recognition_CVPR_2025_paper.html": {
    "title": "On Denoising Walking Videos for Gait Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyang Jin",
      "Chao Fan",
      "Jingzhe Ma",
      "Jingkai Zhou",
      "Weihua Chen",
      "Shiqi Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Silva-Rodriguez_Conformal_Prediction_for_Zero-Shot_Models_CVPR_2025_paper.html": {
    "title": "Conformal Prediction for Zero-Shot Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Julio Silva-Rodríguez",
      "Ismail Ben Ayed",
      "Jose Dolz"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_PhysAnimator_Physics-Guided_Generative_Cartoon_Animation_CVPR_2025_paper.html": {
    "title": "PhysAnimator: Physics-Guided Generative Cartoon Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Xie",
      "Yiwei Zhao",
      "Ying Jiang",
      "Chenfanfu Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_SeriesBench_A_Benchmark_for_Narrative-Driven_Drama_Series_Understanding_CVPR_2025_paper.html": {
    "title": "SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenkai Zhang",
      "Yiming Lei",
      "Zeming Liu",
      "Haitao Leng",
      "ShaoGuo Liu",
      "Tingting Gao",
      "Qingjie Liu",
      "Yunhong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Weakly_Supervised_Temporal_Action_Localization_via_Dual-Prior_Collaborative_Learning_Guided_CVPR_2025_paper.html": {
    "title": "Weakly Supervised Temporal Action Localization via Dual-Prior Collaborative Learning Guided by Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quan Zhang",
      "Jinwei Fang",
      "Rui Yuan",
      "Xi Tang",
      "Yuxin Qi",
      "Ke Zhang",
      "Chun Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix_CVPR_2025_paper.html": {
    "title": "FIMA-Q: Post-Training Quantization for Vision Transformers by Fisher Information Matrix Approximation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuguanyu Wu",
      "Shihe Wang",
      "Jiayi Zhang",
      "Jiaxin Chen",
      "Yunhong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition_CVPR_2025_paper.html": {
    "title": "HotSpot: Signed Distance Function Optimization with an Asymptotically Sufficient Condition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zimo Wang",
      "Cheng Wang",
      "Taiki Yoshino",
      "Sirui Tao",
      "Ziyang Fu",
      "Tzu-Mao Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_GliaNet_Adaptive_Neural_Network_Structure_Learning_with_Glia-Driven_CVPR_2025_paper.html": {
    "title": "GliaNet: Adaptive Neural Network Structure Learning with Glia-Driven",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengqiao Han",
      "Liyuan Pan",
      "Xiabi Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_BACON_Improving_Clarity_of_Image_Captions_via_Bag-of-Concept_Graphs_CVPR_2025_paper.html": {
    "title": "BACON: Improving Clarity of Image Captions via Bag-of-Concept Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhantao Yang",
      "Ruili Feng",
      "Keyu Yan",
      "Huangji Wang",
      "Zhicai Wang",
      "Shangwen Zhu",
      "Han Zhang",
      "Jie Xiao",
      "Pingyu Wu",
      "Kai Zhu",
      "Jixuan Chen",
      "Chen-Wei Xie",
      "Yue Yang",
      "Hongyang Zhang",
      "Yu Liu",
      "Fan Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_EntitySAM_Segment_Everything_in_Video_CVPR_2025_paper.html": {
    "title": "EntitySAM: Segment Everything in Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingqiao Ye",
      "Seoung Wug Oh",
      "Lei Ke",
      "Joon-Young Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tong_GS-2DGS_Geometrically_Supervised_2DGS_for_Reflective_Object_Reconstruction_CVPR_2025_paper.html": {
    "title": "GS-2DGS: Geometrically Supervised 2DGS for Reflective Object Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinguang Tong",
      "Xuesong Li",
      "Fahira Afzal Maken",
      "Sundaram Muthu",
      "Lars Petersson",
      "Chuong Nguyen",
      "Hongdong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Libra-Merging_Importance-redundancy_and_Pruning-merging_Trade-off_for_Acceleration_Plug-in_in_Large_CVPR_2025_paper.html": {
    "title": "Libra-Merging: Importance-redundancy and Pruning-merging Trade-off for Acceleration Plug-in in Large Vision-Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longrong Yang",
      "Dong Shen",
      "Chaoxiang Cai",
      "Kaibing Chen",
      "Fan Yang",
      "Tingting Gao",
      "Di Zhang",
      "Xi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VasTSD_Learning_3D_Vascular_Tree-state_Space_Diffusion_Model_for_Angiography_CVPR_2025_paper.html": {
    "title": "VasTSD: Learning 3D Vascular Tree-state Space Diffusion Model for Angiography Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhifeng Wang",
      "Renjiao Yi",
      "Xin Wen",
      "Chenyang Zhu",
      "Kai Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_PanSplat_4K_Panorama_Synthesis_with_Feed-Forward_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Zhang",
      "Haofei Xu",
      "Qianyi Wu",
      "Camilo Cruz Gambardella",
      "Dinh Phung",
      "Jianfei Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_WISNet_Pseudo_Label_Generation_on_Unbalanced_and_Patch_Annotated_Waste_CVPR_2025_paper.html": {
    "title": "WISNet: Pseudo Label Generation on Unbalanced and Patch Annotated Waste Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shifan Zhang",
      "Hongzi Zhu",
      "Yinan He",
      "Minyi Guo",
      "Ziyang Lou",
      "Shan Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ruiz-Ponce_MixerMDM_Learnable_Composition_of_Human_Motion_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "MixerMDM: Learnable Composition of Human Motion Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pablo Ruiz-Ponce",
      "German Barquero",
      "Cristina Palmero",
      "Sergio Escalera",
      "José García-Rodríguez"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Hand-held_Object_Reconstruction_from_RGB_Video_with_Dynamic_Interaction_CVPR_2025_paper.html": {
    "title": "Hand-held Object Reconstruction from RGB Video with Dynamic Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijian Jiang",
      "Qi Ye",
      "Rengan Xie",
      "Yuchi Huo",
      "Jiming Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_LEDiff_Latent_Exposure_Diffusion_for_HDR_Generation_CVPR_2025_paper.html": {
    "title": "LEDiff: Latent Exposure Diffusion for HDR Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Wang",
      "Zhihao Xia",
      "Thomas Leimkuhler",
      "Karol Myszkowski",
      "Xuaner Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos_CVPR_2025_paper.html": {
    "title": "Video Depth Anything: Consistent Depth Estimation for Super-Long Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sili Chen",
      "Hengkai Guo",
      "Shengnan Zhu",
      "Feihu Zhang",
      "Zilong Huang",
      "Jiashi Feng",
      "Bingyi Kang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_VideoAutoArena_An_Automated_Arena_for_Evaluating_Large_Multimodal_Models_in_CVPR_2025_paper.html": {
    "title": "VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Luo",
      "Haoning Wu",
      "Dongxu Li",
      "Jing Ma",
      "Mohan Kankanhalli",
      "Junnan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_InstanceCap_Improving_Text-to-Video_Generation_via_Instance-aware_Structured_Caption_CVPR_2025_paper.html": {
    "title": "InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiehan Fan",
      "Kepan Nan",
      "Rui Xie",
      "Penghao Zhou",
      "Zhenheng Yang",
      "Chaoyou Fu",
      "Xiang Li",
      "Jian Yang",
      "Ying Tai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guan_AudCast_Audio-Driven_Human_Video_Generation_by_Cascaded_Diffusion_Transformers_CVPR_2025_paper.html": {
    "title": "AudCast: Audio-Driven Human Video Generation by Cascaded Diffusion Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiazhi Guan",
      "Kaisiyuan Wang",
      "Zhiliang Xu",
      "Quanwei Yang",
      "Yasheng Sun",
      "Shengyi He",
      "Borong Liang",
      "Yukang Cao",
      "Yingying Li",
      "Haocheng Feng",
      "Errui Ding",
      "Jingdong Wang",
      "Youjian Zhao",
      "Hang Zhou",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cui_Luminance-GS_Adapting_3D_Gaussian_Splatting_to_Challenging_Lighting_Conditions_with_CVPR_2025_paper.html": {
    "title": "Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting Conditions with View-Adaptive Curve Adjustment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziteng Cui",
      "Xuangeng Chu",
      "Tatsuya Harada"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yura_EventSplat_3D_Gaussian_Splatting_from_Moving_Event_Cameras_for_Real-time_CVPR_2025_paper.html": {
    "title": "EventSplat: 3D Gaussian Splatting from Moving Event Cameras for Real-time Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Toshiya Yura",
      "Ashkan Mirzaei",
      "Igor Gilitschenski"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember_CVPR_2025_paper.html": {
    "title": "Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihan Yang",
      "Shusheng Yang",
      "Anjali W. Gupta",
      "Rilyn Han",
      "Li Fei-Fei",
      "Saining Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_3D_Student_Splatting_and_Scooping_CVPR_2025_paper.html": {
    "title": "3D Student Splatting and Scooping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialin Zhu",
      "Jiangbei Yue",
      "Feixiang He",
      "He Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling_CVPR_2025_paper.html": {
    "title": "World-consistent Video Diffusion with Explicit 3D Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihang Zhang",
      "Shuangfei Zhai",
      "Miguel Ángel Bautista Martin",
      "Kevin Miao",
      "Alexander Toshev",
      "Joshua Susskind",
      "Jiatao Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_A_Stitch_in_Time_Saves_Nine_Small_VLM_is_a_CVPR_2025_paper.html": {
    "title": "A Stitch in Time Saves Nine: Small VLM is a Precise Guidance for Accelerating Large VLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wangbo Zhao",
      "Yizeng Han",
      "Jiasheng Tang",
      "Zhikai Li",
      "Yibing Song",
      "Kai Wang",
      "Zhangyang Wang",
      "Yang You"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guizilini_Zero-Shot_Novel_View_and_Depth_Synthesis_with_Multi-View_Geometric_Diffusion_CVPR_2025_paper.html": {
    "title": "Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vitor Guizilini",
      "Muhammad Zubair Irshad",
      "Dian Chen",
      "Greg Shakhnarovich",
      "Rares Ambrus"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bi_Unveiling_Visual_Perception_in_Language_Models_An_Attention_Head_Analysis_CVPR_2025_paper.html": {
    "title": "Unveiling Visual Perception in Language Models: An Attention Head Analysis Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Bi",
      "Junjia Guo",
      "Yunlong Tang",
      "Lianggong Bruce Wen",
      "Zhang Liu",
      "Bingjie Wang",
      "Chenliang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_SemanticDraw_Towards_Real-Time_Interactive_Content_Creation_from_Image_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaerin Lee",
      "Daniel Sungho Jung",
      "Kanggeon Lee",
      "Kyoung Mu Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Basak_SemiDAViL_Semi-supervised_Domain_Adaptation_with_Vision-Language_Guidance_for_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "SemiDAViL: Semi-supervised Domain Adaptation with Vision-Language Guidance for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hritam Basak",
      "Zhaozheng Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ruan_Learning_Partonomic_3D_Reconstruction_from_Image_Collections_CVPR_2025_paper.html": {
    "title": "Learning Partonomic 3D Reconstruction from Image Collections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoqian Ruan",
      "Pei Yu",
      "Dian Jia",
      "Hyeonjeong Park",
      "Peixi Xiong",
      "Wei Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gerogiannis_Arc2Avatar_Generating_Expressive_3D_Avatars_from_a_Single_Image_via_CVPR_2025_paper.html": {
    "title": "Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimitrios Gerogiannis",
      "Foivos Paraperas Papantoniou",
      "Rolandos Alexandros Potamias",
      "Alexandros Lattas",
      "Stefanos Zafeiriou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ryu_Seeing_Speech_and_Sound_Distinguishing_and_Locating_Audio_Sources_in_CVPR_2025_paper.html": {
    "title": "Seeing Speech and Sound: Distinguishing and Locating Audio Sources in Visual Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeonggon Ryu",
      "Seongyu Kim",
      "Joon Son Chung",
      "Arda Senocak"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kaneko_Structure_from_Collision_CVPR_2025_paper.html": {
    "title": "Structure from Collision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takuhiro Kaneko"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_ODA-GAN_Orthogonal_Decoupling_Alignment_GAN_Assisted_by_Weakly-supervised_Learning_for_CVPR_2025_paper.html": {
    "title": "ODA-GAN: Orthogonal Decoupling Alignment GAN Assisted by Weakly-supervised Learning for Virtual Immunohistochemistry Staining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Wang",
      "Mingkang Wang",
      "Zhongze Wang",
      "Hongkai Wang",
      "Qi Xu",
      "Fengyu Cong",
      "Hongming Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_EVOS_Efficient_Implicit_Neural_Training_via_EVOlutionary_Selector_CVPR_2025_paper.html": {
    "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weixiang Zhang",
      "Shuzhao Xie",
      "Chengwei Ren",
      "Siyi Xie",
      "Chen Tang",
      "Shijia Ge",
      "Mingzi Wang",
      "Zhi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Du_Crab_A_Unified_Audio-Visual_Scene_Understanding_Model_with_Explicit_Cooperation_CVPR_2025_paper.html": {
    "title": "Crab: A Unified Audio-Visual Scene Understanding Model with Explicit Cooperation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Henghui Du",
      "Guangyao Li",
      "Chang Zhou",
      "Chunjie Zhang",
      "Alan Zhao",
      "Di Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Nullu_Mitigating_Object_Hallucinations_in_Large_Vision-Language_Models_via_HalluSpace_CVPR_2025_paper.html": {
    "title": "Nullu: Mitigating Object Hallucinations in Large Vision-Language Models via HalluSpace Projection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Le Yang",
      "Ziwei Zheng",
      "Boxu Chen",
      "Zhengyu Zhao",
      "Chenhao Lin",
      "Chao Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_OralXrays-9_Towards_Hospital-Scale_Panoramic_X-ray_Anomaly_Detection_via_Personalized_Multi-Object_CVPR_2025_paper.html": {
    "title": "OralXrays-9: Towards Hospital-Scale Panoramic X-ray Anomaly Detection via Personalized Multi-Object Query-Aware Mining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingzhi Chen",
      "Sisi Fu",
      "Xiaocheng Fang",
      "Jieyi Cai",
      "Boya Zhang",
      "Minhua Lu",
      "Yishu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_MEET_Towards_Memory-Efficient_Temporal_Sparse_Deep_Neural_Networks_CVPR_2025_paper.html": {
    "title": "MEET: Towards Memory-Efficient Temporal Sparse Deep Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeqi Zhu",
      "Ibrahim Batuhan Akkaya",
      "Luc Waeijen",
      "Egor Bondarev",
      "Arash Pourtaherian",
      "Orlando Moreira"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hess_SplatAD_Real-Time_Lidar_and_Camera_Rendering_with_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "SplatAD: Real-Time Lidar and Camera Rendering with 3D Gaussian Splatting for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Georg Hess",
      "Carl Lindström",
      "Maryam Fatemi",
      "Christoffer Petersson",
      "Lennart Svensson"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Audio-Visual_Instance_Segmentation_CVPR_2025_paper.html": {
    "title": "Audio-Visual Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruohao Guo",
      "Xianghua Ying",
      "Yaru Chen",
      "Dantong Niu",
      "Guangyao Li",
      "Liao Qu",
      "Yanyu Qi",
      "Jinxing Zhou",
      "Bowei Xing",
      "Wenzhen Yue",
      "Ji Shi",
      "Qixun Wang",
      "Peiliang Zhang",
      "Buwen Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rao_Probabilistic_Prompt_Distribution_Learning_for_Animal_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "Probabilistic Prompt Distribution Learning for Animal Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiyong Rao",
      "Brian Nlong Zhao",
      "Yu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_dFLMoE_Decentralized_Federated_Learning_via_Mixture_of_Experts_for_Medical_CVPR_2025_paper.html": {
    "title": "dFLMoE: Decentralized Federated Learning via Mixture of Experts for Medical Data Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luyuan Xie",
      "Tianyu Luan",
      "Wenyuan Cai",
      "Guochen Yan",
      "Zhaoyu Chen",
      "Nan Xi",
      "Yuejian Fang",
      "Qingni Shen",
      "Zhonghai Wu",
      "Junsong Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton_CVPR_2025_paper.html": {
    "title": "Reconstructing Humans with a Biomechanically Accurate Skeleton",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Xia",
      "Xiaowei Zhou",
      "Etienne Vouga",
      "Qixing Huang",
      "Georgios Pavlakos"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory_CVPR_2025_paper.html": {
    "title": "AdaCM^2: On Understanding Extremely Long-Term Video with Adaptive Cross-Modality Memory Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanbin Man",
      "Ying Huang",
      "Chengming Zhang",
      "Bingzhe Li",
      "Wei Niu",
      "Miao Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/An_Mitigating_Object_Hallucinations_in_Large_Vision-Language_Models_with_Assembly_of_CVPR_2025_paper.html": {
    "title": "Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbin An",
      "Feng Tian",
      "Sicong Leng",
      "Jiahao Nie",
      "Haonan Lin",
      "Qianying Wang",
      "Ping Chen",
      "Xiaoqin Zhang",
      "Shijian Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VGGT_Visual_Geometry_Grounded_Transformer_CVPR_2025_paper.html": {
    "title": "VGGT: Visual Geometry Grounded Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianyuan Wang",
      "Minghao Chen",
      "Nikita Karaev",
      "Andrea Vedaldi",
      "Christian Rupprecht",
      "David Novotny"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jang_Silent_Branding_Attack_Trigger-free_Data_Poisoning_Attack_on_Text-to-Image_Diffusion_CVPR_2025_paper.html": {
    "title": "Silent Branding Attack: Trigger-free Data Poisoning Attack on Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangwon Jang",
      "June Suk Choi",
      "Jaehyeong Jo",
      "Kimin Lee",
      "Sung Ju Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_UniSTD_Towards_Unified_Spatio-Temporal_Learning_across_Diverse_Disciplines_CVPR_2025_paper.html": {
    "title": "UniSTD: Towards Unified Spatio-Temporal Learning across Diverse Disciplines",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Tang",
      "Xinzhu Ma",
      "Encheng Su",
      "Xiufeng Song",
      "Xiaohong Liu",
      "Wei-Hong Li",
      "Lei Bai",
      "Wanli Ouyang",
      "Xiangyu Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Visual_Consensus_Prompting_for_Co-Salient_Object_Detection_CVPR_2025_paper.html": {
    "title": "Visual Consensus Prompting for Co-Salient Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Wang",
      "Nana Yu",
      "Zihao Zhang",
      "Yahong Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_Mani-GS_Gaussian_Splatting_Manipulation_with_Triangular_Mesh_CVPR_2025_paper.html": {
    "title": "Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangjun Gao",
      "Xiaoyu Li",
      "Yiyu Zhuang",
      "Qi Zhang",
      "Wenbo Hu",
      "Chaopeng Zhang",
      "Yao Yao",
      "Ying Shan",
      "Long Quan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_UniHOPE_A_Unified_Approach_for_Hand-Only_and_Hand-Object_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "UniHOPE: A Unified Approach for Hand-Only and Hand-Object Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinqiao Wang",
      "Hao Xu",
      "Pheng-Ann Heng",
      "Chi-Wing Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gadot_RL-RC-DoT_A_Block-level_RL_agent_for_Task-Aware_Video_Compression_CVPR_2025_paper.html": {
    "title": "RL-RC-DoT: A Block-level RL agent for Task-Aware Video Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uri Gadot",
      "Assaf Shocher",
      "Shie Mannor",
      "Gal Chechik",
      "Assaf Hallak"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_Quantization_without_Tears_CVPR_2025_paper.html": {
    "title": "Quantization without Tears",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghao Fu",
      "Hao Yu",
      "Jie Shao",
      "Junjie Zhou",
      "Ke Zhu",
      "Jianxin Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_PHGC_Procedural_Heterogeneous_Graph_Completion_for_Natural_Language_Task_Verification_CVPR_2025_paper.html": {
    "title": "PHGC: Procedural Heterogeneous Graph Completion for Natural Language Task Verification in Egocentric Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xun Jiang",
      "Zhiyi Huang",
      "Xing Xu",
      "Jingkuan Song",
      "Fumin Shen",
      "Heng Tao Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_Recognition-Synergistic_Scene_Text_Editing_CVPR_2025_paper.html": {
    "title": "Recognition-Synergistic Scene Text Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyao Fang",
      "Pengyuan Lyu",
      "Jingjing Wu",
      "Chengquan Zhang",
      "Jun Yu",
      "Guangming Lu",
      "Wenjie Pei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qin_Towards_Consistent_Multi-Task_Learning_Unlocking_the_Potential_of_Task-Specific_Parameters_CVPR_2025_paper.html": {
    "title": "Towards Consistent Multi-Task Learning: Unlocking the Potential of Task-Specific Parameters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohan Qin",
      "Xiaoxing Wang",
      "Junchi Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_WildAvatar_Learning_In-the-wild_3D_Avatars_from_the_Web_CVPR_2025_paper.html": {
    "title": "WildAvatar: Learning In-the-wild 3D Avatars from the Web",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Huang",
      "Shoukang Hu",
      "Guangcong Wang",
      "Tianqi Liu",
      "Yuhang Zang",
      "Zhiguo Cao",
      "Wei Li",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_BooW-VTON_Boosting_In-the-Wild_Virtual_Try-On_via_Mask-Free_Pseudo_Data_Training_CVPR_2025_paper.html": {
    "title": "BooW-VTON: Boosting In-the-Wild Virtual Try-On via Mask-Free Pseudo Data Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanpu Zhang",
      "Dan Song",
      "Pengxin Zhan",
      "Tianyu Chang",
      "Jianhao Zeng",
      "Qingguo Chen",
      "Weihua Luo",
      "An-An Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_Rectified_Diffusion_Guidance_for_Conditional_Generation_CVPR_2025_paper.html": {
    "title": "Rectified Diffusion Guidance for Conditional Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengfei Xia",
      "Nan Xue",
      "Yujun Shen",
      "Ran Yi",
      "Tieliang Gong",
      "Yong-Jin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bokhovkin_SceneFactor_Factored_Latent_3D_Diffusion_for_Controllable_3D_Scene_Generation_CVPR_2025_paper.html": {
    "title": "SceneFactor: Factored Latent 3D Diffusion for Controllable 3D Scene Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aleksey Bokhovkin",
      "Quan Meng",
      "Shubham Tulsiani",
      "Angela Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_HiFi-Portrait_Zero-shot_Identity-preserved_Portrait_Generation_with_High-fidelity_Multi-face_Fusion_CVPR_2025_paper.html": {
    "title": "HiFi-Portrait: Zero-shot Identity-preserved Portrait Generation with High-fidelity Multi-face Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifang Xu",
      "Benxiang Zhai",
      "Yunzhuo Sun",
      "Ming Li",
      "Yang Li",
      "Sidan Du"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_IAAO_Interactive_Affordance_Learning_for_Articulated_Objects_in_3D_Environments_CVPR_2025_paper.html": {
    "title": "IAAO: Interactive Affordance Learning for Articulated Objects in 3D Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Can Zhang",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jin_FloVD_Optical_Flow_Meets_Video_Diffusion_Model_for_Enhanced_Camera-Controlled_CVPR_2025_paper.html": {
    "title": "FloVD: Optical Flow Meets Video Diffusion Model for Enhanced Camera-Controlled Video Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonjoon Jin",
      "Qi Dai",
      "Chong Luo",
      "Seung-Hwan Baek",
      "Sunghyun Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_RAD_Region-Aware_Diffusion_Models_for_Image_Inpainting_CVPR_2025_paper.html": {
    "title": "RAD: Region-Aware Diffusion Models for Image Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sora Kim",
      "Sungho Suh",
      "Minsik Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ding_RaSS_Improving_Denoising_Diffusion_Samplers_with_Reinforced_Active_Sampling_Scheduler_CVPR_2025_paper.html": {
    "title": "RaSS: Improving Denoising Diffusion Samplers with Reinforced Active Sampling Scheduler",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Ding",
      "Lei Yu",
      "Xin Li",
      "Zhijun Tu",
      "Hanting Chen",
      "Jie Hu",
      "Zhibo Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion_CVPR_2025_paper.html": {
    "title": "Supervising Sound Localization by In-the-wild Egomotion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna Min",
      "Ziyang Chen",
      "Hang Zhao",
      "Andrew Owens"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_AutoLUT_LUT-Based_Image_Super-Resolution_with_Automatic_Sampling_and_Adaptive_Residual_CVPR_2025_paper.html": {
    "title": "AutoLUT: LUT-Based Image Super-Resolution with Automatic Sampling and Adaptive Residual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuheng Xu",
      "Shijie Yang",
      "Xin Liu",
      "Jie Liu",
      "Jie Tang",
      "Gangshan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peng_Understanding_Fine-tuning_CLIP_for_Open-vocabulary_Semantic_Segmentation_in_Hyperbolic_Space_CVPR_2025_paper.html": {
    "title": "Understanding Fine-tuning CLIP for Open-vocabulary Semantic Segmentation in Hyperbolic Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zelin Peng",
      "Zhengqin Xu",
      "Zhilin Zeng",
      "Changsong Wen",
      "Yu Huang",
      "Menglin Yang",
      "Feilong Tang",
      "Wei Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiong_TexGaussian_Generating_High-quality_PBR_Material_via_Octree-based_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "TexGaussian: Generating High-quality PBR Material via Octree-based 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bojun Xiong",
      "Jialun Liu",
      "Jiakui Hu",
      "Chenming Wu",
      "Jinbo Wu",
      "Xing Liu",
      "Chen Zhao",
      "Errui Ding",
      "Zhouhui Lian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mao_OSV_One_Step_is_Enough_for_High-Quality_Image_to_Video_CVPR_2025_paper.html": {
    "title": "OSV: One Step is Enough for High-Quality Image to Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofeng Mao",
      "Zhengkai Jiang",
      "Fu-yun Wang",
      "Jiangning Zhang",
      "Hao Chen",
      "Mingmin Chi",
      "Yabiao Wang",
      "Wenhan Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Duan_Fuzzy_Multimodal_Learning_for_Trusted_Cross-modal_Retrieval_CVPR_2025_paper.html": {
    "title": "Fuzzy Multimodal Learning for Trusted Cross-modal Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Duan",
      "Yuan Sun",
      "Dezhong Peng",
      "Zheng Liu",
      "Xiaomin Song",
      "Peng Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qiu_AniGS_Animatable_Gaussian_Avatar_from_a_Single_Image_with_Inconsistent_CVPR_2025_paper.html": {
    "title": "AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent Gaussian Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingteng Qiu",
      "Shenhao Zhu",
      "Qi Zuo",
      "Xiaodong Gu",
      "Yuan Dong",
      "Junfei Zhang",
      "Chao Xu",
      "Zhe Li",
      "Weihao Yuan",
      "Liefeng Bo",
      "Guanying Chen",
      "Zilong Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_GUI-Xplore_Empowering_Generalizable_GUI_Agents_with_One_Exploration_CVPR_2025_paper.html": {
    "title": "GUI-Xplore: Empowering Generalizable GUI Agents with One Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Sun",
      "Shanhui Zhao",
      "Tao Yu",
      "Hao Wen",
      "Samith Va",
      "Mengwei Xu",
      "Yuanchun Li",
      "Chongyang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Few-Shot_Recognition_via_Stage-Wise_Retrieval-Augmented_Finetuning_CVPR_2025_paper.html": {
    "title": "Few-Shot Recognition via Stage-Wise Retrieval-Augmented Finetuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Liu",
      "Huixin Zhang",
      "Shubham Parashar",
      "Shu Kong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Concept_Replacer_Replacing_Sensitive_Concepts_in_Diffusion_Models_via_Precision_CVPR_2025_paper.html": {
    "title": "Concept Replacer: Replacing Sensitive Concepts in Diffusion Models via Precision Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingyun Zhang",
      "Yu Xie",
      "Yanwei Fu",
      "Ping Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bai_A_Regularization-Guided_Equivariant_Approach_for_Image_Restoration_CVPR_2025_paper.html": {
    "title": "A Regularization-Guided Equivariant Approach for Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulu Bai",
      "Jiahong Fu",
      "Qi Xie",
      "Deyu Meng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qiao_RestorGS_Depth-aware_Gaussian_Splatting_for_Efficient_3D_Scene_Restoration_CVPR_2025_paper.html": {
    "title": "RestorGS: Depth-aware Gaussian Splatting for Efficient 3D Scene Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanjian Qiao",
      "Mingwen Shao",
      "Lingzhuang Meng",
      "Kai Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_IM-Portrait_Learning_3D-aware_Video_Diffusion_for_Photorealistic_Talking_Heads_from_CVPR_2025_paper.html": {
    "title": "IM-Portrait: Learning 3D-aware Video Diffusion for Photorealistic Talking Heads from Monocular VideosC",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Li",
      "Ziqian Bai",
      "Feitong Tan",
      "Zhaopeng Cui",
      "Sean Fanello",
      "Yinda Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN_CVPR_2025_paper.html": {
    "title": "Deep Fair Multi-View Clustering with Attention KAN",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "HaiMing Xu",
      "Qianqian Wang",
      "Boyue Wang",
      "Quanxue Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_LineArt_A_Knowledge-guided_Training-free_High-quality_Appearance_Transfer_for_Design_Drawing_CVPR_2025_paper.html": {
    "title": "LineArt: A Knowledge-guided Training-free High-quality Appearance Transfer for Design Drawing with Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Wang",
      "Hongzhen Li",
      "Heng Fang",
      "Yichen Peng",
      "Haoran Xie",
      "Xi Yang",
      "Chuntao Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion_CVPR_2025_paper.html": {
    "title": "4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoyang Wang",
      "Peiye Zhuang",
      "Tuan Duc Ngo",
      "Willi Menapace",
      "Aliaksandr Siarohin",
      "Michael Vasilkovsky",
      "Ivan Skorokhodov",
      "Sergey Tulyakov",
      "Peter Wonka",
      "Hsin-Ying Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kumar_DynaMoDe-NeRF_Motion-aware_Deblurring_Neural_Radiance_Field_for_Dynamic_Scenes_CVPR_2025_paper.html": {
    "title": "DynaMoDe-NeRF: Motion-aware Deblurring Neural Radiance Field for Dynamic Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashish Kumar",
      "Rajagopalan A. N."
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_VideoICL_Confidence-based_Iterative_In-context_Learning_for_Out-of-Distribution_Video_Understanding_CVPR_2025_paper.html": {
    "title": "VideoICL: Confidence-based Iterative In-context Learning for Out-of-Distribution Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangsan Kim",
      "Geon Park",
      "Youngwan Lee",
      "Woongyeong Yeo",
      "Sung Ju Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Garber_Zero-Shot_Image_Restoration_Using_Few-Step_Guidance_of_Consistency_Models_and_CVPR_2025_paper.html": {
    "title": "Zero-Shot Image Restoration Using Few-Step Guidance of Consistency Models (and Beyond)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomer Garber",
      "Tom Tirer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xue_Similarity-Guided_Layer-Adaptive_Vision_Transformer_for_UAV_Tracking_CVPR_2025_paper.html": {
    "title": "Similarity-Guided Layer-Adaptive Vision Transformer for UAV Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaocan Xue",
      "Bineng Zhong",
      "Qihua Liang",
      "Yaozong Zheng",
      "Ning Li",
      "Yuanliang Xue",
      "Shuxiang Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_LidarGait_Learning_Local_Features_and_Size_Awareness_from_LiDAR_Point_CVPR_2025_paper.html": {
    "title": "LidarGait++: Learning Local Features and Size Awareness from LiDAR Point Clouds for 3D Gait Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanfu Shen",
      "Rui Wang",
      "Lixin Duan",
      "Shiqi Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_UrbanCAD_Towards_Highly_Controllable_and_Photorealistic_3D_Vehicles_for_Urban_CVPR_2025_paper.html": {
    "title": "UrbanCAD: Towards Highly Controllable and Photorealistic 3D Vehicles for Urban Scene Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichong Lu",
      "Yichi Cai",
      "Shangzhan Zhang",
      "Hongyu Zhou",
      "Haoji Hu",
      "Huimin Yu",
      "Andreas Geiger",
      "Yiyi Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Coarse_Correspondences_Boost_Spatial-Temporal_Reasoning_in_Multimodal_Language_Model_CVPR_2025_paper.html": {
    "title": "Coarse Correspondences Boost Spatial-Temporal Reasoning in Multimodal Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benlin Liu",
      "Yuhao Dong",
      "Yiqin Wang",
      "Zixian Ma",
      "Yansong Tang",
      "Luming Tang",
      "Yongming Rao",
      "Wei-Chiu Ma",
      "Ranjay Krishna"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jin_Diff-Palm_Realistic_Palmprint_Generation_with_Polynomial_Creases_and_Intra-Class_Variation_CVPR_2025_paper.html": {
    "title": "Diff-Palm: Realistic Palmprint Generation with Polynomial Creases and Intra-Class Variation Controllable Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianlong Jin",
      "Chenglong Zhao",
      "Ruixin Zhang",
      "Sheng Shang",
      "Jianqing Xu",
      "Jingyun Zhang",
      "ShaoMing Wang",
      "Yang Zhao",
      "Shouhong Ding",
      "Wei Jia",
      "Yunsheng Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wen_FoundationStereo_Zero-Shot_Stereo_Matching_CVPR_2025_paper.html": {
    "title": "FoundationStereo: Zero-Shot Stereo Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Wen",
      "Matthew Trepte",
      "Joseph Aribido",
      "Jan Kautz",
      "Orazio Gallo",
      "Stan Birchfield"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_Z-Magic_Zero-shot_Multiple_Attributes_Guided_Image_Creator_CVPR_2025_paper.html": {
    "title": "Z-Magic: Zero-shot Multiple Attributes Guided Image Creator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingying Deng",
      "Xiangyu He",
      "Fan Tang",
      "Weiming Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wei_UniNet_A_Contrastive_Learning-guided_Unified_Framework_with_Feature_Selection_for_CVPR_2025_paper.html": {
    "title": "UniNet: A Contrastive Learning-guided Unified Framework with Feature Selection for Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shun Wei",
      "Jielin Jiang",
      "Xiaolong Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_Chain_of_Semantics_Programming_in_3D_Gaussian_Splatting_Representation_for_CVPR_2025_paper.html": {
    "title": "Chain of Semantics Programming in 3D Gaussian Splatting Representation for 3D Vision Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Shi",
      "Mingyue Xiang",
      "Hao Sun",
      "Yixuan Huang",
      "Zhi Weng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tong_On_the_Zero-shot_Adversarial_Robustness_of_Vision-Language_Models_A_Truly_CVPR_2025_paper.html": {
    "title": "On the Zero-shot Adversarial Robustness of Vision-Language Models: A Truly Zero-shot and Training-free Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baoshun Tong",
      "Hanjiang Lai",
      "Yan Pan",
      "Jian Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Towards_General_Visual-Linguistic_Face_Forgery_Detection_CVPR_2025_paper.html": {
    "title": "Towards General Visual-Linguistic Face Forgery Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Sun",
      "Shen Chen",
      "Taiping Yao",
      "Ziyin Zhou",
      "Jiayi Ji",
      "Xiaoshuai Sun",
      "Chia-Wen Lin",
      "Rongrong Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Movie_Weaver_Tuning-Free_Multi-Concept_Video_Personalization_with_Anchored_Prompts_CVPR_2025_paper.html": {
    "title": "Movie Weaver: Tuning-Free Multi-Concept Video Personalization with Anchored Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Liang",
      "Haoyu Ma",
      "Zecheng He",
      "Tingbo Hou",
      "Ji Hou",
      "Kunpeng Li",
      "Xiaoliang Dai",
      "Felix Juefei-Xu",
      "Samaneh Azadi",
      "Animesh Sinha",
      "Peizhao Zhang",
      "Peter Vajda",
      "Diana Marculescu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Geng_LongVALE_Vision-Audio-Language-Event_Benchmark_Towards_Time-Aware_Omni-Modal_Perception_of_Long_Videos_CVPR_2025_paper.html": {
    "title": "LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware Omni-Modal Perception of Long Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiantian Geng",
      "Jinrui Zhang",
      "Qingni Wang",
      "Teng Wang",
      "Jinming Duan",
      "Feng Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_MVPortrait_Text-Guided_Motion_and_Emotion_Control_for_Multi-view_Vivid_Portrait_CVPR_2025_paper.html": {
    "title": "MVPortrait: Text-Guided Motion and Emotion Control for Multi-view Vivid Portrait Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yukang Lin",
      "Hokit Fung",
      "Jianjin Xu",
      "Zeping Ren",
      "Adela S.M. Lau",
      "Guosheng Yin",
      "Xiu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_MoEdit_On_Learning_Quantity_Perception_for_Multi-object_Image_Editing_CVPR_2025_paper.html": {
    "title": "MoEdit: On Learning Quantity Perception for Multi-object Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanfeng Li",
      "Kahou Chan",
      "Yue Sun",
      "Chantong Lam",
      "Tong Tong",
      "Zitong Yu",
      "Keren Fu",
      "Xiaohong Liu",
      "Tao Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models_CVPR_2025_paper.html": {
    "title": "Seeing More with Less: Human-like Representations in Vision Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrey Gizdov",
      "Shimon Ullman",
      "Daniel Harari"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification_CVPR_2025_paper.html": {
    "title": "Modeling Thousands of Human Annotators for Generalizable Text-to-Image Person Re-identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayu Jiang",
      "Changxing Ding",
      "Wentao Tan",
      "Junhong Wang",
      "Jin Tao",
      "Xiangmin Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Accelerating_Multimodal_Large_Language_Models_by_Searching_Optimal_Vision_Token_CVPR_2025_paper.html": {
    "title": "Accelerating Multimodal Large Language Models by Searching Optimal Vision Token Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyu Zhao",
      "Zhenting Wang",
      "Felix Juefei-Xu",
      "Xide Xia",
      "Miao Liu",
      "Xiaofang Wang",
      "Mingfu Liang",
      "Ning Zhang",
      "Dimitris N. Metaxas",
      "Licheng Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Safari_Matrix-Free_Shared_Intrinsics_Bundle_Adjustment_CVPR_2025_paper.html": {
    "title": "Matrix-Free Shared Intrinsics Bundle Adjustment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Safari"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_AeroGen_Enhancing_Remote_Sensing_Object_Detection_with_Diffusion-Driven_Data_Generation_CVPR_2025_paper.html": {
    "title": "AeroGen: Enhancing Remote Sensing Object Detection with Diffusion-Driven Data Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Datao Tang",
      "Xiangyong Cao",
      "Xuan Wu",
      "Jialin Li",
      "Jing Yao",
      "Xueru Bai",
      "Dongsheng Jiang",
      "Yin Li",
      "Deyu Meng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Tra-MoE_Learning_Trajectory_Prediction_Model_from_Multiple_Domains_for_Adaptive_CVPR_2025_paper.html": {
    "title": "Tra-MoE: Learning Trajectory Prediction Model from Multiple Domains for Adaptive Policy Conditioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiange Yang",
      "Haoyi Zhu",
      "Yating Wang",
      "Gangshan Wu",
      "Tong He",
      "Limin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data_CVPR_2025_paper.html": {
    "title": "Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihe Yang",
      "Xufang Luo",
      "Dongqi Han",
      "Yunjian Xu",
      "Dongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Style_Quantization_for_Data-Efficient_GAN_Training_CVPR_2025_paper.html": {
    "title": "Style Quantization for Data-Efficient GAN Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Wang",
      "Xin Lan",
      "Jizhe Zhou",
      "Yuxin Tian",
      "Jiancheng Lv"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Localizing_Events_in_Videos_with_Multimodal_Queries_CVPR_2025_paper.html": {
    "title": "Localizing Events in Videos with Multimodal Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gengyuan Zhang",
      "Mang Ling Ada Fok",
      "Jialu Ma",
      "Yan Xia",
      "Daniel Cremers",
      "Philip Torr",
      "Volker Tresp",
      "Jindong Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_PhysVLM_Enabling_Visual_Language_Models_to_Understand_Robotic_Physical_Reachability_CVPR_2025_paper.html": {
    "title": "PhysVLM: Enabling Visual Language Models to Understand Robotic Physical Reachability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijie Zhou",
      "Manli Tao",
      "Chaoyang Zhao",
      "Haiyun Guo",
      "Honghui Dong",
      "Ming Tang",
      "Jinqiao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Stracke_CleanDIFT_Diffusion_Features_without_Noise_CVPR_2025_paper.html": {
    "title": "CleanDIFT: Diffusion Features without Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nick Stracke",
      "Stefan Andreas Baumann",
      "Kolja Bauer",
      "Frank Fundel",
      "Björn Ommer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hoogeboom_Simpler_Diffusion_1.5_FID_on_ImageNet512_with_Pixel-space_Diffusion_CVPR_2025_paper.html": {
    "title": "Simpler Diffusion: 1.5 FID on ImageNet512 with Pixel-space Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emiel Hoogeboom",
      "Thomas Mensink",
      "Jonathan Heek",
      "Kay Lamerigts",
      "Ruiqi Gao",
      "Tim Salimans"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Uncertainty-Instructed_Structure_Injection_for_Generalizable_HD_Map_Construction_CVPR_2025_paper.html": {
    "title": "Uncertainty-Instructed Structure Injection for Generalizable HD Map Construction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaolu Liu",
      "Ruizi Yang",
      "Song Wang",
      "Wentong Li",
      "Junbo Chen",
      "Jianke Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection_CVPR_2025_paper.html": {
    "title": "STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage Security Inspection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Divya Velayudhan",
      "Abdelfatah Ahmed",
      "Mohamad Alansari",
      "Neha Gour",
      "Abderaouf Behouch",
      "Taimur Hassan",
      "Syed Talal Wasim",
      "Nabil Maalej",
      "Muzammal Naseer",
      "Juergen Gall",
      "Mohammed Bennamoun",
      "Ernesto Damiani",
      "Naoufel Werghi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Not_All_Parameters_Matter_Masking_Diffusion_Models_for_Enhancing_Generation_CVPR_2025_paper.html": {
    "title": "Not All Parameters Matter: Masking Diffusion Models for Enhancing Generation Ability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Wang",
      "Senmao Li",
      "Fei Yang",
      "Jianye Wang",
      "Ziheng Zhang",
      "Yuhan Liu",
      "Yaxing Wang",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Agro_MAD_Memory-Augmented_Detection_of_3D_Objects_CVPR_2025_paper.html": {
    "title": "MAD: Memory-Augmented Detection of 3D Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Agro",
      "Sergio Casas",
      "Patrick Wang",
      "Thomas Gilles",
      "Raquel Urtasun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kamberov_Doppelgangers_and_Adversarial_Vulnerability_CVPR_2025_paper.html": {
    "title": "Doppelgangers and Adversarial Vulnerability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "George Kamberov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zamfir_Complexity_Experts_are_Task-Discriminative_Learners_for_Any_Image_Restoration_CVPR_2025_paper.html": {
    "title": "Complexity Experts are Task-Discriminative Learners for Any Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eduard Zamfir",
      "Zongwei Wu",
      "Nancy Mehta",
      "Yuedong Tan",
      "Danda Pani Paudel",
      "Yulun Zhang",
      "Radu Timofte"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers_CVPR_2025_paper.html": {
    "title": "Generative Omnimatte: Learning to Decompose Video into Layers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao-Chih Lee",
      "Erika Lu",
      "Sarah Rumbley",
      "Michal Geyer",
      "Jia-Bin Huang",
      "Tali Dekel",
      "Forrester Cole"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_5100_Breaking_Performance_Shackles_of_Full_Fine-Tuning_on_Visual_Recognition_CVPR_2025_paper.html": {
    "title": "5%>100%: Breaking Performance Shackles of Full Fine-Tuning on Visual Recognition Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongshuo Yin",
      "Leiyi Hu",
      "Bin Li",
      "Youqun Zhang",
      "Xue Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Santra_Precise_Event_Spotting_in_Sports_Videos_Solving_Long-Range_Dependency_and_CVPR_2025_paper.html": {
    "title": "Precise Event Spotting in Sports Videos: Solving Long-Range Dependency and Class Imbalance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanchayan Santra",
      "Vishal Chudasama",
      "Pankaj Wasnik",
      "Vineeth N Balasubramanian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Real-IAD_D3_A_Real-World_2DPseudo-3D3D_Dataset_for_Industrial_Anomaly_Detection_CVPR_2025_paper.html": {
    "title": "Real-IAD D3: A Real-World 2D/Pseudo-3D/3D Dataset for Industrial Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbing Zhu",
      "Lidong Wang",
      "Ziqing Zhou",
      "Chengjie Wang",
      "Yurui Pan",
      "Ruoyi Zhang",
      "Zhuhao Chen",
      "Linjie Cheng",
      "Bin-Bin Gao",
      "Jiangning Zhang",
      "Zhenye Gan",
      "Yuxie Wang",
      "Yulong Chen",
      "Shuguang Qian",
      "Mingmin Chi",
      "Bo Peng",
      "Lizhuang Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Steady_Progress_Beats_Stagnation_Mutual_Aid_of_Foundation_and_Conventional_CVPR_2025_paper.html": {
    "title": "Steady Progress Beats Stagnation: Mutual Aid of Foundation and Conventional Models in Mixed Domain Semi-Supervised Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinghe Ma",
      "Jian Zhang",
      "Zekun Li",
      "Lei Qi",
      "Qian Yu",
      "Yinghuan Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Afane_ATP_Adaptive_Threshold_Pruning_for_Efficient_Data_Encoding_in_Quantum_CVPR_2025_paper.html": {
    "title": "ATP: Adaptive Threshold Pruning for Efficient Data Encoding in Quantum Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Afane",
      "Gabrielle Ebbrecht",
      "Ying Wang",
      "Juntao Chen",
      "Junaid Farooq"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shum_Color_Alignment_in_Diffusion_CVPR_2025_paper.html": {
    "title": "Color Alignment in Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ka Chun Shum",
      "Binh-Son Hua",
      "Duc Thanh Nguyen",
      "Sai-Kit Yeung"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Reilly_LLAVIDAL_A_Large_LAnguage_VIsion_Model_for_Daily_Activities_of_CVPR_2025_paper.html": {
    "title": "LLAVIDAL: A Large LAnguage VIsion Model for Daily Activities of Living",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominick Reilly",
      "Rajatsubhra Chakraborty",
      "Arkaprava Sinha",
      "Manish Kumar Govind",
      "Pu Wang",
      "Francois Bremond",
      "Le Xue",
      "Srijan Das"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Language-Guided_Salient_Object_Ranking_CVPR_2025_paper.html": {
    "title": "Language-Guided Salient Object Ranking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fang Liu",
      "Yuhao Liu",
      "Ke Xu",
      "Shuquan Ye",
      "Gerhard Petrus Hancke",
      "Rynson W. H. Lau"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_Decoupled_Motion_Expression_Video_Segmentation_CVPR_2025_paper.html": {
    "title": "Decoupled Motion Expression Video Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Fang",
      "Runmin Cong",
      "Xiankai Lu",
      "Xiaofei Zhou",
      "Sam Kwong",
      "Wei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ouyang_K-LoRA_Unlocking_Training-Free_Fusion_of_Any_Subject_and_Style_LoRAs_CVPR_2025_paper.html": {
    "title": "K-LoRA: Unlocking Training-Free Fusion of Any Subject and Style LoRAs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziheng Ouyang",
      "Zhen Li",
      "Qibin Hou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_Towards_More_General_Video-based_Deepfake_Detection_through_Facial_Component_Guided_CVPR_2025_paper.html": {
    "title": "Towards More General Video-based Deepfake Detection through Facial Component Guided Adaptation for Foundation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue-Hua Han",
      "Tai-Ming Huang",
      "Kai-Lung Hua",
      "Jun-Cheng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_WF-VAE_Enhancing_Video_VAE_by_Wavelet-Driven_Energy_Flow_for_Latent_CVPR_2025_paper.html": {
    "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongjian Li",
      "Bin Lin",
      "Yang Ye",
      "Liuhan Chen",
      "Xinhua Cheng",
      "Shenghai Yuan",
      "Li Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic_CVPR_2025_paper.html": {
    "title": "SP3D: Boosting Sparsely-Supervised 3D Object Detection via Accurate Cross-Modal Semantic Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijia Zhao",
      "Qiming Xia",
      "Xusheng Guo",
      "Pufan Zou",
      "Maoji Zheng",
      "Hai Wu",
      "Chenglu Wen",
      "Cheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ji_ARKit_LabelMaker_A_New_Scale_for_Indoor_3D_Scene_Understanding_CVPR_2025_paper.html": {
    "title": "ARKit LabelMaker: A New Scale for Indoor 3D Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangda Ji",
      "Silvan Weder",
      "Francis Engelmann",
      "Marc Pollefeys",
      "Hermann Blum"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_VoCo-LLaMA_Towards_Vision_Compression_with_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "VoCo-LLaMA: Towards Vision Compression with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xubing Ye",
      "Yukang Gan",
      "Xiaoke Huang",
      "Yixiao Ge",
      "Yansong Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Henschel_StreamingT2V_Consistent_Dynamic_and_Extendable_Long_Video_Generation_from_Text_CVPR_2025_paper.html": {
    "title": "StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roberto Henschel",
      "Levon Khachatryan",
      "Hayk Poghosyan",
      "Daniil Hayrapetyan",
      "Vahram Tadevosyan",
      "Zhangyang Wang",
      "Shant Navasardyan",
      "Humphrey Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_Focal_Split_Untethered_Snapshot_Depth_from_Differential_Defocus_CVPR_2025_paper.html": {
    "title": "Focal Split: Untethered Snapshot Depth from Differential Defocus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Luo",
      "John Mamish",
      "Alan Fu",
      "Thomas Concannon",
      "Josiah Hester",
      "Emma Alexander",
      "Qi Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_AFL_A_Single-Round_Analytic_Approach_for_Federated_Learning_with_Pre-trained_CVPR_2025_paper.html": {
    "title": "AFL: A Single-Round Analytic Approach for Federated Learning with Pre-trained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Run He",
      "Kai Tong",
      "Di Fang",
      "Han Sun",
      "Ziqian Zeng",
      "Haoran Li",
      "Tianyi Chen",
      "Huiping Zhuang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote_CVPR_2025_paper.html": {
    "title": "XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large Ultra-High-Resolution Remote Sensing Imagery?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengxiang Wang",
      "Hongzhen Wang",
      "Zonghao Guo",
      "Di Wang",
      "Yulin Wang",
      "Mingshuo Chen",
      "Qiang Ma",
      "Long Lan",
      "Wenjing Yang",
      "Jing Zhang",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_BOLT_Boost_Large_Vision-Language_Model_Without_Training_for_Long-form_Video_CVPR_2025_paper.html": {
    "title": "BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuming Liu",
      "Chen Zhao",
      "Tianqi Xu",
      "Bernard Ghanem"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Berisha_Efficient_Data_Driven_Mixture-of-Expert_Extraction_from_Trained_Networks_CVPR_2025_paper.html": {
    "title": "Efficient Data Driven Mixture-of-Expert Extraction from Trained Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uranik Berisha",
      "Jens Mehnert",
      "Alexandru Paul Condurache"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes_CVPR_2025_paper.html": {
    "title": "Reference-Based 3D-Aware Image Editing with Triplanes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bahri Batuhan Bilecen",
      "Yigit Yalin",
      "Ning Yu",
      "Aysegul Dundar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style_CVPR_2025_paper.html": {
    "title": "StyleSSP: Sampling StartPoint Enhancement for Training-free Diffusion-based Method for Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruojun Xu",
      "Weijie Xi",
      "XiaoDi Wang",
      "Yongbo Mao",
      "Zach Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shao_PURA_Parameter_Update-Recovery_Test-Time_Adaption_for_RGB-T_Tracking_CVPR_2025_paper.html": {
    "title": "PURA: Parameter Update-Recovery Test-Time Adaption for RGB-T Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zekai Shao",
      "Yufan Hu",
      "Bin Fan",
      "Hongmin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_One_is_Plenty_A_Polymorphic_Feature_Interpreter_for_Immutable_Heterogeneous_CVPR_2025_paper.html": {
    "title": "One is Plenty: A Polymorphic Feature Interpreter for Immutable Heterogeneous Collaborative Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Xia",
      "Quan Yuan",
      "Guiyang Luo",
      "Xiaoyuan Fu",
      "Yang Li",
      "Xuanhan Zhu",
      "Tianyou Luo",
      "Siheng Chen",
      "Jinglin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_Towards_All-in-One_Medical_Image_Re-Identification_CVPR_2025_paper.html": {
    "title": "Towards All-in-One Medical Image Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Tian",
      "Kaiyuan Ji",
      "Rongzhao Zhang",
      "Yankai Jiang",
      "Chunyi Li",
      "Xiaosong Wang",
      "Guangtao Zhai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_SegAgent_Exploring_Pixel_Understanding_Capabilities_in_MLLMs_by_Imitating_Human_CVPR_2025_paper.html": {
    "title": "SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by Imitating Human Annotator Trajectories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muzhi Zhu",
      "Yuzhuo Tian",
      "Hao Chen",
      "Chunluan Zhou",
      "Qingpei Guo",
      "Yang Liu",
      "Ming Yang",
      "Chunhua Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Motions_as_Queries_One-Stage_Multi-Person_Holistic_Human_Motion_Capture_CVPR_2025_paper.html": {
    "title": "Motions as Queries: One-Stage Multi-Person Holistic Human Motion Capture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kenkun Liu",
      "Yurong Fu",
      "Weihao Yuan",
      "Jing Lin",
      "Peihao Li",
      "Xiaodong Gu",
      "Lingteng Qiu",
      "Haoqian Wang",
      "Zilong Dong",
      "Xiaoguang Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_SceneCrafter_Controllable_Multi-View_Driving_Scene_Editing_CVPR_2025_paper.html": {
    "title": "SceneCrafter: Controllable Multi-View Driving Scene Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehao Zhu",
      "Yuliang Zou",
      "Chiyu Max Jiang",
      "Bo Sun",
      "Vincent Casser",
      "Xiukun Huang",
      "Jiahao Wang",
      "Zhenpei Yang",
      "Ruiqi Gao",
      "Leonidas Guibas",
      "Mingxing Tan",
      "Dragomir Anguelov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_AMO_Sampler_Enhancing_Text_Rendering_with_Overshooting_CVPR_2025_paper.html": {
    "title": "AMO Sampler: Enhancing Text Rendering with Overshooting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xixi Hu",
      "Keyang Xu",
      "Bo Liu",
      "Qiang Liu",
      "Hongliang Fei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement_CVPR_2025_paper.html": {
    "title": "ImViD: Immersive Volumetric Videos for Enhanced VR Engagement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengxian Yang",
      "Shi Pan",
      "Shengqi Wang",
      "Haoxiang Wang",
      "Li Lin",
      "Guanjun Li",
      "Zhengqi Wen",
      "Borong Lin",
      "Jianhua Tao",
      "Tao Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wei_Integral_Fast_Fourier_Color_Constancy_CVPR_2025_paper.html": {
    "title": "Integral Fast Fourier Color Constancy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjun Wei",
      "Yanlin Qian",
      "Huaian Chen",
      "Junkang Dai",
      "Yi Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gui_I2VGuard_Safeguarding_Images_against_Misuse_in_Diffusion-based_Image-to-Video_Models_CVPR_2025_paper.html": {
    "title": "I2VGuard: Safeguarding Images against Misuse in Diffusion-based Image-to-Video Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongnan Gui",
      "Xun Guo",
      "Wengang Zhou",
      "Yan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Victorica_Saliuitl_Ensemble_Salience_Guided_Recovery_of_Adversarial_Patches_against_CNNs_CVPR_2025_paper.html": {
    "title": "Saliuitl: Ensemble Salience Guided Recovery of Adversarial Patches against CNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mauricio Byrd Victorica",
      "György Dán",
      "Henrik Sandberg"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Matsubara_HeatFormer_A_Neural_Optimizer_for_Multiview_Human_Mesh_Recovery_CVPR_2025_paper.html": {
    "title": "HeatFormer: A Neural Optimizer for Multiview Human Mesh Recovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuto Matsubara",
      "Ko Nishino"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_ResCLIP_Residual_Attention_for_Training-free_Dense_Vision-language_Inference_CVPR_2025_paper.html": {
    "title": "ResCLIP: Residual Attention for Training-free Dense Vision-language Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Yang",
      "Jinhong Deng",
      "Wen Li",
      "Lixin Duan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_GPS_as_a_Control_Signal_for_Image_Generation_CVPR_2025_paper.html": {
    "title": "GPS as a Control Signal for Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Feng",
      "Ziyang Chen",
      "Aleksander Holynski",
      "Alexei A. Efros",
      "Andrew Owens"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_CPath-Omni_A_Unified_Multimodal_Foundation_Model_for_Patch_and_Whole_CVPR_2025_paper.html": {
    "title": "CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole Slide Image Analysis in Computational Pathology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Sun",
      "Yixuan Si",
      "Chenglu Zhu",
      "Xuan Gong",
      "Kai Zhang",
      "Pingyi Chen",
      "Ye Zhang",
      "Zhongyi Shui",
      "Tao Lin",
      "Lin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation_CVPR_2025_paper.html": {
    "title": "OPTICAL: Leveraging Optimal Transport for Contribution Allocation in Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Cui",
      "Yulei Qin",
      "Wengang Zhou",
      "Hongsheng Li",
      "Houqiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yugay_MAGiC-SLAM_Multi-Agent_Gaussian_Globally_Consistent__SLAM_CVPR_2025_paper.html": {
    "title": "MAGiC-SLAM: Multi-Agent Gaussian Globally Consistent SLAM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vladimir Yugay",
      "Theo Gevers",
      "Martin R. Oswald"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qian_Dispider_Enabling_Video_LLMs_with_Active_Real-Time_Interaction_via_Disentangled_CVPR_2025_paper.html": {
    "title": "Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Qian",
      "Shuangrui Ding",
      "Xiaoyi Dong",
      "Pan Zhang",
      "Yuhang Zang",
      "Yuhang Cao",
      "Dahua Lin",
      "Jiaqi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks_CVPR_2025_paper.html": {
    "title": "NTClick: Achieving Precise Interactive Segmentation With Noise-tolerant Clicks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyi Zhang",
      "Ting Liu",
      "Xiaochao Qu",
      "Luoqi Liu",
      "Yao Zhao",
      "Yunchao Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_Show_and_Segment_Universal_Medical_Image_Segmentation_via_In-Context_Learning_CVPR_2025_paper.html": {
    "title": "Show and Segment: Universal Medical Image Segmentation via In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhe Gao",
      "Di Liu",
      "Zhuowei Li",
      "Yunsheng Li",
      "Dongdong Chen",
      "Mu Zhou",
      "Dimitris N. Metaxas"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cao_MVGenMaster_Scaling_Multi-View_Generation_from_Any_Image_via_3D_Priors_CVPR_2025_paper.html": {
    "title": "MVGenMaster: Scaling Multi-View Generation from Any Image via 3D Priors Enhanced Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenjie Cao",
      "Chaohui Yu",
      "Shang Liu",
      "Fan Wang",
      "Xiangyang Xue",
      "Yanwei Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_CADCrafter_Generating_Computer-Aided_Design_Models_from_Unconstrained_Images_CVPR_2025_paper.html": {
    "title": "CADCrafter: Generating Computer-Aided Design Models from Unconstrained Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Chen",
      "Jiacheng Wei",
      "Tianrun Chen",
      "Chi Zhang",
      "Xiaofeng Yang",
      "Shangzhan Zhang",
      "Bingchen Yang",
      "Chuan-Sheng Foo",
      "Guosheng Lin",
      "Qixing Huang",
      "Fayao Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Bayesian_Test-Time_Adaptation_for_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "Bayesian Test-Time Adaptation for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lihua Zhou",
      "Mao Ye",
      "Shuaifeng Li",
      "Nianxin Li",
      "Xiatian Zhu",
      "Lei Deng",
      "Hongbin Liu",
      "Zhen Lei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation_CVPR_2025_paper.html": {
    "title": "Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hadi Alzayer",
      "Philipp Henzler",
      "Jonathan T. Barron",
      "Jia-Bin Huang",
      "Pratul P. Srinivasan",
      "Dor Verbin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Causal_Composition_Diffusion_Model_for_Closed-loop_Traffic_Generation_CVPR_2025_paper.html": {
    "title": "Causal Composition Diffusion Model for Closed-loop Traffic Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haohong Lin",
      "Xin Huang",
      "Tung Phan",
      "David Hayden",
      "Huan Zhang",
      "Ding Zhao",
      "Siddhartha Srinivasa",
      "Eric Wolff",
      "Hongge Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling_CVPR_2025_paper.html": {
    "title": "Change3D: Revisiting Change Detection and Captioning from A Video Modeling Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duowang Zhu",
      "Xiaohu Huang",
      "Haiyan Huang",
      "Hao Zhou",
      "Zhenfeng Shao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_DyMO_Training-Free_Diffusion_Model_Alignment_with_Dynamic_Multi-Objective_Scheduling_CVPR_2025_paper.html": {
    "title": "DyMO: Training-Free Diffusion Model Alignment with Dynamic Multi-Objective Scheduling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Xie",
      "Dong Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_HiMoR_Monocular_Deformable_Gaussian_Reconstruction_with_Hierarchical_Motion_Representation_CVPR_2025_paper.html": {
    "title": "HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Liang",
      "Tianhan Xu",
      "Yuta Kikuchi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_GENIUS_A_Generative_Framework_for_Universal_Multimodal_Search_CVPR_2025_paper.html": {
    "title": "GENIUS: A Generative Framework for Universal Multimodal Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungyeon Kim",
      "Xinliang Zhu",
      "Xiaofan Lin",
      "Muhammet Bastan",
      "Douglas Gray",
      "Suha Kwak"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition_CVPR_2025_paper.html": {
    "title": "Enhanced Visual-Semantic Interaction with Tailored Prompts for Pedestrian Attribute Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyi Wu",
      "Yan Huang",
      "Min Gao",
      "Yuzhen Niu",
      "Yuzhong Chen",
      "Qiang Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Boss_SF3D_Stable_Fast_3D_Mesh_Reconstruction_with_UV-unwrapping_and_Illumination_CVPR_2025_paper.html": {
    "title": "SF3D: Stable Fast 3D Mesh Reconstruction with UV-unwrapping and Illumination Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mark Boss",
      "Zixuan Huang",
      "Aaryaman Vasishta",
      "Varun Jampani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction_CVPR_2025_paper.html": {
    "title": "HSI-GPT: A General-Purpose Large Scene-Motion-Language Model for Human Scene Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Wang",
      "Yali Li",
      "Xiang Li",
      "Shengjin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Towards_Precise_Embodied_Dialogue_Localization_via_Causality_Guided_Diffusion_CVPR_2025_paper.html": {
    "title": "Towards Precise Embodied Dialogue Localization via Causality Guided Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Wang",
      "Le Wang",
      "Sanping Zhou",
      "Jingyi Tian",
      "Zheng Qin",
      "Yabing Wang",
      "Gang Hua",
      "Wei Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Vid2Avatar-Pro_Authentic_Avatar_from_Videos_in_the_Wild_via_Universal_CVPR_2025_paper.html": {
    "title": "Vid2Avatar-Pro: Authentic Avatar from Videos in the Wild via Universal Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Guo",
      "Junxuan Li",
      "Yash Kant",
      "Yaser Sheikh",
      "Shunsuke Saito",
      "Chen Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_RoomPainter_View-Integrated_Diffusion_for_Consistent_Indoor_Scene_Texturing_CVPR_2025_paper.html": {
    "title": "RoomPainter: View-Integrated Diffusion for Consistent Indoor Scene Texturing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhipeng Huang",
      "Wangbo Yu",
      "Xinhua Cheng",
      "Chengshu Zhao",
      "Yunyang Ge",
      "Mingyi Guo",
      "Li Yuan",
      "Yonghong Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Attribute-formed_Class-specific_Concept_Space_Endowing_Language_Bottleneck_Model_with_Better_CVPR_2025_paper.html": {
    "title": "Attribute-formed Class-specific Concept Space: Endowing Language Bottleneck Model with Better Interpretability and Scalability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianyang Zhang",
      "Qianli Luo",
      "Guowu Yang",
      "Wenjing Yang",
      "Weide Liu",
      "Guosheng Lin",
      "Fengmao Lv"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qi_Customized_Condition_Controllable_Generation_for_Video_Soundtrack_CVPR_2025_paper.html": {
    "title": "Customized Condition Controllable Generation for Video Soundtrack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Qi",
      "Kunsheng Ma",
      "Changsheng Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_ProjAttacker_A_Configurable_Physical_Adversarial_Attack_for_Face_Recognition_via_CVPR_2025_paper.html": {
    "title": "ProjAttacker: A Configurable Physical Adversarial Attack for Face Recognition via Projector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanwei Liu",
      "Hui Wei",
      "Chengyu Jia",
      "Ruqi Xiao",
      "Weijian Ruan",
      "Xingxing Wei",
      "Joey Tianyi Zhou",
      "Zheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_EfficientViM_Efficient_Vision_Mamba_with_Hidden_State_Mixer_based_State_CVPR_2025_paper.html": {
    "title": "EfficientViM: Efficient Vision Mamba with Hidden State Mixer based State Space Duality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanghyeok Lee",
      "Joonmyung Choi",
      "Hyunwoo J. Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tu_A4A_Adapter_for_Adapter_Transfer_via_All-for-All_Mapping_for_Cross-Architecture_CVPR_2025_paper.html": {
    "title": "A4A: Adapter for Adapter Transfer via All-for-All Mapping for Cross-Architecture Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keyu Tu",
      "Mengqi Huang",
      "Zhuowei Chen",
      "Zhendong Mao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Athar_ViCaS_A_Dataset_for_Combining_Holistic_and_Pixel-level_Video_Understanding_CVPR_2025_paper.html": {
    "title": "ViCaS: A Dataset for Combining Holistic and Pixel-level Video Understanding using Captions with Grounded Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Athar",
      "Xueqing Deng",
      "Liang-Chieh Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_A_Universal_Scale-Adaptive_Deformable_Transformer_for_Image_Restoration_across_Diverse_CVPR_2025_paper.html": {
    "title": "A Universal Scale-Adaptive Deformable Transformer for Image Restoration across Diverse Artifacts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuyi He",
      "Yuhui Quan",
      "Ruotao Xu",
      "Hui Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mao_WISE_A_Framework_for_Gigapixel_Whole-Slide-Image_Lossless_Compression_CVPR_2025_paper.html": {
    "title": "WISE: A Framework for Gigapixel Whole-Slide-Image Lossless Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Mao",
      "Jun Wang",
      "Nan Guan",
      "Chun Jason Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry_CVPR_2025_paper.html": {
    "title": "Gromov-Wasserstein Problem with Cyclic Symmetry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shoichiro Takeda",
      "Yasunori Akagi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_IRIS_Inverse_Rendering_of_Indoor_Scenes_from_Low_Dynamic_Range_CVPR_2025_paper.html": {
    "title": "IRIS: Inverse Rendering of Indoor Scenes from Low Dynamic Range Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chih-Hao Lin",
      "Jia-Bin Huang",
      "Zhengqin Li",
      "Zhao Dong",
      "Christian Richardt",
      "Tuotuo Li",
      "Michael Zollhöfer",
      "Johannes Kopf",
      "Shenlong Wang",
      "Changil Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_SimAvatar_Simulation-Ready_Avatars_with_Layered_Hair_and_Clothing_CVPR_2025_paper.html": {
    "title": "SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueting Li",
      "Ye Yuan",
      "Shalini De Mello",
      "Gilles Daviet",
      "Jonathan Leaf",
      "Miles Macklin",
      "Jan Kautz",
      "Umar Iqbal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Test-Time_Backdoor_Detection_for_Object_Detection_Models_CVPR_2025_paper.html": {
    "title": "Test-Time Backdoor Detection for Object Detection Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hangtao Zhang",
      "Yichen Wang",
      "Shihui Yan",
      "Chenyu Zhu",
      "Ziqi Zhou",
      "Linshan Hou",
      "Shengshan Hu",
      "Minghui Li",
      "Yanjun Zhang",
      "Leo Yu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_Towards_Precise_Scaling_Laws_for_Video_Diffusion_Transformers_CVPR_2025_paper.html": {
    "title": "Towards Precise Scaling Laws for Video Diffusion Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanyang Yin",
      "Yaqi Zhao",
      "Mingwu Zheng",
      "Ke Lin",
      "Jiarong Ou",
      "Rui Chen",
      "Victor Shea-Jay Huang",
      "Jiahao Wang",
      "Xin Tao",
      "Pengfei Wan",
      "Di Zhang",
      "Baoqun Yin",
      "Wentao Zhang",
      "Kun Gai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_RoGSplat_Learning_Robust_Generalizable_Human_Gaussian_Splatting_from_Sparse_Multi-View_CVPR_2025_paper.html": {
    "title": "RoGSplat: Learning Robust Generalizable Human Gaussian Splatting from Sparse Multi-View Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjin Xiao",
      "Qing Zhang",
      "Yonewei Nie",
      "Lei Zhu",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bai_SDBF_Steep-Decision-Boundary_Fingerprinting_for_Hard-Label_Tampering_Detection_of_DNN_Models_CVPR_2025_paper.html": {
    "title": "SDBF: Steep-Decision-Boundary Fingerprinting for Hard-Label Tampering Detection of DNN Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofan Bai",
      "Shixin Li",
      "Xiaojing Ma",
      "Bin Benjamin Zhu",
      "Dongmei Zhang",
      "Linchen Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_EnliveningGS_Active_Locomotion_of_3DGS_CVPR_2025_paper.html": {
    "title": "EnliveningGS: Active Locomotion of 3DGS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Shen",
      "Tianjia Shao",
      "Kun Zhou",
      "Chenfanfu Jiang",
      "Yin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cai_SPMTrack_Spatio-Temporal_Parameter-Efficient_Fine-Tuning_with_Mixture_of_Experts_for_Scalable_CVPR_2025_paper.html": {
    "title": "SPMTrack: Spatio-Temporal Parameter-Efficient Fine-Tuning with Mixture of Experts for Scalable Visual Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenrui Cai",
      "Qingjie Liu",
      "Yunhong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wimbauer_AnyCam_Learning_to_Recover_Camera_Poses_and_Intrinsics_from_Casual_CVPR_2025_paper.html": {
    "title": "AnyCam: Learning to Recover Camera Poses and Intrinsics from Casual Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Wimbauer",
      "Weirong Chen",
      "Dominik Muhle",
      "Christian Rupprecht",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_Knowledge-Aligned_Counterfactual-Enhancement_Diffusion_Perception_for_Unsupervised_Cross-Domain_Visual_Emotion_Recognition_CVPR_2025_paper.html": {
    "title": "Knowledge-Aligned Counterfactual-Enhancement Diffusion Perception for Unsupervised Cross-Domain Visual Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen Yin",
      "Yong Wang",
      "Guiduo Duan",
      "Dongyang Zhang",
      "Xin Hu",
      "Yuan-Fang Li",
      "Tao He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hegde_Distilling_Multi-modal_Large_Language_Models_for_Autonomous_Driving_CVPR_2025_paper.html": {
    "title": "Distilling Multi-modal Large Language Models for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deepti Hegde",
      "Rajeev Yasarla",
      "Hong Cai",
      "Shizhong Han",
      "Apratim Bhattacharyya",
      "Shweta Mahajan",
      "Litian Liu",
      "Risheek Garrepalli",
      "Vishal M. Patel",
      "Fatih Porikli"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Pixel-aligned_RGB-NIR_Stereo_Imaging_and_Dataset_for_Robot_Vision_CVPR_2025_paper.html": {
    "title": "Pixel-aligned RGB-NIR Stereo Imaging and Dataset for Robot Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinnyeong Kim",
      "Seung-Hwan Baek"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image_CVPR_2025_paper.html": {
    "title": "Can Machines Understand Composition? Dataset and Benchmark for Photographic Image Composition Embedding and Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoran Zhao",
      "Peng Lu",
      "Anran Zhang",
      "Peipei Li",
      "Xia Li",
      "Xuannan Liu",
      "Yang Hu",
      "Shiyi Chen",
      "Liwei Wang",
      "Wenhao Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Stojnic_LPOSS_Label_Propagation_Over_Patches_and_Pixels_for_Open-vocabulary_Semantic_CVPR_2025_paper.html": {
    "title": "LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vladan Stojnić",
      "Yannis Kalantidis",
      "Jiří Matas",
      "Giorgos Tolias"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Towards_Efficient_Foundation_Model_for_Zero-shot_Amodal_Segmentation_CVPR_2025_paper.html": {
    "title": "Towards Efficient Foundation Model for Zero-shot Amodal Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaochen Liu",
      "Limeng Qiao",
      "Xiangxiang Chu",
      "Lin Ma",
      "Tingting Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_PhysGen3D_Crafting_a_Miniature_Interactive_World_from_a_Single_Image_CVPR_2025_paper.html": {
    "title": "PhysGen3D: Crafting a Miniature Interactive World from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyuan Chen",
      "Hanxiao Jiang",
      "Shaowei Liu",
      "Saurabh Gupta",
      "Yunzhu Li",
      "Hao Zhao",
      "Shenlong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Duan_Docopilot_Improving_Multimodal_Models_for_Document-Level_Understanding_CVPR_2025_paper.html": {
    "title": "Docopilot: Improving Multimodal Models for Document-Level Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Duan",
      "Zhe Chen",
      "Yusong Hu",
      "Weiyun Wang",
      "Shenglong Ye",
      "Botian Shi",
      "Lewei Lu",
      "Qibin Hou",
      "Tong Lu",
      "Hongsheng Li",
      "Jifeng Dai",
      "Wenhai Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ravishankar_Scaling_Properties_of_Diffusion_Models_For_Perceptual_Tasks_CVPR_2025_paper.html": {
    "title": "Scaling Properties of Diffusion Models For Perceptual Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rahul Ravishankar",
      "Zeeshan Patel",
      "Jathushan Rajasegaran",
      "Jitendra Malik"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Perrett_HD-EPIC_A_Highly-Detailed_Egocentric_Video_Dataset_CVPR_2025_paper.html": {
    "title": "HD-EPIC: A Highly-Detailed Egocentric Video Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Toby Perrett",
      "Ahmad Darkhalil",
      "Saptarshi Sinha",
      "Omar Emara",
      "Sam Pollard",
      "Kranti Kumar Parida",
      "Kaiting Liu",
      "Prajwal Gatti",
      "Siddhant Bansal",
      "Kevin Flanagan",
      "Jacob Chalk",
      "Zhifan Zhu",
      "Rhodri Guerrier",
      "Fahd Abdelazim",
      "Bin Zhu",
      "Davide Moltisanti",
      "Michael Wray",
      "Hazel Doughty",
      "Dima Damen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image_CVPR_2025_paper.html": {
    "title": "Exact: Exploring Space-Time Perceptive Clues for Weakly Supervised Satellite Image Time Series Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zhu",
      "Yan Zhu",
      "Jiayu Xiao",
      "Tianxiang Xiao",
      "Yike Ma",
      "Yucheng Zhang",
      "Feng Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Advancing_Myopia_To_Holism_Fully_Contrastive_Language-Image_Pre-training_CVPR_2025_paper.html": {
    "title": "Advancing Myopia To Holism: Fully Contrastive Language-Image Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haicheng Wang",
      "Chen Ju",
      "Weixiong Lin",
      "Shuai Xiao",
      "Mengting Chen",
      "Yixuan Huang",
      "Chang Liu",
      "Mingshuai Yao",
      "Jinsong Lan",
      "Ying Chen",
      "Qingwen Liu",
      "Yanfeng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yao_PolarFree_Polarization-based_Reflection-Free_Imaging_CVPR_2025_paper.html": {
    "title": "PolarFree: Polarization-based Reflection-Free Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingde Yao",
      "Menglu Wang",
      "King-Man Tam",
      "Lingen Li",
      "Tianfan Xue",
      "Jinwei Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis_CVPR_2025_paper.html": {
    "title": "H-MoRe: Learning Human-centric Motion Representation for Action Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanbo Huang",
      "Xiaoming Liu",
      "Yu Kong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kucuksozen_Hierarchical_Compact_Clustering_Attention_COCA_for_Unsupervised_Object-Centric_Learning_CVPR_2025_paper.html": {
    "title": "Hierarchical Compact Clustering Attention (COCA) for Unsupervised Object-Centric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Can Kucuksozen",
      "Yucel Yemez"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Effortless_Active_Labeling_for_Long-Term_Test-Time_Adaptation_CVPR_2025_paper.html": {
    "title": "Effortless Active Labeling for Long-Term Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guowei Wang",
      "Changxing Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_Leveraging_Temporal_Cues_for_Semi-Supervised_Multi-View_3D_Object_Detection_CVPR_2025_paper.html": {
    "title": "Leveraging Temporal Cues for Semi-Supervised Multi-View 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhyung Park",
      "Navyata Sanghvi",
      "Hiroki Adachi",
      "Yoshihisa Shibata",
      "Shawn Hunt",
      "Shinya Tanaka",
      "Hironobu Fujiyoshi",
      "Kris Kitani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_Self-supervised_ControlNet_with_Spatio-Temporal_Mamba_for_Real-world_Video_Super-resolution_CVPR_2025_paper.html": {
    "title": "Self-supervised ControlNet with Spatio-Temporal Mamba for Real-world Video Super-resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijun Shi",
      "Jing Xu",
      "Lijing Lu",
      "Zhihang Li",
      "Kai Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Etaat_LATTE-MV_Learning_to_Anticipate_Table_Tennis_Hits_from_Monocular_Videos_CVPR_2025_paper.html": {
    "title": "LATTE-MV: Learning to Anticipate Table Tennis Hits from Monocular Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Etaat",
      "Dvij Kalaria",
      "Nima Rahmanian",
      "S. Shankar Sastry"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Logits_DeConfusion_with_CLIP_for_Few-Shot_Learning_CVPR_2025_paper.html": {
    "title": "Logits DeConfusion with CLIP for Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Li",
      "Fang Liu",
      "Zehua Hao",
      "Xinyi Wang",
      "Lingling Li",
      "Xu Liu",
      "Puhua Chen",
      "Wenping Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Distilling_Spatially-Heterogeneous_Distortion_Perception_for_Blind_Image_Quality_Assessment_CVPR_2025_paper.html": {
    "title": "Distilling Spatially-Heterogeneous Distortion Perception for Blind Image Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xudong Li",
      "Wenjie Nie",
      "Yan Zhang",
      "Runze Hu",
      "Ke Li",
      "Xiawu Zheng",
      "Liujuan Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_Pay_Attention_to_the_Foreground_in_Object-Centric_Learning_CVPR_2025_paper.html": {
    "title": "Pay Attention to the Foreground in Object-Centric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pinzhuo Tian",
      "Shengjie Yang",
      "Hang Yu",
      "Alex Kot"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_2DMamba_Efficient_State_Space_Model_for_Image_Representation_with_Applications_CVPR_2025_paper.html": {
    "title": "2DMamba: Efficient State Space Model for Image Representation with Applications on Giga-Pixel Whole Slide Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingwei Zhang",
      "Anh Tien Nguyen",
      "Xi Han",
      "Vincent Quoc-Huy Trinh",
      "Hong Qin",
      "Dimitris Samaras",
      "Mahdi S. Hosseini"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Unboxed_Geometrically_and_Temporally_Consistent_Video_Outpainting_CVPR_2025_paper.html": {
    "title": "Unboxed: Geometrically and Temporally Consistent Video Outpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongrui Yu",
      "Martina Megaro-Boldini",
      "Robert W. Sumner",
      "Abdelaziz Djelouah"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_K-Sort_Arena_Efficient_and_Reliable_Benchmarking_for_Generative_Models_via_CVPR_2025_paper.html": {
    "title": "K-Sort Arena: Efficient and Reliable Benchmarking for Generative Models via K-wise Human Preferences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhikai Li",
      "Xuewen Liu",
      "Dongrong Joe Fu",
      "Jianquan Li",
      "Qingyi Gu",
      "Kurt Keutzer",
      "Zhen Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Seeking_Consistent_Flat_Minima_for_Better_Domain_Generalization_via_Refining_CVPR_2025_paper.html": {
    "title": "Seeking Consistent Flat Minima for Better Domain Generalization via Refining Loss Landscapes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aodi Li",
      "Liansheng Zhuang",
      "Xiao Long",
      "Minghong Yao",
      "Shafei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lincetto_MultimodalStudio_A_Heterogeneous_Sensor_Dataset_and_Framework_for_Neural_Rendering_CVPR_2025_paper.html": {
    "title": "MultimodalStudio: A Heterogeneous Sensor Dataset and Framework for Neural Rendering across Multiple Imaging Modalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Federico Lincetto",
      "Gianluca Agresti",
      "Mattia Rossi",
      "Pietro Zanuttigh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Dense-SfM_Structure_from_Motion_with_Dense_Consistent_Matching_CVPR_2025_paper.html": {
    "title": "Dense-SfM: Structure from Motion with Dense Consistent Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JongMin Lee",
      "Sungjoo Yoo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video_CVPR_2025_paper.html": {
    "title": "FluidNexus: 3D Fluid Reconstruction and Prediction from a Single Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Gao",
      "Hong-Xing Yu",
      "Bo Zhu",
      "Jiajun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_MuTri_Multi-view_Tri-alignment_for_OCT_to_OCTA_3D_Image_Translation_CVPR_2025_paper.html": {
    "title": "MuTri: Multi-view Tri-alignment for OCT to OCTA 3D Image Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuangzhuang Chen",
      "Hualiang Wang",
      "Chubin Ou",
      "Xiaomeng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_Sketchy_Bounding-box_Supervision_for_3D_Instance_Segmentation_CVPR_2025_paper.html": {
    "title": "Sketchy Bounding-box Supervision for 3D Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Deng",
      "Le Hui",
      "Jin Xie",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_Image_Quality_Assessment_Investigating_Causal_Perceptual_Effects_with_Abductive_Counterfactual_CVPR_2025_paper.html": {
    "title": "Image Quality Assessment: Investigating Causal Perceptual Effects with Abductive Counterfactual Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Shen",
      "Mingliang Zhou",
      "Yu Chen",
      "Xuekai Wei",
      "Yong Feng",
      "Huayan Pu",
      "Weijia Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_Pos3R_6D_Pose_Estimation_for_Unseen_Objects_Made_Easy_CVPR_2025_paper.html": {
    "title": "Pos3R: 6D Pose Estimation for Unseen Objects Made Easy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijian Deng",
      "Dylan Campbell",
      "Chunyi Sun",
      "Jiahao Zhang",
      "Shubham Kanitkar",
      "Matt E. Shaffer",
      "Stephen Gould"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_DeformCL_Learning_Deformable_Centerline_Representation_for_Vessel_Extraction_in_3D_CVPR_2025_paper.html": {
    "title": "DeformCL: Learning Deformable Centerline Representation for Vessel Extraction in 3D Medical Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziwei Zhao",
      "Zhixing Zhang",
      "Yuhang Liu",
      "Zhao Zhang",
      "Haojun Yu",
      "Dong Wang",
      "Liwei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_StreetCrafter_Street_View_Synthesis_with_Controllable_Video_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "StreetCrafter: Street View Synthesis with Controllable Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunzhi Yan",
      "Zhen Xu",
      "Haotong Lin",
      "Haian Jin",
      "Haoyu Guo",
      "Yida Wang",
      "Kun Zhan",
      "Xianpeng Lang",
      "Hujun Bao",
      "Xiaowei Zhou",
      "Sida Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_OCRT_Boosting_Foundation_Models_in_the_Open_World_with_Object-Concept-Relation_CVPR_2025_paper.html": {
    "title": "OCRT: Boosting Foundation Models in the Open World with Object-Concept-Relation Triad",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luyao Tang",
      "Yuxuan Yuan",
      "Chaoqi Chen",
      "Zeyu Zhang",
      "Yue Huang",
      "Kun Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_SPARS3R_Semantic_Prior_Alignment_and_Regularization_for_Sparse_3D_Reconstruction_CVPR_2025_paper.html": {
    "title": "SPARS3R: Semantic Prior Alignment and Regularization for Sparse 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutao Tang",
      "Yuxiang Guo",
      "Deming Li",
      "Cheng Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_VidBot_Learning_Generalizable_3D_Actions_from_In-the-Wild_2D_Human_Videos_CVPR_2025_paper.html": {
    "title": "VidBot: Learning Generalizable 3D Actions from In-the-Wild 2D Human Videos for Zero-Shot Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanzhi Chen",
      "Boyang Sun",
      "Anran Zhang",
      "Marc Pollefeys",
      "Stefan Leutenegger"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mao_Learning_Person-Specific_Animatable_Face_Models_from_In-the-Wild_Images_via_a_CVPR_2025_paper.html": {
    "title": "Learning Person-Specific Animatable Face Models from In-the-Wild Images via a Shared Base Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiang Mao",
      "Zhenfeng Fan",
      "ZhiJie Zhang",
      "Zhiheng Zhang",
      "Shihong Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_TIMotion_Temporal_and_Interactive_Framework_for_Efficient_Human-Human_Motion_Generation_CVPR_2025_paper.html": {
    "title": "TIMotion: Temporal and Interactive Framework for Efficient Human-Human Motion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yabiao Wang",
      "Shuo Wang",
      "Jiangning Zhang",
      "Ke Fan",
      "Jiafu Wu",
      "Zhucun Xue",
      "Yong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View_CVPR_2025_paper.html": {
    "title": "Which Viewpoint Shows it Best? Language for Weakly Supervising View Selection in Multi-view Instructional Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sagnik Majumder",
      "Tushar Nagarajan",
      "Ziad Al-Halah",
      "Reina Pradhan",
      "Kristen Grauman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chu_RaCFormer_Towards_High-Quality_3D_Object_Detection_via_Query-based_Radar-Camera_Fusion_CVPR_2025_paper.html": {
    "title": "RaCFormer: Towards High-Quality 3D Object Detection via Query-based Radar-Camera Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaomeng Chu",
      "Jiajun Deng",
      "Guoliang You",
      "Yifan Duan",
      "Houqiang Li",
      "Yanyong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_Hybrid_Reciprocal_Transformer_with_Triplet_Feature_Alignment_for_Scene_Graph_CVPR_2025_paper.html": {
    "title": "Hybrid Reciprocal Transformer with Triplet Feature Alignment for Scene Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Fu",
      "Tiantian Zhang",
      "Kai Chen",
      "Qi Dou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos_CVPR_2025_paper.html": {
    "title": "Understanding Multi-Task Activities from Single-Task Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Shen",
      "Ehsan Elhamifar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Co-Speech_Gesture_Video_Generation_with_Implicit_Motion-Audio_Entanglement_CVPR_2025_paper.html": {
    "title": "Co-Speech Gesture Video Generation with Implicit Motion-Audio Entanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinjie Li",
      "Ziyi Chen",
      "Xinlu Yu",
      "Iek-Heng Chu",
      "Peng Chang",
      "Jing Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_TransPixeler_Advancing_Text-to-Video_Generation_with_Transparency_CVPR_2025_paper.html": {
    "title": "TransPixeler: Advancing Text-to-Video Generation with Transparency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luozhou Wang",
      "Yijun Li",
      "Zhifei Chen",
      "Jui-Hsien Wang",
      "Zhifei Zhang",
      "He Zhang",
      "Zhe Lin",
      "Ying-Cong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Adaptive_Keyframe_Sampling_for_Long_Video_Understanding_CVPR_2025_paper.html": {
    "title": "Adaptive Keyframe Sampling for Long Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Tang",
      "Jihao Qiu",
      "Lingxi Xie",
      "Yunjie Tian",
      "Jianbin Jiao",
      "Qixiang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kaduri_Whats_in_the_Image_A_Deep-Dive_into_the_Vision_of_CVPR_2025_paper.html": {
    "title": "What's in the Image? A Deep-Dive into the Vision of Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omri Kaduri",
      "Shai Bagon",
      "Tali Dekel"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peng_Person_De-reidentification_A_Variation-guided_Identity_Shift_Modeling_CVPR_2025_paper.html": {
    "title": "Person De-reidentification: A Variation-guided Identity Shift Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Xing Peng",
      "Yu-Ming Tang",
      "Kun-Yu Lin",
      "Qize Yang",
      "Jingke Meng",
      "Xihan Wei",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_FreeSim_Toward_Free-viewpoint_Camera_Simulation_in_Driving_Scenes_CVPR_2025_paper.html": {
    "title": "FreeSim: Toward Free-viewpoint Camera Simulation in Driving Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lue Fan",
      "Hao Zhang",
      "Qitai Wang",
      "Hongsheng Li",
      "Zhaoxiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sami_Gradient_Inversion_Attacks_on_Parameter-Efficient_Fine-Tuning_CVPR_2025_paper.html": {
    "title": "Gradient Inversion Attacks on Parameter-Efficient Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hasin Us Sami",
      "Swapneel Sen",
      "Amit K. Roy-Chowdhury",
      "Srikanth V. Krishnamurthy",
      "Basak Guler"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_UPME_An_Unsupervised_Peer_Review_Framework_for_Multimodal_Large_Language_CVPR_2025_paper.html": {
    "title": "UPME: An Unsupervised Peer Review Framework for Multimodal Large Language Model Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihui Zhang",
      "Munan Ning",
      "Zheyuan Liu",
      "Yue Huang",
      "Shuo Yang",
      "Yanbo Wang",
      "Jiayi Ye",
      "Xiao Chen",
      "Yibing Song",
      "Li Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_DiGIT_Multi-Dilated_Gated_Encoder_and_Central-Adjacent_Region_Integrated_Decoder_for_CVPR_2025_paper.html": {
    "title": "DiGIT: Multi-Dilated Gated Encoder and Central-Adjacent Region Integrated Decoder for Temporal Action Detection Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ho-Joong Kim",
      "Yearang Lee",
      "Jung-Ho Hong",
      "Seong-Whan Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_MBQ_Modality-Balanced_Quantization_for_Large_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "MBQ: Modality-Balanced Quantization for Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyao Li",
      "Yingchun Hu",
      "Xuefei Ning",
      "Xihui Liu",
      "Ke Hong",
      "Xiaotao Jia",
      "Xiuhong Li",
      "Yaqi Yan",
      "Pei Ran",
      "Guohao Dai",
      "Shengen Yan",
      "Huazhong Yang",
      "Yu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Florence-VL_Enhancing_Vision-Language_Models_with_Generative_Vision_Encoder_and_Depth-Breadth_CVPR_2025_paper.html": {
    "title": "Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiuhai Chen",
      "Jianwei Yang",
      "Haiping Wu",
      "Dianqi Li",
      "Jianfeng Gao",
      "Tianyi Zhou",
      "Bin Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_VideoDPO_Omni-Preference_Alignment_for_Video_Diffusion_Generation_CVPR_2025_paper.html": {
    "title": "VideoDPO: Omni-Preference Alignment for Video Diffusion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runtao Liu",
      "Haoyu Wu",
      "Ziqiang Zheng",
      "Chen Wei",
      "Yingqing He",
      "Renjie Pi",
      "Qifeng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_Seq2Time_Sequential_Knowledge_Transfer_for_Video_LLM_Temporal_Grounding_CVPR_2025_paper.html": {
    "title": "Seq2Time: Sequential Knowledge Transfer for Video LLM Temporal Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andong Deng",
      "Zhongpai Gao",
      "Anwesa Choudhuri",
      "Benjamin Planche",
      "Meng Zheng",
      "Bin Wang",
      "Terrence Chen",
      "Chen Chen",
      "Ziyan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_GPVK-VL_Geometry-Preserving_Virtual_Keyframes_for_Visual_Localization_under_Large_Viewpoint_CVPR_2025_paper.html": {
    "title": "GPVK-VL: Geometry-Preserving Virtual Keyframes for Visual Localization under Large Viewpoint Changes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunxuan Li",
      "Lei Fan",
      "Xiaoying Xing",
      "Jianxiong Zhou",
      "Ying Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "Realistic Test-Time Adaptation of Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxime Zanella",
      "Clément Fuchs",
      "Christophe De Vleeschouwer",
      "Ismail Ben Ayed"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kang_SelfSplat_Pose-Free_and_3D_Prior-Free_Generalizable_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "SelfSplat: Pose-Free and 3D Prior-Free Generalizable 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gyeongjin Kang",
      "Jisang Yoo",
      "Jihyeon Park",
      "Seungtae Nam",
      "Hyeonsoo Im",
      "Sangheon Shin",
      "Sangpil Kim",
      "Eunbyung Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Enhancing_Virtual_Try-On_with_Synthetic_Pairs_and_Error-Aware_Noise_Scheduling_CVPR_2025_paper.html": {
    "title": "Enhancing Virtual Try-On with Synthetic Pairs and Error-Aware Noise Scheduling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nannan Li",
      "Kevin J. Shih",
      "Bryan A. Plummer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lai_Exploring_Simple_Open-Vocabulary_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Exploring Simple Open-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihang Lai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MP-GUI_Modality_Perception_with_MLLMs_for_GUI_Understanding_CVPR_2025_paper.html": {
    "title": "MP-GUI: Modality Perception with MLLMs for GUI Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziwei Wang",
      "Weizhi Chen",
      "Leyang Yang",
      "Sheng Zhou",
      "Shengchu Zhao",
      "Hanbei Zhan",
      "Jiongchao Jin",
      "Liangcheng Li",
      "Zirui Shao",
      "Jiajun Bu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Associative_Transformer_CVPR_2025_paper.html": {
    "title": "Associative Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwei Sun",
      "Hideya Ochiai",
      "Zhirong Wu",
      "Stephen Lin",
      "Ryota Kanai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_Improving_Adversarial_Transferability_on_Vision_Transformers_via_Forward_Propagation_Refinement_CVPR_2025_paper.html": {
    "title": "Improving Adversarial Transferability on Vision Transformers via Forward Propagation Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Ren",
      "Zhengyu Zhao",
      "Chenhao Lin",
      "Bo Yang",
      "Lu Zhou",
      "Zhe Liu",
      "Chao Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pei_Seeing_What_Matters_Empowering_CLIP_with_Patch_Generation-to-Selection_CVPR_2025_paper.html": {
    "title": "Seeing What Matters: Empowering CLIP with Patch Generation-to-Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gensheng Pei",
      "Tao Chen",
      "Yujia Wang",
      "Xinhao Cai",
      "Xiangbo Shu",
      "Tianfei Zhou",
      "Yazhou Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bian_ChatGarment_Garment_Estimation_Generation_and_Editing_via_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "ChatGarment: Garment Estimation, Generation and Editing via Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Bian",
      "Chenghao Xu",
      "Yuliang Xiu",
      "Artur Grigorev",
      "Zhen Liu",
      "Cewu Lu",
      "Michael J. Black",
      "Yao Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Enhancing_Online_Continual_Learning_with_Plug-and-Play_State_Space_Model_and_CVPR_2025_paper.html": {
    "title": "Enhancing Online Continual Learning with Plug-and-Play State Space Model and Class-Conditional Mixture of Discretization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sihao Liu",
      "Yibo Yang",
      "Xiaojie Li",
      "David A. Clifton",
      "Bernard Ghanem"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_RDD_Robust_Feature_Detector_and_Descriptor_using_Deformable_Transformer_CVPR_2025_paper.html": {
    "title": "RDD: Robust Feature Detector and Descriptor using Deformable Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gonglin Chen",
      "Tianwen Fu",
      "Haiwei Chen",
      "Wenbin Teng",
      "Hanyuan Xiao",
      "Yajie Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Building_Vision_Models_upon_Heat_Conduction_CVPR_2025_paper.html": {
    "title": "Building Vision Models upon Heat Conduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaozhi Wang",
      "Yue Liu",
      "Yunjie Tian",
      "Yunfan Liu",
      "Yaowei Wang",
      "Qixiang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_GRAPHGPT-O_Synergistic_Multimodal_Comprehension_and_Generation_on_Graphs_CVPR_2025_paper.html": {
    "title": "GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Fang",
      "Bowen Jin",
      "Jiacheng Shen",
      "Sirui Ding",
      "Qiaoyu Tan",
      "Jiawei Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Model_Poisoning_Attacks_to_Federated_Learning_via_Multi-Round_Consistency_CVPR_2025_paper.html": {
    "title": "Model Poisoning Attacks to Federated Learning via Multi-Round Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yueqi Xie",
      "Minghong Fang",
      "Neil Zhenqiang Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via_CVPR_2025_paper.html": {
    "title": "TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented Reality via 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianchuan Chen",
      "Jingchuan Hu",
      "Gaige Wang",
      "Zhonghua Jiang",
      "Tiansong Zhou",
      "Zhiwen Chen",
      "Chengfei Lv"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Erasing_Undesirable_Influence_in_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Erasing Undesirable Influence in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Wu",
      "Trung Le",
      "Munawar Hayat",
      "Mehrtash Harandi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Meng_LT3SD_Latent_Trees_for_3D_Scene_Diffusion_CVPR_2025_paper.html": {
    "title": "LT3SD: Latent Trees for 3D Scene Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quan Meng",
      "Lei Li",
      "Matthias Nießner",
      "Angela Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_Stacking_Brick_by_Brick_Aligned_Feature_Isolation_for_Incremental_Face_CVPR_2025_paper.html": {
    "title": "Stacking Brick by Brick: Aligned Feature Isolation for Incremental Face Forgery Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jikang Cheng",
      "Zhiyuan Yan",
      "Ying Zhang",
      "Li Hao",
      "Jiaxin Ai",
      "Qin Zou",
      "Chen Li",
      "Zhongyuan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_CO-SPY_Combining_Semantic_and_Pixel_Features_to_Detect_Synthetic_Images_CVPR_2025_paper.html": {
    "title": "CO-SPY: Combining Semantic and Pixel Features to Detect Synthetic Images by AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Cheng",
      "Lingjuan Lyu",
      "Zhenting Wang",
      "Xiangyu Zhang",
      "Vikash Sehwag"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Meng_Closest_Neighbors_are_Harmful_for_Lightweight_Masked_Auto-encoders_CVPR_2025_paper.html": {
    "title": "Closest Neighbors are Harmful for Lightweight Masked Auto-encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Meng",
      "Ahmed Hasssan",
      "Li Yang",
      "Deliang Fan",
      "Jinwoo Shin",
      "Jae-sun Seo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive_CVPR_2025_paper.html": {
    "title": "CraftsMan3D: High-fidelity Mesh Generation with 3D Native Diffusion and Interactive Geometry Refiner",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiyu Li",
      "Jiarui Liu",
      "Hongyu Yan",
      "Rui Chen",
      "Yixun Liang",
      "Xuelin Chen",
      "Ping Tan",
      "Xiaoxiao Long"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Decouple-Then-Merge_Finetune_Diffusion_Models_as_Multi-Task_Learning_CVPR_2025_paper.html": {
    "title": "Decouple-Then-Merge: Finetune Diffusion Models as Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianli Ma",
      "Xuefei Ning",
      "Dongrui Liu",
      "Li Niu",
      "Linfeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ebrahimi_GIF_Generative_Inspiration_for_Face_Recognition_at_Scale_CVPR_2025_paper.html": {
    "title": "GIF: Generative Inspiration for Face Recognition at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saeed Ebrahimi",
      "Sahar Rahimi",
      "Ali Dabouei",
      "Srinjoy Das",
      "Jeremy M. Dawson",
      "Nasser M. Nasrabadi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation_CVPR_2025_paper.html": {
    "title": "HELVIPAD: A Real-World Dataset for Omnidirectional Stereo Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehdi Zayene",
      "Jannik Endres",
      "Albias Havolli",
      "Charles Corbière",
      "Salim Cherkaoui",
      "Alexandre Kontouli",
      "Alexandre Alahi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_GENMANIP_LLM-driven_Simulation_for_Generalizable_Instruction-Following_Manipulation_CVPR_2025_paper.html": {
    "title": "GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ning Gao",
      "Yilun Chen",
      "Shuai Yang",
      "Xinyi Chen",
      "Yang Tian",
      "Hao Li",
      "Haifeng Huang",
      "Hanqing Wang",
      "Tai Wang",
      "Jiangmiao Pang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons_CVPR_2025_paper.html": {
    "title": "SKDream: Controllable Multi-view and 3D Generation with Arbitrary Skeletons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanyou Xu",
      "Zongxin Yang",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving_CVPR_2025_paper.html": {
    "title": "Towards Enhanced Image Inpainting: Mitigating Unwanted Object Insertion and Preserving Color Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yikai Wang",
      "Chenjie Cao",
      "Junqiu Yu",
      "Ke Fan",
      "Xiangyang Xue",
      "Yanwei Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Optimus-2_Multimodal_Minecraft_Agent_with_Goal-Observation-Action_Conditioned_Policy_CVPR_2025_paper.html": {
    "title": "Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zaijing Li",
      "Yuquan Xie",
      "Rui Shao",
      "Gongwei Chen",
      "Dongmei Jiang",
      "Liqiang Nie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jin_Classic_Video_Denoising_in_a_Machine_Learning_World_Robust_Fast_CVPR_2025_paper.html": {
    "title": "Classic Video Denoising in a Machine Learning World: Robust, Fast, and Controllable",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Jin",
      "Simon Niklaus",
      "Zhoutong Zhang",
      "Zhihao Xia",
      "Chunle Guo",
      "Yuting Yang",
      "Jiawen Chen",
      "Chongyi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tzamos_Practical_Solutions_to_the_Relative_Pose_of_Three_Calibrated_Cameras_CVPR_2025_paper.html": {
    "title": "Practical Solutions to the Relative Pose of Three Calibrated Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charalambos Tzamos",
      "Viktor Kocur",
      "Yaqing Ding",
      "Daniel Barath",
      "Zuzana Berger Haladova",
      "Torsten Sattler",
      "Zuzana Kukelova"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Localized_Concept_Erasure_for_Text-to-Image_Diffusion_Models_Using_Training-Free_Gated_CVPR_2025_paper.html": {
    "title": "Localized Concept Erasure for Text-to-Image Diffusion Models Using Training-Free Gated Low-Rank Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byung Hyun Lee",
      "Sungjin Lim",
      "Se Young Chun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Schmalfuss_PARC_A_Quantitative_Framework_Uncovering_the_Symmetries_within_Vision_Language_CVPR_2025_paper.html": {
    "title": "PARC: A Quantitative Framework Uncovering the Symmetries within Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jenny Schmalfuss",
      "Nadine Chang",
      "Vibashan VS",
      "Maying Shen",
      "Andres Bruhn",
      "Jose M. Alvarez"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins_CVPR_2025_paper.html": {
    "title": "RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Mu",
      "Tianxing Chen",
      "Zanxin Chen",
      "Shijia Peng",
      "Zhiqian Lan",
      "Zeyu Gao",
      "Zhixuan Liang",
      "Qiaojun Yu",
      "Yude Zou",
      "Mingkun Xu",
      "Lunkai Lin",
      "Zhiqiang Xie",
      "Mingyu Ding",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Population_Normalization_for_Federated_Learning_CVPR_2025_paper.html": {
    "title": "Population Normalization for Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoyao Wang",
      "Fan Yi",
      "Peizhu Gong",
      "Caitou He",
      "Cheng Jin",
      "Weizhong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lei_AnimateAnything_Consistent_and_Controllable_Animation_for_Video_Generation_CVPR_2025_paper.html": {
    "title": "AnimateAnything: Consistent and Controllable Animation for Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guojun Lei",
      "Chi Wang",
      "Rong Zhang",
      "Yikai Wang",
      "Hong Li",
      "Weiwei Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sinitsyn_PRaDA_Projective_Radial_Distortion_Averaging_CVPR_2025_paper.html": {
    "title": "PRaDA: Projective Radial Distortion Averaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniil Sinitsyn",
      "Linus Härenstam-Nielsen",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_GenAssets_Generating_in-the-wild_3D_Assets_in_Latent_Space_CVPR_2025_paper.html": {
    "title": "GenAssets: Generating in-the-wild 3D Assets in Latent Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ze Yang",
      "Jingkang Wang",
      "Haowei Zhang",
      "Sivabalan Manivasagam",
      "Yun Chen",
      "Raquel Urtasun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dumitriu_RipVIS_Rip_Currents_Video_Instance_Segmentation_Benchmark_for_Beach_Monitoring_CVPR_2025_paper.html": {
    "title": "RipVIS: Rip Currents Video Instance Segmentation Benchmark for Beach Monitoring and Safety",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrei Dumitriu",
      "Florin Tatui",
      "Florin Miron",
      "Aakash Ralhan",
      "Radu Tudor Ionescu",
      "Radu Timofte"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ta_Low-Rank_Adaptation_in_Multilinear_Operator_Networks_for_Security-Preserving_Incremental_Learning_CVPR_2025_paper.html": {
    "title": "Low-Rank Adaptation in Multilinear Operator Networks for Security-Preserving Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huu Binh Ta",
      "Duc Nguyen",
      "Quyen Tran",
      "Toan Tran",
      "Tung Pham"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted_CVPR_2025_paper.html": {
    "title": "Camera Resection from Known Line Pencils and a Radially Distorted Scanline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juan C. Dibene",
      "Enrique Dunn"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bekci_ESCAPE_Equivariant_Shape_Completion_via_Anchor_Point_Encoding_CVPR_2025_paper.html": {
    "title": "ESCAPE: Equivariant Shape Completion via Anchor Point Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Burak Bekci",
      "Nassir Navab",
      "Federico Tombari",
      "Mahdi Saleh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liao_SPC-GS_Gaussian_Splatting_with_Semantic-Prompt_Consistency_for_Indoor_Open-World_Free-view_CVPR_2025_paper.html": {
    "title": "SPC-GS: Gaussian Splatting with Semantic-Prompt Consistency for Indoor Open-World Free-view Synthesis from Sparse Inputs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guibiao Liao",
      "Qing Li",
      "Zhenyu Bao",
      "Guoping Qiu",
      "Kanglin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_M3amba_Memory_Mamba_is_All_You_Need_for_Whole_Slide_CVPR_2025_paper.html": {
    "title": "M3amba: Memory Mamba is All You Need for Whole Slide Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tingting Zheng",
      "Kui Jiang",
      "Yi Xiao",
      "Sicheng Zhao",
      "Hongxun Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Satellite_to_GroundScape_-_Large-scale_Consistent_Ground_View_Generation_from_CVPR_2025_paper.html": {
    "title": "Satellite to GroundScape - Large-scale Consistent Ground View Generation from Satellite Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ningli Xu",
      "Rongjun Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Samira_Variance-Based_Membership_Inference_Attacks_Against_Large-Scale_Image_Captioning_Models_CVPR_2025_paper.html": {
    "title": "Variance-Based Membership Inference Attacks Against Large-Scale Image Captioning Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Samira",
      "Edan Habler",
      "Yuval Elovici",
      "Asaf Shabtai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_Redefining_Creative_in_Dictionary_Towards_an_Enhanced_Semantic_Understanding_of_CVPR_2025_paper.html": {
    "title": "Redefining <Creative> in Dictionary: Towards an Enhanced Semantic Understanding of Creative Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fu Feng",
      "Yucheng Xie",
      "Xu Yang",
      "Jing Wang",
      "Xin Geng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Terris_FiRe_Fixed-points_of_Restoration_Priors_for_Solving_Inverse_Problems_CVPR_2025_paper.html": {
    "title": "FiRe: Fixed-points of Restoration Priors for Solving Inverse Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthieu Terris",
      "Ulugbek S. Kamilov",
      "Thomas Moreau"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Learning_Dynamic_Collaborative_Network_for_Semi-supervised_3D_Vessel_Segmentation_CVPR_2025_paper.html": {
    "title": "Learning Dynamic Collaborative Network for Semi-supervised 3D Vessel Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiao Xu",
      "Xin Chen",
      "Lihe Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition_CVPR_2025_paper.html": {
    "title": "Temporal Alignment-Free Video Matching for Few-shot Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SuBeen Lee",
      "WonJun Moon",
      "Hyun Seok Seong",
      "Jae-Pil Heo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/C_OSLoPrompt_Bridging_Low-Supervision_Challenges_and_Open-Set_Domain_Generalization_in_CLIP_CVPR_2025_paper.html": {
    "title": "OSLoPrompt: Bridging Low-Supervision Challenges and Open-Set Domain Generalization in CLIP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamad Hassan N C",
      "Divyam Gupta",
      "Mainak Singha",
      "Sai Bhargav Rongali",
      "Ankit Jha",
      "Muhammad Haris Khan",
      "Biplab Banerjee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Dinomaly_The_Less_Is_More_Philosophy_in_Multi-Class_Unsupervised_Anomaly_CVPR_2025_paper.html": {
    "title": "Dinomaly: The Less Is More Philosophy in Multi-Class Unsupervised Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia Guo",
      "Shuai Lu",
      "Weihang Zhang",
      "Fang Chen",
      "Huiqi Li",
      "Hongen Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_VLog_Video-Language_Models_by_Generative_Retrieval_of_Narration_Vocabulary_CVPR_2025_paper.html": {
    "title": "VLog: Video-Language Models by Generative Retrieval of Narration Vocabulary",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Qinghong Lin",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_CoMBO_Conflict_Mitigation_via_Branched_Optimization_for_Class_Incremental_Segmentation_CVPR_2025_paper.html": {
    "title": "CoMBO: Conflict Mitigation via Branched Optimization for Class Incremental Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Fang",
      "Anqi Zhang",
      "Guangyu Gao",
      "Jianbo Jiao",
      "Chi Harold Liu",
      "Yunchao Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Forensics-Bench_A_Comprehensive_Forgery_Detection_Benchmark_Suite_for_Large_Vision_CVPR_2025_paper.html": {
    "title": "Forensics-Bench: A Comprehensive Forgery Detection Benchmark Suite for Large Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Wang",
      "Chenghui Lv",
      "Xian Li",
      "Shichao Dong",
      "Huadong Li",
      "Kelu Yao",
      "Chao Li",
      "Wenqi Shao",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Detect-and-Guide_Self-regulation_of_Diffusion_Models_for_Safe_Text-to-Image_Generation_via_CVPR_2025_paper.html": {
    "title": "Detect-and-Guide: Self-regulation of Diffusion Models for Safe Text-to-Image Generation via Guideline Token Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feifei Li",
      "Mi Zhang",
      "Yiming Sun",
      "Min Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dhiman_MirrorVerse_Pushing_Diffusion_Models_to_Realistically_Reflect_the_World_CVPR_2025_paper.html": {
    "title": "MirrorVerse: Pushing Diffusion Models to Realistically Reflect the World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ankit Dhiman",
      "Manan Shah",
      "R Venkatesh Babu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dai_EAP-GS_Efficient_Augmentation_of_Pointcloud_for_3D_Gaussian_Splatting_in_CVPR_2025_paper.html": {
    "title": "EAP-GS: Efficient Augmentation of Pointcloud for 3D Gaussian Splatting in Few-shot Scene Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongrui Dai",
      "Yuxiang Xing"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yuan_Empowering_Large_Language_Models_with_3D_Situation_Awareness_CVPR_2025_paper.html": {
    "title": "Empowering Large Language Models with 3D Situation Awareness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Yuan",
      "Yibo Peng",
      "Jinke Ren",
      "Yinghong Liao",
      "Yatong Han",
      "Chun-Mei Feng",
      "Hengshuang Zhao",
      "Guanbin Li",
      "Shuguang Cui",
      "Zhen Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nguyen_Forensic_Self-Descriptions_Are_All_You_Need_for_Zero-Shot_Detection_Open-Set_CVPR_2025_paper.html": {
    "title": "Forensic Self-Descriptions Are All You Need for Zero-Shot Detection, Open-Set Source Attribution, and Clustering of AI-generated Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tai D. Nguyen",
      "Aref Azizpour",
      "Matthew C. Stamm"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xing_EchoTraffic_Enhancing_Traffic_Anomaly_Understanding_with_Audio-Visual_Insights_CVPR_2025_paper.html": {
    "title": "EchoTraffic: Enhancing Traffic Anomaly Understanding with Audio-Visual Insights",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenghao Xing",
      "Hao Chen",
      "Binzhu Xie",
      "Jiaqi Xu",
      "Ziyu Guo",
      "Xuemiao Xu",
      "Jianye Hao",
      "Chi-Wing Fu",
      "Xiaowei Hu",
      "Pheng-Ann Heng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_FlexDrive_Toward_Trajectory_Flexibility_in_Driving_Scene_Gaussian_Splatting_Reconstruction_CVPR_2025_paper.html": {
    "title": "FlexDrive: Toward Trajectory Flexibility in Driving Scene Gaussian Splatting Reconstruction and Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingqiu Zhou",
      "Lue Fan",
      "Linjiang Huang",
      "Xiaoyu Shi",
      "Si Liu",
      "Zhaoxiang Zhang",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian_CVPR_2025_paper.html": {
    "title": "Taming Video Diffusion Prior with Scene-Grounding Guidance for 3D Gaussian Splatting from Sparse Inputs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingji Zhong",
      "Zhihao Li",
      "Dave Zhenyu Chen",
      "Lanqing Hong",
      "Dan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_Interactive_Medical_Image_Segmentation_A_Benchmark_Dataset_and_Baseline_CVPR_2025_paper.html": {
    "title": "Interactive Medical Image Segmentation: A Benchmark Dataset and Baseline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junlong Cheng",
      "Bin Fu",
      "Jin Ye",
      "Guoan Wang",
      "Tianbin Li",
      "Haoyu Wang",
      "Ruoyu Li",
      "He Yao",
      "Junren Cheng",
      "Jingwen Li",
      "Yanzhou Su",
      "Min Zhu",
      "Junjun He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities_CVPR_2025_paper.html": {
    "title": "GigaHands: A Massive Annotated Dataset of Bimanual Hand Activities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rao Fu",
      "Dingxi Zhang",
      "Alex Jiang",
      "Wanjia Fu",
      "Austin Funk",
      "Daniel Ritchie",
      "Srinath Sridhar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lian_AutoSSVH_Exploring_Automated_Frame_Sampling_for_Efficient_Self-Supervised_Video_Hashing_CVPR_2025_paper.html": {
    "title": "AutoSSVH: Exploring Automated Frame Sampling for Efficient Self-Supervised Video Hashing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niu Lian",
      "Jun Li",
      "Jinpeng Wang",
      "Ruisheng Luo",
      "Yaowei Wang",
      "Shu-Tao Xia",
      "Bin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cocchi_Augmenting_Multimodal_LLMs_with_Self-Reflective_Tokens_for_Knowledge-based_Visual_Question_CVPR_2025_paper.html": {
    "title": "Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Federico Cocchi",
      "Nicholas Moratelli",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_DifIISR_A_Diffusion_Model_with_Gradient_Guidance_for_Infrared_Image_CVPR_2025_paper.html": {
    "title": "DifIISR: A Diffusion Model with Gradient Guidance for Infrared Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyuan Li",
      "Zirui Wang",
      "Yang Zou",
      "Zhixin Chen",
      "Jun Ma",
      "Zhiying Jiang",
      "Long Ma",
      "Jinyuan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Recurrent_Feature_Mining_and_Keypoint_Mixup_Padding_for_Category-Agnostic_Pose_CVPR_2025_paper.html": {
    "title": "Recurrent Feature Mining and Keypoint Mixup Padding for Category-Agnostic Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Chen",
      "Weilong Chen",
      "Yifan Zuo",
      "Yuming Fang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_FrugalNeRF_Fast_Convergence_for_Extreme_Few-shot_Novel_View_Synthesis_without_CVPR_2025_paper.html": {
    "title": "FrugalNeRF: Fast Convergence for Extreme Few-shot Novel View Synthesis without Learned Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chin-Yang Lin",
      "Chung-Ho Wu",
      "Chang-Han Yeh",
      "Shih-Han Yen",
      "Cheng Sun",
      "Yu-Lun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jang_3D-GSW_3D_Gaussian_Splatting_for_Robust_Watermarking_CVPR_2025_paper.html": {
    "title": "3D-GSW: 3D Gaussian Splatting for Robust Watermarking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngdong Jang",
      "Hyunje Park",
      "Feng Yang",
      "Heeju Ko",
      "Euijin Choo",
      "Sangpil Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Pioneering_4-Bit_FP_Quantization_for_Diffusion_Models_Mixup-Sign_Quantization_and_CVPR_2025_paper.html": {
    "title": "Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maosen Zhao",
      "Pengtao Chen",
      "Chong Yu",
      "Yan Wen",
      "Xudong Tan",
      "Tao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation_CVPR_2025_paper.html": {
    "title": "OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengfei Zhou",
      "Xiaopeng Peng",
      "Jiajun Song",
      "Chuanhao Li",
      "Zhaopan Xu",
      "Yue Yang",
      "Ziyao Guo",
      "Hao Zhang",
      "Yuqi Lin",
      "Yefei He",
      "Lirui Zhao",
      "Shuo Liu",
      "Tianhua Li",
      "Yuxuan Xie",
      "Xiaojun Chang",
      "Yu Qiao",
      "Wenqi Shao",
      "Kaipeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Choi_Dual_Exposure_Stereo_for_Extended_Dynamic_Range_3D_Imaging_CVPR_2025_paper.html": {
    "title": "Dual Exposure Stereo for Extended Dynamic Range 3D Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juhyung Choi",
      "Jinnyeong Kim",
      "Seokjun Choi",
      "Jinwoo Lee",
      "Samuel Brucker",
      "Mario Bijelic",
      "Felix Heide",
      "Seung-Hwan Baek"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shvetsova_Unbiasing_through_Textual_Descriptions_Mitigating_Representation_Bias_in_Video_Benchmarks_CVPR_2025_paper.html": {
    "title": "Unbiasing through Textual Descriptions: Mitigating Representation Bias in Video Benchmarks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nina Shvetsova",
      "Arsha Nagrani",
      "Bernt Schiele",
      "Hilde Kuehne",
      "Christian Rupprecht"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Embodied_Scene_Understanding_for_Vision_Language_Models_via_MetaVQA_CVPR_2025_paper.html": {
    "title": "Embodied Scene Understanding for Vision Language Models via MetaVQA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weizhen Wang",
      "Chenda Duan",
      "Zhenghao Peng",
      "Yuxin Liu",
      "Bolei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ge_CompGS_Unleashing_2D_Compositionality_for_Compositional_Text-to-3D_via_Dynamically_Optimizing_CVPR_2025_paper.html": {
    "title": "CompGS: Unleashing 2D Compositionality for Compositional Text-to-3D via Dynamically Optimizing 3D Gaussians",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chongjian Ge",
      "Chenfeng Xu",
      "Yuanfeng Ji",
      "Chensheng Peng",
      "Masayoshi Tomizuka",
      "Ping Luo",
      "Mingyu Ding",
      "Varun Jampani",
      "Wei Zhan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shao_Learning_Temporally_Consistent_Video_Depth_from_Video_Diffusion_Priors_CVPR_2025_paper.html": {
    "title": "Learning Temporally Consistent Video Depth from Video Diffusion Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Shao",
      "Yuanbo Yang",
      "Hongyu Zhou",
      "Youmin Zhang",
      "Yujun Shen",
      "Vitor Guizilini",
      "Yue Wang",
      "Matteo Poggi",
      "Yiyi Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chu_FIRE_Robust_Detection_of_Diffusion-Generated_Images_via_Frequency-Guided_Reconstruction_Error_CVPR_2025_paper.html": {
    "title": "FIRE: Robust Detection of Diffusion-Generated Images via Frequency-Guided Reconstruction Error",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beilin Chu",
      "Xuan Xu",
      "Xin Wang",
      "Yufei Zhang",
      "Weike You",
      "Linna Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models_CVPR_2025_paper.html": {
    "title": "Assessing and Learning Alignment of Unimodal Vision and Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Le Zhang",
      "Qian Yang",
      "Aishwarya Agrawal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection_CVPR_2025_paper.html": {
    "title": "Samba: A Unified Mamba-based Framework for General Salient Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao He",
      "Keren Fu",
      "Xiaohong Liu",
      "Qijun Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Action_Detail_Matters_Refining_Video_Recognition_with_Local_Action_Queries_CVPR_2025_paper.html": {
    "title": "Action Detail Matters: Refining Video Recognition with Local Action Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengmeng Wang",
      "Zeyi Huang",
      "Xiangjie Kong",
      "Guojiang Shen",
      "Guang Dai",
      "Jingdong Wang",
      "Yong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_PAVE_Patching_and_Adapting_Video_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "PAVE: Patching and Adapting Video Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoming Liu",
      "Yiquan Li",
      "Khoi Duc Nguyen",
      "Yiwu Zhong",
      "Yin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rokuss_LesionLocator_Zero-Shot_Universal_Tumor_Segmentation_and_Tracking_in_3D_Whole-Body_CVPR_2025_paper.html": {
    "title": "LesionLocator: Zero-Shot Universal Tumor Segmentation and Tracking in 3D Whole-Body Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilian Rokuss",
      "Yannick Kirchhoff",
      "Seval Akbal",
      "Balint Kovacs",
      "Saikat Roy",
      "Constantin Ulrich",
      "Tassilo Wald",
      "Lukas T. Rotkopf",
      "Heinz-Peter Schlemmer",
      "Klaus Maier-Hein"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_Generative_Map_Priors_for_Collaborative_BEV_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Generative Map Priors for Collaborative BEV Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahui Fu",
      "Yue Gong",
      "Luting Wang",
      "Shifeng Zhang",
      "Xu Zhou",
      "Si Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Coherent_3D_Portrait_Video_Reconstruction_via_Triplane_Fusion_CVPR_2025_paper.html": {
    "title": "Coherent 3D Portrait Video Reconstruction via Triplane Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengze Wang",
      "Xueting Li",
      "Chao Liu",
      "Matthew Chan",
      "Michael Stengel",
      "Henry Fuchs",
      "Shalini De Mello",
      "Koki Nagano"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Generative_Image_Layer_Decomposition_with_Visual_Effects_CVPR_2025_paper.html": {
    "title": "Generative Image Layer Decomposition with Visual Effects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinrui Yang",
      "Qing Liu",
      "Yijun Li",
      "Soo Ye Kim",
      "Daniil Pakhomov",
      "Mengwei Ren",
      "Jianming Zhang",
      "Zhe Lin",
      "Cihang Xie",
      "Yuyin Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_AR-Diffusion_Asynchronous_Video_Generation_with_Auto-Regressive_Diffusion_CVPR_2025_paper.html": {
    "title": "AR-Diffusion: Asynchronous Video Generation with Auto-Regressive Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingzhen Sun",
      "Weining Wang",
      "Gen Li",
      "Jiawei Liu",
      "Jiahui Sun",
      "Wanquan Feng",
      "Shanshan Lao",
      "Siyu Zhou",
      "Qian He",
      "Jing Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping_CVPR_2025_paper.html": {
    "title": "ManiVideo: Generating Hand-Object Manipulation Video with Dexterous and Generalizable Grasping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youxin Pang",
      "Ruizhi Shao",
      "Jiajun Zhang",
      "Hanzhang Tu",
      "Yun Liu",
      "Boyao Zhou",
      "Hongwen Zhang",
      "Yebin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DOF-GS_Adjustable_Depth-of-Field_3D_Gaussian_Splatting_for_Post-Capture_Refocusing_Defocus_CVPR_2025_paper.html": {
    "title": "DOF-GS: Adjustable Depth-of-Field 3D Gaussian Splatting for Post-Capture Refocusing, Defocus Rendering and Blur Removal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Wang",
      "Praneeth Chakravarthula",
      "Baoquan Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qi_The_Photographers_Eye_Teaching_Multimodal_Large_Language_Models_to_See_CVPR_2025_paper.html": {
    "title": "The Photographer's Eye: Teaching Multimodal Large Language Models to See, and Critique Like Photographers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daiqing Qi",
      "Handong Zhao",
      "Jing Shi",
      "Simon Jenni",
      "Yifei Fan",
      "Franck Dernoncourt",
      "Scott Cohen",
      "Sheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Revisiting_Audio-Visual_Segmentation_with_Vision-Centric_Transformer_CVPR_2025_paper.html": {
    "title": "Revisiting Audio-Visual Segmentation with Vision-Centric Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaofei Huang",
      "Rui Ling",
      "Tianrui Hui",
      "Hongyu Li",
      "Xu Zhou",
      "Shifeng Zhang",
      "Si Liu",
      "Richang Hong",
      "Meng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Synergizing_Motion_and_Appearance_Multi-Scale_Compensatory_Codebooks_for_Talking_Head_CVPR_2025_paper.html": {
    "title": "Synergizing Motion and Appearance: Multi-Scale Compensatory Codebooks for Talking Head Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuling Zhao",
      "Fa-Ting Hong",
      "Xiaoshui Huang",
      "Dan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_HOIGPT_Learning_Long-Sequence_Hand-Object_Interaction_with_Language_Models_CVPR_2025_paper.html": {
    "title": "HOIGPT: Learning Long-Sequence Hand-Object Interaction with Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingzhen Huang",
      "Fu-Jen Chu",
      "Bugra Tekin",
      "Kevin J. Liang",
      "Haoyu Ma",
      "Weiyao Wang",
      "Xingyu Chen",
      "Pierre Gleize",
      "Hongfei Xue",
      "Siwei Lyu",
      "Kris Kitani",
      "Matt Feiszli",
      "Hao Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bie_GraphI2P_Image-to-Point_Cloud_Registration_with_Exploring_Pattern_of_Correspondence_via_CVPR_2025_paper.html": {
    "title": "GraphI2P: Image-to-Point Cloud Registration with Exploring Pattern of Correspondence via Graph Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Bie",
      "Shouan Pan",
      "Siqi Li",
      "Yining Zhao",
      "Yue Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_SoftVQ-VAE_Efficient_1-Dimensional_Continuous_Tokenizer_CVPR_2025_paper.html": {
    "title": "SoftVQ-VAE: Efficient 1-Dimensional Continuous Tokenizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Chen",
      "Ze Wang",
      "Xiang Li",
      "Ximeng Sun",
      "Fangyi Chen",
      "Jiang Liu",
      "Jindong Wang",
      "Bhiksha Raj",
      "Zicheng Liu",
      "Emad Barsoum"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hao_FedCS_Coreset_Selection_for_Federated_Learning_CVPR_2025_paper.html": {
    "title": "FedCS: Coreset Selection for Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhe Hao",
      "Weiying Xie",
      "Daixun Li",
      "Haonan Qin",
      "Hangyu Ye",
      "Leyuan Fang",
      "Yunsong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_DPC_Dual-Prompt_Collaboration_for_Tuning_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "DPC: Dual-Prompt Collaboration for Tuning Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyang Li",
      "Liang Wang",
      "Chao Wang",
      "Jing Jiang",
      "Yan Peng",
      "Guodong Long"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xing_Dual-Granularity_Semantic_Guided_Sparse_Routing_Diffusion_Model_for_General_Pansharpening_CVPR_2025_paper.html": {
    "title": "Dual-Granularity Semantic Guided Sparse Routing Diffusion Model for General Pansharpening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinghui Xing",
      "Litao Qu",
      "Shizhou Zhang",
      "Di Xu",
      "Yingkun Yang",
      "Yanning Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_AIM-Fair_Advancing_Algorithmic_Fairness_via_Selectively_Fine-Tuning_Biased_Models_with_CVPR_2025_paper.html": {
    "title": "AIM-Fair: Advancing Algorithmic Fairness via Selectively Fine-Tuning Biased Models with Contextual Synthetic Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zengqun Zhao",
      "Ziquan Liu",
      "Yu Cao",
      "Shaogang Gong",
      "Ioannis Patras"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chu_Robust_Multi-Object_4D_Generation_for_In-the-wild_Videos_CVPR_2025_paper.html": {
    "title": "Robust Multi-Object 4D Generation for In-the-wild Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen-Hsuan Chu",
      "Lei Ke",
      "Jianmeng Liu",
      "Mingxiao Huo",
      "Pavel Tokmakov",
      "Katerina Fragkiadaki"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_OmniMMI_A_Comprehensive_Multi-modal_Interaction_Benchmark_in_Streaming_Video_Contexts_CVPR_2025_paper.html": {
    "title": "OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Wang",
      "Yueqian Wang",
      "Bo Chen",
      "Tong Wu",
      "Dongyan Zhao",
      "Zilong Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_SOAP_Vision-Centric_3D_Semantic_Scene_Completion_with_Scene-Adaptive_Decoder_and_CVPR_2025_paper.html": {
    "title": "SOAP: Vision-Centric 3D Semantic Scene Completion with Scene-Adaptive Decoder and Occluded Region-Aware View Projection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyo-Jun Lee",
      "Yeong Jun Koh",
      "Hanul Kim",
      "Hyunseop Kim",
      "Yonguk Lee",
      "Jinu Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Snaebjarnarson_Taxonomy-Aware_Evaluation_of_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "Taxonomy-Aware Evaluation of Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vésteinn Snæbjarnarson",
      "Kevin Du",
      "Niklas Stoehr",
      "Serge Belongie",
      "Ryan Cotterell",
      "Nico Lang",
      "Stella Frank"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Active_Event-based_Stereo_Vision_CVPR_2025_paper.html": {
    "title": "Active Event-based Stereo Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianing Li",
      "Yunjian Zhang",
      "Haiqian Han",
      "Xiangyang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_Mono-InternVL_Pushing_the_Boundaries_of_Monolithic_Multimodal_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gen Luo",
      "Xue Yang",
      "Wenhan Dou",
      "Zhaokai Wang",
      "Jiawen Liu",
      "Jifeng Dai",
      "Yu Qiao",
      "Xizhou Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Trevithick_SimVS_Simulating_World_Inconsistencies_for_Robust_View_Synthesis_CVPR_2025_paper.html": {
    "title": "SimVS: Simulating World Inconsistencies for Robust View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Trevithick",
      "Roni Paiss",
      "Philipp Henzler",
      "Dor Verbin",
      "Rundi Wu",
      "Hadi Alzayer",
      "Ruiqi Gao",
      "Ben Poole",
      "Jonathan T. Barron",
      "Aleksander Holynski",
      "Ravi Ramamoorthi",
      "Pratul P. Srinivasan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_FLAVC_Learned_Video_Compression_with_Feature_Level_Attention_CVPR_2025_paper.html": {
    "title": "FLAVC: Learned Video Compression with Feature Level Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun Zhang",
      "Heming Sun",
      "Jiro Katto"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qu_An_End-to-End_Robust_Point_Cloud_Semantic_Segmentation_Network_with_Single-Step_CVPR_2025_paper.html": {
    "title": "An End-to-End Robust Point Cloud Semantic Segmentation Network with Single-Step Conditional Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wentao Qu",
      "Jing Wang",
      "YongShun Gong",
      "Xiaoshui Huang",
      "Liang Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_From_Zero_to_Detail_Deconstructing_Ultra-High-Definition_Image_Restoration_from_Progressive_CVPR_2025_paper.html": {
    "title": "From Zero to Detail: Deconstructing Ultra-High-Definition Image Restoration from Progressive Spectral Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Zhao",
      "Zhizhou Chen",
      "Yunzhe Xu",
      "Enxuan Gu",
      "Jian Li",
      "Zili Yi",
      "Qian Wang",
      "Jian Yang",
      "Ying Tai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Thoker_SMILE_Infusing_Spatial_and_Motion_Semantics_in_Masked_Video_Learning_CVPR_2025_paper.html": {
    "title": "SMILE: Infusing Spatial and Motion Semantics in Masked Video Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fida Mohammad Thoker",
      "Letian Jiang",
      "Chen Zhao",
      "Bernard Ghanem"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Video_Language_Model_Pretraining_with_Spatio-temporal_Masking_CVPR_2025_paper.html": {
    "title": "Video Language Model Pretraining with Spatio-temporal Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Wu",
      "Zhaobo Qi",
      "Junshu Sun",
      "Yaowei Wang",
      "Qingming Huang",
      "Shuhui Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_COSMOS_Cross-Modality_Self-Distillation_for_Vision_Language_Pre-training_CVPR_2025_paper.html": {
    "title": "COSMOS: Cross-Modality Self-Distillation for Vision Language Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanghwan Kim",
      "Rui Xiao",
      "Mariana-Iuliana Georgescu",
      "Stephan Alaniz",
      "Zeynep Akata"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion_CVPR_2025_paper.html": {
    "title": "Lifting Motion to the 3D World via 2D Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaman Li",
      "C. Karen Liu",
      "Jiajun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_TAPT_Test-Time_Adversarial_Prompt_Tuning_for_Robust_Inference_in_Vision-Language_CVPR_2025_paper.html": {
    "title": "TAPT: Test-Time Adversarial Prompt Tuning for Robust Inference in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Wang",
      "Kai Chen",
      "Jiaming Zhang",
      "Jingjing Chen",
      "Xingjun Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Udandarao_Active_Data_Curation_Effectively_Distills_Large-Scale_Multimodal_Models_CVPR_2025_paper.html": {
    "title": "Active Data Curation Effectively Distills Large-Scale Multimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vishaal Udandarao",
      "Nikhil Parthasarathy",
      "Muhammad Ferjad Naeem",
      "Talfan Evans",
      "Samuel Albanie",
      "Federico Tombari",
      "Yongqin Xian",
      "Alessio Tonioni",
      "Olivier J. Henaff"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wei_PCDreamer_Point_Cloud_Completion_Through_Multi-view_Diffusion_Priors_CVPR_2025_paper.html": {
    "title": "PCDreamer: Point Cloud Completion Through Multi-view Diffusion Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangshun Wei",
      "Yuan Feng",
      "Long Ma",
      "Chen Wang",
      "Yuanfeng Zhou",
      "Changjian Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model_CVPR_2025_paper.html": {
    "title": "Your ViT is Secretly an Image Segmentation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tommie Kerssies",
      "Niccolò Cavagnero",
      "Alexander Hermans",
      "Narges Norouzi",
      "Giuseppe Averta",
      "Bastian Leibe",
      "Gijs Dubbelman",
      "Daan de Geus"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mao_Cross-Rejective_Open-Set_SAR_Image_Registration_CVPR_2025_paper.html": {
    "title": "Cross-Rejective Open-Set SAR Image Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shasha Mao",
      "Shiming Lu",
      "Zhaolong Du",
      "Licheng Jiao",
      "Shuiping Gou",
      "Luntian Mou",
      "Xuequan Lu",
      "Lin Xiong",
      "Yimeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Synthetic_Data_is_an_Elegant_GIFT_for_Continual_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "Synthetic Data is an Elegant GIFT for Continual Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Wu",
      "Wuxuan Shi",
      "Jinqiao Wang",
      "Mang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_SplineGS_Robust_Motion-Adaptive_Spline_for_Real-Time_Dynamic_3D_Gaussians_from_CVPR_2025_paper.html": {
    "title": "SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongmin Park",
      "Minh-Quan Viet Bui",
      "Juan Luis Gonzalez Bello",
      "Jaeho Moon",
      "Jihyong Oh",
      "Munchurl Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style_CVPR_2025_paper.html": {
    "title": "SCSA: A Plug-and-Play Semantic Continuous-Sparse Attention for Arbitrary Semantic Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunnan Shang",
      "Zhizhong Wang",
      "Hongwei Wang",
      "Xiangming Meng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion_CVPR_2025_paper.html": {
    "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Liu",
      "Shiwei Zhang",
      "Xiaofeng Wang",
      "Yujie Wei",
      "Haonan Qiu",
      "Yuzhong Zhao",
      "Yingya Zhang",
      "Qixiang Ye",
      "Fang Wan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Cant_Slow_Me_Down_Learning_Robust_and_Hardware-Adaptive_Object_Detectors_CVPR_2025_paper.html": {
    "title": "Can't Slow Me Down: Learning Robust and Hardware-Adaptive Object Detectors against Latency Attacks for Edge Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Wang",
      "Zichen Wang",
      "Cong Wang",
      "Yuanchao Shu",
      "Ruilong Deng",
      "Peng Cheng",
      "Jiming Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jeong_Multi-modal_Knowledge_Distillation-based_Human_Trajectory_Forecasting_CVPR_2025_paper.html": {
    "title": "Multi-modal Knowledge Distillation-based Human Trajectory Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaewoo Jeong",
      "Seohee Lee",
      "Daehee Park",
      "Giwon Lee",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_SAM2Object_Consolidating_View_Consistency_via_SAM2_for_Zero-Shot_3D_Instance_CVPR_2025_paper.html": {
    "title": "SAM2Object: Consolidating View Consistency via SAM2 for Zero-Shot 3D Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihuai Zhao",
      "Junbao Zhuo",
      "Jiansheng Chen",
      "Huimin Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dubinski_CDI_Copyrighted_Data_Identification_in_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "CDI: Copyrighted Data Identification in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Dubiński",
      "Antoni Kowalczuk",
      "Franziska Boenisch",
      "Adam Dziedzic"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fixelle_Hypergraph_Vision_Transformers_Images_are_More_than_Nodes_More_than_CVPR_2025_paper.html": {
    "title": "Hypergraph Vision Transformers: Images are More than Nodes, More than Edges",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua Fixelle"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hou_Binarized_Neural_Network_for_Multi-spectral_Image_Fusion_CVPR_2025_paper.html": {
    "title": "Binarized Neural Network for Multi-spectral Image Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junming Hou",
      "Xiaoyu Chen",
      "Ran Ran",
      "Xiaofeng Cong",
      "Xinyang Liu",
      "Jian Wei You",
      "Liang-Jian Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation_CVPR_2025_paper.html": {
    "title": "CRISP: Object Pose and Shape Estimation with Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingnan Shi",
      "Rajat Talak",
      "Harry Zhang",
      "David Jin",
      "Luca Carlone"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_ShiftwiseConv_Small_Convolutional_Kernel_with_Large_Kernel_Effect_CVPR_2025_paper.html": {
    "title": "ShiftwiseConv: Small Convolutional Kernel with Large Kernel Effect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dachong Li",
      "Li Li",
      "Zhuangzhuang Chen",
      "Jianqiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_GaussianIP_Identity-Preserving_Realistic_3D_Human_Generation_via_Human-Centric_Diffusion_Prior_CVPR_2025_paper.html": {
    "title": "GaussianIP: Identity-Preserving Realistic 3D Human Generation via Human-Centric Diffusion Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichen Tang",
      "Yuan Yao",
      "Miaomiao Cui",
      "Liefeng Bo",
      "Hongyu Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanxi Liu",
      "Yifang Men",
      "Zhouhui Lian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment_CVPR_2025_paper.html": {
    "title": "FineVQ: Fine-Grained User Generated Content Video Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiyu Duan",
      "Qiang Hu",
      "Jiarui Wang",
      "Liu Yang",
      "Zitong Xu",
      "Lu Liu",
      "Xiongkuo Min",
      "Chunlei Cai",
      "Tianxiao Ye",
      "Xiaoyun Zhang",
      "Guangtao Zhai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Unveiling_the_Ignorance_of_MLLMs_Seeing_Clearly_Answering_Incorrectly_CVPR_2025_paper.html": {
    "title": "Unveiling the Ignorance of MLLMs: Seeing Clearly, Answering Incorrectly",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yexin Liu",
      "Zhengyang Liang",
      "Yueze Wang",
      "Xianfeng Wu",
      "Feilong Tang",
      "Muyang He",
      "Jian Li",
      "Zheng Liu",
      "Harry Yang",
      "Sernam Lim",
      "Bo Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_Object-Shot_Enhanced_Grounding_Network_for_Egocentric_Video_CVPR_2025_paper.html": {
    "title": "Object-Shot Enhanced Grounding Network for Egocentric Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yisen Feng",
      "Haoyu Zhang",
      "Meng Liu",
      "Weili Guan",
      "Liqiang Nie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zatsarynna_MANTA_Diffusion_Mamba_for_Efficient_and_Effective_Stochastic_Long-Term_Dense_CVPR_2025_paper.html": {
    "title": "MANTA: Diffusion Mamba for Efficient and Effective Stochastic Long-Term Dense Action Anticipation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olga Zatsarynna",
      "Emad Bahrami",
      "Yazan Abu Farha",
      "Gianpiero Francesca",
      "Juergen Gall"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_METASCENES_Towards_Automated_Replica_Creation_for_Real-world_3D_Scans_CVPR_2025_paper.html": {
    "title": "METASCENES: Towards Automated Replica Creation for Real-world 3D Scans",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huangyue Yu",
      "Baoxiong Jia",
      "Yixin Chen",
      "Yandan Yang",
      "Puhao Li",
      "Rongpeng Su",
      "Jiaxin Li",
      "Qing Li",
      "Wei Liang",
      "Song-Chun Zhu",
      "Tengyu Liu",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Robust_Multimodal_Survival_Prediction_with_Conditional_Latent_Differentiation_Variational_AutoEncoder_CVPR_2025_paper.html": {
    "title": "Robust Multimodal Survival Prediction with Conditional Latent Differentiation Variational AutoEncoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Zhou",
      "Jiao Tang",
      "Yingli Zuo",
      "Peng Wan",
      "Daoqiang Zhang",
      "Wei Shao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rahimi_Sim-to-Real_Causal_Transfer_A_Metric_Learning_Approach_to_Causally-Aware_Interaction_CVPR_2025_paper.html": {
    "title": "Sim-to-Real Causal Transfer: A Metric Learning Approach to Causally-Aware Interaction Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmad Rahimi",
      "Po-Chien Luan",
      "Yuejiang Liu",
      "Frano Rajič",
      "Alexandre Alahi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with_CVPR_2025_paper.html": {
    "title": "Ev-3DOD: Pushing the Temporal Boundaries of 3D Object Detection with Event Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoonhee Cho",
      "Jae-Young Kang",
      "Youngho Kim",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Quan_Zero-Shot_Blind-spot_Image_Denoising_via_Implicit_Neural_Sampling_CVPR_2025_paper.html": {
    "title": "Zero-Shot Blind-spot Image Denoising via Implicit Neural Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhui Quan",
      "Tianxiang Zheng",
      "Zhiyuan Ma",
      "Hui Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ahn_Nearly_Zero-Cost_Protection_Against_Mimicry_by_Personalized_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Nearly Zero-Cost Protection Against Mimicry by Personalized Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Namhyuk Ahn",
      "KiYoon Yoo",
      "Wonhyuk Ahn",
      "Daesik Kim",
      "Seung-Hun Nam"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Tripartite_Weight-Space_Ensemble_for_Few-Shot_Class-Incremental_Learning_CVPR_2025_paper.html": {
    "title": "Tripartite Weight-Space Ensemble for Few-Shot Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juntae Lee",
      "Munawar Hayat",
      "Sungrack Yun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gong_The_Devil_is_in_Temporal_Token_High_Quality_Video_Reasoning_CVPR_2025_paper.html": {
    "title": "The Devil is in Temporal Token: High Quality Video Reasoning Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sitong Gong",
      "Yunzhi Zhuge",
      "Lu Zhang",
      "Zongxin Yang",
      "Pingping Zhang",
      "Huchuan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mei_PerLA_Perceptive_3D_Language_Assistant_CVPR_2025_paper.html": {
    "title": "PerLA: Perceptive 3D Language Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guofeng Mei",
      "Wei Lin",
      "Luigi Riz",
      "Yujiao Wu",
      "Fabio Poiesi",
      "Yiming Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_LITA-GS_Illumination-Agnostic_Novel_View_Synthesis_via_Reference-Free_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "LITA-GS: Illumination-Agnostic Novel View Synthesis via Reference-Free 3D Gaussian Splatting and Physical Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Zhou",
      "Wei Dong",
      "Jun Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xue_PhyT2V_LLM-Guided_Iterative_Self-Refinement_for_Physics-Grounded_Text-to-Video_Generation_CVPR_2025_paper.html": {
    "title": "PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyao Xue",
      "Xiangyu Yin",
      "Boyuan Yang",
      "Wei Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jeong_Track4Gen_Teaching_Video_Diffusion_Models_to_Track_Points_Improves_Video_CVPR_2025_paper.html": {
    "title": "Track4Gen: Teaching Video Diffusion Models to Track Points Improves Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeonho Jeong",
      "Chun-Hao P. Huang",
      "Jong Chul Ye",
      "Niloy J. Mitra",
      "Duygu Ceylan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qi_Mask2DiT_Dual_Mask-based_Diffusion_Transformer_for_Multi-Scene_Long_Video_Generation_CVPR_2025_paper.html": {
    "title": "Mask^2DiT: Dual Mask-based Diffusion Transformer for Multi-Scene Long Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhao Qi",
      "Jianlong Yuan",
      "Wanquan Feng",
      "Shancheng Fang",
      "Jiawei Liu",
      "SiYu Zhou",
      "Qian He",
      "Hongtao Xie",
      "Yongdong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_JamMa_Ultra-lightweight_Local_Feature_Matching_with_Joint_Mamba_CVPR_2025_paper.html": {
    "title": "JamMa: Ultra-lightweight Local Feature Matching with Joint Mamba",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyong Lu",
      "Songlin Du"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tao_DyCoke_Dynamic_Compression_of_Tokens_for_Fast_Video_Large_Language_CVPR_2025_paper.html": {
    "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keda Tao",
      "Can Qin",
      "Haoxuan You",
      "Yang Sui",
      "Huan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Flotho_T-FAKE_Synthesizing_Thermal_Images_for_Facial_Landmarking_CVPR_2025_paper.html": {
    "title": "T-FAKE: Synthesizing Thermal Images for Facial Landmarking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philipp Flotho",
      "Moritz Piening",
      "Anna Kukleva",
      "Gabriele Steidl"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Albastaki_Multi-Resolution_Pathology-Language_Pre-training_Model_with_Text-Guided_Visual_Representation_CVPR_2025_paper.html": {
    "title": "Multi-Resolution Pathology-Language Pre-training Model with Text-Guided Visual Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shahad Albastaki",
      "Anabia Sohail",
      "Iyyakutti Iyappan Ganapathi",
      "Basit Alawode",
      "Asim Khan",
      "Sajid Javed",
      "Naoufel Werghi",
      "Mohammed Bennamoun",
      "Arif Mahmood"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and_CVPR_2025_paper.html": {
    "title": "InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinlu Zhang",
      "Yixin Chen",
      "Zan Wang",
      "Jie Yang",
      "Yizhou Wang",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals_CVPR_2025_paper.html": {
    "title": "MammAlps: A Multi-view Video Behavior Monitoring Dataset of Wild Mammals in the Swiss Alps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Valentin Gabeff",
      "Haozhe Qi",
      "Brendan Flaherty",
      "Gencer Sumbul",
      "Alexander Mathis",
      "Devis Tuia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling_CVPR_2025_paper.html": {
    "title": "Diffusion-based Realistic Listening Head Generation via Hybrid Motion Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinuo Wang",
      "Yanbo Fan",
      "Xuan Wang",
      "Guo Yu",
      "Fei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Su_SAT-HMR_Real-Time_Multi-Person_3D_Mesh_Estimation_via_Scale-Adaptive_Tokens_CVPR_2025_paper.html": {
    "title": "SAT-HMR: Real-Time Multi-Person 3D Mesh Estimation via Scale-Adaptive Tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi Su",
      "Xiaoxuan Ma",
      "Jiajun Su",
      "Yizhou Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_PICD_Versatile_Perceptual_Image_Compression_with_Diffusion_Rendering_CVPR_2025_paper.html": {
    "title": "PICD: Versatile Perceptual Image Compression with Diffusion Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongda Xu",
      "Jiahao Li",
      "Bin Li",
      "Yan Wang",
      "Ya-Qin Zhang",
      "Yan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_UniScene_Unified_Occupancy-centric_Driving_Scene_Generation_CVPR_2025_paper.html": {
    "title": "UniScene: Unified Occupancy-centric Driving Scene Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bohan Li",
      "Jiazhe Guo",
      "Hongsi Liu",
      "Yingshuang Zou",
      "Yikang Ding",
      "Xiwu Chen",
      "Hu Zhu",
      "Feiyang Tan",
      "Chi Zhang",
      "Tiancai Wang",
      "Shuchang Zhou",
      "Li Zhang",
      "Xiaojuan Qi",
      "Hao Zhao",
      "Mu Yang",
      "Wenjun Zeng",
      "Xin Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Wonderland_Navigating_3D_Scenes_from_a_Single_Image_CVPR_2025_paper.html": {
    "title": "Wonderland: Navigating 3D Scenes from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanwen Liang",
      "Junli Cao",
      "Vidit Goel",
      "Guocheng Qian",
      "Sergei Korolev",
      "Demetri Terzopoulos",
      "Konstantinos N. Plataniotis",
      "Sergey Tulyakov",
      "Jian Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_Learning_from_Streaming_Video_with_Orthogonal_Gradients_CVPR_2025_paper.html": {
    "title": "Learning from Streaming Video with Orthogonal Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tengda Han",
      "Dilara Gokay",
      "Joseph Heyward",
      "Chuhan Zhang",
      "Daniel Zoran",
      "Viorica Patraucean",
      "Joao Carreira",
      "Dima Damen",
      "Andrew Zisserman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_Towards_Satellite_Image_Road_Graph_Extraction_A_Global-Scale_Dataset_and_CVPR_2025_paper.html": {
    "title": "Towards Satellite Image Road Graph Extraction: A Global-Scale Dataset and A Novel Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pan Yin",
      "Kaiyu Li",
      "Xiangyong Cao",
      "Jing Yao",
      "Lei Liu",
      "Xueru Bai",
      "Feng Zhou",
      "Deyu Meng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_SuperLightNet_Lightweight_Parameter_Aggregation_Network_for_Multimodal_Brain_Tumor_Segmentation_CVPR_2025_paper.html": {
    "title": "SuperLightNet: Lightweight Parameter Aggregation Network for Multimodal Brain Tumor Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Yu",
      "Jiacheng Cao",
      "Li Liu",
      "Minghua Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gonzalez_VideoSPatS_Video_SPatiotemporal_Splines_for_Disentangled_Occlusion_Appearance_and_Motion_CVPR_2025_paper.html": {
    "title": "VideoSPatS: Video SPatiotemporal Splines for Disentangled Occlusion, Appearance and Motion Modeling and Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juan Luis Gonzalez",
      "Xu Yao",
      "Alex Whelan",
      "Kyle Olszewski",
      "Hyeongwoo Kim",
      "Pablo Garrido"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guimard_Classifier-to-Bias_Toward_Unsupervised_Automatic_Bias_Detection_for_Visual_Classifiers_CVPR_2025_paper.html": {
    "title": "Classifier-to-Bias: Toward Unsupervised Automatic Bias Detection for Visual Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quentin Guimard",
      "Moreno D'Incà",
      "Massimiliano Mancini",
      "Elisa Ricci"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shrivastava_Self-Supervised_Spatial_Correspondence_Across_Modalities_CVPR_2025_paper.html": {
    "title": "Self-Supervised Spatial Correspondence Across Modalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayush Shrivastava",
      "Andrew Owens"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_MOS-Attack_A_Scalable_Multi-objective_Adversarial_Attack_Framework_CVPR_2025_paper.html": {
    "title": "MOS-Attack: A Scalable Multi-objective Adversarial Attack Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ping Guo",
      "Cheng Gong",
      "Xi Lin",
      "Fei Liu",
      "Zhichao Lu",
      "Qingfu Zhang",
      "Zhenkun Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_Six-CD_Benchmarking_Concept_Removals_for_Text-to-image_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Six-CD: Benchmarking Concept Removals for Text-to-image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Ren",
      "Kangrui Chen",
      "Yingqian Cui",
      "Shenglai Zeng",
      "Hui Liu",
      "Yue Xing",
      "Jiliang Tang",
      "Lingjuan Lyu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pandey_Motion_Modes_What_Could_Happen_Next_CVPR_2025_paper.html": {
    "title": "Motion Modes: What Could Happen Next?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karran Pandey",
      "Yannick Hold-Geoffroy",
      "Matheus Gadelha",
      "Niloy J. Mitra",
      "Karan Singh",
      "Paul Guerrero"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Finer-CAM_Spotting_the_Difference_Reveals_Finer_Details_for_Visual_Explanation_CVPR_2025_paper.html": {
    "title": "Finer-CAM: Spotting the Difference Reveals Finer Details for Visual Explanation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziheng Zhang",
      "Jianyang Gu",
      "Arpita Chowdhury",
      "Zheda Mai",
      "David Carlyn",
      "Tanya Berger-Wolf",
      "Yu Su",
      "Wei-Lun Chao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Benidir_The_Change_You_Want_To_Detect_Semantic_Change_Detection_In_CVPR_2025_paper.html": {
    "title": "The Change You Want To Detect: Semantic Change Detection In Earth Observation With Hybrid Data Generationf",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanis Benidir",
      "Nicolas Gonthier",
      "Clement Mallet"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Black-Box Forgery Attacks on Semantic Watermarks for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas Müller",
      "Denis Lukovnikov",
      "Jonas Thietke",
      "Asja Fischer",
      "Erwin Quiring"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hui_An_Image-like_Diffusion_Method_for_Human-Object_Interaction_Detection_CVPR_2025_paper.html": {
    "title": "An Image-like Diffusion Method for Human-Object Interaction Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofei Hui",
      "Haoxuan Qu",
      "Hossein Rahmani",
      "Jun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VidSeg_Training-free_Video_Semantic_Segmentation_based_on_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "VidSeg: Training-free Video Semantic Segmentation based on Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Wang",
      "Abdelrahman Eldesokey",
      "Mohit Mendiratta",
      "Fangneng Zhan",
      "Adam Kortylewski",
      "Christian Theobalt",
      "Peter Wonka"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_COB-GS_Clear_Object_Boundaries_in_3DGS_Segmentation_Based_on_Boundary-Adaptive_CVPR_2025_paper.html": {
    "title": "COB-GS: Clear Object Boundaries in 3DGS Segmentation Based on Boundary-Adaptive Gaussian Splitting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Zhang",
      "Junjun Jiang",
      "Youyu Chen",
      "Kui Jiang",
      "Xianming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Weakly_Supervised_Semantic_Segmentation_via_Progressive_Confidence_Region_Expansion_CVPR_2025_paper.html": {
    "title": "Weakly Supervised Semantic Segmentation via Progressive Confidence Region Expansion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangfeng Xu",
      "Pinyi Zhang",
      "Wenxuan Huang",
      "Yunhang Shen",
      "Haosheng Chen",
      "Jingzhong Lin",
      "Wei Li",
      "Gaoqi He",
      "Jiao Xie",
      "Shaohui Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Khosla_RELOCATE_A_Simple_Training-Free_Baseline_for_Visual_Query_Localization_Using_CVPR_2025_paper.html": {
    "title": "RELOCATE: A Simple Training-Free Baseline for Visual Query Localization Using Region-Based Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Savya Khosla",
      "Sethuraman T V",
      "Alexander Schwing",
      "Derek Hoiem"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cho_PEER_Pressure_Model-to-Model_Regularization_for_Single_Source_Domain_Generalization_CVPR_2025_paper.html": {
    "title": "PEER Pressure: Model-to-Model Regularization for Single Source Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Kyu Cho",
      "Inwoo Hwang",
      "Sanghack Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Griffiths_HOTFormerLoc_Hierarchical_Octree_Transformer_for_Versatile_Lidar_Place_Recognition_Across_CVPR_2025_paper.html": {
    "title": "HOTFormerLoc: Hierarchical Octree Transformer for Versatile Lidar Place Recognition Across Ground and Aerial Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ethan Griffiths",
      "Maryam Haghighat",
      "Simon Denman",
      "Clinton Fookes",
      "Milad Ramezani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qin_Revisiting_Fairness_in_Multitask_Learning_A_Performance-Driven_Approach_for_Variance_CVPR_2025_paper.html": {
    "title": "Revisiting Fairness in Multitask Learning: A Performance-Driven Approach for Variance Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohan Qin",
      "Xiaoxing Wang",
      "Junchi Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Piccinelli_UniK3D_Universal_Camera_Monocular_3D_Estimation_CVPR_2025_paper.html": {
    "title": "UniK3D: Universal Camera Monocular 3D Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luigi Piccinelli",
      "Christos Sakaridis",
      "Mattia Segu",
      "Yung-Hsu Yang",
      "Siyuan Li",
      "Wim Abbeloos",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_ConMo_Controllable_Motion_Disentanglement_and_Recomposition_for_Zero-Shot_Motion_Transfer_CVPR_2025_paper.html": {
    "title": "ConMo: Controllable Motion Disentanglement and Recomposition for Zero-Shot Motion Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Gao",
      "Zijin Yin",
      "Changcheng Hua",
      "Yuxin Peng",
      "Kongming Liang",
      "Zhanyu Ma",
      "Jun Guo",
      "Yang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_VideoMage_Multi-Subject_and_Motion_Customization_of_Text-to-Video_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "VideoMage: Multi-Subject and Motion Customization of Text-to-Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi-Pin Huang",
      "Yen-Siang Wu",
      "Hung-Kai Chung",
      "Kai-Po Chang",
      "Fu-En Yang",
      "Yu-Chiang Frank Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nguyen_AG-VPReID_A_Challenging_Large-Scale_Benchmark_for_Aerial-Ground_Video-based_Person_Re-Identification_CVPR_2025_paper.html": {
    "title": "AG-VPReID: A Challenging Large-Scale Benchmark for Aerial-Ground Video-based Person Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huy Nguyen",
      "Kien Nguyen",
      "Akila Pemasiri",
      "Feng Liu",
      "Sridha Sridharan",
      "Clinton Fookes"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking_CVPR_2025_paper.html": {
    "title": "EBS-EKF: Accurate and High Frequency Event-based Star Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Albert W. Reed",
      "Connor Hashemi",
      "Dennis Melamed",
      "Nitesh Menon",
      "Keigo Hirakawa",
      "Scott McCloskey"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_PersonaBooth_Personalized_Text-to-Motion_Generation_CVPR_2025_paper.html": {
    "title": "PersonaBooth: Personalized Text-to-Motion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boeun Kim",
      "Hea In Jeong",
      "JungHoon Sung",
      "Yihua Cheng",
      "Jeongmin Lee",
      "Ju Yong Chang",
      "Sang-Il Choi",
      "Younggeun Choi",
      "Saim Shin",
      "Jungho Kim",
      "Hyung Jin Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Al-Emadi_Benchmarking_Object_Detectors_under_Real-World_Distribution_Shifts_in_Satellite_Imagery_CVPR_2025_paper.html": {
    "title": "Benchmarking Object Detectors under Real-World Distribution Shifts in Satellite Imagery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sara A. Al-Emadi",
      "Yin Yang",
      "Ferda Ofli"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_SAIST_Segment_Any_Infrared_Small_Target_Model_Guided_by_Contrastive_CVPR_2025_paper.html": {
    "title": "SAIST: Segment Any Infrared Small Target Model Guided by Contrastive Language-Image Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingjin Zhang",
      "Xiaolong Li",
      "Fei Gao",
      "Jie Guo",
      "Xinbo Gao",
      "Jing Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peng_Star_with_Bilinear_Mapping_CVPR_2025_paper.html": {
    "title": "Star with Bilinear Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zelin Peng",
      "Yu Huang",
      "Zhengqin Xu",
      "Feilong Tang",
      "Ming Hu",
      "Xiaokang Yang",
      "Wei Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models_CVPR_2025_paper.html": {
    "title": "Closed-Loop Supervised Fine-Tuning of Tokenized Traffic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhejun Zhang",
      "Peter Karkus",
      "Maximilian Igl",
      "Wenhao Ding",
      "Yuxiao Chen",
      "Boris Ivanovic",
      "Marco Pavone"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "DIFIX3D+: Improving 3D Reconstructions with Single-Step Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jay Zhangjie Wu",
      "Yuxuan Zhang",
      "Haithem Turki",
      "Xuanchi Ren",
      "Jun Gao",
      "Mike Zheng Shou",
      "Sanja Fidler",
      "Zan Gojcic",
      "Huan Ling"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly_CVPR_2025_paper.html": {
    "title": "Time of the Flight of the Gaussians: Optimizing Depth Indirectly in Dynamic Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runfeng Li",
      "Mikhail Okunev",
      "Zixuan Guo",
      "Anh Ha Duong",
      "Christian Richardt",
      "Matthew O'Toole",
      "James Tompkin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos_CVPR_2025_paper.html": {
    "title": "Align3R: Aligned Monocular Depth Estimation for Dynamic Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Lu",
      "Tianyu Huang",
      "Peng Li",
      "Zhiyang Dou",
      "Cheng Lin",
      "Zhiming Cui",
      "Zhen Dong",
      "Sai-Kit Yeung",
      "Wenping Wang",
      "Yuan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection_CVPR_2025_paper.html": {
    "title": "Compositional Caching for Training-free Open-vocabulary Attribute Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Garosi",
      "Alessandro Conti",
      "Gaowen Liu",
      "Elisa Ricci",
      "Massimiliano Mancini"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_Seek_Common_Ground_While_Reserving_Differences_Semi-Supervised_Image-Text_Sentiment_Recognition_CVPR_2025_paper.html": {
    "title": "Seek Common Ground While Reserving Differences: Semi-Supervised Image-Text Sentiment Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wuyou Xia",
      "Guoli Jia",
      "Sicheng Zhao",
      "Jufeng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huynh_CoLLM_A_Large_Language_Model_for_Composed_Image_Retrieval_CVPR_2025_paper.html": {
    "title": "CoLLM: A Large Language Model for Composed Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuong Huynh",
      "Jinyu Yang",
      "Ashish Tawari",
      "Mubarak Shah",
      "Son Tran",
      "Raffay Hamid",
      "Trishul Chilimbi",
      "Abhinav Shrivastava"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Anomize_Better_Open_Vocabulary_Video_Anomaly_Detection_CVPR_2025_paper.html": {
    "title": "Anomize: Better Open Vocabulary Video Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Li",
      "Wenxuan Liu",
      "Jingjing Chen",
      "Ruixu Zhang",
      "Yuran Wang",
      "Xian Zhong",
      "Zheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lan_Efficient_Diffusion_as_Low_Light_Enhancer_CVPR_2025_paper.html": {
    "title": "Efficient Diffusion as Low Light Enhancer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanzhou Lan",
      "Qianli Ma",
      "Yuqi Yang",
      "Zhigang Wang",
      "Dong Wang",
      "Xuelong Li",
      "Bin Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_GraphMimic_Graph-to-Graphs_Generative_Modeling_from_Videos_for_Policy_Learning_CVPR_2025_paper.html": {
    "title": "GraphMimic: Graph-to-Graphs Generative Modeling from Videos for Policy Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangyan Chen",
      "Te Cui",
      "Meiling Wang",
      "Chengcai Yang",
      "Mengxiao Hu",
      "Haoyang Lu",
      "Yao Mu",
      "Zicai Peng",
      "Tianxing Zhou",
      "Xinran Jiang",
      "Yi Yang",
      "Yufeng Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Koneputugodage_VI3NR_Variance_Informed_Initialization_for_Implicit_Neural_Representations_CVPR_2025_paper.html": {
    "title": "VI^3NR: Variance Informed Initialization for Implicit Neural Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chamin Hewa Koneputugodage",
      "Yizhak Ben-Shabat",
      "Sameera Ramasinghe",
      "Stephen Gould"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_MMVU_Measuring_Expert-Level_Multi-Discipline_Video_Understanding_CVPR_2025_paper.html": {
    "title": "MMVU: Measuring Expert-Level Multi-Discipline Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilun Zhao",
      "Haowei Zhang",
      "Lujing Xie",
      "Tongyan Hu",
      "Guo Gan",
      "Yitao Long",
      "Zhiyuan Hu",
      "Weiyuan Chen",
      "Chuhan Li",
      "Zhijian Xu",
      "Chengye Wang",
      "Ziyao Shangguan",
      "Zhenwen Liang",
      "Yixin Liu",
      "Chen Zhao",
      "Arman Cohan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_M-LLM_Based_Video_Frame_Selection_for_Efficient_Video_Understanding_CVPR_2025_paper.html": {
    "title": "M-LLM Based Video Frame Selection for Efficient Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Hu",
      "Feng Gao",
      "Xiaohan Nie",
      "Peng Zhou",
      "Son Tran",
      "Tal Neiman",
      "Lingyun Wang",
      "Mubarak Shah",
      "Raffay Hamid",
      "Bing Yin",
      "Trishul Chilimbi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sidhu_Search_and_Detect_Training-Free_Long_Tail_Object_Detection_via_Web-Image_CVPR_2025_paper.html": {
    "title": "Search and Detect: Training-Free Long Tail Object Detection via Web-Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mankeerat Sidhu",
      "Hetarth Chopra",
      "Ansel Blume",
      "Jeonghwan Kim",
      "Revanth Gangi Reddy",
      "Heng Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions_CVPR_2025_paper.html": {
    "title": "EgoLM: Multi-Modal Language Model of Egocentric Motions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangzhou Hong",
      "Vladimir Guzov",
      "Hyo Jin Kim",
      "Yuting Ye",
      "Richard Newcombe",
      "Ziwei Liu",
      "Lingni Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Unleashing_the_Potential_of_Multi-modal_Foundation_Models_and_Video_Diffusion_CVPR_2025_paper.html": {
    "title": "Unleashing the Potential of Multi-modal Foundation Models and Video Diffusion for 4D Dynamic Physical Scene Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoman Liu",
      "Weicai Ye",
      "Yan Luximon",
      "Pengfei Wan",
      "Di Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Diffusion_Model_is_Effectively_Its_Own_Teacher_CVPR_2025_paper.html": {
    "title": "Diffusion Model is Effectively Its Own Teacher",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyin Ma",
      "Runpeng Yu",
      "Songhua Liu",
      "Gongfan Fang",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Long_Video_Diffusion_Generation_with_Segmented_Cross-Attention_and_Content-Rich_Video_CVPR_2025_paper.html": {
    "title": "Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Yan",
      "Yuxuan Cai",
      "Qiuyue Wang",
      "Yuan Zhou",
      "Wenhao Huang",
      "Huan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Learning_Heterogeneous_Tissues_with_Mixture_of_Experts_for_Gigapixel_Whole_CVPR_2025_paper.html": {
    "title": "Learning Heterogeneous Tissues with Mixture of Experts for Gigapixel Whole Slide Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junxian Wu",
      "Minheng Chen",
      "Xinyi Ke",
      "Tianwang Xun",
      "Xiaoming Jiang",
      "Hongyu Zhou",
      "Lizhi Shao",
      "Youyong Kong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pilligua_HyperNVD_Accelerating_Neural_Video_Decomposition_via_Hypernetworks_CVPR_2025_paper.html": {
    "title": "HyperNVD: Accelerating Neural Video Decomposition via Hypernetworks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maria Pilligua",
      "Danna Xue",
      "Javier Vazquez-Corral"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_UnCommon_Objects_in_3D_CVPR_2025_paper.html": {
    "title": "UnCommon Objects in 3D",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingchen Liu",
      "Piyush Tayal",
      "Jianyuan Wang",
      "Jesus Zarzar",
      "Tom Monnier",
      "Konstantinos Tertikas",
      "Jiali Duan",
      "Antoine Toisoul",
      "Jason Y. Zhang",
      "Natalia Neverova",
      "Andrea Vedaldi",
      "Roman Shapovalov",
      "David Novotny"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_Disentangled_Pose_and_Appearance_Guidance_for_Multi-Pose_Generation_CVPR_2025_paper.html": {
    "title": "Disentangled Pose and Appearance Guidance for Multi-Pose Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tengfei Xiao",
      "Yue Wu",
      "Yuelong Li",
      "Can Qin",
      "Maoguo Gong",
      "Qiguang Miao",
      "Wenping Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Mind_the_Gap_Confidence_Discrepancy_Can_Guide_Federated_Semi-Supervised_Learning_CVPR_2025_paper.html": {
    "title": "Mind the Gap: Confidence Discrepancy Can Guide Federated Semi-Supervised Learning Across Pseudo-Mismatch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijie Liu",
      "Xinyi Shang",
      "Yiqun Zhang",
      "Yang Lu",
      "Chen Gong",
      "Jing-Hao Xue",
      "Hanzi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lei_Instant_Adversarial_Purification_with_Adversarial_Consistency_Distillation_CVPR_2025_paper.html": {
    "title": "Instant Adversarial Purification with Adversarial Consistency Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun Tong Lei",
      "Hon Ming Yam",
      "Zhongliang Guo",
      "Yifei Qian",
      "Chun Pong Lau"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_Learning_Textual_Prompts_for_Open-World_Semi-Supervised_Learning_CVPR_2025_paper.html": {
    "title": "Learning Textual Prompts for Open-World Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Fan",
      "Junbiao Cui",
      "Jiye Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis_CVPR_2025_paper.html": {
    "title": "Electromyography-Informed Facial Expression Reconstruction for Physiological-Based Synthesis and Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Büchner",
      "Christoph Anders",
      "Orlando Guntinas-Lichius",
      "Joachim Denzler"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_LongDiff_Training-Free_Long_Video_Generation_in_One_Go_CVPR_2025_paper.html": {
    "title": "LongDiff: Training-Free Long Video Generation in One Go",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoling Li",
      "Hossein Rahmani",
      "Qiuhong Ke",
      "Jun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kansabanik_Feature_Selection_for_Latent_Factor_Models_CVPR_2025_paper.html": {
    "title": "Feature Selection for Latent Factor Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rittwika Kansabanik",
      "Adrian Barbu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Preserve_or_Modify_Context-Aware_Evaluation_for_Balancing_Preservation_and_Modification_CVPR_2025_paper.html": {
    "title": "Preserve or Modify? Context-Aware Evaluation for Balancing Preservation and Modification in Text-Guided Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoonjeon Kim",
      "Soohyun Ryu",
      "Yeonsung Jung",
      "Hyunkoo Lee",
      "Joowon Kim",
      "June Yong Yang",
      "Jaeryong Hwang",
      "Eunho Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Mask-Adapter_The_Devil_is_in_the_Masks_for_Open-Vocabulary_Segmentation_CVPR_2025_paper.html": {
    "title": "Mask-Adapter: The Devil is in the Masks for Open-Vocabulary Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongkang Li",
      "Tianheng Cheng",
      "Bin Feng",
      "Wenyu Liu",
      "Xinggang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous_CVPR_2025_paper.html": {
    "title": "MPDrive: Improving Spatial Understanding with Marker-Based Prompt Learning for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Zhang",
      "Xiaofan Li",
      "Zhihao Xu",
      "Wenjie Peng",
      "Zijian Zhou",
      "Miaojing Shi",
      "Shuangping Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Improving_the_Transferability_of_Adversarial_Attacks_on_Face_Recognition_with_CVPR_2025_paper.html": {
    "title": "Improving the Transferability of Adversarial Attacks on Face Recognition with Diverse Parameters Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengfan Zhou",
      "Bangjie Yin",
      "Hefei Ling",
      "Qianyu Zhou",
      "Wenxuan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qiu_Adapting_to_Observation_Length_of_Trajectory_Prediction_via_Contrastive_Learning_CVPR_2025_paper.html": {
    "title": "Adapting to Observation Length of Trajectory Prediction via Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiqi Qiu",
      "Jun Gong",
      "Xinyu Zhang",
      "Siqi Luo",
      "Bowen Zhang",
      "Yi Cen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Choi_Fine-Grained_Image-Text_Correspondence_with_Cost_Aggregation_for_Open-Vocabulary_Part_Segmentation_CVPR_2025_paper.html": {
    "title": "Fine-Grained Image-Text Correspondence with Cost Aggregation for Open-Vocabulary Part Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiho Choi",
      "Seonho Lee",
      "Minhyun Lee",
      "Seungho Lee",
      "Hyunjung Shim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_NitroFusion_High-Fidelity_Single-Step_Diffusion_through_Dynamic_Adversarial_Training_CVPR_2025_paper.html": {
    "title": "NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dar-Yen Chen",
      "Hmrishav Bandyopadhyay",
      "Kai Zou",
      "Yi-Zhe Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bu_ByTheWay_Boost_Your_Text-to-Video_Generation_Model_to_Higher_Quality_in_CVPR_2025_paper.html": {
    "title": "ByTheWay: Boost Your Text-to-Video Generation Model to Higher Quality in a Training-free Way",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiazi Bu",
      "Pengyang Ling",
      "Pan Zhang",
      "Tong Wu",
      "Xiaoyi Dong",
      "Yuhang Zang",
      "Yuhang Cao",
      "Dahua Lin",
      "Jiaqi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_CMMLoc_Advancing_Text-to-PointCloud_Localization_with_Cauchy-Mixture-Model_Based_Framework_CVPR_2025_paper.html": {
    "title": "CMMLoc: Advancing Text-to-PointCloud Localization with Cauchy-Mixture-Model Based Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanlong Xu",
      "Haoxuan Qu",
      "Jun Liu",
      "Wenxiao Zhang",
      "Xun Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Masked_Point-Entity_Contrast_for_Open-Vocabulary_3D_Scene_Understanding_CVPR_2025_paper.html": {
    "title": "Masked Point-Entity Contrast for Open-Vocabulary 3D Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Wang",
      "Baoxiong Jia",
      "Ziyu Zhu",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Decoupling_Training-Free_Guided_Diffusion_by_ADMM_CVPR_2025_paper.html": {
    "title": "Decoupling Training-Free Guided Diffusion by ADMM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youyuan Zhang",
      "Zehua Liu",
      "Zenan Li",
      "Zhaoyu Li",
      "James J. Clark",
      "Xujie Si"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Garrido-Munoz_On_the_Generalization_of_Handwritten_Text_Recognition_Models_CVPR_2025_paper.html": {
    "title": "On the Generalization of Handwritten Text Recognition Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carlos Garrido-Munoz",
      "Jorge Calvo-Zaragoza"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nguyen_SwiftEdit_Lightning_Fast_Text-Guided_Image_Editing_via_One-Step_Diffusion_CVPR_2025_paper.html": {
    "title": "SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trong-Tung Nguyen",
      "Quang Nguyen",
      "Khoi Nguyen",
      "Anh Tran",
      "Cuong Pham"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Learning_from_Synchronization_Self-Supervised_Uncalibrated_Multi-View_Person_Association_in_Challenging_CVPR_2025_paper.html": {
    "title": "Learning from Synchronization: Self-Supervised Uncalibrated Multi-View Person Association in Challenging Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keqi Chen",
      "Vinkle Srivastav",
      "Didier Mutter",
      "Nicolas Padoy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luu_RC-AutoCalib_An_End-to-End_Radar-Camera_Automatic_Calibration_Network_CVPR_2025_paper.html": {
    "title": "RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Van-Tin Luu",
      "Yon-Lin Cai",
      "Vu-Hoang Tran",
      "Wei-Chen Chiu",
      "Yi-Ting Chen",
      "Ching-Chun Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhuang_Argus_A_Compact_and_Versatile_Foundation_Model_for_Vision_CVPR_2025_paper.html": {
    "title": "Argus: A Compact and Versatile Foundation Model for Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiming Zhuang",
      "Chen Chen",
      "Zhizhong Li",
      "Sina Sajadmanesh",
      "Jingtao Li",
      "Jiabo Huang",
      "Vikash Sehwag",
      "Vivek Sharma",
      "Hirotaka Shinozaki",
      "Felan Carlo Garcia",
      "Yihao Zhan",
      "Naohiro Adachi",
      "Ryoji Eki",
      "Michael Spranger",
      "Peter Stone",
      "Lingjuan Lyu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_CLIP-driven_Coarse-to-fine_Semantic_Guidance_for_Fine-grained_Open-set_Semi-supervised_Learning_CVPR_2025_paper.html": {
    "title": "CLIP-driven Coarse-to-fine Semantic Guidance for Fine-grained Open-set Semi-supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaokun Li",
      "Yaping Huang",
      "Qingji Guan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_InsTaG_Learning_Personalized_3D_Talking_Head_from_Few-Second_Video_CVPR_2025_paper.html": {
    "title": "InsTaG: Learning Personalized 3D Talking Head from Few-Second Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahe Li",
      "Jiawei Zhang",
      "Xiao Bai",
      "Jin Zheng",
      "Jun Zhou",
      "Lin Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_Sampling_Innovation-Based_Adaptive_Compressive_Sensing_CVPR_2025_paper.html": {
    "title": "Sampling Innovation-Based Adaptive Compressive Sensing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhifu Tian",
      "Tao Hu",
      "Chaoyang Niu",
      "Di Wu",
      "Shu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_A_Simple_Data_Augmentation_for_Feature_Distribution_Skewed_Federated_Learning_CVPR_2025_paper.html": {
    "title": "A Simple Data Augmentation for Feature Distribution Skewed Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunlu Yan",
      "Huazhu Fu",
      "Yuexiang Li",
      "Jinheng Xie",
      "Jun Ma",
      "Guang Yang",
      "Lei Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hong_MotionBench_Benchmarking_and_Improving_Fine-grained_Video_Motion_Understanding_for_Vision_CVPR_2025_paper.html": {
    "title": "MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyi Hong",
      "Yean Cheng",
      "Zhuoyi Yang",
      "Weihan Wang",
      "Lefan Wang",
      "Xiaotao Gu",
      "Shiyu Huang",
      "Yuxiao Dong",
      "Jie Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Benchmarking_Large_Vision-Language_Models_via_Directed_Scene_Graph_for_Comprehensive_CVPR_2025_paper.html": {
    "title": "Benchmarking Large Vision-Language Models via Directed Scene Graph for Comprehensive Image Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Lu",
      "Wei Wu",
      "Kecheng Zheng",
      "Shuailei Ma",
      "Biao Gong",
      "Jiawei Liu",
      "Wei Zhai",
      "Yang Cao",
      "Yujun Shen",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pu_ART_Anonymous_Region_Transformer_for_Variable_Multi-Layer_Transparent_Image_Generation_CVPR_2025_paper.html": {
    "title": "ART: Anonymous Region Transformer for Variable Multi-Layer Transparent Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Pu",
      "Yiming Zhao",
      "Zhicong Tang",
      "Ruihong Yin",
      "Haoxing Ye",
      "Yuhui Yuan",
      "Dong Chen",
      "Jianmin Bao",
      "Sirui Zhang",
      "Yanbin Wang",
      "Lin Liang",
      "Lijuan Wang",
      "Ji Li",
      "Xiu Li",
      "Zhouhui Lian",
      "Gao Huang",
      "Baining Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Rotation-Equivariant_Self-Supervised_Method_in_Image_Denoising_CVPR_2025_paper.html": {
    "title": "Rotation-Equivariant Self-Supervised Method in Image Denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanze Liu",
      "Jiahong Fu",
      "Qi Xie",
      "Deyu Meng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points_CVPR_2025_paper.html": {
    "title": "ArcPro: Architectural Programs for Structured 3D Abstraction of Sparse Points",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qirui Huang",
      "Runze Zhang",
      "Kangjun Liu",
      "Minglun Gong",
      "Hao Zhang",
      "Hui Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ozturk_GLane3D_Detecting_Lanes_with_Graph_of_3D_Keypoints_CVPR_2025_paper.html": {
    "title": "GLane3D: Detecting Lanes with Graph of 3D Keypoints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Halil İbrahim Öztürk",
      "Muhammet Esat Kalfaoğlu",
      "Ozsel Kilinc"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Minimal_Interaction_Seperated_Tuning_A_New_Paradigm_for_Visual_Adaptation_CVPR_2025_paper.html": {
    "title": "Minimal Interaction Seperated Tuning: A New Paradigm for Visual Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ningyuan Tang",
      "Minghao Fu",
      "Jianxin Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "Hardware-Rasterized Ray-Based Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Rota Bulò",
      "Nemanja Bartolovic",
      "Lorenzo Porzi",
      "Peter Kontschieder"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tong_FlashSloth__Lightning_Multimodal_Large_Language_Models_via_Embedded_Visual_CVPR_2025_paper.html": {
    "title": "FlashSloth : Lightning Multimodal Large Language Models via Embedded Visual Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Tong",
      "Bokai Lai",
      "Yiyi Zhou",
      "Gen Luo",
      "Yunhang Shen",
      "Ke Li",
      "Xiaoshuai Sun",
      "Rongrong Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kashiani_FreqDebias_Towards_Generalizable_Deepfake_Detection_via_Consistency-Driven_Frequency_Debiasing_CVPR_2025_paper.html": {
    "title": "FreqDebias: Towards Generalizable Deepfake Detection via Consistency-Driven Frequency Debiasing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hossein Kashiani",
      "Niloufar Alipour Talemi",
      "Fatemeh Afghah"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Multi-subject_Open-set_Personalization_in_Video_Generation_CVPR_2025_paper.html": {
    "title": "Multi-subject Open-set Personalization in Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tsai-Shien Chen",
      "Aliaksandr Siarohin",
      "Willi Menapace",
      "Yuwei Fang",
      "Kwot Sin Lee",
      "Ivan Skorokhodov",
      "Kfir Aberman",
      "Jun-Yan Zhu",
      "Ming-Hsuan Yang",
      "Sergey Tulyakov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Wav2Sem_Plug-and-Play_Audio_Semantic_Decoupling_for_3D_Speech-Driven_Facial_Animation_CVPR_2025_paper.html": {
    "title": "Wav2Sem: Plug-and-Play Audio Semantic Decoupling for 3D Speech-Driven Facial Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Li",
      "Ju Dai",
      "Xin Zhao",
      "Feng Zhou",
      "Junjun Pan",
      "Lei Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Attraction_Diminishing_and_Distributing_for_Few-Shot_Class-Incremental_Learning_CVPR_2025_paper.html": {
    "title": "Attraction Diminishing and Distributing for Few-Shot Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li-Jun Zhao",
      "Zhen-Duo Chen",
      "Yongxin Wang",
      "Xin Luo",
      "Xin-Shun Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Matsuki_4DTAM_Non-Rigid_Tracking_and_Mapping_via_Dynamic_Surface_Gaussians_CVPR_2025_paper.html": {
    "title": "4DTAM: Non-Rigid Tracking and Mapping via Dynamic Surface Gaussians",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hidenobu Matsuki",
      "Gwangbin Bae",
      "Andrew J. Davison"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lv_T2SG_Traffic_Topology_Scene_Graph_for_Topology_Reasoning_in_Autonomous_CVPR_2025_paper.html": {
    "title": "T2SG: Traffic Topology Scene Graph for Topology Reasoning in Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changsheng Lv",
      "Mengshi Qi",
      "Liang Liu",
      "Huadong Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Unseen_Visual_Anomaly_Generation_CVPR_2025_paper.html": {
    "title": "Unseen Visual Anomaly Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Sun",
      "Yunkang Cao",
      "Hao Dong",
      "Olga Fink"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting_CVPR_2025_paper.html": {
    "title": "T2ICount: Enhancing Cross-modal Understanding for Zero-Shot Counting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Qian",
      "Zhongliang Guo",
      "Bowen Deng",
      "Chun Tong Lei",
      "Shuai Zhao",
      "Chun Pong Lau",
      "Xiaopeng Hong",
      "Michael P. Pound"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sushko_RealEdit_Reddit_Edits_As_a_Large-scale_Empirical_Dataset_for_Image_CVPR_2025_paper.html": {
    "title": "RealEdit: Reddit Edits As a Large-scale Empirical Dataset for Image Transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Sushko",
      "Ayana Bharadwaj",
      "Zhi Yang Lim",
      "Vasily Ilin",
      "Ben Caffee",
      "Dongping Chen",
      "Mohammadreza Salehi",
      "Cheng-Yu Hsieh",
      "Ranjay Krishna"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in_CVPR_2025_paper.html": {
    "title": "VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyang Wang",
      "Fangfu Liu",
      "Jiawei Chi",
      "Yueqi Duan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_3D-HGS_3D_Half-Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "3D-HGS: 3D Half-Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haolin Li",
      "Jinyang Liu",
      "Mario Sznaier",
      "Octavia Camps"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_FG2_Fine-Grained_Cross-View_Localization_by_Fine-Grained_Feature_Matching_CVPR_2025_paper.html": {
    "title": "FG^2: Fine-Grained Cross-View Localization by Fine-Grained Feature Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zimin Xia",
      "Alexandre Alahi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance_CVPR_2025_paper.html": {
    "title": "ReNeg: Learning Negative Embedding with Reward Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaomin Li",
      "Yixuan Liu",
      "Takashi Isobe",
      "Xu Jia",
      "Qinpeng Cui",
      "Dong Zhou",
      "Dong Li",
      "You He",
      "Huchuan Lu",
      "Zhongdao Wang",
      "Emad Barsoum"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Scale_Efficient_Training_for_Large_Datasets_CVPR_2025_paper.html": {
    "title": "Scale Efficient Training for Large Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qing Zhou",
      "Junyu Gao",
      "Qi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Distilled_Prompt_Learning_for_Incomplete_Multimodal_Survival_Prediction_CVPR_2025_paper.html": {
    "title": "Distilled Prompt Learning for Incomplete Multimodal Survival Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingxue Xu",
      "Fengtao Zhou",
      "Chenyu Zhao",
      "Yihui Wang",
      "Can Yang",
      "Hao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/An_Decoder_Gradient_Shield_Provable_and_High-Fidelity_Prevention_of_Gradient-Based_Box-Free_CVPR_2025_paper.html": {
    "title": "Decoder Gradient Shield: Provable and High-Fidelity Prevention of Gradient-Based Box-Free Watermark Removal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan An",
      "Guang Hua",
      "Zhengru Fang",
      "Guowen Xu",
      "Susanto Rahardja",
      "Yuguang Fang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_MotionPro_A_Precise_Motion_Controller_for_Image-to-Video_Generation_CVPR_2025_paper.html": {
    "title": "MotionPro: A Precise Motion Controller for Image-to-Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongwei Zhang",
      "Fuchen Long",
      "Zhaofan Qiu",
      "Yingwei Pan",
      "Wu Liu",
      "Ting Yao",
      "Tao Mei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Goku_Flow_Based_Video_Generative_Foundation_Models_CVPR_2025_paper.html": {
    "title": "Goku: Flow Based Video Generative Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shoufa Chen",
      "Chongjian Ge",
      "Yuqi Zhang",
      "Yida Zhang",
      "Fengda Zhu",
      "Hao Yang",
      "Hongxiang Hao",
      "Hui Wu",
      "Zhichao Lai",
      "Yifei Hu",
      "Ting-Che Lin",
      "Shilong Zhang",
      "Fu Li",
      "Chuan Li",
      "Xing Wang",
      "Yanghua Peng",
      "Peize Sun",
      "Ping Luo",
      "Yi Jiang",
      "Zehuan Yuan",
      "Bingyue Peng",
      "Xiaobing Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning_CVPR_2025_paper.html": {
    "title": "Learning Conditional Space-Time Prompt Distributions for Video Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohan Zou",
      "Wenchao Ma",
      "Shu Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Convex_Combination_Star_Shape_Prior_for_Data-driven_Image_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Convex Combination Star Shape Prior for Data-driven Image Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Zhao",
      "Jun Xie",
      "Shengzhe Chen",
      "Jun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "Hyperbolic Safety-Aware Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobia Poppi",
      "Tejaswi Kasarla",
      "Pascal Mettes",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels_CVPR_2025_paper.html": {
    "title": "WISH: Weakly Supervised Instance Segmentation using Heterogeneous Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeokjun Kweon",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_SinGS_Animatable_Single-Image_Human_Gaussian_Splats_with_Kinematic_Priors_CVPR_2025_paper.html": {
    "title": "SinGS: Animatable Single-Image Human Gaussian Splats with Kinematic Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufan Wu",
      "Xuanhong Chen",
      "Wen Li",
      "Shunran Jia",
      "Hualiang Wei",
      "Kairui Feng",
      "Jialiang Chen",
      "Yuhan Li",
      "Ang He",
      "Weimin Zhang",
      "Bingbing Ni",
      "Wenjun Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peng_Parameter-efficient_Fine-tuning_in_Hyperspherical_Space_for_Open-vocabulary_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Parameter-efficient Fine-tuning in Hyperspherical Space for Open-vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zelin Peng",
      "Zhengqin Xu",
      "Zhilin Zeng",
      "Yu Huang",
      "Yaoming Wang",
      "Wei Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors_CVPR_2025_paper.html": {
    "title": "Relative Pose Estimation through Affine Corrections of Monocular Depth Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Yu",
      "Shaohui Liu",
      "Rémi Pautrat",
      "Marc Pollefeys",
      "Viktor Larsson"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Zero-1-to-A_Zero-Shot_One_Image_to_Animatable_Head_Avatars_Using_Video_CVPR_2025_paper.html": {
    "title": "Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenglin Zhou",
      "Fan Ma",
      "Hehe Fan",
      "Tat-Seng Chua"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nguyen_Occlusion-aware_Text-Image-Point_Cloud_Pretraining_for_Open-World_3D_Object_Recognition_CVPR_2025_paper.html": {
    "title": "Occlusion-aware Text-Image-Point Cloud Pretraining for Open-World 3D Object Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khanh Nguyen",
      "Ghulam Mubashar Hassan",
      "Ajmal Mian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xing_Conical_Visual_Concentration_for_Efficient_Large_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "Conical Visual Concentration for Efficient Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long Xing",
      "Qidong Huang",
      "Xiaoyi Dong",
      "Jiajie Lu",
      "Pan Zhang",
      "Yuhang Zang",
      "Yuhang Cao",
      "Conghui He",
      "Jiaqi Wang",
      "Feng Wu",
      "Dahua Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion_CVPR_2025_paper.html": {
    "title": "Good, Cheap, and Fast: Overfitted Image Compression with Wasserstein Distortion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jona Ballé",
      "Luca Versari",
      "Emilien Dupont",
      "Hyunjik Kim",
      "Matthias Bauer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Period-LLM_Extending_the_Periodic_Capability_of_Multimodal_Large_Language_Model_CVPR_2025_paper.html": {
    "title": "Period-LLM: Extending the Periodic Capability of Multimodal Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuting Zhang",
      "Hao Lu",
      "Qingyong Hu",
      "Yin Wang",
      "Kaishen Yuan",
      "Xin Liu",
      "Kaishun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_V2X-R_Cooperative_LiDAR-4D_Radar_Fusion_with_Denoising_Diffusion_for_3D_CVPR_2025_paper.html": {
    "title": "V2X-R: Cooperative LiDAR-4D Radar Fusion with Denoising Diffusion for 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xun Huang",
      "Jinlong Wang",
      "Qiming Xia",
      "Siheng Chen",
      "Bisheng Yang",
      "Xin Li",
      "Cheng Wang",
      "Chenglu Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dai_Multi-Modal_Synergistic_Implicit_Image_Enhancement_for_Efficient_Optical_Flow_Estimation_CVPR_2025_paper.html": {
    "title": "Multi-Modal Synergistic Implicit Image Enhancement for Efficient Optical Flow Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weichen Dai",
      "Hexing Wu",
      "Xiaoyang Weng",
      "Yuxin Zheng",
      "Yuhang Ming",
      "Wanzeng Kong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_TAROT_Towards_Essentially_Domain-Invariant_Robustness_with_Theoretical_Justification_CVPR_2025_paper.html": {
    "title": "TAROT: Towards Essentially Domain-Invariant Robustness with Theoretical Justification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyoon Yang",
      "Jihu Lee",
      "Yongdai Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pierard_Foundations_of_the_Theory_of_Performance-Based_Ranking_CVPR_2025_paper.html": {
    "title": "Foundations of the Theory of Performance-Based Ranking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sébastien Piérard",
      "Anaïs Halin",
      "Anthony Cioppa",
      "Adrien Deliege",
      "Marc Van Droogenbroeck"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Unveiling_the_Mist_over_3D_Vision-Language_Understanding_Object-centric_Evaluation_with_CVPR_2025_paper.html": {
    "title": "Unveiling the Mist over 3D Vision-Language Understanding: Object-centric Evaluation with Chain-of-Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangyong Huang",
      "Baoxiong Jia",
      "Yan Wang",
      "Ziyu Zhu",
      "Xiongkun Linghu",
      "Qing Li",
      "Song-Chun Zhu",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Generating_Multimodal_Driving_Scenes_via_Next-Scene_Prediction_CVPR_2025_paper.html": {
    "title": "Generating Multimodal Driving Scenes via Next-Scene Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanhao Wu",
      "Haoyang Zhang",
      "Tianwei Lin",
      "Lichao Huang",
      "Shujie Luo",
      "Rui Wu",
      "Congpei Qiu",
      "Wei Ke",
      "Tong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/On_BIGS_Bimanual_Category-agnostic_Interaction_Reconstruction_from_Monocular_Videos_via_3D_CVPR_2025_paper.html": {
    "title": "BIGS: Bimanual Category-agnostic Interaction Reconstruction from Monocular Videos via 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongwan On",
      "Kyeonghwan Gwak",
      "Gunyoung Kang",
      "Junuk Cha",
      "Soohyun Hwang",
      "Hyein Hwang",
      "Seungryul Baek"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chae_APT_Adaptive_Personalized_Training_for_Diffusion_Models_with_Limited_Data_CVPR_2025_paper.html": {
    "title": "APT: Adaptive Personalized Training for Diffusion Models with Limited Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JungWoo Chae",
      "Jiyoon Kim",
      "JaeWoong Choi",
      "Kyungyul Kim",
      "Sangheum Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation_CVPR_2025_paper.html": {
    "title": "Symmetry Strikes Back: From Single-Image Symmetry Detection to 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Li",
      "Zixuan Huang",
      "Anh Thai",
      "James M. Rehg"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Frequency-Biased_Synergistic_Design_for_Image_Compression_and_Compensation_CVPR_2025_paper.html": {
    "title": "Frequency-Biased Synergistic Design for Image Compression and Compensation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Liu",
      "Qi Zheng",
      "Zihao Liu",
      "Yilian Zhong",
      "Peiye Liu",
      "Tao Liu",
      "Shusong Xu",
      "Yanheng Lu",
      "Sicheng Li",
      "Dimin Niu",
      "Yibo Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_PosterMaker_Towards_High-Quality_Product_Poster_Generation_with_Accurate_Text_Rendering_CVPR_2025_paper.html": {
    "title": "PosterMaker: Towards High-Quality Product Poster Generation with Accurate Text Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Gao",
      "Zihang Lin",
      "Chuanbin Liu",
      "Min Zhou",
      "Tiezheng Ge",
      "Bo Zheng",
      "Hongtao Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Sparse_Voxels_Rasterization_Real-time_High-fidelity_Radiance_Field_Rendering_CVPR_2025_paper.html": {
    "title": "Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Sun",
      "Jaesung Choe",
      "Charles Loop",
      "Wei-Chiu Ma",
      "Yu-Chiang Frank Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An_CVPR_2025_paper.html": {
    "title": "Rethinking Personalized Aesthetics Assessment: Employing Physique Aesthetics Assessment as An Exemplification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haobin Zhong",
      "Shuai He",
      "Anlong Ming",
      "Huadong Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_You_See_it_You_Got_it_Learning_3D_Creation_on_CVPR_2025_paper.html": {
    "title": "You See it, You Got it: Learning 3D Creation on Pose-Free Videos at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baorui Ma",
      "Huachen Gao",
      "Haoge Deng",
      "Zhengxiong Luo",
      "Tiejun Huang",
      "Lulu Tang",
      "Xinlong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zeng_MambaIC_State_Space_Models_for_High-Performance_Learned_Image_Compression_CVPR_2025_paper.html": {
    "title": "MambaIC: State Space Models for High-Performance Learned Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanhu Zeng",
      "Hao Tang",
      "Yihua Shao",
      "Siyu Chen",
      "Ling Shao",
      "Yan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_SCAP_Transductive_Test-Time_Adaptation_via_Supportive_Clique-based_Attribute_Prompting_CVPR_2025_paper.html": {
    "title": "SCAP: Transductive Test-Time Adaptation via Supportive Clique-based Attribute Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyu Zhang",
      "Kunlun Xu",
      "Zichen Liu",
      "Yuxin Peng",
      "Jiahuan Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene_CVPR_2025_paper.html": {
    "title": "Instant Gaussian Stream: Fast and Generalizable Streaming of Dynamic Scene Reconstruction via Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinbo Yan",
      "Rui Peng",
      "Zhiyan Wang",
      "Luyang Tang",
      "Jiayu Yang",
      "Jie Liang",
      "Jiahao Wu",
      "Ronggang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Locality-Aware_Zero-Shot_Human-Object_Interaction_Detection_CVPR_2025_paper.html": {
    "title": "Locality-Aware Zero-Shot Human-Object Interaction Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanghyun Kim",
      "Deunsol Jung",
      "Minsu Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_PEACE_Empowering_Geologic_Map_Holistic_Understanding_with_MLLMs_CVPR_2025_paper.html": {
    "title": "PEACE: Empowering Geologic Map Holistic Understanding with MLLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangyu Huang",
      "Tianyi Gao",
      "Haoran Xu",
      "Qihao Zhao",
      "Yang Song",
      "Zhipeng Gui",
      "Tengchao Lv",
      "Hao Chen",
      "Lei Cui",
      "Scarlett Li",
      "Furu Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better_CVPR_2025_paper.html": {
    "title": "Tracktention: Leveraging Point Tracking to Attend Videos Faster and Better",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihang Lai",
      "Andrea Vedaldi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_ConceptGuard_Continual_Personalized_Text-to-Image_Generation_with_Forgetting_and_Confusion_Mitigation_CVPR_2025_paper.html": {
    "title": "ConceptGuard: Continual Personalized Text-to-Image Generation with Forgetting and Confusion Mitigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirun Guo",
      "Tao Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qi_Two_by_Two_Learning_Multi-Task_Pairwise_Objects_Assembly_for_Generalizable_CVPR_2025_paper.html": {
    "title": "Two by Two: Learning Multi-Task Pairwise Objects Assembly for Generalizable Robot Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Qi",
      "Yuanchen Ju",
      "Tianming Wei",
      "Chi Chu",
      "Lawson L.S. Wong",
      "Huazhe Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_SGFormer_Satellite-Ground_Fusion_for_3D_Semantic_Scene_Completion_CVPR_2025_paper.html": {
    "title": "SGFormer: Satellite-Ground Fusion for 3D Semantic Scene Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiyue Guo",
      "Jiarui Hu",
      "Junjie Hu",
      "Hujun Bao",
      "Guofeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sinha_MARVEL-40M_Multi-Level_Visual_Elaboration_for_High-Fidelity_Text-to-3D_Content_Creation_CVPR_2025_paper.html": {
    "title": "MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D Content Creation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sankalp Sinha",
      "Mohammad Sadil Khan",
      "Muhammad Usama",
      "Shino Sam",
      "Didier Stricker",
      "Sk Aziz Ali",
      "Muhammad Zeshan Afzal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Random_Conditioning_for_Diffusion_Model_Compression_with_Distillation_CVPR_2025_paper.html": {
    "title": "Random Conditioning for Diffusion Model Compression with Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dohyun Kim",
      "Sehwan Park",
      "Geonhee Han",
      "Seung Wook Kim",
      "Paul Hongsuck Seo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Hierarchical_Gaussian_Mixture_Model_Splatting_for_Efficient_and_Part_Controllable_CVPR_2025_paper.html": {
    "title": "Hierarchical Gaussian Mixture Model Splatting for Efficient and Part Controllable 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qitong Yang",
      "Mingtao Feng",
      "Zijie Wu",
      "Weisheng Dong",
      "Fangfang Wu",
      "Yaonan Wang",
      "Ajmal Mian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shugaev_ERUPT_Efficient_Rendering_with_Unposed_Patch_Transformer_CVPR_2025_paper.html": {
    "title": "ERUPT: Efficient Rendering with Unposed Patch Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxim V. Shugaev",
      "Vincent Chen",
      "Maxim Karrenbach",
      "Kyle Ashley",
      "Bridget Kennedy",
      "Naresh P. Cuntoor"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Rethinking_End-to-End_2D_to_3D_Scene_Segmentation_in_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "Rethinking End-to-End 2D to 3D Scene Segmentation in Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runsong Zhu",
      "Shi Qiu",
      "Zhengzhe Liu",
      "Ka-Hei Hui",
      "Qianyi Wu",
      "Pheng-Ann Heng",
      "Chi-Wing Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Quad-Pixel_Image_Defocus_Deblurring_A_New_Benchmark_and_Model_CVPR_2025_paper.html": {
    "title": "Quad-Pixel Image Defocus Deblurring: A New Benchmark and Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Chen",
      "Yin Xie",
      "Xiaoxiu Peng",
      "Lihu Sun",
      "Wenkai Su",
      "Xiaodong Yang",
      "Chengming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nacson_DocVLM_Make_Your_VLM_an_Efficient_Reader_CVPR_2025_paper.html": {
    "title": "DocVLM: Make Your VLM an Efficient Reader",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mor Shpigel Nacson",
      "Aviad Aberdam",
      "Roy Ganz",
      "Elad Ben Avraham",
      "Alona Golts",
      "Yair Kittenplon",
      "Shai Mazor",
      "Ron Litman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Revisiting_Source-Free_Domain_Adaptation_Insights_into_Representativeness_Generalization_and_Variety_CVPR_2025_paper.html": {
    "title": "Revisiting Source-Free Domain Adaptation: Insights into Representativeness, Generalization, and Variety",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ronghang Zhu",
      "Mengxuan Hu",
      "Weiming Zhuang",
      "Lingjuan Lyu",
      "Xiang Yu",
      "Sheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Adaptive_Unimodal_Regulation_for_Balanced_Multimodal_Information_Acquisition_CVPR_2025_paper.html": {
    "title": "Adaptive Unimodal Regulation for Balanced Multimodal Information Acquisition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengxiang Huang",
      "Yake Wei",
      "Zequn Yang",
      "Di Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Heterogeneous_Skeleton-Based_Action_Representation_Learning_CVPR_2025_paper.html": {
    "title": "Heterogeneous Skeleton-Based Action Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongsong Wang",
      "Xiaoyan Ma",
      "Jidong Kuang",
      "Jie Gui"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_FLARE_Feed-forward_Geometry_Appearance_and_Camera_Estimation_from_Uncalibrated_Sparse_CVPR_2025_paper.html": {
    "title": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangzhan Zhang",
      "Jianyuan Wang",
      "Yinghao Xu",
      "Nan Xue",
      "Christian Rupprecht",
      "Xiaowei Zhou",
      "Yujun Shen",
      "Gordon Wetzstein"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management_CVPR_2025_paper.html": {
    "title": "Improving Gaussian Splatting with Localized Points Management",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haosen Yang",
      "Chenhao Zhang",
      "Wenqing Wang",
      "Marco Volino",
      "Adrian Hilton",
      "Li Zhang",
      "Xiatian Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_GEAL_Generalizable_3D_Affordance_Learning_with_Cross-Modal_Consistency_CVPR_2025_paper.html": {
    "title": "GEAL: Generalizable 3D Affordance Learning with Cross-Modal Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyue Lu",
      "Lingdong Kong",
      "Tianxin Huang",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Dynamic_Derivation_and_Elimination_Audio_Visual_Segmentation_with_Enhanced_Audio_CVPR_2025_paper.html": {
    "title": "Dynamic Derivation and Elimination: Audio Visual Segmentation with Enhanced Audio Semantics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Liu",
      "Liying Yang",
      "Peike Li",
      "Dadong Wang",
      "Lincheng Li",
      "Xin Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dal_Cin_AnyMap_Learning_a_General_Camera_Model_for_Structure-from-Motion_with_Unknown_CVPR_2025_paper.html": {
    "title": "AnyMap: Learning a General Camera Model for Structure-from-Motion with Unknown Distortion in Dynamic Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Porfiri Dal Cin",
      "Georgi Dikov",
      "Jihong Ju",
      "Mohsen Ghafoorian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion_CVPR_2025_paper.html": {
    "title": "ESC: Erasing Space Concept for Knowledge Deletion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tae-Young Lee",
      "Sundong Park",
      "Minwoo Jeon",
      "Hyoseok Hwang",
      "Gyeong-Moon Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Language_Guided_Concept_Bottleneck_Models_for_Interpretable_Continual_Learning_CVPR_2025_paper.html": {
    "title": "Language Guided Concept Bottleneck Models for Interpretable Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Yu",
      "Haoyu Han",
      "Zhe Tao",
      "Hantao Yao",
      "Changsheng Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_One-Way_Ticket_Time-Independent_Unified_Encoder_for_Distilling_Text-to-Image_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "One-Way Ticket: Time-Independent Unified Encoder for Distilling Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Senmao Li",
      "Lei Wang",
      "Kai Wang",
      "Tao Liu",
      "Jiehang Xie",
      "Joost van de Weijer",
      "Fahad Shahbaz Khan",
      "Shiqi Yang",
      "Yaxing Wang",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Su_Domain_Adaptive_Diabetic_Retinopathy_Grading_with_Model_Absence_and_Flowing_CVPR_2025_paper.html": {
    "title": "Domain Adaptive Diabetic Retinopathy Grading with Model Absence and Flowing Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxin Su",
      "Song Tang",
      "Xiaofeng Liu",
      "Xiaojing Yi",
      "Mao Ye",
      "Chunxiao Zu",
      "Jiahao Li",
      "Xiatian Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Temporal_Separation_with_Entropy_Regularization_for_Knowledge_Distillation_in_Spiking_CVPR_2025_paper.html": {
    "title": "Temporal Separation with Entropy Regularization for Knowledge Distillation in Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kairong Yu",
      "Chengting Yu",
      "Tianqing Zhang",
      "Xiaochen Zhao",
      "Shu Yang",
      "Hongwei Wang",
      "Qiang Zhang",
      "Qi Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in_CVPR_2025_paper.html": {
    "title": "LoRASculpt: Sculpting LoRA for Harmonizing General and Specialized Knowledge in Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Liang",
      "Wenke Huang",
      "Guancheng Wan",
      "Qu Yang",
      "Mang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation_CVPR_2025_paper.html": {
    "title": "SEAL: Semantic Attention Learning for Long Video Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lan Wang",
      "Yujia Chen",
      "Du Tran",
      "Vishnu Naresh Boddeti",
      "Wen-Sheng Chu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_Re-HOLD_Video_Hand_Object_Interaction_Reenactment_via_adaptive_Layout-instructed_Diffusion_CVPR_2025_paper.html": {
    "title": "Re-HOLD: Video Hand Object Interaction Reenactment via adaptive Layout-instructed Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingying Fan",
      "Quanwei Yang",
      "Kaisiyuan Wang",
      "Hang Zhou",
      "Yingying Li",
      "Haocheng Feng",
      "Errui Ding",
      "Yu Wu",
      "Jingdong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization_CVPR_2025_paper.html": {
    "title": "Theoretical Insights in Model Inversion Robustness and Conditional Entropy Maximization for Collaborative Inference Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Song Xia",
      "Yi Yu",
      "Wenhan Yang",
      "Meiwen Ding",
      "Zhuo Chen",
      "Ling-Yu Duan",
      "Alex C. Kot",
      "Xudong Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bhunia_Odd-One-Out_Anomaly_Detection_by_Comparing_with_Neighbors_CVPR_2025_paper.html": {
    "title": "Odd-One-Out: Anomaly Detection by Comparing with Neighbors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ankan Bhunia",
      "Changjian Li",
      "Hakan Bilen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SCFlow2_Plug-and-Play_Object_Pose_Refiner_with_Shape-Constraint_Scene_Flow_CVPR_2025_paper.html": {
    "title": "SCFlow2: Plug-and-Play Object Pose Refiner with Shape-Constraint Scene Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyuan Wang",
      "Rui Song",
      "Jiaojiao Li",
      "Kerui Cheng",
      "David Ferstl",
      "Yinlin Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_D3CTTA_Domain-Dependent_Decorrelation_for_Continual_Test-Time_Adaption_of_3D_LiDAR_CVPR_2025_paper.html": {
    "title": "D^3CTTA: Domain-Dependent Decorrelation for Continual Test-Time Adaption of 3D LiDAR Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jichun Zhao",
      "Haiyong Jiang",
      "Haoxuan Song",
      "Jun Xiao",
      "Dong Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality_CVPR_2025_paper.html": {
    "title": "Flowing from Words to Pixels: A Noise-Free Framework for Cross-Modality Evolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihao Liu",
      "Xi Yin",
      "Alan Yuille",
      "Andrew Brown",
      "Mannat Singh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bandyopadhyay_FlipSketch_Flipping_Static_Drawings_to_Text-Guided_Sketch_Animations_CVPR_2025_paper.html": {
    "title": "FlipSketch: Flipping Static Drawings to Text-Guided Sketch Animations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hmrishav Bandyopadhyay",
      "Yi-Zhe Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kulkarni_Interpretable_Generative_Models_through_Post-hoc_Concept_Bottlenecks_CVPR_2025_paper.html": {
    "title": "Interpretable Generative Models through Post-hoc Concept Bottlenecks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Kulkarni",
      "Ge Yan",
      "Chung-En Sun",
      "Tuomas Oikarinen",
      "Tsui-Wei Weng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Vinker_SketchAgent_Language-Driven_Sequential_Sketch_Generation_CVPR_2025_paper.html": {
    "title": "SketchAgent: Language-Driven Sequential Sketch Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yael Vinker",
      "Tamar Rott Shaham",
      "Kristine Zheng",
      "Alex Zhao",
      "Judith E Fan",
      "Antonio Torralba"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_DRAWER_Digital_Reconstruction_and_Articulation_With_Environment_Realism_CVPR_2025_paper.html": {
    "title": "DRAWER: Digital Reconstruction and Articulation With Environment Realism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongchi Xia",
      "Entong Su",
      "Marius Memmel",
      "Arhan Jain",
      "Raymond Yu",
      "Numfor Mbiziwo-Tiapo",
      "Ali Farhadi",
      "Abhishek Gupta",
      "Shenlong Wang",
      "Wei-Chiu Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_GoLF-NRT_Integrating_Global_Context_and_Local_Geometry_for_Few-Shot_View_CVPR_2025_paper.html": {
    "title": "GoLF-NRT: Integrating Global Context and Local Geometry for Few-Shot View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "You Wang",
      "Li Fang",
      "Hao Zhu",
      "Fei Hu",
      "Long Ye",
      "Zhan Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a_CVPR_2025_paper.html": {
    "title": "Deep Change Monitoring: A Hyperbolic Representative Learning Framework and a Dataset for Long-term Fine-grained Tree Change Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yante Li",
      "Hanwen Qi",
      "Haoyu Chen",
      "Xinlian Liang",
      "Guoying Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_A_Closer_Look_at_Time_Steps_is_Worthy_of_Triple_CVPR_2025_paper.html": {
    "title": "A Closer Look at Time Steps is Worthy of Triple Speed-Up for Diffusion Model Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Wang",
      "Mingjia Shi",
      "Yukun Zhou",
      "Zekai Li",
      "Zhihang Yuan",
      "Yuzhang Shang",
      "Xiaojiang Peng",
      "Hanwang Zhang",
      "Yang You"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xing_Empowering_LLMs_to_Understand_and_Generate_Complex_Vector_Graphics_CVPR_2025_paper.html": {
    "title": "Empowering LLMs to Understand and Generate Complex Vector Graphics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ximing Xing",
      "Juncheng Hu",
      "Guotao Liang",
      "Jing Zhang",
      "Dong Xu",
      "Qian Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhai_PanoGS_Gaussian-based_Panoptic_Segmentation_for_3D_Open_Vocabulary_Scene_Understanding_CVPR_2025_paper.html": {
    "title": "PanoGS: Gaussian-based Panoptic Segmentation for 3D Open Vocabulary Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongjia Zhai",
      "Hai Li",
      "Zhenzhe Li",
      "Xiaokun Pan",
      "Yijia He",
      "Guofeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Watermarking_One_for_All_A_Robust_Watermarking_Scheme_Against_Partial_CVPR_2025_paper.html": {
    "title": "Watermarking One for All: A Robust Watermarking Scheme Against Partial Image Theft",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaozhi Liu",
      "Silu Cao",
      "Zhenxing Qian",
      "Xinpeng Zhang",
      "Sheng Li",
      "Wanli Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hong_ITA-MDT_Image-Timestep-Adaptive_Masked_Diffusion_Transformer_Framework_for_Image-Based_Virtual_Try-On_CVPR_2025_paper.html": {
    "title": "ITA-MDT: Image-Timestep-Adaptive Masked Diffusion Transformer Framework for Image-Based Virtual Try-On",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ji Woo Hong",
      "Tri Ton",
      "Trung X. Pham",
      "Gwanhyeong Koo",
      "Sunjae Yoon",
      "Chang D. Yoo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kriz_MultiVENT_2.0_A_Massive_Multilingual_Benchmark_for_Event-Centric_Video_Retrieval_CVPR_2025_paper.html": {
    "title": "MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reno Kriz",
      "Kate Sanders",
      "David Etter",
      "Kenton Murray",
      "Cameron Carpenter",
      "Hannah Recknor",
      "Jimena Guallar-Blasco",
      "Alexander Martin",
      "Eugene Yang",
      "Benjamin Van Durme"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_VolFormer_Explore_More_Comprehensive_Cube_Interaction_for_Hyperspectral_Image_Restoration_CVPR_2025_paper.html": {
    "title": "VolFormer: Explore More Comprehensive Cube Interaction for Hyperspectral Image Restoration and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dabing Yu",
      "Zheng Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_Minding_Fuzzy_Regions_A_Data-driven_Alternating_Learning_Paradigm_for_Stable_CVPR_2025_paper.html": {
    "title": "Minding Fuzzy Regions: A Data-driven Alternating Learning Paradigm for Stable Lesion Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lexin Fang",
      "Yunyang Xu",
      "Xiang Ma",
      "Xuemei Li",
      "Caiming Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peng_BizGen_Advancing_Article-level_Visual_Text_Rendering_for_Infographics_Generation_CVPR_2025_paper.html": {
    "title": "BizGen: Advancing Article-level Visual Text Rendering for Infographics Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyang Peng",
      "Shishi Xiao",
      "Keming Wu",
      "Qisheng Liao",
      "Bohan Chen",
      "Kevin Lin",
      "Danqing Huang",
      "Ji Li",
      "Yuhui Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_MLVU_Benchmarking_Multi-task_Long_Video_Understanding_CVPR_2025_paper.html": {
    "title": "MLVU: Benchmarking Multi-task Long Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Zhou",
      "Yan Shu",
      "Bo Zhao",
      "Boya Wu",
      "Zhengyang Liang",
      "Shitao Xiao",
      "Minghao Qin",
      "Xi Yang",
      "Yongping Xiong",
      "Bo Zhang",
      "Tiejun Huang",
      "Zheng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Recovering_Dynamic_3D_Sketches_from_Videos_CVPR_2025_paper.html": {
    "title": "Recovering Dynamic 3D Sketches from Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaeah Lee",
      "Changwoon Choi",
      "Young Min Kim",
      "Jaesik Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_IM-Zero_Instance-level_Motion_Controllable_Video_Generation_in_a_Zero-shot_Manner_CVPR_2025_paper.html": {
    "title": "IM-Zero: Instance-level Motion Controllable Video Generation in a Zero-shot Manner",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyang Huang",
      "Yabo Chen",
      "Li Ding",
      "Xiaopeng Zhang",
      "Wenrui Dai",
      "Junni Zou",
      "Hongkai Xiong",
      "Qi Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tai_EigenGS_Representation_From_Eigenspace_to_Gaussian_Image_Space_CVPR_2025_paper.html": {
    "title": "EigenGS Representation: From Eigenspace to Gaussian Image Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lo-Wei Tai",
      "Ching-En Li",
      "Cheng-Lin Chen",
      "Chih-Jung Tsai",
      "Hwann-Tzong Chen",
      "Tyng-Luh Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Link-based_Contrastive_Learning_for_One-Shot_Unsupervised_Domain_Adaptation_CVPR_2025_paper.html": {
    "title": "Link-based Contrastive Learning for One-Shot Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Zhang",
      "Mingyue Bin",
      "Yuyang Zhang",
      "Zhongyuan Wang",
      "Zhen Han",
      "Chao Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees_CVPR_2025_paper.html": {
    "title": "SmartCLIP: Modular Vision-language Alignment with Identification Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaoan Xie",
      "Lingjing Lingjing",
      "Yujia Zheng",
      "Yu Yao",
      "Zeyu Tang",
      "Eric P. Xing",
      "Guangyi Chen",
      "Kun Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jin_UniMamba_Unified_Spatial-Channel_Representation_Learning_with_Group-Efficient_Mamba_for_LiDAR-based_CVPR_2025_paper.html": {
    "title": "UniMamba: Unified Spatial-Channel Representation Learning with Group-Efficient Mamba for LiDAR-based 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Jin",
      "Haisheng Su",
      "Kai Liu",
      "Cong Ma",
      "Wei Wu",
      "Fei HUI",
      "Junchi Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_MaSS13K_A_Matting-level_Semantic_Segmentation_Benchmark_CVPR_2025_paper.html": {
    "title": "MaSS13K: A Matting-level Semantic Segmentation Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxi Xie",
      "Minghan Li",
      "Hui Zeng",
      "Jun Luo",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Rethinking_the_Adversarial_Robustness_of_Multi-Exit_Neural_Networks_in_an_CVPR_2025_paper.html": {
    "title": "Rethinking the Adversarial Robustness of Multi-Exit Neural Networks in an Attack-Defense Game",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keyizhi Xu",
      "Chi Zhang",
      "Zhan Chen",
      "Zhongyuan Wang",
      "Chunxia Xiao",
      "Chao Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Enhancing_Testing-Time_Robustness_for_Trusted_Multi-View_Classification_in_the_Wild_CVPR_2025_paper.html": {
    "title": "Enhancing Testing-Time Robustness for Trusted Multi-View Classification in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Liu",
      "Yufei Chen",
      "Xiaodong Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Q-DiT_Accurate_Post-Training_Quantization_for_Diffusion_Transformers_CVPR_2025_paper.html": {
    "title": "Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Chen",
      "Yuan Meng",
      "Chen Tang",
      "Xinzhu Ma",
      "Jingyan Jiang",
      "Xin Wang",
      "Zhi Wang",
      "Wenwu Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_ROD-MLLM_Towards_More_Reliable_Object_Detection_in_Multimodal_Large_Language_CVPR_2025_paper.html": {
    "title": "ROD-MLLM: Towards More Reliable Object Detection in Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heng Yin",
      "Yuqiang Ren",
      "Ke Yan",
      "Shouhong Ding",
      "Yongtao Hao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_RoboGround_Robotic_Manipulation_with_Grounded_Vision-Language_Priors_CVPR_2025_paper.html": {
    "title": "RoboGround: Robotic Manipulation with Grounded Vision-Language Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haifeng Huang",
      "Xinyi Chen",
      "Yilun Chen",
      "Hao Li",
      "Xiaoshen Han",
      "Zehan Wang",
      "Tai Wang",
      "Jiangmiao Pang",
      "Zhou Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_VideoGuide_Improving_Video_Diffusion_Models_without_Training_Through_a_Teachers_CVPR_2025_paper.html": {
    "title": "VideoGuide: Improving Video Diffusion Models without Training Through a Teacher's Guide",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dohun Lee",
      "Bryan Sangwoo Kim",
      "Geon Yeong Park",
      "Jong Chul Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Improving_Transferable_Targeted_Attacks_with_Feature_Tuning_Mixup_CVPR_2025_paper.html": {
    "title": "Improving Transferable Targeted Attacks with Feature Tuning Mixup",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaisheng Liang",
      "Xuelong Dai",
      "Yanjie Li",
      "Dong Wang",
      "Bin Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_OmniStereo_Real-time_Omnidireactional_Depth_Estimation_with_Multiview_Fisheye_Cameras_CVPR_2025_paper.html": {
    "title": "OmniStereo: Real-time Omnidireactional Depth Estimation with Multiview Fisheye Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxi Deng",
      "Yushen Wang",
      "Haitao Meng",
      "Zuoxun Hou",
      "Yi Chang",
      "Gang Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild_CVPR_2025_paper.html": {
    "title": "DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiadong Tang",
      "Yu Gao",
      "Dianyi Yang",
      "Liqi Yan",
      "Yufeng Yue",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Duan_SDGOCC_Semantic_and_Depth-Guided_Birds-Eye_View_Transformation_for_3D_Multimodal_CVPR_2025_paper.html": {
    "title": "SDGOCC: Semantic and Depth-Guided Bird's-Eye View Transformation for 3D Multimodal Occupancy Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ZaiPeng Duan",
      "ChenXu Dang",
      "Xuzhong Hu",
      "Pei An",
      "Junfeng Ding",
      "Jie Zhan",
      "YunBiao Xu",
      "Jie Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_DrivingSphere_Building_a_High-fidelity_4D_World_for_Closed-loop_Simulation_CVPR_2025_paper.html": {
    "title": "DrivingSphere: Building a High-fidelity 4D World for Closed-loop Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Yan",
      "Dongming Wu",
      "Wencheng Han",
      "Junpeng Jiang",
      "Xia Zhou",
      "Kun Zhan",
      "Cheng-zhong Xu",
      "Jianbing Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_nnWNet_Rethinking_the_Use_of_Transformers_in_Biomedical_Image_Segmentation_CVPR_2025_paper.html": {
    "title": "nnWNet: Rethinking the Use of Transformers in Biomedical Image Segmentation and Calling for a Unified Evaluation Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanfeng Zhou",
      "Lingrui Li",
      "Le Lu",
      "Minfeng Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Efficient_Video_Face_Enhancement_with_Enhanced_Spatial-Temporal_Consistency_CVPR_2025_paper.html": {
    "title": "Efficient Video Face Enhancement with Enhanced Spatial-Temporal Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutong Wang",
      "Jiajie Teng",
      "Jiajiong Cao",
      "Yuming Li",
      "Chenguang Ma",
      "Hongteng Xu",
      "Dixin Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Saravanan_VELOCITI_Benchmarking_Video-Language_Compositional_Reasoning_with_Strict_Entailment_CVPR_2025_paper.html": {
    "title": "VELOCITI: Benchmarking Video-Language Compositional Reasoning with Strict Entailment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Darshana Saravanan",
      "Varun Gupta",
      "Darshan Singh",
      "Zeeshan Khan",
      "Vineet Gandhi",
      "Makarand Tapaswi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Seeing_is_Not_Believing_Adversarial_Natural_Object_Optimization_for_Hard-Label_CVPR_2025_paper.html": {
    "title": "Seeing is Not Believing: Adversarial Natural Object Optimization for Hard-Label 3D Scene Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daizong Liu",
      "Wei Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Song_IDProtector_An_Adversarial_Noise_Encoder_to_Protect_Against_ID-Preserving_Image_CVPR_2025_paper.html": {
    "title": "IDProtector: An Adversarial Noise Encoder to Protect Against ID-Preserving Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiren Song",
      "Pei Yang",
      "Hai Ci",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation_CVPR_2025_paper.html": {
    "title": "HuPerFlow: A Comprehensive Benchmark for Human vs. Machine Motion Estimation Comparison",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yung-Hao Yang",
      "Zitang Sun",
      "Taiki Fukiage",
      "Shin'ya Nishida"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Spartalis_LoTUS_Large-Scale_Machine_Unlearning_with_a_Taste_of_Uncertainty_CVPR_2025_paper.html": {
    "title": "LoTUS: Large-Scale Machine Unlearning with a Taste of Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christoforos N. Spartalis",
      "Theodoros Semertzidis",
      "Efstratios Gavves",
      "Petros Daras"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SleeperMark_Towards_Robust_Watermark_against_Fine-Tuning_Text-to-image_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "SleeperMark: Towards Robust Watermark against Fine-Tuning Text-to-image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zilan Wang",
      "Junfeng Guo",
      "Jiacheng Zhu",
      "Yiming Li",
      "Heng Huang",
      "Muhao Chen",
      "Zhengzhong Tu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning_CVPR_2025_paper.html": {
    "title": "Lessons and Insights from a Unifying Study of Parameter-Efficient Fine-Tuning (PEFT) in Visual Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheda Mai",
      "Ping Zhang",
      "Cheng-Hao Tu",
      "Hong-You Chen",
      "Quang-Huy Nguyen",
      "Li Zhang",
      "Wei-Lun Chao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image_CVPR_2025_paper.html": {
    "title": "Pippo: High-Resolution Multi-View Humans from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yash Kant",
      "Ethan Weber",
      "Jin Kyu Kim",
      "Rawal Khirodkar",
      "Su Zhaoen",
      "Julieta Martinez",
      "Igor Gilitschenski",
      "Shunsuke Saito",
      "Timur Bagautdinov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_H2ST_Hierarchical_Two-Sample_Tests_for_Continual_Out-of-Distribution_Detection_CVPR_2025_paper.html": {
    "title": "H2ST: Hierarchical Two-Sample Tests for Continual Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Liu",
      "Wenjie Zhao",
      "Yunhui Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gu_MetaWriter_Personalized_Handwritten_Text_Recognition_Using_Meta-Learned_Prompt_Tuning_CVPR_2025_paper.html": {
    "title": "MetaWriter: Personalized Handwritten Text Recognition Using Meta-Learned Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Gu",
      "Li Gu",
      "Chingyee Yee Suen",
      "Yang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jeon_Subnet-Aware_Dynamic_Supernet_Training_for_Neural_Architecture_Search_CVPR_2025_paper.html": {
    "title": "Subnet-Aware Dynamic Supernet Training for Neural Architecture Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeimin Jeon",
      "Youngmin Oh",
      "Junghyup Lee",
      "Donghyeon Baek",
      "Dohyung Kim",
      "Chanho Eom",
      "Bumsub Ham"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cao_MoVE-KD_Knowledge_Distillation_for_VLMs_with_Mixture_of_Visual_Encoders_CVPR_2025_paper.html": {
    "title": "MoVE-KD: Knowledge Distillation for VLMs with Mixture of Visual Encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajun Cao",
      "Yuan Zhang",
      "Tao Huang",
      "Ming Lu",
      "Qizhe Zhang",
      "Ruichuan An",
      "Ningning Ma",
      "Shanghang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yuan_CamFreeDiff_Camera-free_Image_to_Panorama_Generation_with_Diffusion_Model_CVPR_2025_paper.html": {
    "title": "CamFreeDiff: Camera-free Image to Panorama Generation with Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoding Yuan",
      "Shitao Tang",
      "Kejie Li",
      "Peng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gu_Improving_Visual_and_Downstream_Performance_of_Low-Light_Enhancer_with_Vision_CVPR_2025_paper.html": {
    "title": "Improving Visual and Downstream Performance of Low-Light Enhancer with Vision Foundation Models Collaboration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Gu",
      "Haoxuan Wang",
      "Pengyang Ling",
      "Zhixiang Wei",
      "Huaian Chen",
      "Yi Jin",
      "Enhong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yue_EchoWorld_Learning_Motion-Aware_World_Models_for_Echocardiography_Probe_Guidance_CVPR_2025_paper.html": {
    "title": "EchoWorld: Learning Motion-Aware World Models for Echocardiography Probe Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Yue",
      "Yulin Wang",
      "Haojun Jiang",
      "Pan Liu",
      "Shiji Song",
      "Gao Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Choi_Controllable_Human_Image_Generation_with_Personalized_Multi-Garments_CVPR_2025_paper.html": {
    "title": "Controllable Human Image Generation with Personalized Multi-Garments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yisol Choi",
      "Sangkyung Kwak",
      "Sihyun Yu",
      "Hyungwon Choi",
      "Jinwoo Shin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Asokan_FineLIP_Extending_CLIPs_Reach_via_Fine-Grained_Alignment_with_Longer_Text_CVPR_2025_paper.html": {
    "title": "FineLIP: Extending CLIP's Reach via Fine-Grained Alignment with Longer Text Inputs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mothilal Asokan",
      "Kebin Wu",
      "Fatima Albreiki"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Oh_Illumination_Spectrum_Estimation_for_Multispectral_Images_via_Surface_Reflectance_Modeling_CVPR_2025_paper.html": {
    "title": "Illumination Spectrum Estimation for Multispectral Images via Surface Reflectance Modeling and Spatial-Spectral Feature Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyejin Oh",
      "Woo-Shik Kim",
      "Sangyoon Lee",
      "YungKyung Park",
      "Je-Won Kang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_UHD-processer_Unified_UHD_Image_Restoration_with_Progressive_Frequency_Learning_and_CVPR_2025_paper.html": {
    "title": "UHD-processer: Unified UHD Image Restoration with Progressive Frequency Learning and Degradation-aware Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yidi Liu",
      "Dong Li",
      "Xueyang Fu",
      "Xin Lu",
      "Jie Huang",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ge_Divot_Diffusion_Powers_Video_Tokenizer_for_Comprehension_and_Generation_CVPR_2025_paper.html": {
    "title": "Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuying Ge",
      "Yizhuo Li",
      "Yixiao Ge",
      "Ying Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language_CVPR_2025_paper.html": {
    "title": "Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacong Xu",
      "Shao-Yuan Lo",
      "Bardia Safaei",
      "Vishal M. Patel",
      "Isht Dwivedi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease_CVPR_2025_paper.html": {
    "title": "Towards Explainable and Unprecedented Accuracy in Matching Challenging Finger Crease Patterns",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Zhou",
      "Chengdong Dong",
      "Ajay Kumar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Neural_Hierarchical_Decomposition_for_Single_Image_Plant_Modeling_CVPR_2025_paper.html": {
    "title": "Neural Hierarchical Decomposition for Single Image Plant Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Liu",
      "Zhanglin Cheng",
      "Naoto Yokoya"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tu_GBC-Splat_Generalizable_Gaussian-Based_Clothed_Human_Digitalization_under_Sparse_RGB_Cameras_CVPR_2025_paper.html": {
    "title": "GBC-Splat: Generalizable Gaussian-Based Clothed Human Digitalization under Sparse RGB Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanzhang Tu",
      "Zhanfeng Liao",
      "Boyao Zhou",
      "Shunyuan Zheng",
      "Xilong Zhou",
      "Liuxin Zhang",
      "QianYing Wang",
      "Yebin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bahmani_AC3D_Analyzing_and_Improving_3D_Camera_Control_in_Video_Diffusion_CVPR_2025_paper.html": {
    "title": "AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sherwin Bahmani",
      "Ivan Skorokhodov",
      "Guocheng Qian",
      "Aliaksandr Siarohin",
      "Willi Menapace",
      "Andrea Tagliasacchi",
      "David B. Lindell",
      "Sergey Tulyakov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jatyani_A_Unified_Model_for_Compressed_Sensing_MRI_Across_Undersampling_Patterns_CVPR_2025_paper.html": {
    "title": "A Unified Model for Compressed Sensing MRI Across Undersampling Patterns",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armeet Singh Jatyani",
      "Jiayun Wang",
      "Aditi Chandrashekar",
      "Zihui Wu",
      "Miguel Liu-Schiaffini",
      "Bahareh Tolooshams",
      "Anima Anandkumar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Video-Guided_Foley_Sound_Generation_with_Multimodal_Controls_CVPR_2025_paper.html": {
    "title": "Video-Guided Foley Sound Generation with Multimodal Controls",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Chen",
      "Prem Seetharaman",
      "Bryan Russell",
      "Oriol Nieto",
      "David Bourgin",
      "Andrew Owens",
      "Justin Salamon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Dual-Agent_Optimization_framework_for_Cross-Domain_Few-Shot_Segmentation_CVPR_2025_paper.html": {
    "title": "Dual-Agent Optimization framework for Cross-Domain Few-Shot Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyang Li",
      "Yuan Wang",
      "Wangkai Li",
      "Tianzhu Zhang",
      "Xiang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration_CVPR_2025_paper.html": {
    "title": "SACB-Net: Spatial-awareness Convolutions for Medical Image Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinxing Cheng",
      "Tianyang Zhang",
      "Wenqi Lu",
      "Qingjie Meng",
      "Alejandro F. Frangi",
      "Jinming Duan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Text_Embedding_is_Not_All_You_Need_Attention_Control_for_CVPR_2025_paper.html": {
    "title": "Text Embedding is Not All You Need: Attention Control for Text-to-Image Semantic Alignment with Text Self-Attention Maps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeeyung Kim",
      "Erfan Esmaeili",
      "Qiang Qiu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_DCEvo_Discriminative_Cross-Dimensional_Evolutionary_Learning_for_Infrared_and_Visible_Image_CVPR_2025_paper.html": {
    "title": "DCEvo: Discriminative Cross-Dimensional Evolutionary Learning for Infrared and Visible Image Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyuan Liu",
      "Bowei Zhang",
      "Qingyun Mei",
      "Xingyuan Li",
      "Yang Zou",
      "Zhiying Jiang",
      "Long Ma",
      "Risheng Liu",
      "Xin Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dong_TSD-SR_One-Step_Diffusion_with_Target_Score_Distillation_for_Real-World_Image_CVPR_2025_paper.html": {
    "title": "TSD-SR: One-Step Diffusion with Target Score Distillation for Real-World Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linwei Dong",
      "Qingnan Fan",
      "Yihong Guo",
      "Zhonghao Wang",
      "Qi Zhang",
      "Jinwei Chen",
      "Yawei Luo",
      "Changqing Zou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments_CVPR_2025_paper.html": {
    "title": "AIpparel: A Multimodal Foundation Model for Digital Garments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kiyohiro Nakayama",
      "Jan Ackermann",
      "Timur Levent Kesdogan",
      "Yang Zheng",
      "Maria Korosteleva",
      "Olga Sorkine-Hornung",
      "Leonidas J. Guibas",
      "Guandao Yang",
      "Gordon Wetzstein"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Fast3R_Towards_3D_Reconstruction_of_1000_Images_in_One_Forward_CVPR_2025_paper.html": {
    "title": "Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianing Yang",
      "Alexander Sax",
      "Kevin J. Liang",
      "Mikael Henaff",
      "Hao Tang",
      "Ang Cao",
      "Joyce Chai",
      "Franziska Meier",
      "Matt Feiszli"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lei_StyleStudio_Text-Driven_Style_Transfer_with_Selective_Control_of_Style_Elements_CVPR_2025_paper.html": {
    "title": "StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingkun Lei",
      "Xue Song",
      "Beier Zhu",
      "Hao Wang",
      "Chi Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Didolkar_CTRL-O_Language-Controllable_Object-Centric_Visual_Representation_Learning_CVPR_2025_paper.html": {
    "title": "CTRL-O: Language-Controllable Object-Centric Visual Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aniket Didolkar",
      "Andrii Zadaianchuk",
      "Rabiul Awal",
      "Maximilian Seitzer",
      "Efstratios Gavves",
      "Aishwarya Agrawal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_PO3AD_Predicting_Point_Offsets_toward_Better_3D_Point_Cloud_Anomaly_CVPR_2025_paper.html": {
    "title": "PO3AD: Predicting Point Offsets toward Better 3D Point Cloud Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianan Ye",
      "Weiguang Zhao",
      "Xi Yang",
      "Guangliang Cheng",
      "Kaizhu Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nandam_Text_Augmented_Correlation_Transformer_For_Few-shot_Classification__Segmentation_CVPR_2025_paper.html": {
    "title": "Text Augmented Correlation Transformer For Few-shot Classification & Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Srinivasa Rao Nandam",
      "Sara Atito",
      "Zhenhua Feng",
      "Josef Kittler",
      "Muhammad Awais"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal_CVPR_2025_paper.html": {
    "title": "F^3OCUS - Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Updating Strategy via Multi-objective Meta-Heuristics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pramit Saha",
      "Felix Wagner",
      "Divyanshu Mishra",
      "Can Peng",
      "Anshul Thakur",
      "David A. Clifton",
      "Konstantinos Kamnitsas",
      "J. Alison Noble"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_ICT_Image-Object_Cross-Level_Trusted_Intervention_for_Mitigating_Object_Hallucination_in_CVPR_2025_paper.html": {
    "title": "ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junzhe Chen",
      "Tianshu Zhang",
      "Shiyu Huang",
      "Yuwei Niu",
      "Linfeng Zhang",
      "Lijie Wen",
      "Xuming Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bernal-Berdun_PreciseCam_Precise_Camera_Control_for_Text-to-Image_Generation_CVPR_2025_paper.html": {
    "title": "PreciseCam: Precise Camera Control for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edurne Bernal-Berdun",
      "Ana Serrano",
      "Belen Masia",
      "Matheus Gadelha",
      "Yannick Hold-Geoffroy",
      "Xin Sun",
      "Diego Gutierrez"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Oh_3D_Occupancy_Prediction_with_Low-Resolution_Queries_via_Prototype-aware_View_Transformation_CVPR_2025_paper.html": {
    "title": "3D Occupancy Prediction with Low-Resolution Queries via Prototype-aware View Transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gyeongrok Oh",
      "Sungjune Kim",
      "Heeju Ko",
      "Hyung-gun Chi",
      "Jinkyu Kim",
      "Dongwook Lee",
      "Daehyun Ji",
      "Sungjoon Choi",
      "Sujin Jang",
      "Sangpil Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Unified_Dense_Prediction_of_Video_Diffusion_CVPR_2025_paper.html": {
    "title": "Unified Dense Prediction of Video Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lehan Yang",
      "Lu Qi",
      "Xiangtai Li",
      "Sheng Li",
      "Varun Jampani",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liao_Can_Large_Vision-Language_Models_Correct_Semantic_Grounding_Errors_By_Themselves_CVPR_2025_paper.html": {
    "title": "Can Large Vision-Language Models Correct Semantic Grounding Errors By Themselves?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan-Hong Liao",
      "Rafid Mahmood",
      "Sanja Fidler",
      "David Acuna"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_SET_Spectral_Enhancement_for_Tiny_Object_Detection_CVPR_2025_paper.html": {
    "title": "SET: Spectral Enhancement for Tiny Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huixin Sun",
      "Runqi Wang",
      "Yanjing Li",
      "Linlin Yang",
      "Shaohui Lin",
      "Xianbin Cao",
      "Baochang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_g3D-LF_Generalizable_3D-Language_Feature_Fields_for_Embodied_Tasks_CVPR_2025_paper.html": {
    "title": "g3D-LF: Generalizable 3D-Language Feature Fields for Embodied Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Wang",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Towards_Million-Scale_Adversarial_Robustness_Evaluation_With_Stronger_Individual_Attacks_CVPR_2025_paper.html": {
    "title": "Towards Million-Scale Adversarial Robustness Evaluation With Stronger Individual Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yong Xie",
      "Weijie Zheng",
      "Hanxun Huang",
      "Guangnan Ye",
      "Xingjun Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Temporal_Action_Detection_Model_Compression_by_Progressive_Block_Drop_CVPR_2025_paper.html": {
    "title": "Temporal Action Detection Model Compression by Progressive Block Drop",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyong Chen",
      "Yong Guo",
      "Jiaming Liang",
      "Sitong Zhuang",
      "Runhao Zeng",
      "Xiping Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chung_Differentiable_Inverse_Rendering_with_Interpretable_Basis_BRDFs_CVPR_2025_paper.html": {
    "title": "Differentiable Inverse Rendering with Interpretable Basis BRDFs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoon-Gyu Chung",
      "Seokjun Choi",
      "Seung-Hwan Baek"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_EquiPose_Exploiting_Permutation_Equivariance_for_Relative_Camera_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "EquiPose: Exploiting Permutation Equivariance for Relative Camera Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhen Liu",
      "Qiulei Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Face_Forgery_Video_Detection_via_Temporal_Forgery_Cue_Unraveling_CVPR_2025_paper.html": {
    "title": "Face Forgery Video Detection via Temporal Forgery Cue Unraveling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zonghui Guo",
      "Yingjie Liu",
      "Jie Zhang",
      "Haiyong Zheng",
      "Shiguang Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots_CVPR_2025_paper.html": {
    "title": "Temporally Consistent Object-Centric Learning by Contrasting Slots",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna Manasyan",
      "Maximilian Seitzer",
      "Filip Radovic",
      "Georg Martius",
      "Andrii Zadaianchuk"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_MC2_Multi-concept_Guidance_for_Customized__Multi-concept_Generation_CVPR_2025_paper.html": {
    "title": "MC^2: Multi-concept Guidance for Customized Multi-concept Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxiu Jiang",
      "Yabo Zhang",
      "Kailai Feng",
      "Xiaohe Wu",
      "Wenbo Li",
      "Renjing Pei",
      "Fan Li",
      "Wangmeng Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics_CVPR_2025_paper.html": {
    "title": "UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Chen",
      "Zhifei Zhang",
      "He Zhang",
      "Yuqian Zhou",
      "Soo Ye Kim",
      "Qing Liu",
      "Yijun Li",
      "Jianming Zhang",
      "Nanxuan Zhao",
      "Yilin Wang",
      "Hui Ding",
      "Zhe Lin",
      "Hengshuang Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Pursuing_Temporal-Consistent_Video_Virtual_Try-On_via_Dynamic_Pose_Interaction_CVPR_2025_paper.html": {
    "title": "Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Li",
      "Wenqi Zhong",
      "Wei Yu",
      "Yingwei Pan",
      "Dingwen Zhang",
      "Ting Yao",
      "Junwei Han",
      "Tao Mei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Exploring_Contextual_Attribute_Density_in_Referring_Expression_Counting_CVPR_2025_paper.html": {
    "title": "Exploring Contextual Attribute Density in Referring Expression Counting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhicheng Wang",
      "Zhiyu Pan",
      "Zhan Peng",
      "Jian Cheng",
      "Liwen Xiao",
      "Wei Jiang",
      "Zhiguo Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jose_DINOv2_Meets_Text_A_Unified_Framework_for_Image-_and_Pixel-Level_CVPR_2025_paper.html": {
    "title": "DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cijo Jose",
      "Théo Moutakanni",
      "Dahyun Kang",
      "Federico Baldassarre",
      "Timothée Darcet",
      "Hu Xu",
      "Daniel Li",
      "Marc Szafraniec",
      "Michaël Ramamonjisoa",
      "Maxime Oquab",
      "Oriane Siméoni",
      "Huy V. Vo",
      "Patrick Labatut",
      "Piotr Bojanowski"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Learning_Affine_Correspondences_by_Integrating_Geometric_Constraints_CVPR_2025_paper.html": {
    "title": "Learning Affine Correspondences by Integrating Geometric Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengju Sun",
      "Banglei Guan",
      "Zhenbao Yu",
      "Yang Shang",
      "Qifeng Yu",
      "Daniel Barath"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning_CVPR_2025_paper.html": {
    "title": "UCOD-DPL: Unsupervised Camouflaged Object Detection via Dynamic Pseudo-label Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiqi Yan",
      "Lvhai Chen",
      "Huaijia Kou",
      "Shengchuan Zhang",
      "Yan Zhang",
      "Liujuan Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dinh_Geometry_in_Style_3D_Stylization_via_Surface_Normal_Deformation_CVPR_2025_paper.html": {
    "title": "Geometry in Style: 3D Stylization via Surface Normal Deformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nam Anh Dinh",
      "Itai Lang",
      "Hyunwoo Kim",
      "Oded Stein",
      "Rana Hanocka"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis_CVPR_2025_paper.html": {
    "title": "Multi-modal Vision Pre-training for Medical Image Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaohao Rui",
      "Lingzhi Chen",
      "Zhenyu Tang",
      "Lilong Wang",
      "Mianxin Liu",
      "Shaoting Zhang",
      "Xiaosong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_SegMAN_Omni-scale_Context_Modeling_with_State_Space_Models_and_Local_CVPR_2025_paper.html": {
    "title": "SegMAN: Omni-scale Context Modeling with State Space Models and Local Attention for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunxiang Fu",
      "Meng Lou",
      "Yizhou Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qiu_STEP_Enhancing_Video-LLMs_Compositional_Reasoning_by_Spatio-Temporal_Graph-guided_Self-Training_CVPR_2025_paper.html": {
    "title": "STEP: Enhancing Video-LLMs' Compositional Reasoning by Spatio-Temporal Graph-guided Self-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyi Qiu",
      "Minghe Gao",
      "Long Qian",
      "Kaihang Pan",
      "Qifan Yu",
      "Juncheng Li",
      "Wenjie Wang",
      "Siliang Tang",
      "Yueting Zhuang",
      "Tat-Seng Chua"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_OmniFlow_Any-to-Any_Generation_with_Multi-Modal_Rectified_Flows_CVPR_2025_paper.html": {
    "title": "OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shufan Li",
      "Konstantinos Kallidromitis",
      "Akash Gokul",
      "Zichun Liao",
      "Yusuke Kato",
      "Kazuki Kozuka",
      "Aditya Grover"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_PVC_Progressive_Visual_Token_Compression_for_Unified_Image_and_Video_CVPR_2025_paper.html": {
    "title": "PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyu Yang",
      "Xuan Dong",
      "Xizhou Zhu",
      "Weijie Su",
      "Jiahao Wang",
      "Hao Tian",
      "Zhe Chen",
      "Wenhai Wang",
      "Lewei Lu",
      "Jifeng Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sabathier_LIM_Large_Interpolator_Model_for_Dynamic_Reconstruction_CVPR_2025_paper.html": {
    "title": "LIM: Large Interpolator Model for Dynamic Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Remy Sabathier",
      "Niloy J. Mitra",
      "David Novotny"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_Multiple_Object_Tracking_as_ID_Prediction_CVPR_2025_paper.html": {
    "title": "Multiple Object Tracking as ID Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruopeng Gao",
      "Ji Qi",
      "Limin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ge_AutoPresent_Designing_Structured_Visuals_from_Scratch_CVPR_2025_paper.html": {
    "title": "AutoPresent: Designing Structured Visuals from Scratch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Ge",
      "Zora Zhiruo Wang",
      "Xuhui Zhou",
      "Yi-Hao Peng",
      "Sanjay Subramanian",
      "Qinyue Tan",
      "Maarten Sap",
      "Alane Suhr",
      "Daniel Fried",
      "Graham Neubig",
      "Trevor Darrell"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos_CVPR_2025_paper.html": {
    "title": "SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzheng Liu",
      "Siyan Dong",
      "Shuzhe Wang",
      "Yingda Yin",
      "Yanchao Yang",
      "Qingnan Fan",
      "Baoquan Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_PIDLoc_Cross-View_Pose_Optimization_Network_Inspired_by_PID_Controllers_CVPR_2025_paper.html": {
    "title": "PIDLoc: Cross-View Pose Optimization Network Inspired by PID Controllers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wooju Lee",
      "Juhye Park",
      "Dasol Hong",
      "Changki Sung",
      "Youngwoo Seo",
      "DongWan Kang",
      "Hyun Myung"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chou_VisionArena_230k_Real_World_User-VLM_Conversations_with_Preference_Labels_CVPR_2025_paper.html": {
    "title": "VisionArena: 230k Real World User-VLM Conversations with Preference Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Chou",
      "Lisa Dunlap",
      "Koki Mashita",
      "Krishna Mandal",
      "Trevor Darrell",
      "Ion Stoica",
      "Joseph E. Gonzalez",
      "Wei-Lin Chiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_FAM_Diffusion_Frequency_and_Attention_Modulation_for_High-Resolution_Image_Generation_CVPR_2025_paper.html": {
    "title": "FAM Diffusion: Frequency and Attention Modulation for High-Resolution Image Generation with Stable Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haosen Yang",
      "Adrian Bulat",
      "Isma Hadji",
      "Hai X. Pham",
      "Xiatian Zhu",
      "Georgios Tzimiropoulos",
      "Brais Martinez"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_DreamOmni_Unified_Image_Generation_and_Editing_CVPR_2025_paper.html": {
    "title": "DreamOmni: Unified Image Generation and Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Xia",
      "Yuechen Zhang",
      "Jingyao Li",
      "Chengyao Wang",
      "Yitong Wang",
      "Xinglong Wu",
      "Bei Yu",
      "Jiaya Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Hash3D_Training-free_Acceleration_for_3D_Generation_CVPR_2025_paper.html": {
    "title": "Hash3D: Training-free Acceleration for 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyi Yang",
      "Songhua Liu",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cong_SemGeoMo_Dynamic_Contextual_Human_Motion_Generation_with_Semantic_and_Geometric_CVPR_2025_paper.html": {
    "title": "SemGeoMo: Dynamic Contextual Human Motion Generation with Semantic and Geometric Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peishan Cong",
      "Ziyi Wang",
      "Yuexin Ma",
      "Xiangyu Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_MultiGO_Towards_Multi-level_Geometry_Learning_for_Monocular_3D_Textured_Human_CVPR_2025_paper.html": {
    "title": "MultiGO: Towards Multi-level Geometry Learning for Monocular 3D Textured Human Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gangjian Zhang",
      "Nanjie Yao",
      "Shunsi Zhang",
      "Hanfeng Zhao",
      "Guoliang Pang",
      "Jian Shu",
      "Hao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Generative_Photomontage_CVPR_2025_paper.html": {
    "title": "Generative Photomontage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sean J. Liu",
      "Nupur Kumari",
      "Ariel Shamir",
      "Jun-Yan Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation_CVPR_2025_paper.html": {
    "title": "Multi-view Reconstruction via SfM-guided Monocular Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Guo",
      "He Zhu",
      "Sida Peng",
      "Haotong Lin",
      "Yunzhi Yan",
      "Tao Xie",
      "Wenguan Wang",
      "Xiaowei Zhou",
      "Hujun Bao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Learning_Hazing_to_Dehazing_Towards_Realistic_Haze_Generation_for_Real-World_CVPR_2025_paper.html": {
    "title": "Learning Hazing to Dehazing: Towards Realistic Haze Generation for Real-World Image Dehazing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiyi Wang",
      "Yushuo Zheng",
      "Zicheng Zhang",
      "Chunyi Li",
      "Shuaicheng Liu",
      "Guangtao Zhai",
      "Xiaohong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_HuMoCon_Concept_Discovery_for_Human_Motion_Understanding_CVPR_2025_paper.html": {
    "title": "HuMoCon: Concept Discovery for Human Motion Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihang Fang",
      "Chengcheng Tang",
      "Bugra Tekin",
      "Shugao Ma",
      "Yanchao Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Loiseau_RUBIK_A_Structured_Benchmark_for_Image_Matching_across_Geometric_Challenges_CVPR_2025_paper.html": {
    "title": "RUBIK: A Structured Benchmark for Image Matching across Geometric Challenges",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thibaut Loiseau",
      "Guillaume Bourmaud"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Fast_and_Accurate_Gigapixel_Pathological_Image_Classification_with_Hierarchical_Distillation_CVPR_2025_paper.html": {
    "title": "Fast and Accurate Gigapixel Pathological Image Classification with Hierarchical Distillation Multi-Instance Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiuyang Dong",
      "Junjun Jiang",
      "Kui Jiang",
      "Jiahan Li",
      "Yongbing Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bai_FreeScene_Mixed_Graph_Diffusion_for_3D_Scene_Synthesis_from_Free_CVPR_2025_paper.html": {
    "title": "FreeScene: Mixed Graph Diffusion for 3D Scene Synthesis from Free Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongyuan Bai",
      "Wangyuanfan Bai",
      "Dong Chen",
      "Tieru Wu",
      "Manyi Li",
      "Rui Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_Rethinking_Correspondence-based_Category-Level_Object_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "Rethinking Correspondence-based Category-Level Object Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huan Ren",
      "Wenfei Yang",
      "Shifeng Zhang",
      "Tianzhu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Croitoru_Curriculum_Direct_Preference_Optimization_for_Diffusion_and_Consistency_Models_CVPR_2025_paper.html": {
    "title": "Curriculum Direct Preference Optimization for Diffusion and Consistency Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florinel-Alin Croitoru",
      "Vlad Hondru",
      "Radu Tudor Ionescu",
      "Nicu Sebe",
      "Mubarak Shah"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera_CVPR_2025_paper.html": {
    "title": "IncEventGS: Pose-Free Gaussian Splatting from a Single Event Camera",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Huang",
      "Chengrui Dong",
      "Xuanhua Chen",
      "Peidong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gutbrod_OpenMIBOOD_Open_Medical_Imaging_Benchmarks_for_Out-Of-Distribution_Detection_CVPR_2025_paper.html": {
    "title": "OpenMIBOOD: Open Medical Imaging Benchmarks for Out-Of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max Gutbrod",
      "David Rauber",
      "Danilo Weber Nunes",
      "Christoph Palm"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Detecting_Open_World_Objects_via_Partial_Attribute_Assignment_CVPR_2025_paper.html": {
    "title": "Detecting Open World Objects via Partial Attribute Assignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muli Yang",
      "Gabriel James Goenawan",
      "Huaiyuan Qin",
      "Kai Han",
      "Xi Peng",
      "Yanhua Yang",
      "Hongyuan Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Heiman_FactCheXcker_Mitigating_Measurement_Hallucinations_in_Chest_X-ray_Report_Generation_Models_CVPR_2025_paper.html": {
    "title": "FactCheXcker: Mitigating Measurement Hallucinations in Chest X-ray Report Generation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alice Heiman",
      "Xiaoman Zhang",
      "Emma Chen",
      "Sung Eun Kim",
      "Pranav Rajpurkar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Malik_Neural_Inverse_Rendering_from_Propagating_Light_CVPR_2025_paper.html": {
    "title": "Neural Inverse Rendering from Propagating Light",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anagh Malik",
      "Benjamin Attal",
      "Andrew Xie",
      "Matthew O'Toole",
      "David B. Lindell"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_When_the_Future_Becomes_the_Past_Taming_Temporal_Correspondence_for_CVPR_2025_paper.html": {
    "title": "When the Future Becomes the Past: Taming Temporal Correspondence for Self-supervised Video Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Liu",
      "Qianqian Xu",
      "Peisong Wen",
      "Siran Dai",
      "Qingming Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dang_Personalized_Preference_Fine-tuning_of_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Personalized Preference Fine-tuning of Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meihua Dang",
      "Anikait Singh",
      "Linqi Zhou",
      "Stefano Ermon",
      "Jiaming Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DecoupledGaussian_Object-Scene_Decoupling_for_Physics-Based_Interaction_CVPR_2025_paper.html": {
    "title": "DecoupledGaussian: Object-Scene Decoupling for Physics-Based Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miaowei Wang",
      "Yibo Zhang",
      "Weiwei Xu",
      "Rui Ma",
      "Changqing Zou",
      "Daniel Morris"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation_CVPR_2025_paper.html": {
    "title": "UniPose: A Unified Multimodal Framework for Human Pose Comprehension, Generation and Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiheng Li",
      "Ruibing Hou",
      "Hong Chang",
      "Shiguang Shan",
      "Xilin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ji_POMP_Physics-consistent_Motion_Generative_Model_through_Phase_Manifolds_CVPR_2025_paper.html": {
    "title": "POMP: Physics-consistent Motion Generative Model through Phase Manifolds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Ji",
      "Ye Pan",
      "Zhimeng Liu",
      "Shuai Tan",
      "Xiaogang Jin",
      "Xiaokang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_NN-Former_Rethinking_Graph_Structure_in_Neural_Architecture_Representation_CVPR_2025_paper.html": {
    "title": "NN-Former: Rethinking Graph Structure in Neural Architecture Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihan Xu",
      "Haokui Zhang",
      "Yaowei Wang",
      "Wei Zeng",
      "Shiliang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds_CVPR_2025_paper.html": {
    "title": "DashGaussian: Optimizing 3D Gaussian Splatting in 200 Seconds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youyu Chen",
      "Junjun Jiang",
      "Kui Jiang",
      "Xiao Tang",
      "Zhihao Li",
      "Xianming Liu",
      "Yinyu Nie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qian_Reasoning_to_Attend_Try_to_Understand_How_SEG_Token_Works_CVPR_2025_paper.html": {
    "title": "Reasoning to Attend: Try to Understand How <SEG> Token Works",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Qian",
      "Xin Yin",
      "Dejing Dou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_ReSpec_Relevance_and_Specificity_Grounded_Online_Filtering_for_Learning_on_CVPR_2025_paper.html": {
    "title": "ReSpec: Relevance and Specificity Grounded Online Filtering for Learning on Video-Text Data Streams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chris Dongjoo Kim",
      "Jihwan Moon",
      "Sangwoo Moon",
      "Heeseung Yun",
      "Sihaeng Lee",
      "Aniruddha Kembhavi",
      "Soonyoung Lee",
      "Gunhee Kim",
      "Sangho Lee",
      "Christopher Clark"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_A_Unified_Image-Dense_Annotation_Generation_Model_for_Underwater_Scenes_CVPR_2025_paper.html": {
    "title": "A Unified Image-Dense Annotation Generation Model for Underwater Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongkai Lin",
      "Dingkang Liang",
      "Zhenghao Qi",
      "Xiang Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dhouib_PACT_Pruning_and_Clustering-Based_Token_Reduction_for_Faster_Visual_Language_CVPR_2025_paper.html": {
    "title": "PACT: Pruning and Clustering-Based Token Reduction for Faster Visual Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Dhouib",
      "Davide Buscaldi",
      "Sonia Vanier",
      "Aymen Shabou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_R-SCoRe_Revisiting_Scene_Coordinate_Regression_for_Robust_Large-Scale_Visual_Localization_CVPR_2025_paper.html": {
    "title": "R-SCoRe: Revisiting Scene Coordinate Regression for Robust Large-Scale Visual Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xudong Jiang",
      "Fangjinhua Wang",
      "Silvano Galliani",
      "Christoph Vogel",
      "Marc Pollefeys"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_DynRefer_Delving_into_Region-level_Multimodal_Tasks_via_Dynamic_Resolution_CVPR_2025_paper.html": {
    "title": "DynRefer: Delving into Region-level Multimodal Tasks via Dynamic Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhong Zhao",
      "Feng Liu",
      "Yue Liu",
      "Mingxiang Liao",
      "Chen Gong",
      "Qixiang Ye",
      "Fang Wan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jeong_Playing_the_Fool_Jailbreaking_LLMs_and_Multimodal_LLMs_with_Out-of-Distribution_CVPR_2025_paper.html": {
    "title": "Playing the Fool: Jailbreaking LLMs and Multimodal LLMs with Out-of-Distribution Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joonhyun Jeong",
      "Seyun Bae",
      "Yeonsung Jung",
      "Jaeryong Hwang",
      "Eunho Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection_CVPR_2025_paper.html": {
    "title": "Style Evolving along Chain-of-Thought for Unknown-Domain Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Zhang",
      "Aming Wu",
      "Yahong Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_NTR-Gaussian_Nighttime_Dynamic_Thermal_Reconstruction_with_4D_Gaussian_Splatting_Based_CVPR_2025_paper.html": {
    "title": "NTR-Gaussian: Nighttime Dynamic Thermal Reconstruction with 4D Gaussian Splatting Based on Thermodynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Yang",
      "Yuxiang Liu",
      "Zeyu Cui",
      "Yu Liu",
      "Maojun Zhang",
      "Shen Yan",
      "Qing Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with_CVPR_2025_paper.html": {
    "title": "OmniSplat: Taming Feed-Forward 3D Gaussian Splatting for Omnidirectional Images with Editable Capabilities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suyoung Lee",
      "Jaeyoung Chung",
      "Kihoon Kim",
      "Jaeyoo Huh",
      "Gunhee Lee",
      "Minsoo Lee",
      "Kyoung Mu Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_VideoWorld_Exploring_Knowledge_Learning_from_Unlabeled_Videos_CVPR_2025_paper.html": {
    "title": "VideoWorld: Exploring Knowledge Learning from Unlabeled Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongwei Ren",
      "Yunchao Wei",
      "Xun Guo",
      "Yao Zhao",
      "Bingyi Kang",
      "Jiashi Feng",
      "Xiaojie Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_FSHNet_Fully_Sparse_Hybrid_Network_for_3D_Object_Detection_CVPR_2025_paper.html": {
    "title": "FSHNet: Fully Sparse Hybrid Network for 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Liu",
      "Mingyue Cui",
      "Boyang Li",
      "Quanmin Liang",
      "Tinghe Hong",
      "Kai Huang",
      "Yunxiao Shan",
      "Kai Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_3D-SLNR_A_Super_Lightweight_Neural_Representation_for_Large-scale_3D_Mapping_CVPR_2025_paper.html": {
    "title": "3D-SLNR: A Super Lightweight Neural Representation for Large-scale 3D Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhui Shi",
      "Fulin Tang",
      "Ning An",
      "Yihong Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gu_UniVAD_A_Training-free_Unified_Model_for_Few-shot_Visual_Anomaly_Detection_CVPR_2025_paper.html": {
    "title": "UniVAD: A Training-free Unified Model for Few-shot Visual Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaopeng Gu",
      "Bingke Zhu",
      "Guibo Zhu",
      "Yingying Chen",
      "Ming Tang",
      "Jinqiao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_STINR_Deciphering_Spatial_Transcriptomics_via_Implicit_Neural_Representation_CVPR_2025_paper.html": {
    "title": "STINR: Deciphering Spatial Transcriptomics via Implicit Neural Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yisi Luo",
      "Xile Zhao",
      "Kai Ye",
      "Deyu Meng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shao_Remote_Photoplethysmography_in_Real-World_and_Extreme_Lighting_Scenarios_CVPR_2025_paper.html": {
    "title": "Remote Photoplethysmography in Real-World and Extreme Lighting Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Shao",
      "Lei Luo",
      "Jianjun Qian",
      "Mengkai Yan",
      "Shuo Chen",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jamal_Multi-Modal_Contrastive_Masked_Autoencoders_A_Two-Stage_Progressive_Pre-training_Approach_for_CVPR_2025_paper.html": {
    "title": "Multi-Modal Contrastive Masked Autoencoders: A Two-Stage Progressive Pre-training Approach for RGBD Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Abdullah Jamal",
      "Omid Mohareri"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_JTD-UAV_MLLM-Enhanced_Joint_Tracking_and_Description_Framework_for_Anti-UAV_Systems_CVPR_2025_paper.html": {
    "title": "JTD-UAV: MLLM-Enhanced Joint Tracking and Description Framework for Anti-UAV Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Wang",
      "Jian Zhao",
      "Zhaoxin Fan",
      "Xin Zhang",
      "Xuecheng Wu",
      "Yudian Zhang",
      "Lei Jin",
      "Xinyue Li",
      "Gang Wang",
      "Mengxi Jia",
      "Ping Hu",
      "Zheng Zhu",
      "Xuelong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos_CVPR_2025_paper.html": {
    "title": "HaWoR: World-Space Hand Motion Reconstruction from Egocentric Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinglei Zhang",
      "Jiankang Deng",
      "Chao Ma",
      "Rolandos Alexandros Potamias"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lai_Font-Agent_Enhancing_Font_Understanding_with_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "Font-Agent: Enhancing Font Understanding with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingxin Lai",
      "Cuijie Xu",
      "Haitian Shi",
      "Guoqing Yang",
      "Xiaoning Li",
      "Zhiming Luo",
      "Shaozi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jia_Secret_Lies_in_Color_Enhancing_AI-Generated_Images_Detection_with_Color_CVPR_2025_paper.html": {
    "title": "Secret Lies in Color: Enhancing AI-Generated Images Detection with Color Distribution Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zexi Jia",
      "Chuanwei Huang",
      "Yeshuang Zhu",
      "Hongyan Fei",
      "Xiaoyue Duan",
      "Zhiqiang Yuan",
      "Ying Deng",
      "Jiapei Zhang",
      "Jinchao Zhang",
      "Jie Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Heinrich_RADIOv2.5_Improved_Baselines_for_Agglomerative_Vision_Foundation_Models_CVPR_2025_paper.html": {
    "title": "RADIOv2.5: Improved Baselines for Agglomerative Vision Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Greg Heinrich",
      "Mike Ranzinger",
      "Hongxu Yin",
      "Yao Lu",
      "Jan Kautz",
      "Andrew Tao",
      "Bryan Catanzaro",
      "Pavlo Molchanov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Vincent_High_Temporal_Consistency_through_Semantic_Similarity_Propagation_in_Semi-Supervised_Video_CVPR_2025_paper.html": {
    "title": "High Temporal Consistency through Semantic Similarity Propagation in Semi-Supervised Video Semantic Segmentation for Autonomous Flight",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cédric Vincent",
      "Taehyoung Kim",
      "Henri Meeß"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Cross-Modal_and_Uncertainty-Aware_Agglomeration_for_Open-Vocabulary_3D_Scene_Understanding_CVPR_2025_paper.html": {
    "title": "Cross-Modal and Uncertainty-Aware Agglomeration for Open-Vocabulary 3D Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinlong Li",
      "Cristiano Saltori",
      "Fabio Poiesi",
      "Nicu Sebe"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Generative_Gaussian_Splatting_for_Unbounded_3D_City_Generation_CVPR_2025_paper.html": {
    "title": "Generative Gaussian Splatting for Unbounded 3D City Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haozhe Xie",
      "Zhaoxi Chen",
      "Fangzhou Hong",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Du_SVLTA_Benchmarking_Vision-Language_Temporal_Alignment_via_Synthetic_Video_Situation_CVPR_2025_paper.html": {
    "title": "SVLTA: Benchmarking Vision-Language Temporal Alignment via Synthetic Video Situation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Du",
      "Bo Wu",
      "Yan Lu",
      "Zhendong Mao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Mixture_of_Submodules_for_Domain_Adaptive_Person_Search_CVPR_2025_paper.html": {
    "title": "Mixture of Submodules for Domain Adaptive Person Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minsu Kim",
      "Seungryong Kim",
      "Kwanghoon Sohn"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tourani_Unsupervised_Discovery_of_Facial_Landmarks_and_Head_Pose_CVPR_2025_paper.html": {
    "title": "Unsupervised Discovery of Facial Landmarks and Head Pose",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Satyajit Tourani",
      "Siddharth Tourani",
      "Arif Mahmood",
      "Muhammad Haris Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Instruct-CLIP_Improving_Instruction-Guided_Image_Editing_with_Automated_Data_Refinement_Using_CVPR_2025_paper.html": {
    "title": "Instruct-CLIP: Improving Instruction-Guided Image Editing with Automated Data Refinement Using Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sherry X. Chen",
      "Misha Sra",
      "Pradeep Sen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Stabilizing_and_Accelerating_Autofocus_with_Expert_Trajectory_Regularized_Deep_Reinforcement_CVPR_2025_paper.html": {
    "title": "Stabilizing and Accelerating Autofocus with Expert Trajectory Regularized Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shouhang Zhu",
      "Chenglin Li",
      "Yuankun Jiang",
      "Li Wei",
      "Nuowen Kan",
      "Ziyang Zheng",
      "Wenrui Dai",
      "Junni Zou",
      "Hongkai Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pham_SharpDepth_Sharpening_Metric_Depth_Predictions_Using_Diffusion_Distillation_CVPR_2025_paper.html": {
    "title": "SharpDepth: Sharpening Metric Depth Predictions Using Diffusion Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duc-Hai Pham",
      "Tung Do",
      "Phong Nguyen",
      "Binh-Son Hua",
      "Khoi Nguyen",
      "Rang Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Karmann_Repurposing_Stable_Diffusion_Attention_for_Training-Free_Unsupervised_Interactive_Segmentation_CVPR_2025_paper.html": {
    "title": "Repurposing Stable Diffusion Attention for Training-Free Unsupervised Interactive Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Markus Karmann",
      "Onay Urfalioglu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_GO-N3RDet_Geometry_Optimized_NeRF-enhanced_3D_Object_Detector_CVPR_2025_paper.html": {
    "title": "GO-N3RDet: Geometry Optimized NeRF-enhanced 3D Object Detector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zechuan Li",
      "Hongshan Yu",
      "Yihao Ding",
      "Jinhao Qiao",
      "Basim Azam",
      "Naveed Akhtar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_DPSeg_Dual-Prompt_Cost_Volume_Learning_for_Open-Vocabulary_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "DPSeg: Dual-Prompt Cost Volume Learning for Open-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyu Zhao",
      "Xiaoguang Li",
      "Lingjia Shi",
      "Nasrin Imanpour",
      "Song Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video_CVPR_2025_paper.html": {
    "title": "EvEnhancer: Empowering Effectiveness, Efficiency and Generalizability for Continuous Space-Time Video Super-Resolution with Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuoyan Wei",
      "Feng Li",
      "Shengeng Tang",
      "Yao Zhao",
      "Huihui Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Seeing_A_3D_World_in_A_Grain_of_Sand_CVPR_2025_paper.html": {
    "title": "Seeing A 3D World in A Grain of Sand",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufan Zhang",
      "Yu Ji",
      "Yu Guo",
      "Jinwei Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for_CVPR_2025_paper.html": {
    "title": "Simulator HC: Regression-based Online Simulation of Starting Problem-Solution Pairs for Homotopy Continuation in Geometric Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyue Zhang",
      "Zijia Dai",
      "Wanting Xu",
      "Laurent Kneip"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Dynamic_Integration_of_Task-Specific_Adapters_for_Class_Incremental_Learning_CVPR_2025_paper.html": {
    "title": "Dynamic Integration of Task-Specific Adapters for Class Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiashuo Li",
      "Shaokun Wang",
      "Bo Qian",
      "Yuhang He",
      "Xing Wei",
      "Qiang Wang",
      "Yihong Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_MoFlow_One-Step_Flow_Matching_for_Human_Trajectory_Forecasting_via_Implicit_CVPR_2025_paper.html": {
    "title": "MoFlow: One-Step Flow Matching for Human Trajectory Forecasting via Implicit Maximum Likelihood Estimation based Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiang Fu",
      "Qi Yan",
      "Lele Wang",
      "Ke Li",
      "Renjie Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in_CVPR_2025_paper.html": {
    "title": "EgoPressure: A Dataset for Hand Pressure and Pose Estimation in Egocentric Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Zhao",
      "Taein Kwon",
      "Paul Streli",
      "Marc Pollefeys",
      "Christian Holz"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Morshed_DiverseFlow_Sample-Efficient_Diverse_Mode_Coverage_in_Flows_CVPR_2025_paper.html": {
    "title": "DiverseFlow: Sample-Efficient Diverse Mode Coverage in Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mashrur M. Morshed",
      "Vishnu Boddeti"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval_CVPR_2025_paper.html": {
    "title": "Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot Composed Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanmin Tang",
      "Jue Zhang",
      "Xiaoting Qin",
      "Jing Yu",
      "Gaopeng Gou",
      "Gang Xiong",
      "Qingwei Lin",
      "Saravan Rajmohan",
      "Dongmei Zhang",
      "Qi Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_UniGraspTransformer_Simplified_Policy_Distillation_for_Scalable_Dexterous_Robotic_Grasping_CVPR_2025_paper.html": {
    "title": "UniGraspTransformer: Simplified Policy Distillation for Scalable Dexterous Robotic Grasping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbo Wang",
      "Fangyun Wei",
      "Lei Zhou",
      "Xi Chen",
      "Lin Luo",
      "Xiaohan Yi",
      "Yizhong Zhang",
      "Yaobo Liang",
      "Chang Xu",
      "Yan Lu",
      "Jiaolong Yang",
      "Baining Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mei_GeoMM_On_Geodesic_Perspective_for_Multi-modal_Learning_CVPR_2025_paper.html": {
    "title": "GeoMM: On Geodesic Perspective for Multi-modal Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shibin Mei",
      "Hang Wang",
      "Bingbing Ni"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_VISCO_Benchmarking_Fine-Grained_Critique_and_Correction_Towards_Self-Improvement_in_Visual_CVPR_2025_paper.html": {
    "title": "VISCO: Benchmarking Fine-Grained Critique and Correction Towards Self-Improvement in Visual Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueqing Wu",
      "Yuheng Ding",
      "Bingxuan Li",
      "Pan Lu",
      "Da Yin",
      "Kai-Wei Chang",
      "Nanyun Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ni_MaskGWM_A_Generalizable_Driving_World_Model_with_Video_Mask_Reconstruction_CVPR_2025_paper.html": {
    "title": "MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingcheng Ni",
      "Yuxin Guo",
      "Yichen Liu",
      "Rui Chen",
      "Lewei Lu",
      "Zehuan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qian_3D-MVP_3D_Multiview_Pretraining_for_Manipulation_CVPR_2025_paper.html": {
    "title": "3D-MVP: 3D Multiview Pretraining for Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengyi Qian",
      "Kaichun Mo",
      "Valts Blukis",
      "David F. Fouhey",
      "Dieter Fox",
      "Ankit Goyal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Enhanced_OoD_Detection_through_Cross-Modal_Alignment_of_Multi-Modal_Representations_CVPR_2025_paper.html": {
    "title": "Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeonghyeon Kim",
      "Sangheum Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Adaptive_Dropout_Unleashing_Dropout_across_Layers_for_Generalizable_Image_Super-Resolution_CVPR_2025_paper.html": {
    "title": "Adaptive Dropout: Unleashing Dropout across Layers for Generalizable Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Xu",
      "Jie Huang",
      "Wei Yu",
      "Jiangtong Tan",
      "Zhen Zou",
      "Feng Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy_CVPR_2025_paper.html": {
    "title": "Breaking the Memory Barrier of Contrastive Loss via Tile-Based Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zesen Cheng",
      "Hang Zhang",
      "Kehan Li",
      "Sicong Leng",
      "Zhiqiang Hu",
      "Fei Wu",
      "Deli Zhao",
      "Xin Li",
      "Lidong Bing"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tan_Mimir_Improving_Video_Diffusion_Models_for_Precise_Text_Understanding_CVPR_2025_paper.html": {
    "title": "Mimir: Improving Video Diffusion Models for Precise Text Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Tan",
      "Biao Gong",
      "Yutong Feng",
      "Kecheng Zheng",
      "Dandan Zheng",
      "Shuwei Shi",
      "Yujun Shen",
      "Jingdong Chen",
      "Ming Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_UCM-VeID_V2_A_Richer_Dataset_and_A_Pre-training_Method_for_CVPR_2025_paper.html": {
    "title": "UCM-VeID V2: A Richer Dataset and A Pre-training Method for UAV Cross-Modality Vehicle Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyue Liu",
      "Jiahao Qi",
      "Chen Chen",
      "KangCheng Bin",
      "Ping Zhong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video_CVPR_2025_paper.html": {
    "title": "Learning Phase Distortion with Selective State Space Models for Video Turbulence Mitigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingguang Zhang",
      "Nicholas Chimitt",
      "Xijun Wang",
      "Yu Yuan",
      "Stanley H. Chan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding_CVPR_2025_paper.html": {
    "title": "RoboPEPP: Vision-Based Robot Pose and Joint Angle Estimation through Embedding Predictive Pre-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raktim Gautam Goswami",
      "Prashanth Krishnamurthy",
      "Yann LeCun",
      "Farshad Khorrami"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model_CVPR_2025_paper.html": {
    "title": "Distraction is All You Need for Multimodal Large Language Model Jailbreaking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zuopeng Yang",
      "Jiluan Fan",
      "Anli Yan",
      "Erdun Gao",
      "Xin Lin",
      "Tao Li",
      "Kanghua Mo",
      "Changyu Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zohar_Apollo__An_Exploration_of_Video_Understanding_in_Large_Multimodal_CVPR_2025_paper.html": {
    "title": "Apollo: An Exploration of Video Understanding in Large Multimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Orr Zohar",
      "Xiaohan Wang",
      "Yann Dubois",
      "Nikhil Mehta",
      "Tong Xiao",
      "Philippe Hansen-Estruch",
      "Licheng Yu",
      "Xiaofang Wang",
      "Felix Juefei-Xu",
      "Ning Zhang",
      "Serena Yeung-Levy",
      "Xide Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Skip_Tuning_Pre-trained_Vision-Language_Models_are_Effective_and_Efficient_Adapters_CVPR_2025_paper.html": {
    "title": "Skip Tuning: Pre-trained Vision-Language Models are Effective and Efficient Adapters Themselves",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shihan Wu",
      "Ji Zhang",
      "Pengpeng Zeng",
      "Lianli Gao",
      "Jingkuan Song",
      "Heng Tao Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_PatchDPO_Patch-level_DPO_for_Finetuning-free_Personalized_Image_Generation_CVPR_2025_paper.html": {
    "title": "PatchDPO: Patch-level DPO for Finetuning-free Personalized Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihan Huang",
      "Long Chan",
      "Jinlong Liu",
      "Wanggui He",
      "Hao Jiang",
      "Mingli Song",
      "Jie Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Learning_to_Normalize_on_the_SPD_Manifold_under_Bures-Wasserstein_Geometry_CVPR_2025_paper.html": {
    "title": "Learning to Normalize on the SPD Manifold under Bures-Wasserstein Geometry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Wang",
      "Shaocheng Jin",
      "Ziheng Chen",
      "Xiaoqing Luo",
      "Xiao-Jun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation_CVPR_2025_paper.html": {
    "title": "SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Claudia Cuttano",
      "Gabriele Trivigno",
      "Gabriele Rosi",
      "Carlo Masone",
      "Giuseppe Averta"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual_CVPR_2025_paper.html": {
    "title": "MegaSaM: Accurate, Fast and Robust Structure and Motion from Casual Dynamic Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengqi Li",
      "Richard Tucker",
      "Forrester Cole",
      "Qianqian Wang",
      "Linyi Jin",
      "Vickie Ye",
      "Angjoo Kanazawa",
      "Aleksander Holynski",
      "Noah Snavely"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance_CVPR_2025_paper.html": {
    "title": "BEVDiffuser: Plug-and-Play Diffusion Model for BEV Denoising with Ground-Truth Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Ye",
      "Burhaneddin Yaman",
      "Sheng Cheng",
      "Feng Tao",
      "Abhirup Mallik",
      "Liu Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_GeoAvatar_Geometrically-Consistent_Multi-Person_Avatar_Reconstruction_from_Sparse_Multi-View_Videos_CVPR_2025_paper.html": {
    "title": "GeoAvatar: Geometrically-Consistent Multi-Person Avatar Reconstruction from Sparse Multi-View Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soohyun Lee",
      "Seoyeon Kim",
      "HeeKyung Lee",
      "Won-Sik Jeong",
      "Joo Ho Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shao_FinePhys_Fine-grained_Human_Action_Generation_by_Explicitly_Incorporating_Physical_Laws_CVPR_2025_paper.html": {
    "title": "FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dian Shao",
      "Mingfei Shi",
      "Shengda Xu",
      "Haodong Chen",
      "Yongle Huang",
      "Binglu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_DiET-GS_Diffusion_Prior_and_Event_Stream-Assisted_Motion_Deblurring_3D_Gaussian_CVPR_2025_paper.html": {
    "title": "DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungjun Lee",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hanson_Speedy-Splat_Fast_3D_Gaussian_Splatting_with_Sparse_Pixels_and_Sparse_CVPR_2025_paper.html": {
    "title": "Speedy-Splat: Fast 3D Gaussian Splatting with Sparse Pixels and Sparse Primitives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Hanson",
      "Allen Tu",
      "Geng Lin",
      "Vasu Singla",
      "Matthias Zwicker",
      "Tom Goldstein"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration_CVPR_2025_paper.html": {
    "title": "SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianyi Wang",
      "Zhijie Lin",
      "Meng Wei",
      "Yang Zhao",
      "Ceyuan Yang",
      "Chen Change Loy",
      "Lu Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhong_Beyond_Generation_A_Diffusion-based_Low-level_Feature_Extractor_for_Detecting_AI-generated_CVPR_2025_paper.html": {
    "title": "Beyond Generation: A Diffusion-based Low-level Feature Extractor for Detecting AI-generated Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan Zhong",
      "Haoyu Chen",
      "Yiran Xu",
      "Zhenxing Qian",
      "Xinpeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Robust-MVTON_Learning_Cross-Pose_Feature_Alignment_and_Fusion_for_Robust_Multi-View_CVPR_2025_paper.html": {
    "title": "Robust-MVTON: Learning Cross-Pose Feature Alignment and Fusion for Robust Multi-View Virtual Try-On",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nannan Zhang",
      "Yijiang Li",
      "Dong Du",
      "Zheng Chong",
      "Zhengwentai Sun",
      "Jianhao Zeng",
      "Yusheng Dai",
      "Zhengyu Xie",
      "Hairui Zhu",
      "Xiaoguang Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Plizzari_Omnia_de_EgoTempo_Benchmarking_Temporal_Understanding_of_Multi-Modal_LLMs_in_CVPR_2025_paper.html": {
    "title": "Omnia de EgoTempo: Benchmarking Temporal Understanding of Multi-Modal LLMs in Egocentric Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chiara Plizzari",
      "Alessio Tonioni",
      "Yongqin Xian",
      "Achin Kulshrestha",
      "Federico Tombari"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_ODHSR_Online_Dense_3D_Reconstruction_of_Humans_and_Scenes_from_CVPR_2025_paper.html": {
    "title": "ODHSR: Online Dense 3D Reconstruction of Humans and Scenes from Monocular Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zetong Zhang",
      "Manuel Kaufmann",
      "Lixin Xue",
      "Jie Song",
      "Martin R. Oswald"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition_CVPR_2025_paper.html": {
    "title": "Identity-Preserving Text-to-Video Generation by Frequency Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenghai Yuan",
      "Jinfa Huang",
      "Xianyi He",
      "Yunyang Ge",
      "Yujun Shi",
      "Liuhan Chen",
      "Jiebo Luo",
      "Li Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_FreeGave_3D_Physics_Learning_from_Dynamic_Videos_by_Gaussian_Velocity_CVPR_2025_paper.html": {
    "title": "FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinxi Li",
      "Ziyang Song",
      "Siyuan Zhou",
      "Bo Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_SpiritSight_Agent_Advanced_GUI_Agent_with_One_Look_CVPR_2025_paper.html": {
    "title": "SpiritSight Agent: Advanced GUI Agent with One Look",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Huang",
      "Ziming Cheng",
      "Junting Pan",
      "Zhaohui Hou",
      "Mingjie Zhan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild_CVPR_2025_paper.html": {
    "title": "Zero-Shot Monocular Scene Flow Estimation in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiqing Liang",
      "Abhishek Badki",
      "Hang Su",
      "James Tompkin",
      "Orazio Gallo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_MG-MotionLLM_A_Unified_Framework_for_Motion_Comprehension_and_Generation_across_CVPR_2025_paper.html": {
    "title": "MG-MotionLLM: A Unified Framework for Motion Comprehension and Generation across Multiple Granularities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bizhu Wu",
      "Jinheng Xie",
      "Keming Shen",
      "Zhe Kong",
      "Jianfeng Ren",
      "Ruibin Bai",
      "Rong Qu",
      "Linlin Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_Retaining_Knowledge_and_Enhancing_Long-Text_Representations_in_CLIP_through_Dual-Teacher_CVPR_2025_paper.html": {
    "title": "Retaining Knowledge and Enhancing Long-Text Representations in CLIP through Dual-Teacher Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuheng Feng",
      "Changsong Wen",
      "Zelin Peng",
      "Li jiaye",
      "Siyu Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_MMRL_Multi-Modal_Representation_Learning_for_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "MMRL: Multi-Modal Representation Learning for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuncheng Guo",
      "Xiaodong Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nam_Optical-Flow_Guided_Prompt_Optimization_for_Coherent_Video_Generation_CVPR_2025_paper.html": {
    "title": "Optical-Flow Guided Prompt Optimization for Coherent Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyelin Nam",
      "Jaemin Kim",
      "Dohun Lee",
      "Jong Chul Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peng_MOS_Modeling_Object-Scene_Associations_in_Generalized_Category_Discovery_CVPR_2025_paper.html": {
    "title": "MOS: Modeling Object-Scene Associations in Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyuan Peng",
      "Jinpeng Ma",
      "Zhimin Sun",
      "Ran Yi",
      "Haichuan Song",
      "Xin Tan",
      "Lizhuang Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tan_Anchor-Aware_Similarity_Cohesion_in_Target_Frames_Enables_Predicting_Temporal_Moment_CVPR_2025_paper.html": {
    "title": "Anchor-Aware Similarity Cohesion in Target Frames Enables Predicting Temporal Moment Boundaries in 2D",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Tan",
      "Hongxing Wang",
      "Junwu Weng",
      "Jiaxin Li",
      "Zhilong Ou",
      "Kang Dang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shanmugam_Test-time_Augmentation_Improves_Efficiency_in_Conformal_Prediction_CVPR_2025_paper.html": {
    "title": "Test-time Augmentation Improves Efficiency in Conformal Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Divya Shanmugam",
      "Helen Lu",
      "Swami Sankaranarayanan",
      "John Guttag"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_Breaking_the_Low-Rank_Dilemma_of_Linear_Attention_CVPR_2025_paper.html": {
    "title": "Breaking the Low-Rank Dilemma of Linear Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihang Fan",
      "Huaibo Huang",
      "Ran He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_StoryGPT-V_Large_Language_Models_as_Consistent_Story_Visualizers_CVPR_2025_paper.html": {
    "title": "StoryGPT-V: Large Language Models as Consistent Story Visualizers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoqian Shen",
      "Mohamed Elhoseiny"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Code-as-Monitor_Constraint-aware_Visual_Programming_for_Reactive_and_Proactive_Robotic_Failure_CVPR_2025_paper.html": {
    "title": "Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enshen Zhou",
      "Qi Su",
      "Cheng Chi",
      "Zhizheng Zhang",
      "Zhongyuan Wang",
      "Tiejun Huang",
      "Lu Sheng",
      "He Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Embracing_Collaboration_Over_Competition_Condensing_Multiple_Prompts_for_Visual_In-Context_CVPR_2025_paper.html": {
    "title": "Embracing Collaboration Over Competition: Condensing Multiple Prompts for Visual In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinpeng Wang",
      "Tianci Luo",
      "Yaohua Zha",
      "Yan Feng",
      "Ruisheng Luo",
      "Bin Chen",
      "Tao Dai",
      "Long Chen",
      "Yaowei Wang",
      "Shu-Tao Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Rethinking_Reconstruction_and_Denoising_in_the_Dark_New_Perspective_General_CVPR_2025_paper.html": {
    "title": "Rethinking Reconstruction and Denoising in the Dark: New Perspective, General Architecture and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tengyu Ma",
      "Long Ma",
      "Ziye Li",
      "Yuetong Wang",
      "Jinyuan Liu",
      "Chengpei Xu",
      "Risheng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hadji_Edge-SD-SR_Low_Latency_and_Parameter_Efficient_On-device_Super-Resolution_with_Stable_CVPR_2025_paper.html": {
    "title": "Edge-SD-SR: Low Latency and Parameter Efficient On-device Super-Resolution with Stable Diffusion via Bidirectional Conditioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isma Hadji",
      "Mehdi Noroozi",
      "Victor Escorcia",
      "Anestis Zaganidis",
      "Brais Martinez",
      "Georgios Tzimiropoulos"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_Unity_in_Diversity_Video_Editing_via_Gradient-Latent_Purification_CVPR_2025_paper.html": {
    "title": "Unity in Diversity: Video Editing via Gradient-Latent Purification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyu Gao",
      "Kunlin Yang",
      "Xuan Yao",
      "Yufan Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective_CVPR_2025_paper.html": {
    "title": "Revealing Key Details to See Differences: A Novel Prototypical Perspective for Skeleton-based Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongda Liu",
      "Yunfan Liu",
      "Min Ren",
      "Hao Wang",
      "Yunlong Wang",
      "Zhenan Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dages_Finsler_Multi-Dimensional_Scaling_Manifold_Learning_for_Asymmetric_Dimensionality_Reduction_and_CVPR_2025_paper.html": {
    "title": "Finsler Multi-Dimensional Scaling: Manifold Learning for Asymmetric Dimensionality Reduction and Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Dagès",
      "Simon Weber",
      "Ya-Wei Eileen Lin",
      "Ronen Talmon",
      "Daniel Cremers",
      "Michael Lindenbaum",
      "Alfred M. Bruckstein",
      "Ron Kimmel"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via_CVPR_2025_paper.html": {
    "title": "VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songhao Han",
      "Wei Huang",
      "Hairong Shi",
      "Le Zhuo",
      "Xiu Su",
      "Shifeng Zhang",
      "Xu Zhou",
      "Xiaojuan Qi",
      "Yue Liao",
      "Si Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_INFP_Audio-Driven_Interactive_Head_Generation_in_Dyadic_Conversations_CVPR_2025_paper.html": {
    "title": "INFP: Audio-Driven Interactive Head Generation in Dyadic Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongming Zhu",
      "Longhao Zhang",
      "Zhengkun Rong",
      "Tianshu Hu",
      "Shuang Liang",
      "Zhipeng Ge"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Federated_Learning_with_Domain_Shift_Eraser_CVPR_2025_paper.html": {
    "title": "Federated Learning with Domain Shift Eraser",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Wang",
      "Zihui Wang",
      "Zheng Wang",
      "Xiaoliang Fan",
      "Cheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lahlali_Cross-Modal_Distillation_for_2D3D_Multi-Object_Discovery_from_2D_Motion_CVPR_2025_paper.html": {
    "title": "Cross-Modal Distillation for 2D/3D Multi-Object Discovery from 2D Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saad Lahlali",
      "Sandra Kara",
      "Hejer Ammar",
      "Florian Chabot",
      "Nicolas Granger",
      "Hervé Le Borgne",
      "Quoc-Cuong Pham"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cai_DiTCtrl_Exploring_Attention_Control_in_Multi-Modal_Diffusion_Transformer_for_Tuning-Free_CVPR_2025_paper.html": {
    "title": "DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghong Cai",
      "Xiaodong Cun",
      "Xiaoyu Li",
      "Wenze Liu",
      "Zhaoyang Zhang",
      "Yong Zhang",
      "Ying Shan",
      "Xiangyu Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Bridge_Frame_and_Event_Common_Spatiotemporal_Fusion_for_High-Dynamic_Scene_CVPR_2025_paper.html": {
    "title": "Bridge Frame and Event: Common Spatiotemporal Fusion for High-Dynamic Scene Optical Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyu Zhou",
      "Haonan Wang",
      "Haoyue Liu",
      "Yuxing Duan",
      "Yi Chang",
      "Luxin Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_EVPGS_Enhanced_View_Prior_Guidance_for_Splatting-based_Extrapolated_View_Synthesis_CVPR_2025_paper.html": {
    "title": "EVPGS: Enhanced View Prior Guidance for Splatting-based Extrapolated View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahe Li",
      "Feiyu Wang",
      "Xiaochao Qu",
      "Chengjing Wu",
      "Luoqi Liu",
      "Ting Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shao_GREAT_Geometry-Intention_Collaborative_Inference_for_Open-Vocabulary_3D_Object_Affordance_Grounding_CVPR_2025_paper.html": {
    "title": "GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D Object Affordance Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yawen Shao",
      "Wei Zhai",
      "Yuhang Yang",
      "Hongchen Luo",
      "Yang Cao",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Marchellus_Link_to_the_Past_Temporal_Propagation_for_Fast_3D_Human_CVPR_2025_paper.html": {
    "title": "Link to the Past: Temporal Propagation for Fast 3D Human Reconstruction from Monocular Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Marchellus",
      "Nadhira Noor",
      "In Kyu Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Inversion_Circle_Interpolation_Diffusion-based_Image_Augmentation_for_Data-scarce_Classification_CVPR_2025_paper.html": {
    "title": "Inversion Circle Interpolation: Diffusion-based Image Augmentation for Data-scarce Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanghao Wang",
      "Long Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Deterministic_Certification_of_Graph_Neural_Networks_against_Graph_Poisoning_Attacks_CVPR_2025_paper.html": {
    "title": "Deterministic Certification of Graph Neural Networks against Graph Poisoning Attacks with Arbitrary Perturbations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiate Li",
      "Meng Pang",
      "Yun Dong",
      "Binghui Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_A3_Few-shot_Prompt_Learning_of_Unlearnable_Examples_with_Cross-Modal_Adversarial_CVPR_2025_paper.html": {
    "title": "A3: Few-shot Prompt Learning of Unlearnable Examples with Cross-Modal Adversarial Feature Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Wang",
      "Xitong Gao",
      "Dongping Liao",
      "Tianrui Qin",
      "Yu-liang Lu",
      "Cheng-zhong Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lv_Adapting_Pre-trained_3D_Models_for_Point_Cloud_Video_Understanding_via_CVPR_2025_paper.html": {
    "title": "Adapting Pre-trained 3D Models for Point Cloud Video Understanding via Cross-frame Spatio-temporal Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baixuan Lv",
      "Yaohua Zha",
      "Tao Dai",
      "Xue Yuerong",
      "Ke Chen",
      "Shu-Tao Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations_CVPR_2025_paper.html": {
    "title": "MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through Disentangled Spatial-Temporal Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyungho Bae",
      "Jinhyung Kim",
      "Sihaeng Lee",
      "Soonyoung Lee",
      "Gunhee Lee",
      "Jinwoo Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lai_UWAV_Uncertainty-weighted_Weakly-supervised_Audio-Visual_Video_Parsing_CVPR_2025_paper.html": {
    "title": "UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yung-Hsuan Lai",
      "Janek Ebbers",
      "Yu-Chiang Frank Wang",
      "François Germain",
      "Michael Jeffrey Jones",
      "Moitreya Chatterjee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Mosaic_of_Modalities_A_Comprehensive_Benchmark_for_Multimodal_Graph_Learning_CVPR_2025_paper.html": {
    "title": "Mosaic of Modalities: A Comprehensive Benchmark for Multimodal Graph Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Zhu",
      "Yuhang Zhou",
      "Shengyi Qian",
      "Zhongmou He",
      "Tong Zhao",
      "Neil Shah",
      "Danai Koutra"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_TSP-Mamba_The_Travelling_Salesman_Problem_Meets_Mamba_for_Image_Super-resolution_CVPR_2025_paper.html": {
    "title": "TSP-Mamba: The Travelling Salesman Problem Meets Mamba for Image Super-resolution and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Zhou",
      "Xinyu Lin",
      "Jiangbo Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_MVPaint_Synchronized_Multi-View_Diffusion_for_Painting_Anything_3D_CVPR_2025_paper.html": {
    "title": "MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Cheng",
      "Juncheng Mu",
      "Xianfang Zeng",
      "Xin Chen",
      "Anqi Pang",
      "Chi Zhang",
      "Zhibin Wang",
      "Bin Fu",
      "Gang Yu",
      "Ziwei Liu",
      "Liang Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D_CVPR_2025_paper.html": {
    "title": "FirePlace: Geometric Refinements of LLM Common Sense Reasoning for 3D Object Placement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ian Huang",
      "Yanan Bao",
      "Karen Truong",
      "Howard Zhou",
      "Cordelia Schmid",
      "Leonidas Guibas",
      "Alireza Fathi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/You_RENO_Real-Time_Neural_Compression_for_3D_LiDAR_Point_Clouds_CVPR_2025_paper.html": {
    "title": "RENO: Real-Time Neural Compression for 3D LiDAR Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kang You",
      "Tong Chen",
      "Dandan Ding",
      "M. Salman Asif",
      "Zhan Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gielisse_End-to-End_Implicit_Neural_Representations_for_Classification_CVPR_2025_paper.html": {
    "title": "End-to-End Implicit Neural Representations for Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Gielisse",
      "Jan van Gemert"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_ASAP_Advancing_Semantic_Alignment_Promotes_Multi-Modal_Manipulation_Detecting_and_Grounding_CVPR_2025_paper.html": {
    "title": "ASAP: Advancing Semantic Alignment Promotes Multi-Modal Manipulation Detecting and Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenxing Zhang",
      "Yaxiong Wang",
      "Lechao Cheng",
      "Zhun Zhong",
      "Dan Guo",
      "Meng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sheng_UNICL-SAM_Uncertainty-Driven_In-Context_Segmentation_with_Part_Prototype_Discovery_CVPR_2025_paper.html": {
    "title": "UNICL-SAM: Uncertainty-Driven In-Context Segmentation with Part Prototype Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dianmo Sheng",
      "Dongdong Chen",
      "Zhentao Tan",
      "Qiankun Liu",
      "Qi Chu",
      "Tao Gong",
      "Bin Liu",
      "Jing Han",
      "Wenbin Tu",
      "Shengwei Xu",
      "Nenghai Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tschernezki_Layered_Motion_Fusion_Lifting_Motion_Segmentation_to_3D_in_Egocentric_CVPR_2025_paper.html": {
    "title": "Layered Motion Fusion: Lifting Motion Segmentation to 3D in Egocentric Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vadim Tschernezki",
      "Diane Larlus",
      "Iro Laina",
      "Andrea Vedaldi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_FADE_Frequency-Aware_Diffusion_Model_Factorization_for_Video_Editing_CVPR_2025_paper.html": {
    "title": "FADE: Frequency-Aware Diffusion Model Factorization for Video Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Zhu",
      "Haolin Wang",
      "Shilin Ma",
      "Wenliang Zhao",
      "Yansong Tang",
      "Lei Chen",
      "Jie Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MotiF_Making_Text_Count_in_Image_Animation_with_Motion_Focal_CVPR_2025_paper.html": {
    "title": "MotiF: Making Text Count in Image Animation with Motion Focal Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijie Wang",
      "Samaneh Azadi",
      "Rohit Girdhar",
      "Saketh Rambhatla",
      "Chen Sun",
      "Xi Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse_CVPR_2025_paper.html": {
    "title": "Towards Explicit Geometry-Reflectance Collaboration for Generalized LiDAR Segmentation in Adverse Weather",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longyu Yang",
      "Ping Hu",
      "Shangbo Yuan",
      "Lu Zhang",
      "Jun Liu",
      "Hengtao Shen",
      "Xiaofeng Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mi_Data_Synthesis_with_Diverse_Styles_for_Face_Recognition_via_3DMM-Guided_CVPR_2025_paper.html": {
    "title": "Data Synthesis with Diverse Styles for Face Recognition via 3DMM-Guided Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxi Mi",
      "Zhizhou Zhong",
      "Yuge Huang",
      "Qiuyang Yuan",
      "Xuan Zhao",
      "Jianqing Xu",
      "Shouhong Ding",
      "Shaoming Wang",
      "Rizen Guo",
      "Shuigeng Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cai_Diffusion_Self-Distillation_for_Zero-Shot_Customized_Image_Generation_CVPR_2025_paper.html": {
    "title": "Diffusion Self-Distillation for Zero-Shot Customized Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengqu Cai",
      "Eric Ryan Chan",
      "Yunzhi Zhang",
      "Leonidas Guibas",
      "Jiajun Wu",
      "Gordon Wetzstein"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Uncertainty-guided_Perturbation_for_Image_Super-Resolution_Diffusion_Model_CVPR_2025_paper.html": {
    "title": "Uncertainty-guided Perturbation for Image Super-Resolution Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leheng Zhang",
      "Weiyi You",
      "Kexuan Shi",
      "Shuhang Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning_CVPR_2025_paper.html": {
    "title": "Geometric Knowledge-Guided Localized Global Distribution Alignment for Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanbiao Ma",
      "Wei Dai",
      "Wenke Huang",
      "Jiayi Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Grobrugge_Towards_Human-Understandable_Multi-Dimensional_Concept_Discovery_CVPR_2025_paper.html": {
    "title": "Towards Human-Understandable Multi-Dimensional Concept Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arne Grobrügge",
      "Niklas Kühl",
      "Gerhard Satzger",
      "Philipp Spitzer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_GlyphMastero_A_Glyph_Encoder_for_High-Fidelity_Scene_Text_Editing_CVPR_2025_paper.html": {
    "title": "GlyphMastero: A Glyph Encoder for High-Fidelity Scene Text Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Wang",
      "Ting Liu",
      "Xiaochao Qu",
      "Chengjing Wu",
      "Luoqi Liu",
      "Xiaolin Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xing_ConText-CIR_Learning_from_Concepts_in_Text_for_Composed_Image_Retrieval_CVPR_2025_paper.html": {
    "title": "ConText-CIR: Learning from Concepts in Text for Composed Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Xing",
      "Pranavi Kolouju",
      "Robert Pless",
      "Abby Stylianou",
      "Nathan Jacobs"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MaskGaussian_Adaptive_3D_Gaussian_Representation_from_Probabilistic_Masks_CVPR_2025_paper.html": {
    "title": "MaskGaussian: Adaptive 3D Gaussian Representation from Probabilistic Masks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Liu",
      "Zhihang Zhong",
      "Yifan Zhan",
      "Sheng Xu",
      "Xiao Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hong_Perturb-and-Revise_Flexible_3D_Editing_with_Generative_Trajectories_CVPR_2025_paper.html": {
    "title": "Perturb-and-Revise: Flexible 3D Editing with Generative Trajectories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Susung Hong",
      "Johanna Karras",
      "Ricardo Martin-Brualla",
      "Ira Kemelmacher-Shlizerman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Geng_Birth_and_Death_of_a_Rose_CVPR_2025_paper.html": {
    "title": "Birth and Death of a Rose",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Geng",
      "Yunzhi Zhang",
      "Shangzhe Wu",
      "Jiajun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Learning_Compatible_Multi-Prize_Subnetworks_for_Asymmetric_Retrieval_CVPR_2025_paper.html": {
    "title": "Learning Compatible Multi-Prize Subnetworks for Asymmetric Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushuai Sun",
      "Zikun Zhou",
      "Dongmei Jiang",
      "Yaowei Wang",
      "Jun Yu",
      "Guangming Lu",
      "Wenjie Pei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding_CVPR_2025_paper.html": {
    "title": "SoundVista: Novel-View Ambient Sound Synthesis via Visual-Acoustic Binding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingfei Chen",
      "Israel D. Gebru",
      "Ishwarya Ananthabhotla",
      "Christian Richardt",
      "Dejan Markovic",
      "Jake Sandakly",
      "Steven Krenn",
      "Todd Keebler",
      "Eli Shlizerman",
      "Alexander Richard"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Abbasi_CLIP_Under_the_Microscope_A_Fine-Grained_Analysis_of_Multi-Object_Representation_CVPR_2025_paper.html": {
    "title": "CLIP Under the Microscope: A Fine-Grained Analysis of Multi-Object Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reza Abbasi",
      "Ali Nazari",
      "Aminreza Sefid",
      "Mohammadali Banayeeanzade",
      "Mohammad Hossein Rohban",
      "Mahdieh Soleymani Baghshah"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit_CVPR_2025_paper.html": {
    "title": "MetricGrids: Arbitrary Nonlinear Approximation with Elementary Metric Grids based Implicit Neural Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shu Wang",
      "Yanbo Gao",
      "Shuai Li",
      "Chong Lv",
      "Xun Cai",
      "Chuankun Li",
      "Hui Yuan",
      "Jinglin Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Navigating_Image_Restoration_with_VARs_Distribution_Alignment_Prior_CVPR_2025_paper.html": {
    "title": "Navigating Image Restoration with VAR's Distribution Alignment Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyang Wang",
      "Naishan Zheng",
      "Jie Huang",
      "Feng Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_MovieBench_A_Hierarchical_Movie_Level_Dataset_for_Long_Video_Generation_CVPR_2025_paper.html": {
    "title": "MovieBench: A Hierarchical Movie Level Dataset for Long Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijia Wu",
      "Mingyu Liu",
      "Zeyu Zhu",
      "Xi Xia",
      "Haoen Feng",
      "Wen Wang",
      "Kevin Qinghong Lin",
      "Chunhua Shen",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_Dissecting_and_Mitigating_Diffusion_Bias_via_Mechanistic_Interpretability_CVPR_2025_paper.html": {
    "title": "Dissecting and Mitigating Diffusion Bias via Mechanistic Interpretability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingdong Shi",
      "Changming Li",
      "Yifan Wang",
      "Yongxiang Zhao",
      "Anqi Pang",
      "Sibei Yang",
      "Jingyi Yu",
      "Kan Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for_CVPR_2025_paper.html": {
    "title": "Graph Neural Network Combining Event Stream and Periodic Aggregation for Low-Latency Event-based Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manon Dampfhoffer",
      "Thomas Mesquida",
      "Damien Joubert",
      "Thomas Dalgaty",
      "Pascal Vivet",
      "Christoph Posch"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Be_More_Specific_Evaluating_Object-centric_Realism_in_Synthetic_Images_CVPR_2025_paper.html": {
    "title": "Be More Specific: Evaluating Object-centric Realism in Synthetic Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anqi Liang",
      "Ciprian Corneanu",
      "Qianli Feng",
      "Giorgio Giannone",
      "Aleix Martinez"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Correlative_and_Discriminative_Label_Grouping_for_Multi-Label_Visual_Prompt_Tuning_CVPR_2025_paper.html": {
    "title": "Correlative and Discriminative Label Grouping for Multi-Label Visual Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei-Lei Ma",
      "Shuo Xu",
      "Ming-Kun Xie",
      "Lei Wang",
      "Dengdi Sun",
      "Haifeng Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Simsar_LoRACLR_Contrastive_Adaptation_for_Customization_of_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "LoRACLR: Contrastive Adaptation for Customization of Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enis Simsar",
      "Thomas Hofmann",
      "Federico Tombari",
      "Pinar Yanardag"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Su_ArtFormer_Controllable_Generation_of_Diverse_3D_Articulated_Objects_CVPR_2025_paper.html": {
    "title": "ArtFormer: Controllable Generation of Diverse 3D Articulated Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Su",
      "Youhe Feng",
      "Zheng Li",
      "Jinhua Song",
      "Yangfan He",
      "Botao Ren",
      "Botian Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nousias_Opportunistic_Single-Photon_Time_of_Flight_CVPR_2025_paper.html": {
    "title": "Opportunistic Single-Photon Time of Flight",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sotiris Nousias",
      "Mian Wei",
      "Howard Xiao",
      "Maxx Wu",
      "Shahmeer Athar",
      "Kevin J. Wang",
      "Anagh Malik",
      "David A. Barmherzig",
      "David B. Lindell",
      "Kyros N. Kutulakos"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Bridging_Gait_Recognition_and_Large_Language_Models_Sequence_Modeling_CVPR_2025_paper.html": {
    "title": "Bridging Gait Recognition and Large Language Models Sequence Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaopeng Yang",
      "Jilong Wang",
      "Saihui Hou",
      "Xu Liu",
      "Chunshui Cao",
      "Liang Wang",
      "Yongzhen Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Man_Argus_Vision-Centric_Reasoning_with_Grounded_Chain-of-Thought_CVPR_2025_paper.html": {
    "title": "Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunze Man",
      "De-An Huang",
      "Guilin Liu",
      "Shiwei Sheng",
      "Shilong Liu",
      "Liang-Yan Gui",
      "Jan Kautz",
      "Yu-Xiong Wang",
      "Zhiding Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_Bootstrap_Your_Own_Views_Masked_Ego-Exo_Modeling_for_Fine-grained_View-invariant_CVPR_2025_paper.html": {
    "title": "Bootstrap Your Own Views: Masked Ego-Exo Modeling for Fine-grained View-invariant Video Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungin Park",
      "Jiyoung Lee",
      "Kwanghoon Sohn"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jin_SFDM_Robust_Decomposition_of_Geometry_and_Reflectance_for_Realistic_Face_CVPR_2025_paper.html": {
    "title": "SFDM: Robust Decomposition of Geometry and Reflectance for Realistic Face Rendering from Sparse-view Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daisheng Jin",
      "Jiangbei Hu",
      "Baixin Xu",
      "Yuxin Dai",
      "Chen Qian",
      "Ying He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_DiSRT-In-Bed_Diffusion-Based_Sim-to-Real_Transfer_Framework_for_In-Bed_Human_Mesh_Recovery_CVPR_2025_paper.html": {
    "title": "DiSRT-In-Bed: Diffusion-Based Sim-to-Real Transfer Framework for In-Bed Human Mesh Recovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Gao",
      "Ce Zheng",
      "Laszlo A. Jeni",
      "Zackory Erickson"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wen_Ouroboros3D_Image-to-3D_Generation_via_3D-aware_Recursive_Diffusion_CVPR_2025_paper.html": {
    "title": "Ouroboros3D: Image-to-3D Generation via 3D-aware Recursive Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Wen",
      "Zehuan Huang",
      "Yaohui Wang",
      "Xinyuan Chen",
      "Lu Sheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Di_QMambaBSR_Burst_Image_Super-Resolution_with_Query_State_Space_Model_CVPR_2025_paper.html": {
    "title": "QMambaBSR: Burst Image Super-Resolution with Query State Space Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Di",
      "Long Peng",
      "Peizhe Xia",
      "Wenbo Li",
      "Renjing Pei",
      "Yang Cao",
      "Yang Wang",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Su_Encapsulated_Composition_of_Text-to-Image_and_Text-to-Video_Models_for_High-Quality_Video_CVPR_2025_paper.html": {
    "title": "Encapsulated Composition of Text-to-Image and Text-to-Video Models for High-Quality Video Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongtong Su",
      "Chengyu Wang",
      "Bingyan Liu",
      "Jun Huang",
      "Dongming Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jung_Multi-Group_Proportional_Representations_for_Text-to-Image_Models_CVPR_2025_paper.html": {
    "title": "Multi-Group Proportional Representations for Text-to-Image Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangwon Jung",
      "Alex Oesterling",
      "Claudio Mayrink Verdun",
      "Sajani Vithana",
      "Taesup Moon",
      "Flavio P. Calmon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Messaoud_Towards_Generalizable_Trajectory_Prediction_using_Dual-Level_Representation_Learning_and_Adaptive_CVPR_2025_paper.html": {
    "title": "Towards Generalizable Trajectory Prediction using Dual-Level Representation Learning and Adaptive Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaouther Messaoud",
      "Matthieu Cord",
      "Alexandre Alahi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_CoMatcher_Multi-View_Collaborative_Feature_Matching_CVPR_2025_paper.html": {
    "title": "CoMatcher: Multi-View Collaborative Feature Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jintao Zhang",
      "Zimin Xia",
      "Mingyue Dong",
      "Shuhan Shen",
      "Linwei Yue",
      "Xianwei Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under_CVPR_2025_paper.html": {
    "title": "COUNTS: Benchmarking Object Detectors and Multimodal Large Language Models under Distribution Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiansheng Li",
      "Xingxuan Zhang",
      "Hao Zou",
      "Yige Guo",
      "Renzhe Xu",
      "Yilong Liu",
      "Chuzhao Zhu",
      "Yue He",
      "Peng Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mughal_Retrieving_Semantics_from_the_Deep_an_RAG_Solution_for_Gesture_CVPR_2025_paper.html": {
    "title": "Retrieving Semantics from the Deep: an RAG Solution for Gesture Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "M. Hamza Mughal",
      "Rishabh Dabral",
      "Merel C.J. Scholman",
      "Vera Demberg",
      "Christian Theobalt"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_HOT_Hadamard-based_Optimized_Training_CVPR_2025_paper.html": {
    "title": "HOT: Hadamard-based Optimized Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seonggon Kim",
      "Juncheol Shin",
      "Seung-taek Woo",
      "Eunhyeok Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kundu_Towards_a_Universal_Synthetic_Video_Detector_From_Face_or_Background_CVPR_2025_paper.html": {
    "title": "Towards a Universal Synthetic Video Detector: From Face or Background Manipulations to Fully AI-Generated Content",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohit Kundu",
      "Hao Xiong",
      "Vishal Mohanty",
      "Athula Balachandran",
      "Amit K. Roy-Chowdhury"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qu_TokenFlow_Unified_Image_Tokenizer_for_Multimodal_Understanding_and_Generation_CVPR_2025_paper.html": {
    "title": "TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liao Qu",
      "Huichao Zhang",
      "Yiheng Liu",
      "Xu Wang",
      "Yi Jiang",
      "Yiming Gao",
      "Hu Ye",
      "Daniel K. Du",
      "Zehuan Yuan",
      "Xinglong Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates_CVPR_2025_paper.html": {
    "title": "Improving Personalized Search with Regularized Low-Rank Parameter Updates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fiona Ryan",
      "Josef Sivic",
      "Fabian Caba Heilbron",
      "Judy Hoffman",
      "James M. Rehg",
      "Bryan Russell"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_A_Focused_Human_Body_Model_for_Accurate_Anthropometric_Measurements_Extraction_CVPR_2025_paper.html": {
    "title": "A Focused Human Body Model for Accurate Anthropometric Measurements Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuhang Chen",
      "Xianliang Huang",
      "Zhizhou Zhong",
      "Juhong Guan",
      "Shuigeng Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_SnapGen-V_Generating_a_Five-Second_Video_within_Five_Seconds_on_a_CVPR_2025_paper.html": {
    "title": "SnapGen-V: Generating a Five-Second Video within Five Seconds on a Mobile Device",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushu Wu",
      "Zhixing Zhang",
      "Yanyu Li",
      "Yanwu Xu",
      "Anil Kag",
      "Yang Sui",
      "Huseyin Coskun",
      "Ke Ma",
      "Aleksei Lebedev",
      "Ju Hu",
      "Dimitris N. Metaxas",
      "Yanzhi Wang",
      "Sergey Tulyakov",
      "Jian Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Adapting_Dense_Matching_for_Homography_Estimation_with_Grid-based_Acceleration_CVPR_2025_paper.html": {
    "title": "Adapting Dense Matching for Homography Estimation with Grid-based Acceleration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaining Zhang",
      "Yuxin Deng",
      "Jiayi Ma",
      "Paolo Favaro"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis_CVPR_2025_paper.html": {
    "title": "HyperLoRA: Parameter-Efficient Adaptive Generation for Portrait Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengtian Li",
      "Jinshu Chen",
      "Wanquan Feng",
      "Bingchuan Li",
      "Fei Dai",
      "Songtao Zhao",
      "Qian He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_ACE_Anti-Editing_Concept_Erasure_in_Text-to-Image_Models_CVPR_2025_paper.html": {
    "title": "ACE: Anti-Editing Concept Erasure in Text-to-Image Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Wang",
      "Yuxiang Wei",
      "Fan Li",
      "Renjing Pei",
      "Hang Xu",
      "Wangmeng Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_EchoMatch_Partial-to-Partial_Shape_Matching_via_Correspondence_Reflection_CVPR_2025_paper.html": {
    "title": "EchoMatch: Partial-to-Partial Shape Matching via Correspondence Reflection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizheng Xie",
      "Viktoria Ehm",
      "Paul Roetzer",
      "Nafie El Amrani",
      "Maolin Gao",
      "Florian Bernard",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_CoSDH_Communication-Efficient_Collaborative_Perception_via_Supply-Demand_Awareness_and_Intermediate-Late_Hybridization_CVPR_2025_paper.html": {
    "title": "CoSDH: Communication-Efficient Collaborative Perception via Supply-Demand Awareness and Intermediate-Late Hybridization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhao Xu",
      "Yanan Zhang",
      "Zhi Cai",
      "Di Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bartolomei_Stereo_Anywhere_Robust_Zero-Shot_Deep_Stereo_Matching_Even_Where_Either_CVPR_2025_paper.html": {
    "title": "Stereo Anywhere: Robust Zero-Shot Deep Stereo Matching Even Where Either Stereo or Mono Fail",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Bartolomei",
      "Fabio Tosi",
      "Matteo Poggi",
      "Stefano Mattoccia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lai_Order-Robust_Class_Incremental_Learning_Graph-Driven_Dynamic_Similarity_Grouping_CVPR_2025_paper.html": {
    "title": "Order-Robust Class Incremental Learning: Graph-Driven Dynamic Similarity Grouping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guannan Lai",
      "Yujie Li",
      "Xiangkun Wang",
      "Junbo Zhang",
      "Tianrui Li",
      "Xin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yuan_Where_the_Devil_Hides_Deepfake_Detectors_Can_No_Longer_Be_CVPR_2025_paper.html": {
    "title": "Where the Devil Hides: Deepfake Detectors Can No Longer Be Trusted",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuaiwei Yuan",
      "Junyu Dong",
      "Yuezun Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Synthetic-to-Real_Self-supervised_Robust_Depth_Estimation_via_Learning_with_Motion_and_CVPR_2025_paper.html": {
    "title": "Synthetic-to-Real Self-supervised Robust Depth Estimation via Learning with Motion and Structure Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weilong Yan",
      "Ming Li",
      "Haipeng Li",
      "Shuwei Shao",
      "Robby T. Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Daryani_CaMuViD_Calibration-Free_Multi-View_Detection_CVPR_2025_paper.html": {
    "title": "CaMuViD: Calibration-Free Multi-View Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Etefaghi Daryani",
      "M. Usman Maqbool Bhutta",
      "Byron Hernandez",
      "Henry Medeiros"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Prosody-Enhanced_Acoustic_Pre-training_and_Acoustic-Disentangled_Prosody_Adapting_for_Movie_Dubbing_CVPR_2025_paper.html": {
    "title": "Prosody-Enhanced Acoustic Pre-training and Acoustic-Disentangled Prosody Adapting for Movie Dubbing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhedong Zhang",
      "Liang Li",
      "Chenggang Yan",
      "Chunshan Liu",
      "Anton van den Hengel",
      "Yuankai Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Hierarchical_Knowledge_Prompt_Tuning_for_Multi-task_Test-Time_Adaptation_CVPR_2025_paper.html": {
    "title": "Hierarchical Knowledge Prompt Tuning for Multi-task Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiang Zhang",
      "Mengsheng Zhao",
      "Jiawei Liu",
      "Fanrui Zhang",
      "Yongchao Xu",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mei_Cross-Modal_Interactive_Perception_Network_with_Mamba_for_Lung_Tumor_Segmentation_CVPR_2025_paper.html": {
    "title": "Cross-Modal Interactive Perception Network with Mamba for Lung Tumor Segmentation in PET-CT Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Mei",
      "Chenyu Lin",
      "Yu Qiu",
      "Yaonan Wang",
      "Hui Zhang",
      "Ziyang Wang",
      "Dong Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending_CVPR_2025_paper.html": {
    "title": "LaTexBlend: Scaling Multi-concept Customized Generation with Latent Textual Blending",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Jin",
      "Zhenbo Yu",
      "Yang Shen",
      "Zhenyong Fu",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ho_DejaVid_Encoder-Agnostic_Learned_Temporal_Matching_for_Video_Classification_CVPR_2025_paper.html": {
    "title": "DejaVid: Encoder-Agnostic Learned Temporal Matching for Video Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Darryl Ho",
      "Samuel Madden"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_HVI_A_New_Color_Space_for_Low-light_Image_Enhancement_CVPR_2025_paper.html": {
    "title": "HVI: A New Color Space for Low-light Image Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingsen Yan",
      "Yixu Feng",
      "Cheng Zhang",
      "Guansong Pang",
      "Kangbiao Shi",
      "Peng Wu",
      "Wei Dong",
      "Jinqiu Sun",
      "Yanning Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose_CVPR_2025_paper.html": {
    "title": "DualPM: Dual Posed-Canonical Point Maps for 3D Shape and Pose Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Kaye",
      "Tomas Jakab",
      "Shangzhe Wu",
      "Christian Ruprecht",
      "Andrea Vedaldi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Du_SuperPC_A_Single_Diffusion_Model_for_Point_Cloud_Completion_Upsampling_CVPR_2025_paper.html": {
    "title": "SuperPC: A Single Diffusion Model for Point Cloud Completion, Upsampling, Denoising, and Colorization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Du",
      "Zhipeng Zhao",
      "Shaoshu Su",
      "Sharath Golluri",
      "Haoze Zheng",
      "Runmao Yao",
      "Chen Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Le_One_Diffusion_to_Generate_Them_All_CVPR_2025_paper.html": {
    "title": "One Diffusion to Generate Them All",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duong H. Le",
      "Tuan Pham",
      "Sangho Lee",
      "Christopher Clark",
      "Aniruddha Kembhavi",
      "Stephan Mandt",
      "Ranjay Krishna",
      "Jiasen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Lets_Verify_and_Reinforce_Image_Generation_Step_by_Step_CVPR_2025_paper.html": {
    "title": "Let's Verify and Reinforce Image Generation Step by Step",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renrui Zhang",
      "Chengzhuo Tong",
      "Zhizheng Zhao",
      "Ziyu Guo",
      "Haoquan Zhang",
      "Manyuan Zhang",
      "Jiaming Liu",
      "Peng Gao",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising_CVPR_2025_paper.html": {
    "title": "All-Optical Nonlinear Diffractive Deep Network for Ultrafast Image Denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoling Zhou",
      "Zhemg Lee",
      "Wei Ye",
      "Rui Xie",
      "Wenbo Zhang",
      "Guanju Peng",
      "Zongze Li",
      "Shikun Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ni_Maintaining_Consistent_Inter-Class_Topology_in_Continual_Test-Time_Adaptation_CVPR_2025_paper.html": {
    "title": "Maintaining Consistent Inter-Class Topology in Continual Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenggong Ni",
      "Fan Lyu",
      "Jiayao Tan",
      "Fuyuan Hu",
      "Rui Yao",
      "Tao Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_UNOPose_Unseen_Object_Pose_Estimation_with_an_Unposed_RGB-D_Reference_CVPR_2025_paper.html": {
    "title": "UNOPose: Unseen Object Pose Estimation with an Unposed RGB-D Reference Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Liu",
      "Gu Wang",
      "Ruida Zhang",
      "Chenyangguang Zhang",
      "Federico Tombari",
      "Xiangyang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation_CVPR_2025_paper.html": {
    "title": "CoSER: Towards Consistent Dense Multiview Text-to-Image Generator for 3D Creation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bonan Li",
      "Zicheng Zhang",
      "Xingyi Yang",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sarvestani_HybridMQA_Exploring_Geometry-Texture_Interactions_for_Colored_Mesh_Quality_Assessment_CVPR_2025_paper.html": {
    "title": "HybridMQA: Exploring Geometry-Texture Interactions for Colored Mesh Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armin Shafiee Sarvestani",
      "Sheyang Tang",
      "Zhou Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peng_Generalized_Gaussian_Entropy_Model_for_Point_Cloud_Attribute_Compression_with_CVPR_2025_paper.html": {
    "title": "Generalized Gaussian Entropy Model for Point Cloud Attribute Compression with Dynamic Likelihood Intervals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changhao Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Self-Learning_Hyperspectral_and_Multispectral_Image_Fusion_via_Adaptive_Residual_Guided_CVPR_2025_paper.html": {
    "title": "Self-Learning Hyperspectral and Multispectral Image Fusion via Adaptive Residual Guided Subspace Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Zhu",
      "He Wang",
      "Yang Xu",
      "Zebin Wu",
      "Zhihui Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mao_SIR-DIFF_Sparse_Image_Sets_Restoration_with_Multi-View_Diffusion_Model_CVPR_2025_paper.html": {
    "title": "SIR-DIFF: Sparse Image Sets Restoration with Multi-View Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yucheng Mao",
      "Boyang Wang",
      "Nilesh Kulkarni",
      "Jeong Joon Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_StickMotion_Generating_3D_Human_Motions_by_Drawing_a_Stickman_CVPR_2025_paper.html": {
    "title": "StickMotion: Generating 3D Human Motions by Drawing a Stickman",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Wang",
      "Zhihua Wu",
      "Qiaozhi He",
      "Jiaming Chu",
      "Ling Qian",
      "Yu Cheng",
      "Junliang Xing",
      "Jian Zhao",
      "Lei Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Reversible_Decoupling_Network_for_Single_Image_Reflection_Removal_CVPR_2025_paper.html": {
    "title": "Reversible Decoupling Network for Single Image Reflection Removal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zhao",
      "Mingjia Li",
      "Qiming Hu",
      "Xiaojie Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhong_Hierarchical_Features_Matter_A_Deep_Exploration_of_Progressive_Parameterization_Method_CVPR_2025_paper.html": {
    "title": "Hierarchical Features Matter: A Deep Exploration of Progressive Parameterization Method for Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhao Zhong",
      "Hao Fang",
      "Bin Chen",
      "Xulin Gu",
      "Meikang Qiu",
      "Shuhan Qi",
      "Shu-Tao Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving_CVPR_2025_paper.html": {
    "title": "Enduring, Efficient and Robust Trajectory Prediction Attack in Autonomous Driving via Optimization-Driven Multi-Frame Perturbation Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Yu",
      "Weizhen Han",
      "Libing Wu",
      "Bingyi Liu",
      "Enshu Wang",
      "Zhuangzhuang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Singh_GLASS_Guided_Latent_Slot_Diffusion_for_Object-Centric_Learning_CVPR_2025_paper.html": {
    "title": "GLASS: Guided Latent Slot Diffusion for Object-Centric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krishnakant Singh",
      "Simone Schaub-Meyer",
      "Stefan Roth"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_UNEM_UNrolled_Generalized_EM_for_Transductive_Few-Shot_Learning_CVPR_2025_paper.html": {
    "title": "UNEM: UNrolled Generalized EM for Transductive Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long Zhou",
      "Fereshteh Shakeri",
      "Aymen Sadraoui",
      "Mounir Kaaniche",
      "Jean-Christophe Pesquet",
      "Ismail Ben Ayed"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_SASep_Saliency-Aware_Structured_Separation_of_Geometry_and_Feature_for_Open_CVPR_2025_paper.html": {
    "title": "SASep: Saliency-Aware Structured Separation of Geometry and Feature for Open Set Learning on Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinfeng Xu",
      "Xianzhi Li",
      "Yuan Tang",
      "Xu Han",
      "Qiao Yu",
      "Yixue Hao",
      "Long Hu",
      "Min Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Low-Biased_General_Annotated_Dataset_Generation_CVPR_2025_paper.html": {
    "title": "Low-Biased General Annotated Dataset Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dengyang Jiang",
      "Haoyu Wang",
      "Lei Zhang",
      "Wei Wei",
      "Guang Dai",
      "Mengmeng Wang",
      "Jingdong Wang",
      "Yanning Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_G3Flow_Generative_3D_Semantic_Flow_for_Pose-aware_and_Generalizable_Object_CVPR_2025_paper.html": {
    "title": "G3Flow: Generative 3D Semantic Flow for Pose-aware and Generalizable Object Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianxing Chen",
      "Yao Mu",
      "Zhixuan Liang",
      "Zanxin Chen",
      "Shijia Peng",
      "Qiangyu Chen",
      "Mingkun Xu",
      "Ruizhen Hu",
      "Hongyuan Zhang",
      "Xuelong Li",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Generative_Hard_Example_Augmentation_for_Semantic_Point_Cloud_Segmentation_CVPR_2025_paper.html": {
    "title": "Generative Hard Example Augmentation for Semantic Point Cloud Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zhang",
      "Jibin Peng",
      "Zhao Huang",
      "Wei Feng",
      "Di Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Toward_Generalized_Image_Quality_Assessment_Relaxing_the_Perfect_Reference_Quality_CVPR_2025_paper.html": {
    "title": "Toward Generalized Image Quality Assessment: Relaxing the Perfect Reference Quality Assumption",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Du Chen",
      "Tianhe Wu",
      "Kede Ma",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zeng_Explaining_Domain_Shifts_in_Language_Concept_Erasing_for_Interpretable_Image_CVPR_2025_paper.html": {
    "title": "Explaining Domain Shifts in Language: Concept Erasing for Interpretable Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zequn Zeng",
      "Yudi Su",
      "Jianqiao Sun",
      "Tiansheng Wen",
      "Hao Zhang",
      "Zhengjue Wang",
      "Bo Chen",
      "Hongwei Liu",
      "Jiawei Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ni_Hazy_Low-Quality_Satellite_Video_Restoration_Via_Learning_Optimal_Joint_Degradation_CVPR_2025_paper.html": {
    "title": "Hazy Low-Quality Satellite Video Restoration Via Learning Optimal Joint Degradation Patterns and Continuous-Scale Super-Resolution Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ning Ni",
      "Libao Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chao_Textured_Gaussians_for_Enhanced_3D_Scene_Appearance_Modeling_CVPR_2025_paper.html": {
    "title": "Textured Gaussians for Enhanced 3D Scene Appearance Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Chao",
      "Hung-Yu Tseng",
      "Lorenzo Porzi",
      "Chen Gao",
      "Tuotuo Li",
      "Qinbo Li",
      "Ayush Saraf",
      "Jia-Bin Huang",
      "Johannes Kopf",
      "Gordon Wetzstein",
      "Changil Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_NeighborRetr_Balancing_Hub_Centrality_in_Cross-Modal_Retrieval_CVPR_2025_paper.html": {
    "title": "NeighborRetr: Balancing Hub Centrality in Cross-Modal Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zengrong Lin",
      "Zheng Wang",
      "Tianwen Qian",
      "Pan Mu",
      "Sixian Chan",
      "Cong Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hamann_ETAP_Event-based_Tracking_of_Any_Point_CVPR_2025_paper.html": {
    "title": "ETAP: Event-based Tracking of Any Point",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Friedhelm Hamann",
      "Daniel Gehrig",
      "Filbert Febryanto",
      "Kostas Daniilidis",
      "Guillermo Gallego"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Beyond_Sight_Towards_Cognitive_Alignment_in_LVLM_via_Enriched_Visual_CVPR_2025_paper.html": {
    "title": "Beyond Sight: Towards Cognitive Alignment in LVLM via Enriched Visual Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaqi Zhao",
      "Yuanyang Yin",
      "Lin Li",
      "Mingan Lin",
      "Victor Shea-Jay Huang",
      "Siwei Chen",
      "Weipeng Chen",
      "Baoqun Yin",
      "Zenan Zhou",
      "Wentao Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_Global-Local_Tree_Search_in_VLMs_for_3D_Indoor_Scene_Generation_CVPR_2025_paper.html": {
    "title": "Global-Local Tree Search in VLMs for 3D Indoor Scene Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Deng",
      "Mengshi Qi",
      "Huadong Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Esposito_Volumetric_Surfaces_Representing_Fuzzy_Geometries_with_Layered_Meshes_CVPR_2025_paper.html": {
    "title": "Volumetric Surfaces: Representing Fuzzy Geometries with Layered Meshes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefano Esposito",
      "Anpei Chen",
      "Christian Reiser",
      "Samuel Rota Bulò",
      "Lorenzo Porzi",
      "Katja Schwarz",
      "Christian Richardt",
      "Michael Zollhöfer",
      "Peter Kontschieder",
      "Andreas Geiger"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection_CVPR_2025_paper.html": {
    "title": "Overcoming Shortcut Problem in VLM for Robust Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuo Xu",
      "Xiang Xiang",
      "Yifan Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kang_GFlowVLM_Enhancing_Multi-step_Reasoning_in_Vision-Language_Models_with_Generative_Flow_CVPR_2025_paper.html": {
    "title": "GFlowVLM: Enhancing Multi-step Reasoning in Vision-Language Models with Generative Flow Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoqiang Kang",
      "Enna Sachdeva",
      "Piyush Gupta",
      "Sangjae Bae",
      "Kwonjoon Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qiu_STEPS_Sequential_Probability_Tensor_Estimation_for_Text-to-Image_Hard_Prompt_Search_CVPR_2025_paper.html": {
    "title": "STEPS: Sequential Probability Tensor Estimation for Text-to-Image Hard Prompt Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuning Qiu",
      "Andong Wang",
      "Chao Li",
      "Haonan Huang",
      "Guoxu Zhou",
      "Qibin Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chan Hee Song",
      "Valts Blukis",
      "Jonathan Tremblay",
      "Stephen Tyree",
      "Yu Su",
      "Stan Birchfield"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Weng_VIRES_Video_Instance_Repainting_via_Sketch_and_Text_Guided_Generation_CVPR_2025_paper.html": {
    "title": "VIRES: Video Instance Repainting via Sketch and Text Guided Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuchen Weng",
      "Haojie Zheng",
      "Peixuan Zhang",
      "Yuchen Hong",
      "Han Jiang",
      "Si Li",
      "Boxin Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MAP_Unleashing_Hybrid_Mamba-Transformer_Vision_Backbones_Potential_with_Masked_Autoregressive_CVPR_2025_paper.html": {
    "title": "MAP: Unleashing Hybrid Mamba-Transformer Vision Backbone's Potential with Masked Autoregressive Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunze Liu",
      "Li Yi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Segment_Any-Quality_Images_with_Generative_Latent_Space_Enhancement_CVPR_2025_paper.html": {
    "title": "Segment Any-Quality Images with Generative Latent Space Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangqian Guo",
      "Yong Guo",
      "Xuehui Yu",
      "Wenbo Li",
      "Yaoxing Wang",
      "Shan Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_BG-Triangle_Bezier_Gaussian_Triangle_for_3D_Vectorization_and_Rendering_CVPR_2025_paper.html": {
    "title": "BG-Triangle: Bezier Gaussian Triangle for 3D Vectorization and Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minye Wu",
      "Haizhao Dai",
      "Kaixin Yao",
      "Tinne Tuytelaars",
      "Jingyi Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Men_MIMO_Controllable_Character_Video_Synthesis_with_Spatial_Decomposed_Modeling_CVPR_2025_paper.html": {
    "title": "MIMO: Controllable Character Video Synthesis with Spatial Decomposed Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifang Men",
      "Yuan Yao",
      "Miaomiao Cui",
      "Liefeng Bo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model_CVPR_2025_paper.html": {
    "title": "TKG-DM: Training-free Chroma Key Content Generation Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryugo Morita",
      "Stanislav Frolov",
      "Brian Bernhard Moser",
      "Takahiro Shirakawa",
      "Ko Watanabe",
      "Andreas Dengel",
      "Jinjia Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jia_Lift3D_Policy_Lifting_2D_Foundation_Models_for_Robust_3D_Robotic_CVPR_2025_paper.html": {
    "title": "Lift3D Policy: Lifting 2D Foundation Models for Robust 3D Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yueru Jia",
      "Jiaming Liu",
      "Sixiang Chen",
      "Chenyang Gu",
      "Zhilve Wang",
      "Longzan Luo",
      "Xiaoqi Li",
      "Pengwei Wang",
      "Zhongyuan Wang",
      "Renrui Zhang",
      "Shanghang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Galappaththige_Multi-View_Pose-Agnostic_Change_Localization_with_Zero_Labels_CVPR_2025_paper.html": {
    "title": "Multi-View Pose-Agnostic Change Localization with Zero Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chamuditha Jayanga Galappaththige",
      "Jason Lai",
      "Lloyd Windrim",
      "Donald Dansereau",
      "Niko Sunderhauf",
      "Dimity Miller"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_From_Sparse_to_Dense_Camera_Relocalization_with_Scene-Specific_Detector_from_CVPR_2025_paper.html": {
    "title": "From Sparse to Dense: Camera Relocalization with Scene-Specific Detector from Feature Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwei Huang",
      "Hailin Yu",
      "Yichun Shentu",
      "Jin Yuan",
      "Guofeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Accelerating_Diffusion_Transformer_via_Increment-Calibrated_Caching_with_Channel-Aware_Singular_Value_CVPR_2025_paper.html": {
    "title": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Chen",
      "Keyi Li",
      "Yifan Jia",
      "Le Ye",
      "Yufei Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_CityWalker_Learning_Embodied_Urban_Navigation_from_Web-Scale_Videos_CVPR_2025_paper.html": {
    "title": "CityWalker: Learning Embodied Urban Navigation from Web-Scale Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhao Liu",
      "Jintong Li",
      "Yicheng Jiang",
      "Niranjan Sujay",
      "Zhicheng Yang",
      "Juexiao Zhang",
      "John Abanes",
      "Jing Zhang",
      "Chen Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_A_Simple_yet_Effective_Layout_Token_in_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "A Simple yet Effective Layout Token in Large Language Models for Document Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoqing Zhu",
      "Chuwei Luo",
      "Zirui Shao",
      "Feiyu Gao",
      "Hangdi Xing",
      "Qi Zheng",
      "Ji Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingfeng Yao",
      "Bin Yang",
      "Xinggang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tu_StableAnimator_High-Quality_Identity-Preserving_Human_Image_Animation_CVPR_2025_paper.html": {
    "title": "StableAnimator: High-Quality Identity-Preserving Human Image Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyuan Tu",
      "Zhen Xing",
      "Xintong Han",
      "Zhi-Qi Cheng",
      "Qi Dai",
      "Chong Luo",
      "Zuxuan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Stone_Learning_Visual_Composition_through_Improved_Semantic_Guidance_CVPR_2025_paper.html": {
    "title": "Learning Visual Composition through Improved Semantic Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Austin Stone",
      "Hagen Soltau",
      "Robert Geirhos",
      "Xi Yi",
      "Ye Xia",
      "Bingyi Cao",
      "Kaifeng Chen",
      "Abhijit Ogale",
      "Jonathon Shlens"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_OODD_Test-time_Out-of-Distribution_Detection_with_Dynamic_Dictionary_CVPR_2025_paper.html": {
    "title": "OODD: Test-time Out-of-Distribution Detection with Dynamic Dictionary",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifeng Yang",
      "Lin Zhu",
      "Zewen Sun",
      "Hengyu Liu",
      "Qinying Gu",
      "Nanyang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MEAT_Multiview_Diffusion_Model_for_Human_Generation_on_Megapixels_with_CVPR_2025_paper.html": {
    "title": "MEAT: Multiview Diffusion Model for Human Generation on Megapixels with Mesh Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Wang",
      "Fangzhou Hong",
      "Shuai Yang",
      "Liming Jiang",
      "Wayne Wu",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Meng_Free_Lunch_Enhancements_for_Multi-modal_Crowd_Counting_CVPR_2025_paper.html": {
    "title": "Free Lunch Enhancements for Multi-modal Crowd Counting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoliang Meng",
      "Xiaopeng Hong",
      "Zhengqin Lai",
      "Miao Shang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Islam_BIMBA_Selective-Scan_Compression_for_Long-Range_Video_Question_Answering_CVPR_2025_paper.html": {
    "title": "BIMBA: Selective-Scan Compression for Long-Range Video Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Mohaiminul Islam",
      "Tushar Nagarajan",
      "Huiyu Wang",
      "Gedas Bertasius",
      "Lorenzo Torresani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Miao_EVolSplat_Efficient_Volume-based_Gaussian_Splatting_for_Urban_View_Synthesis_CVPR_2025_paper.html": {
    "title": "EVolSplat: Efficient Volume-based Gaussian Splatting for Urban View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Miao",
      "Jiaxin Huang",
      "Dongfeng Bai",
      "Xu Yan",
      "Hongyu Zhou",
      "Yue Wang",
      "Bingbing Liu",
      "Andreas Geiger",
      "Yiyi Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Schusterbauer_Diff2Flow_Training_Flow_Matching_Models_via_Diffusion_Model_Alignment_CVPR_2025_paper.html": {
    "title": "Diff2Flow: Training Flow Matching Models via Diffusion Model Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johannes Schusterbauer",
      "Ming Gui",
      "Frank Fundel",
      "Björn Ommer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_JanusFlow_Harmonizing_Autoregression_and_Rectified_Flow_for_Unified_Multimodal_Understanding_CVPR_2025_paper.html": {
    "title": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Ma",
      "Xingchao Liu",
      "Xiaokang Chen",
      "Wen Liu",
      "Chengyue Wu",
      "Zhiyu Wu",
      "Zizheng Pan",
      "Zhenda Xie",
      "Haowei Zhang",
      "Xingkai Yu",
      "Liang Zhao",
      "Yisong Wang",
      "Jiaying Liu",
      "Chong Ruan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Visual_Prompting_for_One-shot_Controllable_Video_Editing_without_Inversion_CVPR_2025_paper.html": {
    "title": "Visual Prompting for One-shot Controllable Video Editing without Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengbo Zhang",
      "Yuxi Zhou",
      "Duo Peng",
      "Joo-Hwee Lim",
      "Zhigang Tu",
      "De Wen Soh",
      "Lin Geng Foo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_PIDSR_Complementary_Polarized_Image_Demosaicing_and_Super-Resolution_CVPR_2025_paper.html": {
    "title": "PIDSR: Complementary Polarized Image Demosaicing and Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuangfan Zhou",
      "Chu Zhou",
      "Youwei Lyu",
      "Heng Guo",
      "Zhanyu Ma",
      "Boxin Shi",
      "Imari Sato"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_MegaSynth_Scaling_Up_3D_Scene_Reconstruction_with_Synthesized_Data_CVPR_2025_paper.html": {
    "title": "MegaSynth: Scaling Up 3D Scene Reconstruction with Synthesized Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanwen Jiang",
      "Zexiang Xu",
      "Desai Xie",
      "Ziwen Chen",
      "Haian Jin",
      "Fujun Luan",
      "Zhixin Shu",
      "Kai Zhang",
      "Sai Bi",
      "Xin Sun",
      "Jiuxiang Gu",
      "Qixing Huang",
      "Georgios Pavlakos",
      "Hao Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ruan_Prof._Robot_Differentiable_Robot_Rendering_Without_Static_and_Self-Collisions_CVPR_2025_paper.html": {
    "title": "Prof. Robot: Differentiable Robot Rendering Without Static and Self-Collisions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quanyuan Ruan",
      "Jiabao Lei",
      "Wenhao Yuan",
      "Yanglin Zhang",
      "Dekun Lu",
      "Guiliang Liu",
      "Kui Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_AVQACL_A_Novel_Benchmark_for_Audio-Visual_Question_Answering_Continual_Learning_CVPR_2025_paper.html": {
    "title": "AVQACL: A Novel Benchmark for Audio-Visual Question Answering Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaixuan Wu",
      "Xinde Li",
      "Xinling Li",
      "Chuanfei Hu",
      "Guoliang Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Flash-Split_2D_Reflection_Removal_with_Flash_Cues_and_Latent_Diffusion_CVPR_2025_paper.html": {
    "title": "Flash-Split: 2D Reflection Removal with Flash Cues and Latent Diffusion Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianfu Wang",
      "Mingyang Xie",
      "Haoming Cai",
      "Sachin Shah",
      "Christopher A. Metzler"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Serianni_Attention_IoU_Examining_Biases_in_CelebA_using_Attention_Maps_CVPR_2025_paper.html": {
    "title": "Attention IoU: Examining Biases in CelebA using Attention Maps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aaron Serianni",
      "Tyler Zhu",
      "Olga Russakovsky",
      "Vikram V. Ramaswamy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_HEIE_MLLM-Based_Hierarchical_Explainable_AIGC_Image_Implausibility_Evaluator_CVPR_2025_paper.html": {
    "title": "HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility Evaluator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Yang",
      "Ru Zhen",
      "Jianing Wang",
      "Yanhao Zhang",
      "Haoxiang Chen",
      "Haonan Lu",
      "Sicheng Zhao",
      "Guiguang Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Segment_Any_Motion_in_Videos_CVPR_2025_paper.html": {
    "title": "Segment Any Motion in Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan Huang",
      "Wenzhao Zheng",
      "Chenfeng Xu",
      "Kurt Keutzer",
      "Shanghang Zhang",
      "Angjoo Kanazawa",
      "Qianqian Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_HandOS_3D_Hand_Reconstruction_in_One_Stage_CVPR_2025_paper.html": {
    "title": "HandOS: 3D Hand Reconstruction in One Stage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Chen",
      "Zhuheng Song",
      "Xiaoke Jiang",
      "Yaoqing Hu",
      "Junzhi Yu",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Task-aware_Cross-modal_Feature_Refinement_Transformer_with_Large_Language_Models_for_CVPR_2025_paper.html": {
    "title": "Task-aware Cross-modal Feature Refinement Transformer with Large Language Models for Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbo Chen",
      "Zhen Xu",
      "Ruotao Xu",
      "Si Wu",
      "Hau-San Wong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_DropGaussian_Structural_Regularization_for_Sparse-view_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "DropGaussian: Structural Regularization for Sparse-view Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunwoo Park",
      "Gun Ryu",
      "Wonjun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_All-Day_Multi-Camera_Multi-Target_Tracking_CVPR_2025_paper.html": {
    "title": "All-Day Multi-Camera Multi-Target Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huijie Fan",
      "Yu Qiao",
      "Yihao Zhen",
      "Tinghui Zhao",
      "Baojie Fan",
      "Qiang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with_CVPR_2025_paper.html": {
    "title": "Blurred LiDAR for Sharper 3D: Robust Handheld 3D Scanning with Diffuse LiDAR and RGB",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikhil Behari",
      "Aaron Young",
      "Siddharth Somasundaram",
      "Tzofi Klinghoffer",
      "Akshat Dave",
      "Ramesh Raskar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in_CVPR_2025_paper.html": {
    "title": "EnergyMoGen: Compositional Human Motion Generation with Energy-Based Diffusion Model in Latent Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianrong Zhang",
      "Hehe Fan",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jacob_PatchDEMUX_A_Certifiably_Robust_Framework_for_Multi-label_Classifiers_Against_Adversarial_CVPR_2025_paper.html": {
    "title": "PatchDEMUX: A Certifiably Robust Framework for Multi-label Classifiers Against Adversarial Patches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dennis Jacob",
      "Chong Xiang",
      "Prateek Mittal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rodriguez_StarVector_Generating_Scalable_Vector_Graphics_Code_from_Images_and_Text_CVPR_2025_paper.html": {
    "title": "StarVector: Generating Scalable Vector Graphics Code from Images and Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juan A. Rodriguez",
      "Abhay Puri",
      "Shubham Agarwal",
      "Issam H. Laradji",
      "Pau Rodriguez",
      "Sai Rajeswar",
      "David Vazquez",
      "Christopher Pal",
      "Marco Pedersoli"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Elata_Novel_View_Synthesis_with_Pixel-Space_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Novel View Synthesis with Pixel-Space Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noam Elata",
      "Bahjat Kawar",
      "Yaron Ostrovsky-Berman",
      "Miriam Farber",
      "Ron Sokolovsky"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Object_Detection_using_Event_Camera_A_MoE_Heat_Conduction_based_CVPR_2025_paper.html": {
    "title": "Object Detection using Event Camera: A MoE Heat Conduction based Detector and A New Benchmark Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Wang",
      "Yu Jin",
      "Wentao Wu",
      "Wei Zhang",
      "Lin Zhu",
      "Bo Jiang",
      "Yonghong Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_EgoTextVQA_Towards_Egocentric_Scene-Text_Aware_Video_Question_Answering_CVPR_2025_paper.html": {
    "title": "EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Zhou",
      "Junbin Xiao",
      "Qingyun Li",
      "Yicong Li",
      "Xun Yang",
      "Dan Guo",
      "Meng Wang",
      "Tat-Seng Chua",
      "Angela Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bergner_Token_Cropr_Faster_ViTs_for_Quite_a_Few_Tasks_CVPR_2025_paper.html": {
    "title": "Token Cropr: Faster ViTs for Quite a Few Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Bergner",
      "Christoph Lippert",
      "Aravindh Mahendran"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liao_STCOcc_Sparse_Spatial-Temporal_Cascade_Renovation_for_3D_Occupancy_and_Scene_CVPR_2025_paper.html": {
    "title": "STCOcc: Sparse Spatial-Temporal Cascade Renovation for 3D Occupancy and Scene Flow Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhimin Liao",
      "Ping Wei",
      "Shuaijia Chen",
      "Haoxuan Wang",
      "Ziyang Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Document_Haystacks__Vision-Language_Reasoning_Over_Piles_of_1000_Documents_CVPR_2025_paper.html": {
    "title": "Document Haystacks: Vision-Language Reasoning Over Piles of 1000+ Documents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Chen",
      "Dannong Xu",
      "Junjie Fei",
      "Chun-Mei Feng",
      "Mohamed Elhoseiny"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Farina_Rethinking_Few-Shot_Adaptation_of_Vision-Language_Models_in_Two_Stages_CVPR_2025_paper.html": {
    "title": "Rethinking Few-Shot Adaptation of Vision-Language Models in Two Stages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Farina",
      "Massimiliano Mancini",
      "Giovanni Iacca",
      "Elisa Ricci"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_Resilient_Sensor_Fusion_Under_Adverse_Sensor_Failures_via_Multi-Modal_Expert_CVPR_2025_paper.html": {
    "title": "Resilient Sensor Fusion Under Adverse Sensor Failures via Multi-Modal Expert Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konyul Park",
      "Yecheol Kim",
      "Daehun Kim",
      "Jun Won Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhai_TAGA_Self-supervised_Learning_for_Template-free_Animatable_Gaussian_Articulated_Model_CVPR_2025_paper.html": {
    "title": "TAGA: Self-supervised Learning for Template-free Animatable Gaussian Articulated Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhichao Zhai",
      "Guikun Chen",
      "Wenguan Wang",
      "Dong Zheng",
      "Jun Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MambaVO_Deep_Visual_Odometry_Based_on_Sequential_Matching_Refinement_and_CVPR_2025_paper.html": {
    "title": "MambaVO: Deep Visual Odometry Based on Sequential Matching Refinement and Training Smoothing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Wang",
      "Wanting Li",
      "Yongcai Wang",
      "Zhaoxin Fan",
      "Zhe Huang",
      "Xudong Cai",
      "Jian Zhao",
      "Deying Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kazimi_Explaining_in_Diffusion_Explaining_a_Classifier_with_Diffusion_Semantics_CVPR_2025_paper.html": {
    "title": "Explaining in Diffusion: Explaining a Classifier with Diffusion Semantics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tahira Kazimi",
      "Ritika Allada",
      "Pinar Yanardag"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Horizon-GS_Unified_3D_Gaussian_Splatting_for_Large-Scale_Aerial-to-Ground_Scenes_CVPR_2025_paper.html": {
    "title": "Horizon-GS: Unified 3D Gaussian Splatting for Large-Scale Aerial-to-Ground Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lihan Jiang",
      "Kerui Ren",
      "Mulin Yu",
      "Linning Xu",
      "Junting Dong",
      "Tao Lu",
      "Feng Zhao",
      "Dahua Lin",
      "Bo Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Attention_Distillation_A_Unified_Approach_to_Visual_Characteristics_Transfer_CVPR_2025_paper.html": {
    "title": "Attention Distillation: A Unified Approach to Visual Characteristics Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Zhou",
      "Xu Gao",
      "Zichong Chen",
      "Hui Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for_CVPR_2025_paper.html": {
    "title": "From Words to Structured Visuals: A Benchmark and Framework for Text-to-Diagram Generation and Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingxuan Wei",
      "Cheng Tan",
      "Qi Chen",
      "Gaowei Wu",
      "Siyuan Li",
      "Zhangyang Gao",
      "Linzhuang Sun",
      "Bihui Yu",
      "Ruifeng Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Matsui_LotusFilter_Fast_Diverse_Nearest_Neighbor_Search_via_a_Learned_Cutoff_CVPR_2025_paper.html": {
    "title": "LotusFilter: Fast Diverse Nearest Neighbor Search via a Learned Cutoff Table",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusuke Matsui"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_DreamRelation_Bridging_Customization_and_Relation_Generation_CVPR_2025_paper.html": {
    "title": "DreamRelation: Bridging Customization and Relation Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyu Shi",
      "Lu Qi",
      "Jianzong Wu",
      "Jinbin Bai",
      "Jingbo Wang",
      "Yunhai Tong",
      "Xiangtai Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ruan_IndoorGS_Geometric_Cues_Guided_Gaussian_Splatting_for_Indoor_Scene_Reconstruction_CVPR_2025_paper.html": {
    "title": "IndoorGS: Geometric Cues Guided Gaussian Splatting for Indoor Scene Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Ruan",
      "Yuesong Wang",
      "Tao Guan",
      "Bin Zhang",
      "Lili Ju"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Point-Cache_Test-time_Dynamic_and_Hierarchical_Cache_for_Robust_and_Generalizable_CVPR_2025_paper.html": {
    "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and Generalizable Point Cloud Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyu Sun",
      "Qiuhong Ke",
      "Ming Cheng",
      "Yongcai Wang",
      "Deying Li",
      "Chenhui Gou",
      "Jianfei Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yao_Think_Small_Act_Big_Primitive_Prompt_Learning_for_Lifelong_Robot_CVPR_2025_paper.html": {
    "title": "Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanqi Yao",
      "Siao Liu",
      "Haoming Song",
      "Delin Qu",
      "Qizhi Chen",
      "Yan Ding",
      "Bin Zhao",
      "Zhigang Wang",
      "Xuelong Li",
      "Dong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Doldo_Stop_Walking_in_Circles_Bailing_Out_Early_in_Projected_Gradient_CVPR_2025_paper.html": {
    "title": "Stop Walking in Circles! Bailing Out Early in Projected Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philip Doldo",
      "Derek Everett",
      "Amol Khanna",
      "Andre T Nguyen",
      "Edward Raff"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_MoManipVLA_Transferring_Vision-language-action_Models_for_General_Mobile_Manipulation_CVPR_2025_paper.html": {
    "title": "MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Wu",
      "Yuheng Zhou",
      "Xiuwei Xu",
      "Ziwei Wang",
      "Haibin Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable_CVPR_2025_paper.html": {
    "title": "SoMA: Singular Value Decomposed Minor Components Adaptation for Domain Generalizable Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seokju Yun",
      "Seunghye Chae",
      "Dongheon Lee",
      "Youngmin Ro"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_Depth-Guided_Bundle_Sampling_for_Efficient_Generalizable_Neural_Radiance_Field_Reconstruction_CVPR_2025_paper.html": {
    "title": "Depth-Guided Bundle Sampling for Efficient Generalizable Neural Radiance Field Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Fang",
      "Hao Zhu",
      "Longlong Chen",
      "Fei Hu",
      "Long Ye",
      "Zhan Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow_CVPR_2025_paper.html": {
    "title": "TinyFusion: Diffusion Transformers Learned Shallow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gongfan Fang",
      "Kunjun Li",
      "Xinyin Ma",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Ref-GS_Directional_Factorization_for_2D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "Ref-GS: Directional Factorization for 2D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youjia Zhang",
      "Anpei Chen",
      "Yumin Wan",
      "Zikai Song",
      "Junqing Yu",
      "Yawei Luo",
      "Wei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_SVG-IR_Spatially-Varying_Gaussian_Splatting_for_Inverse_Rendering_CVPR_2025_paper.html": {
    "title": "SVG-IR: Spatially-Varying Gaussian Splatting for Inverse Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanxiao Sun",
      "Yupeng Gao",
      "Jin Xie",
      "Jian Yang",
      "Beibei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Stable-SCore_A_Stable_Registration-based_Framework_for_3D_Shape_Correspondence_CVPR_2025_paper.html": {
    "title": "Stable-SCore: A Stable Registration-based Framework for 3D Shape Correspondence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haolin Liu",
      "Xiaohang Zhan",
      "Zizheng Yan",
      "Zhongjin Luo",
      "Yuxin Wen",
      "Xiaoguang Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mao_Beyond_Single-Modal_Boundary_Cross-Modal_Anomaly_Detection_through_Visual_Prototype_and_CVPR_2025_paper.html": {
    "title": "Beyond Single-Modal Boundary: Cross-Modal Anomaly Detection through Visual Prototype and Harmonization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Mao",
      "Ping Wei",
      "Yiyang Lian",
      "Yangyang Wang",
      "Nanning Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tanaka_VDocRAG_Retrieval-Augmented_Generation_over_Visually-Rich_Documents_CVPR_2025_paper.html": {
    "title": "VDocRAG: Retrieval-Augmented Generation over Visually-Rich Documents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryota Tanaka",
      "Taichi Iki",
      "Taku Hasegawa",
      "Kyosuke Nishida",
      "Kuniko Saito",
      "Jun Suzuki"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_Align-KD_Distilling_Cross-Modal_Alignment_Knowledge_for_Mobile_Vision-Language_Large_Model_CVPR_2025_paper.html": {
    "title": "Align-KD: Distilling Cross-Modal Alignment Knowledge for Mobile Vision-Language Large Model Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianhan Feng",
      "Wenshuo Li",
      "Tong Lin",
      "Xinghao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Subramanian_Pose_Priors_from_Language_Models_CVPR_2025_paper.html": {
    "title": "Pose Priors from Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanjay Subramanian",
      "Evonne Ng",
      "Lea Müller",
      "Dan Klein",
      "Shiry Ginosar",
      "Trevor Darrell"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_Concept_Lancet_Image_Editing_with_Compositional_Representation_Transplant_CVPR_2025_paper.html": {
    "title": "Concept Lancet: Image Editing with Compositional Representation Transplant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinqi Luo",
      "Tianjiao Ding",
      "Kwan Ho Ryan Chan",
      "Hancheng Min",
      "Chris Callison-Burch",
      "Rene Vidal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Weng_Scaling_Mesh_Generation_via_Compressive_Tokenization_CVPR_2025_paper.html": {
    "title": "Scaling Mesh Generation via Compressive Tokenization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haohan Weng",
      "Zibo Zhao",
      "Biwen Lei",
      "Xianghui Yang",
      "Jian Liu",
      "Zeqiang Lai",
      "Zhuo Chen",
      "Yuhong Liu",
      "Jie Jiang",
      "Chunchao Guo",
      "Tong Zhang",
      "Shenghua Gao",
      "C.L. Philip Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D_CVPR_2025_paper.html": {
    "title": "Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungtae Nam",
      "Xiangyu Sun",
      "Gyeongjin Kang",
      "Younggeun Lee",
      "Seungjun Oh",
      "Eunbyung Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_LogoSP_Local-global_Grouping_of_Superpoints_for_Unsupervised_Semantic_Segmentation_of_CVPR_2025_paper.html": {
    "title": "LogoSP: Local-global Grouping of Superpoints for Unsupervised Semantic Segmentation of 3D Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihui Zhang",
      "Weisheng Dai",
      "Hongtao Wen",
      "Bo Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_Exploring_Intrinsic_Normal_Prototypes_within_a_Single_Image_for_Universal_CVPR_2025_paper.html": {
    "title": "Exploring Intrinsic Normal Prototypes within a Single Image for Universal Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Luo",
      "Yunkang Cao",
      "Haiming Yao",
      "Xiaotian Zhang",
      "Jianan Lou",
      "Yuqi Cheng",
      "Weiming Shen",
      "Wenyong Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Augmenting_Perceptual_Super-Resolution_via_Image_Quality_Predictors_CVPR_2025_paper.html": {
    "title": "Augmenting Perceptual Super-Resolution via Image Quality Predictors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengjia Zhang",
      "Samrudhdhi B. Rangrej",
      "Tristan Aumentado-Armstrong",
      "Afsaneh Fazly",
      "Alex Levinshtein"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_TurboFill_Adapting_Few-step_Text-to-image_Model_for_Fast_Image_Inpainting_CVPR_2025_paper.html": {
    "title": "TurboFill: Adapting Few-step Text-to-image Model for Fast Image Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liangbin Xie",
      "Daniil Pakhomov",
      "Zhonghao Wang",
      "Zongze Wu",
      "Ziyan Chen",
      "Yuqian Zhou",
      "Haitian Zheng",
      "Zhifei Zhang",
      "Zhe Lin",
      "Jiantao Zhou",
      "Chao Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Stochastic_Human_Motion_Prediction_with_Memory_of_Action_Transition_and_CVPR_2025_paper.html": {
    "title": "Stochastic Human Motion Prediction with Memory of Action Transition and Action Characteristic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianwei Tang",
      "Hong Yang",
      "Tengyue Chen",
      "Jian-Fang Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in_CVPR_2025_paper.html": {
    "title": "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoyou Fu",
      "Yuhan Dai",
      "Yongdong Luo",
      "Lei Li",
      "Shuhuai Ren",
      "Renrui Zhang",
      "Zihan Wang",
      "Chenyu Zhou",
      "Yunhang Shen",
      "Mengdan Zhang",
      "Peixian Chen",
      "Yanwei Li",
      "Shaohui Lin",
      "Sirui Zhao",
      "Ke Li",
      "Tong Xu",
      "Xiawu Zheng",
      "Enhong Chen",
      "Caifeng Shan",
      "Ran He",
      "Xing Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bigverdi_Perception_Tokens_Enhance_Visual_Reasoning_in_Multimodal_Language_Models_CVPR_2025_paper.html": {
    "title": "Perception Tokens Enhance Visual Reasoning in Multimodal Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahtab Bigverdi",
      "Zelun Luo",
      "Cheng-Yu Hsieh",
      "Ethan Shen",
      "Dongping Chen",
      "Linda G. Shapiro",
      "Ranjay Krishna"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/You_Are_Images_Indistinguishable_to_Humans_Also_Indistinguishable_to_Classifiers_CVPR_2025_paper.html": {
    "title": "Are Images Indistinguishable to Humans Also Indistinguishable to Classifiers?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zebin You",
      "Xinyu Zhang",
      "Hanzhong Guo",
      "Jingdong Wang",
      "Chongxuan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation_CVPR_2025_paper.html": {
    "title": "X-Dyna: Expressive Dynamic Human Image Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Chang",
      "Hongyi Xu",
      "You Xie",
      "Yipeng Gao",
      "Zhengfei Kuang",
      "Shengqu Cai",
      "Chenxu Zhang",
      "Guoxian Song",
      "Chao Wang",
      "Yichun Shi",
      "Zeyuan Chen",
      "Shijie Zhou",
      "Linjie Luo",
      "Gordon Wetzstein",
      "Mohammad Soleymani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Levin_Understanding_Multi-layered_Transmission_Matrices_CVPR_2025_paper.html": {
    "title": "Understanding Multi-layered Transmission Matrices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anat Levin",
      "Marina Alterman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bian_GS-DiT_Advancing_Video_Generation_with_Dynamic_3D_Gaussian_Fields_through_CVPR_2025_paper.html": {
    "title": "GS-DiT: Advancing Video Generation with Dynamic 3D Gaussian Fields through Efficient Dense 3D Point Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weikang Bian",
      "Zhaoyang Huang",
      "Xiaoyu Shi",
      "Yijin Li",
      "Fu-Yun Wang",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yun_AnyMoLe_Any_Character_Motion_In-betweening_Leveraging_Video_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwan Yun",
      "Seokhyeon Hong",
      "Chaelin Kim",
      "Junyong Noh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kahl_Towards_Optimizing_Large-Scale_Multi-Graph_Matching_in_Bioimaging_CVPR_2025_paper.html": {
    "title": "Towards Optimizing Large-Scale Multi-Graph Matching in Bioimaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max Kahl",
      "Sebastian Stricker",
      "Lisa Hutschenreiter",
      "Florian Bernard",
      "Carsten Rother",
      "Bogdan Savchynskyy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lun_Towards_Effective_and_Sparse_Adversarial_Attack_on_Spiking_Neural_Networks_CVPR_2025_paper.html": {
    "title": "Towards Effective and Sparse Adversarial Attack on Spiking Neural Networks via Breaking Invisible Surrogate Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Lun",
      "Kunyu Feng",
      "Qinglong Ni",
      "Ling Liang",
      "Yuan Wang",
      "Ying Li",
      "Dunshan Yu",
      "Xiaoxin Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Franchi_Towards_Understanding_and_Quantifying_Uncertainty_for_Text-to-Image_Generation_CVPR_2025_paper.html": {
    "title": "Towards Understanding and Quantifying Uncertainty for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gianni Franchi",
      "Nacim Belkhir",
      "Dat Nguyen Trong",
      "Guoxuan Xia",
      "Andrea Pilzer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_PS-Diffusion_Photorealistic_Subject-Driven_Image_Editing_with_Disentangled_Control_and_Attention_CVPR_2025_paper.html": {
    "title": "PS-Diffusion: Photorealistic Subject-Driven Image Editing with Disentangled Control and Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weicheng Wang",
      "Guoli Jia",
      "Zhongqi Zhang",
      "Liang Lin",
      "Jufeng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hao_Exploring_Visual_Vulnerabilities_via_Multi-Loss_Adversarial_Search_for_Jailbreaking_Vision-Language_CVPR_2025_paper.html": {
    "title": "Exploring Visual Vulnerabilities via Multi-Loss Adversarial Search for Jailbreaking Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyang Hao",
      "Bryan Hooi",
      "Jun Liu",
      "Kai-Wei Chang",
      "Zi Huang",
      "Yujun Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_LLaVA-ST_A_Multimodal_Large_Language_Model_for_Fine-Grained_Spatial-Temporal_Understanding_CVPR_2025_paper.html": {
    "title": "LLaVA-ST: A Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyu Li",
      "Jinyu Chen",
      "Ziyu Wei",
      "Shaofei Huang",
      "Tianrui Hui",
      "Jialin Gao",
      "Xiaoming Wei",
      "Si Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Dong",
      "Zuyan Liu",
      "Hai-Long Sun",
      "Jingkang Yang",
      "Winston Hu",
      "Yongming Rao",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_MaIR_A_Locality-_and_Continuity-Preserving_Mamba_for_Image_Restoration_CVPR_2025_paper.html": {
    "title": "MaIR: A Locality- and Continuity-Preserving Mamba for Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyun Li",
      "Haiyu Zhao",
      "Wenxin Wang",
      "Peng Hu",
      "Yuanbiao Gou",
      "Xi Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Few-shot_Implicit_Function_Generation_via_Equivariance_CVPR_2025_paper.html": {
    "title": "Few-shot Implicit Function Generation via Equivariance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suizhi Huang",
      "Xingyi Yang",
      "Hongtao Lu",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_RSAR_Restricted_State_Angle_Resolver_and_Rotated_SAR_Benchmark_CVPR_2025_paper.html": {
    "title": "RSAR: Restricted State Angle Resolver and Rotated SAR Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Zhang",
      "Xue Yang",
      "Yuxuan Li",
      "Jian Yang",
      "Ming-Ming Cheng",
      "Xiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Dual_Energy-Based_Model_with_Open-World_Uncertainty_Estimation_for_Out-of-distribution_Detection_CVPR_2025_paper.html": {
    "title": "Dual Energy-Based Model with Open-World Uncertainty Estimation for Out-of-distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Chen",
      "Hu Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_DTGBrepGen_A_Novel_B-rep_Generative_Model_through_Decoupling_Topology_and_CVPR_2025_paper.html": {
    "title": "DTGBrepGen: A Novel B-rep Generative Model through Decoupling Topology and Geometry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Li",
      "Yihang Fu",
      "Falai Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Continuous_Space-Time_Video_Resampling_with__Invertible_Motion_Steganography_CVPR_2025_paper.html": {
    "title": "Continuous Space-Time Video Resampling with Invertible Motion Steganography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuantong Zhang",
      "Zhenzhong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_Schedule_On_the_Fly_Diffusion_Time_Prediction_for_Faster_and_CVPR_2025_paper.html": {
    "title": "Schedule On the Fly: Diffusion Time Prediction for Faster and Better Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zilyu Ye",
      "Zhiyang Chen",
      "Tiancheng Li",
      "Zemin Huang",
      "Weijian Luo",
      "Guo-Jun Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval_CVPR_2025_paper.html": {
    "title": "Learning Audio-guided Video Representation with Gated Attention for Video-Text Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boseung Jeong",
      "Jicheol Park",
      "Sungyeon Kim",
      "Suha Kwak"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rim_ProtoDepth_Unsupervised_Continual_Depth_Completion_with_Prototypes_CVPR_2025_paper.html": {
    "title": "ProtoDepth: Unsupervised Continual Depth Completion with Prototypes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Rim",
      "Hyoungseob Park",
      "S. Gangopadhyay",
      "Ziyao Zeng",
      "Younjoon Chung",
      "Alex Wong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wittmann_vesselFM_A_Foundation_Model_for_Universal_3D_Blood_Vessel_Segmentation_CVPR_2025_paper.html": {
    "title": "vesselFM: A Foundation Model for Universal 3D Blood Vessel Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bastian Wittmann",
      "Yannick Wattenberg",
      "Tamaz Amiranashvili",
      "Suprosanna Shit",
      "Bjoern Menze"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_TASTE-Rob_Advancing_Video_Generation_of_Task-Oriented_Hand-Object_Interaction_for_Generalizable_CVPR_2025_paper.html": {
    "title": "TASTE-Rob: Advancing Video Generation of Task-Oriented Hand-Object Interaction for Generalizable Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongxiang Zhao",
      "Xingchen Liu",
      "Mutian Xu",
      "Yiming Hao",
      "Weikai Chen",
      "Xiaoguang Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Khalil_NoT_Federated_Unlearning_via_Weight_Negation_CVPR_2025_paper.html": {
    "title": "NoT: Federated Unlearning via Weight Negation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasser H. Khalil",
      "Leo Brunswic",
      "Soufiane Lamghari",
      "Xu Li",
      "Mahdi Beitollahi",
      "Xi Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_ParaHome_Parameterizing_Everyday_Home_Activities_Towards_3D_Generative_Modeling_of_CVPR_2025_paper.html": {
    "title": "ParaHome: Parameterizing Everyday Home Activities Towards 3D Generative Modeling of Human-Object Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeonghwan Kim",
      "Jisoo Kim",
      "Jeonghyeon Na",
      "Hanbyul Joo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shaar_Adapting_to_the_Unknown_Training-Free_Audio-Visual_Event_Perception_with_Dynamic_CVPR_2025_paper.html": {
    "title": "Adapting to the Unknown: Training-Free Audio-Visual Event Perception with Dynamic Thresholds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eitan Shaar",
      "Ariel Shaulov",
      "Gal Chechik",
      "Lior Wolf"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation_CVPR_2025_paper.html": {
    "title": "OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing Human-Centric Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Li",
      "Mingwang Xu",
      "Yun Zhan",
      "Shan Mu",
      "Jiaye Li",
      "Kaihui Cheng",
      "Yuxuan Chen",
      "Tan Chen",
      "Mao Ye",
      "Jingdong Wang",
      "Siyu Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jain_Classifier-Free_Guidance_Inside_the_Attraction_Basin_May_Cause_Memorization_CVPR_2025_paper.html": {
    "title": "Classifier-Free Guidance Inside the Attraction Basin May Cause Memorization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anubhav Jain",
      "Yuya Kobayashi",
      "Takashi Shibuya",
      "Yuhta Takida",
      "Nasir Memon",
      "Julian Togelius",
      "Yuki Mitsufuji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Track_Any_Anomalous_ObjectA_Granular_Video_Anomaly_Detection_Pipeline_CVPR_2025_paper.html": {
    "title": "Track Any Anomalous Object:A Granular Video Anomaly Detection Pipeline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhi Huang",
      "Chenxin Li",
      "Haitao Zhang",
      "Zixu Lin",
      "Yunlong Lin",
      "Hengyu Liu",
      "Wuyang Li",
      "Xinyu Liu",
      "Jiechao Gao",
      "Yue Huang",
      "Xinghao Ding",
      "Yixuan Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dhakal_RANGE_Retrieval_Augmented_Neural_Fields_for_Multi-Resolution_Geo-Embeddings_CVPR_2025_paper.html": {
    "title": "RANGE: Retrieval Augmented Neural Fields for Multi-Resolution Geo-Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aayush Dhakal",
      "Srikumar Sastry",
      "Subash Khanal",
      "Adeel Ahmad",
      "Eric Xing",
      "Nathan Jacobs"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Magma_A_Foundation_Model_for_Multimodal_AI_Agents_CVPR_2025_paper.html": {
    "title": "Magma: A Foundation Model for Multimodal AI Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianwei Yang",
      "Reuben Tan",
      "Qianhui Wu",
      "Ruijie Zheng",
      "Baolin Peng",
      "Yongyuan Liang",
      "Yu Gu",
      "Mu Cai",
      "Seonghyeon Ye",
      "Joel Jang",
      "Yuquan Deng",
      "Jianfeng Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_SimMotionEdit_Text-Based_Human_Motion_Editing_with_Motion_Similarity_Prediction_CVPR_2025_paper.html": {
    "title": "SimMotionEdit: Text-Based Human Motion Editing with Motion Similarity Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyuan Li",
      "Kai Cheng",
      "Anindita Ghosh",
      "Uttaran Bhattacharya",
      "Liangyan Gui",
      "Aniket Bera"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Um_Object-aware_Sound_Source_Localization_via_Audio-Visual_Scene_Understanding_CVPR_2025_paper.html": {
    "title": "Object-aware Sound Source Localization via Audio-Visual Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sung Jin Um",
      "Dongjin Kim",
      "Sangmin Lee",
      "Jung Uk Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising_CVPR_2025_paper.html": {
    "title": "Volume Tells: Dual Cycle-Consistent Diffusion for 3D Fluorescence Microscopy De-noising and Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zelin Li",
      "Chenwei Wang",
      "Zhaoke Huang",
      "Yiming Ma",
      "Cunming Zhao",
      "Zhongying Zhao",
      "Hong Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_SerialGen_Personalized_Image_Generation_by_First_Standardization_Then_Personalization_CVPR_2025_paper.html": {
    "title": "SerialGen: Personalized Image Generation by First Standardization Then Personalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Xie",
      "Han Zou",
      "Ruiqi Yu",
      "Yan Zhang",
      "Zhenpeng Zhan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_From_Head_to_Tail_Efficient_Black-box_Model_Inversion_Attack_via_CVPR_2025_paper.html": {
    "title": "From Head to Tail: Efficient Black-box Model Inversion Attack via Long-tailed Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziang Li",
      "Hongguang Zhang",
      "Juan Wang",
      "Meihui Chen",
      "Hongxin Hu",
      "Wenzhe Yi",
      "Xiaoyang Xu",
      "Mengda Yang",
      "Chenjun Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding_CVPR_2025_paper.html": {
    "title": "Augmented Deep Contexts for Spatially Embedded Video Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Bian",
      "Chuanbo Tang",
      "Li Li",
      "Dong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_SIDA_Social_Media_Image_Deepfake_Detection_Localization_and_Explanation_with_CVPR_2025_paper.html": {
    "title": "SIDA: Social Media Image Deepfake Detection, Localization and Explanation with Large Multimodal Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenglin Huang",
      "Jinwei Hu",
      "Xiangtai Li",
      "Yiwei He",
      "Xingyu Zhao",
      "Bei Peng",
      "Baoyuan Wu",
      "Xiaowei Huang",
      "Guangliang Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One_CVPR_2025_paper.html": {
    "title": "Matrix3D: Large Photogrammetry Model All-in-One",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanxun Lu",
      "Jingyang Zhang",
      "Tian Fang",
      "Jean-Daniel Nahmias",
      "Yanghai Tsin",
      "Long Quan",
      "Xun Cao",
      "Yao Yao",
      "Shiwei Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Object-Centric_Prompt-Driven_Vision-Language-Action_Model_for_Robotic_Manipulation_CVPR_2025_paper.html": {
    "title": "Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoqi Li",
      "Jingyun Xu",
      "Mingxu Zhang",
      "Jiaming Liu",
      "Yan Shen",
      "Iaroslav Ponomarenko",
      "Jiahui Xu",
      "Liang Heng",
      "Siyuan Huang",
      "Shanghang Zhang",
      "Hao Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Proximal_Algorithm_Unrolling_Flexible_and_Efficient_Reconstruction_Networks_for_Single-Pixel_CVPR_2025_paper.html": {
    "title": "Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction Networks for Single-Pixel Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ping Wang",
      "Lishun Wang",
      "Gang Qu",
      "Xiaodong Wang",
      "Yulun Zhang",
      "Xin Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_3DEnhancer_Consistent_Multi-View_Diffusion_for_3D_Enhancement_CVPR_2025_paper.html": {
    "title": "3DEnhancer: Consistent Multi-View Diffusion for 3D Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihang Luo",
      "Shangchen Zhou",
      "Yushi Lan",
      "Xingang Pan",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Investigating_the_Role_of_Weight_Decay_in_Enhancing_Nonconvex_SGD_CVPR_2025_paper.html": {
    "title": "Investigating the Role of Weight Decay in Enhancing Nonconvex SGD",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Sun",
      "Yuhao Huang",
      "Li Shen",
      "Kele Xu",
      "Bao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Morin_MarkushGrapher_Joint_Visual_and_Textual_Recognition_of_Markush_Structures_CVPR_2025_paper.html": {
    "title": "MarkushGrapher: Joint Visual and Textual Recognition of Markush Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Morin",
      "Valery Weber",
      "Ahmed Nassar",
      "Gerhard Ingmar Meijer",
      "Luc Van Gool",
      "Yawei Li",
      "Peter Staar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Depth_Any_Camera_Zero-Shot_Metric_Depth_Estimation_from_Any_Camera_CVPR_2025_paper.html": {
    "title": "Depth Any Camera: Zero-Shot Metric Depth Estimation from Any Camera",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuliang Guo",
      "Sparsh Garg",
      "S. Mahdi H. Miangoleh",
      "Xinyu Huang",
      "Liu Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Image_Quality_Assessment_From_Human_to_Machine_Preference_CVPR_2025_paper.html": {
    "title": "Image Quality Assessment: From Human to Machine Preference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunyi Li",
      "Yuan Tian",
      "Xiaoyue Ling",
      "Zicheng Zhang",
      "Haodong Duan",
      "Haoning Wu",
      "Ziheng Jia",
      "Xiaohong Liu",
      "Xiongkuo Min",
      "Guo Lu",
      "Weisi Lin",
      "Guangtao Zhai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kara_ShotAdapter_Text-to-Multi-Shot_Video_Generation_with_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ozgur Kara",
      "Krishna Kumar Singh",
      "Feng Liu",
      "Duygu Ceylan",
      "James M. Rehg",
      "Tobias Hinz"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Roth_Context-Aware_Multimodal_Pretraining_CVPR_2025_paper.html": {
    "title": "Context-Aware Multimodal Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karsten Roth",
      "Zeynep Akata",
      "Dima Damen",
      "Ivana Balazevic",
      "Olivier J. Henaff"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Sound_Bridge_Associating_Egocentric_and_Exocentric_Videos_via_Audio_Cues_CVPR_2025_paper.html": {
    "title": "Sound Bridge: Associating Egocentric and Exocentric Videos via Audio Cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sihong Huang",
      "Jiaxin Wu",
      "Xiaoyong Wei",
      "Yi Cai",
      "Dongmei Jiang",
      "Yaowei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection_CVPR_2025_paper.html": {
    "title": "Detecting Backdoor Attacks in Federated Learning via Direction Alignment Inspection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Xu",
      "Zikai Zhang",
      "Rui Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ouyang_OmniDocBench_Benchmarking_Diverse_PDF_Document_Parsing_with_Comprehensive_Annotations_CVPR_2025_paper.html": {
    "title": "OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linke Ouyang",
      "Yuan Qu",
      "Hongbin Zhou",
      "Jiawei Zhu",
      "Rui Zhang",
      "Qunshu Lin",
      "Bin Wang",
      "Zhiyuan Zhao",
      "Man Jiang",
      "Xiaomeng Zhao",
      "Jin Shi",
      "Fan Wu",
      "Pei Chu",
      "Minghao Liu",
      "Zhenxiang Li",
      "Chao Xu",
      "Bo Zhang",
      "Botian Shi",
      "Zhongying Tu",
      "Conghui He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_LayoutVLM_Differentiable_Optimization_of_3D_Layout_via_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "LayoutVLM: Differentiable Optimization of 3D Layout via Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan-Yun Sun",
      "Weiyu Liu",
      "Siyi Gu",
      "Dylan Lim",
      "Goutam Bhat",
      "Federico Tombari",
      "Manling Li",
      "Nick Haber",
      "Jiajun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Point_Clouds_Meets_Physics_Dynamic_Acoustic_Field_Fitting_Network_for_CVPR_2025_paper.html": {
    "title": "Point Clouds Meets Physics: Dynamic Acoustic Field Fitting Network for Point Cloud Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changshuo Wang",
      "Shuting He",
      "Xiang Fang",
      "Jiawei Han",
      "Zhonghang Liu",
      "Xin Ning",
      "Weijun Li",
      "Prayag Tiwari"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Faster_Parameter-Efficient_Tuning_with_Token_Redundancy_Reduction_CVPR_2025_paper.html": {
    "title": "Faster Parameter-Efficient Tuning with Token Redundancy Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwonyoung Kim",
      "Jungin Park",
      "Jin Kim",
      "Hyeongjun Kwon",
      "Kwanghoon Sohn"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_BlockDance_Reuse_Structurally_Similar_Spatio-Temporal_Features_to_Accelerate_Diffusion_Transformers_CVPR_2025_paper.html": {
    "title": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to Accelerate Diffusion Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Zhang",
      "Tingwei Gao",
      "Jie Shao",
      "Zuxuan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_Panorama_Generation_From_NFoV_Image_Done_Right_CVPR_2025_paper.html": {
    "title": "Panorama Generation From NFoV Image Done Right",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dian Zheng",
      "Cheng Zhang",
      "Xiao-Ming Wu",
      "Cao Li",
      "Chengfei Lv",
      "Jian-Fang Hu",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Mamba-Adaptor_State_Space_Model_Adaptor_for_Visual_Recognition_CVPR_2025_paper.html": {
    "title": "Mamba-Adaptor: State Space Model Adaptor for Visual Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Xie",
      "Jiahao Nie",
      "Yujin Tang",
      "Wenkang Zhang",
      "Hongshen Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_Robust_Message_Embedding_via_Attention_Flow-Based_Steganography_CVPR_2025_paper.html": {
    "title": "Robust Message Embedding via Attention Flow-Based Steganography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huayuan Ye",
      "Shenzhuo Zhang",
      "Shiqi Jiang",
      "Jing Liao",
      "Shuhang Gu",
      "Dejun Zheng",
      "Changbo Wang",
      "Chenhui Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss_CVPR_2025_paper.html": {
    "title": "Task-driven Image Fusion with Learnable Fusion Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haowen Bai",
      "Jiangshe Zhang",
      "Zixiang Zhao",
      "Yichen Wu",
      "Lilun Deng",
      "Yukun Cui",
      "Tao Feng",
      "Shuang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mahmood_Compositional_Targeted_Multi-Label_Universal_Perturbations_CVPR_2025_paper.html": {
    "title": "Compositional Targeted Multi-Label Universal Perturbations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hassan Mahmood",
      "Ehsan Elhamifar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nafez_PatchGuard_Adversarially_Robust_Anomaly_Detection_and_Localization_through_Vision_Transformers_CVPR_2025_paper.html": {
    "title": "PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mojtaba Nafez",
      "Amirhossein Koochakian",
      "Arad Maleki",
      "Jafar Habibi",
      "Mohammad Hossein Rohban"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Sparse_Point_Cloud_Patches_Rendering_via_Splitting_2D_Gaussians_CVPR_2025_paper.html": {
    "title": "Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changfeng Ma",
      "Ran Bi",
      "Jie Guo",
      "Chongjun Wang",
      "Yanwen Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Distilling_Monocular_Foundation_Model_for_Fine-grained_Depth_Completion_CVPR_2025_paper.html": {
    "title": "Distilling Monocular Foundation Model for Fine-grained Depth Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingping Liang",
      "Yutao Hu",
      "Wenqi Shao",
      "Ying Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_LamRA_Large_Multimodal_Model_as_Your_Advanced_Retrieval_Assistant_CVPR_2025_paper.html": {
    "title": "LamRA: Large Multimodal Model as Your Advanced Retrieval Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yikun Liu",
      "Yajie Zhang",
      "Jiayin Cai",
      "Xiaolong Jiang",
      "Yao Hu",
      "Jiangchao Yao",
      "Yanfeng Wang",
      "Weidi Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Stier_AniGrad_Anisotropic_Gradient-Adaptive_Sampling_for_3D_Reconstruction_From_Monocular_Video_CVPR_2025_paper.html": {
    "title": "AniGrad: Anisotropic Gradient-Adaptive Sampling for 3D Reconstruction From Monocular Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noah Stier",
      "Alex Rich",
      "Pradeep Sen",
      "Tobias Höllerer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Neural_Video_Compression_with_Context_Modulation_CVPR_2025_paper.html": {
    "title": "Neural Video Compression with Context Modulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanbo Tang",
      "Zhuoyuan Li",
      "Yifan Bian",
      "Li Li",
      "Dong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Less_Attention_is_More_Prompt_Transformer_for_Generalized_Category_Discovery_CVPR_2025_paper.html": {
    "title": "Less Attention is More: Prompt Transformer for Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Zhang",
      "Baopeng Zhang",
      "Zhu Teng",
      "Wenxin Luo",
      "Junnan Zou",
      "Jianping Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hagenaars_On-Device_Self-Supervised_Learning_of_Low-Latency_Monocular_Depth_from_Only_Events_CVPR_2025_paper.html": {
    "title": "On-Device Self-Supervised Learning of Low-Latency Monocular Depth from Only Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jesse J. Hagenaars",
      "Yilun Wu",
      "Federico Paredes-Valles",
      "Stein Stroobants",
      "Guido C.H.E. de Croon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and_CVPR_2025_paper.html": {
    "title": "CoMM: A Coherent Interleaved Image-Text Dataset for Multimodal Understanding and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Chen",
      "Lin Li",
      "Yongqi Yang",
      "Bin Wen",
      "Fan Yang",
      "Tingting Gao",
      "Yu Wu",
      "Long Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with_CVPR_2025_paper.html": {
    "title": "MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruicheng Wang",
      "Sicheng Xu",
      "Cassie Dai",
      "Jianfeng Xiang",
      "Yu Deng",
      "Xin Tong",
      "Jiaolong Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_Beyond_Background_Shift_Rethinking_Instance_Replay_in_Continual_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Beyond Background Shift: Rethinking Instance Replay in Continual Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongmei Yin",
      "Tingliang Feng",
      "Fan Lyu",
      "Fanhua Shang",
      "Hongying Liu",
      "Wei Feng",
      "Liang Wan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ke_ScaleLSD_Scalable_Deep_Line_Segment_Detection_Streamlined_CVPR_2025_paper.html": {
    "title": "ScaleLSD: Scalable Deep Line Segment Detection Streamlined",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeran Ke",
      "Bin Tan",
      "Xianwei Zheng",
      "Yujun Shen",
      "Tianfu Wu",
      "Nan Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_AToM_Aligning_Text-to-Motion_Model_at_Event-Level_with_GPT-4Vision_Reward_CVPR_2025_paper.html": {
    "title": "AToM: Aligning Text-to-Motion Model at Event-Level with GPT-4Vision Reward",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Han",
      "Xiangzuo Wu",
      "Huan Liao",
      "Zunnan Xu",
      "Zhongyuan Hu",
      "Ronghui Li",
      "Yachao Zhang",
      "Xiu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation_CVPR_2025_paper.html": {
    "title": "Revisiting MAE Pre-training for 3D Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tassilo Wald",
      "Constantin Ulrich",
      "Stanislav Lukyanenko",
      "Andrei Goncharov",
      "Alberto Paderno",
      "Maximilian Miller",
      "Leander Maerkisch",
      "Paul Jaeger",
      "Klaus Maier-Hein"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Learning_with_Noisy_Triplet_Correspondence_for_Composed_Image_Retrieval_CVPR_2025_paper.html": {
    "title": "Learning with Noisy Triplet Correspondence for Composed Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuxian Li",
      "Changhao He",
      "Xiting Liu",
      "Joey Tianyi Zhou",
      "Xi Peng",
      "Peng Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_DoF-Gaussian_Controllable_Depth-of-Field_for_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "DoF-Gaussian: Controllable Depth-of-Field for 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liao Shen",
      "Tianqi Liu",
      "Huiqiang Sun",
      "Jiaqi Li",
      "Zhiguo Cao",
      "Wei Li",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Parallelized_Autoregressive_Visual_Generation_CVPR_2025_paper.html": {
    "title": "Parallelized Autoregressive Visual Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqing Wang",
      "Shuhuai Ren",
      "Zhijie Lin",
      "Yujin Han",
      "Haoyuan Guo",
      "Zhenheng Yang",
      "Difan Zou",
      "Jiashi Feng",
      "Xihui Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_CGMatch_A_Different_Perspective_of_Semi-supervised_Learning_CVPR_2025_paper.html": {
    "title": "CGMatch: A Different Perspective of Semi-supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Cheng",
      "Jueqing Lu",
      "Yuan Tian",
      "Haifeng Zhao",
      "Yi Chang",
      "Lan Du"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_ChatHuman_Chatting_about_3D_Humans_with_Tools_CVPR_2025_paper.html": {
    "title": "ChatHuman: Chatting about 3D Humans with Tools",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Lin",
      "Yao Feng",
      "Weiyang Liu",
      "Michael J. Black"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video_CVPR_2025_paper.html": {
    "title": "FIction: 4D Future Interaction Prediction from Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kumar Ashutosh",
      "Georgios Pavlakos",
      "Kristen Grauman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jia_D2iT_Dynamic_Diffusion_Transformer_for_Accurate_Image_Generation_CVPR_2025_paper.html": {
    "title": "D^2iT: Dynamic Diffusion Transformer for Accurate Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weinan Jia",
      "Mengqi Huang",
      "Nan Chen",
      "Lei Zhang",
      "Zhendong Mao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Scalable_Autoregressive_Monocular_Depth_Estimation_CVPR_2025_paper.html": {
    "title": "Scalable Autoregressive Monocular Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhong Wang",
      "Jian Liu",
      "Dongqi Tang",
      "Weiqiang Wang",
      "Wentong Li",
      "Danny Chen",
      "Jintai Chen",
      "Jian Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Reconciling_Stochastic_and_Deterministic_Strategies_for_Zero-shot_Image_Restoration_using_CVPR_2025_paper.html": {
    "title": "Reconciling Stochastic and Deterministic Strategies for Zero-shot Image Restoration using Diffusion Model in Dual",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chong Wang",
      "Lanqing Guo",
      "Zixuan Fu",
      "Siyuan Yang",
      "Hao Cheng",
      "Alex C. Kot",
      "Bihan Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hai_Hierarchical_Flow_Diffusion_for_Efficient_Frame_Interpolation_CVPR_2025_paper.html": {
    "title": "Hierarchical Flow Diffusion for Efficient Frame Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Hai",
      "Guo Wang",
      "Tan Su",
      "Wenjie Jiang",
      "Yinlin Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Caffagni_Recurrence-Enhanced_Vision-and-Language_Transformers_for_Robust_Multimodal_Document_Retrieval_CVPR_2025_paper.html": {
    "title": "Recurrence-Enhanced Vision-and-Language Transformers for Robust Multimodal Document Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davide Caffagni",
      "Sara Sarto",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Das_Camouflage_Anything_Learning_to_Hide_using_Controlled_Out-painting_and_Representation_CVPR_2025_paper.html": {
    "title": "Camouflage Anything: Learning to Hide using Controlled Out-painting and Representation Engineering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Biplab Das",
      "Viswanath Gopalakrishnan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_Test-Time_Fine-Tuning_of_Image_Compression_Models_for_Multi-Task_Adaptability_CVPR_2025_paper.html": {
    "title": "Test-Time Fine-Tuning of Image Compression Models for Multi-Task Adaptability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Unki Park",
      "Seongmoon Jeong",
      "Youngchan Jang",
      "Gyeong-Moon Park",
      "Jong Hwan Ko"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pan_BASKET_A_Large-Scale_Video_Dataset_for_Fine-Grained_Skill_Estimation_CVPR_2025_paper.html": {
    "title": "BASKET: A Large-Scale Video Dataset for Fine-Grained Skill Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulu Pan",
      "Ce Zhang",
      "Gedas Bertasius"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Meng_AniDoc_Animation_Creation_Made_Easier_CVPR_2025_paper.html": {
    "title": "AniDoc: Animation Creation Made Easier",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihao Meng",
      "Hao Ouyang",
      "Hanlin Wang",
      "Qiuyu Wang",
      "Wen Wang",
      "Ka Leong Cheng",
      "Zhiheng Liu",
      "Yujun Shen",
      "Huamin Qu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_DynPose_Largely_Improving_the_Efficiency_of_Human_Pose_Estimation_by_CVPR_2025_paper.html": {
    "title": "DynPose: Largely Improving the Efficiency of Human Pose Estimation by a Simple Dynamic Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yalong Xu",
      "Lin Zhao",
      "Chen Gong",
      "Guangyu Li",
      "Di Wang",
      "Nannan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yue_Arbitrary-steps_Image_Super-resolution_via_Diffusion_Inversion_CVPR_2025_paper.html": {
    "title": "Arbitrary-steps Image Super-resolution via Diffusion Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongsheng Yue",
      "Kang Liao",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Malic_LiSu_A_Dataset_and_Method_for_LiDAR_Surface_Normal_Estimation_CVPR_2025_paper.html": {
    "title": "LiSu: A Dataset and Method for LiDAR Surface Normal Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dušan Malić",
      "Christian Fruhwirth-Reisinger",
      "Samuel Schulter",
      "Horst Possegger"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nizamani_Dynamic_Neural_Surfaces_for_Elastic_4D_Shape_Representation_and_Analysis_CVPR_2025_paper.html": {
    "title": "Dynamic Neural Surfaces for Elastic 4D Shape Representation and Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Awais Nizamani",
      "Hamid Laga",
      "Guanjin Wang",
      "Farid Boussaid",
      "Mohammed Bennamoun",
      "Anuj Srivastava"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Spk2SRImgNet_Super-Resolve_Dynamic_Scene_from_Spike_Stream_via_Motion_Aligned_CVPR_2025_paper.html": {
    "title": "Spk2SRImgNet: Super-Resolve Dynamic Scene from Spike Stream via Motion Aligned Collaborative Filtering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanlin Wang",
      "Yiyang Zhang",
      "Ruiqin Xiong",
      "Jing Zhao",
      "Jian Zhang",
      "Xiaopeng Fan",
      "Tiejun Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xue_ComfyBench_Benchmarking_LLM-based_Agents_in_ComfyUI_for_Autonomously_Designing_Collaborative_CVPR_2025_paper.html": {
    "title": "ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyuan Xue",
      "Zeyu Lu",
      "Di Huang",
      "Zidong Wang",
      "Wanli Ouyang",
      "Lei Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Munasinghe_VideoGLaMM__A_Large_Multimodal_Model_for_Pixel-Level_Visual_Grounding_CVPR_2025_paper.html": {
    "title": "VideoGLaMM : A Large Multimodal Model for Pixel-Level Visual Grounding in Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shehan Munasinghe",
      "Hanan Gani",
      "Wenqi Zhu",
      "Jiale Cao",
      "Eric Xing",
      "Fahad Shahbaz Khan",
      "Salman Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Incomplete_Multi-View_Multi-label_Learning_via_Disentangled_Representation_and_Label_Semantic_CVPR_2025_paper.html": {
    "title": "Incomplete Multi-View Multi-label Learning via Disentangled Representation and Label Semantic Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Yan",
      "Jun Yin",
      "Jie Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_AutoURDF_Unsupervised_Robot_Modeling_from_Point_Cloud_Frames_Using_Cluster_CVPR_2025_paper.html": {
    "title": "AutoURDF: Unsupervised Robot Modeling from Point Cloud Frames Using Cluster Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiong Lin",
      "Lechen Zhang",
      "Kwansoo Lee",
      "Jialong Ning",
      "Judah Goldfeder",
      "Hod Lipson"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Iwase_ZeroGrasp_Zero-Shot_Shape_Reconstruction_Enabled_Robotic_Grasping_CVPR_2025_paper.html": {
    "title": "ZeroGrasp: Zero-Shot Shape Reconstruction Enabled Robotic Grasping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shun Iwase",
      "Muhammad Zubair Irshad",
      "Katherine Liu",
      "Vitor Guizilini",
      "Robert Lee",
      "Takuya Ikeda",
      "Ayako Amma",
      "Koichi Nishiwaki",
      "Kris Kitani",
      "Rares Ambrus",
      "Sergey Zakharov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Golden_Cudgel_Network_for_Real-Time_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Golden Cudgel Network for Real-Time Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoyu Yang",
      "Yuan Wang",
      "Daming Shi",
      "Yanzhong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic_CVPR_2025_paper.html": {
    "title": "PDFactor: Learning Tri-Perspective View Policy Diffusion Field for Multi-Task Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi Tian",
      "Le Wang",
      "Sanping Zhou",
      "Sen Wang",
      "Jiayi Li",
      "Haowen Sun",
      "Wei Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VideoTree_Adaptive_Tree-based_Video_Representation_for_LLM_Reasoning_on_Long_CVPR_2025_paper.html": {
    "title": "VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Wang",
      "Shoubin Yu",
      "Elias Stengel-Eskin",
      "Jaehong Yoon",
      "Feng Cheng",
      "Gedas Bertasius",
      "Mohit Bansal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rao_Multi-modal_Contrastive_Learning_with_Negative_Sampling_Calibration_for_Phenotypic_Drug_CVPR_2025_paper.html": {
    "title": "Multi-modal Contrastive Learning with Negative Sampling Calibration for Phenotypic Drug Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahua Rao",
      "Hanjing Lin",
      "Leyu Chen",
      "Jiancong Xie",
      "Shuangjia Zheng",
      "Yuedong Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sheng_R-TPT_Improving_Adversarial_Robustness_of_Vision-Language_Models_through_Test-Time_Prompt_CVPR_2025_paper.html": {
    "title": "R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lijun Sheng",
      "Jian Liang",
      "Zilei Wang",
      "Ran He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Distinguish_Then_Exploit_Source-free_Open_Set_Domain_Adaptation_via_Weight_CVPR_2025_paper.html": {
    "title": "Distinguish Then Exploit: Source-free Open Set Domain Adaptation via Weight Barcode Estimation and Sparse Label Assignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiming Liu",
      "Jun Dan",
      "Fan Wang",
      "Xinting Liao",
      "Junhao Dong",
      "Hua Yu",
      "Shunjie Dong",
      "Lianyong Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Multi-Sensor_Object_Anomaly_Detection_Unifying_Appearance_Geometry_and_Internal_Properties_CVPR_2025_paper.html": {
    "title": "Multi-Sensor Object Anomaly Detection: Unifying Appearance, Geometry, and Internal Properties",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenqiao Li",
      "Bozhong Zheng",
      "Xiaohao Xu",
      "Jinye Gan",
      "Fading Lu",
      "Xiang Li",
      "Na Ni",
      "Zheng Tian",
      "Xiaonan Huang",
      "Shenghua Gao",
      "Yingna Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Go_SplatFlow_Multi-View_Rectified_Flow_Model_for_3D_Gaussian_Splatting_Synthesis_CVPR_2025_paper.html": {
    "title": "SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian Splatting Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyojun Go",
      "Byeongjun Park",
      "Jiho Jang",
      "Jin-Young Kim",
      "Soonwoo Kwon",
      "Changick Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Fancy123_One_Image_to_High-Quality_3D_Mesh_Generation_via_Plug-and-Play_CVPR_2025_paper.html": {
    "title": "Fancy123: One Image to High-Quality 3D Mesh Generation via Plug-and-Play Deformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiao Yu",
      "Xianzhi Li",
      "Yuan Tang",
      "Xu Han",
      "Long Hu",
      "Yixue Hao",
      "Min Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shin_Dense_Dispersed_Structured_Light_for_Hyperspectral_3D_Imaging_of_Dynamic_CVPR_2025_paper.html": {
    "title": "Dense Dispersed Structured Light for Hyperspectral 3D Imaging of Dynamic Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suhyun Shin",
      "Seungwoo Yoon",
      "Ryota Maeda",
      "Seung-Hwan Baek"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes_CVPR_2025_paper.html": {
    "title": "PlanarSplatting: Accurate Planar Surface Reconstruction in 3 Minutes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Tan",
      "Rui Yu",
      "Yujun Shen",
      "Nan Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qian_Omni-ID_Holistic_Identity_Representation_Designed_for_Generative_Tasks_CVPR_2025_paper.html": {
    "title": "Omni-ID: Holistic Identity Representation Designed for Generative Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guocheng Qian",
      "Kuan-Chieh Wang",
      "Or Patashnik",
      "Negin Heravi",
      "Daniil Ostashev",
      "Sergey Tulyakov",
      "Daniel Cohen-Or",
      "Kfir Aberman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ozsoy_MM-OR_A_Large_Multimodal_Operating_Room_Dataset_for_Semantic_Understanding_CVPR_2025_paper.html": {
    "title": "MM-OR: A Large Multimodal Operating Room Dataset for Semantic Understanding of High-Intensity Surgical Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ege Özsoy",
      "Chantal Pellegrini",
      "Tobias Czempiel",
      "Felix Tristram",
      "Kun Yuan",
      "David Bani-Harouni",
      "Ulrich Eck",
      "Benjamin Busam",
      "Matthias Keicher",
      "Nassir Navab"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jayasundara_MIRE_Matched_Implicit_Neural_Representations_CVPR_2025_paper.html": {
    "title": "MIRE: Matched Implicit Neural Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dhananjaya Jayasundara",
      "Heng Zhao",
      "Demetrio Labate",
      "Vishal M. Patel"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Joo_AeSPa__Attention-guided_Self-supervised_Parallel_Imaging_for_MRI_Reconstruction_CVPR_2025_paper.html": {
    "title": "AeSPa : Attention-guided Self-supervised Parallel Imaging for MRI Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinho Joo",
      "Hyeseong Kim",
      "Hyeyeon Won",
      "Deukhee Lee",
      "Taejoon Eo",
      "Dosik Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Boltzmann_Attention_Sampling_for_Image_Analysis_with_Small_Objects_CVPR_2025_paper.html": {
    "title": "Boltzmann Attention Sampling for Image Analysis with Small Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Theodore Zhao",
      "Sid Kiblawi",
      "Naoto Usuyama",
      "Ho Hin Lee",
      "Sam Preston",
      "Hoifung Poon",
      "Mu Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Dora_Sampling_and_Benchmarking_for_3D_Shape_Variational_Auto-Encoders_CVPR_2025_paper.html": {
    "title": "Dora: Sampling and Benchmarking for 3D Shape Variational Auto-Encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Chen",
      "Jianfeng Zhang",
      "Yixun Liang",
      "Guan Luo",
      "Weiyu Li",
      "Jiarui Liu",
      "Xiu Li",
      "Xiaoxiao Long",
      "Jiashi Feng",
      "Ping Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Monroy_Generalized_Recorrupted-to-Recorrupted_Self-Supervised_Learning_Beyond_Gaussian_Noise_CVPR_2025_paper.html": {
    "title": "Generalized Recorrupted-to-Recorrupted: Self-Supervised Learning Beyond Gaussian Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brayan Monroy",
      "Jorge Bacca",
      "Julián Tachella"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Once-Tuning-Multiple-Variants_Tuning_Once_and_Expanded_as_Multiple_Vision-Language_Model_Variants_CVPR_2025_paper.html": {
    "title": "Once-Tuning-Multiple-Variants: Tuning Once and Expanded as Multiple Vision-Language Model Variants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chong Yu",
      "Tao Chen",
      "Zhongxue Gan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Dynamic_Motion_Blending_for_Versatile_Motion_Editing_CVPR_2025_paper.html": {
    "title": "Dynamic Motion Blending for Versatile Motion Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan Jiang",
      "Hongjie Li",
      "Ziye Yuan",
      "Zimo He",
      "Yixin Chen",
      "Tengyu Liu",
      "Yixin Zhu",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_StdGEN_Semantic-Decomposed_3D_Character_Generation_from_Single_Images_CVPR_2025_paper.html": {
    "title": "StdGEN: Semantic-Decomposed 3D Character Generation from Single Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuze He",
      "Yanning Zhou",
      "Wang Zhao",
      "Zhongkai Wu",
      "Kaiwen Xiao",
      "Wei Yang",
      "Yong-Jin Liu",
      "Xiao Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kulits_Reconstructing_Animals_and_the_Wild_CVPR_2025_paper.html": {
    "title": "Reconstructing Animals and the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Kulits",
      "Michael J. Black",
      "Silvia Zuffi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kha_RobSense_A_Robust_Multi-modal_Foundation_Model_for_Remote_Sensing_with_CVPR_2025_paper.html": {
    "title": "RobSense: A Robust Multi-modal Foundation Model for Remote Sensing with Static, Temporal, and Incomplete Data Adaptability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minh Kha Do",
      "Kang Han",
      "Phu Lai",
      "Khoa T. Phan",
      "Wei Xiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Spatiotemporal_Decoupling_for_Efficient_Vision-Based_Occupancy_Forecasting_CVPR_2025_paper.html": {
    "title": "Spatiotemporal Decoupling for Efficient Vision-Based Occupancy Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi Xu",
      "Xieyuanli Chen",
      "Junyi Ma",
      "Jiawei Huang",
      "Jintao Xu",
      "Yue Wang",
      "Ling Pei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving_CVPR_2025_paper.html": {
    "title": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bencheng Liao",
      "Shaoyu Chen",
      "Haoran Yin",
      "Bo Jiang",
      "Cheng Wang",
      "Sixu Yan",
      "Xinbang Zhang",
      "Xiangyu Li",
      "Ying Zhang",
      "Qian Zhang",
      "Xinggang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_MAC-Ego3D_Multi-Agent_Gaussian_Consensus_for_Real-Time_Collaborative_Ego-Motion_and_Photorealistic_CVPR_2025_paper.html": {
    "title": "MAC-Ego3D: Multi-Agent Gaussian Consensus for Real-Time Collaborative Ego-Motion and Photorealistic 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohao Xu",
      "Feng Xue",
      "Shibo Zhao",
      "Yike Pan",
      "Sebastian Scherer",
      "Xiaonan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_FFR_Frequency_Feature_Rectification_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "FFR: Frequency Feature Rectification for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqian Yang",
      "Xinqiao Zhao",
      "Xiaolei Wang",
      "Quan Zhang",
      "Jimin Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_DVHGNN_Multi-Scale_Dilated_Vision_HGNN_for_Efficient_Vision_Recognition_CVPR_2025_paper.html": {
    "title": "DVHGNN: Multi-Scale Dilated Vision HGNN for Efficient Vision Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caoshuo Li",
      "Tanzhe Li",
      "Xiaobin Hu",
      "Donghao Luo",
      "Taisong Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding_CVPR_2025_paper.html": {
    "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Shu",
      "Zheng Liu",
      "Peitian Zhang",
      "Minghao Qin",
      "Junjie Zhou",
      "Zhengyang Liang",
      "Tiejun Huang",
      "Bo Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wen_Reconstructing_In-the-Wild_Open-Vocabulary_Human-Object_Interactions_CVPR_2025_paper.html": {
    "title": "Reconstructing In-the-Wild Open-Vocabulary Human-Object Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boran Wen",
      "Dingbang Huang",
      "Zichen Zhang",
      "Jiahong Zhou",
      "Jianbin Deng",
      "Jingyu Gong",
      "Yulong Chen",
      "Lizhuang Ma",
      "Yong-Lu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill_CVPR_2025_paper.html": {
    "title": "GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jieming Cui",
      "Tengyu Liu",
      "Ziyu Meng",
      "Jiale Yu",
      "Ran Song",
      "Wei Zhang",
      "Yixin Zhu",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations_CVPR_2025_paper.html": {
    "title": "Sonata: Self-Supervised Learning of Reliable Point Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyang Wu",
      "Daniel DeTone",
      "Duncan Frost",
      "Tianwei Shen",
      "Chris Xie",
      "Nan Yang",
      "Jakob Engel",
      "Richard Newcombe",
      "Hengshuang Zhao",
      "Julian Straub"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_DriveGEN_Generalized_and_Robust_3D_Detection_in_Driving_via_Controllable_CVPR_2025_paper.html": {
    "title": "DriveGEN: Generalized and Robust 3D Detection in Driving via Controllable Text-to-Image Diffusion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongbin Lin",
      "Zilu Guo",
      "Yifan Zhang",
      "Shuaicheng Niu",
      "Yafeng Li",
      "Ruimao Zhang",
      "Shuguang Cui",
      "Zhen Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_GauSTAR_Gaussian_Surface_Tracking_and_Reconstruction_CVPR_2025_paper.html": {
    "title": "GauSTAR: Gaussian Surface Tracking and Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengwei Zheng",
      "Lixin Xue",
      "Juan Zarate",
      "Jie Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Training-free_Dense-Aligned_Diffusion_Guidance_for_Modular_Conditional_Image_Synthesis_CVPR_2025_paper.html": {
    "title": "Training-free Dense-Aligned Diffusion Guidance for Modular Conditional Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Wang",
      "Duo Peng",
      "Feng Chen",
      "Yuwei Yang",
      "Yinjie Lei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_DRiVE_Diffusion-based_Rigging_Empowers_Generation_of_Versatile_and_Expressive_Characters_CVPR_2025_paper.html": {
    "title": "DRiVE: Diffusion-based Rigging Empowers Generation of Versatile and Expressive Characters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingze Sun",
      "Junhao Chen",
      "Junting Dong",
      "Yurun Chen",
      "Xinyu Jiang",
      "Shiwei Mao",
      "Puhua Jiang",
      "Jingbo Wang",
      "Bo Dai",
      "Ruqi Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Online_Video_Understanding_OVBench_and_VideoChat-Online_CVPR_2025_paper.html": {
    "title": "Online Video Understanding: OVBench and VideoChat-Online",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenpeng Huang",
      "Xinhao Li",
      "Jiaqi Li",
      "Jing Wang",
      "Xiangyu Zeng",
      "Cheng Liang",
      "Tao Wu",
      "Xi Chen",
      "Liang Li",
      "Limin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Baek_TADFormer_Task-Adaptive_Dynamic_TransFormer_for_Efficient_Multi-Task_Learning_CVPR_2025_paper.html": {
    "title": "TADFormer: Task-Adaptive Dynamic TransFormer for Efficient Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungmin Baek",
      "Soyul Lee",
      "Hayeon Jo",
      "Hyesong Choi",
      "Dongbo Min"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D_CVPR_2025_paper.html": {
    "title": "A Unified Approach to Interpreting Self-supervised Pre-training Methods for 3D Point Clouds via Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiang Li",
      "Jian Ruan",
      "Fanghao Wu",
      "Yuchi Chen",
      "Zhihua Wei",
      "Wen Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised_CVPR_2025_paper.html": {
    "title": "Enhancing SAM with Efficient Prompting and Preference Optimization for Semi-supervised Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aishik Konwer",
      "Zhijian Yang",
      "Erhan Bas",
      "Cao Xiao",
      "Prateek Prasanna",
      "Parminder Bhatia",
      "Taha Kass-Hout"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_CamPoint_Boosting_Point_Cloud_Segmentation_with_Virtual_Camera_CVPR_2025_paper.html": {
    "title": "CamPoint: Boosting Point Cloud Segmentation with Virtual Camera",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianhui Zhang",
      "Yizhi Luo",
      "Zicheng Zhang",
      "Xuecheng Nie",
      "Bonan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_LightLoc_Learning_Outdoor_LiDAR_Localization_at_Light_Speed_CVPR_2025_paper.html": {
    "title": "LightLoc: Learning Outdoor LiDAR Localization at Light Speed",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen Li",
      "Chen Liu",
      "Shangshu Yu",
      "Dunqiang Liu",
      "Yin Zhou",
      "Siqi Shen",
      "Chenglu Wen",
      "Cheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ganguly_MERGE_Multi-faceted_Hierarchical_Graph-based_GNN_for_Gene_Expression_Prediction_from_CVPR_2025_paper.html": {
    "title": "MERGE: Multi-faceted Hierarchical Graph-based GNN for Gene Expression Prediction from Whole Slide Histopathology Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aniruddha Ganguly",
      "Debolina Chatterjee",
      "Wentao Huang",
      "Jie Zhang",
      "Alisa Yurovsky",
      "Travis Steele Johnson",
      "Chao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chetan_Accurate_Differential_Operators_for_Hybrid_Neural_Fields_CVPR_2025_paper.html": {
    "title": "Accurate Differential Operators for Hybrid Neural Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Chetan",
      "Guandao Yang",
      "Zichen Wang",
      "Steve Marschner",
      "Bharath Hariharan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_FeedEdit_Text-Based_Image_Editing_with_Dynamic_Feedback_Regulation_CVPR_2025_paper.html": {
    "title": "FeedEdit: Text-Based Image Editing with Dynamic Feedback Regulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengyi Fu",
      "Lei Zhang",
      "Mengqi Huang",
      "Zhendong Mao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Classifier-guided_CLIP_Distillation_for_Unsupervised_Multi-label_Classification_CVPR_2025_paper.html": {
    "title": "Classifier-guided CLIP Distillation for Unsupervised Multi-label Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongseob Kim",
      "Hyunjung Shim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units_CVPR_2025_paper.html": {
    "title": "UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huakun Liu",
      "Hiroki Ota",
      "Xin Wei",
      "Yutaro Hirao",
      "Monica Perusquia-Hernandez",
      "Hideaki Uchiyama",
      "Kiyoshi Kiyokawa"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from_CVPR_2025_paper.html": {
    "title": "STEREO: A Two-Stage Framework for Adversarially Robust Concept Erasing from Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Koushik Srivatsan",
      "Fahad Shamshad",
      "Muzammal Naseer",
      "Vishal M. Patel",
      "Karthik Nandakumar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_Scene_Map-based_Prompt_Tuning_for_Navigation_Instruction_Generation_CVPR_2025_paper.html": {
    "title": "Scene Map-based Prompt Tuning for Navigation Instruction Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Fan",
      "Rui Liu",
      "Wenguan Wang",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image_CVPR_2025_paper.html": {
    "title": "GenVDM: Generating Vector Displacement Maps From a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuezhi Yang",
      "Qimin Chen",
      "Vladimir G. Kim",
      "Siddhartha Chaudhuri",
      "Qixing Huang",
      "Zhiqin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_DropoutGS_Dropping_Out_Gaussians_for_Better_Sparse-view_Rendering_CVPR_2025_paper.html": {
    "title": "DropoutGS: Dropping Out Gaussians for Better Sparse-view Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yexing Xu",
      "Longguang Wang",
      "Minglin Chen",
      "Sheng Ao",
      "Li Li",
      "Yulan Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Effective SAM Combination for Open-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minhyeok Lee",
      "Suhwan Cho",
      "Jungho Lee",
      "Sunghun Yang",
      "Heeseung Choi",
      "Ig-Jae Kim",
      "Sangyoun Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Bridging_Modalities_Improving_Universal_Multimodal_Retrieval_by_Multimodal_Large_Language_CVPR_2025_paper.html": {
    "title": "Bridging Modalities: Improving Universal Multimodal Retrieval by Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Zhang",
      "Yanzhao Zhang",
      "Wen Xie",
      "Mingxin Li",
      "Ziqi Dai",
      "Dingkun Long",
      "Pengjun Xie",
      "Meishan Zhang",
      "Wenjie Li",
      "Min Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tran_Enhancing_Dataset_Distillation_via_Non-Critical_Region_Refinement_CVPR_2025_paper.html": {
    "title": "Enhancing Dataset Distillation via Non-Critical Region Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minh-Tuan Tran",
      "Trung Le",
      "Xuan-May Le",
      "Thanh-Toan Do",
      "Dinh Phung"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Towards_Visual_Discrimination_and_Reasoning_of_Real-World_Physical_Dynamics_Physics-Grounded_CVPR_2025_paper.html": {
    "title": "Towards Visual Discrimination and Reasoning of Real-World Physical Dynamics: Physics-Grounded Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenqiao Li",
      "Yao Gu",
      "Xintao Chen",
      "Xiaohao Xu",
      "Ming Hu",
      "Xiaonan Huang",
      "Yingna Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_SPA-VL_A_Comprehensive_Safety_Preference_Alignment_Dataset_for_Vision_Language_CVPR_2025_paper.html": {
    "title": "SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongting Zhang",
      "Lu Chen",
      "Guodong Zheng",
      "Yifeng Gao",
      "Rui Zheng",
      "Jinlan Fu",
      "Zhenfei Yin",
      "Senjie Jin",
      "Yu Qiao",
      "Xuanjing Huang",
      "Feng Zhao",
      "Tao Gui",
      "Jing Shao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hanson_PUP_3D-GS_Principled_Uncertainty_Pruning_for_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Hanson",
      "Allen Tu",
      "Vasu Singla",
      "Mayuka Jayawardhana",
      "Matthias Zwicker",
      "Tom Goldstein"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer_CVPR_2025_paper.html": {
    "title": "UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Lin",
      "Ke Wu",
      "Jie Li",
      "Jun Li",
      "Wu-Jun Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_PTDiffusion_Free_Lunch_for_Generating_Optical_Illusion_Hidden_Pictures_with_CVPR_2025_paper.html": {
    "title": "PTDiffusion: Free Lunch for Generating Optical Illusion Hidden Pictures with Phase-Transferred Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Gao",
      "Shuai Yang",
      "Jiaying Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Choi_ScribbleLight_Single_Image_Indoor_Relighting_with_Scribbles_CVPR_2025_paper.html": {
    "title": "ScribbleLight: Single Image Indoor Relighting with Scribbles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Myeong Choi",
      "Annie Wang",
      "Pieter Peers",
      "Anand Bhattad",
      "Roni Sengupta"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Vuong_Preserving_Clusters_in_Prompt_Learning_for_Unsupervised_Domain_Adaptation_CVPR_2025_paper.html": {
    "title": "Preserving Clusters in Prompt Learning for Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tung-Long Vuong",
      "Hoang Phan",
      "Vy Vo",
      "Anh Bui",
      "Thanh-Toan Do",
      "Trung Le",
      "Dinh Phung"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_InsightEdit_Towards_Better_Instruction_Following_for_Image_Editing_CVPR_2025_paper.html": {
    "title": "InsightEdit: Towards Better Instruction Following for Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingjing Xu",
      "Jie Kong",
      "Jiazhi Wang",
      "Xiao Pan",
      "Bo Lin",
      "Qiang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_Attend_to_Not_Attended_Structure-then-Detail_Token_Merging_for_Post-training_DiT_CVPR_2025_paper.html": {
    "title": "Attend to Not Attended: Structure-then-Detail Token Merging for Post-training DiT Acceleration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haipeng Fang",
      "Sheng Tang",
      "Juan Cao",
      "Enshuo Zhang",
      "Fan Tang",
      "Tong-Yee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_Turbo3D_Ultra-fast_Text-to-3D_Generation_CVPR_2025_paper.html": {
    "title": "Turbo3D: Ultra-fast Text-to-3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanzhe Hu",
      "Tianwei Yin",
      "Fujun Luan",
      "Yiwei Hu",
      "Hao Tan",
      "Zexiang Xu",
      "Sai Bi",
      "Shubham Tulsiani",
      "Kai Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_SUM_Parts_Benchmarking_Part-Level_Semantic_Segmentation_of_Urban_Meshes_CVPR_2025_paper.html": {
    "title": "SUM Parts: Benchmarking Part-Level Semantic Segmentation of Urban Meshes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weixiao Gao",
      "Liangliang Nan",
      "Hugo Ledoux"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_One-for-More_Continual_Diffusion_Model_for_Anomaly_Detection_CVPR_2025_paper.html": {
    "title": "One-for-More: Continual Diffusion Model for Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofan Li",
      "Xin Tan",
      "Zhuo Chen",
      "Zhizhong Zhang",
      "Ruixin Zhang",
      "Rizen Guo",
      "Guanna Jiang",
      "Yulong Chen",
      "Yanyun Qu",
      "Lizhuang Ma",
      "Yuan Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_MODA_Motion-Drift_Augmentation_for_Inertial_Human_Motion_Analysis_CVPR_2025_paper.html": {
    "title": "MODA: Motion-Drift Augmentation for Inertial Human Motion Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinghao Wu",
      "Shihui Guo",
      "Yipeng Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Roetzer_Higher-Order_Ratio_Cycles_for_Fast_and_Globally_Optimal_Shape_Matching_CVPR_2025_paper.html": {
    "title": "Higher-Order Ratio Cycles for Fast and Globally Optimal Shape Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Roetzer",
      "Viktoria Ehm",
      "Daniel Cremers",
      "Zorah Lähner",
      "Florian Bernard"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Heo_Omni-RGPT_Unifying_Image_and_Video_Region-level_Understanding_via_Token_Marks_CVPR_2025_paper.html": {
    "title": "Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miran Heo",
      "Min-Hung Chen",
      "De-An Huang",
      "Sifei Liu",
      "Subhashree Radhakrishnan",
      "Seon Joo Kim",
      "Yu-Chiang Frank Wang",
      "Ryo Hachiuma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Hyperdimensional_Uncertainty_Quantification_for_Multimodal_Uncertainty_Fusion_in_Autonomous_Vehicles_CVPR_2025_paper.html": {
    "title": "Hyperdimensional Uncertainty Quantification for Multimodal Uncertainty Fusion in Autonomous Vehicles Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luke Chen",
      "Junyao Wang",
      "Trier Mortlock",
      "Pramod Khargonekar",
      "Mohammad Abdullah Al Faruque"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jung_EDM_Equirectangular_Projection-Oriented_Dense_Kernelized_Feature_Matching_CVPR_2025_paper.html": {
    "title": "EDM: Equirectangular Projection-Oriented Dense Kernelized Feature Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongki Jung",
      "Jaehoon Choi",
      "Yonghan Lee",
      "Somi Jeong",
      "Taejae Lee",
      "Dinesh Manocha",
      "Suyong Yeon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_EZSR_Event-based_Zero-Shot_Recognition_CVPR_2025_paper.html": {
    "title": "EZSR: Event-based Zero-Shot Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Yang",
      "Liyuan Pan",
      "Dongxu Li",
      "Liu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_FlowRAM_Grounding_Flow_Matching_Policy_with_Region-Aware_Mamba_Framework_for_CVPR_2025_paper.html": {
    "title": "FlowRAM: Grounding Flow Matching Policy with Region-Aware Mamba Framework for Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sen Wang",
      "Le Wang",
      "Sanping Zhou",
      "Jingyi Tian",
      "Jiayi Li",
      "Haowen Sun",
      "Wei Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Visual_Lexicon_Rich_Image_Features_in_Language_Space_CVPR_2025_paper.html": {
    "title": "Visual Lexicon: Rich Image Features in Language Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "XuDong Wang",
      "Xingyi Zhou",
      "Alireza Fathi",
      "Trevor Darrell",
      "Cordelia Schmid"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SVFR_A_Unified_Framework_for_Generalized_Video_Face_Restoration_CVPR_2025_paper.html": {
    "title": "SVFR: A Unified Framework for Generalized Video Face Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyao Wang",
      "Xu Chen",
      "Chengming Xu",
      "Junwei Zhu",
      "Xiaobin Hu",
      "Jiangning Zhang",
      "Chengjie Wang",
      "Yuqi Liu",
      "Yiyi Zhou",
      "Rongrong Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_Decoupling_Fine_Detail_and_Global_Geometry_for_Compressed_Depth_Map_CVPR_2025_paper.html": {
    "title": "Decoupling Fine Detail and Global Geometry for Compressed Depth Map Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huan Zheng",
      "Wencheng Han",
      "Jianbing Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Test-Time_Visual_In-Context_Tuning_CVPR_2025_paper.html": {
    "title": "Test-Time Visual In-Context Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Xie",
      "Alessio Tonioni",
      "Nathalie Rauschmayr",
      "Federico Tombari",
      "Bernt Schiele"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_Prior_Does_Matter_Visual_Navigation_via_Denoising_Diffusion_Bridge_Models_CVPR_2025_paper.html": {
    "title": "Prior Does Matter: Visual Navigation via Denoising Diffusion Bridge Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Ren",
      "Yiming Zeng",
      "Zetong Bi",
      "Zhaoliang Wan",
      "Junlong Huang",
      "Hui Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin_CVPR_2025_paper.html": {
    "title": "Digital Twin Catalog: A Large-Scale Photorealistic 3D Object Digital Twin Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhao Dong",
      "Ka Chen",
      "Zhaoyang Lv",
      "Hong-Xing Yu",
      "Yunzhi Zhang",
      "Cheng Zhang",
      "Yufeng Zhu",
      "Stephen Tian",
      "Zhengqin Li",
      "Geordie Moffatt",
      "Sean Christofferson",
      "James Fort",
      "Xiaqing Pan",
      "Mingfei Yan",
      "Jiajun Wu",
      "Carl Yuheng Ren",
      "Richard Newcombe"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images_CVPR_2025_paper.html": {
    "title": "SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiyu Li",
      "Ruixun Liu",
      "Xiangyong Cao",
      "Xueru Bai",
      "Feng Zhou",
      "Deyu Meng",
      "Zhi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative_CVPR_2025_paper.html": {
    "title": "MeshGen: Generating PBR Textured Mesh with Render-Enhanced Auto-Encoder and Generative Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zilong Chen",
      "Yikai Wang",
      "Wenqiang Sun",
      "Feng Wang",
      "Yiwen Chen",
      "Huaping Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_GIFStream_4D_Gaussian-based_Immersive_Video_with_Feature_Stream_CVPR_2025_paper.html": {
    "title": "GIFStream: 4D Gaussian-based Immersive Video with Feature Stream",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Li",
      "Sicheng Li",
      "Xiang Gao",
      "Abudouaihati Batuer",
      "Lu Yu",
      "Yiyi Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nam_DeClotH_Decomposable_3D_Cloth_and_Human_Body_Reconstruction_from_a_CVPR_2025_paper.html": {
    "title": "DeClotH: Decomposable 3D Cloth and Human Body Reconstruction from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeongjin Nam",
      "Donghwan Kim",
      "Jeongtaek Oh",
      "Kyoung Mu Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Neuron_Learning_Context-Aware_Evolving_Representations_for_Zero-Shot_Skeleton_Action_Recognition_CVPR_2025_paper.html": {
    "title": "Neuron: Learning Context-Aware Evolving Representations for Zero-Shot Skeleton Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Chen",
      "Jingcai Guo",
      "Song Guo",
      "Dacheng Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Abdelsamad_Multi-Scale_Neighborhood_Occupancy_Masked_Autoencoder_for_Self-Supervised_Learning_in_LiDAR_CVPR_2025_paper.html": {
    "title": "Multi-Scale Neighborhood Occupancy Masked Autoencoder for Self-Supervised Learning in LiDAR Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Abdelsamad",
      "Michael Ulrich",
      "Claudius Glaeser",
      "Abhinav Valada"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Do_We_Really_Need_Curated_Malicious_Data_for_Safety_Alignment_CVPR_2025_paper.html": {
    "title": "Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanbo Wang",
      "Jiyang Guan",
      "Jian Liang",
      "Ran He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_High-Fidelity_Relightable_Monocular_Portrait_Animation_with_Lighting-Controllable_Video_Diffusion_Model_CVPR_2025_paper.html": {
    "title": "High-Fidelity Relightable Monocular Portrait Animation with Lighting-Controllable Video Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingtao Guo",
      "Guanyu Xing",
      "Yanli Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Plug-and-Play_PPO_An_Adaptive_Point_Prompt_Optimizer_Making_SAM_Greater_CVPR_2025_paper.html": {
    "title": "Plug-and-Play PPO: An Adaptive Point Prompt Optimizer Making SAM Greater",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueyu Liu",
      "Rui Wang",
      "Yexin Lai",
      "Guangze Shi",
      "Feixue Shao",
      "Fang Hao",
      "Jianan Zhang",
      "Jia Shen",
      "Yongfei Wu",
      "Wen Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Harnessing_Global-Local_Collaborative_Adversarial_Perturbation_for_Anti-Customization_CVPR_2025_paper.html": {
    "title": "Harnessing Global-Local Collaborative Adversarial Perturbation for Anti-Customization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long Xu",
      "Jiakai Wang",
      "Haojie Hao",
      "Haotong Qin",
      "Jiejie Zhao",
      "Xianglong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_EchoONE_Segmenting_Multiple_Echocardiography_Planes_in_One_Model_CVPR_2025_paper.html": {
    "title": "EchoONE: Segmenting Multiple Echocardiography Planes in One Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiongtong Hu",
      "Wufeng Xue",
      "Jun Cheng",
      "Yingying Liu",
      "Wei Zhuo",
      "Dong Ni"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Acc3D_Accelerating_Single_Image_to_3D_Diffusion_Models_via_Edge_CVPR_2025_paper.html": {
    "title": "Acc3D: Accelerating Single Image to 3D Diffusion Models via Edge Consistency Guided Score Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kendong Liu",
      "Zhiyu Zhu",
      "Hui Liu",
      "Junhui Hou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_EasyHOI_Unleashing_the_Power_of_Large_Models_for_Reconstructing_Hand-Object_CVPR_2025_paper.html": {
    "title": "EasyHOI: Unleashing the Power of Large Models for Reconstructing Hand-Object Interactions in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yumeng Liu",
      "Xiaoxiao Long",
      "Zemin Yang",
      "Yuan Liu",
      "Marc Habermann",
      "Christian Theobalt",
      "Yuexin Ma",
      "Wenping Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nasery_PLeaS_-_Merging_Models_with_Permutations_and_Least_Squares_CVPR_2025_paper.html": {
    "title": "PLeaS - Merging Models with Permutations and Least Squares",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anshul Nasery",
      "Jonathan Hayase",
      "Pang Wei Koh",
      "Sewoong Oh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Incremental_Object_Keypoint_Learning_CVPR_2025_paper.html": {
    "title": "Incremental Object Keypoint Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingfu Liang",
      "Jiahuan Zhou",
      "Xu Zou",
      "Ying Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Soft_Self-labeling_and_Potts_Relaxations_for_Weakly-supervised_Segmentation_CVPR_2025_paper.html": {
    "title": "Soft Self-labeling and Potts Relaxations for Weakly-supervised Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongwen Zhang",
      "Yuri Boykov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Izquierdo_MVSAnywhere_Zero-Shot_Multi-View_Stereo_CVPR_2025_paper.html": {
    "title": "MVSAnywhere: Zero-Shot Multi-View Stereo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sergio Izquierdo",
      "Mohamed Sayed",
      "Michael Firman",
      "Guillermo Garcia-Hernando",
      "Daniyar Turmukhambetov",
      "Javier Civera",
      "Oisin Mac Aodha",
      "Gabriel Brostow",
      "Jamie Watson"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dwivedi_InteractVLM_3D_Interaction_Reasoning_from_2D_Foundational_Models_CVPR_2025_paper.html": {
    "title": "InteractVLM: 3D Interaction Reasoning from 2D Foundational Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sai Kumar Dwivedi",
      "Dimitrije Antić",
      "Shashank Tripathi",
      "Omid Taheri",
      "Cordelia Schmid",
      "Michael J. Black",
      "Dimitrios Tzionas"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peng_Patch_Matters_Training-free_Fine-grained_Image_Caption_Enhancement_via_Local_Perception_CVPR_2025_paper.html": {
    "title": "Patch Matters: Training-free Fine-grained Image Caption Enhancement via Local Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruotian Peng",
      "Haiying He",
      "Yake Wei",
      "Yandong Wen",
      "Di Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Attribute-Missing_Multi-view_Graph_Clustering_CVPR_2025_paper.html": {
    "title": "Attribute-Missing Multi-view Graph Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Zhao",
      "Qianqian Wang",
      "Zhengming Ding",
      "Quanxue Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric_CVPR_2025_paper.html": {
    "title": "Generating 6DoF Object Manipulation Trajectories from Action Description in Egocentric Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomoya Yoshida",
      "Shuhei Kurita",
      "Taichi Nishimura",
      "Shinsuke Mori"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_Pose-Guided_Temporal_Enhancement_for_Robust_Low-Resolution_Hand_Reconstruction_CVPR_2025_paper.html": {
    "title": "Pose-Guided Temporal Enhancement for Robust Low-Resolution Hand Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaixin Fan",
      "Pengfei Ren",
      "Jingyu Wang",
      "Haifeng Sun",
      "Qi Qi",
      "Zirui Zhuang",
      "Jianxin Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_ReDiffDet_Rotation-equivariant_Diffusion_Model_for_Oriented_Object_Detection_CVPR_2025_paper.html": {
    "title": "ReDiffDet: Rotation-equivariant Diffusion Model for Oriented Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Zhao",
      "Zeyu Ding",
      "Yong Zhou",
      "Hancheng Zhu",
      "Wen-Liang Du",
      "Rui Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hsu_PosterO_Structuring_Layout_Trees_to_Enable_Language_Models_in_Generalized_CVPR_2025_paper.html": {
    "title": "PosterO: Structuring Layout Trees to Enable Language Models in Generalized Content-Aware Layout Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "HsiaoYuan Hsu",
      "Yuxin Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lozano_BIOMEDICA_An_Open_Biomedical_Image-Caption_Archive_Dataset_and_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and Vision-Language Models Derived from Scientific Literature",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alejandro Lozano",
      "Min Woo Sun",
      "James Burgess",
      "Liangyu Chen",
      "Jeffrey J. Nirschl",
      "Jeffrey Gu",
      "Ivan Lopez",
      "Josiah Aklilu",
      "Anita Rau",
      "Austin Wolfgang Katzer",
      "Yuhui Zhang",
      "Collin Chiu",
      "Xiaohan Wang",
      "Alfred Seunghoon Song",
      "Robert Tibshirani",
      "Serena Yeung-Levy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration_CVPR_2025_paper.html": {
    "title": "Unlocking Generalization Power in LiDAR Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenxuan Zeng",
      "Qiao Wu",
      "Xiyu Zhang",
      "Lin Yuanbo Wu",
      "Pei An",
      "Jiaqi Yang",
      "Ji Wang",
      "Peng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "Structure-Aware Correspondence Learning for Relative Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihan Chen",
      "Wenfei Yang",
      "Huan Ren",
      "Shifeng Zhang",
      "Tianzhu Zhang",
      "Feng Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_LoRA_Recycle_Unlocking_Tuning-Free_Few-Shot_Adaptability_in_Visual_Foundation_Models_CVPR_2025_paper.html": {
    "title": "LoRA Recycle: Unlocking Tuning-Free Few-Shot Adaptability in Visual Foundation Models by Recycling Pre-Tuned LoRAs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Hu",
      "Yongxian Wei",
      "Li Shen",
      "Chun Yuan",
      "Dacheng Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_One2Any_One-Reference_6D_Pose_Estimation_for_Any_Object_CVPR_2025_paper.html": {
    "title": "One2Any: One-Reference 6D Pose Estimation for Any Object",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengya Liu",
      "Siyuan Li",
      "Ajad Chhatkuli",
      "Prune Truong",
      "Luc Van Gool",
      "Federico Tombari"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Stekovic_PyTorchGeoNodes_Enabling_Differentiable_Shape_Programs_for_3D_Shape_Reconstruction_CVPR_2025_paper.html": {
    "title": "PyTorchGeoNodes: Enabling Differentiable Shape Programs for 3D Shape Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sinisa Stekovic",
      "Arslan Artykov",
      "Stefan Ainetter",
      "Mattia D'Urso",
      "Friedrich Fraundorfer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Contextual_AD_Narration_with_Interleaved_Multimodal_Sequence_CVPR_2025_paper.html": {
    "title": "Contextual AD Narration with Interleaved Multimodal Sequence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanlin Wang",
      "Zhan Tong",
      "Kecheng Zheng",
      "Yujun Shen",
      "Limin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_FIFA_Fine-grained_Inter-frame_Attention_for_Drivers_Video_Gaze_Estimation_CVPR_2025_paper.html": {
    "title": "FIFA: Fine-grained Inter-frame Attention for Driver's Video Gaze Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daosong Hu",
      "Mingyue Cui",
      "Kai Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_MNE-SLAM_Multi-Agent_Neural_SLAM_for_Mobile_Robots_CVPR_2025_paper.html": {
    "title": "MNE-SLAM: Multi-Agent Neural SLAM for Mobile Robots",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianchen Deng",
      "Guole Shen",
      "Chen Xun",
      "Shenghai Yuan",
      "Tongxin Jin",
      "Hongming Shen",
      "Yanbo Wang",
      "Jingchuan Wang",
      "Hesheng Wang",
      "Danwei Wang",
      "Weidong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gu_TensoFlow_Tensorial_Flow-based_Sampler_for_Inverse_Rendering_CVPR_2025_paper.html": {
    "title": "TensoFlow: Tensorial Flow-based Sampler for Inverse Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun Gu",
      "Xiaofei Wei",
      "Li Zhang",
      "Xiatian Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_FRAMES-VQA_Benchmarking_Fine-Tuning_Robustness_across_Multi-Modal_Shifts_in_Visual_Question_CVPR_2025_paper.html": {
    "title": "FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengyue Huang",
      "Brisa Maneechotesuwan",
      "Shivang Chopra",
      "Zsolt Kira"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions_CVPR_2025_paper.html": {
    "title": "Shape Abstraction via Marching Differentiable Support Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunkyung Park",
      "Jeongmin Lee",
      "Dongjun Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhi_LSceneLLM_Enhancing_Large_3D_Scene_Understanding_Using_Adaptive_Visual_Preferences_CVPR_2025_paper.html": {
    "title": "LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyan Zhi",
      "Peihao Chen",
      "Junyan Li",
      "Shuailei Ma",
      "Xinyu Sun",
      "Tianhang Xiang",
      "Yinjie Lei",
      "Mingkui Tan",
      "Chuang Gan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and_CVPR_2025_paper.html": {
    "title": "Event Fields: Capturing Light Fields at High Speed, Resolution, and Dynamic Range",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyuan Qu",
      "Zihao Zou",
      "Vivek Boominathan",
      "Praneeth Chakravarthula",
      "Adithya Pediredla"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_HyperFree_A_Channel-adaptive_and_Tuning-free_Foundation_Model_for_Hyperspectral_Remote_CVPR_2025_paper.html": {
    "title": "HyperFree: A Channel-adaptive and Tuning-free Foundation Model for Hyperspectral Remote Sensing Imagery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingtao Li",
      "Yingyi Liu",
      "Xinyu Wang",
      "Yunning Peng",
      "Chen Sun",
      "Shaoyu Wang",
      "Zhendong Sun",
      "Tian Ke",
      "Xiao Jiang",
      "Tangwei Lu",
      "Anran Zhao",
      "Yanfei Zhong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Exploring_Temporally-Aware_Features_for_Point_Tracking_CVPR_2025_paper.html": {
    "title": "Exploring Temporally-Aware Features for Point Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inès Hyeonsu Kim",
      "Seokju Cho",
      "Jiahui Huang",
      "Jung Yi",
      "Joon-Young Lee",
      "Seungryong Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Malic_GBlobs_Explicit_Local_Structure_via_Gaussian_Blobs_for_Improved_Cross-Domain_CVPR_2025_paper.html": {
    "title": "GBlobs: Explicit Local Structure via Gaussian Blobs for Improved Cross-Domain LiDAR-based 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dušan Malić",
      "Christian Fruhwirth-Reisinger",
      "Samuel Schulter",
      "Horst Possegger"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Abdessaied_V2Dial_Unification_of_Video_and_Visual_Dialog_via_Multimodal_Experts_CVPR_2025_paper.html": {
    "title": "V^2Dial: Unification of Video and Visual Dialog via Multimodal Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adnen Abdessaied",
      "Anna Rohrbach",
      "Marcus Rohrbach",
      "Andreas Bulling"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Detail-Preserving_Latent_Diffusion_for_Stable_Shadow_Removal_CVPR_2025_paper.html": {
    "title": "Detail-Preserving Latent Diffusion for Stable Shadow Removal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiamin Xu",
      "Yuxin Zheng",
      "Zelong Li",
      "Chi Wang",
      "Renshu Gu",
      "Weiwei Xu",
      "Gang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Scaling_Down_Text_Encoders_of_Text-to-Image_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Scaling Down Text Encoders of Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lifu Wang",
      "Daqing Liu",
      "Xinchen Liu",
      "Xiaodong He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_3D_Gaussian_Head_Avatars_with_Expressive_Dynamic_Appearances_by_Compact_CVPR_2025_paper.html": {
    "title": "3D Gaussian Head Avatars with Expressive Dynamic Appearances by Compact Tensorial Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yating Wang",
      "Xuan Wang",
      "Ran Yi",
      "Yanbo Fan",
      "Jichen Hu",
      "Jingcheng Zhu",
      "Lizhuang Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_MambaIRv2_Attentive_State_Space_Restoration_CVPR_2025_paper.html": {
    "title": "MambaIRv2: Attentive State Space Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Guo",
      "Yong Guo",
      "Yaohua Zha",
      "Yulun Zhang",
      "Wenbo Li",
      "Tao Dai",
      "Shu-Tao Xia",
      "Yawei Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Man_Floating_No_More_Object-Ground_Reconstruction_from_a_Single_Image_CVPR_2025_paper.html": {
    "title": "Floating No More: Object-Ground Reconstruction from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunze Man",
      "Yichen Sheng",
      "Jianming Zhang",
      "Liang-Yan Gui",
      "Yu-Xiong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_POT_Prototypical_Optimal_Transport_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "POT: Prototypical Optimal Transport for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Wang",
      "Tianhong Dai",
      "Bingfeng Zhang",
      "Siyue Yu",
      "Eng Gee Lim",
      "Jimin Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment_CVPR_2025_paper.html": {
    "title": "CrossOver: 3D Scene Cross-Modal Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayan Deb Sarkar",
      "Ondrej Miksik",
      "Marc Pollefeys",
      "Daniel Barath",
      "Iro Armeni"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Rethinking_Temporal_Fusion_with_a_Unified_Gradient_Descent_View_for_CVPR_2025_paper.html": {
    "title": "Rethinking Temporal Fusion with a Unified Gradient Descent View for 3D Semantic Occupancy Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dubing Chen",
      "Huan Zheng",
      "Jin Fang",
      "Xingping Dong",
      "Xianfei Li",
      "Wenlong Liao",
      "Tao He",
      "Pai Peng",
      "Jianbing Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SKE-Layout_Spatial_Knowledge_Enhanced_Layout_Generation_with_LLMs_CVPR_2025_paper.html": {
    "title": "SKE-Layout: Spatial Knowledge Enhanced Layout Generation with LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junsheng Wang",
      "Nieqing Cao",
      "Yan Ding",
      "Mengying Xie",
      "Fuqiang Gu",
      "Chao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zielonka_Gaussian_Eigen_Models_for_Human_Heads_CVPR_2025_paper.html": {
    "title": "Gaussian Eigen Models for Human Heads",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wojciech Zielonka",
      "Timo Bolkart",
      "Thabo Beeler",
      "Justus Thies"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jang_Scalable_Video-to-Dataset_Generation_for_Cross-Platform_Mobile_Agents_CVPR_2025_paper.html": {
    "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunseok Jang",
      "Yeda Song",
      "Sungryull Sohn",
      "Lajanugen Logeswaran",
      "Tiange Luo",
      "Dong-Ki Kim",
      "Kyunghoon Bae",
      "Honglak Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ganeshan_Pattern_Analogies_Learning_to_Perform_Programmatic_Image_Edits_by_Analogy_CVPR_2025_paper.html": {
    "title": "Pattern Analogies: Learning to Perform Programmatic Image Edits by Analogy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Ganeshan",
      "Thibault Groueix",
      "Paul Guerrero",
      "Radomir Mech",
      "Matthew Fisher",
      "Daniel Ritchie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_4D-Fly_Fast_4D_Reconstruction_from_a_Single_Monocular_Video_CVPR_2025_paper.html": {
    "title": "4D-Fly: Fast 4D Reconstruction from a Single Monocular Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diankun Wu",
      "Fangfu Liu",
      "Yi-Hsin Hung",
      "Yue Qian",
      "Xiaohang Zhan",
      "Yueqi Duan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_STAR-Edge_Structure-aware_Local_Spherical_Curve_Representation_for_Thin-walled_Edge_Extraction_CVPR_2025_paper.html": {
    "title": "STAR-Edge: Structure-aware Local Spherical Curve Representation for Thin-walled Edge Extraction from Unstructured Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zikuan Li",
      "Honghua Chen",
      "Yuecheng Wang",
      "Sibo Wu",
      "Mingqiang Wei",
      "Jun Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Tokenize_Image_Patches_Global_Context_Fusion_for_Effective_Haze_Removal_CVPR_2025_paper.html": {
    "title": "Tokenize Image Patches: Global Context Fusion for Effective Haze Removal in Large Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiuchen Chen",
      "Xinyu Yan",
      "Qizhi Xu",
      "Kaiqi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Complementary_Advantages_Exploiting_Cross-Field_Frequency_Correlation_for_NIR-Assisted_Image_Denoising_CVPR_2025_paper.html": {
    "title": "Complementary Advantages: Exploiting Cross-Field Frequency Correlation for NIR-Assisted Image Denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Wang",
      "Hongyuan Wang",
      "Lizhi Wang",
      "Xin Wang",
      "Lin Zhu",
      "Wanxuan Lu",
      "Hua Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Duggal_Eval3D_Interpretable_and_Fine-grained_Evaluation_for_3D_Generation_CVPR_2025_paper.html": {
    "title": "Eval3D: Interpretable and Fine-grained Evaluation for 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shivam Duggal",
      "Yushi Hu",
      "Oscar Michel",
      "Aniruddha Kembhavi",
      "William T. Freeman",
      "Noah A. Smith",
      "Ranjay Krishna",
      "Antonio Torralba",
      "Ali Farhadi",
      "Wei-Chiu Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qin_Boosting_the_Dual-Stream_Architecture_in_Ultra-High_Resolution_Segmentation_with_Resolution-Biased_CVPR_2025_paper.html": {
    "title": "Boosting the Dual-Stream Architecture in Ultra-High Resolution Segmentation with Resolution-Biased Uncertainty Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rong Qin",
      "Xingyu Liu",
      "Jinglei Shi",
      "Liang Lin",
      "Jufeng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_DiffLO_Semantic-Aware_LiDAR_Odometry_with_Diffusion-Based_Refinement_CVPR_2025_paper.html": {
    "title": "DiffLO: Semantic-Aware LiDAR Odometry with Diffusion-Based Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongshu Huang",
      "Chen Liu",
      "Minghang Zhu",
      "Sheng Ao",
      "Chenglu Wen",
      "Cheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_pFedMxF_Personalized_Federated_Class-Incremental_Learning_with_Mixture_of_Frequency_Aggregation_CVPR_2025_paper.html": {
    "title": "pFedMxF: Personalized Federated Class-Incremental Learning with Mixture of Frequency Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Zhang",
      "Hao Zhu",
      "Alysa Ziying Tan",
      "Dianzhi Yu",
      "Longtao Huang",
      "Han Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_Style-Editor_Text-driven_Object-centric_Style_Editing_CVPR_2025_paper.html": {
    "title": "Style-Editor: Text-driven Object-centric Style Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihun Park",
      "Jongmin Gim",
      "Kyoungmin Lee",
      "Seunghun Lee",
      "Sunghoon Im"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pan_Transfer_Your_Perspective_Controllable_3D_Generation_from_Any_Viewpoint_in_CVPR_2025_paper.html": {
    "title": "Transfer Your Perspective: Controllable 3D Generation from Any Viewpoint in a Driving Scene",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tai-Yu Pan",
      "Sooyoung Jeon",
      "Mengdi Fan",
      "Jinsu Yoo",
      "Zhenyang Feng",
      "Mark Campbell",
      "Kilian Q. Weinberger",
      "Bharath Hariharan",
      "Wei-Lun Chao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Efficient_Transfer_Learning_for_Video-language_Foundation_Models_CVPR_2025_paper.html": {
    "title": "Efficient Transfer Learning for Video-language Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxing Chen",
      "Zizheng Huang",
      "Yan Hong",
      "Yanshuo Wang",
      "Zhongcai Lyu",
      "Zhuoer Xu",
      "Jun Lan",
      "Zhangxuan Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Radio_Frequency_Ray_Tracing_with_Neural_Object_Representation_for_Enhanced_CVPR_2025_paper.html": {
    "title": "Radio Frequency Ray Tracing with Neural Object Representation for Enhanced RF Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Chen",
      "Zihao Feng",
      "Kun Qian",
      "Xinyu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Su_ANNEXE_Unified_Analyzing_Answering_and_Pixel_Grounding_for_Egocentric_Interaction_CVPR_2025_paper.html": {
    "title": "ANNEXE: Unified Analyzing, Answering, and Pixel Grounding for Egocentric Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuejiao Su",
      "Yi Wang",
      "Qiongyang Hu",
      "Chuang Yang",
      "Lap-Pui Chau"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Asim_MET3R_Measuring_Multi-View_Consistency_in_Generated_Images_CVPR_2025_paper.html": {
    "title": "MET3R: Measuring Multi-View Consistency in Generated Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Asim",
      "Christopher Wewer",
      "Thomas Wimmer",
      "Bernt Schiele",
      "Jan Eric Lenssen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bolelli_Segmenting_Maxillofacial_Structures_in_CBCT_Volumes_CVPR_2025_paper.html": {
    "title": "Segmenting Maxillofacial Structures in CBCT Volumes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Federico Bolelli",
      "Kevin Marchesini",
      "Niels van Nistelrooij",
      "Luca Lumetti",
      "Vittorio Pipoli",
      "Elisa Ficarra",
      "Shankeeth Vinayahalingam",
      "Costantino Grana"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xi_3D_Dental_Model_Segmentation_with_Geometrical_Boundary_Preserving_CVPR_2025_paper.html": {
    "title": "3D Dental Model Segmentation with Geometrical Boundary Preserving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shufan Xi",
      "Zexian Liu",
      "Junlin Chang",
      "Hongyu Wu",
      "Xiaogang Wang",
      "Aimin Hao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Neuro-3D_Towards_3D_Visual_Decoding_from_EEG_Signals_CVPR_2025_paper.html": {
    "title": "Neuro-3D: Towards 3D Visual Decoding from EEG Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanqiang Guo",
      "Jiamin Wu",
      "Yonghao Song",
      "Jiahui Bu",
      "Weijian Mai",
      "Qihao Zheng",
      "Wanli Ouyang",
      "Chunfeng Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Vasu_FastVLM_Efficient_Vision_Encoding_for_Vision_Language_Models_CVPR_2025_paper.html": {
    "title": "FastVLM: Efficient Vision Encoding for Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pavan Kumar Anasosalu Vasu",
      "Fartash Faghri",
      "Chun-Liang Li",
      "Cem Koc",
      "Nate True",
      "Albert Antony",
      "Gokula Santhanam",
      "James Gabriel",
      "Peter Grasch",
      "Oncel Tuzel",
      "Hadi Pouransari"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_VISTA3D_A_Unified_Segmentation_Foundation_Model_For_3D_Medical_Imaging_CVPR_2025_paper.html": {
    "title": "VISTA3D: A Unified Segmentation Foundation Model For 3D Medical Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufan He",
      "Pengfei Guo",
      "Yucheng Tang",
      "Andriy Myronenko",
      "Vishwesh Nath",
      "Ziyue Xu",
      "Dong Yang",
      "Can Zhao",
      "Benjamin Simon",
      "Mason Belue",
      "Stephanie Harmon",
      "Baris Turkbey",
      "Daguang Xu",
      "Wenqi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_VideoGigaGAN_Towards_Detail-rich_Video_Super-Resolution_CVPR_2025_paper.html": {
    "title": "VideoGigaGAN: Towards Detail-rich Video Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiran Xu",
      "Taesung Park",
      "Richard Zhang",
      "Yang Zhou",
      "Eli Shechtman",
      "Feng Liu",
      "Jia-Bin Huang",
      "Difan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Probing_the_Mid-level_Vision_Capabilities_of_Self-Supervised_Learning_CVPR_2025_paper.html": {
    "title": "Probing the Mid-level Vision Capabilities of Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuweiyi Chen",
      "Markus Marks",
      "Zezhou Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_S2D-LFE_Sparse-to-Dense_Light_Field_Event_Generation_CVPR_2025_paper.html": {
    "title": "S2D-LFE: Sparse-to-Dense Light Field Event Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutong Liu",
      "Wenming Weng",
      "Yueyi Zhang",
      "Zhiwei Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gomez-Villa_The_Art_of_Deception_Color_Visual_Illusions_and_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "The Art of Deception: Color Visual Illusions and Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandra Gomez-Villa",
      "Kai Wang",
      "C.Alejandro Parraga",
      "Bartłomiej Twardowski",
      "Jesus Malo",
      "Javier Vazquez-Corral",
      "Joost van den Weijer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_GLUS_Global-Local_Reasoning_Unified_into_A_Single_Large_Language_Model_CVPR_2025_paper.html": {
    "title": "GLUS: Global-Local Reasoning Unified into A Single Large Language Model for Video Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lang Lin",
      "Xueyang Yu",
      "Ziqi Pang",
      "Yu-Xiong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Progressive_Rendering_Distillation_Adapting_Stable_Diffusion_for_Instant_Text-to-Mesh_Generation_CVPR_2025_paper.html": {
    "title": "Progressive Rendering Distillation: Adapting Stable Diffusion for Instant Text-to-Mesh Generation without 3D Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Ma",
      "Xinyue Liang",
      "Rongyuan Wu",
      "Xiangyu Zhu",
      "Zhen Lei",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jang_Efficient_Long_Video_Tokenization_via_Coordinate-based_Patch_Reconstruction_CVPR_2025_paper.html": {
    "title": "Efficient Long Video Tokenization via Coordinate-based Patch Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiwon Jang",
      "Sihyun Yu",
      "Jinwoo Shin",
      "Pieter Abbeel",
      "Younggyo Seo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Derivative-Free_Diffusion_Manifold-Constrained_Gradient_for_Unified_XAI_CVPR_2025_paper.html": {
    "title": "Derivative-Free Diffusion Manifold-Constrained Gradient for Unified XAI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Won Jun Kim",
      "Hyungjin Chung",
      "Jaemin Kim",
      "Sangmin Lee",
      "Byeongsu Sim",
      "Jong Chul Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yellapragada_ZoomLDM_Latent_Diffusion_Model_for_Multi-scale_Image_Generation_CVPR_2025_paper.html": {
    "title": "ZoomLDM: Latent Diffusion Model for Multi-scale Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Srikar Yellapragada",
      "Alexandros Graikos",
      "Kostas Triaridis",
      "Prateek Prasanna",
      "Rajarsi Gupta",
      "Joel Saltz",
      "Dimitris Samaras"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of_CVPR_2025_paper.html": {
    "title": "Do Computer Vision Foundation Models Learn the Low-level Characteristics of the Human Visual System?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yancheng Cai",
      "Fei Yin",
      "Dounia Hammou",
      "Rafal Mantiuk"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "GaussianUDF: Inferring Unsigned Distance Functions through 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shujuan Li",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Towards_RAW_Object_Detection_in_Diverse_Conditions_CVPR_2025_paper.html": {
    "title": "Towards RAW Object Detection in Diverse Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhong-Yu Li",
      "Xin Jin",
      "Bo-Yuan Sun",
      "Chun-Le Guo",
      "Ming-Ming Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cao_FLAME_Frozen_Large_Language_Models_Enable_Data-Efficient_Language-Image_Pre-training_CVPR_2025_paper.html": {
    "title": "FLAME: Frozen Large Language Models Enable Data-Efficient Language-Image Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anjia Cao",
      "Xing Wei",
      "Zhiheng Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Walker_CrossSDF_3D_Reconstruction_of_Thin_Structures_From_Cross-Sections_CVPR_2025_paper.html": {
    "title": "CrossSDF: 3D Reconstruction of Thin Structures From Cross-Sections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Walker",
      "Salvatore Esposito",
      "Daniel Rebain",
      "Amir Vaxman",
      "Arno Onken",
      "Changjian Li",
      "Oisin Mac Aodha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_DV-Matcher_Deformation-based_Non-rigid_Point_Cloud_Matching_Guided_by_Pre-trained_Visual_CVPR_2025_paper.html": {
    "title": "DV-Matcher: Deformation-based Non-rigid Point Cloud Matching Guided by Pre-trained Visual Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangquan Chen",
      "Puhua Jiang",
      "Ruqi Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Reasoning_Mamba_Hypergraph-Guided_Region_Relation_Calculating_for_Weakly_Supervised_Affordance_CVPR_2025_paper.html": {
    "title": "Reasoning Mamba: Hypergraph-Guided Region Relation Calculating for Weakly Supervised Affordance Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Wang",
      "Aming Wu",
      "Muli Yang",
      "Yukuan Min",
      "Yihang Zhu",
      "Cheng Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fukuda_Adapter_Merging_with_Centroid_Prototype_Mapping_for_Scalable_Class-Incremental_Learning_CVPR_2025_paper.html": {
    "title": "Adapter Merging with Centroid Prototype Mapping for Scalable Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takuma Fukuda",
      "Hiroshi Kera",
      "Kazuhiko Kawamoto"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_OpenSDI_Spotting_Diffusion-Generated_Images_in_the_Open_World_CVPR_2025_paper.html": {
    "title": "OpenSDI: Spotting Diffusion-Generated Images in the Open World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yabin Wang",
      "Zhiwu Huang",
      "Xiaopeng Hong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dai_Adaptive_Part_Learning_for_Fine-Grained_Generalized_Category_Discovery_A_Plug-and-Play_CVPR_2025_paper.html": {
    "title": "Adaptive Part Learning for Fine-Grained Generalized Category Discovery: A Plug-and-Play Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyuan Dai",
      "Hanzhuo Huang",
      "Yu Wu",
      "Sibei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_Online_Task-Free_Continual_Learning_via_Dynamic_Expansionable_Memory_Distribution_CVPR_2025_paper.html": {
    "title": "Online Task-Free Continual Learning via Dynamic Expansionable Memory Distribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Ye",
      "Adrian G. Bors"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mei_Lux_Post_Facto_Learning_Portrait_Performance_Relighting_with_Conditional_Video_CVPR_2025_paper.html": {
    "title": "Lux Post Facto: Learning Portrait Performance Relighting with Conditional Video Diffusion and a Hybrid Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiqun Mei",
      "Mingming He",
      "Li Ma",
      "Julien Philip",
      "Wenqi Xian",
      "David M George",
      "Xueming Yu",
      "Gabriel Dedic",
      "Ahmet Levent Taşel",
      "Ning Yu",
      "Vishal M. Patel",
      "Paul Debevec"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_DiG_Scalable_and_Efficient_Diffusion_Models_with_Gated_Linear_Attention_CVPR_2025_paper.html": {
    "title": "DiG: Scalable and Efficient Diffusion Models with Gated Linear Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lianghui Zhu",
      "Zilong Huang",
      "Bencheng Liao",
      "Jun Hao Liew",
      "Hanshu Yan",
      "Jiashi Feng",
      "Xinggang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gong_Monocular_and_Generalizable_Gaussian_Talking_Head_Animation_CVPR_2025_paper.html": {
    "title": "Monocular and Generalizable Gaussian Talking Head Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengjie Gong",
      "Haojie Li",
      "Jiapeng Tang",
      "Dongming Hu",
      "Shuangping Huang",
      "Hao Chen",
      "Tianshui Chen",
      "Zhuoman Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lei_Rethinking_Token_Reduction_with_Parameter-Efficient_Fine-Tuning_in_ViT_for_Pixel-Level_CVPR_2025_paper.html": {
    "title": "Rethinking Token Reduction with Parameter-Efficient Fine-Tuning in ViT for Pixel-Level Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Lei",
      "Ao Li",
      "Hu Yao",
      "Ce Zhu",
      "Le Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_SVDC_Consistent_Direct_Time-of-Flight_Video_Depth_Completion_with_Frequency_Selective_CVPR_2025_paper.html": {
    "title": "SVDC: Consistent Direct Time-of-Flight Video Depth Completion with Frequency Selective Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Zhu",
      "Jijun Xiang",
      "Xianqi Wang",
      "Longliang Liu",
      "Yu Wang",
      "Hong Zhang",
      "Fei Guo",
      "Xin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering_CVPR_2025_paper.html": {
    "title": "Locally Orderless Images for Optimization in Differentiable Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ishit Mehta",
      "Manmohan Chandraker",
      "Ravi Ramamoorthi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Azam_Plug-and-Play_Interpretable_Responsible_Text-to-Image_Generation_via_Dual-Space_Multi-facet_Concept_Control_CVPR_2025_paper.html": {
    "title": "Plug-and-Play Interpretable Responsible Text-to-Image Generation via Dual-Space Multi-facet Concept Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Basim Azam",
      "Naveed Akhtar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Rethinking_Training_for_De-biasing_Text-to-Image_Generation_Unlocking_the_Potential_of_CVPR_2025_paper.html": {
    "title": "Rethinking Training for De-biasing Text-to-Image Generation: Unlocking the Potential of Stable Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eunji Kim",
      "Siwon Kim",
      "Minjun Park",
      "Rahim Entezari",
      "Sungroh Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_FLAIR_VLM_with_Fine-grained_Language-informed_Image_Representations_CVPR_2025_paper.html": {
    "title": "FLAIR: VLM with Fine-grained Language-informed Image Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Xiao",
      "Sanghwan Kim",
      "Mariana-Iuliana Georgescu",
      "Zeynep Akata",
      "Stephan Alaniz"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zubic_GG-SSMs_Graph-Generating_State_Space_Models_CVPR_2025_paper.html": {
    "title": "GG-SSMs: Graph-Generating State Space Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikola Zubic",
      "Davide Scaramuzza"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Barda_Instant3dit_Multiview_Inpainting_for_Fast_Editing_of_3D_Objects_CVPR_2025_paper.html": {
    "title": "Instant3dit: Multiview Inpainting for Fast Editing of 3D Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Barda",
      "Matheus Gadelha",
      "Vladimir G. Kim",
      "Noam Aigerman",
      "Amit H. Bermano",
      "Thibault Groueix"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yao_STDD_Spatio-Temporal_Dual_Diffusion_for_Video_Generation_CVPR_2025_paper.html": {
    "title": "STDD: Spatio-Temporal Dual Diffusion for Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuaizhen Yao",
      "Xiaoya Zhang",
      "Xin Liu",
      "Mengyi Liu",
      "Zhen Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration_CVPR_2025_paper.html": {
    "title": "Implicit Correspondence Learning for Image-to-Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinjun Li",
      "Wenfei Yang",
      "Jiacheng Deng",
      "Zhixin Cheng",
      "Xu Zhou",
      "Tianzhu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Continuous_Adverse_Weather_Removal_via_Degradation-Aware_Distillation_CVPR_2025_paper.html": {
    "title": "Continuous Adverse Weather Removal via Degradation-Aware Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Lu",
      "Jie Xiao",
      "Yurui Zhu",
      "Xueyang Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Thakral_Fine-Grained_Erasure_in_Text-to-Image_Diffusion-based_Foundation_Models_CVPR_2025_paper.html": {
    "title": "Fine-Grained Erasure in Text-to-Image Diffusion-based Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kartik Thakral",
      "Tamar Glaser",
      "Tal Hassner",
      "Mayank Vatsa",
      "Richa Singh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kordopatis-Zilos_ILIAS_Instance-Level_Image_retrieval_At_Scale_CVPR_2025_paper.html": {
    "title": "ILIAS: Instance-Level Image retrieval At Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giorgos Kordopatis-Zilos",
      "Vladan Stojnić",
      "Anna Manko",
      "Pavel Suma",
      "Nikolaos-Antonios Ypsilantis",
      "Nikos Efthymiadis",
      "Zakaria Laskar",
      "Jiri Matas",
      "Ondrej Chum",
      "Giorgos Tolias"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hesham_Exploiting_Temporal_State_Space_Sharing_for_Video_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Exploiting Temporal State Space Sharing for Video Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Syed Ariff Syed Hesham",
      "Yun Liu",
      "Guolei Sun",
      "Henghui Ding",
      "Jing Yang",
      "Ender Konukoglu",
      "Xue Geng",
      "Xudong Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_DeRS_Towards_Extremely_Efficient_Upcycled_Mixture-of-Experts_Models_CVPR_2025_paper.html": {
    "title": "DeRS: Towards Extremely Efficient Upcycled Mixture-of-Experts Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongqi Huang",
      "Peng Ye",
      "Chenyu Huang",
      "Jianjian Cao",
      "Lin Zhang",
      "Baopu Li",
      "Gang Yu",
      "Tao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_GeoDepth_From_Point-to-Depth_to_Plane-to-Depth_Modeling_for_Self-Supervised_Monocular_Depth_CVPR_2025_paper.html": {
    "title": "GeoDepth: From Point-to-Depth to Plane-to-Depth Modeling for Self-Supervised Monocular Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haifeng Wu",
      "Shuhang Gu",
      "Lixin Duan",
      "Wen Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split_CVPR_2025_paper.html": {
    "title": "SSHNet: Unsupervised Cross-modal Homography Estimation via Problem Reformulation and Split Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junchen Yu",
      "Si-Yuan Cao",
      "Runmin Zhang",
      "Chenghao Zhang",
      "Zhu Yu",
      "Shujie Chen",
      "Bailin Yang",
      "Hui-Liang Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian_CVPR_2025_paper.html": {
    "title": "High-fidelity 3D Object Generation from Single Image with RGBN-Volume Gaussian Reconstruction Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Shen",
      "Kun Zhou",
      "He Wang",
      "Yin Yang",
      "Tianjia Shao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Steepest_Descent_Density_Control_for_Compact_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "Steepest Descent Density Control for Compact 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peihao Wang",
      "Yuehao Wang",
      "Dilin Wang",
      "Sreyas Mohan",
      "Zhiwen Fan",
      "Lemeng Wu",
      "Ruisi Cai",
      "Yu-Ying Yeh",
      "Zhangyang Wang",
      "Qiang Liu",
      "Rakesh Ranjan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Optimal_Transport-Guided_Source-Free_Adaptation_for_Face_Anti-Spoofing_CVPR_2025_paper.html": {
    "title": "Optimal Transport-Guided Source-Free Adaptation for Face Anti-Spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuowei Li",
      "Tianchen Zhao",
      "Xiang Xu",
      "Zheng Zhang",
      "Zhihua Li",
      "Xuanbai Chen",
      "Qin Zhang",
      "Alessandro Bergamo",
      "Anil K. Jain",
      "Yifan Xing"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "USP-Gaussian: Unifying Spike-based Image Reconstruction, Pose Correction and Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kang Chen",
      "Jiyuan Zhang",
      "Zecheng Hao",
      "Yajing Zheng",
      "Tiejun Huang",
      "Zhaofei Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cho_Robust_3D_Shape_Reconstruction_in_Zero-Shot_from_a_Single_Image_CVPR_2025_paper.html": {
    "title": "Robust 3D Shape Reconstruction in Zero-Shot from a Single Image in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhyeong Cho",
      "Kim Youwang",
      "Hunmin Yang",
      "Tae-Hyun Oh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_BOE-ViT_Boosting_Orientation_Estimation_with_Equivariance_in_Self-Supervised_3D_Subtomogram_CVPR_2025_paper.html": {
    "title": "BOE-ViT: Boosting Orientation Estimation with Equivariance in Self-Supervised 3D Subtomogram Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runmin Jiang",
      "Jackson Daggett",
      "Shriya Pingulkar",
      "Yizhou Zhao",
      "Priyanshu Dhingra",
      "Daniel Brown",
      "Qifeng Wu",
      "Xiangrui Zeng",
      "Xingjian Li",
      "Min Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity_CVPR_2025_paper.html": {
    "title": "Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any Granularity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huaxin Zhang",
      "Xiaohao Xu",
      "Xiang Wang",
      "Jialong Zuo",
      "Xiaonan Huang",
      "Changxin Gao",
      "Shanjun Zhang",
      "Li Yu",
      "Nong Sang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Adventurer_Optimizing_Vision_Mamba_Architecture_Designs_for_Efficiency_CVPR_2025_paper.html": {
    "title": "Adventurer: Optimizing Vision Mamba Architecture Designs for Efficiency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Wang",
      "Timing Yang",
      "Yaodong Yu",
      "Sucheng Ren",
      "Guoyizhe Wei",
      "Angtian Wang",
      "Wei Shao",
      "Yuyin Zhou",
      "Alan Yuille",
      "Cihang Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Caldarola_Beyond_Local_Sharpness_Communication-Efficient_Global_Sharpness-aware_Minimization_for_Federated_Learning_CVPR_2025_paper.html": {
    "title": "Beyond Local Sharpness: Communication-Efficient Global Sharpness-aware Minimization for Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debora Caldarola",
      "Pietro Cagnasso",
      "Barbara Caputo",
      "Marco Ciccone"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary_CVPR_2025_paper.html": {
    "title": "ALIEN: Implicit Neural Representations for Human Motion Prediction under Arbitrary Latency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Wei",
      "Xiaoning Sun",
      "Xizhan Gao",
      "Shengxiang Hu",
      "Huaijiang Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_Parameterized_Blur_Kernel_Prior_Learning_for_Local_Motion_Deblurring_CVPR_2025_paper.html": {
    "title": "Parameterized Blur Kernel Prior Learning for Local Motion Deblurring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenxuan Fang",
      "Fangfang Wu",
      "Tao Huang",
      "Le Dong",
      "Weisheng Dong",
      "Xin Li",
      "Guangming Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_QuartDepth_Post-Training_Quantization_for_Real-Time_Depth_Estimation_on_the_Edge_CVPR_2025_paper.html": {
    "title": "QuartDepth: Post-Training Quantization for Real-Time Depth Estimation on the Edge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Shen",
      "Weize Ma",
      "Jing Liu",
      "Changdi Yang",
      "Rui Ding",
      "Quanyi Wang",
      "Henghui Ding",
      "Wei Niu",
      "Yanzhi Wang",
      "Pu Zhao",
      "Jun Lin",
      "Jiuxiang Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Diko_ReWind_Understanding_Long_Videos_with_Instructed_Learnable_Memory_CVPR_2025_paper.html": {
    "title": "ReWind: Understanding Long Videos with Instructed Learnable Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anxhelo Diko",
      "Tinghuai Wang",
      "Wassim Swaileh",
      "Shiyan Sun",
      "Ioannis Patras"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Sufficient_Invariant_Learning_for_Distribution_Shift_CVPR_2025_paper.html": {
    "title": "Sufficient Invariant Learning for Distribution Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taero Kim",
      "Subeen Park",
      "Sungjun Lim",
      "Yonghan Jung",
      "Krikamol Muandet",
      "Kyungwoo Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ju_DirectTriGS_Triplane-based_Gaussian_Splatting_Field_Representation_for_3D_Generation_CVPR_2025_paper.html": {
    "title": "DirectTriGS: Triplane-based Gaussian Splatting Field Representation for 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoliang Ju",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wen_Domain_Generalization_in_CLIP_via_Learning_with_Diverse_Text_Prompts_CVPR_2025_paper.html": {
    "title": "Domain Generalization in CLIP via Learning with Diverse Text Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changsong Wen",
      "Zelin Peng",
      "Yu Huang",
      "Xiaokang Yang",
      "Wei Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Scene4U_Hierarchical_Layered_3D_Scene_Reconstruction_from_Single_Panoramic_Image_CVPR_2025_paper.html": {
    "title": "Scene4U: Hierarchical Layered 3D Scene Reconstruction from Single Panoramic Image for Your Immerse Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zilong Huang",
      "Jun He",
      "Junyan Ye",
      "Lihan Jiang",
      "Weijia Li",
      "Yiping Chen",
      "Ting Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters_CVPR_2025_paper.html": {
    "title": "Make-It-Animatable: An Efficient Framework for Authoring Animation-Ready 3D Characters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyang Guo",
      "Jinxu Xiang",
      "Kai Ma",
      "Wengang Zhou",
      "Houqiang Li",
      "Ran Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_IterIS_Iterative_Inference-Solving_Alignment_for_LoRA_Merging_CVPR_2025_paper.html": {
    "title": "IterIS: Iterative Inference-Solving Alignment for LoRA Merging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongxu Chen",
      "Zhen Wang",
      "Runshi Li",
      "Bowei Zhu",
      "Long Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiang_ACAttack_Adaptive_Cross_Attacking_RGB-T_Tracker_via_Multi-Modal_Response_Decoupling_CVPR_2025_paper.html": {
    "title": "ACAttack: Adaptive Cross Attacking RGB-T Tracker via Multi-Modal Response Decoupling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Xiang",
      "Qinglong Yan",
      "Hao Zhang",
      "Jiayi Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_DeCafNet_Delegate_and_Conquer_for_Efficient_Temporal_Grounding_in_Long_CVPR_2025_paper.html": {
    "title": "DeCafNet: Delegate and Conquer for Efficient Temporal Grounding in Long Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijia Lu",
      "A S M Iftekhar",
      "Gaurav Mittal",
      "Tianjian Meng",
      "Xiawei Wang",
      "Cheng Zhao",
      "Rohith Kukkala",
      "Ehsan Elhamifar",
      "Mei Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Efficient_ANN-Guided_Distillation_Aligning_Rate-based_Features_of_Spiking_Neural_Networks_CVPR_2025_paper.html": {
    "title": "Efficient ANN-Guided Distillation: Aligning Rate-based Features of Spiking Neural Networks through Hybrid Block-wise Replacement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shu Yang",
      "Chengting Yu",
      "Lei Liu",
      "Hanzhi Ma",
      "Aili Wang",
      "Erping Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Erkoc_PrEditor3D_Fast_and_Precise_3D_Shape_Editing_CVPR_2025_paper.html": {
    "title": "PrEditor3D: Fast and Precise 3D Shape Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziya Erkoç",
      "Can Gümeli",
      "Chaoyang Wang",
      "Matthias Nießner",
      "Angela Dai",
      "Peter Wonka",
      "Hsin-Ying Lee",
      "Peiye Zhuang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Subspace_Constraint_and_Contribution_Estimation_for_Heterogeneous_Federated_Learning_CVPR_2025_paper.html": {
    "title": "Subspace Constraint and Contribution Estimation for Heterogeneous Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangtao Zhang",
      "Sheng Li",
      "Ao Li",
      "Yipeng Liu",
      "Fan Zhang",
      "Ce Zhu",
      "Le Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_HoGS_Unified_Near_and_Far_Object_Reconstruction_via_Homogeneous_Gaussian_CVPR_2025_paper.html": {
    "title": "HoGS: Unified Near and Far Object Reconstruction via Homogeneous Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinpeng Liu",
      "Zeyi Huang",
      "Fumio Okura",
      "Yasuyuki Matsushita"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_SmartEraser_Remove_Anything_from_Images_using_Masked-Region_Guidance_CVPR_2025_paper.html": {
    "title": "SmartEraser: Remove Anything from Images using Masked-Region Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longtao Jiang",
      "Zhendong Wang",
      "Jianmin Bao",
      "Wengang Zhou",
      "Dongdong Chen",
      "Lei Shi",
      "Dong Chen",
      "Houqiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_ComRoPE_Scalable_and_Robust_Rotary_Position_Embedding_Parameterized_by_Trainable_CVPR_2025_paper.html": {
    "title": "ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Yu",
      "Tangyu Jiang",
      "Shuning Jia",
      "Shannan Yan",
      "Shunning Liu",
      "Haolong Qian",
      "Guanghao Li",
      "Shuting Dong",
      "Chun Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Amrani_Sample-_and_Parameter-Efficient_Auto-Regressive_Image_Models_CVPR_2025_paper.html": {
    "title": "Sample- and Parameter-Efficient Auto-Regressive Image Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elad Amrani",
      "Leonid Karlinsky",
      "Alex Bronstein"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Robust_Audio-Visual_Segmentation_via_Audio-Guided_Visual_Convergent_Alignment_CVPR_2025_paper.html": {
    "title": "Robust Audio-Visual Segmentation via Audio-Guided Visual Convergent Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Liu",
      "Peike Li",
      "Liying Yang",
      "Dadong Wang",
      "Lincheng Li",
      "Xin Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_LOCORE_Image_Re-ranking_with_Long-Context_Sequence_Modeling_CVPR_2025_paper.html": {
    "title": "LOCORE: Image Re-ranking with Long-Context Sequence Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zilin Xiao",
      "Pavel Suma",
      "Ayush Sachdeva",
      "Hao-Jen Wang",
      "Giorgos Kordopatis-Zilos",
      "Giorgos Tolias",
      "Vicente Ordonez"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor_CVPR_2025_paper.html": {
    "title": "NeRFPrior: Learning Neural Radiance Field as a Prior for Indoor Scene Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyuan Zhang",
      "Emily Yue-ting Jia",
      "Junsheng Zhou",
      "Baorui Ma",
      "Kanle Shi",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_BiLoRA_Almost-Orthogonal_Parameter_Spaces_for_Continual_Learning_CVPR_2025_paper.html": {
    "title": "BiLoRA: Almost-Orthogonal Parameter Spaces for Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zhu",
      "Yifei Zhang",
      "Junhao Dong",
      "Piotr Koniusz"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Vid2Sim_Generalizable_Video-based_Reconstruction_of_Appearance_Geometry_and_Physics_for_CVPR_2025_paper.html": {
    "title": "Vid2Sim: Generalizable, Video-based Reconstruction of Appearance, Geometry and Physics for Mesh-free Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuhao Chen",
      "Zhiyang Dou",
      "Chen Wang",
      "Yiming Huang",
      "Anjun Chen",
      "Qiao Feng",
      "Jiatao Gu",
      "Lingjie Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cao_SceneTAP_Scene-Coherent_Typographic_Adversarial_Planner_against_Vision-Language_Models_in_Real-World_CVPR_2025_paper.html": {
    "title": "SceneTAP: Scene-Coherent Typographic Adversarial Planner against Vision-Language Models in Real-World Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Cao",
      "Yun Xing",
      "Jie Zhang",
      "Di Lin",
      "Tianwei Zhang",
      "Ivor Tsang",
      "Yang Liu",
      "Qing Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Collaborative_Decoding_Makes_Visual_Auto-Regressive_Modeling_Efficient_CVPR_2025_paper.html": {
    "title": "Collaborative Decoding Makes Visual Auto-Regressive Modeling Efficient",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zigeng Chen",
      "Xinyin Ma",
      "Gongfan Fang",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Vuong_AerialMegaDepth_Learning_Aerial-Ground_Reconstruction_and_View_Synthesis_CVPR_2025_paper.html": {
    "title": "AerialMegaDepth: Learning Aerial-Ground Reconstruction and View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khiem Vuong",
      "Anurag Ghosh",
      "Deva Ramanan",
      "Srinivasa Narasimhan",
      "Shubham Tulsiani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Towards_Training-free_Anomaly_Detection_with_Vision_and_Language_Foundation_Models_CVPR_2025_paper.html": {
    "title": "Towards Training-free Anomaly Detection with Vision and Language Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinjin Zhang",
      "Guodong Wang",
      "Yizhou Jin",
      "Di Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_LiVOS_Light_Video_Object_Segmentation_with_Gated_Linear_Matching_CVPR_2025_paper.html": {
    "title": "LiVOS: Light Video Object Segmentation with Gated Linear Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qin Liu",
      "Jianfeng Wang",
      "Zhengyuan Yang",
      "Linjie Li",
      "Kevin Lin",
      "Marc Niethammer",
      "Lijuan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Dynamic_Content_Prediction_with_Motion-aware_Priors_for_Blind_Face_Video_CVPR_2025_paper.html": {
    "title": "Dynamic Content Prediction with Motion-aware Priors for Blind Face Video Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lianxin Xie",
      "Bingbing Zheng",
      "Si Wu",
      "Hau San Wong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Enomoto_Polarized_Color_Screen_Matting_CVPR_2025_paper.html": {
    "title": "Polarized Color Screen Matting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kenji Enomoto",
      "Scott Cohen",
      "Brian Price",
      "TJ Rhodes"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing_CVPR_2025_paper.html": {
    "title": "Visual Representation Learning through Causal Intervention for Controllable Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanshan Huang",
      "Haoxuan Li",
      "Chunyuan Zheng",
      "Lei Wang",
      "Guorui Liao",
      "Zhili Gong",
      "Huayi Yang",
      "Li Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Exploring_the_Deep_Fusion_of_Large_Language_Models_and_Diffusion_CVPR_2025_paper.html": {
    "title": "Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingda Tang",
      "Boyang Zheng",
      "Sayak Paul",
      "Saining Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_A_Comprehensive_Study_of_Decoder-Only_LLMs_for_Text-to-Image_Generation_CVPR_2025_paper.html": {
    "title": "A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Z. Wang",
      "Songwei Ge",
      "Tero Karras",
      "Ming-Yu Liu",
      "Yogesh Balaji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Exploring_Sparse_MoE_in_GANs_for_Text-conditioned_Image_Synthesis_CVPR_2025_paper.html": {
    "title": "Exploring Sparse MoE in GANs for Text-conditioned Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiapeng Zhu",
      "Ceyuan Yang",
      "Kecheng Zheng",
      "Yinghao Xu",
      "Zifan Shi",
      "Yifei Zhang",
      "Qifeng Chen",
      "Yujun Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Deformable_Radial_Kernel_Splatting_CVPR_2025_paper.html": {
    "title": "Deformable Radial Kernel Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Hua Huang",
      "Ming-Xian Lin",
      "Yang-Tian Sun",
      "Ziyi Yang",
      "Xiaoyang Lyu",
      "Yan-Pei Cao",
      "Xiaojuan Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Choi_GOAL_Global-local_Object_Alignment_Learning_CVPR_2025_paper.html": {
    "title": "GOAL: Global-local Object Alignment Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyungyu Choi",
      "Young Kyun Jang",
      "Chanho Eom"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qu_Bayesian_Prompt_Flow_Learning_for_Zero-Shot_Anomaly_Detection_CVPR_2025_paper.html": {
    "title": "Bayesian Prompt Flow Learning for Zero-Shot Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Qu",
      "Xian Tao",
      "Xinyi Gong",
      "ShiChen Qu",
      "Qiyu Chen",
      "Zhengtao Zhang",
      "Xingang Wang",
      "Guiguang Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yamaguchi_Post-pre-training_for_Modality_Alignment_in_Vision-Language_Foundation_Models_CVPR_2025_paper.html": {
    "title": "Post-pre-training for Modality Alignment in Vision-Language Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shin'ya Yamaguchi",
      "Dewei Feng",
      "Sekitoshi Kanai",
      "Kazuki Adachi",
      "Daiki Chijiwa"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ahmed_Efficient_Event-Based_Object_Detection_A_Hybrid_Neural_Network_with_Spatial_CVPR_2025_paper.html": {
    "title": "Efficient Event-Based Object Detection: A Hybrid Neural Network with Spatial and Temporal Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soikat Hasan Ahmed",
      "Jan Finkbeiner",
      "Emre Neftci"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chaturvedi_SynthLight_Portrait_Relighting_with_Diffusion_Model_by_Learning_to_Re-render_CVPR_2025_paper.html": {
    "title": "SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sumit Chaturvedi",
      "Mengwei Ren",
      "Yannick Hold-Geoffroy",
      "Jingyuan Liu",
      "Julie Dorsey",
      "Zhixin Shu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Pseudo_Visible_Feature_Fine-Grained_Fusion_for_Thermal_Object_Detection_CVPR_2025_paper.html": {
    "title": "Pseudo Visible Feature Fine-Grained Fusion for Thermal Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting Li",
      "Mao Ye",
      "Tianwen Wu",
      "Nianxin Li",
      "Shuaifeng Li",
      "Song Tang",
      "Luping Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_HUNet_Homotopy_Unfolding_Network_for_Image_Compressive_Sensing_CVPR_2025_paper.html": {
    "title": "HUNet: Homotopy Unfolding Network for Image Compressive Sensing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feiyang Shen",
      "Hongping Gan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_HalLoc_Token-level_Localization_of_Hallucinations_for_Vision_Language_Models_CVPR_2025_paper.html": {
    "title": "HalLoc: Token-level Localization of Hallucinations for Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eunkyu Park",
      "Minyeong Kim",
      "Gunhee Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gu_DiffPortrait360_Consistent_Portrait_Diffusion_for_360_View_Synthesis_CVPR_2025_paper.html": {
    "title": "DiffPortrait360: Consistent Portrait Diffusion for 360 View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuming Gu",
      "Phong Tran",
      "Yujian Zheng",
      "Hongyi Xu",
      "Heyuan Li",
      "Adilbek Karmanov",
      "Hao Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity_CVPR_2025_paper.html": {
    "title": "SURGEON: Memory-Adaptive Fully Test-Time Adaptation via Dynamic Activation Sparsity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Ma",
      "Jiaqi Tang",
      "Bin Guo",
      "Fan Dang",
      "Sicong Liu",
      "Zhui Zhu",
      "Lei Wu",
      "Cheng Fang",
      "Ying-Cong Chen",
      "Zhiwen Yu",
      "Yunhao Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_NVILA_Efficient_Frontier_Visual_Language_Models_CVPR_2025_paper.html": {
    "title": "NVILA: Efficient Frontier Visual Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijian Liu",
      "Ligeng Zhu",
      "Baifeng Shi",
      "Zhuoyang Zhang",
      "Yuming Lou",
      "Shang Yang",
      "Haocheng Xi",
      "Shiyi Cao",
      "Yuxian Gu",
      "Dacheng Li",
      "Xiuyu Li",
      "Haotian Tang",
      "Yunhao Fang",
      "Yukang Chen",
      "Cheng-Yu Hsieh",
      "De-An Huang",
      "An-Chieh Cheng",
      "Jinyi Hu",
      "Sifei Liu",
      "Ranjay Krishna",
      "Pavlo Molchanov",
      "Jan Kautz",
      "Hongxu Yin",
      "Song Han",
      "Yao Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_SemiETS_Integrating_Spatial_and_Content_Consistencies_for_Semi-Supervised_End-to-end_Text_CVPR_2025_paper.html": {
    "title": "SemiETS: Integrating Spatial and Content Consistencies for Semi-Supervised End-to-end Text Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongliang Luo",
      "Hanshen Zhu",
      "Ziyang Zhang",
      "Dingkang Liang",
      "Xudong Xie",
      "Yuliang Liu",
      "Xiang Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_See_Further_When_Clear_Curriculum_Consistency_Model_CVPR_2025_paper.html": {
    "title": "See Further When Clear: Curriculum Consistency Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunpeng Liu",
      "Boxiao Liu",
      "Yi Zhang",
      "Xingzhong Hou",
      "Guanglu Song",
      "Yu Liu",
      "Haihang You"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_From_Slow_Bidirectional_to_Fast_Autoregressive_Video_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianwei Yin",
      "Qiang Zhang",
      "Richard Zhang",
      "William T. Freeman",
      "Fredo Durand",
      "Eli Shechtman",
      "Xun Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_PassionSR_Post-Training_Quantization_with_Adaptive_Scale_in_One-Step_Diffusion_based_CVPR_2025_paper.html": {
    "title": "PassionSR: Post-Training Quantization with Adaptive Scale in One-Step Diffusion based Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Libo Zhu",
      "Jianze Li",
      "Haotong Qin",
      "Wenbo Li",
      "Yulun Zhang",
      "Yong Guo",
      "Xiaokang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dai_RainyGS_Efficient_Rain_Synthesis_with_Physically-Based_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "RainyGS: Efficient Rain Synthesis with Physically-Based Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyu Dai",
      "Xingyu Ni",
      "Qianfan Shen",
      "Wenzheng Chen",
      "Baoquan Chen",
      "Mengyu Chu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Miao_Noise_Diffusion_for_Enhancing_Semantic_Faithfulness_in_Text-to-Image_Synthesis_CVPR_2025_paper.html": {
    "title": "Noise Diffusion for Enhancing Semantic Faithfulness in Text-to-Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boming Miao",
      "Chunxiao Li",
      "Xiaoxiao Wang",
      "Andi Zhang",
      "Rui Sun",
      "Zizhe Wang",
      "Yao Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_MonoInstance_Enhancing_Monocular_Priors_via_Multi-view_Instance_Alignment_for_Neural_CVPR_2025_paper.html": {
    "title": "MonoInstance: Enhancing Monocular Priors via Multi-view Instance Alignment for Neural Rendering and Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyuan Zhang",
      "Yixiao Yang",
      "Han Huang",
      "Liang Han",
      "Kanle Shi",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ding_Three-view_Focal_Length_Recovery_From_Homographies_CVPR_2025_paper.html": {
    "title": "Three-view Focal Length Recovery From Homographies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaqing Ding",
      "Viktor Kocur",
      "Zuzana Berger Haladova",
      "Qianliang Wu",
      "Shen Cai",
      "Jian Yang",
      "Zuzana Kukelova"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_NoPain_No-box_Point_Cloud_Attack_via_Optimal_Transport_Singular_Boundary_CVPR_2025_paper.html": {
    "title": "NoPain: No-box Point Cloud Attack via Optimal Transport Singular Boundary",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zezeng Li",
      "Xiaoyu Du",
      "Na Lei",
      "Liming Chen",
      "Weimin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hao_RAP_Retrieval-Augmented_Personalization_for_Multimodal_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "RAP: Retrieval-Augmented Personalization for Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Hao",
      "Jiaming Han",
      "Changsheng Li",
      "Yu-Feng Li",
      "Xiangyu Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhong_FADA_Fast_Diffusion_Avatar_Synthesis_with_Mixed-Supervised_Multi-CFG_Distillation_CVPR_2025_paper.html": {
    "title": "FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyun Zhong",
      "Chao Liang",
      "Jianwen Jiang",
      "Gaojie Lin",
      "Jiaqi Yang",
      "Zhou Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rundi Wu",
      "Ruiqi Gao",
      "Ben Poole",
      "Alex Trevithick",
      "Changxi Zheng",
      "Jonathan T. Barron",
      "Aleksander Holynski"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Exploring_Semantic_Feature_Discrimination_for_Perceptual_Image_Super-Resolution_and_Opinion-Unaware_CVPR_2025_paper.html": {
    "title": "Exploring Semantic Feature Discrimination for Perceptual Image Super-Resolution and Opinion-Unaware No-Reference Image Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanglu Dong",
      "Xiangyu Liao",
      "Mingyang Li",
      "Guihuan Guo",
      "Chao Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Distilling_Long-tailed_Datasets_CVPR_2025_paper.html": {
    "title": "Distilling Long-tailed Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenghao Zhao",
      "Haoxuan Wang",
      "Yuzhang Shang",
      "Kai Wang",
      "Yan Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders_CVPR_2025_paper.html": {
    "title": "Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fiona Ryan",
      "Ajay Bati",
      "Sangmin Lee",
      "Daniel Bolya",
      "Judy Hoffman",
      "James M. Rehg"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Distilling_Spectral_Graph_for_Object-Context_Aware_Open-Vocabulary_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Distilling Spectral Graph for Object-Context Aware Open-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chanyoung Kim",
      "Dayun Ju",
      "Woojung Han",
      "Ming-Hsuan Yang",
      "Seong Jae Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cui_Incorporating_Dense_Knowledge_Alignment_into_Unified_Multimodal_Representation_Models_CVPR_2025_paper.html": {
    "title": "Incorporating Dense Knowledge Alignment into Unified Multimodal Representation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Cui",
      "Xinxing Zu",
      "Wenhua Zhang",
      "Zhongzhou Zhao",
      "Jinyang Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Geometry_Field_Splatting_with_Gaussian_Surfels_CVPR_2025_paper.html": {
    "title": "Geometry Field Splatting with Gaussian Surfels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiwen Jiang",
      "Venkataram Sivaram",
      "Cheng Peng",
      "Ravi Ramamoorthi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo_CVPR_2025_paper.html": {
    "title": "Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linyi Jin",
      "Richard Tucker",
      "Zhengqi Li",
      "David Fouhey",
      "Noah Snavely",
      "Aleksander Holynski"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kitazawa_PS-EIP_Robust_Photometric_Stereo_Based_on_Event_Interval_Profile_CVPR_2025_paper.html": {
    "title": "PS-EIP: Robust Photometric Stereo Based on Event Interval Profile",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kazuma Kitazawa",
      "Takahito Aoto",
      "Satoshi Ikehata",
      "Tsuyoshi Takatani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_GenPC_Zero-shot_Point_Cloud_Completion_via_3D_Generative_Priors_CVPR_2025_paper.html": {
    "title": "GenPC: Zero-shot Point Cloud Completion via 3D Generative Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "An Li",
      "Zhe Zhu",
      "Mingqiang Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation_CVPR_2025_paper.html": {
    "title": "FoundHand: Large-Scale Domain-Specific Learning for Controllable Hand Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kefan Chen",
      "Chaerin Min",
      "Linguang Zhang",
      "Shreyas Hampali",
      "Cem Keskin",
      "Srinath Sridhar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Akkerman_InterDyn_Controllable_Interactive_Dynamics_with_Video_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "InterDyn: Controllable Interactive Dynamics with Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rick Akkerman",
      "Haiwen Feng",
      "Michael J. Black",
      "Dimitrios Tzionas",
      "Victoria Fernández Abrevaya"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of_CVPR_2025_paper.html": {
    "title": "LLMDet: Learning Strong Open-Vocabulary Object Detectors under the Supervision of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenghao Fu",
      "Qize Yang",
      "Qijie Mo",
      "Junkai Yan",
      "Xihan Wei",
      "Jingke Meng",
      "Xiaohua Xie",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization_CVPR_2025_paper.html": {
    "title": "Boost Your Human Image Generation Model via Direct Preference Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanghyeon Na",
      "Yonggyu Kim",
      "Hyunjoon Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Learning_to_Highlight_Audio_by_Watching_Movies_CVPR_2025_paper.html": {
    "title": "Learning to Highlight Audio by Watching Movies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Huang",
      "Ruohan Gao",
      "J. M. F. Tsang",
      "Jan Kurcius",
      "Cagdas Bilen",
      "Chenliang Xu",
      "Anurag Kumar",
      "Sanjeel Parekh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Capellera_Unified_Uncertainty-Aware_Diffusion_for_Multi-Agent_Trajectory_Modeling_CVPR_2025_paper.html": {
    "title": "Unified Uncertainty-Aware Diffusion for Multi-Agent Trajectory Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillem Capellera",
      "Antonio Rubio",
      "Luis Ferraz",
      "Antonio Agudo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_WeGen_A_Unified_Model_for_Interactive_Multimodal_Generation_as_We_CVPR_2025_paper.html": {
    "title": "WeGen: A Unified Model for Interactive Multimodal Generation as We Chat",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhipeng Huang",
      "Shaobin Zhuang",
      "Canmiao Fu",
      "Binxin Yang",
      "Ying Zhang",
      "Chong Sun",
      "Zhizheng Zhang",
      "Yali Wang",
      "Chen Li",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_HRAvatar_High-Quality_and_Relightable_Gaussian_Head_Avatar_CVPR_2025_paper.html": {
    "title": "HRAvatar: High-Quality and Relightable Gaussian Head Avatar",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongbin Zhang",
      "Yunfei Liu",
      "Lijian Lin",
      "Ye Zhu",
      "Kangjie Chen",
      "Minghan Qin",
      "Yu Li",
      "Haoqian Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis_CVPR_2025_paper.html": {
    "title": "Latent Drifting in Diffusion Models for Counterfactual Medical Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yousef Yeganeh",
      "Azade Farshad",
      "Ioannis Charisiadis",
      "Marta Hasny",
      "Martin Hartenberger",
      "Björn Ommer",
      "Nassir Navab",
      "Ehsan Adeli"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking_CVPR_2025_paper.html": {
    "title": "Rethinking Spiking Self-Attention Mechanism: Implementing a-XNOR Similarity Calculation in Spiking Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichen Xiao",
      "Shuai Wang",
      "Dehao Zhang",
      "Wenjie Wei",
      "Yimeng Shan",
      "Xiaoli Liu",
      "Yulin Jiang",
      "Malu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MagicQuill_An_Intelligent_Interactive_Image_Editing_System_CVPR_2025_paper.html": {
    "title": "MagicQuill: An Intelligent Interactive Image Editing System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichen Liu",
      "Yue Yu",
      "Hao Ouyang",
      "Qiuyu Wang",
      "Ka Leong Cheng",
      "Wen Wang",
      "Zhiheng Liu",
      "Qifeng Chen",
      "Yujun Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_HeMoRa_Unsupervised_Heuristic_Consensus_Sampling_for_Robust_Point_Cloud_Registration_CVPR_2025_paper.html": {
    "title": "HeMoRa: Unsupervised Heuristic Consensus Sampling for Robust Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaocheng Yan",
      "Yiming Wang",
      "Kaiyan Zhao",
      "Pengcheng Shi",
      "Zhenjun Zhao",
      "Yongjun Zhang",
      "Jiayuan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Reducing_Class-wise_Confusion_for_Incremental_Learning_with_Disentangled_Manifolds_CVPR_2025_paper.html": {
    "title": "Reducing Class-wise Confusion for Incremental Learning with Disentangled Manifolds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huitong Chen",
      "Yu Wang",
      "Yan Fan",
      "Guosong Jiang",
      "Qinghua Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces_CVPR_2025_paper.html": {
    "title": "Open-Vocabulary Functional 3D Scene Graphs for Real-World Indoor Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyangguang Zhang",
      "Alexandros Delitzas",
      "Fangjinhua Wang",
      "Ruida Zhang",
      "Xiangyang Ji",
      "Marc Pollefeys",
      "Francis Engelmann"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Boosting_Adversarial_Transferability_through_Augmentation_in_Hypothesis_Space_CVPR_2025_paper.html": {
    "title": "Boosting Adversarial Transferability through Augmentation in Hypothesis Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Guo",
      "Weiquan Liu",
      "Qingshan Xu",
      "Shijun Zheng",
      "Shujun Huang",
      "Yu Zang",
      "Siqi Shen",
      "Chenglu Wen",
      "Cheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_AniMo_Species-Aware_Model_for_Text-Driven_Animal_Motion_Generation_CVPR_2025_paper.html": {
    "title": "AniMo: Species-Aware Model for Text-Driven Animal Motion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Wang",
      "Kai Ruan",
      "Xing Zhang",
      "Gaoang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mu_EditAR_Unified_Conditional_Generation_with_Autoregressive_Models_CVPR_2025_paper.html": {
    "title": "EditAR: Unified Conditional Generation with Autoregressive Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiteng Mu",
      "Nuno Vasconcelos",
      "Xiaolong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Matsuo_Instance-wise_Supervision-level_Optimization_in_Active_Learning_CVPR_2025_paper.html": {
    "title": "Instance-wise Supervision-level Optimization in Active Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shinnosuke Matsuo",
      "Riku Togashi",
      "Ryoma Bise",
      "Seiichi Uchida",
      "Masahiro Nomura"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Djeghim_ViiNeuS_Volumetric_Initialization_for_Implicit_Neural_Surface_Reconstruction_of_Urban_CVPR_2025_paper.html": {
    "title": "ViiNeuS: Volumetric Initialization for Implicit Neural Surface Reconstruction of Urban Scenes with Limited Image Overlap",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hala Djeghim",
      "Nathan Piasco",
      "Moussab Bennehar",
      "Luis Roldao",
      "Dzmitry Tsishkou",
      "Désiré Sidibé"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Model_Diagnosis_and_Correction_via_Linguistic_and_Implicit_Attribute_Editing_CVPR_2025_paper.html": {
    "title": "Model Diagnosis and Correction via Linguistic and Implicit Attribute Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanbai Chen",
      "Xiang Xu",
      "Zhihua Li",
      "Tianchen Zhao",
      "Pietro Perona",
      "Qin Zhang",
      "Yifan Xing"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_BHViT_Binarized_Hybrid_Vision_Transformer_CVPR_2025_paper.html": {
    "title": "BHViT: Binarized Hybrid Vision Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Gao",
      "Yu Zhang",
      "Zhiyuan Zhang",
      "Huajun Liu",
      "Kaijie Yin",
      "Chengzhong Xu",
      "Hui Kong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mittal_UniPhy_Learning_a_Unified_Constitutive_Model_for_Inverse_Physics_Simulation_CVPR_2025_paper.html": {
    "title": "UniPhy: Learning a Unified Constitutive Model for Inverse Physics Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Himangi Mittal",
      "Peiye Zhuang",
      "Hsin-Ying Lee",
      "Shubham Tulsiani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_STAA-SNN_Spatial-Temporal_Attention_Aggregator_for_Spiking_Neural_Networks_CVPR_2025_paper.html": {
    "title": "STAA-SNN: Spatial-Temporal Attention Aggregator for Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianqing Zhang",
      "Kairong Yu",
      "Xian Zhong",
      "Hongwei Wang",
      "Qi Xu",
      "Qiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rotstein_Pathways_on_the_Image_Manifold_Image_Editing_via_Video_Generation_CVPR_2025_paper.html": {
    "title": "Pathways on the Image Manifold: Image Editing via Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noam Rotstein",
      "Gal Yona",
      "Daniel Silver",
      "Roy Velich",
      "David Bensaid",
      "Ron Kimmel"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DeSplat_Decomposed_Gaussian_Splatting_for_Distractor-Free_Rendering_CVPR_2025_paper.html": {
    "title": "DeSplat: Decomposed Gaussian Splatting for Distractor-Free Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihao Wang",
      "Marcus Klasson",
      "Matias Turkulainen",
      "Shuzhe Wang",
      "Juho Kannala",
      "Arno Solin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_Knowledge_Memorization_and_Rumination_for_Pre-trained_Model-based_Class-Incremental_Learning_CVPR_2025_paper.html": {
    "title": "Knowledge Memorization and Rumination for Pre-trained Model-based Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijian Gao",
      "Wangwang Jia",
      "Xingxing Zhang",
      "Dulan Zhou",
      "Kele Xu",
      "Feng Dawei",
      "Yong Dou",
      "Xinjun Mao",
      "Huaimin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Videnovic_A_Distractor-Aware_Memory_for_Visual_Object_Tracking_with_SAM2_CVPR_2025_paper.html": {
    "title": "A Distractor-Aware Memory for Visual Object Tracking with SAM2",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jovana Videnovic",
      "Alan Lukezic",
      "Matej Kristan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_Activating_Sparse_Part_Concepts_for_3D_Class_Incremental_Learning_CVPR_2025_paper.html": {
    "title": "Activating Sparse Part Concepts for 3D Class Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenya Tian",
      "Jun Xiao",
      "Lupeng Liu",
      "Haiyong Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peng_ProxyTransformation_Preshaping_Point_Cloud_Manifold_With_Proxy_Attention_For_3D_CVPR_2025_paper.html": {
    "title": "ProxyTransformation: Preshaping Point Cloud Manifold With Proxy Attention For 3D Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihang Peng",
      "Henry Zheng",
      "Gao Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_BFANet_Revisiting_3D_Semantic_Segmentation_with_Boundary_Feature_Analysis_CVPR_2025_paper.html": {
    "title": "BFANet: Revisiting 3D Semantic Segmentation with Boundary Feature Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiguang Zhao",
      "Rui Zhang",
      "Qiufeng Wang",
      "Guangliang Cheng",
      "Kaizhu Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Avrahami_Stable_Flow_Vital_Layers_for_Training-Free_Image_Editing_CVPR_2025_paper.html": {
    "title": "Stable Flow: Vital Layers for Training-Free Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omri Avrahami",
      "Or Patashnik",
      "Ohad Fried",
      "Egor Nemchinov",
      "Kfir Aberman",
      "Dani Lischinski",
      "Daniel Cohen-Or"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Reddy_Video-ColBERT_Contextualized_Late_Interaction_for_Text-to-Video_Retrieval_CVPR_2025_paper.html": {
    "title": "Video-ColBERT: Contextualized Late Interaction for Text-to-Video Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arun Reddy",
      "Alexander Martin",
      "Eugene Yang",
      "Andrew Yates",
      "Kate Sanders",
      "Kenton Murray",
      "Reno Kriz",
      "Celso M. de Melo",
      "Benjamin Van Durme",
      "Rama Chellappa"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_Beyond_Words_Augmenting_Discriminative_Richness_via_Diffusions_in_Unsupervised_Prompt_CVPR_2025_paper.html": {
    "title": "Beyond Words: Augmenting Discriminative Richness via Diffusions in Unsupervised Prompt Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hairui Ren",
      "Fan Tang",
      "He Zhao",
      "Zixuan Wang",
      "Dandan Guo",
      "Yi Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Unlocking_the_Potential_of_Unlabeled_Data_in_Semi-Supervised_Domain_Generalization_CVPR_2025_paper.html": {
    "title": "Unlocking the Potential of Unlabeled Data in Semi-Supervised Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongkwan Lee",
      "Kyomin Hwang",
      "Nojun Kwak"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_TokenMotion_Decoupled_Motion_Control_via_Token_Disentanglement_for_Human-centric_Video_CVPR_2025_paper.html": {
    "title": "TokenMotion: Decoupled Motion Control via Token Disentanglement for Human-centric Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruineng Li",
      "Daitao Xing",
      "Huiming Sun",
      "Yuanzhou Ha",
      "Jinglin Shen",
      "Chiuman Ho"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nwoye_CholecTrack20_A_Multi-Perspective_Tracking_Dataset_for_Surgical_Tools_CVPR_2025_paper.html": {
    "title": "CholecTrack20: A Multi-Perspective Tracking Dataset for Surgical Tools",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chinedu Innocent Nwoye",
      "Kareem Elgohary",
      "Anvita Srinivas",
      "Fauzan Zaid",
      "Joël L. Lavanchy",
      "Nicolas Padoy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Visual_and_Semantic_Prompt_Collaboration_for_Generalized_Zero-Shot_Learning_CVPR_2025_paper.html": {
    "title": "Visual and Semantic Prompt Collaboration for Generalized Zero-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huajie Jiang",
      "Zhengxian Li",
      "Xiaohan Yu",
      "Yongli Hu",
      "Baocai Yin",
      "Jian Yang",
      "Yuankai Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Steering_Away_from_Harm_An_Adaptive_Approach_to_Defending_Vision_CVPR_2025_paper.html": {
    "title": "Steering Away from Harm: An Adaptive Approach to Defending Vision Language Model Against Jailbreaks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Wang",
      "Gang Wang",
      "Huan Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_Neural_LightRig_Unlocking_Accurate_Object_Normal_and_Material_Estimation_with_CVPR_2025_paper.html": {
    "title": "Neural LightRig: Unlocking Accurate Object Normal and Material Estimation with Multi-Light Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zexin He",
      "Tengfei Wang",
      "Xin Huang",
      "Xingang Pan",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_VidMuse_A_Simple_Video-to-Music_Generation_Framework_with_Long-Short-Term_Modeling_CVPR_2025_paper.html": {
    "title": "VidMuse: A Simple Video-to-Music Generation Framework with Long-Short-Term Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyue Tian",
      "Zhaoyang Liu",
      "Ruibin Yuan",
      "Jiahao Pan",
      "Qifeng Liu",
      "Xu Tan",
      "Qifeng Chen",
      "Wei Xue",
      "Yike Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qin_Human-centered_Interactive_Learning_via_MLLMs_for_Text-to-Image_Person_Re-identification_CVPR_2025_paper.html": {
    "title": "Human-centered Interactive Learning via MLLMs for Text-to-Image Person Re-identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Qin",
      "Chao Chen",
      "Zhihang Fu",
      "Dezhong Peng",
      "Xi Peng",
      "Peng Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cohen_Conditional_Balance_Improving_Multi-Conditioning_Trade-Offs_in_Image_Generation_CVPR_2025_paper.html": {
    "title": "Conditional Balance: Improving Multi-Conditioning Trade-Offs in Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nadav Z. Cohen",
      "Oron Nir",
      "Ariel Shamir"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bigata_KeyFace_Expressive_Audio-Driven_Facial_Animation_for_Long_Sequences_via_KeyFrame_CVPR_2025_paper.html": {
    "title": "KeyFace: Expressive Audio-Driven Facial Animation for Long Sequences via KeyFrame Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoni Bigata",
      "Michał Stypułkowski",
      "Rodrigo Mira",
      "Stella Bounareli",
      "Konstantinos Vougioukas",
      "Zoe Landgraf",
      "Nikita Drobyshev",
      "Maciej Zieba",
      "Stavros Petridis",
      "Maja Pantic"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pang_Context-Enhanced_Memory-Refined_Transformer_for_Online_Action_Detection_CVPR_2025_paper.html": {
    "title": "Context-Enhanced Memory-Refined Transformer for Online Action Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanzhong Pang",
      "Fadime Sener",
      "Angela Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Towards_Natural_Language-Based_Document_Image_Retrieval_New_Dataset_and_Benchmark_CVPR_2025_paper.html": {
    "title": "Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Guo",
      "Xugong Qin",
      "Jun Jie Ou Yang",
      "Peng Zhang",
      "Gangyan Zeng",
      "Yubo Li",
      "Hailun Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Mitigating_Ambiguities_in_3D_Classification_with_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "Mitigating Ambiguities in 3D Classification with Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiqi Zhang",
      "Hao Zhu",
      "Jingyi Zhao",
      "Qi Zhang",
      "Xun Cao",
      "Zhan Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jung_Exposure-slot_Exposure-centric_Representations_Learning_with_Slot-in-Slot_Attention_for_Region-aware_Exposure_CVPR_2025_paper.html": {
    "title": "Exposure-slot: Exposure-centric Representations Learning with Slot-in-Slot Attention for Region-aware Exposure Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donggoo Jung",
      "Daehyun Kim",
      "Guanghui Wang",
      "Tae Hyun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pan_Data-Free_Group-Wise_Fully_Quantized_Winograd_Convolution_via_Learnable_Scales_CVPR_2025_paper.html": {
    "title": "Data-Free Group-Wise Fully Quantized Winograd Convolution via Learnable Scales",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuokai Pan",
      "Gerti Tuzi",
      "Sudarshan Sreeram",
      "Dibakar Gope"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_EdgeDiff_Edge-aware_Diffusion_Network_for_Building_Reconstruction_from_Point_Clouds_CVPR_2025_paper.html": {
    "title": "EdgeDiff: Edge-aware Diffusion Network for Building Reconstruction from Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujun Liu",
      "Ruisheng Wang",
      "Shangfeng Huang",
      "Guorong Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control_CVPR_2025_paper.html": {
    "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanchi Ren",
      "Tianchang Shen",
      "Jiahui Huang",
      "Huan Ling",
      "Yifan Lu",
      "Merlin Nimier-David",
      "Thomas Müller",
      "Alexander Keller",
      "Sanja Fidler",
      "Jun Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Laskar_A_Dataset_for_Semantic_Segmentation_in_the_Presence_of_Unknowns_CVPR_2025_paper.html": {
    "title": "A Dataset for Semantic Segmentation in the Presence of Unknowns",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zakaria Laskar",
      "Tomas Vojir",
      "Matej Grcic",
      "Iaroslav Melekhov",
      "Shankar Gangisetty",
      "Juho Kannala",
      "Jiri Matas",
      "Giorgos Tolias",
      "C.V. Jawahar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Azad_HierarQ_Task-Aware_Hierarchical_Q-Former_for_Enhanced_Video_Understanding_CVPR_2025_paper.html": {
    "title": "HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shehreen Azad",
      "Vibhav Vineet",
      "Yogesh Singh Rawat"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_DeNVeR_Deformable_Neural_Vessel_Representations_for_Unsupervised_Video_Vessel_Segmentation_CVPR_2025_paper.html": {
    "title": "DeNVeR: Deformable Neural Vessel Representations for Unsupervised Video Vessel Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun-Hung Wu",
      "Shih-Hong Chen",
      "Chih-Yao Hu",
      "Hsin-Yu Wu",
      "Kai-Hsin Chen",
      "Yu-You Chen",
      "Chih-Hai Su",
      "Chih-Kuo Lee",
      "Yu-Lun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_DH-Set_Improving_Vision-Language_Alignment_with_Diverse_and_Hybrid_Set-Embeddings_Learning_CVPR_2025_paper.html": {
    "title": "DH-Set: Improving Vision-Language Alignment with Diverse and Hybrid Set-Embeddings Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Zhang",
      "Jingyu Li",
      "Zhe Li",
      "S.Kevin Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hao_Task-Aware_Clustering_for_Prompting_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "Task-Aware Clustering for Prompting Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fusheng Hao",
      "Fengxiang He",
      "Fuxiang Wu",
      "Tichao Wang",
      "Chengqun Song",
      "Jun Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Georg_FSboard_Over_3_Million_Characters_of_ASL_Fingerspelling_Collected_via_CVPR_2025_paper.html": {
    "title": "FSboard: Over 3 Million Characters of ASL Fingerspelling Collected via Smartphones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manfred Georg",
      "Garrett Tanzer",
      "Esha Uboweja",
      "Saad Hassan",
      "Maximus Shengelia",
      "Sam Sepah",
      "Sean Forbes",
      "Thad Starner"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity_CVPR_2025_paper.html": {
    "title": "CASP: Compression of Large Multimodal Models Based on Attention Sparsity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohsen Gholami",
      "Mohammad Akbari",
      "Kevin Cannons",
      "Yong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Duan_UNIC-Adapter_Unified_Image-instruction_Adapter_with_Multi-modal_Transformer_for_Image_Generation_CVPR_2025_paper.html": {
    "title": "UNIC-Adapter: Unified Image-instruction Adapter with Multi-modal Transformer for Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lunhao Duan",
      "Shanshan Zhao",
      "Wenjun Yan",
      "Yinglun Li",
      "Qing-Guo Chen",
      "Zhao Xu",
      "Weihua Luo",
      "Kaifu Zhang",
      "Mingming Gong",
      "Gui-Song Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_Towards_Cost-Effective_Learning_A_Synergy_of_Semi-Supervised_and_Active_Learning_CVPR_2025_paper.html": {
    "title": "Towards Cost-Effective Learning: A Synergy of Semi-Supervised and Active Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianxiang Yin",
      "Ningzhong Liu",
      "Han Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Unveil_Inversion_and_Invariance_in_Flow_Transformer_for_Versatile_Image_CVPR_2025_paper.html": {
    "title": "Unveil Inversion and Invariance in Flow Transformer for Versatile Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengcheng Xu",
      "Boyuan Jiang",
      "Xiaobin Hu",
      "Donghao Luo",
      "Qingdong He",
      "Jiangning Zhang",
      "Chengjie Wang",
      "Yunsheng Wu",
      "Charles Ling",
      "Boyu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D_CVPR_2025_paper.html": {
    "title": "Light Transport-aware Diffusion Posterior Sampling for Single-View Reconstruction of 3D Volumes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ludwic Leonard",
      "Nils Thurey",
      "Rüdiger Westermann"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Assefa_DyCON_Dynamic_Uncertainty-aware_Consistency_and_Contrastive_Learning_for_Semi-supervised_Medical_CVPR_2025_paper.html": {
    "title": "DyCON: Dynamic Uncertainty-aware Consistency and Contrastive Learning for Semi-supervised Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maregu Assefa",
      "Muzammal Naseer",
      "Iyyakutti Iyappan Ganapathi",
      "Syed Sadaf Ali",
      "Mohamed L Seghier",
      "Naoufel Werghi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Du_STiL_Semi-supervised_Tabular-Image_Learning_for_Comprehensive_Task-Relevant_Information_Exploration_in_CVPR_2025_paper.html": {
    "title": "STiL: Semi-supervised Tabular-Image Learning for Comprehensive Task-Relevant Information Exploration in Multimodal Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyi Du",
      "Xinzhe Luo",
      "Declan P. O'Regan",
      "Chen Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sariyildiz_DUNE_Distilling_a_Universal_Encoder_from_Heterogeneous_2D_and_3D_CVPR_2025_paper.html": {
    "title": "DUNE: Distilling a Universal Encoder from Heterogeneous 2D and 3D Teachers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mert Bülent Sarıyıldız",
      "Philippe Weinzaepfel",
      "Thomas Lucas",
      "Pau de Jorge",
      "Diane Larlus",
      "Yannis Kalantidis"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shaheryar_Black_Hole-Driven_Identity_Absorbing_in_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Black Hole-Driven Identity Absorbing in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Shaheryar",
      "Jong Taek Lee",
      "Soon Ki Jung"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_HiRes-LLaVA_Restoring_Fragmentation_Input_in_High-Resolution_Large_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "HiRes-LLaVA: Restoring Fragmentation Input in High-Resolution Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runhui Huang",
      "Xinpeng Ding",
      "Chunwei Wang",
      "Jianhua Han",
      "Yulong Liu",
      "Hengshuang Zhao",
      "Hang Xu",
      "Lu Hou",
      "Wei Zhang",
      "Xiaodan Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cui_Hallo3_Highly_Dynamic_and_Realistic_Portrait_Image_Animation_with_Video_CVPR_2025_paper.html": {
    "title": "Hallo3: Highly Dynamic and Realistic Portrait Image Animation with Video Diffusion Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Cui",
      "Hui Li",
      "Yun Zhan",
      "Hanlin Shang",
      "Kaihui Cheng",
      "Yuqi Ma",
      "Shan Mu",
      "Hang Zhou",
      "Jingdong Wang",
      "Siyu Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis_CVPR_2025_paper.html": {
    "title": "Generative Photography: Scene-Consistent Camera Control for Realistic Text-to-Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Yuan",
      "Xijun Wang",
      "Yichen Sheng",
      "Prateek Chennuri",
      "Xingguang Zhang",
      "Stanley Chan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Advancing_Manga_Analysis_Comprehensive_Segmentation_Annotations_for_the_Manga109_Dataset_CVPR_2025_paper.html": {
    "title": "Advancing Manga Analysis: Comprehensive Segmentation Annotations for the Manga109 Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minshan Xie",
      "Jian Lin",
      "Hanyuan Liu",
      "Chengze Li",
      "Tien-Tsin Wong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SeqMvRL_A_Sequential_Fusion_Framework_for_Multi-view_Representation_Learning_CVPR_2025_paper.html": {
    "title": "SeqMvRL: A Sequential Fusion Framework for Multi-view Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ren Wang",
      "Haoliang Sun",
      "Yuxiu Lin",
      "Chuanhui Zuo",
      "Yongshun Gong",
      "Yilong Yin",
      "Wenjia Meng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Auto_Cherry-Picker_Learning_from_High-quality_Generative_Data_Driven_by_Language_CVPR_2025_paper.html": {
    "title": "Auto Cherry-Picker: Learning from High-quality Generative Data Driven by Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yicheng Chen",
      "Xiangtai Li",
      "Yining Li",
      "Yanhong Zeng",
      "Jianzong Wu",
      "Xiangyu Zhao",
      "Kai Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_EnvGS_Modeling_View-Dependent_Appearance_with_Environment_Gaussian_CVPR_2025_paper.html": {
    "title": "EnvGS: Modeling View-Dependent Appearance with Environment Gaussian",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Xie",
      "Xi Chen",
      "Zhen Xu",
      "Yiman Xie",
      "Yudong Jin",
      "Yujun Shen",
      "Sida Peng",
      "Hujun Bao",
      "Xiaowei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Provoking_Multi-modal_Few-Shot_LVLM_via_Exploration-Exploitation_In-Context_Learning_CVPR_2025_paper.html": {
    "title": "Provoking Multi-modal Few-Shot LVLM via Exploration-Exploitation In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Chen",
      "Yunpeng Zhai",
      "Yifan Zhao",
      "Jinyang Gao",
      "Bolin Ding",
      "Jia Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yuan_BadToken_Token-level_Backdoor_Attacks_to_Multi-modal_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "BadToken: Token-level Backdoor Attacks to Multi-modal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zenghui Yuan",
      "Jiawen Shi",
      "Pan Zhou",
      "Neil Zhenqiang Gong",
      "Lichao Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Berdan_ReRAW_RGB-to-RAW_Image_Reconstruction_via_Stratified_Sampling_for_Efficient_Object_CVPR_2025_paper.html": {
    "title": "ReRAW: RGB-to-RAW Image Reconstruction via Stratified Sampling for Efficient Object Detection on the Edge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Radu Berdan",
      "Beril Besbinar",
      "Christoph Reinders",
      "Junji Otsuka",
      "Daisuke Iso"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_VLMs-Guided_Representation_Distillation_for_Efficient_Vision-Based_Reinforcement_Learning_CVPR_2025_paper.html": {
    "title": "VLMs-Guided Representation Distillation for Efficient Vision-Based Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Xu",
      "Peixi Peng",
      "Guang Tan",
      "Yiqian Chang",
      "Luntong Li",
      "Yonghong Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pu_MonoDGP_Monocular_3D_Object_Detection_with_Decoupled-Query_and_Geometry-Error_Priors_CVPR_2025_paper.html": {
    "title": "MonoDGP: Monocular 3D Object Detection with Decoupled-Query and Geometry-Error Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanqi Pu",
      "Yifan Wang",
      "Jiru Deng",
      "Wenming Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_NeISF_Neural_Incident_Stokes_Field_for_Polarized_Inverse_Rendering_of_CVPR_2025_paper.html": {
    "title": "NeISF++: Neural Incident Stokes Field for Polarized Inverse Rendering of Conductors and Dielectrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhao Li",
      "Taishi Ono",
      "Takeshi Uemori",
      "Sho Nitta",
      "Hajime Mihara",
      "Alexander Gatto",
      "Hajime Nagahara",
      "Yusuke Moriuchi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_HunyuanPortrait_Implicit_Condition_Control_for_Enhanced_Portrait_Animation_CVPR_2025_paper.html": {
    "title": "HunyuanPortrait: Implicit Condition Control for Enhanced Portrait Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zunnan Xu",
      "Zhentao Yu",
      "Zixiang Zhou",
      "Jun Zhou",
      "Xiaoyu Jin",
      "Fa-ting Hong",
      "Xiaozhong Ji",
      "Junwei Zhu",
      "Chengfei Cai",
      "Shiyu Tang",
      "Qin Lin",
      "Xiu Li",
      "Qinglin Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Flexible_Group_Count_Enables_Hassle-Free_Structured_Pruning_CVPR_2025_paper.html": {
    "title": "Flexible Group Count Enables Hassle-Free Structured Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiamu Zhang",
      "Shaochen Zhong",
      "Andrew Ye",
      "Zirui Liu",
      "Sebastian Zhao",
      "Kaixiong Zhou",
      "Li Li",
      "Soo-Hyun Choi",
      "Rui Chen",
      "Xia Hu",
      "Shuai Xu",
      "Vipin Chaudhary"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_EasyCraft_A_Robust_and_Efficient_Framework_for_Automatic_Avatar_Crafting_CVPR_2025_paper.html": {
    "title": "EasyCraft: A Robust and Efficient Framework for Automatic Avatar Crafting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suzhen Wang",
      "Weijie Chen",
      "Wei Zhang",
      "Minda Zhao",
      "Lincheng Li",
      "Rongsheng Zhang",
      "Zhipeng Hu",
      "Xin Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_MeshArt_Generating_Articulated_Meshes_with_Structure-Guided_Transformers_CVPR_2025_paper.html": {
    "title": "MeshArt: Generating Articulated Meshes with Structure-Guided Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daoyi Gao",
      "Yawar Siddiqui",
      "Lei Li",
      "Angela Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Non-Natural_Image_Understanding_with_Advancing_Frequency-based_Vision_Encoders_CVPR_2025_paper.html": {
    "title": "Non-Natural Image Understanding with Advancing Frequency-based Vision Encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wang Lin",
      "QingSong Wang",
      "Yueying Feng",
      "Shulei Wang",
      "Tao Jin",
      "Zhou Zhao",
      "Fei Wu",
      "Chang Yao",
      "Jingyuan Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens_CVPR_2025_paper.html": {
    "title": "Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaihang Pan",
      "Wang Lin",
      "Zhongqi Yue",
      "Tenglong Ao",
      "Liyu Jia",
      "Wei Zhao",
      "Juncheng Li",
      "Siliang Tang",
      "Hanwang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field_CVPR_2025_paper.html": {
    "title": "SplatFlow: Self-Supervised Dynamic Gaussian Splatting in Neural Motion Flow Field for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Su Sun",
      "Cheng Zhao",
      "Zhuoyang Sun",
      "Yingjie Victor Chen",
      "Mei Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Zero-shot_3D_Question_Answering_via_Voxel-based_Dynamic_Token_Compression_CVPR_2025_paper.html": {
    "title": "Zero-shot 3D Question Answering via Voxel-based Dynamic Token Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hsiang-Wei Huang",
      "Fu-Chen Chen",
      "Wenhao Chai",
      "Che-Chun Su",
      "Lu Xia",
      "Sanghun Jung",
      "Cheng-Yen Yang",
      "Jenq-Neng Hwang",
      "Min Sun",
      "Cheng-Hao Kuo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Patnaik_AesthetiQ_Enhancing_Graphic_Layout_Design_via_Aesthetic-Aware_Preference_Alignment_of_CVPR_2025_paper.html": {
    "title": "AesthetiQ: Enhancing Graphic Layout Design via Aesthetic-Aware Preference Alignment of Multi-modal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sohan Patnaik",
      "Rishabh Jain",
      "Balaji Krishnamurthy",
      "Mausoom Sarkar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Enhanced_then_Progressive_Fusion_with_View_Graph_for_Multi-View_Clustering_CVPR_2025_paper.html": {
    "title": "Enhanced then Progressive Fusion with View Graph for Multi-View Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhibin Dong",
      "Meng Liu",
      "Siwei Wang",
      "Ke Liang",
      "Yi Zhang",
      "Suyuan Liu",
      "Jiaqi Jin",
      "Xinwang Liu",
      "En Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hua_FINECAPTION_Compositional_Image_Captioning_Focusing_on_Wherever_You_Want_at_CVPR_2025_paper.html": {
    "title": "FINECAPTION: Compositional Image Captioning Focusing on Wherever You Want at Any Granularity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Hua",
      "Qing Liu",
      "Lingzhi Zhang",
      "Jing Shi",
      "Soo Ye Kim",
      "Zhifei Zhang",
      "Yilin Wang",
      "Jianming Zhang",
      "Zhe Lin",
      "Jiebo Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Adaptive_Non-Uniform_Timestep_Sampling_for_Accelerating_Diffusion_Model_Training_CVPR_2025_paper.html": {
    "title": "Adaptive Non-Uniform Timestep Sampling for Accelerating Diffusion Model Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Myunsoo Kim",
      "Donghyeon Ki",
      "Seong-Woong Shim",
      "Byung-Jun Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Evani_Chebyshev_Attention_Depth_Permutation_Texture_Network_with_Latent_Texture_Attribute_CVPR_2025_paper.html": {
    "title": "Chebyshev Attention Depth Permutation Texture Network with Latent Texture Attribute Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ravishankar Evani",
      "Deepu Rajan",
      "Shangbo Mao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Explainable_Saliency_Articulating_Reasoning_with_Contextual_Prioritization_CVPR_2025_paper.html": {
    "title": "Explainable Saliency: Articulating Reasoning with Contextual Prioritization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nuo Chen",
      "Ming Jiang",
      "Qi Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/McAllister_Decentralized_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Decentralized Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David McAllister",
      "Matthew Tancik",
      "Jiaming Song",
      "Angjoo Kanazawa"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea_CVPR_2025_paper.html": {
    "title": "AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qifan Yu",
      "Wei Chow",
      "Zhongqi Yue",
      "Kaihang Pan",
      "Yang Wu",
      "Xiaoyang Wan",
      "Juncheng Li",
      "Siliang Tang",
      "Hanwang Zhang",
      "Yueting Zhuang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Parihar_Compass_Control_Multi_Object_Orientation_Control_for_Text-to-Image_Generation_CVPR_2025_paper.html": {
    "title": "Compass Control: Multi Object Orientation Control for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishubh Parihar",
      "Vaibhav Agrawal",
      "Sachidanand VS",
      "Venkatesh Babu Radhakrishnan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Beizaee_Correcting_Deviations_from_Normality_A_Reformulated_Diffusion_Model_for_Multi-Class_CVPR_2025_paper.html": {
    "title": "Correcting Deviations from Normality: A Reformulated Diffusion Model for Multi-Class Unsupervised Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Farzad Beizaee",
      "Gregory A. Lodygensky",
      "Christian Desrosiers",
      "Jose Dolz"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Continuous_3D_Perception_Model_with_Persistent_State_CVPR_2025_paper.html": {
    "title": "Continuous 3D Perception Model with Persistent State",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianqian Wang",
      "Yifei Zhang",
      "Aleksander Holynski",
      "Alexei A. Efros",
      "Angjoo Kanazawa"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate_CVPR_2025_paper.html": {
    "title": "LP-Diff: Towards Improved Restoration of Real-World Degraded License Plate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyan Gong",
      "Zhenrong Zhang",
      "Yuzheng Feng",
      "Anh Nguyen",
      "Hongbin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Unleashing_the_Potential_of_Consistency_Learning_for_Detecting_and_Grounding_CVPR_2025_paper.html": {
    "title": "Unleashing the Potential of Consistency Learning for Detecting and Grounding Multi-Modal Media Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiheng Li",
      "Yang Yang",
      "Zichang Tan",
      "Huan Liu",
      "Weihua Chen",
      "Xu Zhou",
      "Zhen Lei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields_CVPR_2025_paper.html": {
    "title": "DNF: Unconditional 4D Generation with Dictionary-based Neural Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyi Zhang",
      "Naiqi Li",
      "Angela Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation_CVPR_2025_paper.html": {
    "title": "ARM: Appearance Reconstruction Model for Relightable 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Feng",
      "Chang Yu",
      "Zoubin Bi",
      "Yintong Shang",
      "Feng Gao",
      "Hongzhi Wu",
      "Kun Zhou",
      "Chenfanfu Jiang",
      "Yin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Vogel_VideoGEM_Training-free_Action_Grounding_in_Videos_CVPR_2025_paper.html": {
    "title": "VideoGEM: Training-free Action Grounding in Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Vogel",
      "Walid Bousselham",
      "Anna Kukleva",
      "Nina Shvetsova",
      "Hilde Kuehne"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_FilmComposer_LLM-Driven_Music_Production_for_Silent_Film_Clips_CVPR_2025_paper.html": {
    "title": "FilmComposer: LLM-Driven Music Production for Silent Film Clips",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhifeng Xie",
      "Qile He",
      "Youjia Zhu",
      "Qiwei He",
      "Mengtian Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zong_Ground-V_Teaching_VLMs_to_Ground_Complex_Instructions_in_Pixels_CVPR_2025_paper.html": {
    "title": "Ground-V: Teaching VLMs to Ground Complex Instructions in Pixels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongshuo Zong",
      "Qin Zhang",
      "Dongsheng An",
      "Zhihua Li",
      "Xiang Xu",
      "Linghan Xu",
      "Zhuowen Tu",
      "Yifan Xing",
      "Onkar Dabeer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model_CVPR_2025_paper.html": {
    "title": "Structure-from-Motion with a Non-Parametric Camera Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihan Wang",
      "Linfei Pan",
      "Marc Pollefeys",
      "Viktor Larsson"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using_CVPR_2025_paper.html": {
    "title": "EventPSR: Surface Normal and Reflectance Estimation from Photometric Stereo Using an Event Camera",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bohan Yu",
      "Jin Han",
      "Boxin Shi",
      "Imari Sato"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_LAL_Enhancing_3D_Human_Motion_Prediction_with_Latency-aware_Auxiliary_Learning_CVPR_2025_paper.html": {
    "title": "LAL: Enhancing 3D Human Motion Prediction with Latency-aware Auxiliary Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoning Sun",
      "Dong Wei",
      "Huaijiang Sun",
      "Shengxiang Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wan_CASP_Consistency-aware_Audio-induced_Saliency_Prediction_Model_for_Omnidirectional_Video_CVPR_2025_paper.html": {
    "title": "CASP: Consistency-aware Audio-induced Saliency Prediction Model for Omnidirectional Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaolin Wan",
      "Han Qin",
      "Zhiyang Li",
      "Xiaopeng Fan",
      "Wangmeng Zuo",
      "Debin Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lionar_TreeMeshGPT_Artistic_Mesh_Generation_with_Autoregressive_Tree_Sequencing_CVPR_2025_paper.html": {
    "title": "TreeMeshGPT: Artistic Mesh Generation with Autoregressive Tree Sequencing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Lionar",
      "Jiabin Liang",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_RefPose_Leveraging_Reference_Geometric_Correspondences_for_Accurate_6D_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "RefPose: Leveraging Reference Geometric Correspondences for Accurate 6D Pose Estimation of Unseen Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaeguk Kim",
      "Jaewoo Park",
      "Keuntek Lee",
      "Nam Ik Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Relation3D__Enhancing_Relation_Modeling_for_Point_Cloud_Instance_Segmentation_CVPR_2025_paper.html": {
    "title": "Relation3D : Enhancing Relation Modeling for Point Cloud Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Lu",
      "Jiacheng Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liao_Shape_My_Moves_Text-Driven_Shape-Aware_Synthesis_of_Human_Motions_CVPR_2025_paper.html": {
    "title": "Shape My Moves: Text-Driven Shape-Aware Synthesis of Human Motions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting-Hsuan Liao",
      "Yi Zhou",
      "Yu Shen",
      "Chun-Hao Paul Huang",
      "Saayan Mitra",
      "Jia-Bin Huang",
      "Uttaran Bhattacharya"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chou_Generating_3D-Consistent_Videos_from_Unposed_Internet_Photos_CVPR_2025_paper.html": {
    "title": "Generating 3D-Consistent Videos from Unposed Internet Photos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gene Chou",
      "Kai Zhang",
      "Sai Bi",
      "Hao Tan",
      "Zexiang Xu",
      "Fujun Luan",
      "Bharath Hariharan",
      "Noah Snavely"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Gazing_at_Rewards_Eye_Movements_as_a_Lens_into_Human_CVPR_2025_paper.html": {
    "title": "Gazing at Rewards: Eye Movements as a Lens into Human and AI Decision-Making in Hybrid Visual Foraging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Wang",
      "Dingwei Tan",
      "Yen-Ling Kuo",
      "Zhaowei Sun",
      "Jeremy M. Wolfe",
      "Tat-Jen Cham",
      "Mengmi Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_FOCUS_Knowledge-enhanced_Adaptive_Visual_Compression_for_Few-shot_Whole_Slide_Image_CVPR_2025_paper.html": {
    "title": "FOCUS: Knowledge-enhanced Adaptive Visual Compression for Few-shot Whole Slide Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengrui Guo",
      "Conghao Xiong",
      "Jiabo Ma",
      "Qichen Sun",
      "Lishuang Feng",
      "Jinzhuo Wang",
      "Hao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Beyond_Human_Perception_Understanding_Multi-Object_World_from_Monocular_View_CVPR_2025_paper.html": {
    "title": "Beyond Human Perception: Understanding Multi-Object World from Monocular View",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keyu Guo",
      "Yongle Huang",
      "Shijie Sun",
      "Xiangyu Song",
      "Mingtao Feng",
      "Zedong Liu",
      "Huansheng Song",
      "Tiantian Wang",
      "Jianxin Li",
      "Naveed Akhtar",
      "Ajmal Saeed Mian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_GRAE-3DMOT_Geometry_Relation-Aware_Encoder_for_Online_3D_Multi-Object_Tracking_CVPR_2025_paper.html": {
    "title": "GRAE-3DMOT: Geometry Relation-Aware Encoder for Online 3D Multi-Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunseop Kim",
      "Hyo-Jun Lee",
      "Yonguk Lee",
      "Jinu Lee",
      "Hanul Kim",
      "Yeong Jun Koh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qu_Automatic_Joint_Structured_Pruning_and_Quantization_for_Efficient_Neural_Network_CVPR_2025_paper.html": {
    "title": "Automatic Joint Structured Pruning and Quantization for Efficient Neural Network Training and Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyi Qu",
      "David Aponte",
      "Colby Banbury",
      "Daniel P. Robinson",
      "Tianyu Ding",
      "Kazuhito Koishida",
      "Ilya Zharkov",
      "Tianyi Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ham_Parameter_Efficient_Mamba_Tuning_via_Projector-targeted_Diagonal-centric_Linear_Transformation_CVPR_2025_paper.html": {
    "title": "Parameter Efficient Mamba Tuning via Projector-targeted Diagonal-centric Linear Transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seokil Ham",
      "Hee-Seon Kim",
      "Sangmin Woo",
      "Changick Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Panagopoulou_ViUniT_Visual_Unit_Tests_for_More_Robust_Visual_Programming_CVPR_2025_paper.html": {
    "title": "ViUniT: Visual Unit Tests for More Robust Visual Programming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artemis Panagopoulou",
      "Honglu Zhou",
      "Silvio Savarese",
      "Caiming Xiong",
      "Chris Callison-Burch",
      "Mark Yatskar",
      "Juan Carlos Niebles"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_LIRM_Large_Inverse_Rendering_Model_for_Progressive_Reconstruction_of_Shape_CVPR_2025_paper.html": {
    "title": "LIRM: Large Inverse Rendering Model for Progressive Reconstruction of Shape, Materials and View-dependent Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengqin Li",
      "Dilin Wang",
      "Ka Chen",
      "Zhaoyang Lv",
      "Thu Nguyen-Phuoc",
      "Milim Lee",
      "Jia-Bin Huang",
      "Lei Xiao",
      "Yufeng Zhu",
      "Carl S. Marshall",
      "Yuheng Ren",
      "Richard Newcombe",
      "Zhao Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peng_DualTalk_Dual-Speaker_Interaction_for_3D_Talking_Head_Conversations_CVPR_2025_paper.html": {
    "title": "DualTalk: Dual-Speaker Interaction for 3D Talking Head Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqiao Peng",
      "Yanbo Fan",
      "Haoyu Wu",
      "Xuan Wang",
      "Hongyan Liu",
      "Jun He",
      "Zhaoxin Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation_CVPR_2025_paper.html": {
    "title": "MAR-3D: Progressive Masked Auto-regressor for High-Resolution 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinnan Chen",
      "Lingting Zhu",
      "Zeyu Hu",
      "Shengju Qian",
      "Yugang Chen",
      "Xin Wang",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_beta-FFT_Nonlinear_Interpolation_and_Differentiated_Training_Strategies_for_Semi-Supervised_Medical_CVPR_2025_paper.html": {
    "title": "beta-FFT: Nonlinear Interpolation and Differentiated Training Strategies for Semi-Supervised Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Hu",
      "Jianfu Yin",
      "Zhuangzhuang Ma",
      "Jianheng Ma",
      "Feiyu Zhu",
      "Bingbing Wu",
      "Ya Wen",
      "Meng Wu",
      "Cong Hu",
      "Bingliang Hu",
      "Quan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Smadar_Dynamic_Group_Normalization_Spatio-Temporal_Adaptation_to_Evolving_Data_Statistics_CVPR_2025_paper.html": {
    "title": "Dynamic Group Normalization: Spatio-Temporal Adaptation to Evolving Data Statistics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yair Smadar",
      "Assaf Hoogi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jeong_Latent_Space_Super-Resolution_for_Higher-Resolution_Image_Generation_with_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Latent Space Super-Resolution for Higher-Resolution Image Generation with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinho Jeong",
      "Sangmin Han",
      "Jinwoo Kim",
      "Seon Joo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_SynerGen-VL_Towards_Synergistic_Image_Understanding_and_Generation_with_Vision_Experts_CVPR_2025_paper.html": {
    "title": "SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Li",
      "Changyao Tian",
      "Jie Shao",
      "Xizhou Zhu",
      "Zhaokai Wang",
      "Jinguo Zhu",
      "Wenhan Dou",
      "Xiaogang Wang",
      "Hongsheng Li",
      "Lewei Lu",
      "Jifeng Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zielonka_Synthetic_Prior_for_Few-Shot_Drivable_Head_Avatar_Inversion_CVPR_2025_paper.html": {
    "title": "Synthetic Prior for Few-Shot Drivable Head Avatar Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wojciech Zielonka",
      "Stephan J. Garbin",
      "Alexandros Lattas",
      "George Kopanas",
      "Paulo Gotardo",
      "Thabo Beeler",
      "Justus Thies",
      "Timo Bolkart"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical_CVPR_2025_paper.html": {
    "title": "Reasoning in Visual Navigation of End-to-end Trained Agents: A Dynamical Systems Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steeven Janny",
      "Hervé Poirier",
      "Leonid Antsfeld",
      "Guillaume Bono",
      "Gianluca Monaci",
      "Boris Chidlovskii",
      "Francesco Giuliari",
      "Alessio Del Bue",
      "Christian Wolf"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lai_Rethinking_Noisy_Video-Text_Retrieval_via_Relation-aware_Alignment_CVPR_2025_paper.html": {
    "title": "Rethinking Noisy Video-Text Retrieval via Relation-aware Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huakai Lai",
      "Guoxin Xiong",
      "Huayu Mai",
      "Xiang Liu",
      "Tianzhu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_DFormerv2_Geometry_Self-Attention_for_RGBD_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "DFormerv2: Geometry Self-Attention for RGBD Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo-Wen Yin",
      "Jiao-Long Cao",
      "Ming-Ming Cheng",
      "Qibin Hou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_Scaling_Vision_Pre-Training_to_4K_Resolution_CVPR_2025_paper.html": {
    "title": "Scaling Vision Pre-Training to 4K Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baifeng Shi",
      "Boyi Li",
      "Han Cai",
      "Yao Lu",
      "Sifei Liu",
      "Marco Pavone",
      "Jan Kautz",
      "Song Han",
      "Trevor Darrell",
      "Pavlo Molchanov",
      "Hongxu Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_GarmentPile_Point-Level_Visual_Affordance_Guided_Retrieval_and_Adaptation_for_Cluttered_CVPR_2025_paper.html": {
    "title": "GarmentPile: Point-Level Visual Affordance Guided Retrieval and Adaptation for Cluttered Garments Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihai Wu",
      "Ziyu Zhu",
      "Yuran Wang",
      "Yue Chen",
      "Jiarui Wang",
      "Hao Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Uncertain_Multimodal_Intention_and_Emotion_Understanding_in_the_Wild_CVPR_2025_paper.html": {
    "title": "Uncertain Multimodal Intention and Emotion Understanding in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qu Yang",
      "Qinghongya Shi",
      "Tongxin Wang",
      "Mang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_GroomLight_Hybrid_Inverse_Rendering_for_Relightable_Human_Hair_Appearance_Modeling_CVPR_2025_paper.html": {
    "title": "GroomLight: Hybrid Inverse Rendering for Relightable Human Hair Appearance Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Zheng",
      "Menglei Chai",
      "Delio Vicini",
      "Yuxiao Zhou",
      "Yinghao Xu",
      "Leonidas Guibas",
      "Gordon Wetzstein",
      "Thabo Beeler"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Improving_Editability_in_Image_Generation_with_Layer-wise_Memory_CVPR_2025_paper.html": {
    "title": "Improving Editability in Image Generation with Layer-wise Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daneul Kim",
      "Jaeah Lee",
      "Jaesik Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Varghese_Sea-ing_in_Low-light_CVPR_2025_paper.html": {
    "title": "Sea-ing in Low-light",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nisha Varghese",
      "A. N. Rajagopalan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VidTwin_Video_VAE_with_Decoupled_Structure_and_Dynamics_CVPR_2025_paper.html": {
    "title": "VidTwin: Video VAE with Decoupled Structure and Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchi Wang",
      "Junliang Guo",
      "Xinyi Xie",
      "Tianyu He",
      "Xu Sun",
      "Jiang Bian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_CL-LoRA_Continual_Low-Rank_Adaptation_for_Rehearsal-Free_Class-Incremental_Learning_CVPR_2025_paper.html": {
    "title": "CL-LoRA: Continual Low-Rank Adaptation for Rehearsal-Free Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangpeng He",
      "Zhihao Duan",
      "Fengqing Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning_CVPR_2025_paper.html": {
    "title": "Generative Modeling of Class Probability for Multi-Modal Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JungKyoo Shin",
      "Bumsoo Kim",
      "Eunwoo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_VisionZip_Longer_is_Better_but_Not_Necessary_in_Vision_Language_CVPR_2025_paper.html": {
    "title": "VisionZip: Longer is Better but Not Necessary in Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Senqiao Yang",
      "Yukang Chen",
      "Zhuotao Tian",
      "Chengyao Wang",
      "Jingyao Li",
      "Bei Yu",
      "Jiaya Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Simplification_Is_All_You_Need_against_Out-of-Distribution_Overconfidence_CVPR_2025_paper.html": {
    "title": "Simplification Is All You Need against Out-of-Distribution Overconfidence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keke Tang",
      "Chao Hou",
      "Weilong Peng",
      "Xiang Fang",
      "Zhize Wu",
      "Yongwei Nie",
      "Wenping Wang",
      "Zhihong Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lv_SpatialDreamer_Self-supervised_Stereo_Video_Synthesis_from_Monocular_Input_CVPR_2025_paper.html": {
    "title": "SpatialDreamer: Self-supervised Stereo Video Synthesis from Monocular Input",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Lv",
      "Yangqi Long",
      "Congzhentao Huang",
      "Cao Li",
      "Chengfei Lv",
      "Hao Ren",
      "Dian Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_LOD-GS_Achieving_Levels_of_Detail_using_Scalable_Gaussian_Soup_CVPR_2025_paper.html": {
    "title": "LOD-GS: Achieving Levels of Detail using Scalable Gaussian Soup",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianxiong Shen",
      "Yue Qian",
      "Xiaohang Zhan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing_CVPR_2025_paper.html": {
    "title": "BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunqi Gu",
      "Ian Huang",
      "Jihyeon Je",
      "Guandao Yang",
      "Leonidas Guibas"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_VoteFlow_Enforcing_Local_Rigidity_in_Self-Supervised_Scene_Flow_CVPR_2025_paper.html": {
    "title": "VoteFlow: Enforcing Local Rigidity in Self-Supervised Scene Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yancong Lin",
      "Shiming Wang",
      "Liangliang Nan",
      "Julian Kooij",
      "Holger Caesar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_The_Devil_is_in_Low-Level_Features_for_Cross-Domain_Few-Shot_Segmentation_CVPR_2025_paper.html": {
    "title": "The Devil is in Low-Level Features for Cross-Domain Few-Shot Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Liu",
      "Yixiong Zou",
      "Yuhua Li",
      "Ruixuan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Design2GarmentCode_Turning_Design_Concepts_to_Tangible_Garments_Through_Program_Synthesis_CVPR_2025_paper.html": {
    "title": "Design2GarmentCode: Turning Design Concepts to Tangible Garments Through Program Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Zhou",
      "Ruiyang Liu",
      "Chen Liu",
      "Gaofeng He",
      "Yong-Lu Li",
      "Xiaogang Jin",
      "Huamin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Uncertainty_Weighted_Gradients_for_Model_Calibration_CVPR_2025_paper.html": {
    "title": "Uncertainty Weighted Gradients for Model Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinxu Lin",
      "Linwei Tao",
      "Minjing Dong",
      "Chang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kwon_Efficient_Dynamic_Scene_Editing_via_4D_Gaussian-based_Static-Dynamic_Separation_CVPR_2025_paper.html": {
    "title": "Efficient Dynamic Scene Editing via 4D Gaussian-based Static-Dynamic Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joohyun Kwon",
      "Hanbyel Cho",
      "Junmo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhong_Unlearning_through_Knowledge_Overwriting_Reversible_Federated_Unlearning_via_Selective_Sparse_CVPR_2025_paper.html": {
    "title": "Unlearning through Knowledge Overwriting: Reversible Federated Unlearning via Selective Sparse Adapter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyi Zhong",
      "Weidong Bao",
      "Ji Wang",
      "Shuai Zhang",
      "Jingxuan Zhou",
      "Lingjuan Lyu",
      "Wei Yang Bryan Lim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_SocialMOIF_Multi-Order_Intention_Fusion_for_Pedestrian_Trajectory_Prediction_CVPR_2025_paper.html": {
    "title": "SocialMOIF: Multi-Order Intention Fusion for Pedestrian Trajectory Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Chen",
      "Xiaodong Zhao",
      "Yujie Huang",
      "Guoyu Fang",
      "Xiao Song",
      "Ruiping Wang",
      "Ziyuan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yun_FFaceNeRF_Few-shot_Face_Editing_in_Neural_Radiance_Fields_CVPR_2025_paper.html": {
    "title": "FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwan Yun",
      "Chaelin Kim",
      "Hangyeul Shin",
      "Junyong Noh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Discrete_to_Continuous_Generating_Smooth_Transition_Poses_from_Sign_Language_CVPR_2025_paper.html": {
    "title": "Discrete to Continuous: Generating Smooth Transition Poses from Sign Language Observations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengeng Tang",
      "Jiayi He",
      "Lechao Cheng",
      "Jingjing Wu",
      "Dan Guo",
      "Richang Hong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Raswa_HistoFS_Non-IID_Histopathologic_Whole_Slide_Image_Classification_via_Federated_Style_CVPR_2025_paper.html": {
    "title": "HistoFS: Non-IID Histopathologic Whole Slide Image Classification via Federated Style Transfer with RoI-Preserving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Farchan Hakim Raswa",
      "Chun-Shien Lu",
      "Jia-Ching Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chang_Unified_Medical_Lesion_Segmentation_via_Self-referring_Indicator_CVPR_2025_paper.html": {
    "title": "Unified Medical Lesion Segmentation via Self-referring Indicator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijie Chang",
      "Xiaoqi Zhao",
      "Lihe Zhang",
      "Tiancheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Galerne_SGSST_Scaling_Gaussian_Splatting_Style_Transfer_CVPR_2025_paper.html": {
    "title": "SGSST: Scaling Gaussian Splatting Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bruno Galerne",
      "Jianling Wang",
      "Lara Raad",
      "Jean-Michel Morel"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Noda_Learning_Bijective_Surface_Parameterization_for_Inferring_Signed_Distance_Functions_from_CVPR_2025_paper.html": {
    "title": "Learning Bijective Surface Parameterization for Inferring Signed Distance Functions from Sparse Point Clouds with Grid Deformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takeshi Noda",
      "Chao Chen",
      "Junsheng Zhou",
      "Weiqi Zhang",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wei_Minimizing_Labeled_Maximizing_Unlabeled_An_Image-Driven_Approach_for_Video_Instance_CVPR_2025_paper.html": {
    "title": "Minimizing Labeled, Maximizing Unlabeled: An Image-Driven Approach for Video Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangyun Wei",
      "Jinjing Zhao",
      "Kun Yan",
      "Chang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/You_Layer-_and_Timestep-Adaptive_Differentiable_Token_Compression_Ratios_for_Efficient_Diffusion_CVPR_2025_paper.html": {
    "title": "Layer- and Timestep-Adaptive Differentiable Token Compression Ratios for Efficient Diffusion Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran You",
      "Connelly Barnes",
      "Yuqian Zhou",
      "Yan Kang",
      "Zhenbang Du",
      "Wei Zhou",
      "Lingzhi Zhang",
      "Yotam Nitzan",
      "Xiaoyang Liu",
      "Zhe Lin",
      "Eli Shechtman",
      "Sohrab Amirghodsi",
      "Yingyan Celine Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Zero-shot_RGB-D_Point_Cloud_Registration_with_Pre-trained_Large_Vision_Model_CVPR_2025_paper.html": {
    "title": "Zero-shot RGB-D Point Cloud Registration with Pre-trained Large Vision Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haobo Jiang",
      "Jin Xie",
      "Jian Yang",
      "Liang Yu",
      "Jianmin Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ni_Balancing_Two_Classifiers_via_A_Simplex_ETF_Structure_for_Model_CVPR_2025_paper.html": {
    "title": "Balancing Two Classifiers via A Simplex ETF Structure for Model Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiani Ni",
      "He Zhao",
      "Jintong Gao",
      "Dandan Guo",
      "Hongyuan Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts_CVPR_2025_paper.html": {
    "title": "DistinctAD: Distinctive Audio Description Generation in Contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Fang",
      "Wenhao Wu",
      "Qiangqiang Wu",
      "Yuxin Song",
      "Antoni B. Chan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction_CVPR_2025_paper.html": {
    "title": "DAMM-Diffusion: Learning Divergence-Aware Multi-Modal Diffusion Model for Nanoparticles Distribution Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Zhou",
      "Shouju Wang",
      "Yuxia Tang",
      "Qi Zhu",
      "Daoqiang Zhang",
      "Wei Shao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach_CVPR_2025_paper.html": {
    "title": "Unveiling Differences in Generative Models: A Scalable Differential Clustering Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingwei Zhang",
      "Mohammad Jalali",
      "Cheuk Ting Li",
      "Farzan Farnia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts_CVPR_2025_paper.html": {
    "title": "CL-MoE: Enhancing Multimodal Large Language Model with Dual Momentum Mixture-of-Experts for Continual Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Huai",
      "Jie Zhou",
      "Xingjiao Wu",
      "Qin Chen",
      "Qingchun Bai",
      "Ze Zhou",
      "Liang He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qorbani_Semantic_Library_Adaptation_LoRA_Retrieval_and_Fusion_for_Open-Vocabulary_Semantic_CVPR_2025_paper.html": {
    "title": "Semantic Library Adaptation: LoRA Retrieval and Fusion for Open-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reza Qorbani",
      "Gianluca Villani",
      "Theodoros Panagiotakopoulos",
      "Marc Botet Colomer",
      "Linus Härenstam-Nielsen",
      "Mattia Segu",
      "Pier Luigi Dovesi",
      "Jussi Karlgren",
      "Daniel Cremers",
      "Federico Tombari",
      "Matteo Poggi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cai_PhyS-EdiT_Physics-aware_Semantic_Image_Editing_with_Text_Description_CVPR_2025_paper.html": {
    "title": "PhyS-EdiT: Physics-aware Semantic Image Editing with Text Description",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqi Cai",
      "Shuchen Weng",
      "Yifei Xia",
      "Boxin Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_U-Know-DiffPAN_An_Uncertainty-aware_Knowledge_Distillation_Diffusion_Framework_with_Details_Enhancement_CVPR_2025_paper.html": {
    "title": "U-Know-DiffPAN: An Uncertainty-aware Knowledge Distillation Diffusion Framework with Details Enhancement for PAN-Sharpening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungpyo Kim",
      "Jeonghyeok Do",
      "Jaehyup Lee",
      "Munchurl Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tan_SceneDiffuser_City-Scale_Traffic_Simulation_via_a_Generative_World_Model_CVPR_2025_paper.html": {
    "title": "SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuhan Tan",
      "John Lambert",
      "Hong Jeon",
      "Sakshum Kulshrestha",
      "Yijing Bai",
      "Jing Luo",
      "Dragomir Anguelov",
      "Mingxing Tan",
      "Chiyu Max Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pietrantoni_Gaussian_Splatting_Feature_Fields_for_Privacy-Preserving_Visual_Localization_CVPR_2025_paper.html": {
    "title": "Gaussian Splatting Feature Fields for (Privacy-Preserving) Visual Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxime Pietrantoni",
      "Gabriela Csurka",
      "Torsten Sattler"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Point_Cloud_Upsampling_Using_Conditional_Diffusion_Module_with_Adaptive_Noise_CVPR_2025_paper.html": {
    "title": "Point Cloud Upsampling Using Conditional Diffusion Module with Adaptive Noise Suppression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boqian Zhang",
      "Shen Yang",
      "Hao Chen",
      "Chao Yang",
      "Jing Jia",
      "Guang Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mazzamuto_Gazing_Into_Missteps_Leveraging_Eye-Gaze_for_Unsupervised_Mistake_Detection_in_CVPR_2025_paper.html": {
    "title": "Gazing Into Missteps: Leveraging Eye-Gaze for Unsupervised Mistake Detection in Egocentric Videos of Skilled Human Activities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michele Mazzamuto",
      "Antonino Furnari",
      "Yoichi Sato",
      "Giovanni Maria Farinella"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Trajectory_Mamba_Efficient_Attention-Mamba_Forecasting_Model_Based_on_Selective_SSM_CVPR_2025_paper.html": {
    "title": "Trajectory Mamba: Efficient Attention-Mamba Forecasting Model Based on Selective SSM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizhou Huang",
      "Yihua Cheng",
      "Kezhi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tan_Recover_and_Match_Open-Vocabulary_Multi-Label_Recognition_through_Knowledge-Constrained_Optimal_Transport_CVPR_2025_paper.html": {
    "title": "Recover and Match: Open-Vocabulary Multi-Label Recognition through Knowledge-Constrained Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Tan",
      "Zichang Tan",
      "Jun Li",
      "Ajian Liu",
      "Jun Wan",
      "Zhen Lei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Koch_RelationField_Relate_Anything_in_Radiance_Fields_CVPR_2025_paper.html": {
    "title": "RelationField: Relate Anything in Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Koch",
      "Johanna Wald",
      "Mirco Colosi",
      "Narunas Vaskevicius",
      "Pedro Hermosilla",
      "Federico Tombari",
      "Timo Ropinski"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs_CVPR_2025_paper.html": {
    "title": "DyFo: A Training-Free Dynamic Focus Visual Search for Enhancing LMMs in Fine-Grained Visual Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geng Li",
      "Jinglin Xu",
      "Yunzhen Zhao",
      "Yuxin Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Song_From_Head_to_Tail_Towards_Balanced_Representation_in_LargeVision-Language_Models_CVPR_2025_paper.html": {
    "title": "From Head to Tail: Towards Balanced Representation in Large Vision-Language Models through Adaptive Data Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyang Song",
      "Xiaoye Qu",
      "Jiawei Zhou",
      "Yu Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Let_Humanoids_Hike_Integrative_Skill_Development_on_Complex_Trails_CVPR_2025_paper.html": {
    "title": "Let Humanoids Hike! Integrative Skill Development on Complex Trails",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwan-Yee Lin",
      "Stella X. Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Corona_VLOGGER_Multimodal_Diffusion_for_Embodied_Avatar_Synthesis_CVPR_2025_paper.html": {
    "title": "VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enric Corona",
      "Andrei Zanfir",
      "Eduard Gabriel Bazavan",
      "Nikos Kolotouros",
      "Thiemo Alldieck",
      "Cristian Sminchisescu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_DEIM_DETR_with_Improved_Matching_for_Fast_Convergence_CVPR_2025_paper.html": {
    "title": "DEIM: DETR with Improved Matching for Fast Convergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shihua Huang",
      "Zhichao Lu",
      "Xiaodong Cun",
      "Yongjun Yu",
      "Xiao Zhou",
      "Xi Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_BF-STVSR_B-Splines_and_Fourier---Best_Friends_for_High_Fidelity_Spatial-Temporal_Video_CVPR_2025_paper.html": {
    "title": "BF-STVSR: B-Splines and Fourier---Best Friends for High Fidelity Spatial-Temporal Video Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eunjin Kim",
      "Hyeonjin Kim",
      "Kyong Hwan Jin",
      "Jaejun Yoo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Diehl_DIO_Decomposable_Implicit_4D_Occupancy-Flow_World_Model_CVPR_2025_paper.html": {
    "title": "DIO: Decomposable Implicit 4D Occupancy-Flow World Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Diehl",
      "Quinlan Sykora",
      "Ben Agro",
      "Thomas Gilles",
      "Sergio Casas",
      "Raquel Urtasun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hossain_SLADE_Shielding_against_Dual_Exploits_in_Large_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "SLADE: Shielding against Dual Exploits in Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Zarif Hossain",
      "Ahmed Imteaj"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Human_Motion_Instruction_Tuning_CVPR_2025_paper.html": {
    "title": "Human Motion Instruction Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Li",
      "Sen Jia",
      "Jianhao Wang",
      "Zhongyu Jiang",
      "Feng Zhou",
      "Ju Dai",
      "Tianfang Zhang",
      "Zongkai Wu",
      "Jenq-Neng Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mankovich_A_Flag_Decomposition_for_Hierarchical_Datasets_CVPR_2025_paper.html": {
    "title": "A Flag Decomposition for Hierarchical Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathan Mankovich",
      "Ignacio Santamaria",
      "Gustau Camps-Valls",
      "Tolga Birdal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Du_RCP-Bench_Benchmarking_Robustness_for_Collaborative_Perception_Under_Diverse_Corruptions_CVPR_2025_paper.html": {
    "title": "RCP-Bench: Benchmarking Robustness for Collaborative Perception Under Diverse Corruptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shihang Du",
      "Sanqing Qu",
      "Tianhang Wang",
      "Xudong Zhang",
      "Yunwei Zhu",
      "Jian Mao",
      "Fan Lu",
      "Qiao Lin",
      "Guang Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks_CVPR_2025_paper.html": {
    "title": "Olympus: A Universal Task Router for Computer Vision Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanze Lin",
      "Yunsheng Li",
      "Dongdong Chen",
      "Weijian Xu",
      "Ronald Clark",
      "Philip Torr"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cai_HERA_Hybrid_Explicit_Representation_for_Ultra-Realistic_Head_Avatars_CVPR_2025_paper.html": {
    "title": "HERA: Hybrid Explicit Representation for Ultra-Realistic Head Avatars",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongrui Cai",
      "Yuting Xiao",
      "Xuan Wang",
      "Jiafei Li",
      "Yudong Guo",
      "Yanbo Fan",
      "Shenghua Gao",
      "Juyong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning_CVPR_2025_paper.html": {
    "title": "Circumventing Shortcuts in Audio-visual Deepfake Detection Datasets with Unsupervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Smeu",
      "Dragos-Alexandru Boldisor",
      "Dan Oneata",
      "Elisabeta Oneata"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_CoE_Chain-of-Explanation_via_Automatic_Visual_Concept_Circuit_Description_and_Polysemanticity_CVPR_2025_paper.html": {
    "title": "CoE: Chain-of-Explanation via Automatic Visual Concept Circuit Description and Polysemanticity Quantification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenlong Yu",
      "Qilong Wang",
      "Chuang Liu",
      "Dong Li",
      "Qinghua Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Ego4o_Egocentric_Human_Motion_Capture_and_Understanding_from_Multi-Modal_Input_CVPR_2025_paper.html": {
    "title": "Ego4o: Egocentric Human Motion Capture and Understanding from Multi-Modal Input",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Wang",
      "Rishabh Dabral",
      "Diogo Luvizon",
      "Zhe Cao",
      "Lingjie Liu",
      "Thabo Beeler",
      "Christian Theobalt"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Image_Over_Text_Transforming_Formula_Recognition_Evaluation_with_Character_Detection_CVPR_2025_paper.html": {
    "title": "Image Over Text: Transforming Formula Recognition Evaluation with Character Detection Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Wang",
      "Fan Wu",
      "Linke Ouyang",
      "Zhuangcheng Gu",
      "Rui Zhang",
      "Renqiu Xia",
      "Botian Shi",
      "Bo Zhang",
      "Conghui He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long_CVPR_2025_paper.html": {
    "title": "FreePCA: Integrating Consistency Information across Long-short Frames in Training-free Long Video Generation via Principal Component Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangtong Tan",
      "Hu Yu",
      "Jie Huang",
      "Jie Xiao",
      "Feng Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Hierarchical_Adaptive_Filtering_Network_for_Text_Image_Specular_Highlight_Removal_CVPR_2025_paper.html": {
    "title": "Hierarchical Adaptive Filtering Network for Text Image Specular Highlight Removal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Jiang",
      "Jingbo Hu",
      "Ling Zhang",
      "Gang Fu",
      "Chunxia Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Improving_Semi-Supervised_Semantic_Segmentation_with_Sliced-Wasserstein_Feature_Alignment_and_Uniformity_CVPR_2025_paper.html": {
    "title": "Improving Semi-Supervised Semantic Segmentation with Sliced-Wasserstein Feature Alignment and Uniformity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen-Yi Lu",
      "Kasra Derakhshandeh",
      "Somali Chaterji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Mind_the_Time_Temporally-Controlled_Multi-Event_Video_Generation_CVPR_2025_paper.html": {
    "title": "Mind the Time: Temporally-Controlled Multi-Event Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Wu",
      "Aliaksandr Siarohin",
      "Willi Menapace",
      "Ivan Skorokhodov",
      "Yuwei Fang",
      "Varnith Chordia",
      "Igor Gilitschenski",
      "Sergey Tulyakov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_Learning_Extremely_High_Density_Crowds_as_Active_Matters_CVPR_2025_paper.html": {
    "title": "Learning Extremely High Density Crowds as Active Matters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feixiang He",
      "Jiangbei Yue",
      "Jialin Zhu",
      "Armin Seyfried",
      "Dan Casas",
      "Julien Pettré",
      "He Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Audio-Visual_Semantic_Graph_Network_for_Audio-Visual_Event_Localization_CVPR_2025_paper.html": {
    "title": "Audio-Visual Semantic Graph Network for Audio-Visual Event Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Liu",
      "Shuaiyong Li",
      "Yongqiang Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_3D-Mem_3D_Scene_Memory_for_Embodied_Exploration_and_Reasoning_CVPR_2025_paper.html": {
    "title": "3D-Mem: 3D Scene Memory for Embodied Exploration and Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuncong Yang",
      "Han Yang",
      "Jiachen Zhou",
      "Peihao Chen",
      "Hongxin Zhang",
      "Yilun Du",
      "Chuang Gan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Meng_EchoMimicV2_Towards_Striking_Simplified_and_Semi-Body_Human_Animation_CVPR_2025_paper.html": {
    "title": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rang Meng",
      "Xingyu Zhang",
      "Yuming Li",
      "Chenguang Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bar_Navigation_World_Models_CVPR_2025_paper.html": {
    "title": "Navigation World Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Bar",
      "Gaoyue Zhou",
      "Danny Tran",
      "Trevor Darrell",
      "Yann LeCun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pondaven_Video_Motion_Transfer_with_Diffusion_Transformers_CVPR_2025_paper.html": {
    "title": "Video Motion Transfer with Diffusion Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Pondaven",
      "Aliaksandr Siarohin",
      "Sergey Tulyakov",
      "Philip Torr",
      "Fabio Pizzati"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Aira_Gaussian_Splatting_for_Efficient_Satellite_Image_Photogrammetry_CVPR_2025_paper.html": {
    "title": "Gaussian Splatting for Efficient Satellite Image Photogrammetry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Savant Aira",
      "Gabriele Facciolo",
      "Thibaud Ehret"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events_CVPR_2025_paper.html": {
    "title": "Unified Reconstruction of Static and Dynamic Scenes from Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyao Gao",
      "Peiqi Duan",
      "Hanyue Lou",
      "Minggui Teng",
      "Ziqi Cai",
      "Xu Chen",
      "Boxin Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Du_Automatic_Spectral_Calibration_of_Hyperspectral_Images_Method_Dataset_and_Benchmark_CVPR_2025_paper.html": {
    "title": "Automatic Spectral Calibration of Hyperspectral Images: Method, Dataset and Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoran Du",
      "Shaodi You",
      "Cheng Cheng",
      "Shikui Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nag_Conformal_Prediction_and_MLLM_aided_Uncertainty_Quantification_in_Scene_Graph_CVPR_2025_paper.html": {
    "title": "Conformal Prediction and MLLM aided Uncertainty Quantification in Scene Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayak Nag",
      "Udita Ghosh",
      "Calvin-Khang Ta",
      "Sarosij Bose",
      "Jiachen Li",
      "Amit K. Roy-Chowdhury"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting_CVPR_2025_paper.html": {
    "title": "Point-to-Region Loss for Semi-Supervised Point-Based Crowd Counting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Lin",
      "Chenyang Zhao",
      "Antoni B. Chan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Reconstructing_Close_Human_Interaction_with_Appearance_and_Proxemics_Reasoning_CVPR_2025_paper.html": {
    "title": "Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Buzhen Huang",
      "Chen Li",
      "Chongyang Xu",
      "Dongyue Lu",
      "Jinnan Chen",
      "Yangang Wang",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long_CVPR_2025_paper.html": {
    "title": "Towards Improved Text-Aligned Codebook Learning: Multi-Hierarchical Codebook-Text Alignment with Long Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guotao Liang",
      "Baoquan Zhang",
      "Zhiyuan Wen",
      "Junteng Zhao",
      "Yunming Ye",
      "Kola Ye",
      "Yao He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Parallel_Sequence_Modeling_via_Generalized_Spatial_Propagation_Network_CVPR_2025_paper.html": {
    "title": "Parallel Sequence Modeling via Generalized Spatial Propagation Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongjun Wang",
      "Wonmin Byeon",
      "Jiarui Xu",
      "Jinwei Gu",
      "Ka Chun Cheung",
      "Xiaolong Wang",
      "Kai Han",
      "Jan Kautz",
      "Sifei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rowe_Scenario_Dreamer_Vectorized_Latent_Diffusion_for_Generating_Driving_Simulation_Environments_CVPR_2025_paper.html": {
    "title": "Scenario Dreamer: Vectorized Latent Diffusion for Generating Driving Simulation Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luke Rowe",
      "Roger Girgis",
      "Anthony Gosselin",
      "Liam Paull",
      "Christopher Pal",
      "Felix Heide"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Thakkar_Poly-Autoregressive_Prediction_for_Modeling_Interactions_CVPR_2025_paper.html": {
    "title": "Poly-Autoregressive Prediction for Modeling Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neerja Thakkar",
      "Tara Sadjadpour",
      "Jathushan Rajasegeran",
      "Shiry Ginosar",
      "Jitendra Malik"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_NADER_Neural_Architecture_Design_via_Multi-Agent_Collaboration_CVPR_2025_paper.html": {
    "title": "NADER: Neural Architecture Design via Multi-Agent Collaboration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zekang Yang",
      "Wang Zeng",
      "Sheng Jin",
      "Chen Qian",
      "Ping Luo",
      "Wentao Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Move-in-2D_2D-Conditioned_Human_Motion_Generation_CVPR_2025_paper.html": {
    "title": "Move-in-2D: 2D-Conditioned Human Motion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hsin-Ping Huang",
      "Yang Zhou",
      "Jui-Hsien Wang",
      "Difan Liu",
      "Feng Liu",
      "Ming-Hsuan Yang",
      "Zhan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jeong_PoseBH_Prototypical_Multi-Dataset_Training_Beyond_Human_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "PoseBH: Prototypical Multi-Dataset Training Beyond Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uyoung Jeong",
      "Jonathan Freer",
      "Seungryul Baek",
      "Hyung Jin Chang",
      "Kwang In Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xue_MATCHA_Towards_Matching_Anything_CVPR_2025_paper.html": {
    "title": "MATCHA: Towards Matching Anything",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Xue",
      "Sven Elflein",
      "Laura Leal-Taixé",
      "Qunjie Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_CTRL-D_Controllable_Dynamic_3D_Scene_Editing_with_Personalized_2D_Diffusion_CVPR_2025_paper.html": {
    "title": "CTRL-D: Controllable Dynamic 3D Scene Editing with Personalized 2D Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai He",
      "Chin-Hsuan Wu",
      "Igor Gilitschenski"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Separation_of_Powers_On_Segregating_Knowledge_from_Observation_in_LLM-enabled_CVPR_2025_paper.html": {
    "title": "Separation of Powers: On Segregating Knowledge from Observation in LLM-enabled Knowledge-based Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Yang",
      "Zhuo Tao",
      "Qi Chen",
      "Liang Li",
      "Yuankai Qi",
      "Anton van den Hengel",
      "Qingming Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Decision_SpikeFormer_Spike-Driven_Transformer_for_Decision_Making_CVPR_2025_paper.html": {
    "title": "Decision SpikeFormer: Spike-Driven Transformer for Decision Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Huang",
      "Qinying Gu",
      "Nanyang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_SF2T_Self-supervised_Fragment_Finetuning_of_Video-LLMs_for_Fine-Grained_Understanding_CVPR_2025_paper.html": {
    "title": "SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangliu Hu",
      "Zikai Song",
      "Na Feng",
      "Yawei Luo",
      "Junqing Yu",
      "Yi-Ping Phoebe Chen",
      "Wei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Theory-Inspired_Deep_Multi-View_Multi-Label_Learning_with_Incomplete_Views_and_Noisy_CVPR_2025_paper.html": {
    "title": "Theory-Inspired Deep Multi-View Multi-Label Learning with Incomplete Views and Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quanjiang Li",
      "Tingjin Luo",
      "Jiahui Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Fitted_Neural_Lossless_Image_Compression_CVPR_2025_paper.html": {
    "title": "Fitted Neural Lossless Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Zhang",
      "Zhenzhong Chen",
      "Shan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kumar_Fortifying_Federated_Learning_Towards_Trustworthiness_via_Auditable_Data_Valuation_and_CVPR_2025_paper.html": {
    "title": "Fortifying Federated Learning Towards Trustworthiness via Auditable Data Valuation and Verifiable Client Contribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "K Naveen Kumar",
      "Ranjeet Ranjan Jha",
      "C Krishna Mohan",
      "Ravindra Babu Tallamraju"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_EMOE_Modality-Specific_Enhanced_Dynamic_Emotion_Experts_CVPR_2025_paper.html": {
    "title": "EMOE: Modality-Specific Enhanced Dynamic Emotion Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Fang",
      "Wenke Huang",
      "Guancheng Wan",
      "Kehua Su",
      "Mang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_JarvisIR_Elevating_Autonomous_Driving_Perception_with_Intelligent_Image_Restoration_CVPR_2025_paper.html": {
    "title": "JarvisIR: Elevating Autonomous Driving Perception with Intelligent Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunlong Lin",
      "Zixu Lin",
      "Haoyu Chen",
      "Panwang Pan",
      "Chenxin Li",
      "Sixiang Chen",
      "Kairun Wen",
      "Yeying Jin",
      "Wenbo Li",
      "Xinghao Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_UniPre3D_Unified_Pre-training_of_3D_Point_Cloud_Models_with_Cross-Modal_CVPR_2025_paper.html": {
    "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Wang",
      "Yanran Zhang",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Behrad_Charm_The_Missing_Piece_in_ViT_Fine-Tuning_for_Image_Aesthetic_CVPR_2025_paper.html": {
    "title": "Charm: The Missing Piece in ViT Fine-Tuning for Image Aesthetic Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fatemeh Behrad",
      "Tinne Tuytelaars",
      "Johan Wagemans"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_F-LMM_Grounding_Frozen_Large_Multimodal_Models_CVPR_2025_paper.html": {
    "title": "F-LMM: Grounding Frozen Large Multimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Size Wu",
      "Sheng Jin",
      "Wenwei Zhang",
      "Lumin Xu",
      "Wentao Liu",
      "Wei Li",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_EntityErasure_Erasing_Entity_Cleanly_via_Amodal_Entity_Segmentation_and_Completion_CVPR_2025_paper.html": {
    "title": "EntityErasure: Erasing Entity Cleanly via Amodal Entity Segmentation and Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixing Zhu",
      "Qing Zhang",
      "Yitong Wang",
      "Yongwei Nie",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Generative_Video_Propagation_CVPR_2025_paper.html": {
    "title": "Generative Video Propagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaoteng Liu",
      "Tianyu Wang",
      "Jui-Hsien Wang",
      "Qing Liu",
      "Zhifei Zhang",
      "Joon-Young Lee",
      "Yijun Li",
      "Bei Yu",
      "Zhe Lin",
      "Soo Ye Kim",
      "Jiaya Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons_CVPR_2025_paper.html": {
    "title": "From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Szot",
      "Bogdan Mazoure",
      "Omar Attia",
      "Aleksei Timofeev",
      "Harsh Agrawal",
      "Devon Hjelm",
      "Zhe Gan",
      "Zsolt Kira",
      "Alexander Toshev"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Mosaic3D_Foundation_Dataset_and_Model_for_Open-Vocabulary_3D_Segmentation_CVPR_2025_paper.html": {
    "title": "Mosaic3D: Foundation Dataset and Model for Open-Vocabulary 3D Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junha Lee",
      "Chunghyun Park",
      "Jaesung Choe",
      "Yu-Chiang Frank Wang",
      "Jan Kautz",
      "Minsu Cho",
      "Chris Choy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hwang_T-CIL_Temperature_Scaling_using_Adversarial_Perturbation_for_Calibration_in_Class-Incremental_CVPR_2025_paper.html": {
    "title": "T-CIL: Temperature Scaling using Adversarial Perturbation for Calibration in Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seong-Hyeon Hwang",
      "Minsu Kim",
      "Steven Euijong Whang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_LoRA_Subtraction_for_Drift-Resistant_Space_in_Exemplar-Free_Continual_Learning_CVPR_2025_paper.html": {
    "title": "LoRA Subtraction for Drift-Resistant Space in Exemplar-Free Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Liu",
      "Xiaobin Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Schmidt_Joint_Out-of-Distribution_Filtering_and_Data_Discovery_Active_Learning_CVPR_2025_paper.html": {
    "title": "Joint Out-of-Distribution Filtering and Data Discovery Active Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Schmidt",
      "Leonard Schenk",
      "Leo Schwinn",
      "Stephan Günnemann"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lyu_AniMer_Animal_Pose_and_Shape_Estimation_Using_Family_Aware_Transformer_CVPR_2025_paper.html": {
    "title": "AniMer: Animal Pose and Shape Estimation Using Family Aware Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Lyu",
      "Tianyi Zhu",
      "Yi Gu",
      "Li Lin",
      "Pujin Cheng",
      "Yebin Liu",
      "Xiaoying Tang",
      "Liang An"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Moon_Co-op_Correspondence-based_Novel_Object_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "Co-op: Correspondence-based Novel Object Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungphill Moon",
      "Hyeontae Son",
      "Dongcheol Hur",
      "Sangwook Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qiu_Finding_Local_Diffusion_Schrodinger_Bridge_using_Kolmogorov-Arnold_Network_CVPR_2025_paper.html": {
    "title": "Finding Local Diffusion Schrodinger Bridge using Kolmogorov-Arnold Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Qiu",
      "Mengying Yang",
      "Xinghua Ma",
      "Fanding Li",
      "Dong Liang",
      "Gongning Luo",
      "Wei Wang",
      "Kuanquan Wang",
      "Shuo Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xue_CorrBEV_Multi-View_3D_Object_Detection_by_Correlation_Learning_with_Multi-modal_CVPR_2025_paper.html": {
    "title": "CorrBEV: Multi-View 3D Object Detection by Correlation Learning with Multi-modal Prototypes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziteng Xue",
      "Mingzhe Guo",
      "Heng Fan",
      "Shihui Zhang",
      "Zhipeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Completion_as_Enhancement_A_Degradation-Aware_Selective_Image_Guided_Network_for_CVPR_2025_paper.html": {
    "title": "Completion as Enhancement: A Degradation-Aware Selective Image Guided Network for Depth Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqiang Yan",
      "Zhengxue Wang",
      "Kun Wang",
      "Jun Li",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_CATANet_Efficient_Content-Aware_Token_Aggregation_for_Lightweight_Image_Super-Resolution_CVPR_2025_paper.html": {
    "title": "CATANet: Efficient Content-Aware Token Aggregation for Lightweight Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Liu",
      "Jie Liu",
      "Jie Tang",
      "Gangshan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_SeeGround_See_and_Ground_for_Zero-Shot_Open-Vocabulary_3D_Visual_Grounding_CVPR_2025_paper.html": {
    "title": "SeeGround: See and Ground for Zero-Shot Open-Vocabulary 3D Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rong Li",
      "Shijie Li",
      "Lingdong Kong",
      "Xulei Yang",
      "Junwei Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shao_RayFlow_Instance-Aware_Diffusion_Acceleration_via_Adaptive_Flow_Trajectories_CVPR_2025_paper.html": {
    "title": "RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow Trajectories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiyang Shao",
      "Xin Xia",
      "Yuhong Yang",
      "Yuxi Ren",
      "Xing Wang",
      "Xuefeng Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_Linear_Attention_Modeling_for_Learned_Image_Compression_CVPR_2025_paper.html": {
    "title": "Linear Attention Modeling for Learned Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donghui Feng",
      "Zhengxue Cheng",
      "Shen Wang",
      "Ronghua Wu",
      "Hongwei Hu",
      "Guo Lu",
      "Li Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dufour_Around_the_World_in_80_Timesteps_A_Generative_Approach_to_CVPR_2025_paper.html": {
    "title": "Around the World in 80 Timesteps: A Generative Approach to Global Visual Geolocation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolas Dufour",
      "Vicky Kalogeiton",
      "David Picard",
      "Loic Landrieu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Asynchronous_Collaborative_Graph_Representation_for_Frames_and_Events_CVPR_2025_paper.html": {
    "title": "Asynchronous Collaborative Graph Representation for Frames and Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dianze Li",
      "Jianing Li",
      "Xu Liu",
      "Xiaopeng Fan",
      "Yonghong Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially_CVPR_2025_paper.html": {
    "title": "Real-time High-fidelity Gaussian Human Avatars with Position-based Interpolation of Spatially Distributed MLPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youyi Zhan",
      "Tianjia Shao",
      "Yin Yang",
      "Kun Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ni_ReconDreamer_Crafting_World_Models_for_Driving_Scene_Reconstruction_via_Online_CVPR_2025_paper.html": {
    "title": "ReconDreamer: Crafting World Models for Driving Scene Reconstruction via Online Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaojun Ni",
      "Guosheng Zhao",
      "Xiaofeng Wang",
      "Zheng Zhu",
      "Wenkang Qin",
      "Guan Huang",
      "Chen Liu",
      "Yuyin Chen",
      "Yida Wang",
      "Xueyang Zhang",
      "Yifei Zhan",
      "Kun Zhan",
      "Peng Jia",
      "Xianpeng Lang",
      "Xingang Wang",
      "Wenjun Mei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Su_RoboSense_Large-scale_Dataset_and_Benchmark_for_Egocentric_Robot_Perception_and_CVPR_2025_paper.html": {
    "title": "RoboSense: Large-scale Dataset and Benchmark for Egocentric Robot Perception and Navigation in Crowded and Unstructured Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haisheng Su",
      "Feixiang Song",
      "Cong Ma",
      "Wei Wu",
      "Junchi Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Self-Supervised_Large_Scale_Point_Cloud_Completion_for_Archaeological_Site_Restoration_CVPR_2025_paper.html": {
    "title": "Self-Supervised Large Scale Point Cloud Completion for Archaeological Site Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aocheng Li",
      "James R. Zimmer-Dauphinee",
      "Rajesh Kalyanam",
      "Ian Lindsay",
      "Parker VanValkenburgh",
      "Steven Wernke",
      "Daniel Aliaga"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Chain_of_Attack_On_the_Robustness_of_Vision-Language_Models_Against_CVPR_2025_paper.html": {
    "title": "Chain of Attack: On the Robustness of Vision-Language Models Against Transfer-Based Adversarial Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Xie",
      "Yequan Bie",
      "Jianda Mao",
      "Yangqiu Song",
      "Yang Wang",
      "Hao Chen",
      "Kani Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zeevi_Rate-In_Information-Driven_Adaptive_Dropout_Rates_for_Improved_Inference-Time_Uncertainty_Estimation_CVPR_2025_paper.html": {
    "title": "Rate-In: Information-Driven Adaptive Dropout Rates for Improved Inference-Time Uncertainty Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tal Zeevi",
      "Ravid Shwartz-Ziv",
      "Yann LeCun",
      "Lawrence H. Staib",
      "John A. Onofrey"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kairanda_Thin-Shell-SfT_Fine-Grained_Monocular_Non-rigid_3D_Surface_Tracking_with_Neural_Deformation_CVPR_2025_paper.html": {
    "title": "Thin-Shell-SfT: Fine-Grained Monocular Non-rigid 3D Surface Tracking with Neural Deformation Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Navami Kairanda",
      "Marc Habermann",
      "Shanthika Naik",
      "Christian Theobalt",
      "Vladislav Golyanik"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DeCLIP_Decoupled_Learning_for_Open-Vocabulary_Dense_Perception_CVPR_2025_paper.html": {
    "title": "DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Wang",
      "Bin Chen",
      "Yulin Li",
      "Bin Kang",
      "Yichi Chen",
      "Zhuotao Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cao_SocialGesture_Delving_into_Multi-person_Gesture_Understanding_CVPR_2025_paper.html": {
    "title": "SocialGesture: Delving into Multi-person Gesture Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Cao",
      "Pranav Virupaksha",
      "Wenqi Jia",
      "Bolin Lai",
      "Fiona Ryan",
      "Sangmin Lee",
      "James M. Rehg"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_GenFusion_Closing_the_Loop_between_Reconstruction_and_Generation_via_Videos_CVPR_2025_paper.html": {
    "title": "GenFusion: Closing the Loop between Reconstruction and Generation via Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sibo Wu",
      "Congrong Xu",
      "Binbin Huang",
      "Andreas Geiger",
      "Anpei Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife_CVPR_2025_paper.html": {
    "title": "The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Otto Brookes",
      "Maksim Kukushkin",
      "Majid Mirmehdi",
      "Colleen Stephens",
      "Paula Dieguez",
      "Thurston C. Hicks",
      "Sorrel Jones",
      "Kevin Lee",
      "Maureen S. McCarthy",
      "Amelia Meier",
      "Emmanuelle Normand",
      "Erin G. Wessling",
      "Roman M. Wittig",
      "Kevin Langergraber",
      "Klaus Zuberbühler",
      "Lukas Boesch",
      "Thomas Schmid",
      "Mimi Arandjelovic",
      "Hjalmar Kühl",
      "Tilo Burghardt"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_Multi-modal_Topology-embedded_Graph_Learning_for_Spatially_Resolved_Genes_Prediction_from_CVPR_2025_paper.html": {
    "title": "Multi-modal Topology-embedded Graph Learning for Spatially Resolved Genes Prediction from Pathology Images with Prior Gene Similarity Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Shi",
      "Changxi Chi",
      "Peng Wan",
      "Daoqiang Zhang",
      "Wei Shao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering_CVPR_2025_paper.html": {
    "title": "Question-Aware Gaussian Experts for Audio-Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyeob Kim",
      "Inyoung Jung",
      "Dayoon Suh",
      "Youjia Zhang",
      "Sangmin Lee",
      "Sungeun Hong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ji_Sonic_Shifting_Focus_to_Global_Audio_Perception_in_Portrait_Animation_CVPR_2025_paper.html": {
    "title": "Sonic: Shifting Focus to Global Audio Perception in Portrait Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaozhong Ji",
      "Xiaobin Hu",
      "Zhihong Xu",
      "Junwei Zhu",
      "Chuming Lin",
      "Qingdong He",
      "Jiangning Zhang",
      "Donghao Luo",
      "Yi Chen",
      "Qin Lin",
      "Qinglin Lu",
      "Chengjie Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control_CVPR_2025_paper.html": {
    "title": "Multitwine: Multi-Object Compositing with Text and Layout Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gemma Canet Tarrés",
      "Zhe Lin",
      "Zhifei Zhang",
      "He Zhang",
      "Andrew Gilbert",
      "John Collomosse",
      "Soo Ye Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_DEFOM-Stereo_Depth_Foundation_Model_Based_Stereo_Matching_CVPR_2025_paper.html": {
    "title": "DEFOM-Stereo: Depth Foundation Model Based Stereo Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hualie Jiang",
      "Zhiqiang Lou",
      "Laiyan Ding",
      "Rui Xu",
      "Minglang Tan",
      "Wenjie Jiang",
      "Rui Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Adaptive_Rectangular_Convolution_for_Remote_Sensing_Pansharpening_CVPR_2025_paper.html": {
    "title": "Adaptive Rectangular Convolution for Remote Sensing Pansharpening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueyang Wang",
      "Zhixin Zheng",
      "Jiandong Shao",
      "Yule Duan",
      "Liang-Jian Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ke_Video_Depth_without_Video_Models_CVPR_2025_paper.html": {
    "title": "Video Depth without Video Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingxin Ke",
      "Dominik Narnhofer",
      "Shengyu Huang",
      "Lei Ke",
      "Torben Peters",
      "Katerina Fragkiadaki",
      "Anton Obukhov",
      "Konrad Schindler"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_PointLoRA_Low-Rank_Adaptation_with_Token_Selection_for_Point_Cloud_Learning_CVPR_2025_paper.html": {
    "title": "PointLoRA: Low-Rank Adaptation with Token Selection for Point Cloud Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Song Wang",
      "Xiaolu Liu",
      "Lingdong Kong",
      "Jianyun Xu",
      "Chunyong Hu",
      "Gongfan Fang",
      "Wentong Li",
      "Jianke Zhu",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large_CVPR_2025_paper.html": {
    "title": "HumanRig: Learning Automatic Rigging for Humanoid Character in a Large Scale Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zedong Chu",
      "Feng Xiong",
      "Meiduo Liu",
      "Jinzhi Zhang",
      "Mingqi Shao",
      "Zhaoxu Sun",
      "Di Wang",
      "Mu Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_GaussHDR_High_Dynamic_Range_Gaussian_Splatting_via_Learning_Unified_3D_CVPR_2025_paper.html": {
    "title": "GaussHDR: High Dynamic Range Gaussian Splatting via Learning Unified 3D and 2D Local Tone Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinfeng Liu",
      "Lingtong Kong",
      "Bo Li",
      "Dan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "UIBDiffusion: Universal Imperceptible Backdoor Attack for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuning Han",
      "Bingyin Zhao",
      "Rui Chu",
      "Feng Luo",
      "Biplab Sikdar",
      "Yingjie Lao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_DiskVPS_Vanishing_Point_Detector_via_Hough_Transform_in_a_Disk_CVPR_2025_paper.html": {
    "title": "DiskVPS: Vanishing Point Detector via Hough Transform in a Disk Region",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianping Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention_CVPR_2025_paper.html": {
    "title": "Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feilong Tang",
      "Chengzhi Liu",
      "Zhongxing Xu",
      "Ming Hu",
      "Zile Huang",
      "Haochen Xue",
      "Ziyang Chen",
      "Zelin Peng",
      "Zhiwei Yang",
      "Sijin Zhou",
      "Wenxue Li",
      "Yulong Li",
      "Wenxuan Song",
      "Shiyan Su",
      "Wei Feng",
      "Jionglong Su",
      "Mingquan Lin",
      "Yifan Peng",
      "Xuelian Cheng",
      "Imran Razzak",
      "Zongyuan Ge"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation_CVPR_2025_paper.html": {
    "title": "Towards Autonomous Micromobility through Scalable Urban Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wayne Wu",
      "Honglin He",
      "Chaoyuan Zhang",
      "Jack He",
      "Seth Z. Zhao",
      "Ran Gong",
      "Quanyi Li",
      "Bolei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_FisherTune_Fisher-Guided_Robust_Tuning_of_Vision_Foundation_Models_for_Domain_CVPR_2025_paper.html": {
    "title": "FisherTune: Fisher-Guided Robust Tuning of Vision Foundation Models for Domain Generalized Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Zhao",
      "Jinlong Li",
      "Shuang Wang",
      "Mengyao Wu",
      "Qi Zang",
      "Nicu Sebe",
      "Zhun Zhong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_Language-Assisted_Debiasing_and_Smoothing_for_Foundation_Model-Based_Semi-Supervised_Learning_CVPR_2025_paper.html": {
    "title": "Language-Assisted Debiasing and Smoothing for Foundation Model-Based Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Na Zheng",
      "Xuemeng Song",
      "Xue Dong",
      "Aashish Nikhil Ghosh",
      "Liqiang Nie",
      "Roger Zimmermann"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_EdgeMovingNet_Edge-preserving_Point_Cloud_Reconstruction_via_Joint_Geometry_Features_CVPR_2025_paper.html": {
    "title": "EdgeMovingNet: Edge-preserving Point Cloud Reconstruction via Joint Geometry Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinran Yang",
      "Donghao Ji",
      "Yuanqi Li",
      "Junyuan Xie",
      "Jie Guo",
      "Yanwen Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chang_AdMiT_Adaptive_Multi-Source_Tuning_in_Dynamic_Environments_CVPR_2025_paper.html": {
    "title": "AdMiT: Adaptive Multi-Source Tuning in Dynamic Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Chang",
      "Fahim Faisal Niloy",
      "Sk Miraj Ahmed",
      "Srikanth V. Krishnamurthy",
      "Basak Guler",
      "Ananthram Swami",
      "Samet Oymak",
      "Amit Roy-Chowdhury"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Unbiased_Video_Scene_Graph_Generation_via_Visual_and_Semantic_Dual_CVPR_2025_paper.html": {
    "title": "Unbiased Video Scene Graph Generation via Visual and Semantic Dual Debiasing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanjun Li",
      "Zhaoyang Li",
      "Honghui Chen",
      "Lizhi Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Choi_Channel-wise_Noise_Scheduled_Diffusion_for_Inverse_Rendering_in_Indoor_Scenes_CVPR_2025_paper.html": {
    "title": "Channel-wise Noise Scheduled Diffusion for Inverse Rendering in Indoor Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JunYong Choi",
      "Min-cheol Sagong",
      "SeokYeong Lee",
      "Seung-Won Jung",
      "Ig-Jae Kim",
      "Junghyun Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Targeted_Forgetting_of_Image_Subgroups_in_CLIP_Models_CVPR_2025_paper.html": {
    "title": "Targeted Forgetting of Image Subgroups in CLIP Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeliang Zhang",
      "Gaowen Liu",
      "Charles Fleming",
      "Ramana Rao Kompella",
      "Chenliang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation_CVPR_2025_paper.html": {
    "title": "Unleashing In-context Learning of Autoregressive Models for Few-shot Image Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bolin Lai",
      "Felix Juefei-Xu",
      "Miao Liu",
      "Xiaoliang Dai",
      "Nikhil Mehta",
      "Chenguang Zhu",
      "Zeyi Huang",
      "James M. Rehg",
      "Sangmin Lee",
      "Ning Zhang",
      "Tong Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Maniparambil_Harnessing_Frozen_Unimodal_Encoders_for_Flexible_Multimodal_Alignment_CVPR_2025_paper.html": {
    "title": "Harnessing Frozen Unimodal Encoders for Flexible Multimodal Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mayug Maniparambil",
      "Raiymbek Akshulakov",
      "Yasser Abdelaziz Dahou Djilali",
      "Sanath Narayan",
      "Ankit Singh",
      "Noel E. O'Connor"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bian_Feature_Information_Driven_Position_Gaussian_Distribution_Estimation_for_Tiny_Object_CVPR_2025_paper.html": {
    "title": "Feature Information Driven Position Gaussian Distribution Estimation for Tiny Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinghao Bian",
      "Mingtao Feng",
      "Weisheng Dong",
      "Fangfang Wu",
      "Jianqiao Luo",
      "Yaonan Wang",
      "Guangming Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Enhancing_Diversity_for_Data-free_Quantization_CVPR_2025_paper.html": {
    "title": "Enhancing Diversity for Data-free Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Zhao",
      "Zhihao Zhuang",
      "Miao Zhang",
      "Chenjuan Guo",
      "Yang Shu",
      "Bin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_SeqAfford_Sequential_3D_Affordance_Reasoning_via_Multimodal_Large_Language_Model_CVPR_2025_paper.html": {
    "title": "SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunlin Yu",
      "Hanqing Wang",
      "Ye Shi",
      "Haoyang Luo",
      "Sibei Yang",
      "Jingyi Yu",
      "Jingya Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Karimi_DSV-LFS_Unifying_LLM-Driven_Semantic_Cues_with_Visual_Features_for_Robust_CVPR_2025_paper.html": {
    "title": "DSV-LFS: Unifying LLM-Driven Semantic Cues with Visual Features for Robust Few-Shot Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amin Karimi",
      "Charalambos Poullis"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Revisiting_Generative_Replay_for_Class_Incremental_Object_Detection_CVPR_2025_paper.html": {
    "title": "Revisiting Generative Replay for Class Incremental Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shizhou Zhang",
      "Xueqiang Lv",
      "Yinghui Xing",
      "Qirui Wu",
      "Di Xu",
      "Yanning Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wandel_SemAlign3D_Semantic_Correspondence_between_RGB-Images_through_Aligning_3D_Object-Class_Representations_CVPR_2025_paper.html": {
    "title": "SemAlign3D: Semantic Correspondence between RGB-Images through Aligning 3D Object-Class Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krispin Wandel",
      "Hesheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qian_Bridging_Viewpoint_Gaps_Geometric_Reasoning_Boosts_Semantic_Correspondence_CVPR_2025_paper.html": {
    "title": "Bridging Viewpoint Gaps: Geometric Reasoning Boosts Semantic Correspondence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyang Qian",
      "Hansheng Chen",
      "Masayoshi Tomizuka",
      "Kurt Keutzer",
      "Qianqian Wang",
      "Chenfeng Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection_CVPR_2025_paper.html": {
    "title": "DPU: Dynamic Prototype Updating for Multimodal Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shawn Li",
      "Huixian Gong",
      "Hao Dong",
      "Tiankai Yang",
      "Zhengzhong Tu",
      "Yue Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation_CVPR_2025_paper.html": {
    "title": "Towards Unbiased and Robust Spatio-Temporal Scene Graph Generation and Anticipation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohith Peddi",
      "Saurabh Saurabh",
      "Ayush Abhay Shrivastava",
      "Parag Singla",
      "Vibhav Gogate"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_Spatial_Transport_Optimization_by_Repositioning_Attention_Map_for_Training-Free_Text-to-Image_CVPR_2025_paper.html": {
    "title": "Spatial Transport Optimization by Repositioning Attention Map for Training-Free Text-to-Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woojung Han",
      "Yeonkyung Lee",
      "Chanyoung Kim",
      "Kwanghyun Park",
      "Seong Jae Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bouniot_From_Alexnet_to_Transformers_Measuring_the_Non-linearity_of_Deep_Neural_CVPR_2025_paper.html": {
    "title": "From Alexnet to Transformers: Measuring the Non-linearity of Deep Neural Networks with Affine Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quentin Bouniot",
      "Ievgen Redko",
      "Anton Mallasto",
      "Charlotte Laclau",
      "Oliver Struckmeier",
      "Karol Arndt",
      "Markus Heinonen",
      "Ville Kyrki",
      "Samuel Kaski"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Medghalchi_Prompt2Perturb_P2P_Text-Guided_Diffusion-Based_Adversarial_Attack_on_Breast_Ultrasound_Images_CVPR_2025_paper.html": {
    "title": "Prompt2Perturb (P2P): Text-Guided Diffusion-Based Adversarial Attack on Breast Ultrasound Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasamin Medghalchi",
      "Moein Heidari",
      "Clayton Allard",
      "Leonid Sigal",
      "Ilker Hacihaliloglu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Boosting_Domain_Incremental_Learning_Selecting_the_Optimal_Parameters_is_All_CVPR_2025_paper.html": {
    "title": "Boosting Domain Incremental Learning: Selecting the Optimal Parameters is All You Need",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiang Wang",
      "Xiang Song",
      "Yuhang He",
      "Jizhou Han",
      "Chenhao Ding",
      "Xinyuan Gao",
      "Yihong Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_COAP_Memory-Efficient_Training_with_Correlation-Aware_Gradient_Projection_CVPR_2025_paper.html": {
    "title": "COAP: Memory-Efficient Training with Correlation-Aware Gradient Projection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinqi Xiao",
      "Shen Sang",
      "Tiancheng Zhi",
      "Jing Liu",
      "Qing Yan",
      "Linjie Luo",
      "Bo Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Perceptual_Inductive_Bias_Is_What_You_Need_Before_Contrastive_Learning_CVPR_2025_paper.html": {
    "title": "Perceptual Inductive Bias Is What You Need Before Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junru Zhao",
      "Tianqin Li",
      "Dunhan Jiang",
      "Shenghao Wu",
      "Alan Ramirez",
      "Tai Sing Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_FaceBench_A_Multi-View_Multi-Level_Facial_Attribute_VQA_Dataset_for_Benchmarking_CVPR_2025_paper.html": {
    "title": "FaceBench: A Multi-View Multi-Level Facial Attribute VQA Dataset for Benchmarking Face Perception MLLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoqin Wang",
      "Xusen Ma",
      "Xianxu Hou",
      "Meidan Ding",
      "Yudong Li",
      "Junliang Chen",
      "Wenting Chen",
      "Xiaoyang Peng",
      "Linlin Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical_CVPR_2025_paper.html": {
    "title": "EffiDec3D: An Optimized Decoder for High-Performance and Efficient 3D Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Mostafijur Rahman",
      "Radu Marculescu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Exploring_Historical_Information_for_RGBE_Visual_Tracking_with_Mamba_CVPR_2025_paper.html": {
    "title": "Exploring Historical Information for RGBE Visual Tracking with Mamba",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanyu Sun",
      "Jiqing Zhang",
      "Yang Wang",
      "Huilin Ge",
      "Qianchen Xia",
      "Baocai Yin",
      "Xin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Gyro-based_Neural_Single_Image_Deblurring_CVPR_2025_paper.html": {
    "title": "Gyro-based Neural Single Image Deblurring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heemin Yang",
      "Jaesung Rim",
      "Seungyong Lee",
      "Seung-Hwan Baek",
      "Sunghyun Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gu_ArtiScene_Language-Driven_Artistic_3D_Scene_Generation_Through_Image_Intermediary_CVPR_2025_paper.html": {
    "title": "ArtiScene: Language-Driven Artistic 3D Scene Generation Through Image Intermediary",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeqi Gu",
      "Yin Cui",
      "Zhaoshuo Li",
      "Fangyin Wei",
      "Yunhao Ge",
      "Jinwei Gu",
      "Ming-Yu Liu",
      "Abe Davis",
      "Yifan Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MobileH2R_Learning_Generalizable_Human_to_Mobile_Robot_Handover_Exclusively_from_CVPR_2025_paper.html": {
    "title": "MobileH2R: Learning Generalizable Human to Mobile Robot Handover Exclusively from Scalable and Diverse Synthetic Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zifan Wang",
      "Ziqing Chen",
      "Junyu Chen",
      "Jilong Wang",
      "Yuxin Yang",
      "Yunze Liu",
      "Xueyi Liu",
      "He Wang",
      "Li Yi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Improving_Sound_Source_Localization_with_Joint_Slot_Attention_on_Image_CVPR_2025_paper.html": {
    "title": "Improving Sound Source Localization with Joint Slot Attention on Image and Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inho Kim",
      "Youngkil Song",
      "Jicheol Park",
      "Won Hwa Kim",
      "Suha Kwak"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hariat_Improved_Monocular_Depth_Prediction_Using_Distance_Transform_Over_Pre-semantic_Contours_CVPR_2025_paper.html": {
    "title": "Improved Monocular Depth Prediction Using Distance Transform Over Pre-semantic Contours with Self-supervised Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marwane Hariat",
      "Antoine Manzanera",
      "David Filliat"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Heep_Feature-Preserving_Mesh_Decimation_for_Normal_Integration_CVPR_2025_paper.html": {
    "title": "Feature-Preserving Mesh Decimation for Normal Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moritz Heep",
      "Sven Behnke",
      "Eduard Zell"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and_CVPR_2025_paper.html": {
    "title": "Is this Generated Person Existed in Real-world? Fine-grained Detecting and Calibrating Abnormal Human-body",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeqing Wang",
      "Qingyang Ma",
      "Wentao Wan",
      "Haojie Li",
      "Keze Wang",
      "Yonghong Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cha_PERSE_Personalized_3D_Generative_Avatars_from_A_Single_Portrait_CVPR_2025_paper.html": {
    "title": "PERSE: Personalized 3D Generative Avatars from A Single Portrait",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunsoo Cha",
      "Inhee Lee",
      "Hanbyul Joo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Automated_Generation_of_Challenging_Multiple-Choice_Questions_for_Vision_Language_Model_CVPR_2025_paper.html": {
    "title": "Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhui Zhang",
      "Yuchang Su",
      "Yiming Liu",
      "Xiaohan Wang",
      "James Burgess",
      "Elaine Sui",
      "Chenyu Wang",
      "Josiah Aklilu",
      "Alejandro Lozano",
      "Anjiang Wei",
      "Ludwig Schmidt",
      "Serena Yeung-Levy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_DexHandDiff_Interaction-aware_Diffusion_Planning_for_Adaptive_Dexterous_Manipulation_CVPR_2025_paper.html": {
    "title": "DexHandDiff: Interaction-aware Diffusion Planning for Adaptive Dexterous Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixuan Liang",
      "Yao Mu",
      "Yixiao Wang",
      "Tianxing Chen",
      "Wenqi Shao",
      "Wei Zhan",
      "Masayoshi Tomizuka",
      "Ping Luo",
      "Mingyu Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cha_VerbDiff_Text-Only_Diffusion_Models_with_Enhanced_Interaction_Awareness_CVPR_2025_paper.html": {
    "title": "VerbDiff: Text-Only Diffusion Models with Enhanced Interaction Awareness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SeungJu Cha",
      "Kwanyoung Lee",
      "Ye-Chan Kim",
      "Hyunwoo Oh",
      "Dong-Jin Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy_CVPR_2025_paper.html": {
    "title": "ROLL: Robust Noisy Pseudo-label Learning for Multi-View Clustering with Noisy Correspondence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Sun",
      "Yongxiang Li",
      "Zhenwen Ren",
      "Guiduo Duan",
      "Dezhong Peng",
      "Peng Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image_CVPR_2025_paper.html": {
    "title": "Towards In-the-wild 3D Plane Reconstruction from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen Liu",
      "Rui Yu",
      "Sili Chen",
      "Sharon X. Huang",
      "Hengkai Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rusanovsky_Memories_of_Forgotten_Concepts_CVPR_2025_paper.html": {
    "title": "Memories of Forgotten Concepts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matan Rusanovsky",
      "Shimon Malnick",
      "Amir Jevnisek",
      "Ohad Fried",
      "Shai Avidan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Dynamic_Stereotype_Theory_Induced_Micro-expression_Recognition_with_Oriented_Deformation_CVPR_2025_paper.html": {
    "title": "Dynamic Stereotype Theory Induced Micro-expression Recognition with Oriented Deformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bohao Zhang",
      "Xuejiao Wang",
      "Changbo Wang",
      "Gaoqi He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Poesina_PQPP_A_Joint_Benchmark_for_Text-to-Image_Prompt_and_Query_Performance_CVPR_2025_paper.html": {
    "title": "PQPP: A Joint Benchmark for Text-to-Image Prompt and Query Performance Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eduard Poesina",
      "Adriana Valentina Costache",
      "Adrian-Gabriel Chifu",
      "Josiane Mothe",
      "Radu Tudor Ionescu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Antony_CheXwhatsApp_A_Dataset_for_Exploring_Challenges_in_the_Diagnosis_of_CVPR_2025_paper.html": {
    "title": "CheXwhatsApp: A Dataset for Exploring Challenges in the Diagnosis of Chest X-rays through Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mariamma Antony",
      "Rajiv Porana",
      "Sahil M Lathiya",
      "Siva Teja Kakileti",
      "Chiranjib Bhattacharyya"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_PSBD_Prediction_Shift_Uncertainty_Unlocks_Backdoor_Detection_CVPR_2025_paper.html": {
    "title": "PSBD: Prediction Shift Uncertainty Unlocks Backdoor Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Li",
      "Pin-Yu Chen",
      "Sijia Liu",
      "Ren Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_Degradation-Aware_Feature_Perturbation_for_All-in-One_Image_Restoration_CVPR_2025_paper.html": {
    "title": "Degradation-Aware Feature Perturbation for All-in-One Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangpeng Tian",
      "Xiangyu Liao",
      "Xiao Liu",
      "Meng Li",
      "Chao Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gu_ACL_Activating_Capability_of_Linear_Attention_for_Image_Restoration_CVPR_2025_paper.html": {
    "title": "ACL: Activating Capability of Linear Attention for Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yubin Gu",
      "Yuan Meng",
      "Jiayi Ji",
      "Xiaoshuai Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rajagopalan_GenDeg_Diffusion-based_Degradation_Synthesis_for_Generalizable_All-In-One_Image_Restoration_CVPR_2025_paper.html": {
    "title": "GenDeg: Diffusion-based Degradation Synthesis for Generalizable All-In-One Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sudarshan Rajagopalan",
      "Nithin Gopalakrishnan Nair",
      "Jay N. Paranjape",
      "Vishal M. Patel"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_Phoenix_A_Motion-based_Self-Reflection_Framework_for_Fine-grained_Robotic_Action_Correction_CVPR_2025_paper.html": {
    "title": "Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenke Xia",
      "Ruoxuan Feng",
      "Dong Wang",
      "Di Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mei_The_Power_of_Context_How_Multimodality_Improves_Image_Super-Resolution_CVPR_2025_paper.html": {
    "title": "The Power of Context: How Multimodality Improves Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangfu Mei",
      "Hossein Talebi",
      "Mojtaba Ardakani",
      "Vishal M. Patel",
      "Peyman Milanfar",
      "Mauricio Delbracio"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_MARBLE_Material_Recomposition_and_Blending_in_CLIP-Space_CVPR_2025_paper.html": {
    "title": "MARBLE: Material Recomposition and Blending in CLIP-Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ta Ying Cheng",
      "Prafull Sharma",
      "Mark Boss",
      "Varun Jampani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization_CVPR_2025_paper.html": {
    "title": "Multirate Neural Image Compression with Adaptive Lattice Vector Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Xu",
      "Xiaolin Wu",
      "Xi Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kong_EventFly_Event_Camera_Perception_from_Ground_to_the_Sky_CVPR_2025_paper.html": {
    "title": "EventFly: Event Camera Perception from Ground to the Sky",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingdong Kong",
      "Dongyue Lu",
      "Xiang Xu",
      "Lai Xing Ng",
      "Wei Tsang Ooi",
      "Benoit R. Cottereau"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xing_Detect_Any_Mirrors_Boosting_Learning_Reliability_on_Large-Scale_Unlabeled_Data_CVPR_2025_paper.html": {
    "title": "Detect Any Mirrors: Boosting Learning Reliability on Large-Scale Unlabeled Data with an Iterative Data Engine",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaohu Xing",
      "Lihao Liu",
      "Yijun Yang",
      "Hongqiu Wang",
      "Tian Ye",
      "Sixiang Chen",
      "Wenxue Li",
      "Guang Liu",
      "Lei Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching_CVPR_2025_paper.html": {
    "title": "CH3Depth: Efficient and Flexible Depth Foundation Model with Flow Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Li",
      "Yiran Wang",
      "Jinghong Zheng",
      "Junrui Zhang",
      "Liao Shen",
      "Tianqi Liu",
      "Zhiguo Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jang_Pow3R_Empowering_Unconstrained_3D_Reconstruction_with_Camera_and_Scene_Priors_CVPR_2025_paper.html": {
    "title": "Pow3R: Empowering Unconstrained 3D Reconstruction with Camera and Scene Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonbong Jang",
      "Philippe Weinzaepfel",
      "Vincent Leroy",
      "Lourdes Agapito",
      "Jerome Revaud"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kong_Efficient_Visual_State_Space_Model_for_Image_Deblurring_CVPR_2025_paper.html": {
    "title": "Efficient Visual State Space Model for Image Deblurring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingshun Kong",
      "Jiangxin Dong",
      "Jinhui Tang",
      "Ming-Hsuan Yang",
      "Jinshan Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_4D_LangSplat_4D_Language_Gaussian_Splatting_via_Multimodal_Large_Language_CVPR_2025_paper.html": {
    "title": "4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanhua Li",
      "Renping Zhou",
      "Jiawei Zhou",
      "Yingwei Song",
      "Johannes Herter",
      "Minghan Qin",
      "Gao Huang",
      "Hanspeter Pfister"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking_CVPR_2025_paper.html": {
    "title": "MambaVLT: Time-Evolving Multimodal State Space Model for Vision-Language Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinqi Liu",
      "Li Zhou",
      "Zikun Zhou",
      "Jianqiu Chen",
      "Zhenyu He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Vuillecard_Enhancing_3D_Gaze_Estimation_in_the_Wild_using_Weak_Supervision_CVPR_2025_paper.html": {
    "title": "Enhancing 3D Gaze Estimation in the Wild using Weak Supervision with Gaze Following Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre Vuillecard",
      "Jean-Marc Odobez"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jia_Reward_Fine-Tuning_Two-Step_Diffusion_Models_via_Learning_Differentiable_Latent-Space_Surrogate_CVPR_2025_paper.html": {
    "title": "Reward Fine-Tuning Two-Step Diffusion Models via Learning Differentiable Latent-Space Surrogate Reward",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwei Jia",
      "Yuesong Nan",
      "Huixi Zhao",
      "Gengdai Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Detecting_Out-of-Distribution_Through_the_Lens_of_Neural_Collapse_CVPR_2025_paper.html": {
    "title": "Detecting Out-of-Distribution Through the Lens of Neural Collapse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Litian Liu",
      "Yao Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Adaptive_Parameter_Selection_for_Tuning_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "Adaptive Parameter Selection for Tuning Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Zhang",
      "Yi-Xuan Deng",
      "Meng-Hao Guo",
      "Shi-Min Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hosseininejad_MotionMap_Representing_Multimodality_in_Human_Pose_Forecasting_CVPR_2025_paper.html": {
    "title": "MotionMap: Representing Multimodality in Human Pose Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reyhaneh Hosseininejad",
      "Megh Shukla",
      "Saeed Saadatnejad",
      "Mathieu Salzmann",
      "Alexandre Alahi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Learning-enabled_Polynomial_Lyapunov_Function_Synthesis_via_High-Accuracy_Counterexample-Guided_Framework_CVPR_2025_paper.html": {
    "title": "Learning-enabled Polynomial Lyapunov Function Synthesis via High-Accuracy Counterexample-Guided Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanrui Zhao",
      "Niuniu Qi",
      "Mengxin Ren",
      "Banglong Liu",
      "Shuming Shi",
      "Zhengfeng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_Factored-NeuS_Reconstructing_Surfaces_Illumination_and_Materials_of_Possibly_Glossy_Objects_CVPR_2025_paper.html": {
    "title": "Factored-NeuS: Reconstructing Surfaces, Illumination, and Materials of Possibly Glossy Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Fan",
      "Ningjing Fan",
      "Ivan Skorokhodov",
      "Oleg Voynov",
      "Savva Ignatyev",
      "Evgeny Burnaev",
      "Peter Wonka",
      "Yiqun Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_GaussianSpa_An_Optimizing-Sparsifying_Simplification_Framework_for_Compact_and_High-Quality_3D_CVPR_2025_paper.html": {
    "title": "GaussianSpa: An \"Optimizing-Sparsifying\" Simplification Framework for Compact and High-Quality 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangming Zhang",
      "Wenqi Jia",
      "Wei Niu",
      "Miao Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Sparse2DGS_Geometry-Prioritized_Gaussian_Splatting_for_Surface_Reconstruction_from_Sparse_Views_CVPR_2025_paper.html": {
    "title": "Sparse2DGS: Geometry-Prioritized Gaussian Splatting for Surface Reconstruction from Sparse Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiang Wu",
      "Rui Li",
      "Yu Zhu",
      "Rong Guo",
      "Jinqiu Sun",
      "Yanning Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kushwaha_VinTAGe_Joint_Video_and_Text_Conditioning_for_Holistic_Audio_Generation_CVPR_2025_paper.html": {
    "title": "VinTAGe: Joint Video and Text Conditioning for Holistic Audio Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saksham Singh Kushwaha",
      "Yapeng Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dai_Efficient_Decoupled_Feature_3D_Gaussian_Splatting_via_Hierarchical_Compression_CVPR_2025_paper.html": {
    "title": "Efficient Decoupled Feature 3D Gaussian Splatting via Hierarchical Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenqi Dai",
      "Ting Liu",
      "Yanning Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yao_CountLLM_Towards_Generalizable_Repetitive_Action_Counting_via_Large_Language_Model_CVPR_2025_paper.html": {
    "title": "CountLLM: Towards Generalizable Repetitive Action Counting via Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyu Yao",
      "Xuxin Cheng",
      "Zhiqi Huang",
      "Lei Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Navigating_the_Unseen_Zero-shot_Scene_Graph_Generation_via_Capsule-Based_Equivariant_CVPR_2025_paper.html": {
    "title": "Navigating the Unseen: Zero-shot Scene Graph Generation via Capsule-Based Equivariant Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhuan Huang",
      "Yi JI",
      "Guiqian Zhu",
      "Li Ying",
      "Chunping Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models_CVPR_2025_paper.html": {
    "title": "VL-RewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Li",
      "Yuancheng Wei",
      "Zhihui Xie",
      "Xuqing Yang",
      "Yifan Song",
      "Peiyi Wang",
      "Chenxin An",
      "Tianyu Liu",
      "Sujian Li",
      "Bill Yuchen Lin",
      "Lingpeng Kong",
      "Qi Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chang_ASHiTA_Automatic_Scene-grounded_HIerarchical_Task_Analysis_CVPR_2025_paper.html": {
    "title": "ASHiTA: Automatic Scene-grounded HIerarchical Task Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun Chang",
      "Leonor Fermoselle",
      "Duy Ta",
      "Bernadette Bucher",
      "Luca Carlone",
      "Jiuguang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Patient-Level_Anatomy_Meets_Scanning-Level_Physics_Personalized_Federated_Low-Dose_CT_Denoising_CVPR_2025_paper.html": {
    "title": "Patient-Level Anatomy Meets Scanning-Level Physics: Personalized Federated Low-Dose CT Denoising Empowered by Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyuan Yang",
      "Yingyu Chen",
      "Zhiwen Wang",
      "Hongming Shan",
      "Yang Chen",
      "Yi Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Choi_Exploiting_Deblurring_Networks_for_Radiance_Fields_CVPR_2025_paper.html": {
    "title": "Exploiting Deblurring Networks for Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haeyun Choi",
      "Heemin Yang",
      "Janghyeok Han",
      "Sunghyun Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chang_Rethinking_Lanes_and_Points_in_Complex_Scenarios_for_Monocular_3D_CVPR_2025_paper.html": {
    "title": "Rethinking Lanes and Points in Complex Scenarios for Monocular 3D Lane Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Chang",
      "Junjie Huang",
      "Xiaofeng Wang",
      "Yun Ye",
      "Zhujin Liang",
      "Yi Shan",
      "Dalong Du",
      "Xingang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_SPAR3D_Stable_Point-Aware_Reconstruction_of_3D_Objects_from_Single_Images_CVPR_2025_paper.html": {
    "title": "SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Huang",
      "Mark Boss",
      "Aaryaman Vasishta",
      "James M. Rehg",
      "Varun Jampani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Discovering_Fine-Grained_Visual-Concept_Relations_by_Disentangled_Optimal_Transport_Concept_Bottleneck_CVPR_2025_paper.html": {
    "title": "Discovering Fine-Grained Visual-Concept Relations by Disentangled Optimal Transport Concept Bottleneck Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Xie",
      "Zequn Zeng",
      "Hao Zhang",
      "Yucheng Ding",
      "Yi Wang",
      "Zhengjue Wang",
      "Bo Chen",
      "Hongwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation_CVPR_2025_paper.html": {
    "title": "Focus-N-Fix: Region-Aware Fine-Tuning for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoying Xing",
      "Avinab Saha",
      "Junfeng He",
      "Susan Hao",
      "Paul Vicol",
      "Moonkyung Ryu",
      "Gang Li",
      "Sahil Singla",
      "Sarah Young",
      "Yinxiao Li",
      "Feng Yang",
      "Deepak Ramachandran"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_RoomTour3D_Geometry-Aware_Video-Instruction_Tuning_for_Embodied_Navigation_CVPR_2025_paper.html": {
    "title": "RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingfei Han",
      "Liang Ma",
      "Kamila Zhumakhanova",
      "Ekaterina Radionova",
      "Jingyi Zhang",
      "Xiaojun Chang",
      "Xiaodan Liang",
      "Ivan Laptev"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nisar_PSA-SSL_Pose_and_Size-aware_Self-Supervised_Learning_on_LiDAR_Point_Clouds_CVPR_2025_paper.html": {
    "title": "PSA-SSL: Pose and Size-aware Self-Supervised Learning on LiDAR Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Barza Nisar",
      "Steven L. Waslander"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ko_Bringing_CLIP_to_the_Clinic_Dynamic_Soft_Labels_and_Negation-Aware_CVPR_2025_paper.html": {
    "title": "Bringing CLIP to the Clinic: Dynamic Soft Labels and Negation-Aware Learning for Medical Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanbin Ko",
      "Chang-Min Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient_CVPR_2025_paper.html": {
    "title": "SnapGen: Taming High-Resolution Text-to-Image Models for Mobile Devices with Efficient Architectures and Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jierun Chen",
      "Dongting Hu",
      "Xijie Huang",
      "Huseyin Coskun",
      "Arpit Sahni",
      "Aarush Gupta",
      "Anujraaj Goyal",
      "Dishani Lahiri",
      "Rajesh Singh",
      "Yerlan Idelbayev",
      "Junli Cao",
      "Yanyu Li",
      "Kwang-Ting Cheng",
      "S.-H. Gary Chan",
      "Mingming Gong",
      "Sergey Tulyakov",
      "Anil Kag",
      "Yanwu Xu",
      "Jian Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal_CVPR_2025_paper.html": {
    "title": "Label Shift Meets Online Learning: Ensuring Consistent Adaptation with Universal Dynamic Regret",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yucong Dai",
      "Shilin Gu",
      "Ruidong Fan",
      "Chao Xu",
      "Chenping Hou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_A_Physics-Informed_Blur_Learning_Framework_for_Imaging_Systems_CVPR_2025_paper.html": {
    "title": "A Physics-Informed Blur Learning Framework for Imaging Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liqun Chen",
      "Yuxuan Li",
      "Jun Dai",
      "Jinwei Gu",
      "Tianfan Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_A_Semantic_Knowledge_Complementarity_based_Decoupling_Framework_for_Semi-supervised_Class-imbalanced_CVPR_2025_paper.html": {
    "title": "A Semantic Knowledge Complementarity based Decoupling Framework for Semi-supervised Class-imbalanced Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Zhang",
      "Guanchun Yin",
      "Bo Zhang",
      "Wu Liu",
      "Xiuzhuang Zhou",
      "Wendong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_Community_Forensics_Using_Thousands_of_Generators_to_Train_Fake_Image_CVPR_2025_paper.html": {
    "title": "Community Forensics: Using Thousands of Generators to Train Fake Image Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongsoo Park",
      "Andrew Owens"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_ModeSeq_Taming_Sparse_Multimodal_Motion_Prediction_with_Sequential_Mode_Modeling_CVPR_2025_paper.html": {
    "title": "ModeSeq: Taming Sparse Multimodal Motion Prediction with Sequential Mode Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zikang Zhou",
      "Hengjian Zhou",
      "Haibo Hu",
      "Zihao Wen",
      "Jianping Wang",
      "Yung-Hui Li",
      "Yu-Kai Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Stuyck_Quaffure_Real-Time_Quasi-Static_Neural_Hair_Simulation_CVPR_2025_paper.html": {
    "title": "Quaffure: Real-Time Quasi-Static Neural Hair Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuur Stuyck",
      "Gene Wei-Chin Lin",
      "Egor Larionov",
      "Hsiao-yu Chen",
      "Aljaz Bozic",
      "Nikolaos Sarafianos",
      "Doug Roble"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jia_Towards_Practical_Real-Time_Neural_Video_Compression_CVPR_2025_paper.html": {
    "title": "Towards Practical Real-Time Neural Video Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyang Jia",
      "Bin Li",
      "Jiahao Li",
      "Wenxuan Xie",
      "Linfeng Qi",
      "Houqiang Li",
      "Yan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_DepthSplat_Connecting_Gaussian_Splatting_and_Depth_CVPR_2025_paper.html": {
    "title": "DepthSplat: Connecting Gaussian Splatting and Depth",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haofei Xu",
      "Songyou Peng",
      "Fangjinhua Wang",
      "Hermann Blum",
      "Daniel Barath",
      "Andreas Geiger",
      "Marc Pollefeys"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xing_LumiNet_Latent_Intrinsics_Meets_Diffusion_Models_for_Indoor_Scene_Relighting_CVPR_2025_paper.html": {
    "title": "LumiNet: Latent Intrinsics Meets Diffusion Models for Indoor Scene Relighting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyan Xing",
      "Konrad Groh",
      "Sezer Karaoglu",
      "Theo Gevers",
      "Anand Bhattad"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_FedBiP_Heterogeneous_One-Shot_Federated_Learning_with_Personalized_Latent_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "FedBiP: Heterogeneous One-Shot Federated Learning with Personalized Latent Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haokun Chen",
      "Hang Li",
      "Yao Zhang",
      "Jinhe Bi",
      "Gengyuan Zhang",
      "Yueqi Zhang",
      "Philip Torr",
      "Jindong Gu",
      "Denis Krompass",
      "Volker Tresp"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_DiC_Rethinking_Conv3x3_Designs_in_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "DiC: Rethinking Conv3x3 Designs in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchuan Tian",
      "Jing Han",
      "Chengcheng Wang",
      "Yuchen Liang",
      "Chao Xu",
      "Hanting Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rockwell_Dynamic_Camera_Poses_and_Where_to_Find_Them_CVPR_2025_paper.html": {
    "title": "Dynamic Camera Poses and Where to Find Them",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chris Rockwell",
      "Joseph Tung",
      "Tsung-Yi Lin",
      "Ming-Yu Liu",
      "David F. Fouhey",
      "Chen-Hsuan Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion_CVPR_2025_paper.html": {
    "title": "MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahui Lei",
      "Yijia Weng",
      "Adam W. Harley",
      "Leonidas Guibas",
      "Kostas Daniilidis"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_GCE-Pose_Global_Context_Enhancement_for_Category-level_Object_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "GCE-Pose: Global Context Enhancement for Category-level Object Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihang Li",
      "Hongli XU",
      "Junwen Huang",
      "Hyunjun Jung",
      "Peter KT Yu",
      "Nassir Navab",
      "Benjamin Busam"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_OmniGen_Unified_Image_Generation_CVPR_2025_paper.html": {
    "title": "OmniGen: Unified Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shitao Xiao",
      "Yueze Wang",
      "Junjie Zhou",
      "Huaying Yuan",
      "Xingrun Xing",
      "Ruiran Yan",
      "Chaofan Li",
      "Shuting Wang",
      "Tiejun Huang",
      "Zheng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems_CVPR_2025_paper.html": {
    "title": "QuCOOP: A Versatile Framework for Solving Composite and Binary-Parametrised Problems on Quantum Annealers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Natacha Kuete Meli",
      "Vladislav Golyanik",
      "Marcel Seelbach Benkner",
      "Michael Moeller"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Mesh_Mamba_A_Unified_State_Space_Model_for_Saliency_Prediction_CVPR_2025_paper.html": {
    "title": "Mesh Mamba: A Unified State Space Model for Saliency Prediction in Non-Textured and Textured Meshes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiwei Zhang",
      "Dandan Zhu",
      "Xiongkuo Min",
      "Guangtao Zhai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_Not_Just_Text_Uncovering_Vision_Modality_Typographic_Threats_in_Image_CVPR_2025_paper.html": {
    "title": "Not Just Text: Uncovering Vision Modality Typographic Threats in Image Generation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Cheng",
      "Erjia Xiao",
      "Jiayan Yang",
      "Jiahang Cao",
      "Qiang Zhang",
      "Jize Zhang",
      "Kaidi Xu",
      "Jindong Gu",
      "Renjing Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qu_SILMM_Self-Improving_Large_Multimodal_Models_for_Compositional_Text-to-Image_Generation_CVPR_2025_paper.html": {
    "title": "SILMM: Self-Improving Large Multimodal Models for Compositional Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leigang Qu",
      "Haochuan Li",
      "Wenjie Wang",
      "Xiang Liu",
      "Juncheng Li",
      "Liqiang Nie",
      "Tat-Seng Chua"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Calibrated_Multi-Preference_Optimization_for_Aligning_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Calibrated Multi-Preference Optimization for Aligning Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyungmin Lee",
      "Xiahong Li",
      "Qifei Wang",
      "Junfeng He",
      "Junjie Ke",
      "Ming-Hsuan Yang",
      "Irfan Essa",
      "Jinwoo Shin",
      "Feng Yang",
      "Yinxiao Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Learning_from_Neighbors_Category_Extrapolation_for_Long-Tail_Learning_CVPR_2025_paper.html": {
    "title": "Learning from Neighbors: Category Extrapolation for Long-Tail Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shizhen Zhao",
      "Xin Wen",
      "Jiahui Liu",
      "Chuofan Ma",
      "Chunfeng Yuan",
      "Xiaojuan Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion_CVPR_2025_paper.html": {
    "title": "Material Anything: Generating Materials for Any 3D Object via Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Huang",
      "Tengfei Wang",
      "Ziwei Liu",
      "Qing Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization_CVPR_2025_paper.html": {
    "title": "TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Pan",
      "Zeshi Yang",
      "Zhiyang Dou",
      "Wenjia Wang",
      "Buzhen Huang",
      "Bo Dai",
      "Taku Komura",
      "Jingbo Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Maruani_ShapeShifter_3D_Variations_Using_Multiscale_and_Sparse_Point-Voxel_Diffusion_CVPR_2025_paper.html": {
    "title": "ShapeShifter: 3D Variations Using Multiscale and Sparse Point-Voxel Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nissim Maruani",
      "Wang Yifan",
      "Matthew Fisher",
      "Pierre Alliez",
      "Mathieu Desbrun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based_CVPR_2025_paper.html": {
    "title": "ImagineFSL: Self-Supervised Pretraining Matters on Imagined Base Set for VLM-based Few-shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyuan Yang",
      "Xiaoou Li",
      "Jiaming Lv",
      "Xianjun Cheng",
      "Qilong Wang",
      "Peihua Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bae_Continuous_Locomotive_Crowd_Behavior_Generation_CVPR_2025_paper.html": {
    "title": "Continuous Locomotive Crowd Behavior Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inhwan Bae",
      "Junoh Lee",
      "Hae-Gon Jeon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness_CVPR_2025_paper.html": {
    "title": "Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beier Zhu",
      "Jiequan Cui",
      "Hanwang Zhang",
      "Chi Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Implicit_Bias_Injection_Attacks_against_Text-to-Image_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Implicit Bias Injection Attacks against Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huayang Huang",
      "Xiangye Jin",
      "Jiaxu Miao",
      "Yu Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gu_ROICtrl_Boosting_Instance_Control_for_Visual_Generation_CVPR_2025_paper.html": {
    "title": "ROICtrl: Boosting Instance Control for Visual Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchao Gu",
      "Yipin Zhou",
      "Yunfan Ye",
      "Yixin Nie",
      "Licheng Yu",
      "Pingchuan Ma",
      "Kevin Qinghong Lin",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images_CVPR_2025_paper.html": {
    "title": "FRESA: Feedforward Reconstruction of Personalized Skinned Avatars from Few Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rong Wang",
      "Fabian Prada",
      "Ziyan Wang",
      "Zhongshi Jiang",
      "Chengxiang Yin",
      "Junxuan Li",
      "Shunsuke Saito",
      "Igor Santesteban",
      "Javier Romero",
      "Rohan Joshi",
      "Hongdong Li",
      "Jason Saragih",
      "Yaser Sheikh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_ReasonGrounder_LVLM-Guided_Hierarchical_Feature_Splatting_for_Open-Vocabulary_3D_Visual_Grounding_CVPR_2025_paper.html": {
    "title": "ReasonGrounder: LVLM-Guided Hierarchical Feature Splatting for Open-Vocabulary 3D Visual Grounding and Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyang Liu",
      "Yikai Wang",
      "Sixiao Zheng",
      "Tongying Pan",
      "Longfei Liang",
      "Yanwei Fu",
      "Xiangyang Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Cropper_Vision-Language_Model_for_Image_Cropping_through_In-Context_Learning_CVPR_2025_paper.html": {
    "title": "Cropper: Vision-Language Model for Image Cropping through In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seung Hyun Lee",
      "Jijun Jiang",
      "Yiran Xu",
      "Zhuofang Li",
      "Junjie Ke",
      "Yinxiao Li",
      "Junfeng He",
      "Steven Hickson",
      "Katie Datsenko",
      "Sangpil Kim",
      "Ming-Hsuan Yang",
      "Irfan Essa",
      "Feng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Meng_Advancing_Adversarial_Robustness_in_GNeRFs_The_IL2-NeRF_Attack_CVPR_2025_paper.html": {
    "title": "Advancing Adversarial Robustness in GNeRFs: The IL2-NeRF Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicole Meng",
      "Caleb Manicke",
      "Ronak Sahu",
      "Caiwen Ding",
      "Yingjie Lao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image_CVPR_2025_paper.html": {
    "title": "WonderWorld: Interactive 3D Scene Generation from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hong-Xing Yu",
      "Haoyi Duan",
      "Charles Herrmann",
      "William T. Freeman",
      "Jiajun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_A_Lightweight_UDF_Learning_Framework_for_3D_Reconstruction_Based_on_CVPR_2025_paper.html": {
    "title": "A Lightweight UDF Learning Framework for 3D Reconstruction Based on Local Shape Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangbei Hu",
      "Yanggeng Li",
      "Fei Hou",
      "Junhui Hou",
      "Zhebin Zhang",
      "Shengfa Wang",
      "Na Lei",
      "Ying He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences_CVPR_2025_paper.html": {
    "title": "DiffCAM: Data-Driven Saliency Maps by Capturing Feature Differences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingjian Li",
      "Qiming Zhao",
      "Neelesh Bisht",
      "Mostofa Rafid Uddin",
      "Jin Yu Kim",
      "Bryan Zhang",
      "Min Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_PolarNeXt_Rethink_Instance_Segmentation_with_Polar_Representation_CVPR_2025_paper.html": {
    "title": "PolarNeXt: Rethink Instance Segmentation with Polar Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacheng Sun",
      "Xinghong Zhou",
      "Yiqiang Wu",
      "Bin Zhu",
      "Jiaxuan Lu",
      "Yu Qin",
      "Xiaomao Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_ScaMo_Exploring_the_Scaling_Law_in_Autoregressive_Motion_Generation_Model_CVPR_2025_paper.html": {
    "title": "ScaMo: Exploring the Scaling Law in Autoregressive Motion Generation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shunlin Lu",
      "Jingbo Wang",
      "Zeyu Lu",
      "Ling-Hao Chen",
      "Wenxun Dai",
      "Junting Dong",
      "Zhiyang Dou",
      "Bo Dai",
      "Ruimao Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Barquero_From_Sparse_Signal_to_Smooth_Motion_Real-Time_Motion_Generation_with_CVPR_2025_paper.html": {
    "title": "From Sparse Signal to Smooth Motion: Real-Time Motion Generation with Rolling Prediction Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "German Barquero",
      "Nadine Bertsch",
      "Manojkumar Marramreddy",
      "Carlos Chacón",
      "Filippo Arcadu",
      "Ferran Rigual",
      "Nicky Sijia He",
      "Cristina Palmero",
      "Sergio Escalera",
      "Yuting Ye",
      "Robin Kips"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Imagine_and_Seek_Improving_Composed_Image_Retrieval_with_an_Imagined_CVPR_2025_paper.html": {
    "title": "Imagine and Seek: Improving Composed Image Retrieval with an Imagined Proxy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "You Li",
      "Fan Ma",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_EMOVA_Empowering_Language_Models_to_See_Hear_and_Speak_with_CVPR_2025_paper.html": {
    "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Chen",
      "Yunhao Gou",
      "Runhui Huang",
      "Zhili Liu",
      "Daxin Tan",
      "Jing Xu",
      "Chunwei Wang",
      "Yi Zhu",
      "Yihan Zeng",
      "Kuo Yang",
      "Dingdong Wang",
      "Kun Xiang",
      "Haoyuan Li",
      "Haoli Bai",
      "Jianhua Han",
      "Xiaohui Li",
      "Weike Jin",
      "Nian Xie",
      "Yu Zhang",
      "James T. Kwok",
      "Hengshuang Zhao",
      "Xiaodan Liang",
      "Dit-Yan Yeung",
      "Xiao Chen",
      "Zhenguo Li",
      "Wei Zhang",
      "Qun Liu",
      "Lanqing Hong",
      "Lu Hou",
      "Hang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_SAM-REF_Introducing_Image-Prompt_Synergy_during_Interaction_for_Detail_Enhancement_in_CVPR_2025_paper.html": {
    "title": "SAM-REF: Introducing Image-Prompt Synergy during Interaction for Detail Enhancement in the Segment Anything Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chongkai Yu",
      "Ting Liu",
      "Anqi Li",
      "Xiaochao Qu",
      "Chengjing Wu",
      "Luoqi Liu",
      "Xiaolin Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feijoo_DarkIR_Robust_Low-Light_Image_Restoration_CVPR_2025_paper.html": {
    "title": "DarkIR: Robust Low-Light Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Feijoo",
      "Juan C. Benito",
      "Alvaro Garcia",
      "Marcos V. Conde"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bai_R2C_Mapping_Room_to_Chessboard_to_Unlock_LLM_As_Low-Level_CVPR_2025_paper.html": {
    "title": "R2C: Mapping Room to Chessboard to Unlock LLM As Low-Level Action Planner",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Bai",
      "Hanxuan Li",
      "Bin Fu",
      "Chuyan Xiong",
      "Ruiping Wang",
      "Xilin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion_CVPR_2025_paper.html": {
    "title": "ICE: Intrinsic Concept Extraction from a Single Image via Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fernando Julio Cendra",
      "Kai Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_ASIGN_An_Anatomy-aware_Spatial_Imputation_Graphic_Network_for_3D_Spatial_CVPR_2025_paper.html": {
    "title": "ASIGN: An Anatomy-aware Spatial Imputation Graphic Network for 3D Spatial Transcriptomics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junchao Zhu",
      "Ruining Deng",
      "Tianyuan Yao",
      "Juming Xiong",
      "Chongyu Qu",
      "Junlin Guo",
      "Siqi Lu",
      "Mengmeng Yin",
      "Yu Wang",
      "Shilin Zhao",
      "Haichun Yang",
      "Yuankai Huo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qin_Reversing_Flow_for_Image_Restoration_CVPR_2025_paper.html": {
    "title": "Reversing Flow for Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haina Qin",
      "Wenyang Luo",
      "Libin Wang",
      "Dandan Zheng",
      "Jingdong Chen",
      "Ming Yang",
      "Bing Li",
      "Weiming Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Shadow_Generation_Using_Diffusion_Model_with_Geometry_Prior_CVPR_2025_paper.html": {
    "title": "Shadow Generation Using Diffusion Model with Geometry Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Zhao",
      "Qingyang Liu",
      "Xinhao Tao",
      "Li Niu",
      "Guangtao Zhai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zong_Rethinking_Epistemic_and_Aleatoric_Uncertainty_for_Active_Open-Set_Annotation_An_CVPR_2025_paper.html": {
    "title": "Rethinking Epistemic and Aleatoric Uncertainty for Active Open-Set Annotation: An Energy-Based Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen-Chen Zong",
      "Sheng-Jun Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nguyen_Any3DIS_Class-Agnostic_3D_Instance_Segmentation_by_2D_Mask_Tracking_CVPR_2025_paper.html": {
    "title": "Any3DIS: Class-Agnostic 3D Instance Segmentation by 2D Mask Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Phuc Nguyen",
      "Minh Luu",
      "Anh Tran",
      "Cuong Pham",
      "Khoi Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_FDS_Frequency-Aware_Denoising_Score_for_Text-Guided_Latent_Diffusion_Image_Editing_CVPR_2025_paper.html": {
    "title": "FDS: Frequency-Aware Denoising Score for Text-Guided Latent Diffusion Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufan Ren",
      "Zicong Jiang",
      "Tong Zhang",
      "Søren Forchhammer",
      "Sabine Süsstrunk"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_MMAR_Towards_Lossless_Multi-Modal_Auto-Regressive_Probabilistic_Modeling_CVPR_2025_paper.html": {
    "title": "MMAR: Towards Lossless Multi-Modal Auto-Regressive Probabilistic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Yang",
      "Dacheng Yin",
      "Yizhou Zhou",
      "Fengyun Rao",
      "Wei Zhai",
      "Yang Cao",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shan_ROS-SAM_High-Quality_Interactive_Segmentation_for_Remote_Sensing_Moving_Object_CVPR_2025_paper.html": {
    "title": "ROS-SAM: High-Quality Interactive Segmentation for Remote Sensing Moving Object",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Shan",
      "Yang Liu",
      "Lei Zhou",
      "Cheng Yan",
      "Heng Wang",
      "Xia Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Abulnaga_MultiMorph_On-demand_Atlas_Construction_CVPR_2025_paper.html": {
    "title": "MultiMorph: On-demand Atlas Construction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "S. Mazdak Abulnaga",
      "Andrew Hoopes",
      "Neel Dey",
      "Malte Hoffmann",
      "Bruce Fischl",
      "John Guttag",
      "Adrian Dalca"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nan_MI-DETR_An_Object_Detection_Model_with_Multi-time_Inquiries_Mechanism_CVPR_2025_paper.html": {
    "title": "MI-DETR: An Object Detection Model with Multi-time Inquiries Mechanism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixiong Nan",
      "Xianghong Li",
      "Jifeng Dai",
      "Tao Xiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_From_Prototypes_to_General_Distributions_An_Efficient_Curriculum_for_Masked_CVPR_2025_paper.html": {
    "title": "From Prototypes to General Distributions: An Efficient Curriculum for Masked Image Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhong Lin",
      "Cheng-En Wu",
      "Huanran Li",
      "Jifan Zhang",
      "Yu Hen Hu",
      "Pedro Morgado"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_Synthetic_Visual_Genome_CVPR_2025_paper.html": {
    "title": "Synthetic Visual Genome",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jae Sung Park",
      "Zixian Ma",
      "Linjie Li",
      "Chenhao Zheng",
      "Cheng-Yu Hsieh",
      "Ximing Lu",
      "Khyathi Chandu",
      "Quan Kong",
      "Norimasa Kobori",
      "Ali Farhadi",
      "Yejin Choi",
      "Ranjay Krishna"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Difference_Inversion_Interpolate_and_Isolate_the_Difference_with_Token_Consistency_CVPR_2025_paper.html": {
    "title": "Difference Inversion: Interpolate and Isolate the Difference with Token Consistency for Image Analogy Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunsoo Kim",
      "Donghyun Kim",
      "Suhyun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding_CVPR_2025_paper.html": {
    "title": "Octopus: Alleviating Hallucination via Dynamic Contrastive Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Suo",
      "Lijun Zhang",
      "Mengyang Sun",
      "Lin Yuanbo Wu",
      "Peng Wang",
      "Yanning Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_MTADiffusion_Mask_Text_Alignment_Diffusion_Model_for_Object_Inpainting_CVPR_2025_paper.html": {
    "title": "MTADiffusion: Mask Text Alignment Diffusion Model for Object Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Huang",
      "Ting Liu",
      "Yihang Wu",
      "Xiaochao Qu",
      "Luoqi Liu",
      "Xiaolin Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yoon_Stop_Learning_it_all_to_Mitigate_Visual_Hallucination_Focus_on_CVPR_2025_paper.html": {
    "title": "Stop Learning it all to Mitigate Visual Hallucination, Focus on the Hallucination Target",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dokyoon Yoon",
      "Youngsook Song",
      "Woomyoung Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Talon_Seeing_the_Abstract_Translating_the_Abstract_Language_for_Vision_Language_CVPR_2025_paper.html": {
    "title": "Seeing the Abstract: Translating the Abstract Language for Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davide Talon",
      "Federico Girella",
      "Ziyue Liu",
      "Marco Cristani",
      "Yiming Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Spiking_Transformer_Introducing_Accurate_Addition-Only_Spiking_Self-Attention_for_Transformer_CVPR_2025_paper.html": {
    "title": "Spiking Transformer: Introducing Accurate Addition-Only Spiking Self-Attention for Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Guo",
      "Xiaode Liu",
      "Yuanpei Chen",
      "Weihang Peng",
      "Yuhan Zhang",
      "Zhe Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Grounding_3D_Object_Affordance_with_Language_Instructions_Visual_Observations_and_CVPR_2025_paper.html": {
    "title": "Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "He Zhu",
      "Quyu Kong",
      "Kechun Xu",
      "Xunlong Xia",
      "Bing Deng",
      "Jieping Ye",
      "Rong Xiong",
      "Yue Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_MOVIS_Enhancing_Multi-Object_Novel_View_Synthesis_for_Indoor_Scenes_CVPR_2025_paper.html": {
    "title": "MOVIS: Enhancing Multi-Object Novel View Synthesis for Indoor Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruijie Lu",
      "Yixin Chen",
      "Junfeng Ni",
      "Baoxiong Jia",
      "Yu Liu",
      "Diwen Wan",
      "Gang Zeng",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bao_One-Step_Event-Driven_High-Speed_Autofocus_CVPR_2025_paper.html": {
    "title": "One-Step Event-Driven High-Speed Autofocus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Bao",
      "Shaohua Gao",
      "Wenyong Li",
      "Kaiwei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Symbolic_Representation_for_Any-to-Any_Generative_Tasks_CVPR_2025_paper.html": {
    "title": "Symbolic Representation for Any-to-Any Generative Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Chen",
      "Xiaoye Zhu",
      "Yue Wang",
      "Tianyang Liu",
      "Xinhui Chen",
      "Ying Chen",
      "Chak Tou Leong",
      "Yifei Ke",
      "Joseph Liu",
      "Yiwen Yuan",
      "Julian McAuley",
      "Li-jia Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Protecting_Your_Video_Content_Disrupting_Automated_Video-based_LLM_Annotations_CVPR_2025_paper.html": {
    "title": "Protecting Your Video Content: Disrupting Automated Video-based LLM Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haitong Liu",
      "Kuofeng Gao",
      "Yang Bai",
      "Jinmin Li",
      "Jinxiao Shan",
      "Tao Dai",
      "Shu-Tao Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cao_PanDA_Towards_Panoramic_Depth_Anything_with_Unlabeled_Panoramas_and_Mobius_CVPR_2025_paper.html": {
    "title": "PanDA: Towards Panoramic Depth Anything with Unlabeled Panoramas and Mobius Spatial Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zidong Cao",
      "Jinjing Zhu",
      "Weiming Zhang",
      "Hao Ai",
      "Haotian Bai",
      "Hengshuang Zhao",
      "Lin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Towards_High-fidelity_3D_Talking_Avatar_with_Personalized_Dynamic_Texture_CVPR_2025_paper.html": {
    "title": "Towards High-fidelity 3D Talking Avatar with Personalized Dynamic Texture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanchen Li",
      "Jianyu Wang",
      "Yuhao Cheng",
      "Yikun Zeng",
      "Xingyu Ren",
      "Wenhan Zhu",
      "Weiming Zhao",
      "Yichao Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Scene_Splatter_Momentum_3D_Scene_Generation_from_Single_Image_with_CVPR_2025_paper.html": {
    "title": "Scene Splatter: Momentum 3D Scene Generation from Single Image with Video Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengjun Zhang",
      "Jinzhao Li",
      "Xin Fei",
      "Hao Liu",
      "Yueqi Duan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_JiSAM_Alleviate_Labeling_Burden_and_Corner_Case_Problems_in_Autonomous_CVPR_2025_paper.html": {
    "title": "JiSAM: Alleviate Labeling Burden and Corner Case Problems in Autonomous Driving via Minimal Real-World Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runjian Chen",
      "Wenqi Shao",
      "Bo Zhang",
      "Shaoshuai Shi",
      "Li Jiang",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_OSMamba_Omnidirectional_Spectral_Mamba_with_Dual-Domain_Prior_Generator_for_Exposure_CVPR_2025_paper.html": {
    "title": "OSMamba: Omnidirectional Spectral Mamba with Dual-Domain Prior Generator for Exposure Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gehui Li",
      "Bin Chen",
      "Chen Zhao",
      "Lei Zhang",
      "Jian Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cao_Image_is_All_You_Need_to_Empower_Large-scale_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Image is All You Need to Empower Large-scale Diffusion Models for In-Domain Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pu Cao",
      "Feng Zhou",
      "Lu Yang",
      "Tianrui Huang",
      "Qing Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_MedUnifier_Unifying_Vision-and-Language_Pre-training_on_Medical_Data_with_Vision_Generation_CVPR_2025_paper.html": {
    "title": "MedUnifier: Unifying Vision-and-Language Pre-training on Medical Data with Vision Generation Task using Discrete Visual Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Zhang",
      "Yang Yu",
      "Yucheng Chen",
      "Xulei Yang",
      "Si Yong Yeo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders_CVPR_2025_paper.html": {
    "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqi Pang",
      "Tianyuan Zhang",
      "Fujun Luan",
      "Yunze Man",
      "Hao Tan",
      "Kai Zhang",
      "William T. Freeman",
      "Yu-Xiong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_Evolving_High-Quality_Rendering_and_Reconstruction_in_a_Unified_Framework_with_CVPR_2025_paper.html": {
    "title": "Evolving High-Quality Rendering and Reconstruction in a Unified Framework with Contribution-Adaptive Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "You Shen",
      "Zhipeng Zhang",
      "Xinyang Li",
      "Yansong Qu",
      "Yu Lin",
      "Shengchuan Zhang",
      "Liujuan Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_ArticulatedGS_Self-supervised_Digital_Twin_Modeling_of_Articulated_Objects_using_3D_CVPR_2025_paper.html": {
    "title": "ArticulatedGS: Self-supervised Digital Twin Modeling of Articulated Objects using 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junfu Guo",
      "Yu Xin",
      "Gaoyi Liu",
      "Kai Xu",
      "Ligang Liu",
      "Ruizhen Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dai_NoiseCtrl_A_Sampling-Algorithm-Agnostic_Conditional_Generation_Method_for_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "NoiseCtrl: A Sampling-Algorithm-Agnostic Conditional Generation Method for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longquan Dai",
      "He Wang",
      "Jinhui Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Seo_Leveraging_3D_Geometric_Priors_in_2D_Rotation_Symmetry_Detection_CVPR_2025_paper.html": {
    "title": "Leveraging 3D Geometric Priors in 2D Rotation Symmetry Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahyun Seo",
      "Minsu Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_KMD_Koopman_Multi-modality_Decomposition_for_Generalized_Brain_Tumor_Segmentation_under_CVPR_2025_paper.html": {
    "title": "KMD: Koopman Multi-modality Decomposition for Generalized Brain Tumor Segmentation under Incomplete Modalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Liu",
      "Haochuan Jiang",
      "Kaizhu Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Vid2Sim_Realistic_and_Interactive_Simulation_from_Video_for_Urban_Navigation_CVPR_2025_paper.html": {
    "title": "Vid2Sim: Realistic and Interactive Simulation from Video for Urban Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Xie",
      "Zhizheng Liu",
      "Zhenghao Peng",
      "Wayne Wu",
      "Bolei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth_CVPR_2025_paper.html": {
    "title": "DORNet: A Degradation Oriented and Regularized Network for Blind Depth Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengxue Wang",
      "Zhiqiang Yan",
      "Jinshan Pan",
      "Guangwei Gao",
      "Kai Zhang",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Alexandridis_Fractal_Calibration_for_Long-tailed_Object_Detection_CVPR_2025_paper.html": {
    "title": "Fractal Calibration for Long-tailed Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantinos Panagiotis Alexandridis",
      "Ismail Elezi",
      "Jiankang Deng",
      "Anh Nguyen",
      "Shan Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_M3GYM_A_Large-Scale_Multimodal_Multi-view_Multi-person_Pose_Dataset_for_Fitness_CVPR_2025_paper.html": {
    "title": "M3GYM: A Large-Scale Multimodal Multi-view Multi-person Pose Dataset for Fitness Activity Understanding in Real-world Settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingzheng Xu",
      "Ru Cao",
      "Xin Shen",
      "Heming Du",
      "Sen Wang",
      "Xin Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Noise_Calibration_and_Spatial-Frequency_Interactive_Network_for_STEM_Image_Enhancement_CVPR_2025_paper.html": {
    "title": "Noise Calibration and Spatial-Frequency Interactive Network for STEM Image Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hesong Li",
      "Ziqi Wu",
      "Ruiwen Shao",
      "Tao Zhang",
      "Ying Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation_CVPR_2025_paper.html": {
    "title": "Type-R: Automatically Retouching Typos for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wataru Shimoda",
      "Naoto Inoue",
      "Daichi Haraguchi",
      "Hayato Mitani",
      "Seiichi Uchida",
      "Kota Yamaguchi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_Video-3D_LLM_Learning_Position-Aware_Video_Representation_for_3D_Scene_Understanding_CVPR_2025_paper.html": {
    "title": "Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duo Zheng",
      "Shijia Huang",
      "Liwei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tan_FedSPA_Generalizable_Federated_Graph_Learning_under_Homophily_Heterogeneity_CVPR_2025_paper.html": {
    "title": "FedSPA: Generalizable Federated Graph Learning under Homophily Heterogeneity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Tan",
      "Guancheng Wan",
      "Wenke Huang",
      "He Li",
      "Guibin Zhang",
      "Carl Yang",
      "Mang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Homogeneous_Dynamics_Space_for_Heterogeneous_Humans_CVPR_2025_paper.html": {
    "title": "Homogeneous Dynamics Space for Heterogeneous Humans",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinpeng Liu",
      "Junxuan Liang",
      "Chenshuo Zhang",
      "Zixuan Cai",
      "Cewu Lu",
      "Yong-Lu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jung_TailedCore_Few-Shot_Sampling_for_Unsupervised_Long-Tail_Noisy_Anomaly_Detection_CVPR_2025_paper.html": {
    "title": "TailedCore: Few-Shot Sampling for Unsupervised Long-Tail Noisy Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoon Gyo Jung",
      "Jaewoo Park",
      "Jaeho Yoon",
      "Kuan-Chuan Peng",
      "Wonchul Kim",
      "Andrew Beng Jin Teoh",
      "Octavia Camps"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bao_GazeGene_Large-scale_Synthetic_Gaze_Dataset_with_3D_Eyeball_Annotations_CVPR_2025_paper.html": {
    "title": "GazeGene: Large-scale Synthetic Gaze Dataset with 3D Eyeball Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwei Bao",
      "Zhiming Wang",
      "Feng Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Koo_VideoHandles_Editing_3D_Object_Compositions_in_Videos_Using_Video_Generative_CVPR_2025_paper.html": {
    "title": "VideoHandles: Editing 3D Object Compositions in Videos Using Video Generative Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juil Koo",
      "Paul Guerrero",
      "Chun-Hao P. Huang",
      "Duygu Ceylan",
      "Minhyuk Sung"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at_CVPR_2025_paper.html": {
    "title": "Satellite Observations Guided Diffusion Model for Accurate Meteorological States at Arbitrary Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siwei Tu",
      "Ben Fei",
      "Weidong Yang",
      "Fenghua Ling",
      "Hao Chen",
      "Zili Liu",
      "Kun Chen",
      "Hang Fan",
      "Wanli Ouyang",
      "Lei Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Muller_Reconstructing_People_Places_and_Cameras_CVPR_2025_paper.html": {
    "title": "Reconstructing People, Places, and Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lea Müller",
      "Hongsuk Choi",
      "Anthony Zhang",
      "Brent Yi",
      "Jitendra Malik",
      "Angjoo Kanazawa"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion_CVPR_2025_paper.html": {
    "title": "InPO: Inversion Preference Optimization with Reparametrized DDIM for Efficient Diffusion Model Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhong Lu",
      "Qichao Wang",
      "Hengyuan Cao",
      "Xierui Wang",
      "Xiaoyin Xu",
      "Min Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_GaussTR_Foundation_Model-Aligned_Gaussian_Transformer_for_Self-Supervised_3D_Spatial_Understanding_CVPR_2025_paper.html": {
    "title": "GaussTR: Foundation Model-Aligned Gaussian Transformer for Self-Supervised 3D Spatial Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyi Jiang",
      "Liu Liu",
      "Tianheng Cheng",
      "Xinjie Wang",
      "Tianwei Lin",
      "Zhizhong Su",
      "Wenyu Liu",
      "Xinggang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Single_Domain_Generalization_for_Few-Shot_Counting_via_Universal_Representation_Matching_CVPR_2025_paper.html": {
    "title": "Single Domain Generalization for Few-Shot Counting via Universal Representation Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianing Chen",
      "Si Huo",
      "Borui Jiang",
      "Hailin Hu",
      "Xinghao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ke_Discovering_Hidden_Visual_Concepts_Beyond_Linguistic_Input_in_Infant_Learning_CVPR_2025_paper.html": {
    "title": "Discovering Hidden Visual Concepts Beyond Linguistic Input in Infant Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueyi Ke",
      "Satoshi Tsutsui",
      "Yayun Zhang",
      "Bihan Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal_CVPR_2025_paper.html": {
    "title": "Do We Always Need the Simplicity Bias? Looking for Optimal Inductive Biases in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Damien Teney",
      "Liangze Jiang",
      "Florin Gogianu",
      "Ehsan Abbasnejad"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_A_General_Adaptive_Dual-level_Weighting_Mechanism_for_Remote_Sensing_Pansharpening_CVPR_2025_paper.html": {
    "title": "A General Adaptive Dual-level Weighting Mechanism for Remote Sensing Pansharpening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Huang",
      "Haorui Chen",
      "Jiaxuan Ren",
      "Siran Peng",
      "Liangjian Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Debnath_RASP_Revisiting_3D_Anamorphic_Art_for_Shadow-Guided_Packing_of_Irregular_CVPR_2025_paper.html": {
    "title": "RASP: Revisiting 3D Anamorphic Art for Shadow-Guided Packing of Irregular Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soumyaratna Debnath",
      "Ashish Tiwari",
      "Kaustubh Sadekar",
      "Shanmuganathan Raman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chai_Identifying_and_Mitigating_Spurious_Correlation_in_Multi-Task_Learning_CVPR_2025_paper.html": {
    "title": "Identifying and Mitigating Spurious Correlation in Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyi Chai",
      "Shenyu Lu",
      "Xiaoqian Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Baumann_Continuous_Subject-Specific_Attribute_Control_in_T2I_Models_by_Identifying_Semantic_CVPR_2025_paper.html": {
    "title": "Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Andreas Baumann",
      "Felix Krause",
      "Michael Neumayr",
      "Nick Stracke",
      "Melvin Sevi",
      "Vincent Tao Hu",
      "Björn Ommer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Diffusion_Bridge_Leveraging_Diffusion_Model_to_Reduce_the_Modality_Gap_CVPR_2025_paper.html": {
    "title": "Diffusion Bridge: Leveraging Diffusion Model to Reduce the Modality Gap Between Text and Vision for Zero-Shot Image Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeong Ryong Lee",
      "Yejee Shin",
      "Geonhui Son",
      "Dosik Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MODfinity_Unsupervised_Domain_Adaptation_with_Multimodal_Information_Flow_Intertwining_CVPR_2025_paper.html": {
    "title": "MODfinity: Unsupervised Domain Adaptation with Multimodal Information Flow Intertwining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanglin Liu",
      "Jianming Lv",
      "Jingdan Kang",
      "Huaidong Zhang",
      "Zequan Liang",
      "Shengfeng He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rao_Towards_Universal_Soccer_Video_Understanding_CVPR_2025_paper.html": {
    "title": "Towards Universal Soccer Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayuan Rao",
      "Haoning Wu",
      "Hao Jiang",
      "Ya Zhang",
      "Yanfeng Wang",
      "Weidi Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment_CVPR_2025_paper.html": {
    "title": "SimLingo: Vision-Only Closed-Loop Autonomous Driving with Language-Action Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Katrin Renz",
      "Long Chen",
      "Elahe Arani",
      "Oleg Sinavski"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Improved_Video_VAE_for_Latent_Video_Diffusion_Model_CVPR_2025_paper.html": {
    "title": "Improved Video VAE for Latent Video Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pingyu Wu",
      "Kai Zhu",
      "Yu Liu",
      "Liming Zhao",
      "Wei Zhai",
      "Yang Cao",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ghosal_Immune_Improving_Safety_Against_Jailbreaks_in_Multi-modal_LLMs_via_Inference-Time_CVPR_2025_paper.html": {
    "title": "Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via Inference-Time Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soumya Suvra Ghosal",
      "Souradip Chakraborty",
      "Vaibhav Singh",
      "Tianrui Guan",
      "Mengdi Wang",
      "Ahmad Beirami",
      "Furong Huang",
      "Alvaro Velasquez",
      "Dinesh Manocha",
      "Amrit Singh Bedi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_Efficient_Video_Super-Resolution_for_Real-time_Rendering_with_Decoupled_G-buffer_Guidance_CVPR_2025_paper.html": {
    "title": "Efficient Video Super-Resolution for Real-time Rendering with Decoupled G-buffer Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingjun Zheng",
      "Long Sun",
      "Jiangxin Dong",
      "Jinshan Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_CustomKD_Customizing_Large_Vision_Foundation_for_Edge_Model_Improvement_via_CVPR_2025_paper.html": {
    "title": "CustomKD: Customizing Large Vision Foundation for Edge Model Improvement via Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungsoo Lee",
      "Debasmit Das",
      "Munawar Hayat",
      "Sungha Choi",
      "Kyuwoong Hwang",
      "Fatih Porikli"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Enhancing_Privacy-Utility_Trade-offs_to_Mitigate_Memorization_in_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Chen",
      "Daochang Liu",
      "Mubarak Shah",
      "Chang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Learned_Image_Compression_with_Dictionary-based_Entropy_Model_CVPR_2025_paper.html": {
    "title": "Learned Image Compression with Dictionary-based Entropy Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingbo Lu",
      "Leheng Zhang",
      "Xingyu Zhou",
      "Mu Li",
      "Wen Li",
      "Shuhang Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pei_PMNI_Pose-free_Multi-view_Normal_Integration_for_Reflective_and_Textureless_Surface_CVPR_2025_paper.html": {
    "title": "PMNI: Pose-free Multi-view Normal Integration for Reflective and Textureless Surface Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingzhi Pei",
      "Xu Cao",
      "Xiangyi Wang",
      "Heng Guo",
      "Zhanyu Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_NVComposer_Boosting_Generative_Novel_View_Synthesis_with_Multiple_Sparse_and_CVPR_2025_paper.html": {
    "title": "NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingen Li",
      "Zhaoyang Zhang",
      "Yaowei Li",
      "Jiale Xu",
      "Wenbo Hu",
      "Xiaoyu Li",
      "Weihao Cheng",
      "Jinwei Gu",
      "Tianfan Xue",
      "Ying Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_LeanGaussian_Breaking_Pixel_or_Point_Cloud_Correspondence_in_Modeling_3D_CVPR_2025_paper.html": {
    "title": "LeanGaussian: Breaking Pixel or Point Cloud Correspondence in Modeling 3D Gaussians",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiamin Wu",
      "Kenkun Liu",
      "Han Gao",
      "Xiaoke Jiang",
      "Yuan Yao",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Modeling_Multiple_Normal_Action_Representations_for_Error_Detection_in_Procedural_CVPR_2025_paper.html": {
    "title": "Modeling Multiple Normal Action Representations for Error Detection in Procedural Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei-Jin Huang",
      "Yuan-Ming Li",
      "Zhi-Wei Xia",
      "Yu-Ming Tang",
      "Kun-Yu Lin",
      "Jian-Fang Hu",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Seo_Efficient_Personalization_of_Quantized_Diffusion_Model_without_Backpropagation_CVPR_2025_paper.html": {
    "title": "Efficient Personalization of Quantized Diffusion Model without Backpropagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoigi Seo",
      "Wongi Jeong",
      "Kyungryeol Lee",
      "Se Young Chun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion_CVPR_2025_paper.html": {
    "title": "Alias-Free Latent Diffusion Models: Improving Fractional Shift Equivariance of Diffusion Latent Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Zhou",
      "Zeqi Xiao",
      "Shuai Yang",
      "Xingang Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Akshay_A_Unified_Latent_Schrodinger_Bridge_Diffusion_Model_for_Unsupervised_Anomaly_CVPR_2025_paper.html": {
    "title": "A Unified Latent Schrodinger Bridge Diffusion Model for Unsupervised Anomaly Detection and Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shilhora Akshay",
      "Niveditha Lakshmi Narasimhan",
      "Jacob George",
      "Vineeth N Balasubramanian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qu_KVQ_Boosting_Video_Quality_Assessment_via_Saliency-guided_Local_Perception_CVPR_2025_paper.html": {
    "title": "KVQ: Boosting Video Quality Assessment via Saliency-guided Local Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunpeng Qu",
      "Kun Yuan",
      "Qizhi Xie",
      "Ming Sun",
      "Chao Zhou",
      "Jian Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hatamizadeh_MambaVision_A_Hybrid_Mamba-Transformer_Vision_Backbone_CVPR_2025_paper.html": {
    "title": "MambaVision: A Hybrid Mamba-Transformer Vision Backbone",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Hatamizadeh",
      "Jan Kautz"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Learning_Flow_Fields_in_Attention_for_Controllable_Person_Image_Generation_CVPR_2025_paper.html": {
    "title": "Learning Flow Fields in Attention for Controllable Person Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijian Zhou",
      "Shikun Liu",
      "Xiao Han",
      "Haozhe Liu",
      "Kam Woh Ng",
      "Tian Xie",
      "Yuren Cong",
      "Hang Li",
      "Mengmeng Xu",
      "Juan-Manuel Perez-Rua",
      "Aditya Patel",
      "Tao Xiang",
      "Miaojing Shi",
      "Sen He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Multi-Label Prototype Visual Spatial Search for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songsong Duan",
      "Xi Yang",
      "Nannan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Whalen_Early-Bird_Diffusion_Investigating_and_Leveraging_Timestep-Aware_Early-Bird_Tickets_in_Diffusion_CVPR_2025_paper.html": {
    "title": "Early-Bird Diffusion: Investigating and Leveraging Timestep-Aware Early-Bird Tickets in Diffusion Models for Efficient Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lexington Whalen",
      "Zhenbang Du",
      "Haoran You",
      "Chaojian Li",
      "Sixu Li",
      "Yingyan Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_FireEdit_Fine-grained_Instruction-based_Image_Editing_via_Region-aware_Vision_Language_Model_CVPR_2025_paper.html": {
    "title": "FireEdit: Fine-grained Instruction-based Image Editing via Region-aware Vision Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Zhou",
      "Jiahao Li",
      "Zunnan Xu",
      "Hanhui Li",
      "Yiji Cheng",
      "Fa-Ting Hong",
      "Qin Lin",
      "Qinglin Lu",
      "Xiaodan Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features_CVPR_2025_paper.html": {
    "title": "Doppelgangers++: Improved Visual Disambiguation with Geometric 3D Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanbo Xiangli",
      "Ruojin Cai",
      "Hanyu Chen",
      "Jeffrey Byrne",
      "Noah Snavely"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_Learnable_Infinite_Taylor_Gaussian_for_Dynamic_View_Rendering_CVPR_2025_paper.html": {
    "title": "Learnable Infinite Taylor Gaussian for Dynamic View Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingbing Hu",
      "Yanyan Li",
      "Rui Xie",
      "Bo Xu",
      "Haoye Dong",
      "Junfeng Yao",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yi_DL2G_Degradation-guided_Local-to-Global_Restoration_for_Eyeglass_Reflection_Removal_CVPR_2025_paper.html": {
    "title": "DL2G: Degradation-guided Local-to-Global Restoration for Eyeglass Reflection Removal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhilv Yi",
      "Xiao Lu",
      "Hong Ding",
      "Jingbo Hu",
      "Zhi Jiang",
      "Chunxia Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric_CVPR_2025_paper.html": {
    "title": "DIV-FF: Dynamic Image-Video Feature Fields For Environment Understanding in Egocentric Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Mur-Labadia",
      "Josechu Guerrero",
      "Ruben Martinez-Cantin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer_CVPR_2025_paper.html": {
    "title": "SaMam: Style-aware State Space Model for Arbitrary Image Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongda Liu",
      "Longguang Wang",
      "Ye Zhang",
      "Ziru Yu",
      "Yulan Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_MFogHub_Bridging_Multi-Regional_and_Multi-Satellite_Data_for_Global_Marine_Fog_CVPR_2025_paper.html": {
    "title": "MFogHub: Bridging Multi-Regional and Multi-Satellite Data for Global Marine Fog Detection and Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengqiu Xu",
      "Kaixin Chen",
      "Heng Guo",
      "Yixiang Huang",
      "Ming Wu",
      "Zhenwei Shi",
      "Chuang Zhang",
      "Jun Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/George_The_Illusion_of_Unlearning_The_Unstable_Nature_of_Machine_Unlearning_CVPR_2025_paper.html": {
    "title": "The Illusion of Unlearning: The Unstable Nature of Machine Unlearning in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naveen George",
      "Karthik Nandan Dasaraju",
      "Rutheesh Reddy Chittepu",
      "Konda Reddy Mopuri"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mao_Making_Old_Film_Great_Again_Degradation-aware_State_Space_Model_for_CVPR_2025_paper.html": {
    "title": "Making Old Film Great Again: Degradation-aware State Space Model for Old Film Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yudong Mao",
      "Hao Luo",
      "Zhiwei Zhong",
      "Peilin Chen",
      "Zhijiang Zhang",
      "Shiqi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qiu_Leveraging_Global_Stereo_Consistency_for_Category-Level_Shape_and_6D_Pose_CVPR_2025_paper.html": {
    "title": "Leveraging Global Stereo Consistency for Category-Level Shape and 6D Pose Estimation from Stereo Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junning Qiu",
      "Minglei Lu",
      "Fei Wang",
      "Yu Guo",
      "Yonggen Ling"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_AlphaPre_Amplitude-Phase_Disentanglement_Model_for_Precipitation_Nowcasting_CVPR_2025_paper.html": {
    "title": "AlphaPre: Amplitude-Phase Disentanglement Model for Precipitation Nowcasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kenghong Lin",
      "Baoquan Zhang",
      "Demin Yu",
      "Wenzhi Feng",
      "Shidong Chen",
      "Feifan Gao",
      "Xutao Li",
      "Yunming Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_EfficientLLaVA_Generalizable_Auto-Pruning_for_Large_Vision-language_Models_CVPR_2025_paper.html": {
    "title": "EfficientLLaVA: Generalizable Auto-Pruning for Large Vision-language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinan Liang",
      "Ziwei Wang",
      "Xiuwei Xu",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target_CVPR_2025_paper.html": {
    "title": "Detection-Friendly Nonuniformity Correction: A Union Framework for Infrared UAV Target Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Houzhang Fang",
      "Xiaolin Wang",
      "Zengyang Li",
      "Lu Wang",
      "Qingshan Li",
      "Yi Chang",
      "Luxin Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Articulated_Kinematics_Distillation_from_Video_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Articulated Kinematics Distillation from Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Li",
      "Qianli Ma",
      "Tsung-Yi Lin",
      "Yongxin Chen",
      "Chenfanfu Jiang",
      "Ming-Yu Liu",
      "Donglai Xiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ashutosh_ExpertAF_Expert_Actionable_Feedback_from_Video_CVPR_2025_paper.html": {
    "title": "ExpertAF: Expert Actionable Feedback from Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kumar Ashutosh",
      "Tushar Nagarajan",
      "Georgios Pavlakos",
      "Kris Kitani",
      "Kristen Grauman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pataki_MP-SfM_Monocular_Surface_Priors_for_Robust_Structure-from-Motion_CVPR_2025_paper.html": {
    "title": "MP-SfM: Monocular Surface Priors for Robust Structure-from-Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zador Pataki",
      "Paul-Edouard Sarlin",
      "Johannes L. Schönberger",
      "Marc Pollefeys"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_OnlineAnySeg_Online_Zero-Shot_3D_Segmentation_by_Visual_Foundation_Model_Guided_CVPR_2025_paper.html": {
    "title": "OnlineAnySeg: Online Zero-Shot 3D Segmentation by Visual Foundation Model Guided 2D Mask Merging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijie Tang",
      "Jiazhao Zhang",
      "Yuqing Lan",
      "Yulan Guo",
      "Dezun Dong",
      "Chenyang Zhu",
      "Kai Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Tora_Trajectory-oriented_Diffusion_Transformer_for_Video_Generation_CVPR_2025_paper.html": {
    "title": "Tora: Trajectory-oriented Diffusion Transformer for Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenghao Zhang",
      "Junchao Liao",
      "Menghao Li",
      "ZuoZhuo Dai",
      "Bingxue Qiu",
      "Siyu Zhu",
      "Long Qin",
      "Weizhi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization_CVPR_2025_paper.html": {
    "title": "Volumetrically Consistent 3D Gaussian Rasterization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chinmay Talegaonkar",
      "Yash Belhe",
      "Ravi Ramamoorthi",
      "Nicholas Antipa"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hua_Deterministic-to-Stochastic_Diverse_Latent_Feature_Mapping_for_Human_Motion_Synthesis_CVPR_2025_paper.html": {
    "title": "Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Hua",
      "Weiming Liu",
      "Gui Xu",
      "Yaqing Hou",
      "Yew-Soon Ong",
      "Qiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wynn_Morpheus_Text-Driven_3D_Gaussian_Splat_Shape_and_Color_Stylization_CVPR_2025_paper.html": {
    "title": "Morpheus: Text-Driven 3D Gaussian Splat Shape and Color Stylization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jamie Wynn",
      "Zawar Qureshi",
      "Jakub Powierza",
      "Jamie Watson",
      "Mohamed Sayed"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_CacheQuant_Comprehensively_Accelerated_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "CacheQuant: Comprehensively Accelerated Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuewen Liu",
      "Zhikai Li",
      "Qingyi Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nordstrom_The_Impact_Label_Noise_and_Choice_of_Threshold_has_on_CVPR_2025_paper.html": {
    "title": "The Impact Label Noise and Choice of Threshold has on Cross-Entropy and Soft-Dice in Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcus Nordström",
      "Atsuto Maki",
      "Henrik Hult"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Open-World_Objectness_Modeling_Unifies_Novel_Object_Detection_CVPR_2025_paper.html": {
    "title": "Open-World Objectness Modeling Unifies Novel Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shan Zhang",
      "Yao Ni",
      "Jinhao Du",
      "Yuan Xue",
      "Philip Torr",
      "Piotr Koniusz",
      "Anton van den Hengel"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiong_LLaVA-Critic_Learning_to_Evaluate_Multimodal_Models_CVPR_2025_paper.html": {
    "title": "LLaVA-Critic: Learning to Evaluate Multimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Xiong",
      "Xiyao Wang",
      "Dong Guo",
      "Qinghao Ye",
      "Haoqi Fan",
      "Quanquan Gu",
      "Heng Huang",
      "Chunyuan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge_CVPR_2025_paper.html": {
    "title": "VILA-M3: Enhancing Vision-Language Models with Medical Expert Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vishwesh Nath",
      "Wenqi Li",
      "Dong Yang",
      "Andriy Myronenko",
      "Mingxin Zheng",
      "Yao Lu",
      "Zhijian Liu",
      "Hongxu Yin",
      "Yee Man Law",
      "Yucheng Tang",
      "Pengfei Guo",
      "Can Zhao",
      "Ziyue Xu",
      "Yufan He",
      "Stephanie Harmon",
      "Benjamin Simon",
      "Greg Heinrich",
      "Stephen Aylward",
      "Marc Edgar",
      "Michael Zephyr",
      "Pavlo Molchanov",
      "Baris Turkbey",
      "Holger Roth",
      "Daguang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Repurposing_Pre-trained_Video_Diffusion_Models_for_Event-based_Video_Interpolation_CVPR_2025_paper.html": {
    "title": "Repurposing Pre-trained Video Diffusion Models for Event-based Video Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingxi Chen",
      "Brandon Y. Feng",
      "Haoming Cai",
      "Tianfu Wang",
      "Levi Burner",
      "Dehao Yuan",
      "Cornelia Fermuller",
      "Christopher A. Metzler",
      "Yiannis Aloimonos"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and_CVPR_2025_paper.html": {
    "title": "MotionPRO: Exploring the Role of Pressure in Human MoCap and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenghao Ren",
      "Yi Lu",
      "Jiayi Huang",
      "Jiayi Zhao",
      "He Zhang",
      "Tao Yu",
      "Qiu Shen",
      "Xun Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_DiffVsgg_Diffusion-Driven_Online_Video_Scene_Graph_Generation_CVPR_2025_paper.html": {
    "title": "DiffVsgg: Diffusion-Driven Online Video Scene Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mu Chen",
      "Liulei Li",
      "Wenguan Wang",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Large-scale_Multi-view_Tensor_Clustering_with_Implicit_Linear_Kernels_CVPR_2025_paper.html": {
    "title": "Large-scale Multi-view Tensor Clustering with Implicit Linear Kernels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiyuan Liu",
      "Xinwang Liu",
      "Chuankun Li",
      "Xinhang Wan",
      "Hao Tan",
      "Yi Zhang",
      "Weixuan Liang",
      "Qian Qu",
      "Yu Feng",
      "Renxiang Guan",
      "Ke Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_Generalized_Diffusion_Detector_Mining_Robust_Features_from_Diffusion_Models_for_CVPR_2025_paper.html": {
    "title": "Generalized Diffusion Detector: Mining Robust Features from Diffusion Models for Domain-Generalized Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyong He",
      "Yuxiang Ji",
      "Qianwen Ye",
      "Zhuoyue Tan",
      "Liaoni Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content_CVPR_2025_paper.html": {
    "title": "Q-Eval-100K: Evaluating Visual Quality and Alignment Level for Text-to-Vision Content",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zicheng Zhang",
      "Tengchuan Kou",
      "Shushi Wang",
      "Chunyi Li",
      "Wei Sun",
      "Wei Wang",
      "Xiaoyu Li",
      "Zongyu Wang",
      "Xuezhi Cao",
      "Xiongkuo Min",
      "Xiaohong Liu",
      "Guangtao Zhai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_Dual_Focus-Attention_Transformer_for_Robust_Point_Cloud_Registration_CVPR_2025_paper.html": {
    "title": "Dual Focus-Attention Transformer for Robust Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kexue Fu",
      "Mingzhi Yuan",
      "Changwei Wang",
      "Weiguang Pang",
      "Jing Chi",
      "Manning Wang",
      "Longxiang Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Forming_Auxiliary_High-confident_Instance-level_Loss_to_Promote_Learning_from_Label_CVPR_2025_paper.html": {
    "title": "Forming Auxiliary High-confident Instance-level Loss to Promote Learning from Label Proportions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhao Ma",
      "Han Chen",
      "Juncheng Hu",
      "Yungang Zhu",
      "Ximing Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xue_Progress-Aware_Video_Frame_Captioning_CVPR_2025_paper.html": {
    "title": "Progress-Aware Video Frame Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihui Xue",
      "Joungbin An",
      "Xitong Yang",
      "Kristen Grauman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_SMTPD_A_New_Benchmark_for_Temporal_Prediction_of_Social_Media_CVPR_2025_paper.html": {
    "title": "SMTPD: A New Benchmark for Temporal Prediction of Social Media Popularity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijie Xu",
      "Bolun Zheng",
      "Wei Zhu",
      "Hangjia Pan",
      "Yuchen Yao",
      "Ning Xu",
      "Anan Liu",
      "Quan Zhang",
      "Chenggang Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Enhancing_Dance-to-Music_Generation_via_Negative_Conditioning_Latent_Diffusion_Model_CVPR_2025_paper.html": {
    "title": "Enhancing Dance-to-Music Generation via Negative Conditioning Latent Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changchang Sun",
      "Gaowen Liu",
      "Charles Fleming",
      "Yan Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sharan_Neuro-Symbolic_Evaluation_of_Text-to-Video_Models_using_Formal_Verification_CVPR_2025_paper.html": {
    "title": "Neuro-Symbolic Evaluation of Text-to-Video Models using Formal Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "S P Sharan",
      "Minkyu Choi",
      "Sahil Shah",
      "Harsh Goel",
      "Mohammad Omama",
      "Sandeep Chinchali"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Spherical_Manifold_Guided_Diffusion_Model_for_Panoramic_Image_Generation_CVPR_2025_paper.html": {
    "title": "Spherical Manifold Guided Diffusion Model for Panoramic Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiancheng Sun",
      "Mai Xu",
      "Shengxi Li",
      "Senmao Ma",
      "Xin Deng",
      "Lai Jiang",
      "Gang Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Horwitz_Learning_on_Model_Weights_using_Tree_Experts_CVPR_2025_paper.html": {
    "title": "Learning on Model Weights using Tree Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eliahu Horwitz",
      "Bar Cavia",
      "Jonathan Kahana",
      "Yedid Hoshen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Rethinking_Query-based_Transformer_for_Continual_Image_Segmentation_CVPR_2025_paper.html": {
    "title": "Rethinking Query-based Transformer for Continual Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Zhu",
      "Cheng Shi",
      "Dingyou Wang",
      "Jiajin Tang",
      "Zhengxuan Wei",
      "Yu Wu",
      "Guanbin Li",
      "Sibei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays_CVPR_2025_paper.html": {
    "title": "Image Reconstruction from Readout-Multiplexed Single-Photon Detector Arrays",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shashwath Bharadwaj",
      "Ruangrawee Kitichotkul",
      "Akshay Agarwal",
      "Vivek K Goyal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Towards_Smart_Point-and-Shoot_Photography_CVPR_2025_paper.html": {
    "title": "Towards Smart Point-and-Shoot Photography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawan Li",
      "Fei Zhou",
      "Zhipeng Zhong",
      "Jiongzhi Lin",
      "Guoping Qiu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_SlideChat_A_Large_Vision-Language_Assistant_for_Whole-Slide_Pathology_Image_Understanding_CVPR_2025_paper.html": {
    "title": "SlideChat: A Large Vision-Language Assistant for Whole-Slide Pathology Image Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ying Chen",
      "Guoan Wang",
      "Yuanfeng Ji",
      "Yanjun Li",
      "Jin Ye",
      "Tianbin Li",
      "Ming Hu",
      "Rongshan Yu",
      "Yu Qiao",
      "Junjun He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Prototype-Based_Image_Prompting_for_Weakly_Supervised_Histopathological_Image_Segmentation_CVPR_2025_paper.html": {
    "title": "Prototype-Based Image Prompting for Weakly Supervised Histopathological Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingchen Tang",
      "Lei Fan",
      "Maurice Pagnucco",
      "Yang Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Towards_Transformer-Based_Aligned_Generation_with_Self-Coherence_Guidance_CVPR_2025_paper.html": {
    "title": "Towards Transformer-Based Aligned Generation with Self-Coherence Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shulei Wang",
      "Wang Lin",
      "Hai Huang",
      "Hanting Wang",
      "Sihang Cai",
      "WenKang Han",
      "Tao Jin",
      "Jingyuan Chen",
      "Jiacheng Sun",
      "Jieming Zhu",
      "Zhou Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Maracani_Accurate_Scene_Text_Recognition_with_Efficient_Model_Scaling_and_Cloze_CVPR_2025_paper.html": {
    "title": "Accurate Scene Text Recognition with Efficient Model Scaling and Cloze Self-Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Maracani",
      "Savas Ozkan",
      "Sijun Cho",
      "Hyowon Kim",
      "Eunchung Noh",
      "Jeongwon Min",
      "Cho Jung Min",
      "Dookun Park",
      "Mete Ozay"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_DART_Disease-aware_Image-Text_Alignment_and_Self-correcting_Re-alignment_for_Trustworthy_Radiology_CVPR_2025_paper.html": {
    "title": "DART: Disease-aware Image-Text Alignment and Self-correcting Re-alignment for Trustworthy Radiology Report Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sang-Jun Park",
      "Keun-Soo Heo",
      "Dong-Hee Shin",
      "Young-Han Son",
      "Ji-Hye Oh",
      "Tae-Eui Kam"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jung_On_the_Consistency_of_Video_Large_Language_Models_in_Temporal_CVPR_2025_paper.html": {
    "title": "On the Consistency of Video Large Language Models in Temporal Comprehension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minjoon Jung",
      "Junbin Xiao",
      "Byoung-Tak Zhang",
      "Angela Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Mitigating_the_Human-Robot_Domain_Discrepancy_in_Visual_Pre-training_for_Robotic_CVPR_2025_paper.html": {
    "title": "Mitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Zhou",
      "Teli Ma",
      "Kun-Yu Lin",
      "Zifan Wang",
      "Ronghe Qiu",
      "Junwei Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch_CVPR_2025_paper.html": {
    "title": "Less is More: Efficient Model Merging with Binary Task Switch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Biqing Qi",
      "Fangyuan Li",
      "Zhen Wang",
      "Junqi Gao",
      "Dong Li",
      "Peng Ye",
      "Bowen Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dalal_One-Minute_Video_Generation_with_Test-Time_Training_CVPR_2025_paper.html": {
    "title": "One-Minute Video Generation with Test-Time Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karan Dalal",
      "Daniel Koceja",
      "Jiarui Xu",
      "Yue Zhao",
      "Shihao Han",
      "Ka Chun Cheung",
      "Jan Kautz",
      "Yejin Choi",
      "Yu Sun",
      "Xiaolong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_InteractionMap_Improving_Online_Vectorized_HDMap_Construction_with_Interaction_CVPR_2025_paper.html": {
    "title": "InteractionMap: Improving Online Vectorized HDMap Construction with Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kuang Wu",
      "Chuan Yang",
      "Zhanbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding_CVPR_2025_paper.html": {
    "title": "Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxuan Guo",
      "Xiuwei Xu",
      "Ziwei Wang",
      "Jianjiang Feng",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cai_ROCKET-1_Mastering_Open-World_Interaction_with_Visual-Temporal_Context_Prompting_CVPR_2025_paper.html": {
    "title": "ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaofei Cai",
      "Zihao Wang",
      "Kewei Lian",
      "Zhancun Mu",
      "Xiaojian Ma",
      "Anji Liu",
      "Yitao Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sommer_Common3D_Self-Supervised_Learning_of_3D_Morphable_Models_for_Common_Objects_CVPR_2025_paper.html": {
    "title": "Common3D: Self-Supervised Learning of 3D Morphable Models for Common Objects in Neural Feature Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leonhard Sommer",
      "Olaf Dünkel",
      "Christian Theobalt",
      "Adam Kortylewski"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness_CVPR_2025_paper.html": {
    "title": "RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Yu",
      "Haoye Zhang",
      "Qiming Li",
      "Qixin Xu",
      "Yuan Yao",
      "Da Chen",
      "Xiaoman Lu",
      "Ganqu Cui",
      "Yunkai Dang",
      "Taiwen He",
      "Xiaocheng Feng",
      "Jun Song",
      "Bo Zheng",
      "Zhiyuan Liu",
      "Tat-Seng Chua",
      "Maosong Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_ECVC_Exploiting_Non-Local_Correlations_in_Multiple_Frames_for_Contextual_Video_CVPR_2025_paper.html": {
    "title": "ECVC: Exploiting Non-Local Correlations in Multiple Frames for Contextual Video Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Jiang",
      "Junru Li",
      "Kai Zhang",
      "Li Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_LinGen_Towards_High-Resolution_Minute-Length_Text-to-Video_Generation_with_Linear_Computational_Complexity_CVPR_2025_paper.html": {
    "title": "LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongjie Wang",
      "Chih-Yao Ma",
      "Yen-Cheng Liu",
      "Ji Hou",
      "Tao Xu",
      "Jialiang Wang",
      "Felix Juefei-Xu",
      "Yaqiao Luo",
      "Peizhao Zhang",
      "Tingbo Hou",
      "Peter Vajda",
      "Niraj K. Jha",
      "Xiaoliang Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_EditSplat_Multi-View_Fusion_and_Attention-Guided_Optimization_for_View-Consistent_3D_Scene_CVPR_2025_paper.html": {
    "title": "EditSplat: Multi-View Fusion and Attention-Guided Optimization for View-Consistent 3D Scene Editing with 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong In Lee",
      "Hyeongcheol Park",
      "Jiyoung Seo",
      "Eunbyung Park",
      "Hyunje Park",
      "Ha Dam Baek",
      "Sangheon Shin",
      "Sangmin Kim",
      "Sangpil Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SpatialCLIP_Learning_3D-aware_Image_Representations_from_Spatially_Discriminative_Language_CVPR_2025_paper.html": {
    "title": "SpatialCLIP: Learning 3D-aware Image Representations from Spatially Discriminative Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehan Wang",
      "Sashuai Zhou",
      "Shaoxuan He",
      "Haifeng Huang",
      "Lihe Yang",
      "Ziang Zhang",
      "Xize Cheng",
      "Shengpeng Ji",
      "Tao Jin",
      "Hengshuang Zhao",
      "Zhou Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Mono2Stereo_A_Benchmark_and_Empirical_Study_for_Stereo_Conversion_CVPR_2025_paper.html": {
    "title": "Mono2Stereo: A Benchmark and Empirical Study for Stereo Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songsong Yu",
      "Yuxin Chen",
      "Zhongang Qi",
      "Zeke Xie",
      "Yifan Wang",
      "Lijun Wang",
      "Ying Shan",
      "Huchuan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Towards_Open-Vocabulary_Audio-Visual_Event_Localization_CVPR_2025_paper.html": {
    "title": "Towards Open-Vocabulary Audio-Visual Event Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinxing Zhou",
      "Dan Guo",
      "Ruohao Guo",
      "Yuxin Mao",
      "Jingjing Hu",
      "Yiran Zhong",
      "Xiaojun Chang",
      "Meng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency_CVPR_2025_paper.html": {
    "title": "One-shot 3D Object Canonicalization based on Geometric and Semantic Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Jin",
      "Yujie Wang",
      "Wenzheng Chen",
      "Qiyu Dai",
      "Qingzhe Gao",
      "Xueying Qin",
      "Baoquan Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning_CVPR_2025_paper.html": {
    "title": "Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal Instruction Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanxun Yu",
      "Wentong Li",
      "Song Wang",
      "Junbo Chen",
      "Jianke Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wan_S2Gaussian_Sparse-View_Super-Resolution_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "S2Gaussian: Sparse-View Super-Resolution 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yecong Wan",
      "Mingwen Shao",
      "Yuanshuo Cheng",
      "Wangmeng Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_HIIF_Hierarchical_Encoding_based_Implicit_Image_Function_for_Continuous_Super-resolution_CVPR_2025_paper.html": {
    "title": "HIIF: Hierarchical Encoding based Implicit Image Function for Continuous Super-resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Jiang",
      "Ho Man Kwan",
      "Tianhao Peng",
      "Ge Gao",
      "Fan Zhang",
      "Xiaoqing Zhu",
      "Joel Sole",
      "David Bull"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories_CVPR_2025_paper.html": {
    "title": "Motion Prompting: Controlling Video Generation with Motion Trajectories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Geng",
      "Charles Herrmann",
      "Junhwa Hur",
      "Forrester Cole",
      "Serena Zhang",
      "Tobias Pfaff",
      "Tatiana Lopez-Guevara",
      "Yusuf Aytar",
      "Michael Rubinstein",
      "Chen Sun",
      "Oliver Wang",
      "Andrew Owens",
      "Deqing Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_VERA_Explainable_Video_Anomaly_Detection_via_Verbalized_Learning_of_Vision-Language_CVPR_2025_paper.html": {
    "title": "VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muchao Ye",
      "Weiyang Liu",
      "Pan He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wallin_ProHOC_Probabilistic_Hierarchical_Out-of-Distribution_Classification_via_Multi-Depth_Networks_CVPR_2025_paper.html": {
    "title": "ProHOC: Probabilistic Hierarchical Out-of-Distribution Classification via Multi-Depth Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erik Wallin",
      "Fredrik Kahl",
      "Lars Hammarstrand"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval_CVPR_2025_paper.html": {
    "title": "CCIN: Compositional Conflict Identification and Neutralization for Composed Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Likai Tian",
      "Jian Zhao",
      "Zechao Hu",
      "Zhengwei Yang",
      "Hao Li",
      "Lei Jin",
      "Zheng Wang",
      "Xuelong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xing_CLIP_is_Strong_Enough_to_Fight_Back_Test-time_Counterattacks_towards_CVPR_2025_paper.html": {
    "title": "CLIP is Strong Enough to Fight Back: Test-time Counterattacks towards Zero-shot Adversarial Robustness of CLIP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songlong Xing",
      "Zhengyu Zhao",
      "Nicu Sebe"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels_CVPR_2025_paper.html": {
    "title": "OverLoCK: An Overview-first-Look-Closely-next ConvNet with Context-Mixing Dynamic Kernels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meng Lou",
      "Yizhou Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SoftShadow_Leveraging_Soft_Masks_for_Penumbra-Aware_Shadow_Removal_CVPR_2025_paper.html": {
    "title": "SoftShadow: Leveraging Soft Masks for Penumbra-Aware Shadow Removal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinrui Wang",
      "Lanqing Guo",
      "Xiyu Wang",
      "Siyu Huang",
      "Bihan Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Graph-Embedded_Structure-Aware_Perceptual_Hashing_for_Neural_Network_Protection_and_Piracy_CVPR_2025_paper.html": {
    "title": "Graph-Embedded Structure-Aware Perceptual Hashing for Neural Network Protection and Piracy Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiheng Liu",
      "Haozhe Chen",
      "Boyao Zhao",
      "Kejiang Chen",
      "Weiming Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_VTON-HandFit_Virtual_Try-on_for_Arbitrary_Hand_Pose_Guided_by_Hand_CVPR_2025_paper.html": {
    "title": "VTON-HandFit: Virtual Try-on for Arbitrary Hand Pose Guided by Hand Priors Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Liang",
      "Xiaobin Hu",
      "Boyuan Jiang",
      "Donghao Luo",
      "Xu Peng",
      "Kai Wu",
      "Chengming Xu",
      "Wenhui Han",
      "Taisong Jin",
      "Chengjie Wang",
      "Rongrong Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_Interleaved-Modal_Chain-of-Thought_CVPR_2025_paper.html": {
    "title": "Interleaved-Modal Chain-of-Thought",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Gao",
      "Yongqi Li",
      "Ziqiang Cao",
      "Wenjie Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Uni-Renderer_Unifying_Rendering_and_Inverse_Rendering_Via_Dual_Stream_Diffusion_CVPR_2025_paper.html": {
    "title": "Uni-Renderer: Unifying Rendering and Inverse Rendering Via Dual Stream Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhifei Chen",
      "Tianshuo Xu",
      "Wenhang Ge",
      "Leyi Wu",
      "Dongyu Yan",
      "Jing He",
      "Luozhou Wang",
      "Lu Zeng",
      "Shunsi Zhang",
      "Ying-Cong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Enhancing_Adversarial_Transferability_with_Checkpoints_of_a_Single_Models_Training_CVPR_2025_paper.html": {
    "title": "Enhancing Adversarial Transferability with Checkpoints of a Single Model's Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shixin Li",
      "Chaoxiang He",
      "Xiaojing Ma",
      "Bin Benjamin Zhu",
      "Shuo Wang",
      "Hongsheng Hu",
      "Dongmei Zhang",
      "Linchen Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_POSTA_A_Go-to_Framework_for_Customized_Artistic_Poster_Generation_CVPR_2025_paper.html": {
    "title": "POSTA: A Go-to Framework for Customized Artistic Poster Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Chen",
      "Xiaojie Xu",
      "Wenbo Li",
      "Jingjing Ren",
      "Tian Ye",
      "Songhua Liu",
      "Ying-Cong Chen",
      "Lei Zhu",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods_CVPR_2025_paper.html": {
    "title": "NSD-Imagery: A Benchmark Dataset for Extending fMRI Vision Decoding Methods to Mental Imagery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reese Kneeland",
      "Paul S. Scotti",
      "Ghislain St-Yves",
      "Jesse Breedlove",
      "Kendrick Kay",
      "Thomas Naselaris"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_VLsI_Verbalized_Layers-to-Interactions_from_Large_to_Small_Vision_Language_Models_CVPR_2025_paper.html": {
    "title": "VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byung-Kwan Lee",
      "Ryo Hachiuma",
      "Yu-Chiang Frank Wang",
      "Yong Man Ro",
      "Yueh-Hua Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language_CVPR_2025_paper.html": {
    "title": "O-TPT: Orthogonality Constraints for Calibrating Test-time Prompt Tuning in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashshak Sharifdeen",
      "Muhammad Akhtar Munir",
      "Sanoojan Baliah",
      "Salman Khan",
      "Muhammad Haris Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video_CVPR_2025_paper.html": {
    "title": "Just Dance with pi! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Snehashis Majhi",
      "Giacomo D'Amicantonio",
      "Antitza Dantcheva",
      "Quan Kong",
      "Lorenzo Garattoni",
      "Gianpiero Francesca",
      "Egor Bondarev",
      "Francois Bremond"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality_CVPR_2025_paper.html": {
    "title": "Flash3D: Super-scaling Point Transformers through Joint Hardware-Geometry Locality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyan Chen",
      "Gregory P. Meyer",
      "Zaiwei Zhang",
      "Eric M. Wolff",
      "Paul Vernaza"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Analyzing_the_Synthetic-to-Real_Domain_Gap_in_3D_Hand_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "Analyzing the Synthetic-to-Real Domain Gap in 3D Hand Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoran Zhao",
      "Linlin Yang",
      "Pengzhan Sun",
      "Pan Hui",
      "Angela Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Feature4X_Bridging_Any_Monocular_Video_to_4D_Agentic_AI_with_CVPR_2025_paper.html": {
    "title": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijie Zhou",
      "Hui Ren",
      "Yijia Weng",
      "Shuwang Zhang",
      "Zhen Wang",
      "Dejia Xu",
      "Zhiwen Fan",
      "Suya You",
      "Zhangyang Wang",
      "Leonidas Guibas",
      "Achuta Kadambi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Comprehensive_Relighting_Generalizable_and_Consistent_Monocular_Human_Relighting_and_Harmonization_CVPR_2025_paper.html": {
    "title": "Comprehensive Relighting: Generalizable and Consistent Monocular Human Relighting and Harmonization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junying Wang",
      "Jingyuan Liu",
      "Xin Sun",
      "Krishna Kumar Singh",
      "Zhixin Shu",
      "He Zhang",
      "Jimei Yang",
      "Nanxuan Zhao",
      "Tuanfeng Y. Wang",
      "Simon S. Chen",
      "Ulrich Neumann",
      "Jae Shin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_Hyperspectral_Pansharpening_via_Diffusion_Models_with_Iteratively_Zero-Shot_Guidance_CVPR_2025_paper.html": {
    "title": "Hyperspectral Pansharpening via Diffusion Models with Iteratively Zero-Shot Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin-Liang Xiao",
      "Ting-Zhu Huang",
      "Liang-Jian Deng",
      "Guang Lin",
      "Zihan Cao",
      "Chao Li",
      "Qibin Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_EASEMVCEfficient_Dual_Selection_Mechanism_for_Deep_Multi-View_Clustering_CVPR_2025_paper.html": {
    "title": "EASEMVC:Efficient Dual Selection Mechanism for Deep Multi-View Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baili Xiao",
      "Zhibin Dong",
      "Ke Liang",
      "Suyuan Liu",
      "Siwei Wang",
      "Tianrui Liu",
      "Xingchen Hu",
      "En Zhu",
      "Xinwang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Efficient_Motion-Aware_Video_MLLM_CVPR_2025_paper.html": {
    "title": "Efficient Motion-Aware Video MLLM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijia Zhao",
      "Yuqi Huo",
      "Tongtian Yue",
      "Longteng Guo",
      "Haoyu Lu",
      "Bingning Wang",
      "Weipeng Chen",
      "Jing Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_DSPNet_Dual-vision_Scene_Perception_for_Robust_3D_Question_Answering_CVPR_2025_paper.html": {
    "title": "DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingzhou Luo",
      "Yang Liu",
      "Weixing Chen",
      "Zhen Li",
      "Yaowei Wang",
      "Guanbin Li",
      "Liang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Zero-Shot_4D_Lidar_Panoptic_Segmentation_CVPR_2025_paper.html": {
    "title": "Zero-Shot 4D Lidar Panoptic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushan Zhang",
      "Aljoša Ošep",
      "Laura Leal-Taixé",
      "Tim Meinhardt"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism_CVPR_2025_paper.html": {
    "title": "MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Guedon",
      "Tomoki Ichikawa",
      "Kohei Yamashita",
      "Ko Nishino"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bezalel_Extreme_Rotation_Estimation_in_the_Wild_CVPR_2025_paper.html": {
    "title": "Extreme Rotation Estimation in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hana Bezalel",
      "Dotan Ankri",
      "Ruojin Cai",
      "Hadar Averbach-Elor"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lai_ADU_Adaptive_Detection_of_Unknown_Categories_in_Black-Box_Domain_Adaptation_CVPR_2025_paper.html": {
    "title": "ADU: Adaptive Detection of Unknown Categories in Black-Box Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushan Lai",
      "Guowen Li",
      "Haoyuan Liang",
      "Juepeng Zheng",
      "Zhiyu Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_EmotiveTalk_Expressive_Talking_Head_Generation_through_Audio_Information_Decoupling_and_CVPR_2025_paper.html": {
    "title": "EmotiveTalk: Expressive Talking Head Generation through Audio Information Decoupling and Emotional Video Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Wang",
      "Yuzhe Weng",
      "Yueyan Li",
      "Zilu Guo",
      "Jun Du",
      "Shutong Niu",
      "Jiefeng Ma",
      "Shan He",
      "Xiaoyan Wu",
      "Qiming Hu",
      "Bing Yin",
      "Cong Liu",
      "Qingfeng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Traversing_Distortion-Perception_Tradeoff_using_a_Single_Score-Based_Generative_Model_CVPR_2025_paper.html": {
    "title": "Traversing Distortion-Perception Tradeoff using a Single Score-Based Generative Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Wang",
      "Suzhi Bi",
      "Ying-Jun Angela Zhang",
      "Xiaojun Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with_CVPR_2025_paper.html": {
    "title": "IceDiff: High Resolution and High-Quality Arctic Sea Ice Forecasting with Generative Diffusion Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi Xu",
      "Siwei Tu",
      "Weidong Yang",
      "Ben Fei",
      "Shuhao Li",
      "Keyi Liu",
      "Yeqi Luo",
      "Lipeng Ma",
      "Lei Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_DTOS_Dynamic_Time_Object_Sensing_with_Large_Multimodal_Model_CVPR_2025_paper.html": {
    "title": "DTOS: Dynamic Time Object Sensing with Large Multimodal Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jirui Tian",
      "Jinrong Zhang",
      "Shenglan Liu",
      "Luhao Xu",
      "Zhixiong Huang",
      "Gao Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dziadzio_How_to_Merge_Your_Multimodal_Models_Over_Time_CVPR_2025_paper.html": {
    "title": "How to Merge Your Multimodal Models Over Time?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Dziadzio",
      "Vishaal Udandarao",
      "Karsten Roth",
      "Ameya Prabhu",
      "Zeynep Akata",
      "Samuel Albanie",
      "Matthias Bethge"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "Identifying and Mitigating Position Bias of Multi-image Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Tian",
      "Shu Zou",
      "Zhaoyuan Yang",
      "Jing Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lenz_Unsupervised_Foundation_Model-Agnostic_Slide-Level_Representation_Learning_CVPR_2025_paper.html": {
    "title": "Unsupervised Foundation Model-Agnostic Slide-Level Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Lenz",
      "Peter Neidlinger",
      "Marta Ligero",
      "Georg Wölflein",
      "Marko van Treeck",
      "Jakob N. Kather"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Exploring_CLIPs_Dense_Knowledge_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Exploring CLIP's Dense Knowledge for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwei Yang",
      "Yucong Meng",
      "Kexue Fu",
      "Feilong Tang",
      "Shuo Wang",
      "Zhijian Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_UNIALIGN_Scaling_Multimodal_Alignment_within_One_Unified_Model_CVPR_2025_paper.html": {
    "title": "UNIALIGN: Scaling Multimodal Alignment within One Unified Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Zhou",
      "Liulei Li",
      "Yujia Wang",
      "Huafeng Liu",
      "Yazhou Yao",
      "Wenguan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Soucek_ShowHowTo_Generating_Scene-Conditioned_Step-by-Step_Visual_Instructions_CVPR_2025_paper.html": {
    "title": "ShowHowTo: Generating Scene-Conditioned Step-by-Step Visual Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomáš Souček",
      "Prajwal Gatti",
      "Michael Wray",
      "Ivan Laptev",
      "Dima Damen",
      "Josef Sivic"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Savov_Exploration-Driven_Generative_Interactive_Environments_CVPR_2025_paper.html": {
    "title": "Exploration-Driven Generative Interactive Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nedko Savov",
      "Naser Kazemi",
      "Mohammad Mahdi",
      "Danda Pani Paudel",
      "Xi Wang",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_Task-Agnostic_Guided_Feature_Expansion_for_Class-Incremental_Learning_CVPR_2025_paper.html": {
    "title": "Task-Agnostic Guided Feature Expansion for Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Zheng",
      "Da-Wei Zhou",
      "Han-Jia Ye",
      "De-Chuan Zhan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_ShowUI_One_Vision-Language-Action_Model_for_GUI_Visual_Agent_CVPR_2025_paper.html": {
    "title": "ShowUI: One Vision-Language-Action Model for GUI Visual Agent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Qinghong Lin",
      "Linjie Li",
      "Difei Gao",
      "Zhengyuan Yang",
      "Shiwei Wu",
      "Zechen Bai",
      "Stan Weixian Lei",
      "Lijuan Wang",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Lets_Chorus_Partner-aware_Hybrid_Song-Driven_3D_Head_Animation_CVPR_2025_paper.html": {
    "title": "Let's Chorus: Partner-aware Hybrid Song-Driven 3D Head Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiumei Xie",
      "Zikai Huang",
      "Wenhao Xu",
      "Peng Xiao",
      "Xuemiao Xu",
      "Huaidong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zarzar_Twinner_Shining_Light_on_Digital_Twins_in_a_Few_Snaps_CVPR_2025_paper.html": {
    "title": "Twinner: Shining Light on Digital Twins in a Few Snaps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jesus Zarzar",
      "Tom Monnier",
      "Roman Shapovalov",
      "Andrea Vedaldi",
      "David Novotny"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis_CVPR_2025_paper.html": {
    "title": "Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Han",
      "Jinlai Liu",
      "Yi Jiang",
      "Bin Yan",
      "Yuqi Zhang",
      "Zehuan Yuan",
      "Bingyue Peng",
      "Xiaobing Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DreamText_High_Fidelity_Scene_Text_Synthesis_CVPR_2025_paper.html": {
    "title": "DreamText: High Fidelity Scene Text Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yibin Wang",
      "Weizhong Zhang",
      "Honghui Xu",
      "Cheng Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Parihar_MonoPlace3D_Learning_3D-Aware_Object_Placement_for_3D_Monocular_Detection_CVPR_2025_paper.html": {
    "title": "MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishubh Parihar",
      "Srinjay Sarkar",
      "Sarthak Vora",
      "Jogendra Nath Kundu",
      "R. Venkatesh Babu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_HumanDreamer_Generating_Controllable_Human-Motion_Videos_via_Decoupled_Generation_CVPR_2025_paper.html": {
    "title": "HumanDreamer: Generating Controllable Human-Motion Videos via Decoupled Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyuan Wang",
      "Xiaofeng Wang",
      "Chaojun Ni",
      "Guosheng Zhao",
      "Zhiqin Yang",
      "Zheng Zhu",
      "Muyang Zhang",
      "Yukun Zhou",
      "Xinze Chen",
      "Guan Huang",
      "Lihong Liu",
      "Xingang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hannan_ReVisionLLM_Recursive_Vision-Language_Model_for_Temporal_Grounding_in_Hour-Long_Videos_CVPR_2025_paper.html": {
    "title": "ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in Hour-Long Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanveer Hannan",
      "Md Mohaiminul Islam",
      "Jindong Gu",
      "Thomas Seidl",
      "Gedas Bertasius"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_ArtiFade_Learning_to_Generate_High-quality_Subject_from_Blemished_Images_CVPR_2025_paper.html": {
    "title": "ArtiFade: Learning to Generate High-quality Subject from Blemished Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuya Yang",
      "Shaozhe Hao",
      "Yukang Cao",
      "Kwan-Yee K. Wong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_SGCR_Spherical_Gaussians_for_Efficient_3D_Curve_Reconstruction_CVPR_2025_paper.html": {
    "title": "SGCR: Spherical Gaussians for Efficient 3D Curve Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinran Yang",
      "Donghao Ji",
      "Yuanqi Li",
      "Jie Guo",
      "Yanwen Guo",
      "Junyuan Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Prompting_Depth_Anything_for_4K_Resolution_Accurate_Metric_Depth_Estimation_CVPR_2025_paper.html": {
    "title": "Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotong Lin",
      "Sida Peng",
      "Jingxiao Chen",
      "Songyou Peng",
      "Jiaming Sun",
      "Minghuan Liu",
      "Hujun Bao",
      "Jiashi Feng",
      "Xiaowei Zhou",
      "Bingyi Kang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bendou_ProKeR_A_Kernel_Perspective_on_Few-Shot_Adaptation_of_Large_Vision-Language_CVPR_2025_paper.html": {
    "title": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yassir Bendou",
      "Amine Ouasfi",
      "Vincent Gripon",
      "Adnane Boukhayma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Karypidis_Advancing_Semantic_Future_Prediction_through_Multimodal_Visual_Sequence_Transformers_CVPR_2025_paper.html": {
    "title": "Advancing Semantic Future Prediction through Multimodal Visual Sequence Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Efstathios Karypidis",
      "Ioannis Kakogeorgiou",
      "Spyros Gidaris",
      "Nikos Komodakis"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_GET_Unlocking_the_Multi-modal_Potential_of_CLIP_for_Generalized_Category_CVPR_2025_paper.html": {
    "title": "GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enguang Wang",
      "Zhimao Peng",
      "Zhengyuan Xie",
      "Fei Yang",
      "Xialei Liu",
      "Ming-Ming Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_On_the_Out-Of-Distribution_Generalization_of_Large_Multimodal_Models_CVPR_2025_paper.html": {
    "title": "On the Out-Of-Distribution Generalization of Large Multimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingxuan Zhang",
      "Jiansheng Li",
      "Wenjing Chu",
      "junjia hai",
      "Renzhe Xu",
      "Yuqing Yang",
      "Shikai Guan",
      "Jiazheng Xu",
      "Liping Jing",
      "Peng Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Enhanced_Contrastive_Learning_with_Multi-view_Longitudinal_Data_for_Chest_X-ray_CVPR_2025_paper.html": {
    "title": "Enhanced Contrastive Learning with Multi-view Longitudinal Data for Chest X-ray Report Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kang Liu",
      "Zhuoqi Ma",
      "Xiaolu Kang",
      "Yunan Li",
      "Kun Xie",
      "Zhicheng Jiao",
      "Qiguang Miao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MonoTAKD_Teaching_Assistant_Knowledge_Distillation_for_Monocular_3D_Object_Detection_CVPR_2025_paper.html": {
    "title": "MonoTAKD: Teaching Assistant Knowledge Distillation for Monocular 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hou-I Liu",
      "Christine Wu",
      "Jen-Hao Cheng",
      "Wenhao Chai",
      "Shian-Yun Wang",
      "Gaowen Liu",
      "Hugo Latapie",
      "Jhih-Ciang Wu",
      "Jenq-Neng Hwang",
      "Hong-Han Shuai",
      "Wen-Huang Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lv_Test-Time_Domain_Generalization_via_Universe_Learning_A_Multi-Graph_Matching_Approach_CVPR_2025_paper.html": {
    "title": "Test-Time Domain Generalization via Universe Learning: A Multi-Graph Matching Approach for Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingguo Lv",
      "Xingbo Dong",
      "Liwen Wang",
      "Jiewen Yang",
      "Lei Zhao",
      "Bin Pu",
      "Zhe Jin",
      "Xuejun Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Easy-editable_Image_Vectorization_with_Multi-layer_Multi-scale_Distributed_Visual_Feature_Embedding_CVPR_2025_paper.html": {
    "title": "Easy-editable Image Vectorization with Multi-layer Multi-scale Distributed Visual Feature Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Chen",
      "Zhangli Hu",
      "Zhongyin Zhao",
      "Yupeng Zhu",
      "Yue Shi",
      "Yuxuan Xiong",
      "Bingbing Ni"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_Acquire_and_then_Adapt_Squeezing_out_Text-to-Image_Model_for_Image_CVPR_2025_paper.html": {
    "title": "Acquire and then Adapt: Squeezing out Text-to-Image Model for Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyuan Deng",
      "Xinyi Wu",
      "Yongxing Yang",
      "Congchao Zhu",
      "Song Wang",
      "Zhenyao Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hou_DeDe_Detecting_Backdoor_Samples_for_SSL_Encoders_via_Decoders_CVPR_2025_paper.html": {
    "title": "DeDe: Detecting Backdoor Samples for SSL Encoders via Decoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sizai Hou",
      "Songze Li",
      "Duanyi Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing_CVPR_2025_paper.html": {
    "title": "Towards Scalable Human-aligned Benchmark for Text-guided Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suho Ryu",
      "Kihyun Kim",
      "Eugene Baek",
      "Dongsoo Shin",
      "Joonseok Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Devils_in_Middle_Layers_of_Large_Vision-Language_Models_Interpreting_Detecting_CVPR_2025_paper.html": {
    "title": "Devils in Middle Layers of Large Vision-Language Models: Interpreting, Detecting and Mitigating Object Hallucinations via Attention Lens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangqi Jiang",
      "Junkai Chen",
      "Beier Zhu",
      "Tingjin Luo",
      "Yankun Shen",
      "Xu Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_SpectroMotion_Dynamic_3D_Reconstruction_of_Specular_Scenes_CVPR_2025_paper.html": {
    "title": "SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng-De Fan",
      "Chen-Wei Chang",
      "Yi-Ruei Liu",
      "Jie-Ying Lee",
      "Jiun-Long Huang",
      "Yu-Chee Tseng",
      "Yu-Lun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Scaling Inference Time Compute for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nanye Ma",
      "Shangyuan Tong",
      "Haolin Jia",
      "Hexiang Hu",
      "Yu-Chuan Su",
      "Mingda Zhang",
      "Xuan Yang",
      "Yandong Li",
      "Tommi Jaakkola",
      "Xuhui Jia",
      "Saining Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large_CVPR_2025_paper.html": {
    "title": "Coeff-Tuning: A Graph Filter Subspace View for Tuning Attention-Based Large Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichen Miao",
      "Wei Chen",
      "Qiang Qiu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_VTON_360_High-Fidelity_Virtual_Try-On_from_Any_Viewing_Direction_CVPR_2025_paper.html": {
    "title": "VTON 360: High-Fidelity Virtual Try-On from Any Viewing Direction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijian He",
      "Yuwei Ning",
      "Yipeng Qin",
      "Guangrun Wang",
      "Sibei Yang",
      "Liang Lin",
      "Guanbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MVBoost_Boost_3D_Reconstruction_with_Multi-View_Refinement_CVPR_2025_paper.html": {
    "title": "MVBoost: Boost 3D Reconstruction with Multi-View Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Liu",
      "Xiaomei Zhang",
      "Zhiyuan Ma",
      "Xiangyu Zhu",
      "Zhen Lei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bai_Chat-based_Person_Retrieval_via_Dialogue-Refined_Cross-Modal_Alignment_CVPR_2025_paper.html": {
    "title": "Chat-based Person Retrieval via Dialogue-Refined Cross-Modal Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Bai",
      "Yucheng Ji",
      "Min Cao",
      "Jinqiao Wang",
      "Mang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_Category-Agnostic_Neural_Object_Rigging_CVPR_2025_paper.html": {
    "title": "Category-Agnostic Neural Object Rigging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangzhao He",
      "Chen Geng",
      "Shangzhe Wu",
      "Jiajun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_AVF-MAE_Scaling_Affective_Video_Facial_Masked_Autoencoders_via_Efficient_Audio-Visual_CVPR_2025_paper.html": {
    "title": "AVF-MAE++: Scaling Affective Video Facial Masked Autoencoders via Efficient Audio-Visual Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuecheng Wu",
      "Heli Sun",
      "Yifan Wang",
      "Jiayu Nie",
      "Jie Zhang",
      "Yabing Wang",
      "Junxiao Xue",
      "Liang He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_POPEN_Preference-Based_Optimization_and_Ensemble_for_LVLM-Based_Reasoning_Segmentation_CVPR_2025_paper.html": {
    "title": "POPEN: Preference-Based Optimization and Ensemble for LVLM-Based Reasoning Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lanyun Zhu",
      "Tianrun Chen",
      "Qianxiong Xu",
      "Xuanyi Liu",
      "Deyi Ji",
      "Haiyang Wu",
      "De Wen Soh",
      "Jun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_DiffusionSfM_Predicting_Structure_and_Motion_via_Ray_Origin_and_Endpoint_CVPR_2025_paper.html": {
    "title": "DiffusionSfM: Predicting Structure and Motion via Ray Origin and Endpoint Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qitao Zhao",
      "Amy Lin",
      "Jeff Tan",
      "Jason Y. Zhang",
      "Deva Ramanan",
      "Shubham Tulsiani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language_CVPR_2025_paper.html": {
    "title": "GroundingFace: Fine-grained Face Understanding via Pixel Grounding Multimodal Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Han",
      "Jiangning Zhang",
      "Junwei Zhu",
      "Runze Hou",
      "Xiaozhong Ji",
      "Chuming Lin",
      "Xiaobin Hu",
      "Zhucun Xue",
      "Yong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency_CVPR_2025_paper.html": {
    "title": "Self-Supervised Cross-View Correspondence with Predictive Cycle Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alan Baade",
      "Changan Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_MMAudio_Taming_Multimodal_Joint_Training_for_High-Quality_Video-to-Audio_Synthesis_CVPR_2025_paper.html": {
    "title": "MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ho Kei Cheng",
      "Masato Ishii",
      "Akio Hayakawa",
      "Takashi Shibuya",
      "Alexander Schwing",
      "Yuki Mitsufuji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ao_CryptoFace_End-to-End_Encrypted_Face_Recognition_CVPR_2025_paper.html": {
    "title": "CryptoFace: End-to-End Encrypted Face Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Ao",
      "Vishnu Naresh Boddeti"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Relation-Rich_Visual_Document_Generator_for_Visual_Information_Extraction_CVPR_2025_paper.html": {
    "title": "Relation-Rich Visual Document Generator for Visual Information Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zi-Han Jiang",
      "Chien-Wei Lin",
      "Wei-Hua Li",
      "Hsuan-Tung Liu",
      "Yi-Ren Yeh",
      "Chu-Song Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_DynFocus_Dynamic_Cooperative_Network_Empowers_LLMs_with_Video_Understanding_CVPR_2025_paper.html": {
    "title": "DynFocus: Dynamic Cooperative Network Empowers LLMs with Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yudong Han",
      "Qingpei Guo",
      "Liyuan Pan",
      "Liu Liu",
      "Yu Guan",
      "Ming Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Mimic_In-Context_Learning_for_Multimodal_Tasks_CVPR_2025_paper.html": {
    "title": "Mimic In-Context Learning for Multimodal Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchu Jiang",
      "Jiale Fu",
      "Chenduo Hao",
      "Xinting Hu",
      "Yingzhe Peng",
      "Xin Geng",
      "Xu Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zou_PromptHashAffinity-Prompted_Collaborative_Cross-Modal_Learning_for_Adaptive_Hashing_Retrieval_CVPR_2025_paper.html": {
    "title": "PromptHash:Affinity-Prompted Collaborative Cross-Modal Learning for Adaptive Hashing Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiang Zou",
      "Shuli Cheng",
      "Jiayi Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yue_V-Stylist_Video_Stylization_via_Collaboration_and_Reflection_of_MLLM_Agents_CVPR_2025_paper.html": {
    "title": "V-Stylist: Video Stylization via Collaboration and Reflection of MLLM Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengrong Yue",
      "Shaobin Zhuang",
      "Kunchang Li",
      "Yanbo Ding",
      "Yali Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Alhamoud_Vision-Language_Models_Do_Not_Understand_Negation_CVPR_2025_paper.html": {
    "title": "Vision-Language Models Do Not Understand Negation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kumail Alhamoud",
      "Shaden Alshammari",
      "Yonglong Tian",
      "Guohao Li",
      "Philip H.S. Torr",
      "Yoon Kim",
      "Marzyeh Ghassemi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_ID-Patch_Robust_ID_Association_for_Group_Photo_Personalization_CVPR_2025_paper.html": {
    "title": "ID-Patch: Robust ID Association for Group Photo Personalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yimeng Zhang",
      "Tiancheng Zhi",
      "Jing Liu",
      "Shen Sang",
      "Liming Jiang",
      "Qing Yan",
      "Sijia Liu",
      "Linjie Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cao_iG-6DoF_Model-free_6DoF_Pose_Estimation_for_Unseen_Object_via_Iterative_CVPR_2025_paper.html": {
    "title": "iG-6DoF: Model-free 6DoF Pose Estimation for Unseen Object via Iterative 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuo Cao",
      "Fei Luo",
      "Jiongming Qin",
      "Yu Jiang",
      "Yusen Wang",
      "Chunxia Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D_CVPR_2025_paper.html": {
    "title": "NexusGS: Sparse View Synthesis with Epipolar Depth Priors in 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulong Zheng",
      "Zicheng Jiang",
      "Shengfeng He",
      "Yandu Sun",
      "Junyu Dong",
      "Huaidong Zhang",
      "Yong Du"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hedlin_HyperNet_Fields_Efficiently_Training_Hypernetworks_without_Ground_Truth_by_Learning_CVPR_2025_paper.html": {
    "title": "HyperNet Fields: Efficiently Training Hypernetworks without Ground Truth by Learning Weight Trajectories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Hedlin",
      "Munawar Hayat",
      "Fatih Porikli",
      "Kwang Moo Yi",
      "Shweta Mahajan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Universal_Scene_Graph_Generation_CVPR_2025_paper.html": {
    "title": "Universal Scene Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengqiong Wu",
      "Hao Fei",
      "Tat-seng Chua"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Long_RICCARDO_Radar_Hit_Prediction_and_Convolution_for_Camera-Radar_3D_Object_CVPR_2025_paper.html": {
    "title": "RICCARDO: Radar Hit Prediction and Convolution for Camera-Radar 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunfei Long",
      "Abhinav Kumar",
      "Xiaoming Liu",
      "Daniel Morris"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density_CVPR_2025_paper.html": {
    "title": "ForestLPR: LiDAR Place Recognition in Forests Attentioning Multiple BEV Density Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanqing Shen",
      "Turcan Tuna",
      "Marco Hutter",
      "Cesar Cadena",
      "Nanning Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_BLADE_Single-view_Body_Mesh_Estimation_through_Accurate_Depth_Estimation_CVPR_2025_paper.html": {
    "title": "BLADE: Single-view Body Mesh Estimation through Accurate Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengze Wang",
      "Jiefeng Li",
      "Tianye Li",
      "Ye Yuan",
      "Henry Fuchs",
      "Koki Nagano",
      "Shalini De Mello",
      "Michael Stengel"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Du_AdaMMS_Model_Merging_for_Heterogeneous_Multimodal_Large_Language_Models_with_CVPR_2025_paper.html": {
    "title": "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Du",
      "Xiaochen Wang",
      "Chi Chen",
      "Jiabo Ye",
      "Yiru Wang",
      "Peng Li",
      "Ming Yan",
      "Ji Zhang",
      "Fei Huang",
      "Zhifang Sui",
      "Maosong Sun",
      "Yang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MoEE_Mixture_of_Emotion_Experts_for_Audio-Driven_Portrait_Animation_CVPR_2025_paper.html": {
    "title": "MoEE: Mixture of Emotion Experts for Audio-Driven Portrait Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huaize Liu",
      "Wenzhang Sun",
      "Donglin Di",
      "Shibo Sun",
      "Jiahui Yang",
      "Changqing Zou",
      "Hujun Bao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_ReCap_Better_Gaussian_Relighting_with_Cross-Environment_Captures_CVPR_2025_paper.html": {
    "title": "ReCap: Better Gaussian Relighting with Cross-Environment Captures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingzhi Li",
      "Zongwei Wu",
      "Eduard Zamfir",
      "Radu Timofte"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Split_Adaptation_for_Pre-trained_Vision_Transformers_CVPR_2025_paper.html": {
    "title": "Split Adaptation for Pre-trained Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lixu Wang",
      "Bingqi Shang",
      "Yi Li",
      "Payal Mohapatra",
      "Wei Dong",
      "Xiao Wang",
      "Qi Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models_CVPR_2025_paper.html": {
    "title": "SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wufei Ma",
      "Luoxin Ye",
      "Celso M de Melo",
      "Alan Yuille",
      "Jieneng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ni_SLVR_Super-Light_Visual_Reconstruction_via_Blueprint_Controllable_Convolutions_and_Exploring_CVPR_2025_paper.html": {
    "title": "SLVR: Super-Light Visual Reconstruction via Blueprint Controllable Convolutions and Exploring Feature Diversity Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ning Ni",
      "Libao Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Vision-Language_Embodiment_for_Monocular_Depth_Estimation_CVPR_2025_paper.html": {
    "title": "Vision-Language Embodiment for Monocular Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinchang Zhang",
      "Guoyu Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Layered_Image_Vectorization_via_Semantic_Simplification_CVPR_2025_paper.html": {
    "title": "Layered Image Vectorization via Semantic Simplification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Wang",
      "Jianxi Huang",
      "Zhida Sun",
      "Yuanhao Gong",
      "Daniel Cohen-Or",
      "Min Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Learning_Occlusion-Robust_Vision_Transformers_for_Real-Time_UAV_Tracking_CVPR_2025_paper.html": {
    "title": "Learning Occlusion-Robust Vision Transformers for Real-Time UAV Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "You Wu",
      "Xucheng Wang",
      "Xiangyang Yang",
      "Mengyuan Liu",
      "Dan Zeng",
      "Hengzhou Ye",
      "Shuiwang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zeng_Plug-and-Play_Versatile_Compressed_Video_Enhancement_CVPR_2025_paper.html": {
    "title": "Plug-and-Play Versatile Compressed Video Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huimin Zeng",
      "Jiacheng Li",
      "Zhiwei Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion_CVPR_2025_paper.html": {
    "title": "UltraFusion: Ultra High Dynamic Imaging using Exposure Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Chen",
      "Yujin Wang",
      "Xin Cai",
      "Zhiyuan You",
      "Zheming Lu",
      "Fan Zhang",
      "Shi Guo",
      "Tianfan Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Hearing_Anywhere_in_Any_Environment_CVPR_2025_paper.html": {
    "title": "Hearing Anywhere in Any Environment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiulong Liu",
      "Anurag Kumar",
      "Paul Calamia",
      "Sebastia V. Amengual",
      "Calvin Murdock",
      "Ishwarya Ananthabhotla",
      "Philip Robinson",
      "Eli Shlizerman",
      "Vamsi Krishna Ithapu",
      "Ruohan Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Automated_Proof_of_Polynomial_Inequalities_via_Reinforcement_Learning_CVPR_2025_paper.html": {
    "title": "Automated Proof of Polynomial Inequalities via Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Banglong Liu",
      "Niuniu Qi",
      "Xia Zeng",
      "Lydia Dehbi",
      "Zhengfeng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_Noise-Resistant_Video_Anomaly_Detection_via_RGB_Error-Guided_Multiscale_Predictive_Coding_CVPR_2025_paper.html": {
    "title": "Noise-Resistant Video Anomaly Detection via RGB Error-Guided Multiscale Predictive Coding and Dynamic Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Hu",
      "Wenli Du",
      "Peng Liao",
      "Bing Wang",
      "Siyuan Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Frequency_Dynamic_Convolution_for_Dense_Image_Prediction_CVPR_2025_paper.html": {
    "title": "Frequency Dynamic Convolution for Dense Image Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linwei Chen",
      "Lin Gu",
      "Liang Li",
      "Chenggang Yan",
      "Ying Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_IDEA_Inverted_Text_with_Cooperative_Deformable_Aggregation_for_Multi-modal_Object_CVPR_2025_paper.html": {
    "title": "IDEA: Inverted Text with Cooperative Deformable Aggregation for Multi-modal Object Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Wang",
      "Yongfeng Lv",
      "Pingping Zhang",
      "Huchuan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mei_SAM-I2V_Upgrading_SAM_to_Support_Promptable_Video_Segmentation_with_Less_CVPR_2025_paper.html": {
    "title": "SAM-I2V: Upgrading SAM to Support Promptable Video Segmentation with Less than 0.2% Training Cost",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyang Mei",
      "Pengyu Zhang",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shaker_GroupMamba_Efficient_Group-Based_Visual_State_Space_Model_CVPR_2025_paper.html": {
    "title": "GroupMamba: Efficient Group-Based Visual State Space Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdelrahman Shaker",
      "Syed Talal Wasim",
      "Salman Khan",
      "Juergen Gall",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and_CVPR_2025_paper.html": {
    "title": "How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Prakash",
      "Benjamin Lundell",
      "Dmitry Andreychuk",
      "David Forsyth",
      "Saurabh Gupta",
      "Harpreet Sawhney"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hadgi_Escaping_Platos_Cave_Towards_the_Alignment_of_3D_and_Text_CVPR_2025_paper.html": {
    "title": "Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Souhail Hadgi",
      "Luca Moschella",
      "Andrea Santilli",
      "Diego Gomez",
      "Qixing Huang",
      "Emanuele Rodolà",
      "Simone Melzi",
      "Maks Ovsjanikov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Purohit_Consistency_Posterior_Sampling_for_Diverse_Image_Synthesis_CVPR_2025_paper.html": {
    "title": "Consistency Posterior Sampling for Diverse Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vishal Purohit",
      "Matthew Repasky",
      "Jianfeng Lu",
      "Qiang Qiu",
      "Yao Xie",
      "Xiuyuan Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_IMFine_3D_Inpainting_via_Geometry-guided_Multi-view_Refinement_CVPR_2025_paper.html": {
    "title": "IMFine: 3D Inpainting via Geometry-guided Multi-view Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Shi",
      "Dong Huo",
      "Yuhongze Zhou",
      "Yan Min",
      "Juwei Lu",
      "Xinxin Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_ActiveGAMER_Active_GAussian_Mapping_through_Efficient_Rendering_CVPR_2025_paper.html": {
    "title": "ActiveGAMER: Active GAussian Mapping through Efficient Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyan Chen",
      "Huangying Zhan",
      "Kevin Chen",
      "Xiangyu Xu",
      "Qingan Yan",
      "Changjiang Cai",
      "Yi Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ahmed_DeepCompress-ViT_Rethinking_Model_Compression_to_Enhance_Efficiency_of_Vision_Transformers_CVPR_2025_paper.html": {
    "title": "DeepCompress-ViT: Rethinking Model Compression to Enhance Efficiency of Vision Transformers at the Edge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sabbir Ahmed",
      "Abdullah Al Arafat",
      "Deniz Najafi",
      "Akhlak Mahmood",
      "Mamshad Nayeem Rizve",
      "Mohaiminul Al Nahian",
      "Ranyang Zhou",
      "Shaahin Angizi",
      "Adnan Siraj Rakin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kalble_EvOcc_Accurate_Semantic_Occupancy_for_Automated_Driving_Using_Evidence_Theory_CVPR_2025_paper.html": {
    "title": "EvOcc: Accurate Semantic Occupancy for Automated Driving Using Evidence Theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas Kälble",
      "Sascha Wirges",
      "Maxim Tatarchenko",
      "Eddy Ilg"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Positive2Negative_Breaking_the_Information-Lossy_Barrier_in_Self-Supervised_Single_Image_Denoising_CVPR_2025_paper.html": {
    "title": "Positive2Negative: Breaking the Information-Lossy Barrier in Self-Supervised Single Image Denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Li",
      "Lizhi Wang",
      "Zhiyuan Xu",
      "Lin Zhu",
      "Wanxuan Lu",
      "Hua Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Towards_Continual_Universal_Segmentation_CVPR_2025_paper.html": {
    "title": "Towards Continual Universal Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Lin",
      "Zilei Wang",
      "Xu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose_CVPR_2025_paper.html": {
    "title": "PGC: Physics-Based Gaussian Cloth from a Single Pose",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michelle Guo",
      "Matt Jen-Yuan Chiang",
      "Igor Santesteban",
      "Nikolaos Sarafianos",
      "Hsiao-yu Chen",
      "Oshri Halimi",
      "Aljaž Božič",
      "Shunsuke Saito",
      "Jiajun Wu",
      "C. Karen Liu",
      "Tuur Stuyck",
      "Egor Larionov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Joint_Vision-Language_Social_Bias_Removal_for_CLIP_CVPR_2025_paper.html": {
    "title": "Joint Vision-Language Social Bias Removal for CLIP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Zhang",
      "Yangyang Guo",
      "Mohan Kankanhalli"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds_CVPR_2025_paper.html": {
    "title": "MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2 Seconds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenggang Tang",
      "Yuchen Fan",
      "Dilin Wang",
      "Hongyu Xu",
      "Rakesh Ranjan",
      "Alexander Schwing",
      "Zhicheng Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Explicit_Depth-Aware_Blurry_Video_Frame_Interpolation_Guided_by_Differential_Curves_CVPR_2025_paper.html": {
    "title": "Explicit Depth-Aware Blurry Video Frame Interpolation Guided by Differential Curves",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zaoming Yan",
      "Pengcheng Lei",
      "Tingting Wang",
      "Faming Fang",
      "Junkang Zhang",
      "Yaomin Huang",
      "Haichuan Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Selvaraju_OFER_Occluded_Face_Expression_Reconstruction_CVPR_2025_paper.html": {
    "title": "OFER: Occluded Face Expression Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pratheba Selvaraju",
      "Victoria Fernandez Abrevaya",
      "Timo Bolkart",
      "Rick Akkerman",
      "Tianyu Ding",
      "Faezeh Amjadi",
      "Ilya Zharkov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_SeaLion_Semantic_Part-Aware_Latent_Point_Diffusion_Models_for_3D_Generation_CVPR_2025_paper.html": {
    "title": "SeaLion: Semantic Part-Aware Latent Point Diffusion Models for 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dekai Zhu",
      "Yan Di",
      "Stefan Gavranovic",
      "Slobodan Ilic"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power_CVPR_2025_paper.html": {
    "title": "MonSter: Marry Monodepth to Stereo Unleashes Power",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junda Cheng",
      "Longliang Liu",
      "Gangwei Xu",
      "Xianqi Wang",
      "Zhaoxing Zhang",
      "Yong Deng",
      "Jinliang Zang",
      "Yurui Chen",
      "Zhipeng Cai",
      "Xin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Toward_Real-world_BEV_Perception_Depth_Uncertainty_Estimation_via_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "Toward Real-world BEV Perception: Depth Uncertainty Estimation via Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shu-Wei Lu",
      "Yi-Hsuan Tsai",
      "Yi-Ting Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shirkavand_Efficient_Fine-Tuning_and_Concept_Suppression_for_Pruned_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Efficient Fine-Tuning and Concept Suppression for Pruned Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reza Shirkavand",
      "Peiran Yu",
      "Shangqian Gao",
      "Gowthami Somepalli",
      "Tom Goldstein",
      "Heng Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection_CVPR_2025_paper.html": {
    "title": "Cubify Anything: Scaling Indoor 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justin Lazarow",
      "David Griffiths",
      "Gefen Kohavi",
      "Francisco Crespo",
      "Afshin Dehghan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_WildGS-SLAM_Monocular_Gaussian_Splatting_SLAM_in_Dynamic_Environments_CVPR_2025_paper.html": {
    "title": "WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianhao Zheng",
      "Zihan Zhu",
      "Valentin Bieri",
      "Marc Pollefeys",
      "Songyou Peng",
      "Iro Armeni"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mildenberger_A_Tale_of_Two_Classes_Adapting_Supervised_Contrastive_Learning_to_CVPR_2025_paper.html": {
    "title": "A Tale of Two Classes: Adapting Supervised Contrastive Learning to Binary Imbalanced Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Mildenberger",
      "Paul Hager",
      "Daniel Rueckert",
      "Martin J. Menten"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_DEAL_Data-Efficient_Adversarial_Learning_for_High-Quality_Infrared_Imaging_CVPR_2025_paper.html": {
    "title": "DEAL: Data-Efficient Adversarial Learning for High-Quality Infrared Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhu Liu",
      "Zijun Wang",
      "Jinyuan Liu",
      "Fanqi Meng",
      "Long Ma",
      "Risheng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_RePerformer_Immersive_Human-centric_Volumetric_Videos_from_Playback_to_Photoreal_Reperformance_CVPR_2025_paper.html": {
    "title": "RePerformer: Immersive Human-centric Volumetric Videos from Playback to Photoreal Reperformance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuheng Jiang",
      "Zhehao Shen",
      "Chengcheng Guo",
      "Yu Hong",
      "Zhuo Su",
      "Yingliang Zhang",
      "Marc Habermann",
      "Lan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yue_CheXWorld_Exploring_Image_World_Modeling_for_Radiograph_Representation_Learning_CVPR_2025_paper.html": {
    "title": "CheXWorld: Exploring Image World Modeling for Radiograph Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Yue",
      "Yulin Wang",
      "Chenxin Tao",
      "Pan Liu",
      "Shiji Song",
      "Gao Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Song_Towards_Long-Horizon_Vision-Language_Navigation_Platform_Benchmark_and_Method_CVPR_2025_paper.html": {
    "title": "Towards Long-Horizon Vision-Language Navigation: Platform, Benchmark and Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinshuai Song",
      "Weixing Chen",
      "Yang Liu",
      "Weikai Chen",
      "Guanbin Li",
      "Liang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection_CVPR_2025_paper.html": {
    "title": "Learning Class Prototypes for Unified Sparse-Supervised 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun Zhu",
      "Le Hui",
      "Hang Yang",
      "Jianjun Qian",
      "Jin Xie",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_BimArt_A_Unified_Approach_for_the_Synthesis_of_3D_Bimanual_CVPR_2025_paper.html": {
    "title": "BimArt: A Unified Approach for the Synthesis of 3D Bimanual Interaction with Articulated Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanyue Zhang",
      "Rishabh Dabral",
      "Vladislav Golyanik",
      "Vasileios Choutas",
      "Eduardo Alvarado",
      "Thabo Beeler",
      "Marc Habermann",
      "Christian Theobalt"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ao_Open-World_Amodal_Appearance_Completion_CVPR_2025_paper.html": {
    "title": "Open-World Amodal Appearance Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayang Ao",
      "Yanbei Jiang",
      "Qiuhong Ke",
      "Krista A. Ehinger"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_AIGV-Assessor_Benchmarking_and_Evaluating_the_Perceptual_Quality_of_Text-to-Video_Generation_CVPR_2025_paper.html": {
    "title": "AIGV-Assessor: Benchmarking and Evaluating the Perceptual Quality of Text-to-Video Generation with LMM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiarui Wang",
      "Huiyu Duan",
      "Guangtao Zhai",
      "Juntong Wang",
      "Xiongkuo Min"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_CoSpace_Benchmarking_Continuous_Space_Perception_Ability_for_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "CoSpace: Benchmarking Continuous Space Perception Ability for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiqi Zhu",
      "Ziyue Wang",
      "Can Zhang",
      "Peng Li",
      "Yang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Autoregressive_Distillation_of_Diffusion_Transformers_CVPR_2025_paper.html": {
    "title": "Autoregressive Distillation of Diffusion Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeongmin Kim",
      "Sotiris Anagnostidis",
      "Yuming Du",
      "Edgar Schönfeld",
      "Jonas Kohler",
      "Markos Georgopoulos",
      "Albert Pumarola",
      "Ali Thabet",
      "Artsiom Sanakoyeu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_RivuletMLP_An_MLP-based_Architecture_for_Efficient_Compressed_Video_Quality_Enhancement_CVPR_2025_paper.html": {
    "title": "RivuletMLP: An MLP-based Architecture for Efficient Compressed Video Quality Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gang He",
      "Weiran Wang",
      "Guancheng Quan",
      "Shihao Wang",
      "Dajiang Zhou",
      "Yunsong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as_CVPR_2025_paper.html": {
    "title": "OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingjie Pan",
      "Jiyao Zhang",
      "Tianshu Wu",
      "Yinghao Zhao",
      "Wenlong Gao",
      "Hao Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_FreeTimeGS_Free_Gaussian_Primitives_at_Anytime_Anywhere_for_Dynamic_Scene_CVPR_2025_paper.html": {
    "title": "FreeTimeGS: Free Gaussian Primitives at Anytime Anywhere for Dynamic Scene Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Wang",
      "Peishan Yang",
      "Zhen Xu",
      "Jiaming Sun",
      "Zhanhua Zhang",
      "Yong Chen",
      "Hujun Bao",
      "Sida Peng",
      "Xiaowei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware_CVPR_2025_paper.html": {
    "title": "Show and Tell: Visually Explainable Deep Neural Nets via Spatially-Aware Concept Bottleneck Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Itay Benou",
      "Tammy Riklin Raviv"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_DiscoVLA_Discrepancy_Reduction_in_Vision_Language_and_Alignment_for_Parameter-Efficient_CVPR_2025_paper.html": {
    "title": "DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leqi Shen",
      "Guoqiang Gong",
      "Tianxiang Hao",
      "Tao He",
      "Yifeng Zhang",
      "Pengzhang Liu",
      "Sicheng Zhao",
      "Jungong Han",
      "Guiguang Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli_CVPR_2025_paper.html": {
    "title": "Reanimating Images using Neural Representations of Dynamic Stimuli",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacob Yeung",
      "Andrew F. Luo",
      "Gabriel Sarch",
      "Margaret M. Henderson",
      "Deva Ramanan",
      "Michael J. Tarr"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_Visual-Instructed_Degradation_Diffusion_for_All-in-One_Image_Restoration_CVPR_2025_paper.html": {
    "title": "Visual-Instructed Degradation Diffusion for All-in-One Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyang Luo",
      "Haina Qin",
      "Zewen Chen",
      "Libin Wang",
      "Dandan Zheng",
      "Yuming Li",
      "Yufan Liu",
      "Bing Li",
      "Weiming Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Roh_Insightful_Instance_Features_for_3D_Instance_Segmentation_CVPR_2025_paper.html": {
    "title": "Insightful Instance Features for 3D Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonseok Roh",
      "Hwanhee Jung",
      "Giljoo Nam",
      "Dong In Lee",
      "Hyeongcheol Park",
      "Sang Ho Yoon",
      "Jungseock Joo",
      "Sangpil Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual_CVPR_2025_paper.html": {
    "title": "Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengqiong Wu",
      "Hao Fei",
      "Jingkang Yang",
      "Xiangtai Li",
      "Juncheng Li",
      "Hanwang Zhang",
      "Tat-seng Chua"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ke_Knowledge_Bridger_Towards_Training-Free_Missing_Modality_Completion_CVPR_2025_paper.html": {
    "title": "Knowledge Bridger: Towards Training-Free Missing Modality Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanzhou Ke",
      "Shengfeng He",
      "Xiaoli Wang",
      "Bo Wang",
      "Guoqing Chao",
      "Yuanyang Zhang",
      "Yi Xie",
      "Hexing Su"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing__CVPR_2025_paper.html": {
    "title": "EmoDubber: Towards High Quality and Emotion Controllable Movie Dubbing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaoxiang Cong",
      "Jiadong Pan",
      "Liang Li",
      "Yuankai Qi",
      "Yuxin Peng",
      "Anton van den Hengel",
      "Jian Yang",
      "Qingming Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos_CVPR_2025_paper.html": {
    "title": "DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbo Hu",
      "Xiangjun Gao",
      "Xiaoyu Li",
      "Sijie Zhao",
      "Xiaodong Cun",
      "Yong Zhang",
      "Long Quan",
      "Ying Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_TexGarment_Consistent_Garment_UV_Texture_Generation_via_Efficient_3D_Structure-Guided_CVPR_2025_paper.html": {
    "title": "TexGarment: Consistent Garment UV Texture Generation via Efficient 3D Structure-Guided Diffusion Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialun Liu",
      "Jinbo Wu",
      "Xiaobo Gao",
      "Jiakui Hu",
      "Bojun Xiong",
      "Xing Liu",
      "Chen Zhao",
      "Hongbin Pei",
      "Haocheng Feng",
      "Yingying Li",
      "Errui Ding",
      "Jingdong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_A_Hubness_Perspective_on_Representation_Learning_for_Graph-Based_Multi-View_Clustering_CVPR_2025_paper.html": {
    "title": "A Hubness Perspective on Representation Learning for Graph-Based Multi-View Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheming Xu",
      "He Liu",
      "Congyan Lang",
      "Tao Wang",
      "Yidong Li",
      "Michael C. Kampffmeyer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lv_Spatial-Temporal_Graph_Diffusion_Policy_with_Kinematic_Modeling_for_Bimanual_Robotic_CVPR_2025_paper.html": {
    "title": "Spatial-Temporal Graph Diffusion Policy with Kinematic Modeling for Bimanual Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Lv",
      "Hao Li",
      "Xiang Deng",
      "Rui Shao",
      "Yinchuan Li",
      "Jianye Hao",
      "Longxiang Gao",
      "Michael Yu Wang",
      "Liqiang Nie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video_CVPR_2025_paper.html": {
    "title": "Semi-Supervised State-Space Model with Dynamic Stacking Filter for Real-World Video Deraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangquan Sun",
      "Wenqi Ren",
      "Juxiang Zhou",
      "Shu Wang",
      "Jianhou Gan",
      "Xiaochun Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face_CVPR_2025_paper.html": {
    "title": "Rethinking Vision-Language Model in Face Forensics: Multi-Modal Interpretable Forged Face Detector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Guo",
      "Xiufeng Song",
      "Yue Zhang",
      "Xiaohong Liu",
      "Xiaoming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction_CVPR_2025_paper.html": {
    "title": "TIDE: Training Locally Interpretable Domain Generalization Models Enables Test-time Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aishwarya Agarwal",
      "Srikrishna Karanam",
      "Vineet Gandhi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_VSNet_Focusing_on_the_Linguistic_Characteristics_of_Sign_Language_CVPR_2025_paper.html": {
    "title": "VSNet: Focusing on the Linguistic Characteristics of Sign Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Li",
      "Xinyue Chen",
      "Hongkai Li",
      "Xiaorong Pu",
      "Peng Jin",
      "Yazhou Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera_CVPR_2025_paper.html": {
    "title": "Active Hyperspectral Imaging Using an Event Camera",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bohan Yu",
      "Jinxiu Liang",
      "Zhuofeng Wang",
      "Bin Fan",
      "Art Subpa-asa",
      "Boxin Shi",
      "Imari Sato"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Relic_Bridging_the_Gap_between_Gaussian_Diffusion_Models_and_Universal_Quantization_CVPR_2025_paper.html": {
    "title": "Bridging the Gap between Gaussian Diffusion Models and Universal Quantization for Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Relic",
      "Roberto Azevedo",
      "Yang Zhang",
      "Markus Gross",
      "Christopher Schroers"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lai_ZeroVO_Visual_Odometry_with_Minimal_Assumptions_CVPR_2025_paper.html": {
    "title": "ZeroVO: Visual Odometry with Minimal Assumptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Lai",
      "Zekai Yin",
      "Eshed Ohn-Bar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yuan_VideoRefer_Suite_Advancing_Spatial-Temporal_Object_Understanding_with_Video_LLM_CVPR_2025_paper.html": {
    "title": "VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqian Yuan",
      "Hang Zhang",
      "Wentong Li",
      "Zesen Cheng",
      "Boqiang Zhang",
      "Long Li",
      "Xin Li",
      "Deli Zhao",
      "Wenqiao Zhang",
      "Yueting Zhuang",
      "Jianke Zhu",
      "Lidong Bing"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yun_Learning_to_Sample_Effective_and_Diverse_Prompts_for_Text-to-Image_Generation_CVPR_2025_paper.html": {
    "title": "Learning to Sample Effective and Diverse Prompts for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taeyoung Yun",
      "Dinghuai Zhang",
      "Jinkyoo Park",
      "Ling Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Multi-modal_Medical_Diagnosis_via_Large-small_Model_Collaboration_CVPR_2025_paper.html": {
    "title": "Multi-modal Medical Diagnosis via Large-small Model Collaboration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanyi Chen",
      "Zihua Zhao",
      "Jiangchao Yao",
      "Ya Zhang",
      "Jiajun Bu",
      "Haishuai Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_SAMBLE_Shape-Specific_Point_Cloud_Sampling_for_an_Optimal_Trade-Off_Between_CVPR_2025_paper.html": {
    "title": "SAMBLE: Shape-Specific Point Cloud Sampling for an Optimal Trade-Off Between Local Detail and Global Uniformity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengzhi Wu",
      "Yuxin Wan",
      "Hao Fu",
      "Julius Pfrommer",
      "Zeyun Zhong",
      "Junwei Zheng",
      "Jiaming Zhang",
      "Jürgen Beyerer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Image_Referenced_Sketch_Colorization_Based_on_Animation_Creation_Workflow_CVPR_2025_paper.html": {
    "title": "Image Referenced Sketch Colorization Based on Animation Creation Workflow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingkun Yan",
      "Xinrui Wang",
      "Zhuoru Li",
      "Suguru Saito",
      "Yusuke Iwasawa",
      "Yutaka Matsuo",
      "Jiaxian Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tao_HoVLE_Unleashing_the_Power_of_Monolithic_Vision-Language_Models_with_Holistic_CVPR_2025_paper.html": {
    "title": "HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxin Tao",
      "Shiqian Su",
      "Xizhou Zhu",
      "Chenyu Zhang",
      "Zhe Chen",
      "Jiawen Liu",
      "Wenhai Wang",
      "Lewei Lu",
      "Gao Huang",
      "Yu Qiao",
      "Jifeng Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Maiti_Gen3DEval_Using_vLLMs_for_Automatic_Evaluation_of_Generated_3D_Objects_CVPR_2025_paper.html": {
    "title": "Gen3DEval: Using vLLMs for Automatic Evaluation of Generated 3D Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shalini Maiti",
      "Lourdes Agapito",
      "Filippos Kokkinos"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_SkySense-O_Towards_Open-World_Remote_Sensing_Interpretation_with_Vision-Centric_Visual-Language_Modeling_CVPR_2025_paper.html": {
    "title": "SkySense-O: Towards Open-World Remote Sensing Interpretation with Vision-Centric Visual-Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zhu",
      "Jiangwei Lao",
      "Deyi Ji",
      "Junwei Luo",
      "Kang Wu",
      "Yingying Zhang",
      "Lixiang Ru",
      "Jian Wang",
      "Jingdong Chen",
      "Ming Yang",
      "Dong Liu",
      "Feng Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_AdaDARE-gamma_Balancing_Stability_and_Plasticity_in_Multi-modal_LLMs_through_Efficient_CVPR_2025_paper.html": {
    "title": "AdaDARE-gamma: Balancing Stability and Plasticity in Multi-modal LLMs through Efficient Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi Xie",
      "Jintao Yang",
      "Zhunchen Luo",
      "Yunbo Cao",
      "Qiang Gao",
      "Mengyuan Zhang",
      "Wenpeng Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign_CVPR_2025_paper.html": {
    "title": "Driving by the Rules: A Benchmark for Integrating Traffic Sign Regulations into Vectorized HD Map",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyuan Chang",
      "Maixuan Xue",
      "Xinran Liu",
      "Zheng Pan",
      "Xing Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis_CVPR_2025_paper.html": {
    "title": "LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanlin Wang",
      "Hao Ouyang",
      "Qiuyu Wang",
      "Wen Wang",
      "Ka Leong Cheng",
      "Qifeng Chen",
      "Yujun Shen",
      "Limin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_GaPT-DAR_Category-level_Garments_Pose_Tracking_via_Integrated_2D_Deformation_and_CVPR_2025_paper.html": {
    "title": "GaPT-DAR: Category-level Garments Pose Tracking via Integrated 2D Deformation and 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Zhang",
      "Mingliang Xu",
      "Jianan Wang",
      "Qiaojun Yu",
      "Lixin Yang",
      "Yonglu Li",
      "Cewu Lu",
      "Rujing Wang",
      "Liu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Purkrabek_ProbPose_A_Probabilistic_Approach_to_2D_Human_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "ProbPose: A Probabilistic Approach to 2D Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miroslav Purkrabek",
      "Jiri Matas"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_SapiensID_Foundation_for_Human_Recognition_CVPR_2025_paper.html": {
    "title": "SapiensID: Foundation for Human Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minchul Kim",
      "Dingqiang Ye",
      "Yiyang Su",
      "Feng Liu",
      "Xiaoming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_MIDI_Multi-Instance_Diffusion_for_Single_Image_to_3D_Scene_Generation_CVPR_2025_paper.html": {
    "title": "MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehuan Huang",
      "Yuan-Chen Guo",
      "Xingqiao An",
      "Yunhan Yang",
      "Yangguang Li",
      "Zi-Xin Zou",
      "Ding Liang",
      "Xihui Liu",
      "Yan-Pei Cao",
      "Lu Sheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_S4-Driver_Scalable_Self-Supervised_Driving_Multimodal_Large_Language_Model_with_Spatio-Temporal_CVPR_2025_paper.html": {
    "title": "S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Model with Spatio-Temporal Visual Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichen Xie",
      "Runsheng Xu",
      "Tong He",
      "Jyh-Jing Hwang",
      "Katie Luo",
      "Jingwei Ji",
      "Hubert Lin",
      "Letian Chen",
      "Yiren Lu",
      "Zhaoqi Leng",
      "Dragomir Anguelov",
      "Mingxing Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier_CVPR_2025_paper.html": {
    "title": "Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Tian",
      "Xiaoye Qu",
      "Zhenyi Lu",
      "Wei Wei",
      "Sichen Liu",
      "Yu Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling_CVPR_2025_paper.html": {
    "title": "FreeCloth: Free-form Generation Enhances Challenging Clothed Human Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Ye",
      "Xiaoxuan Ma",
      "Hai Ci",
      "Wentao Zhu",
      "Yizhou Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chiu_ABC-Former_Auxiliary_Bimodal_Cross-domain_Transformer_with_Interactive_Channel_Attention_for_CVPR_2025_paper.html": {
    "title": "ABC-Former: Auxiliary Bimodal Cross-domain Transformer with Interactive Channel Attention for White Balance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Cheng Chiu",
      "Guan-Rong Chen",
      "Zihao Chen",
      "Yan-Tsung Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Science-T2I_Addressing_Scientific_Illusions_in_Image_Synthesis_CVPR_2025_paper.html": {
    "title": "Science-T2I: Addressing Scientific Illusions in Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialuo Li",
      "Wenhao Chai",
      "Xingyu Fu",
      "Haiyang Xu",
      "Saining Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Teng_Fingerprinting_Denoising_Diffusion_Probabilistic_Models_CVPR_2025_paper.html": {
    "title": "Fingerprinting Denoising Diffusion Probabilistic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huan Teng",
      "Yuhui Quan",
      "Chengyu Wang",
      "Jun Huang",
      "Hui Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_MoST_Efficient_Monarch_Sparse_Tuning_for_3D_Representation_Learning_CVPR_2025_paper.html": {
    "title": "MoST: Efficient Monarch Sparse Tuning for 3D Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Han",
      "Yuan Tang",
      "Jinfeng Xu",
      "Xianzhi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_Re-thinking_Temporal_Search_for_Long-Form_Video_Understanding_CVPR_2025_paper.html": {
    "title": "Re-thinking Temporal Search for Long-Form Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhui Ye",
      "Zihan Wang",
      "Haosen Sun",
      "Keshigeyan Chandrasegaran",
      "Zane Durante",
      "Cristobal Eyzaguirre",
      "Yonatan Bisk",
      "Juan Carlos Niebles",
      "Ehsan Adeli",
      "Li Fei-Fei",
      "Jiajun Wu",
      "Manling Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_InstanceGaussian_Appearance-Semantic_Joint_Gaussian_Representation_for_3D_Instance-Level_Perception_CVPR_2025_paper.html": {
    "title": "InstanceGaussian: Appearance-Semantic Joint Gaussian Representation for 3D Instance-Level Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haijie Li",
      "Yanmin Wu",
      "Jiarui Meng",
      "Qiankun Gao",
      "Zhiyao Zhang",
      "Ronggang Wang",
      "Jian Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rathore_When_Domain_Generalization_meets_Generalized_Category_Discovery_An_Adaptive_Task-Arithmetic_CVPR_2025_paper.html": {
    "title": "When Domain Generalization meets Generalized Category Discovery: An Adaptive Task-Arithmetic Driven Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vaibhav Rathore",
      "Shubhranil B",
      "Saikat Dutta",
      "Sarthak Mehrotra",
      "Zsolt Kira",
      "Biplab Banerjee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ding_CSC-PA_Cross-image_Semantic_Correlation_via_Prototype_Attentions_for_Single-network_Semi-supervised_CVPR_2025_paper.html": {
    "title": "CSC-PA: Cross-image Semantic Correlation via Prototype Attentions for Single-network Semi-supervised Breast Tumor Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenhui Ding",
      "Guilian Chen",
      "Qin Zhang",
      "Huisi Wu",
      "Jing Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_BIP3D_Bridging_2D_Images_and_3D_Perception_for_Embodied_Intelligence_CVPR_2025_paper.html": {
    "title": "BIP3D: Bridging 2D Images and 3D Perception for Embodied Intelligence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuewu Lin",
      "Tianwei Lin",
      "Lichao Huang",
      "Hongyu Xie",
      "Zhizhong Su"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Query_Efficient_Black-Box_Visual_Prompting_with_Subspace_Learning_CVPR_2025_paper.html": {
    "title": "Query Efficient Black-Box Visual Prompting with Subspace Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaogeng Liu",
      "Haozhen Zhang",
      "Hualin Zhang",
      "Xingchen Li",
      "Wanli Shi",
      "Bin Gu",
      "Yi Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_VisionPAD_A_Vision-Centric_Pre-training_Paradigm_for_Autonomous_Driving_CVPR_2025_paper.html": {
    "title": "VisionPAD: A Vision-Centric Pre-training Paradigm for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiming Zhang",
      "Wending Zhou",
      "Yiyao Zhu",
      "Xu Yan",
      "Jiantao Gao",
      "Dongfeng Bai",
      "Yingjie Cai",
      "Bingbing Liu",
      "Shuguang Cui",
      "Zhen Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Detecting_Adversarial_Data_Using_Perturbation_Forgery_CVPR_2025_paper.html": {
    "title": "Detecting Adversarial Data Using Perturbation Forgery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Wang",
      "Chen Li",
      "Yuchen Luo",
      "Hefei Ling",
      "Shijuan Huang",
      "Ruoxi Jia",
      "Ning Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_CoA_Towards_Real_Image_Dehazing_via_Compression-and-Adaptation_CVPR_2025_paper.html": {
    "title": "CoA: Towards Real Image Dehazing via Compression-and-Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long Ma",
      "Yuxin Feng",
      "Yan Zhang",
      "Jinyuan Liu",
      "Weimin Wang",
      "Guang-Yong Chen",
      "Chengpei Xu",
      "Zhuo Su"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bi_NightAdapter_Learning_a_Frequency_Adapter_for_Generalizable_Night-time_Scene_Segmentation_CVPR_2025_paper.html": {
    "title": "NightAdapter: Learning a Frequency Adapter for Generalizable Night-time Scene Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Bi",
      "Jingjun Yi",
      "Huimin Huang",
      "Hao Zheng",
      "Haolan Zhan",
      "Yawen Huang",
      "Yuexiang Li",
      "Xian Wu",
      "Yefeng Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pang_UMFN_Unified_Multi-Domain_Face_Normalization_for_Joint_Cross-domain_Prototype_Learning_CVPR_2025_paper.html": {
    "title": "UMFN: Unified Multi-Domain Face Normalization for Joint Cross-domain Prototype Learning and Heterogeneous Face Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meng Pang",
      "Wenjun Zhang",
      "Nanrun Zhou",
      "Shengbo Chen",
      "Hong Rao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_TopV_Compatible_Token_Pruning_with_Inference_Time_Optimization_for_Fast_CVPR_2025_paper.html": {
    "title": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Yang",
      "Yang Sui",
      "Jinqi Xiao",
      "Lingyi Huang",
      "Yu Gong",
      "Chendi Li",
      "Jinghua Yan",
      "Yu Bai",
      "Ponnuswamy Sadayappan",
      "Xia Hu",
      "Bo Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_Improving_Autoregressive_Visual_Generation_with_Cluster-Oriented_Token_Prediction_CVPR_2025_paper.html": {
    "title": "Improving Autoregressive Visual Generation with Cluster-Oriented Token Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Teng Hu",
      "Jiangning Zhang",
      "Ran Yi",
      "Jieyu Weng",
      "Yabiao Wang",
      "Xianfang Zeng",
      "Zhucun Xue",
      "Lizhuang Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and_CVPR_2025_paper.html": {
    "title": "Learned Binocular-Encoding Optics for RGBD Imaging Using Joint Stereo and Focus Cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhui Liu",
      "Liangxun Ou",
      "Qiang Fu",
      "Hadi Amata",
      "Wolfgang Heidrich",
      "Yifan Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tao_Dual-view_X-ray_Detection_Can_AI_Detect_Prohibited_Items_from_Dual-view_CVPR_2025_paper.html": {
    "title": "Dual-view X-ray Detection: Can AI Detect Prohibited Items from Dual-view X-ray Images like Humans?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renshuai Tao",
      "Haoyu Wang",
      "Yuzhe Guo",
      "Hairong Chen",
      "Li Zhang",
      "Xianglong Liu",
      "Yunchao Wei",
      "Yao Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_LUCAS_Layered_Universal_Codec_Avatars_CVPR_2025_paper.html": {
    "title": "LUCAS: Layered Universal Codec Avatars",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Liu",
      "Teng Deng",
      "Giljoo Nam",
      "Yu Rong",
      "Stanislav Pidhorskyi",
      "Junxuan Li",
      "Jason Saragih",
      "Dimitris N. Metaxas",
      "Chen Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_MobilePortrait_Real-Time_One-Shot_Neural_Head_Avatars_on_Mobile_Devices_CVPR_2025_paper.html": {
    "title": "MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianwen Jiang",
      "Gaojie Lin",
      "Zhengkun Rong",
      "Chao Liang",
      "Yongming Zhu",
      "Jiaqi Yang",
      "Tianyun Zhong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_D3_Scaling_Up_Deepfake_Detection_by_Learning_from_Discrepancy_CVPR_2025_paper.html": {
    "title": "D^3: Scaling Up Deepfake Detection by Learning from Discrepancy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongqi Yang",
      "Zhihao Qian",
      "Ye Zhu",
      "Olga Russakovsky",
      "Yu Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiang_Jailbreaking_the_Non-Transferable_Barrier_via_Test-Time_Data_Disguising_CVPR_2025_paper.html": {
    "title": "Jailbreaking the Non-Transferable Barrier via Test-Time Data Disguising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongli Xiang",
      "Ziming Hong",
      "Lina Yao",
      "Dadong Wang",
      "Tongliang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion_CVPR_2025_paper.html": {
    "title": "Light3R-SfM: Towards Feed-forward Structure-from-Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sven Elflein",
      "Qunjie Zhou",
      "Laura Leal-Taixé"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Robotic_Visual_Instruction_CVPR_2025_paper.html": {
    "title": "Robotic Visual Instruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanbang Li",
      "Ziyang Gong",
      "Haoyang Li",
      "Xiaoqi Huang",
      "Haolan Kang",
      "Guangping Bai",
      "Xianzheng Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_Solving_Instance_Detection_from_an_Open-World_Perspective_CVPR_2025_paper.html": {
    "title": "Solving Instance Detection from an Open-World Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianqian Shen",
      "Yunhan Zhao",
      "Nahyun Kwon",
      "Jeeeun Kim",
      "Yanan Li",
      "Shu Kong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Percept_Memory_and_Imagine_World_Feature_Simulating_for_Open-Domain_Unknown_CVPR_2025_paper.html": {
    "title": "Percept, Memory, and Imagine: World Feature Simulating for Open-Domain Unknown Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aming Wu",
      "Cheng Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Efficient_Depth_Estimation_for_Unstable_Stereo_Camera_Systems_on_AR_CVPR_2025_paper.html": {
    "title": "Efficient Depth Estimation for Unstable Stereo Camera Systems on AR Glasses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongfan Liu",
      "Hyoukjun Kwon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_3D-GRAND_A_Million-Scale_Dataset_for_3D-LLMs_with_Better_Grounding_and_CVPR_2025_paper.html": {
    "title": "3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianing Yang",
      "Xuweiyi Chen",
      "Nikhil Madaan",
      "Madhavan Iyengar",
      "Shengyi Qian",
      "David F. Fouhey",
      "Joyce Chai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_LiDAR-RT_Gaussian-based_Ray_Tracing_for_Dynamic_LiDAR_Re-simulation_CVPR_2025_paper.html": {
    "title": "LiDAR-RT: Gaussian-based Ray Tracing for Dynamic LiDAR Re-simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxu Zhou",
      "Lvchang Fu",
      "Sida Peng",
      "Yunzhi Yan",
      "Zhanhua Zhang",
      "Yong Chen",
      "Jiazhi Xia",
      "Xiaowei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Generative_Zero-Shot_Composed_Image_Retrieval_CVPR_2025_paper.html": {
    "title": "Generative Zero-Shot Composed Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lan Wang",
      "Wei Ao",
      "Vishnu Naresh Boddeti",
      "Ser-Nam Lim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shin_Large-Scale_Text-to-Image_Model_with_Inpainting_is_a_Zero-Shot_Subject-Driven_Image_CVPR_2025_paper.html": {
    "title": "Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaehun Shin",
      "Jooyoung Choi",
      "Heeseung Kim",
      "Sungroh Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors_CVPR_2025_paper.html": {
    "title": "MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riku Murai",
      "Eric Dexheimer",
      "Andrew J. Davison"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_Flow-NeRF_Joint_Learning_of_Geometry_Poses_and_Dense_Flow_within_CVPR_2025_paper.html": {
    "title": "Flow-NeRF: Joint Learning of Geometry, Poses, and Dense Flow within Unified Neural Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xunzhi Zheng",
      "Dan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation_CVPR_2025_paper.html": {
    "title": "Viewpoint Rosetta Stone: Unlocking Unpaired Ego-Exo Videos for View-invariant Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mi Luo",
      "Zihui Xue",
      "Alex Dimakis",
      "Kristen Grauman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Cross-modal_Information_Flow_in_Multimodal_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "Cross-modal Information Flow in Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Zhang",
      "Srishti Yadav",
      "Fengze Han",
      "Ekaterina Shutova"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Consistent_and_Controllable_Image_Animation_with_Motion_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Consistent and Controllable Image Animation with Motion Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Ma",
      "Yaohui Wang",
      "Gengyun Jia",
      "Xinyuan Chen",
      "Tien-Tsin Wong",
      "Yuan-Fang Li",
      "Cunjian Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_Towards_Better_Alignment_Training_Diffusion_Models_with_Reinforcement_Learning_Against_CVPR_2025_paper.html": {
    "title": "Towards Better Alignment: Training Diffusion Models with Reinforcement Learning Against Sparse Rewards",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijing Hu",
      "Fengda Zhang",
      "Long Chen",
      "Kun Kuang",
      "Jiahui Li",
      "Kaifeng Gao",
      "Jun Xiao",
      "Xin Wang",
      "Wenwu Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large_CVPR_2025_paper.html": {
    "title": "Spatial457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Mutimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingrui Wang",
      "Wufei Ma",
      "Tiezheng Zhang",
      "Celso M de Melo",
      "Jieneng Chen",
      "Alan Yuille"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_Omnidirectional_Multi-Object_Tracking_CVPR_2025_paper.html": {
    "title": "Omnidirectional Multi-Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Luo",
      "Hao Shi",
      "Sheng Wu",
      "Fei Teng",
      "Mengfei Duan",
      "Chang Huang",
      "Yuhang Wang",
      "Kaiwei Wang",
      "Kailun Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bhatnagar_Potential_Field_Based_Deep_Metric_Learning_CVPR_2025_paper.html": {
    "title": "Potential Field Based Deep Metric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shubhang Bhatnagar",
      "Narendra Ahuja"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Enhancing_Vision-Language_Compositional_Understanding_with_Multimodal_Synthetic_Data_CVPR_2025_paper.html": {
    "title": "Enhancing Vision-Language Compositional Understanding with Multimodal Synthetic Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxin Li",
      "Boyang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hou_Directional_Label_Diffusion_Model_for_Learning_from_Noisy_Labels_CVPR_2025_paper.html": {
    "title": "Directional Label Diffusion Model for Learning from Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Senyu Hou",
      "Gaoxia Jiang",
      "Jia Zhang",
      "Shangrong Yang",
      "Husheng Guo",
      "Yaqing Guo",
      "Wenjian Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_AA-CLIP_Enhancing_Zero-Shot_Anomaly_Detection_via_Anomaly-Aware_CLIP_CVPR_2025_paper.html": {
    "title": "AA-CLIP: Enhancing Zero-Shot Anomaly Detection via Anomaly-Aware CLIP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxin Ma",
      "Xu Zhang",
      "Qingsong Yao",
      "Fenghe Tang",
      "Chenxu Wu",
      "Yingtai Li",
      "Rui Yan",
      "Zihang Jiang",
      "S.Kevin Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_HybridGS_Decoupling_Transients_and_Statics_with_2D_and_3D_Gaussian_CVPR_2025_paper.html": {
    "title": "HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyu Lin",
      "Jiaqi Gu",
      "Lubin Fan",
      "Bojian Wu",
      "Yujing Lou",
      "Renjie Chen",
      "Ligang Liu",
      "Jieping Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Keyframe-Guided_Creative_Video_Inpainting_CVPR_2025_paper.html": {
    "title": "Keyframe-Guided Creative Video Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwei Guo",
      "Ceyuan Yang",
      "Anyi Rao",
      "Chenlin Meng",
      "Omer Bar-Tal",
      "Shuangrui Ding",
      "Maneesh Agrawala",
      "Dahua Lin",
      "Bo Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Channel_Consistency_Prior_and_Self-Reconstruction_Strategy_Based_Unsupervised_Image_Deraining_CVPR_2025_paper.html": {
    "title": "Channel Consistency Prior and Self-Reconstruction Strategy Based Unsupervised Image Deraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanglu Dong",
      "Tianheng Zheng",
      "Yuanzhouhan Cao",
      "Linbo Qing",
      "Chao Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_MobileMamba_Lightweight_Multi-Receptive_Visual_Mamba_Network_CVPR_2025_paper.html": {
    "title": "MobileMamba: Lightweight Multi-Receptive Visual Mamba Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyang He",
      "Jiangning Zhang",
      "Yuxuan Cai",
      "Hongxu Chen",
      "Xiaobin Hu",
      "Zhenye Gan",
      "Yabiao Wang",
      "Chengjie Wang",
      "Yunsheng Wu",
      "Lei Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_EdgeTAM_On-Device_Track_Anything_Model_CVPR_2025_paper.html": {
    "title": "EdgeTAM: On-Device Track Anything Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chong Zhou",
      "Chenchen Zhu",
      "Yunyang Xiong",
      "Saksham Suri",
      "Fanyi Xiao",
      "Lemeng Wu",
      "Raghuraman Krishnamoorthi",
      "Bo Dai",
      "Chen Change Loy",
      "Vikas Chandra",
      "Bilge Soran"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tran_SimLTD_Simple_Supervised_and_Semi-Supervised_Long-Tailed_Object_Detection_CVPR_2025_paper.html": {
    "title": "SimLTD: Simple Supervised and Semi-Supervised Long-Tailed Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Phi Vu Tran"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Soni_EarthDial_Turning_Multi-sensory_Earth_Observations_to_Interactive_Dialogues_CVPR_2025_paper.html": {
    "title": "EarthDial: Turning Multi-sensory Earth Observations to Interactive Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sagar Soni",
      "Akshay Dudhane",
      "Hiyam Debary",
      "Mustansar Fiaz",
      "Muhammad Akhtar Munir",
      "Muhammad Sohail Danish",
      "Paolo Fraccaro",
      "Campbell D Watson",
      "Levente J Klein",
      "Fahad Shahbaz Khan",
      "Salman Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Song_Learning_Endogenous_Attention_for_Incremental_Object_Detection_CVPR_2025_paper.html": {
    "title": "Learning Endogenous Attention for Incremental Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Song",
      "Yuhang He",
      "Jingyuan Li",
      "Qiang Wang",
      "Yihong Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhai_StarGen_A_Spatiotemporal_Autoregression_Framework_with_Video_Diffusion_Model_for_CVPR_2025_paper.html": {
    "title": "StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion Model for Scalable and Controllable Scene Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangjin Zhai",
      "Zhichao Ye",
      "Jialin Liu",
      "Weijian Xie",
      "Jiaqi Hu",
      "Zhen Peng",
      "Hua Xue",
      "Danpeng Chen",
      "Xiaomeng Wang",
      "Lei Yang",
      "Nan Wang",
      "Haomin Liu",
      "Guofeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wei_HyperSeg_Hybrid_Segmentation_Assistant_with_Fine-grained_Visual_Perceiver_CVPR_2025_paper.html": {
    "title": "HyperSeg: Hybrid Segmentation Assistant with Fine-grained Visual Perceiver",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Wei",
      "Yujie Zhong",
      "Haoxian Tan",
      "Yong Liu",
      "Jie Hu",
      "Dengjie Li",
      "Zheng Zhao",
      "Yujiu Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Diffusion-based_Event_Generation_for_High-Quality_Image_Deblurring_CVPR_2025_paper.html": {
    "title": "Diffusion-based Event Generation for High-Quality Image Deblurring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinan Xie",
      "Qing Zhang",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Video_Summarization_with_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "Video Summarization with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min Jung Lee",
      "Dayoung Gong",
      "Minsu Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Khan_Sketchtopia_A_Dataset_and_Foundational_Agents_for_Benchmarking_Asynchronous_Multimodal_CVPR_2025_paper.html": {
    "title": "Sketchtopia: A Dataset and Foundational Agents for Benchmarking Asynchronous Multimodal Communication with Iconic Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohd Hozaifa Khan",
      "Ravi Kiran Sarvadevabhatla"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Consistency-aware_Self-Training_for_Iterative-based_Stereo_Matching_CVPR_2025_paper.html": {
    "title": "Consistency-aware Self-Training for Iterative-based Stereo Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi Zhou",
      "Peng Ye",
      "Haoyu Zhang",
      "Jiakang Yuan",
      "Rao Qiang",
      "Liu YangChenXu",
      "Wu Cailin",
      "Feng Xu",
      "Tao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MV-MATH_Evaluating_Multimodal_Math_Reasoning_in_Multi-Visual_Contexts_CVPR_2025_paper.html": {
    "title": "MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peijie Wang",
      "Zhong-Zhi Li",
      "Fei Yin",
      "Dekang Ran",
      "Cheng-Lin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression_CVPR_2025_paper.html": {
    "title": "Balanced Rate-Distortion Optimization in Learned Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichi Zhang",
      "Zhihao Duan",
      "Yuning Huang",
      "Fengqing Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Bridge_the_Gap_From_Weak_to_Full_Supervision_for_Temporal_CVPR_2025_paper.html": {
    "title": "Bridge the Gap: From Weak to Full Supervision for Temporal Action Localization with PseudoFormer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Liu",
      "Yangcen Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ding_HomoGen_Enhanced_Video_Inpainting_via_Homography_Propagation_and_Diffusion_CVPR_2025_paper.html": {
    "title": "HomoGen: Enhanced Video Inpainting via Homography Propagation and Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ding Ding",
      "Yueming Pan",
      "Ruoyu Feng",
      "Qi Dai",
      "Kai Qiu",
      "Jianmin Bao",
      "Chong Luo",
      "Zhenzhong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/An_Generalized_Few-shot_3D_Point_Cloud_Segmentation_with_Vision-Language_Model_CVPR_2025_paper.html": {
    "title": "Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaochong An",
      "Guolei Sun",
      "Yun Liu",
      "Runjia Li",
      "Junlin Han",
      "Ender Konukoglu",
      "Serge Belongie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Do_ImageNet-trained_Models_Learn_Shortcuts_The_Impact_of_Frequency_Shortcuts_CVPR_2025_paper.html": {
    "title": "Do ImageNet-trained Models Learn Shortcuts? The Impact of Frequency Shortcuts on Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shunxin Wang",
      "Raymond Veldhuis",
      "Nicola Strisciuglio"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Geng_HORP_Human-Object_Relation_Priors_Guided_HOI_Detection_CVPR_2025_paper.html": {
    "title": "HORP: Human-Object Relation Priors Guided HOI Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pei Geng",
      "Jian Yang",
      "Shanshan Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Building_a_Mind_Palace_Structuring_Environment-Grounded_Semantic_Graphs_for_Effective_CVPR_2025_paper.html": {
    "title": "Building a Mind Palace: Structuring Environment-Grounded Semantic Graphs for Effective Long Video Analysis with LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyi Huang",
      "Yuyang Ji",
      "Xiaofang Wang",
      "Nikhil Mehta",
      "Tong Xiao",
      "Donghyun Lee",
      "Sigmund Vanvalkenburgh",
      "Shengxin Zha",
      "Bolin Lai",
      "Licheng Yu",
      "Ning Zhang",
      "Yong Jae Lee",
      "Miao Liu"
    ]
  }
}