{
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_Deterministic_Image-to-Image_Translation_via_Denoising_Brownian_Bridge_Models_with_Dual_CVPR_2025_paper.html": {
    "title": "Deterministic Image-to-Image Translation via Denoising Brownian Bridge Models with Dual Approximators",
    "volume": "main",
    "abstract": "Image-to-Image (I2I) translation involves converting an im- age from one domain to another. Deterministic I2I transla- tion, such as in image super-resolution, extends this con- cept by guaranteeing that each input generates a consistent and predictable output, closely matching the ground truth (GT) with high fidelity. In this paper, we propose a denois- ing Brownian bridge model with dual approximators (Dual- approx Bridge), a novel generative model that exploits the Brownian bridge dynamics and two neural network-based approximators (one for forward and one for reverse pro- cess) to produce faithful output with negligible variance and high image quality in I2I translations. Our extensive exper- iments on benchmark datasets including image generation and super-resolution demonstrate the consistent and supe- rior performance of Dual-approx Bridge in terms of im- age quality and faithfulness to GT when compared to both stochastic and deterministic baselines. Project page and code: https://github.com/bohan95/dual-app-bridge",
    "checked": true,
    "id": "76f1db1e7b31d2815a2318b64412fffdf993900f",
    "semantic_title": "deterministic image-to-image translation via denoising brownian bridge models with dual approximators",
    "citation_count": 0,
    "authors": [
      "Bohan Xiao",
      "Peiyong Wang",
      "Qisheng He",
      "Ming Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ahmed_Towards_Source-Free_Machine_Unlearning_CVPR_2025_paper.html": {
    "title": "Towards Source-Free Machine Unlearning",
    "volume": "main",
    "abstract": "As machine learning become more pervasive and data privacy regulations evolve, the ability to remove private or copyrighted information from trained models is becoming an increasingly critical requirement. Existing unlearning methods often rely on the assumption of having access to the entire training dataset during the forgetting process. However, this assumption may not hold true in practical scenarios where the original training data may not be accessible, i.e., the source-free setting. To address this challenge, we focus on the source-free unlearning scenario, where an unlearning algorithm must be capable of removing specific data from a trained model without requiring access to the original training dataset. Building on recent work, we present a method that can estimate the Hessian of the unknown remaining training data, a crucial component required for efficient unlearning. Leveraging this estimation technique, our method enables efficient zero-shot unlearning while providing robust theoretical guarantees on the unlearning performance, while maintaining performance on the remaining data. Extensive experiments over a wide range of datasets verify the efficacy of our method",
    "checked": true,
    "id": "e0a3f658a37c593fb18a3470eef78eecf5779d72",
    "semantic_title": "towards source-free machine unlearning",
    "citation_count": 0,
    "authors": [
      "Sk Miraj Ahmed",
      "Umit Yigit Basaran",
      "Dripta S. Raychaudhuri",
      "Arindam Dutta",
      "Rohit Kundu",
      "Fahim Faisal Niloy",
      "Basak Guler",
      "Amit K. Roy-Chowdhury"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yao_Uni4D_Unifying_Visual_Foundation_Models_for_4D_Modeling_from_a_CVPR_2025_paper.html": {
    "title": "Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single Video",
    "volume": "main",
    "abstract": "This paper presents a unified approach to understanding dynamic scenes from casual videos. Large pretrained vision foundation models, such as vision-language, video depth prediction, motion tracking, and segmentation models, offer promising capabilities. However, training a single model for comprehensive 4D understanding remains challenging. We introduce Uni4D, a multi-stage optimization framework that harnesses multiple pretrained models to advance dynamic 3D modeling, including static/dynamic reconstruction, camera pose estimation, and dense 3D motion tracking. Our results show state-of-the-art performance in dynamic 4D modeling with superior visual quality. Notably, Uni4D requires no retraining or fine-tuning, highlighting the effectiveness of repurposing visual foundation models for 4D understanding. Code and more results are available at: https://davidyao99.github.io/uni4d",
    "checked": true,
    "id": "56290d3eabfed76709a0d6e8fdae9704d3ab5e4a",
    "semantic_title": "uni4d: unifying visual foundation models for 4d modeling from a single video",
    "citation_count": 2,
    "authors": [
      "David Yifan Yao",
      "Albert J. Zhai",
      "Shenlong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_DynScene_Scalable_Generation_of_Dynamic_Robotic_Manipulation_Scenes_for_Embodied_CVPR_2025_paper.html": {
    "title": "DynScene: Scalable Generation of Dynamic Robotic Manipulation Scenes for Embodied AI",
    "volume": "main",
    "abstract": "Robotic manipulation in embodied AI critically depends on large-scale, high-quality datasets that reflect realistic object interactions and physical dynamics. However, existing data collection pipelines are often slow, expensive, and heavily reliant on manual efforts. We present DynScene, a diffusion-based framework for generating dynamic robotic manipulation scenes directly from textual instructions. Unlike prior methods that focus solely on static environments or isolated robot actions, DynScene decomposes the generation into two phases static scene synthesis and action trajectory generation allowing fine-grained control and diversity. Our model enhances realism and physical feasibility through scene refinement (layout sampling, quaternion quantization) and leverages residual action representation to enable action augmentation, generating multiple diverse trajectories from a single static configuration. Experiments show DynScene achieves 26.8x faster generation, 1.84x higher accuracy, and 28% greater action diversity than human-crafted data. Furthermore, agents trained with DynScene exhibit up to 19.4% higher success rates across complex manipulation tasks. Our approach paves the way for scalable, automated dataset generation in robot learning",
    "checked": true,
    "id": "a17138753a48b55966db7b087a83525eb1b0e723",
    "semantic_title": "dynscene: scalable generation of dynamic robotic manipulation scenes for embodied ai",
    "citation_count": 0,
    "authors": [
      "Sangmin Lee",
      "Sungyong Park",
      "Heewon Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rosu_DiffLocks_Generating_3D_Hair_from_a_Single_Image_using_Diffusion_CVPR_2025_paper.html": {
    "title": "DiffLocks: Generating 3D Hair from a Single Image using Diffusion Models",
    "volume": "main",
    "abstract": "We address the task of generating 3D hair geometry from a single image, which is challenging due to the diversity of hairstyles and the lack of paired image-to-3D hair data. Previous methods are primarily trained on synthetic data and cope with the limited amount of such data by using low-dimensional intermediate representations, such as guide strands and scalp-level embeddings, that require post-processing to decode, upsample, and add realism. These approaches fail to reconstruct detailed hair, struggle with curly hair, or are limited to handling only a few hairstyles. To overcome these limitations, we propose DiffLocks, a novel framework that enables detailed reconstruction of a wide variety of hairstyles directly from a single image. First, we address the lack of 3D hair data by automating the creation of the largest synthetic hair dataset to date, containing 40K hairstyles. Second, we leverage the synthetic hair dataset to learn an image-conditioned diffusion-transfomer model that generates accurate 3D strands from a single frontal image. By using a pretrained image backbone, our method generalizes to in-the-wild images despite being trained only on synthetic data. Our diffusion model predicts a scalp texture map in which any point in the map contains the latent code for an individual hair strand. These codes are directly decoded to 3D strands without post-processing techniques. Representing individual strands, instead of guide strands, enables the transformer to model the detailed spatial structure of complex hairstyles. With this, DiffLocks can recover highly curled hair, like afro hairstyles, from a single image for the first time. Data and code is available at https://radualexandru.github.io/difflocks",
    "checked": true,
    "id": "b0e2c758f302762a4ccaa45110b8e9ebcb51488b",
    "semantic_title": "difflocks: generating 3d hair from a single image using diffusion models",
    "citation_count": 0,
    "authors": [
      "Radu Alexandru Rosu",
      "Keyu Wu",
      "Yao Feng",
      "Youyi Zheng",
      "Michael J. Black"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Hyperbolic_Category_Discovery_CVPR_2025_paper.html": {
    "title": "Hyperbolic Category Discovery",
    "volume": "main",
    "abstract": "Generalized Category Discovery (GCD) is an intriguing open-world problem that has garnered increasing attention. Given a dataset that includes both labelled and unlabelled images, GCD aims to categorize all images in the unlabelled subset, regardless of whether they belong to known or unknown classes. In GCD, the common practice typically involves applying a spherical projection operator at the end of the self-supervised pretrained backbone, operating within Euclidean or spherical space. However, both of these spaces have been shown to be suboptimal for encoding samples that possess hierarchical structures. In contrast, hyperbolic space exhibits exponential volume growth relative to radius, making it inherently strong at capturing the hierarchical structure of samples from both seen and unseen categories. Therefore, we propose to tackle the category discovery challenge in the hyperbolic space. We introduce HypCD, a simple Hyperbolic framework for learning hierarchy-aware representations and classifiers for generalized Category Discovery. HypCD first transforms the Euclidean embedding space of the backbone network into hyperbolic space, facilitating subsequent representation and classification learning by considering both hyperbolic distance and the angle between samples. This approach is particularly helpful for knowledge transfer from known to unknown categories in GCD. We thoroughly evaluate HypCD on public GCD benchmarks, by applying it to various baseline and state-of-the-art methods, consistently achieving significant improvements",
    "checked": true,
    "id": "600b177b96d21126f7505d7915baf9e1f5be3412",
    "semantic_title": "hyperbolic category discovery",
    "citation_count": 1,
    "authors": [
      "Yuanpei Liu",
      "Zhenqi He",
      "Kai Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_The_Language_of_Motion_Unifying_Verbal_and_Non-verbal_Language_of_CVPR_2025_paper.html": {
    "title": "The Language of Motion: Unifying Verbal and Non-verbal Language of 3D Human Motion",
    "volume": "main",
    "abstract": "Human communication is inherently multimodal, involving a combination of verbal and non-verbal cues such as speech, facial expressions, and body gestures. Modeling these behaviors is essential for understanding human interaction and for creating virtual characters that can communicate naturally in applications like games, films, and virtual reality. However, existing motion generation models are typically limited to specific input modalities--either speech, text, or motion data--and cannot fully leverage the diversity of available data. In this paper, we propose a novel framework that unifies verbal and non-verbal language using multimodal language models for human motion understanding and generation. This model is flexible in taking text, speech, and motion or any combination of them as input. Coupled with our novel pre-training strategy, our model not only achieves state-of-the-art performance on co-speech gesture generation but also requires much less data for training. Our model also unlocks an array of novel tasks such as editable gesture generation and emotion prediction from motion. We believe unifying the verbal and non-verbal language of human motion is essential for real-world applications, and language models offer a powerful approach to achieving this goal",
    "checked": true,
    "id": "02b45d58fde7cba3465d2753bcab4f2082630c49",
    "semantic_title": "the language of motion: unifying verbal and non-verbal language of 3d human motion",
    "citation_count": 5,
    "authors": [
      "Changan Chen",
      "Juze Zhang",
      "Shrinidhi K. Lakshmikanth",
      "Yusu Fang",
      "Ruizhi Shao",
      "Gordon Wetzstein",
      "Li Fei-Fei",
      "Ehsan Adeli"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nguyen_CALICO_Part-Focused_Semantic_Co-Segmentation_with_Large_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "CALICO: Part-Focused Semantic Co-Segmentation with Large Vision-Language Models",
    "volume": "main",
    "abstract": "Recent advances in Large Vision-Language Models (LVLMs) have enabled general-purpose vision tasks through visual instruction tuning. While existing LVLMs can generate segmentation masks from text prompts for single images, they struggle with segmentation-grounded reasoning across images, especially at finer granularities such as object parts. In this paper, we introduce the new task of part-focused semantic co-segmentation, which involves identifying and segmenting common objects and their constituent common and unique parts across images. To address this task, we present CALICO, the first LVLM designed for multi-image part-level reasoning segmentation. CALICO features two key components, a novel Correspondence Extraction Module that identifies semantic part-level correspondences, and Correspondence Adaptation Modules that embed this information into the LVLM to facilitate multi-image understanding in a parameter-efficient manner. To support training and evaluation, we curate MixedParts, a large-scale multi-image segmentation dataset containing 2.4M samples across 44K images spanning diverse object and part categories. Experimental results demonstrate that CALICO, with just 0.3% of its parameters finetuned, achieves strong performance on this challenging task",
    "checked": true,
    "id": "c40c28f06427b4763a99fc98885a47b501802df5",
    "semantic_title": "calico: part-focused semantic co-segmentation with large vision-language models",
    "citation_count": 0,
    "authors": [
      "Kiet A. Nguyen",
      "Adheesh Juvekar",
      "Tianjiao Yu",
      "Muntasir Wahed",
      "Ismini Lourentzou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Task_Preference_Optimization_Improving_Multimodal_Large_Language_Models_with_Vision_CVPR_2025_paper.html": {
    "title": "Task Preference Optimization: Improving Multimodal Large Language Models with Vision Task Alignment",
    "volume": "main",
    "abstract": "Current multimodal large language models (MLLMs) struggle with fine-grained or precise understanding of visuals although they give comprehensive perception and reasoning in a spectrum of vision applications. Recent studies either develop tool-using or unify specific visual tasks into the autoregressive framework, often at the expense of overall multimodal performance. To address this issue and enhance MLLMs with visual tasks in a scalable fashion, we propose Task Preference Optimization (TPO), a novel method that utilizes differentiable task preferences derived from typical fine-grained visual tasks. TPO introduces learnable task tokens that establish connections between multiple task-specific heads and the MLLM. By leveraging rich visual labels during training, TPO significantly enhances the MLLM's multimodal capabilities and task-specific performance. Through multi-task co-training within TPO, we observe synergistic benefits that elevate individual task performance beyond what is achievable through single-task training methodologies. Our instantiation of this approach with VideoChat and LLaVA demonstrates an overall 14.6% improvement in multimodal performance compared to baseline models. Additionally, MLLM-TPO demonstrates robust zero-shot capabilities across various tasks, performing comparably to state-of-the-art supervised models",
    "checked": true,
    "id": "6a3b8254a3b803fd0d4618a36fbdc4fe6d8c19db",
    "semantic_title": "task preference optimization: improving multimodal large language models with vision task alignment",
    "citation_count": 6,
    "authors": [
      "Ziang Yan",
      "Zhilin Li",
      "Yinan He",
      "Chenting Wang",
      "Kunchang Li",
      "Xinhao Li",
      "Xiangyu Zeng",
      "Zilei Wang",
      "Yali Wang",
      "Yu Qiao",
      "Limin Wang",
      "Yi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Cross-modal_Causal_Relation_Alignment_for_Video_Question_Grounding_CVPR_2025_paper.html": {
    "title": "Cross-modal Causal Relation Alignment for Video Question Grounding",
    "volume": "main",
    "abstract": "Video question grounding (VideoQG) requires models to answer the questions and simultaneously infer the relevant video segments to support the answers. However, existing VideoQG methods usually suffer from spurious cross-modal correlations, leading to a failure to identify the dominant visual scenes that align with the intended question. Moreover, vision-language models exhibit unfaithful generalization performance and lack robustness on challenging downstream tasks such as VideoQG. In this work, we propose a novel VideoQG framework named Cross-modal Causal Relation Alignment (CRA), to eliminate spurious correlations and improve the causal consistency between question-answering and video temporal grounding. Our CRA involves three essential components: i) Gaussian Smoothing Grounding (GSG) module for estimating the time interval via cross-modal attention, which is de-noised by an adaptive Gaussian filter, ii) Cross-Modal Alignment (CMA) enhances the performance of weakly supervised VideoQG by leveraging bidirectional contrastive learning between estimated video segments and QA features, iii) Explicit Causal Intervention (ECI) module for multimodal deconfounding, which involves front-door intervention for vision and back-door intervention for language. Extensive experiments on two VideoQG datasets demonstrate the superiority of our CRA in discovering visually grounded content and achieving robust question reasoning. Codes are available at https://github.com/WissingChen/CRA-GQA",
    "checked": true,
    "id": "c6c4ea6548b0acfaba0b76863381aaf7a7646f85",
    "semantic_title": "cross-modal causal relation alignment for video question grounding",
    "citation_count": 2,
    "authors": [
      "Weixing Chen",
      "Yang Liu",
      "Binglin Chen",
      "Jiandong Su",
      "Yongsen Zheng",
      "Liang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_Words_or_Vision_Do_Vision-Language_Models_Have_Blind_Faith_in_CVPR_2025_paper.html": {
    "title": "Words or Vision: Do Vision-Language Models Have Blind Faith in Text?",
    "volume": "main",
    "abstract": "Vision-Language Models (VLMs) excel in integrating visual and textual information for vision-centric tasks, but their handling of inconsistencies between modalities is underexplored. We investigate VLMs' modality preferences when faced with visual data and varied textual inputs in vision-centered settings.By introducing textual variations to four vision-centric tasks and evaluating ten Vision-Language Models (VLMs), we discover a \"blind faith in text\" phenomenon: VLMs disproportionately trust textual data over visual data when inconsistencies arise, leading to significant performance drops under corrupted text and raising safety concerns.We analyze factors influencing this text bias, including instruction prompts, language model size, text relevance, token order, and the interplay between visual and textual certainty. While certain factors, such as scaling up the language model size, slightly mitigate text bias, others like token order can exacerbate it due to positional biases inherited from language models. To address this issue, we explore supervised fine-tuning with text augmentation and demonstrate its effectiveness in reducing text bias. Additionally, we provide a theoretical analysis suggesting that the blind faith in text phenomenon may stem from an imbalance of pure text and multi-modal data during training.Our findings highlight the need for balanced training and careful consideration of modality interactions in VLMs to enhance their robustness and reliability in handling multi-modal data inconsistencies",
    "checked": true,
    "id": "15827d3552d7060a16a4cb6a5060650282d21983",
    "semantic_title": "words or vision: do vision-language models have blind faith in text?",
    "citation_count": 3,
    "authors": [
      "Ailin Deng",
      "Tri Cao",
      "Zhirui Chen",
      "Bryan Hooi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Diffusion_Renderer_Neural_Inverse_and_Forward_Rendering_with_Video_Diffusion_CVPR_2025_paper.html": {
    "title": "Diffusion Renderer: Neural Inverse and Forward Rendering with Video Diffusion Models",
    "volume": "main",
    "abstract": "Understanding and modeling lighting effects are fundamental tasks in computer vision and graphics. Classic physically-based rendering (PBR) accurately simulates the light transport, but relies on precise scene representations--explicit 3D geometry, high-quality material properties, and lighting conditions--that are often impractical to obtain in real-world scenarios. Therefore, we introduce Diffusion Renderer, a neural approach that addresses the dual problem of inverse and forward rendering within a holistic framework. Leveraging powerful video diffusion model priors, the inverse rendering model accurately estimates G-buffers from real-world videos, providing an interface for image editing tasks, and training data for the rendering model. Conversely, our rendering model generates photorealistic images from G-buffers without explicit light transport simulation. Specifically, we first train a video diffusion model for inverse rendering on synthetic data, which generalizes well to real-world videos and allows us to auto-label diverse real-world videos. We then co-train our rendering model using both synthetic and auto-labeled real-world data. Experiments demonstrate that Diffusion Renderer effectively approximates inverse and forwards rendering, consistently outperforming the state-of-the-art. Our model enables practical applications from a single video input--including relighting, material editing, and realistic object insertion",
    "checked": false,
    "id": "709c2aaa92be6ddb51a3cb608127946622e0e879",
    "semantic_title": "diffusionrenderer: neural inverse and forward rendering with video diffusion models",
    "citation_count": 10,
    "authors": [
      "Ruofan Liang",
      "Zan Gojcic",
      "Huan Ling",
      "Jacob Munkberg",
      "Jon Hasselgren",
      "Chih-Hao Lin",
      "Jun Gao",
      "Alexander Keller",
      "Nandita Vijaykumar",
      "Sanja Fidler",
      "Zian Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Harnessing_Frequency_Spectrum_Insights_for_Image_Copyright_Protection_Against_Diffusion_CVPR_2025_paper.html": {
    "title": "Harnessing Frequency Spectrum Insights for Image Copyright Protection Against Diffusion Models",
    "volume": "main",
    "abstract": "Diffusion models have achieved remarkable success in novel view synthesis, but their reliance on large, diverse, and often untraceable Web datasets has raised pressing concerns about image copyright protection. Current methods fall short in reliably identifying unauthorized image use, as they struggle to generalize across varied generation tasks and fail when the training dataset includes images from multiple sources with few identifiable (watermarked or poisoned) samples. In this paper, we present novel evidence that diffusion-generated images faithfully preserve the statistical properties of their training data, particularly reflected in their spectral features. Leveraging this insight, we introduce CoprGuard, a robust frequency domain watermarking framework to safeguard against unauthorized image usage in diffusion model training and fine-tuning. CoprGuard demonstrates remarkable effectiveness against a wide range of models, from naive diffusion models to sophisticated text-to-image models, and is robust even when watermarked images comprise a mere 1% of the training dataset. This robust and versatile approach empowers content owners to protect their intellectual property in the era of AI-driven image generation",
    "checked": true,
    "id": "b2898a80bd8236cfcfa995a0b57e323724617af1",
    "semantic_title": "harnessing frequency spectrum insights for image copyright protection against diffusion models",
    "citation_count": 0,
    "authors": [
      "Zhenguang Liu",
      "Chao Shuai",
      "Shaojing Fan",
      "Ziping Dong",
      "Jinwu Hu",
      "Zhongjie Ba",
      "Kui Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_Learning_to_Detect_Objects_from__Multi-Agent_LiDAR_Scans_without_CVPR_2025_paper.html": {
    "title": "Learning to Detect Objects from Multi-Agent LiDAR Scans without Manual Labels",
    "volume": "main",
    "abstract": "Unsupervised 3D object detection serves as an important solution for offline 3D object annotation. However, due to the data sparsity and limited views, the clustering-based label fitting in unsupervised object detection often generates low-quality pseudo-labels. Multi-agent collaborative dataset, which involves the sharing of complementary observations among agents, holds the potential to break through this bottleneck. In this paper, we introduce a novel unsupervised method that learns to Detect Objects from Multi-Agent LiDAR scans, termed DOtA, without using labels from external. DOtA first uses the internally shared ego-pose and ego-shape of collaborative agents to initialize the detector, leveraging the generalization performance of neural networks to infer preliminary labels. Subsequently, DOtA uses the complementary observations between agents to perform multi-scale encoding on preliminary labels, then decodes high-quality and low-quality labels. These labels are further used as prompts to guide a correct feature learning process, thereby enhancing the performance of the unsupervised object detection task. Extensive experiments on the V2V4Real and OPV2V datasets show that our DOtA outperforms state-of-the-art unsupervised 3D object detection methods. Additionally, we also validate the effectiveness of the DOtA labels under various collaborative perception frameworks. The code is available at https://github.com/xmuqimingxia/DOtA",
    "checked": true,
    "id": "327283869a1edfd41aed772d2d373f936a398c65",
    "semantic_title": "learning to detect objects from multi-agent lidar scans without manual labels",
    "citation_count": 0,
    "authors": [
      "Qiming Xia",
      "Wenkai Lin",
      "Haoen Xiang",
      "Xun Huang",
      "Siheng Chen",
      "Zhen Dong",
      "Cheng Wang",
      "Chenglu Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zeng_DeepLA-Net_Very_Deep_Local_Aggregation_Networks_for_Point_Cloud_Analysis_CVPR_2025_paper.html": {
    "title": "DeepLA-Net: Very Deep Local Aggregation Networks for Point Cloud Analysis",
    "volume": "main",
    "abstract": "Due to the irregular and disordered data structure in 3D point clouds, prior works have focused on designing more sophisticated local representation methods to capture these complex local patterns. However, the recognition performance has saturated over the past few years, indicating that increasingly complex and redundant designs no longer make improvements to local learning. This phenomenon prompts us to diverge from the trend in 3D vision and instead pursue an alternative and successful solution: deeper neural networks. In this paper, we propose DeepLA-Net, a series of very deep networks for point cloud analysis. The key insight of our approach is to exploit a small but mighty local learning block, which reduces 10xfewer FLOPs, enabling the construction of very deep networks. Furthermore, we design a training supervision strategy to ensure smooth gradient backpropagation and optimization in very deep networks. We construct the DeepLA-Net family with a depth of up to 120 blocks --- at least 5xdeeper than recent methods --- trained on a single RTX 3090. An ensemble of the DeepLA-Net achieves state-of-the-art performance on classification and segmentation tasks of S3DIS Area5 (+2.2% mIoU), ScanNet test set (+1.6% mIoU), ScanObjectNN (+2.1% OA), and ShapeNetPart (+0.9% cls.mIoU)",
    "checked": false,
    "id": "15178b9b558a7cf03453b34e50a860782ae1a586",
    "semantic_title": "semantic segmentation of point cloud scene via multi-scale feature aggregation and adaptive fusion",
    "citation_count": 0,
    "authors": [
      "Ziyin Zeng",
      "Mingyue Dong",
      "Jian Zhou",
      "Huan Qiu",
      "Zhen Dong",
      "Man Luo",
      "Bijun Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Multi-Layer_Visual_Feature_Fusion_in_Multimodal_LLMs_Methods_Analysis_and_CVPR_2025_paper.html": {
    "title": "Multi-Layer Visual Feature Fusion in Multimodal LLMs: Methods, Analysis, and Best Practices",
    "volume": "main",
    "abstract": "Multimodal Large Language Models (MLLMs) have made significant advancements in recent years, with visual features playing an increasingly critical role in enhancing model performance. However, the integration of multi-layer visual features in MLLMs remains underexplored, particularly with regard to optimal layer selection and fusion strategies. Existing methods often rely on arbitrary design choices, leading to suboptimal outcomes. In this paper, we systematically investigate two core aspects of multi-layer visual feature fusion: (1) selecting the most effective visual layers and (2) identifying the best fusion approach with the language model. Our experiments reveal that while combining visual features from multiple stages improves generalization, incorporating additional features from the same stage typically leads to diminished performance. Furthermore, we find that direct fusion of multi-layer visual features at the input stage consistently yields superior and more stable performance across various configurations",
    "checked": true,
    "id": "042e07d3f2ef78a85e8471c6451d91d099e772ba",
    "semantic_title": "multi-layer visual feature fusion in multimodal llms: methods, analysis, and best practices",
    "citation_count": 0,
    "authors": [
      "Junyan Lin",
      "Haoran Chen",
      "Yue Fan",
      "Yingqi Fan",
      "Xin Jin",
      "Hui Su",
      "Jinlan Fu",
      "Xiaoyu Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_APHQ-ViT_Post-Training_Quantization_with_Average_Perturbation_Hessian_Based_Reconstruction_for_CVPR_2025_paper.html": {
    "title": "APHQ-ViT: Post-Training Quantization with Average Perturbation Hessian Based Reconstruction for Vision Transformers",
    "volume": "main",
    "abstract": "Vision Transformers (ViTs) have become one of the most commonly used backbones for vision tasks. Despite their remarkable performance, they often suffer significant accuracy drop when quantized for practical deployment, particularly by post-training quantization (PTQ) under ultra-low bits. Recently, reconstruction-based PTQ methods have shown promising performance in quantizing Convolutional Neural Networks (CNNs). However, they fail when applied to ViTs, primarily due to the inaccurate estimation of output importance and the substantial accuracy degradation in quantizing post-GELU activations. To address these issues, we propose APHQ-ViT, a novel PTQ approach based on importance estimation with Average Perturbation Hessian (APH). Specifically, we first thoroughly analyze the current approximation approaches with Hessian loss, and propose an improved average perturbation Hessian loss. To deal with the quantization of the post-GELU activations, we design an MLP-Reconstruction (MR) method by replacing the GELU function in MLP with ReLU and reconstructing it by the APH loss on a small unlabeled calibration set. Extensive experiments demonstrate that APHQ-ViT using linear quantizers outperforms existing PTQ methods by substantial margins in 3-bit and 4-bit across different vision tasks. The source code is available at https://github.com/GoatWu/APHQ-ViT",
    "checked": true,
    "id": "24deec15e3b67dd2319137e8e12a0a1f6e4d77fa",
    "semantic_title": "aphq-vit: post-training quantization with average perturbation hessian based reconstruction for vision transformers",
    "citation_count": 1,
    "authors": [
      "Zhuguanyu Wu",
      "Jiayi Zhang",
      "Jiaxin Chen",
      "Jinyang Guo",
      "Di Huang",
      "Yunhong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_AdaptCMVC_Robust_Adaption_to_Incremental_Views_in_Continual_Multi-view_Clustering_CVPR_2025_paper.html": {
    "title": "AdaptCMVC: Robust Adaption to Incremental Views in Continual Multi-view Clustering",
    "volume": "main",
    "abstract": "Most Multi-view Clustering approaches assume that all views are available for clustering. However, this assumption is often unrealistic as views are incrementally accumulated over time, leading to a need for continual multi-view clustering (CMVC) methods. Current approaches to CMVC leverage late fusion-based approaches, where a new model is typically learned individually for each view to obtain the corresponding partition matrix, and then used to update a consensus matrix via a moving average. These approaches are prone to view-specific noise and struggle to adapt to large gaps between different views. To address these shortcomings, we reconsider CMVC from the perspective of domain adaptation and propose AdaptCMVC, which learns how to incrementally accumulate knowledge of new views as they become available and prevents catastrophic forgetting. Specifically, a self-training framework is introduced to extend the model to new views, particularly designed to be robust to view-specific noise. Further, to combat catastrophic forgetting, a structure alignment mechanism is proposed to enable the model to explore the global group structure across multiple views. Experiments on several multi-view benchmarks demonstrate the effectiveness of our proposed method on the CMVC task. The code is available at: AdaptCMVC",
    "checked": true,
    "id": "20eb73b11d6663e4fa38baa9e5370fbc12a44b0f",
    "semantic_title": "adaptcmvc: robust adaption to incremental views in continual multi-view clustering",
    "citation_count": 0,
    "authors": [
      "Jing Wang",
      "Songhe Feng",
      "Kristoffer Knutsen Wickstr√∏m",
      "Michael C. Kampffmeyer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wei_Omni-Scene_Omni-Gaussian_Representation_for_Ego-Centric_Sparse-View_Scene_Reconstruction_CVPR_2025_paper.html": {
    "title": "Omni-Scene: Omni-Gaussian Representation for Ego-Centric Sparse-View Scene Reconstruction",
    "volume": "main",
    "abstract": "Prior works employing pixel-based Gaussian representation have demonstrated efficacy in feed-forward sparse-view reconstruction. However, such representation necessitates cross-view overlap for accurate depth estimation, and is challenged by object occlusions and frustum truncations. As a result, these methods require scene-centric data acquisition to maintain cross-view overlap and complete scene visibility to circumvent occlusions and truncations, which limits their applicability to scene-centric reconstruction. In contrast, in autonomous driving scenarios, a more practical paradigm is ego-centric reconstruction, which is characterized by minimal cross-view overlap and frequent occlusions and truncations. The limitations of pixel-based representation thus hinder the utility of prior works in this task. In light of this, this paper conducts an in-depth analysis of different representations, and introduces Omni-Gaussian representation with tailored network design to complement their strengths and mitigate their drawbacks. Experiments show that our method significantly surpasses state-of-the-art methods, pixelSplat and MVSplat, in ego-centric reconstruction, and achieves comparable performance to prior works in scene-centric reconstruction. Our code is available at https://github.com/WU-CVGL/Omni-Scene",
    "checked": true,
    "id": "a3cf0b5ad5aa6682b4a8a00d1b3b615730022f1a",
    "semantic_title": "omni-scene: omni-gaussian representation for ego-centric sparse-view scene reconstruction",
    "citation_count": 2,
    "authors": [
      "Dongxu Wei",
      "Zhiqi Li",
      "Peidong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_3DTopia-XL_Scaling_High-quality_3D_Asset_Generation_via_Primitive_Diffusion_CVPR_2025_paper.html": {
    "title": "3DTopia-XL: Scaling High-quality 3D Asset Generation via Primitive Diffusion",
    "volume": "main",
    "abstract": "The increasing demand for high-quality 3D assets across various industries necessitates efficient and automated 3D content creation. Despite recent advancements in 3D generative models, existing methods still face challenges with optimization speed, geometric fidelity, and the lack of assets for physically based rendering (PBR). In this paper, we introduce 3DTopia-XL, a scalable native 3D generative model designed to overcome these limitations. 3DTopia-XL leverages a novel primitive-based 3D representation, PrimX, which encodes detailed shape, albedo, and material field into a compact tensorial format, facilitating the modeling of high-resolution geometry with PBR assets. On top of the novel representation, we propose a generative framework based on Diffusion Transformer (DiT), which comprises 1) Primitive Patch Compression, 2) and Latent Primitive Diffusion. 3DTopia-XL learns to generate high-quality 3D assets from textual or visual inputs. Extensive qualitative and quantitative experiments are conducted to demonstrate that 3DTopia-XL significantly outperforms existing methods in generating high-quality 3D assets with fine-grained textures and materials, efficiently bridging the quality gap between generative models and real-world applications",
    "checked": true,
    "id": "7ff2a57cb9c6bc65e87ebc1725f118fddf5629c4",
    "semantic_title": "3dtopia-xl: scaling high-quality 3d asset generation via primitive diffusion",
    "citation_count": 23,
    "authors": [
      "Zhaoxi Chen",
      "Jiaxiang Tang",
      "Yuhao Dong",
      "Ziang Cao",
      "Fangzhou Hong",
      "Yushi Lan",
      "Tengfei Wang",
      "Haozhe Xie",
      "Tong Wu",
      "Shunsuke Saito",
      "Liang Pan",
      "Dahua Lin",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_UA-Pose_Uncertainty-Aware_6D_Object_Pose_Estimation_and_Online_Object_Completion_CVPR_2025_paper.html": {
    "title": "UA-Pose: Uncertainty-Aware 6D Object Pose Estimation and Online Object Completion with Partial References",
    "volume": "main",
    "abstract": "6D object pose estimation has shown strong generalizability to novel objects. However, existing methods often require either a complete, well-reconstructed 3D model or numerous reference images that fully cover the object. Estimating 6D poses from partial references, which capture only fragments of an object's appearance and geometry, remains challenging. To address this, we propose UA-Pose, an uncertainty-aware approach for 6D object pose estimation and online object completion specifically designed for partial references. We assume access to either (1) a limited set of RGBD images with known poses or (2) a single 2D image. For the first case, we initialize a partial object 3D model based on the provided images and poses, while for the second, we use image-to-3D techniques to generate an initial object 3D model. Our method integrates uncertainty into the incomplete 3D model, distinguishing between seen and unseen regions. This uncertainty enables confidence assessment in pose estimation and guides an uncertainty-aware sampling strategy for online object completion, enhancing robustness in pose estimation accuracy and improving object completeness. We evaluate our method on the YCB-Video, YCBInEOAT, and HO3D datasets, including RGBD sequences of YCB objects manipulated by robots and human hands. Experimental results demonstrate significant performance improvements over existing methods, particularly when object observations are incomplete or partially captured",
    "checked": true,
    "id": "59d9cb6156b54cc02e69056132af69c80d777508",
    "semantic_title": "ua-pose: uncertainty-aware 6d object pose estimation and online object completion with partial references",
    "citation_count": 0,
    "authors": [
      "Ming-Feng Li",
      "Xin Yang",
      "Fu-En Wang",
      "Hritam Basak",
      "Yuyin Sun",
      "Shreekant Gayaka",
      "Min Sun",
      "Cheng-Hao Kuo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Missing_Target-Relevant_Information_Prediction_with_World_Model_for_Accurate_Zero-Shot_CVPR_2025_paper.html": {
    "title": "Missing Target-Relevant Information Prediction with World Model for Accurate Zero-Shot Composed Image Retrieval",
    "volume": "main",
    "abstract": "Zero-Shot Composed Image Retrieval (ZS-CIR) involves diverse tasks with a broad range of visual content manipulation intent across domain, scene, object, and attribute. The key challenge for ZS-CIR tasks is to modify a reference image according to manipulation text to accurately retrieve a target image, especially when the reference image is missing essential target content. In this paper, we propose a novel prediction-based mapping network, named PrediCIR, to adaptively predict the missing target visual content in reference images in the latent space before mapping for accurate ZS-CIR. Specifically, a world view generation module first constructs a source view by omitting certain visual content of a target view, coupled with an action that includes the manipulation intent derived from existing image-caption pairs. Then, a target content prediction module trains a world model as a predictor to adaptively predict the missing visual information guided by user intention in manipulating text at the latent space. The two modules map an image with the predicted relevant information to a pseudo-word token without extra supervision. Our model shows strong generalization ability on six ZS-CIR tasks. It obtains consistent and significant performance boosts ranging from 1.73% to 4.45% over the best methods and achieves new state-of-the-art results on ZS-CIR. Our code is available at https://github.com/Pter61/predicir",
    "checked": true,
    "id": "aa42b6b5757164a6fc8b1a9c765194796a2a447c",
    "semantic_title": "missing target-relevant information prediction with world model for accurate zero-shot composed image retrieval",
    "citation_count": 2,
    "authors": [
      "Yuanmin Tang",
      "Jing Yu",
      "Keke Gai",
      "Jiamin Zhuang",
      "Gang Xiong",
      "Gaopeng Gou",
      "Qi Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Binarized_Mamba-Transformer_for_Lightweight_Quad_Bayer_HybridEVS_Demosaicing_CVPR_2025_paper.html": {
    "title": "Binarized Mamba-Transformer for Lightweight Quad Bayer HybridEVS Demosaicing",
    "volume": "main",
    "abstract": "Quad Bayer demosaicing is the central challenge for enabling the widespread application of Hybrid Event-based Vision Sensors (HybridEVS). Although existing learning-based methods that leverage long-range dependency modeling have achieved promising results, their complexity severely limits deployment on mobile devices for real-world applications. To address these limitations, we propose a lightweight Mamba-based binary neural network designed for efficient and high-performing demosaicing of HybridEVS RAW images. First, to effectively capture both global and local dependencies, we introduce a hybrid Binarized Mamba-Transformer architecture that combines the strengths of the Mamba and Swin Transformer architectures. Next, to significantly reduce computational complexity, we propose a binarized Mamba (Bi-Mamba), which binarizes all projections while retaining the core Selective Scan in full precision. Bi-Mamba also incorporates additional global visual information to enhance global context and mitigate precision loss. We conduct quantitative and qualitative experiments to demonstrate the effectiveness of BMTNet in both performance and computational efficiency, providing a lightweight demosaicing solution suited for real-world edge devices. Our codes and models are available at https://github.com/Clausy9/BMTNet",
    "checked": true,
    "id": "f0cbb7ec8c2ed947d3e9e21cca1e92cb1d95cb77",
    "semantic_title": "binarized mamba-transformer for lightweight quad bayer hybridevs demosaicing",
    "citation_count": 1,
    "authors": [
      "Shiyang Zhou",
      "Haijin Zeng",
      "Yunfan Lu",
      "Tong Shao",
      "Ke Tang",
      "Yongyong Chen",
      "Jie Liu",
      "Jingyong Su"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_DiffSensei_Bridging_Multi-Modal_LLMs_and_Diffusion_Models_for_Customized_Manga_CVPR_2025_paper.html": {
    "title": "DiffSensei: Bridging Multi-Modal LLMs and Diffusion Models for Customized Manga Generation",
    "volume": "main",
    "abstract": "Story visualization, the task of creating visual narratives from textual descriptions, has seen progress with text-to-image generation models. However, these models often lack effective control over character appearances and interactions, particularly in multi-character scenes. To address these limitations, we propose a new task: customized manga generation and introduce DiffSensei, an innovative framework specifically designed for generating manga with dynamic multi-character control. DiffSensei integrates a diffusion-based image generator with a multimodal large language model (MLLM) that acts as a text-compatible identity adapter. Our approach employs masked cross-attention to seamlessly incorporate character features, enabling precise layout control without direct pixel transfer. Additionally, the MLLM-based adapter adjusts character features to align with panel-specific text cues, allowing flexible adjustments in character expressions, poses, and actions. We also introduce MangaZero, a large-scale dataset tailored to this task, containing 43,264 manga pages and 427,147 annotated panels, supporting the visualization of varied character interactions and movements across sequential frames. Extensive experiments demonstrate that DiffSensei outperforms existing models, marking a significant advancement in manga generation by enabling text-adaptable character customization. The code, model, and dataset will be open-sourced to the community",
    "checked": true,
    "id": "ecbc2f63c0216c82fa5a22b2f33454b8f25e5a35",
    "semantic_title": "diffsensei: bridging multi-modal llms and diffusion models for customized manga generation",
    "citation_count": 5,
    "authors": [
      "Jianzong Wu",
      "Chao Tang",
      "Jingbo Wang",
      "Yanhong Zeng",
      "Xiangtai Li",
      "Yunhai Tong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hur_Narrating_the_Video_Boosting_Text-Video_Retrieval_via_Comprehensive_Utilization_of_CVPR_2025_paper.html": {
    "title": "Narrating the Video: Boosting Text-Video Retrieval via Comprehensive Utilization of Frame-Level Captions",
    "volume": "main",
    "abstract": "In recent text-video retrieval, the use of additional captions from vision-language models has shown promising effects on the performance. However, existing models using additional captions often have struggled to capture the rich semantics, including temporal changes, inherent in the video. In addition, incorrect information caused by generative models can lead to inaccurate retrieval. To address these issues, we propose a new framework, Narrating the Video (NarVid), which strategically leverages the comprehensive information available from frame-level captions, the narration. The proposed NarVid exploits narration in multiple ways: 1) feature enhancement through cross-modal interactions between narration and video, 2) query-aware adaptive filtering to suppress irrelevant or incorrect information, 3) dual-modal matching score by adding query-video similarity and query-narration similarity, and 4) hard-negative loss to learn discriminative features from multiple perspectives using the two similarities from different views. Experimental results demonstrate that NarVid achieves state-of-the-art performance on various benchmark datasets",
    "checked": true,
    "id": "73aeb825bffdd06a9d8580aba65b02de5a0a9b32",
    "semantic_title": "narrating the video: boosting text-video retrieval via comprehensive utilization of frame-level captions",
    "citation_count": 1,
    "authors": [
      "Chan Hur",
      "Jeong-hun Hong",
      "Dong-hun Lee",
      "Dabin Kang",
      "Semin Myeong",
      "Sang-hyo Park",
      "Hyeyoung Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_IDEA-Bench_How_Far_are_Generative_Models_from_Professional_Designing_CVPR_2025_paper.html": {
    "title": "IDEA-Bench: How Far are Generative Models from Professional Designing?",
    "volume": "main",
    "abstract": "Recent advancements in image generation models enable the creation of high-quality images and targeted modifications based on textual instructions. Some models even support multimodal complex guidance and demonstrate robust task generalization capabilities. However, they still fall short of meeting the nuanced, professional demands of designers. To bridge this gap, we introduce IDEA-Bench, a comprehensive benchmark designed to advance image generation models toward applications with robust task generalization. IDEA-Bench comprises 100 professional image generation tasks and 275 specific cases, categorized into five major types based on the current capabilities of existing models. Furthermore, we provide a representative subset of 18 tasks with enhanced evaluation criteria to facilitate more nuanced and reliable evaluations using Multimodal Large Language Models (MLLMs). By assessing models' ability to comprehend and execute novel, complex tasks, IDEA-Bench paves the way toward the development of generative models with autonomous and versatile visual generation capabilities",
    "checked": true,
    "id": "c3871abb91c8d0d97a5a0b46655a14392d2af91c",
    "semantic_title": "idea-bench: how far are generative models from professional designing?",
    "citation_count": 1,
    "authors": [
      "Chen Liang",
      "Lianghua Huang",
      "Jingwu Fang",
      "Huanzhang Dou",
      "Wei Wang",
      "Zhi-Fan Wu",
      "Yupeng Shi",
      "Junge Zhang",
      "Xin Zhao",
      "Yu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Interpretable_Image_Classification_via_Non-parametric_Part_Prototype_Learning_CVPR_2025_paper.html": {
    "title": "Interpretable Image Classification via Non-parametric Part Prototype Learning",
    "volume": "main",
    "abstract": "Classifying images with an interpretable decision-making process is a long-standing problem in computer vision. In recent years, Prototypical Part Networks has gained traction as an approach for self-explainable neural networks, due to their ability to mimic human visual reasoning by providing explanations based on prototypical object parts. However, the quality of the explanations generated by these methods leaves room for improvement, as the prototypes usually focus on repetitive and redundant concepts. Leveraging recent advances in prototype learning, we present a framework for part-based interpretable image classification that learns a set of semantically distinctive object parts for each class, and provides diverse and comprehensive explanations. The core of our method is to learn the part-prototypes in a non-parametric fashion, through clustering deep features extracted from foundation vision models that encode robust semantic information. To quantitatively evaluate the quality of explanations provided by ProtoPNets, we introduce Distinctiveness Score and Comprehensiveness Score. Through evaluation on CUB-200-2011, Stanford Cars and Stanford Dogs datasets, we show that our framework compares favourably against existing ProtoPNets while achieving better interpretability",
    "checked": true,
    "id": "4a6cd73feab653b8b878900d4546fa5abb30bd6a",
    "semantic_title": "interpretable image classification via non-parametric part prototype learning",
    "citation_count": 0,
    "authors": [
      "Zhijie Zhu",
      "Lei Fan",
      "Maurice Pagnucco",
      "Yang Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_PhD_A_ChatGPT-Prompted_Visual_Hallucination_Evaluation_Dataset_CVPR_2025_paper.html": {
    "title": "PhD: A ChatGPT-Prompted Visual Hallucination Evaluation Dataset",
    "volume": "main",
    "abstract": "Multimodal Large Language Models (MLLMs) hallucinate, resulting in an emerging topic of visual hallucination evaluation (VHE). This paper contributes a ChatGPT-Prompted visual hallucination evaluation Dataset (PhD) for objective VHE at a large scale. The essence of VHE is to ask an MLLM questions about specific images to assess its susceptibility to hallucination. Depending on what to ask (objects, attributes, sentiment, etc.) and how the questions are asked, we structure PhD along two dimensions, i.e. task and mode. Five visual recognition tasks, ranging from low-level (object / attribute recognition) to middle-level (sentiment / position recognition and counting), are considered. Besides a normal visual QA mode, which we term PhD-base, PhD also asks questions with specious context (PhD-sec) or with incorrect context (PhD-icc), or with AI-generated counter common sense images (PhD-ccs). We construct PhD by a ChatGPT-assisted semi-automated pipeline, encompassing four pivotal modules: task-specific hallucinatory item (hitem) selection, hitem-embedded question generation, specious / incorrect context generation, and counter-common-sense (CCS) image generation. With over 14k daily images, 750 CCS images and 102k VQA triplets in total, PhD reveals considerable variability in MLLMs' performance across various modes and tasks, offering valuable insights into the nature of hallucination. As such, PhD stands as a potent tool not only for VHE but may also play a significant role in the refinement of MLLMs",
    "checked": true,
    "id": "0daceeb8f42ad6ec846f80f7407500192d2046ee",
    "semantic_title": "phd: a chatgpt-prompted visual hallucination evaluation dataset",
    "citation_count": 10,
    "authors": [
      "Jiazhen Liu",
      "Yuhan Fu",
      "Ruobing Xie",
      "Runquan Xie",
      "Xingwu Sun",
      "Fengzong Lian",
      "Zhanhui Kang",
      "Xirong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Greer_CARL_A_Framework_for_Equivariant_Image_Registration_CVPR_2025_paper.html": {
    "title": "CARL: A Framework for Equivariant Image Registration",
    "volume": "main",
    "abstract": "Image registration estimates spatial correspondences between image pairs. These estimates are typically obtained via numerical optimization or regression by a deep network. A desirable property is that a correspondence estimate (e.g., the true oracle correspondence) for an image pair is maintained under deformations of the input images. Formally, the estimator should be equivariant to a desired class of image transformations. In this work, we present careful analyses of equivariance properties in the context of multi-step deep registration networks. Based on these analyses we 1) introduce the notions of [U,U] equivariance (network equivariance to the same deformations of the input images) and [W,U] equivariance (where input images can undergo different deformations); we 2) show that in a suitable multi-step registration setup it is sufficient for overall [W,U] equivariance if the first step has [W,U] equivariance and all others have [U,U] equivariance; we 3) show that common displacement-predicting networks only exhibit [U,U] equivariance to translations instead of the more powerful [W,U] equivariance; and we 4) show how to achieve multi-step [W,U] equivariance via a coordinate-attention mechanism combined with displacement-predicting networks. Our approach obtains excellent practical performance for 3D abdomen, lung, and brain medical image registration. We match or outperform state-of-the-art (SOTA) registration approaches on all the datasets with a particularly strong performance for the challening abdomen registration",
    "checked": true,
    "id": "4e98e0c61002a4b5e689cfa901afc143bdab53fd",
    "semantic_title": "carl: a framework for equivariant image registration",
    "citation_count": 0,
    "authors": [
      "Hastings Greer",
      "Lin Tian",
      "Fran√ßois-Xavier Vialard",
      "Roland Kwitt",
      "Raul San Jose Estepar",
      "Marc Niethammer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_ClimbingCap_Multi-Modal_Dataset_and_Method_for_Rock_Climbing_in_World_CVPR_2025_paper.html": {
    "title": "ClimbingCap: Multi-Modal Dataset and Method for Rock Climbing in World Coordinate",
    "volume": "main",
    "abstract": "Human Motion Recovery (HMR) research mainly focuses on ground-based motions such as running. The study on capturing climbing motion, an off-ground motion, is sparse. This is partly due to the limited availability of climbing motion datasets, especially large-scale and challenging 3D labeled datasets. To address the insufficiency of climbing motion datasets, we collect AscendMotion, a large-scale well-annotated, and challenging climbing motion dataset. It consists of 412k RGB, LiDAR frames, and IMU measurements, which includes the challenging climbing motions of 22 professional climbing coaches across 12 different rocks. Capturing the climbing motions is challenging as it requires precise recovery of not only the complex pose but also the global position of climbers. Although multiple global HMR methods have been proposed, they cannot faithfully capture climbing motions. To address the limitations of HMR methods for climbing, we propose ClimbingCap, a motion recovery method that reconstructs continuous 3D human climbing motion in a global coordinate system. One key insight is to use the RGB and the LiDAR modalities to separately reconstruct motions in camera coordinates and global coordinates and optimize them jointly. We demonstrate the quality of the AscendMotion dataset and present promising results from ClimbingCap. The AscendMotion dataset and the source code of ClimbingCap will be released publicly to the research community",
    "checked": true,
    "id": "8442504b07eb8c67600ff803c8e880df74742261",
    "semantic_title": "climbingcap: multi-modal dataset and method for rock climbing in world coordinate",
    "citation_count": 0,
    "authors": [
      "Ming Yan",
      "Xincheng Lin",
      "Yuhua Luo",
      "Shuqi Fan",
      "Yudi Dai",
      "Qixin Zhong",
      "Lincai Zhong",
      "Yuexin Ma",
      "Lan Xu",
      "Chenglu Wen",
      "Siqi Shen",
      "Cheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhuang_DAGSM_Disentangled_Avatar_Generation_with_GS-enhanced_Mesh_CVPR_2025_paper.html": {
    "title": "DAGSM: Disentangled Avatar Generation with GS-enhanced Mesh",
    "volume": "main",
    "abstract": "Text-driven avatar generation has gained significant attention owing to its convenience. However, existing methods typically model the human body with all garments as a single 3D model, limiting its usability, such as clothing replacement, and reducing user control over the generation process. To overcome the limitations above, we propose DAGSM, a novel pipeline that generates disentangled human bodies and garments from the given text prompts. Specifically, we model each part (e.g., body, upper/lower clothes) of the clothed human as one GS-enhanced mesh (GSM), which is a traditional mesh attached with 2D Gaussians to better handle complicated textures (e.g., woolen, translucent clothes) and produce realistic cloth animations. During the generation, we first create the unclothed body, followed by a sequence of individual cloth generation based on the body, where we introduce a semantic-based algorithm to achieve better human-cloth and garment-garment separation. To improve texture quality, we propose a view-consistent texture refinement module, including a cross-view attention mechanism for texture style consistency and an incident-angle-weighted denoising (IAW-DE) strategy to update the appearance. Extensive experiments have demonstrated that DAGSM generates high-quality disentangled avatars, supports clothing replacement and realistic animation, and outperforms the baselines in visual quality",
    "checked": true,
    "id": "7e7e4f090132dae0e753a8a38bdefbfeb89122e8",
    "semantic_title": "dagsm: disentangled avatar generation with gs-enhanced mesh",
    "citation_count": 1,
    "authors": [
      "Jingyu Zhuang",
      "Di Kang",
      "Linchao Bao",
      "Liang Lin",
      "Guanbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yi_Estimating_Body_and_Hand_Motion_in_an_Ego-sensed_World_CVPR_2025_paper.html": {
    "title": "Estimating Body and Hand Motion in an Ego-sensed World",
    "volume": "main",
    "abstract": "We present EgoAllo, a system for human motion estimation from a head-mounted device. Using only egocentric SLAM poses and images, EgoAllo guides sampling from a conditional diffusion model to estimate 3D body pose, height, and hand parameters that capture a device wearer's actions in the allocentric coordinate frame of the scene. To achieve this, our key insight is in representation: we propose spatial and temporal invariance criteria for improving model performance, from which we derive a head motion conditioning parameterization that improves estimation by up to 18%. We also show how the bodies estimated by our system can improve hand estimation: the resulting kinematic and temporal constraints can reduce world-frame errors in single-frame estimates by 40%",
    "checked": true,
    "id": "96c7b6ce9952f935530cf2b55ad47629d8016fac",
    "semantic_title": "estimating body and hand motion in an ego-sensed world",
    "citation_count": 8,
    "authors": [
      "Brent Yi",
      "Vickie Ye",
      "Maya Zheng",
      "Yunqi Li",
      "Lea M√ºller",
      "Georgios Pavlakos",
      "Yi Ma",
      "Jitendra Malik",
      "Angjoo Kanazawa"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guillaro_A_Bias-Free_Training_Paradigm_for_More_General_AI-generated_Image_Detection_CVPR_2025_paper.html": {
    "title": "A Bias-Free Training Paradigm for More General AI-generated Image Detection",
    "volume": "main",
    "abstract": "Successful forensic detectors can produce excellent results in supervised learning benchmarks but struggle to transfer to real-world applications. We believe this limitation is largely due to inadequate training data quality. While most research focuses on developing new algorithms, less attention is given to training data selection, despite evidence that performance can be strongly impacted by spurious correlations such as content, format, or resolution. A well-designed forensic detector should detect generator specific artifacts rather than reflect data biases. To this end, we propose B-Free, a bias-free training paradigm, where fake images are generated from real ones using the conditioning procedure of stable diffusion models. This ensures semantic alignment between real and fake images, allowing any differences to stem solely from the subtle artifacts introduced by AI generation. Through content-based augmentation, we show significant improvements in both generalization and robustness over state-of-the-art detectors and more calibrated results across 27 different generative models, including recent releases, like FLUX and Stable Diffusion 3.5. Our findings emphasize the importance of a careful dataset design, highlighting the need for further research on this topic. Code and data are publicly available at https://grip-unina.github.io/B-Free/",
    "checked": true,
    "id": "7bd8343e28a1c6adab29c721a08aee878fbf3ada",
    "semantic_title": "a bias-free training paradigm for more general ai-generated image detection",
    "citation_count": 6,
    "authors": [
      "Fabrizio Guillaro",
      "Giada Zingarini",
      "Ben Usman",
      "Avneesh Sud",
      "Davide Cozzolino",
      "Luisa Verdoliva"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Truong_FALCON_Fairness_Learning_via_Contrastive_Attention_Approach_to_Continual_Semantic_CVPR_2025_paper.html": {
    "title": "FALCON: Fairness Learning via Contrastive Attention Approach to Continual Semantic Scene Understanding",
    "volume": "main",
    "abstract": "Continual Learning in semantic scene segmentation aims to continually learn new unseen classes in dynamic environments while maintaining previously learned knowledge. Prior studies focused on modeling the catastrophic forgetting and background shift challenges in continual learning. However, fairness, another major challenge that causes unfair predictions leading to low performance among major and minor classes, still needs to be well addressed. In addition, prior methods have yet to model the unknown classes well, thus resulting in producing non-discriminative features among unknown classes. This work presents a novel Fairness Learning via Contrastive Attention Approach to continual learning in semantic scene understanding. In particular, we first introduce a new Fairness Contrastive Clustering loss to address the problems of catastrophic forgetting and fairness. Then, we propose an attention-based visual grammar approach to effectively model the background shift problem and unknown classes, producing better feature representations for different unknown classes. Through our experiments, our proposed approach achieves State-of-the-Art (SoTA) performance on different continual learning benchmarks, i.e., ADE20K, Cityscapes, and Pascal VOC. It promotes the fairness of the continual semantic segmentation model",
    "checked": false,
    "id": "12a7afdf90675ed21a442292317f3bb8f23e7e7e",
    "semantic_title": "falcon: fairness learning via contrastive attention approach to continual semantic scene understanding in open world",
    "citation_count": 3,
    "authors": [
      "Thanh-Dat Truong",
      "Utsav Prabhu",
      "Bhiksha Raj",
      "Jackson Cothren",
      "Khoa Luu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bahari_Certified_Human_Trajectory_Prediction_CVPR_2025_paper.html": {
    "title": "Certified Human Trajectory Prediction",
    "volume": "main",
    "abstract": "Predicting human trajectories is essential for the safe operation of autonomous vehicles, yet current data-driven models often lack robustness in case of noisy inputs such as adversarial examples or imperfect observations. Although some trajectory prediction methods have been developed to provide empirical robustness, these methods are heuristic and do not offer guaranteed robustness. In this work, we propose a certification approach tailored for trajectory prediction that provides guaranteed robustness. To this end, we address the unique challenges associated with trajectory prediction, such as unbounded outputs and multi-modality. To mitigate the inherent performance drop through certification, we propose a diffusion-based trajectory denoiser and integrate it into our method. Moreover, we introduce new certified performance metrics to reliably measure the trajectory prediction performance. Through comprehensive experiments, we demonstrate the accuracy and robustness of the certified predictors and highlight their advantages over the non-certified ones. The code is available online: https://s-attack.github.io/",
    "checked": true,
    "id": "89c5394cacb127b15a38bca91e1d1b9a9dec9012",
    "semantic_title": "certified human trajectory prediction",
    "citation_count": 2,
    "authors": [
      "Mohammadhossein Bahari",
      "Saeed Saadatnejad",
      "Amirhossein Askari Farsangi",
      "Seyed-Mohsen Moosavi-Dezfooli",
      "Alexandre Alahi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Aghzal_Evaluating_Vision-Language_Models_as_Evaluators_in_Path_Planning_CVPR_2025_paper.html": {
    "title": "Evaluating Vision-Language Models as Evaluators in Path Planning",
    "volume": "main",
    "abstract": "Despite their promise to perform complex reasoning, large language models (LLMs) have been shown to have limited effectiveness in end-to-end planning. This has inspired an intriguing question: if these models cannot plan well, can they still contribute to the planning framework as a helpful plan evaluator? In this work, we generalize this question to consider LLMs augmented with visual understanding, i.e., Vision-Language Models (VLMs). We introduce **PathEval**, a novel benchmark evaluating VLMs as plan evaluators in complex path-planning scenarios. Succeeding in the benchmark requires a VLM to be able to abstract traits of optimal paths from the scenario description, demonstrate precise low-level perception on each path, and integrate this information to decide the better path. Our analysis of state-of-the-art VLMs reveals that these models face significant challenges on the benchmark. We observe that the VLMs can precisely abstract given scenarios to identify the desired traits and exhibit mixed performance in integrating the provided information. Yet, their vision component presents a critical bottleneck, with models struggling to perceive low-level details about a path. Our experimental results show that this issue cannot be trivially addressed via end-to-end fine-tuning; rather, task-specific discriminative adaptation of these vision encoders is needed for these VLMs to become effective path evaluators",
    "checked": true,
    "id": "2e62d76e381dd81ed2544e6a408a5ff15b1002b3",
    "semantic_title": "evaluating vision-language models as evaluators in path planning",
    "citation_count": 1,
    "authors": [
      "Mohamed Aghzal",
      "Xiang Yue",
      "Erion Plaku",
      "Ziyu Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dai_Free_on_the_Fly_Enhancing_Flexibility_in_Test-Time_Adaptation_with_CVPR_2025_paper.html": {
    "title": "Free on the Fly: Enhancing Flexibility in Test-Time Adaptation with Online EM",
    "volume": "main",
    "abstract": "Vision-Language Models (VLMs) have become prominent in open-world image recognition for their strong generalization abilities. Yet, their effectiveness in practical applications is compromised by domain shifts and distributional changes, especially when test data distributions diverge from training data. Therefore, the paradigm of test-time adaptation (TTA) has emerged, enabling the use of online off-the-shelf data at test time, supporting independent sample predictions, and eliminating reliance on test annotations. Traditional TTA methods, however, often rely on costly training or optimization processes, or make unrealistic assumptions about accessing or storing historical training and test data.Instead, this study proposes FreeTTA, a training-free and universally available method that makes no assumptions, to enhance the flexibility of TTA. More importantly, FreeTTA is the first to explicitly model the test data distribution, enabling the use of intrinsic relationships among test samples to enhance predictions of individual samples without simultaneous access--a direction not previously explored. FreeTTA achieves these advantages by introducing an online EM algorithm that utilizes zero-shot predictions from VLMs as priors to iteratively compute the posterior probabilities of each online test sample and update parameters. Experiments demonstrate that FreeTTA achieves stable and significant improvements compared to state-of-the-art methods across 15 datasets in both cross-domain and out-of-distribution settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyuan Dai",
      "Sibei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Transformers_without_Normalization_CVPR_2025_paper.html": {
    "title": "Transformers without Normalization",
    "volume": "main",
    "abstract": "Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation DyT(x)=tanh(ax), as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, S-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks",
    "checked": true,
    "id": "1e178f0c5bb9709ae5c7bdb60ecd76f00b0fcd86",
    "semantic_title": "transformers without normalization",
    "citation_count": 18,
    "authors": [
      "Jiachen Zhu",
      "Xinlei Chen",
      "Kaiming He",
      "Yann LeCun",
      "Zhuang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_SGC-Net_Stratified_Granular_Comparison_Network_for_Open-Vocabulary_HOI_Detection_CVPR_2025_paper.html": {
    "title": "SGC-Net: Stratified Granular Comparison Network for Open-Vocabulary HOI Detection",
    "volume": "main",
    "abstract": "Recent open-vocabulary human-object interaction (OV-HOI) detection methods primarily rely on large language model (LLM) for generating auxiliary descriptions and leverage knowledge distilled from CLIP to detect unseen interaction categories. Despite their effectiveness, these methods face two challenges: (1) feature granularity deficiency, due to reliance on last layer visual features for text alignment, leading to the neglect of crucial object-level details from intermediate layers; (2) semantic similarity confusion, resulting from CLIP's inherent biases toward certain classes, while LLM-generated descriptions based solely on labels fail to adequately capture inter-class similarities. To address these challenges, we propose a stratified granular comparison network. First, we introduce a granularity sensing alignment module that aggregates global semantic features with local details, refining interaction representations and ensuring robust alignment between intermediate visual features and text embeddings. Second, we develop a hierarchical group comparison module that recursively compares and groups classes using LLMs, generating fine-grained and discriminative descriptions for each interaction category. Experimental results on two widely-used benchmark datasets, SWIG-HOI and HICO-DET, demonstrate that our method achieves state-of-the-art results in OV-HOI detection. Codes is available at https://github.com/Phil0212/SGC-Net",
    "checked": true,
    "id": "d2cb8e0540566eafb2db20f96be3cf8ff020d74e",
    "semantic_title": "sgc-net: stratified granular comparison network for open-vocabulary hoi detection",
    "citation_count": 0,
    "authors": [
      "Xin Lin",
      "Chong Shi",
      "Zuopeng Yang",
      "Haojin Tang",
      "Zhili Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Galaxy_Walker_Geometry-aware_VLMs_For_Galaxy-scale_Understanding_CVPR_2025_paper.html": {
    "title": "Galaxy Walker: Geometry-aware VLMs For Galaxy-scale Understanding",
    "volume": "main",
    "abstract": "Modern vision-language models (VLMs) develop patch embedding and convolution backbone within vector space, especially Euclidean ones, at the very founding. When expanding VLMs to a galaxy-scale for understanding astronomical phenomena, the integration of spherical space for planetary orbits and hyperbolic spaces for black holes raises two formidable challenges. a) The current pre-training model is confined to Euclidean space rather than a comprehensive geometric embedding. b) The predominant architecture lacks suitable backbones for anisotropic physical geometries. In this paper, we introduced Galaxy-Walker, a geometry-aware VLM, for the universe-level vision understanding tasks. We proposed the geometry prompt that generates geometry tokens by random walks across diverse spaces on a multi-scale physical graph, along with a geometry adapter that compresses and reshapes the space anisotropy in a mixture-of-experts manner. Extensive experiments demonstrate the effectiveness of our approach, with Galaxy-Walker achieving state-of-the-art performance in both galaxy property estimation (R2 scores up to 0.91) and morphology classification tasks (up to +0.17 F1 improvement in challenging features), significantly outperforming both domain-specific models and general-purpose VLMs",
    "checked": true,
    "id": "7e968919b47267df9bb238f7db3591d9933b1351",
    "semantic_title": "galaxy walker: geometry-aware vlms for galaxy-scale understanding",
    "citation_count": 0,
    "authors": [
      "Tianyu Chen",
      "Xingcheng Fu",
      "Yisen Gao",
      "Haodong Qian",
      "Yuecen Wei",
      "Kun Yan",
      "Haoyi Zhou",
      "Jianxin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_HiPART_Hierarchical_Pose_AutoRegressive_Transformer_for_Occluded_3D_Human_Pose_CVPR_2025_paper.html": {
    "title": "HiPART: Hierarchical Pose AutoRegressive Transformer for Occluded 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "Existing 2D-to-3D human pose estimation (HPE) methods struggle with the occlusion issue by enriching information like temporal and visual cues in the lifting stage. In this paper, we argue that these methods ignore the limitation of the sparse skeleton 2D input representation, which fundamentally restricts the 2D-to-3D lifting and worsens the occlusion issue. To address these, we propose a novel two-stage generative densification method, named Hierarchical Pose AutoRegressive Transformer (HiPART), to generate hierarchical 2D dense poses from the original sparse 2D pose. Specifically, we first develop a multi-scale skeleton tokenization module to quantize the highly dense 2D pose into hierarchical tokens and propose a skeleton-aware alignment to strengthen token connections. We then develop a hierarchical autoregressive modeling scheme for hierarchical 2D pose generation. With generated hierarchical poses as inputs for 2D-to-3D lifting, the proposed method shows strong robustness in occluded scenarios and achieves state-of-the-art performance on the single-frame-based 3D HPE. Moreover, it outperforms numerous multi-frame methods while reducing parameter and computational complexity and can also complement them to further enhance performance and robustness",
    "checked": true,
    "id": "451fdd5000033631fce439b4fe215eed185ed247",
    "semantic_title": "hipart: hierarchical pose autoregressive transformer for occluded 3d human pose estimation",
    "citation_count": 1,
    "authors": [
      "Hongwei Zheng",
      "Han Li",
      "Wenrui Dai",
      "Ziyang Zheng",
      "Chenglin Li",
      "Junni Zou",
      "Hongkai Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lai_SnowMaster_Comprehensive_Real-world_Image_Desnowing_via_MLLM_with_Multi-Model_Feedback_CVPR_2025_paper.html": {
    "title": "SnowMaster: Comprehensive Real-world Image Desnowing via MLLM with Multi-Model Feedback Optimization",
    "volume": "main",
    "abstract": "Snowfall presents significant challenges for visual data processing, necessitating specialized desnowing algorithms. However, existing models often fail to generalize effectively due to their heavy reliance on synthetic datasets. Furthermore, current real-world snowfall datasets are limited in scale and lack dedicated evaluation metrics designed specifically for snowfall degradation, thus hindering the effective integration of real snowy images into model training to reduce domain gaps. To address these challenges, we first introduce RealSnow10K, a large-scale, high-quality dataset consisting of over 10,000 annotated real-world snowy images. In addition, we curate a preference dataset comprising 36,000 expert-ranked image pairs, enabling the adaptation of multimodal large language models (MLLMs) to better perceive snowy image quality through our innovative Multi-Model Preference Optimization (MMPO). Finally, we propose the SnowMaster, which employs MMPO-enhanced MLLM to perform accurate snowy image evaluation and pseudo-label filtering for semi-supervised training. Experiments demonstrate that SnowMaster delivers superior desnowing performance under real-world conditions",
    "checked": true,
    "id": "954f41da6ec69f434c5be442c53d9d9122ab31e0",
    "semantic_title": "snowmaster: comprehensive real-world image desnowing via mllm with multi-model feedback optimization",
    "citation_count": 1,
    "authors": [
      "Jianyu Lai",
      "Sixiang Chen",
      "Yunlong Lin",
      "Tian Ye",
      "Yun Liu",
      "Song Fei",
      "Zhaohu Xing",
      "Hongtao Wu",
      "Weiming Wang",
      "Lei Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_From_Faces_to_Voices_Learning_Hierarchical_Representations_for_High-quality_Video-to-Speech_CVPR_2025_paper.html": {
    "title": "From Faces to Voices: Learning Hierarchical Representations for High-quality Video-to-Speech",
    "volume": "main",
    "abstract": "The objective of this study is to generate high-quality speech from silent talking face videos, a task also known as video-to-speech synthesis. A significant challenge in video-to-speech synthesis lies in the substantial modality gap between silent video and multi-faceted speech. In this paper, we propose a novel video-to-speech system that effectively bridges this modality gap, significantly enhancing the quality of synthesized speech. This is achieved by learning of hierarchical representations from video to speech. Specifically, we gradually transform silent video into acoustic feature spaces through three sequential stages -- content, timbre, and prosody modeling. In each stage, we align visual factors -- lip movements, face identity, and facial expressions -- with corresponding acoustic counterparts to ensure the seamless transformation. Additionally, to generate realistic and coherent speech from the visual representations, we employ a flow matching model that estimates direct trajectories from a simple prior distribution to the target speech distribution. Extensive experiments demonstrate that our method achieves exceptional generation quality comparable to real utterances, outperforming existing methods by a significant margin",
    "checked": true,
    "id": "525c82f446aa688bac1960d718000254ac450a68",
    "semantic_title": "from faces to voices: learning hierarchical representations for high-quality video-to-speech",
    "citation_count": 1,
    "authors": [
      "Ji-Hoon Kim",
      "Jeongsoo Choi",
      "Jaehun Kim",
      "Chaeyoung Jung",
      "Joon Son Chung"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_DFM_Differentiable_Feature_Matching_for_Anomaly_Detection_CVPR_2025_paper.html": {
    "title": "DFM: Differentiable Feature Matching for Anomaly Detection",
    "volume": "main",
    "abstract": "Feature matching methods for unsupervised anomaly detection have demonstrated impressive performance. Existing methods primarily rely on self-supervised training and handcrafted matching schemes for task adaptation. However, they can only achieve an inferior feature representation for anomaly detection because the feature extraction and matching modules are separately trained. To address these issues, we propose a Differentiable Feature Matching (DFM) framework for joint optimization of the feature extractor and the matching head. DFM transforms nearest-neighbor matching into a pooling-based module and embeds it within a Feature Matching Network (FMN). This design enables end-to-end feature extraction and feature matching module training, thus providing better feature representation for anomaly detection tasks. DFM is generic and can be incorporated into existing feature-matching methods. We implement DFM with various backbones and conduct extensive experiments across various tasks and datasets, demonstrating its effectiveness. Notably, we achieve state-of-the-art results in the continual anomaly detection task with instance-AUROC improvement of up to 3.9% and pixel-AP improvement of up to 5.5%",
    "checked": false,
    "id": "7bf8fa493e1f2f1c7dad51b9c6c746e9b4a462ba",
    "semantic_title": "representation-enhanced apt detection using contrastive learning",
    "citation_count": 1,
    "authors": [
      "Sheng Wu",
      "Yimi Wang",
      "Xudong Liu",
      "Yuguang Yang",
      "Runqi Wang",
      "Guodong Guo",
      "David Doermann",
      "Baochang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_FlashGS_Efficient_3D_Gaussian_Splatting_for_Large-scale_and_High-resolution_Rendering_CVPR_2025_paper.html": {
    "title": "FlashGS: Efficient 3D Gaussian Splatting for Large-scale and High-resolution Rendering",
    "volume": "main",
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated significant potential over traditional rendering techniques, attracting widespread attention from both industry and academia. However, real-time rendering with 3DGS remains a challenging problem, particularly in large-scale, high-resolution scenes due to the presence of numerous anisotropic Gaussian representations, and it has not been extensively explored. To address this challenge, we introduce FlashGS, an open-source CUDA library with Python bindings, featuring comprehensive algorithm design and optimizations, including redundancy elimination, adaptive scheduling, and efficient pipelining. First, we eliminate substantial redundant computations through precise Gaussian intersection tests, leveraging the intrinsic mechanism of the 3DGS rasterizer. During task partitioning, we propose an adaptive scheduling strategy that accounts for variations in Gaussian size and shape. Additionally, we design a multi-stage pipelining strategy for color computation in the rendering process, further accelerating performance. We conduct an extensive evaluation of FlashGS across a diverse range of synthetic and real-world 3D scenes, encompassing scene sizes of up to 2.7 km^2 cityscape and resolutions of up to over 4K. Our approach improves 3DGS rendering performance by an order of magnitude, achieving an average speedup of 7.2x, and rendering at a minimum of 125.9 FPS, setting a new state-of-the-art in real-time 3DGS rendering. https://github.com/InternLandMark/FlashGS",
    "checked": true,
    "id": "18e4b2a2781c13a0c79a6534f832487bb2d38257",
    "semantic_title": "flashgs: efficient 3d gaussian splatting for large-scale and high-resolution rendering",
    "citation_count": 32,
    "authors": [
      "Guofeng Feng",
      "Siyan Chen",
      "Rong Fu",
      "Zimu Liao",
      "Yi Wang",
      "Tao Liu",
      "Boni Hu",
      "Linning Xu",
      "Zhilin Pei",
      "Hengjie Li",
      "Xiuhong Li",
      "Ninghui Sun",
      "Xingcheng Zhang",
      "Bo Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_PointSR_Self-Regularized_Point_Supervision_for_Drone-View_Object_Detection_CVPR_2025_paper.html": {
    "title": "PointSR: Self-Regularized Point Supervision for Drone-View Object Detection",
    "volume": "main",
    "abstract": "Point-Supervised Object Detection (PSOD) in a discriminative style has recently gained significant attention for its impressive detection performance and cost-effectiveness. However, accurately predicting high-quality pseudo-box labels for drone-view images, which often feature densely packed small objects, remains a challenge. This difficulty arises primarily from the limitation of rigid sampling strategies, which hinder the pseudo-box optimization process. To address this, we propose PointSR, an effective and robust point-supervised object detection framework with self-regularized sampling that integrates temporal and informative constraints throughout the pseudo-box generation process. Specifically, the framework comprises three key components: Temporal-Ensembling Encoder (TE Encoder), Coarse Pseudo-box Prediction, and Pseudo-box Refinement. The TE Encoder builds an anchor prototype library by aggregating temporal information for dynamic anchor adjustment. In Coarse Pseudo-box Prediction, anchors are refined using the prototype library, and a set of informative samples is collected for subsequent refinement. During Pseudo-box Refinement, these informative negative samples are used to suppress low-confidence candidate positive samples, thereby improving the quality of the pseudo-boxes. Experimental results on benchmark datasets demonstrate that PointSR significantly outperforms state-of-the-art methods, achieving up to 2.6%~\\mathbf 7.2% higher AP_ 50 using only point supervision. Additionally, it exhibits strong robustness to perturbation in human-labeled points",
    "checked": true,
    "id": "8b8251c3b063a724c1bb46db0a0fa592a127ef2a",
    "semantic_title": "pointsr: self-regularized point supervision for drone-view object detection",
    "citation_count": 0,
    "authors": [
      "Weizhuo Li",
      "Yue Xi",
      "Wenjing Jia",
      "Zehao Zhang",
      "Fei Li",
      "Xiangzeng Liu",
      "Qiguang Miao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Exploring_Timeline_Control_for_Facial_Motion_Generation_CVPR_2025_paper.html": {
    "title": "Exploring Timeline Control for Facial Motion Generation",
    "volume": "main",
    "abstract": "This paper introduces a new control signal for facial motion generation: timeline control. Compared to audio and text signals, timelines provide more fine-grained control, such as generating specific facial motions with precise timing. Users can specify a multi-track timeline of facial actions arranged in temporal intervals, allowing precise control over the timing of each action. To model the timeline control capability, We first annotate the time intervals of facial actions in natural facial motion sequences at a frame-level granularity. This process is facilitated by Toeplitz Inverse Covariance-based Clustering to minimize human labor. Based on the annotations, we propose a diffusion-based generation model capable of generating facial motions that are natural and accurately aligned with input timelines. Our method supports text-guided motion generation by using ChatGPT to convert text into timelines. Experimental results show that our method can annotate facial action intervals with satisfactory accuracy, and produces natural facial motions accurately aligned with timelines",
    "checked": true,
    "id": "5cc306b099147d27699b10e045e5f342db9a9fb7",
    "semantic_title": "exploring timeline control for facial motion generation",
    "citation_count": 0,
    "authors": [
      "Yifeng Ma",
      "Jinwei Qi",
      "Chaonan Ji",
      "Peng Zhang",
      "Bang Zhang",
      "Zhidong Deng",
      "Liefeng Bo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_v-CLR_View-Consistent_Learning_for_Open-World_Instance_Segmentation_CVPR_2025_paper.html": {
    "title": "v-CLR: View-Consistent Learning for Open-World Instance Segmentation",
    "volume": "main",
    "abstract": "In this paper, we address the challenging problem of open-world instance segmentation. Existing works have shown that vanilla visual networks are biased toward learning appearance information, e.g. texture, to recognize objects. This implicit bias causes the model to fail in detecting novel objects with unseen textures in the open-world setting. To address this challenge, we propose a learning framework, called view-Consistent LeaRning (v-CLR), which aims to enforce the model to learn appearance-invariant representations for robust instance segmentation. In v-CLR, we first introduce additional views for each image, where the texture undergoes significant alterations while preserving the image's underlying structure. We then encourage the model to learn the appearance-invariant representation by enforcing the consistency between object features across different views, for which we obtain class-agnostic object proposals using off-the-shelf unsupervised models that possess strong object-awareness. These proposals enable cross-view object feature matching, greatly reducing the appearance dependency while enhancing the object-awareness. We thoroughly evaluate our method on public benchmarks under both cross-class and cross-dataset settings, achieving state-of-the-art performance. Project page: https://visual-ai.github.io/vclr",
    "checked": true,
    "id": "f5c170a089c4e951cb1f7172008ceaa66d2ca420",
    "semantic_title": "v-clr: view-consistent learning for open-world instance segmentation",
    "citation_count": 0,
    "authors": [
      "Chang-Bin Zhang",
      "Jinhong Ni",
      "Yujie Zhong",
      "Kai Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Chat2SVG_Vector_Graphics_Generation_with_Large_Language_Models_and_Image_CVPR_2025_paper.html": {
    "title": "Chat2SVG: Vector Graphics Generation with Large Language Models and Image Diffusion Models",
    "volume": "main",
    "abstract": "Scalable Vector Graphics (SVG) has become the de facto standard for vector graphics in digital design, offering resolution independence and precise control over individual elements. Despite their advantages, creating high-quality SVG content remains challenging, as it demands technical expertise with professional editing software and a considerable time investment to craft complex shapes. Recent text-to-SVG generation methods aim to make vector graphics creation more accessible, but they still encounter limitations in shape regularity, generalization ability, and expressiveness. To address these challenges, we introduce Chat2SVG, a hybrid framework that combines the strengths of Large Language Models (LLMs) and image diffusion models for text-to-SVG generation. Our approach first uses an LLM to generate semantically meaningful SVG templates from basic geometric primitives. Guided by image diffusion models, a dual-stage optimization pipeline refines paths in latent space and adjusts point coordinates to enhance geometric complexity. Extensive experiments show that Chat2SVG outperforms existing methods in visual fidelity, path regularity, and semantic alignment. Additionally, our system enables intuitive editing through natural language instructions, making professional vector graphics creation accessible to all users. Our code is available at https://chat2svg.github.io/",
    "checked": true,
    "id": "1c774cb5348819f978c95627b3c4c583e4e6d4d2",
    "semantic_title": "chat2svg: vector graphics generation with large language models and image diffusion models",
    "citation_count": 4,
    "authors": [
      "Ronghuan Wu",
      "Wanchao Su",
      "Jing Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_GAF_Gaussian_Avatar_Reconstruction_from_Monocular_Videos_via_Multi-view_Diffusion_CVPR_2025_paper.html": {
    "title": "GAF: Gaussian Avatar Reconstruction from Monocular Videos via Multi-view Diffusion",
    "volume": "main",
    "abstract": "We propose a novel approach for reconstructing animatable 3D Gaussian avatars from monocular videos captured by commodity devices like smartphones. Photorealistic 3D head avatar reconstruction from such recordings is challenging due to limited observations, which leaves unobserved regions under-constrained and can lead to artifacts in novel views. To address this problem, we introduce a multi-view head diffusion model, leveraging its priors to fill in missing regions and ensure view consistency in Gaussian splatting renderings. To enable precise viewpoint control, we use normal maps rendered from FLAME-based head reconstruction, which provides pixel-aligned inductive biases. We also condition the diffusion model on VAE features extracted from the input image to preserve facial identity and appearance details. For Gaussian avatar reconstruction, we distill multi-view diffusion priors by using iteratively denoised images as pseudo-ground truths, effectively mitigating over-saturation issues. To further improve photorealism, we apply latent upsampling priors to refine the denoised latent before decoding it into an image. We evaluate our method on the NeRSemble dataset, showing that GAF outperforms previous state-of-the-art methods in novel view synthesis. Furthermore, we demonstrate higher-fidelity avatar reconstructions from monocular videos captured on commodity devices",
    "checked": true,
    "id": "2d28839ed74f2c576826b1b72c1cd5c477b3c63f",
    "semantic_title": "gaf: gaussian avatar reconstruction from monocular videos via multi-view diffusion",
    "citation_count": 5,
    "authors": [
      "Jiapeng Tang",
      "Davide Davoli",
      "Tobias Kirschstein",
      "Liam Schoneveld",
      "Matthias Nie√üner"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Reloc3r_Large-Scale_Training_of_Relative_Camera_Pose_Regression_for_Generalizable_CVPR_2025_paper.html": {
    "title": "Reloc3r: Large-Scale Training of Relative Camera Pose Regression for Generalizable, Fast, and Accurate Visual Localization",
    "volume": "main",
    "abstract": "Visual localization aims to determine the camera pose of a query image relative to a database of posed images. In recent years, deep neural networks that directly regress camera poses have gained popularity due to their fast inference capabilities. However, existing methods struggle to either generalize well to new scenes or provide accurate camera pose estimates. To address these issues, we present Reloc3r, a simple yet effective visual localization framework. It consists of an elegantly designed relative pose regression network, and a minimalist motion averaging module for absolute pose estimation. Trained on approximately eight million posed image pairs, Reloc3r achieves surprisingly good performance and generalization ability. We conduct extensive experiments on six public datasets, consistently demonstrating the effectiveness and efficiency of the proposed method. It provides high-quality camera pose estimates in real time and generalizes to novel scenes. Code: https://github.com/ffrivera0/reloc3r",
    "checked": true,
    "id": "1929d133e87ba19d0e071d32d4112855d82a49b3",
    "semantic_title": "reloc3r: large-scale training of relative camera pose regression for generalizable, fast, and accurate visual localization",
    "citation_count": 6,
    "authors": [
      "Siyan Dong",
      "Shuzhe Wang",
      "Shaohui Liu",
      "Lulu Cai",
      "Qingnan Fan",
      "Juho Kannala",
      "Yanchao Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_AI-Face_A_Million-Scale_Demographically_Annotated_AI-Generated_Face_Dataset_and_Fairness_CVPR_2025_paper.html": {
    "title": "AI-Face: A Million-Scale Demographically Annotated AI-Generated Face Dataset and Fairness Benchmark",
    "volume": "main",
    "abstract": "AI-generated faces have enriched human life, such as entertainment, education, and art. However, they also pose misuse risks. Therefore, detecting AI-generated faces becomes crucial, yet current detectors show biased performance across different demographic groups. Mitigating biases can be done by designing algorithmic fairness methods, which usually require demographically annotated face datasets for model training. However, no existing dataset encompasses both demographic attributes and diverse generative methods simultaneously, which hinders the development of fair detectors for AI-generated faces. In this work, we introduce the AI-Face dataset, the first million-scale demographically annotated AI-generated face image dataset, including real faces, faces from deepfake videos, and faces generated by Generative Adversarial Networks and Diffusion Models. Based on this dataset, we conduct the first comprehensive fairness benchmark to assess various AI face detectors and provide valuable insights and findings to promote the future fair design of AI face detectors. Our AI-Face dataset and benchmark code are publicly available at https://github.com/Purdue-M2/AI-Face-FairnessBench",
    "checked": true,
    "id": "76e623e687f8a2f2b52ef5e8dcdbcce120755a91",
    "semantic_title": "ai-face: a million-scale demographically annotated ai-generated face dataset and fairness benchmark",
    "citation_count": 12,
    "authors": [
      "Li Lin",
      "Santosh Santosh",
      "Mingyang Wu",
      "Xin Wang",
      "Shu Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bu_Inference-Scale_Complexity_in_ANN-SNN_Conversion_for_High-Performance_and_Low-Power_Applications_CVPR_2025_paper.html": {
    "title": "Inference-Scale Complexity in ANN-SNN Conversion for High-Performance and Low-Power Applications",
    "volume": "main",
    "abstract": "Spiking Neural Networks (SNNs) have emerged as a promising substitute for Artificial Neural Networks (ANNs) due to their advantages of fast inference and low power consumption. However, the lack of efficient training algorithms has hindered their widespread adoption. Even efficient ANN-SNN conversion methods necessitate quantized training of ANNs to enhance the effectiveness of the conversion, incurring additional training costs. To address these challenges, we propose an efficient ANN-SNN conversion framework with only inference scale complexity. The conversion framework includes a local threshold balancing algorithm, which enables efficient calculation of the optimal thresholds and fine-grained adjustment of the threshold value by channel-wise scaling. We also introduce an effective delayed evaluation strategy to mitigate the influence of the spike propagation delays. We demonstrate the scalability of our framework in typical computer vision tasks: image classification, semantic segmentation, object detection, and video classification. Our algorithm outperforms existing methods, highlighting its practical applicability and efficiency. Moreover, we have evaluated the energy consumption of the converted SNNs, demonstrating their superior low-power advantage compared to conventional ANNs. This approach simplifies the deployment of SNNs by leveraging open-source pre-trained ANN models, enabling fast, low-power inference with negligible performance reduction",
    "checked": true,
    "id": "5f61edb98e2dda8e611b7ec15265df4c4103bd32",
    "semantic_title": "inference-scale complexity in ann-snn conversion for high-performance and low-power applications",
    "citation_count": 0,
    "authors": [
      "Tong Bu",
      "Maohua Li",
      "Zhaofei Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Janus_Decoupling_Visual_Encoding_for_Unified_Multimodal_Understanding_and_Generation_CVPR_2025_paper.html": {
    "title": "Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation",
    "volume": "main",
    "abstract": "We introduce Janus, an autoregressive framework that unifies multimodal understanding and generation. Prior research often relies on a single visual encoder for both tasks, such as Chameleon. However, due to the differing levels of information granularity required by multimodal understanding and generation, this approach can lead to suboptimal performance, particularly in multimodal understanding. To address this issue, we decouple visual encoding into separate pathways, while still leveraging a single, unified transformer architecture for processing. The decoupling not only alleviates the conflict between the visual encoder's roles in understanding and generation, but also enhances the framework's flexibility. For instance, both the multimodal understanding and generation components can independently select their most suitable encoding methods. Experiments show that Janus surpasses previous unified model and matches or exceeds the performance of task-specific models. The simplicity, high flexibility, and effectiveness of Janus make it a strong candidate for next-generation unified multimodal models",
    "checked": true,
    "id": "b438a496045feccd1c028f1ce034062636a1ee04",
    "semantic_title": "janus: decoupling visual encoding for unified multimodal understanding and generation",
    "citation_count": 165,
    "authors": [
      "Chengyue Wu",
      "Xiaokang Chen",
      "Zhiyu Wu",
      "Yiyang Ma",
      "Xingchao Liu",
      "Zizheng Pan",
      "Wen Liu",
      "Zhenda Xie",
      "Xingkai Yu",
      "Chong Ruan",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Choi_MVDoppler-Pose_Multi-Modal_Multi-View_mmWave_Sensing_for_Long-Distance_Self-Occluded_Human_Walking_CVPR_2025_paper.html": {
    "title": "MVDoppler-Pose: Multi-Modal Multi-View mmWave Sensing for Long-Distance Self-Occluded Human Walking Pose Estimation",
    "volume": "main",
    "abstract": "One of the main challenges in reliable camera-based 3D pose estimation for walking subjects is to deal with self-occlusions, especially in the case of using low-resolution cameras or at longer distance scenarios. In recent years, millimeter-wave (mmWave) radar has emerged as a promising alternative, offering inherent resilience to the effect of occlusions and distance variations. However, mmWave-based human walking pose estimation (HWPE) is still in the nascent development stages, primarily due to its unique set of practical challenges including the quality of the observed radar signal dependent on the subject's motion direction. This paper introduces the first comprehensive study comparing mmWave radar to camera systems for HWPE, highlighting its utility for distance-agnostic and occlusion-resilient pose estimation. Building upon mmWave's unique advantages, we address its intrinsic directionality issue through a new approach--the synergetic integration of multi-modal, multi-view mmWave signals, achieving robust HWPE against variations both in distance and walking direction. Extensive experiments on a newly curated dataset not only demonstrate the superior potential of mmWave technology over traditional camera-based HWPE systems, but also validate the effectiveness of our approach in overcoming the core limitations of mmWave HWPE",
    "checked": true,
    "id": "1e5262cb19a21b4568f534d345c08b094589e857",
    "semantic_title": "mvdoppler-pose: multi-modal multi-view mmwave sensing for long-distance self-occluded human walking pose estimation",
    "citation_count": 0,
    "authors": [
      "Jaeho Choi",
      "Soheil Hor",
      "Shubo Yang",
      "Amin Arbabian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_TopNet_Transformer-Efficient_Occupancy_Prediction_Network_for_Octree-Structured_Point_Cloud_Geometry_CVPR_2025_paper.html": {
    "title": "TopNet: Transformer-Efficient Occupancy Prediction Network for Octree-Structured Point Cloud Geometry Compression",
    "volume": "main",
    "abstract": "Efficient Point Cloud Geometry Compression (PCGC) with a lower bits per point (BPP) and higher peak signal-to-noise ratio (PSNR) is essential for the transportation of large-scale 3D data. Although octree-based entropy models can reduce BPP without introducing geometry distortion, existing CNN-based models struggle with limited receptive fields to capture long-range dependencies, while Transformer-built architectures always neglect fine-grained details due to their reliance on global self-attention. In this paper, we propose a Transformer-efficient occupancy prediction Network, termed TopNet, to overcome these challenges by developing several novel components: Locally-enhanced Context Encoding (LeCE) for enhancing the translation-invariance of the octree nodes, Adaptive-Length Sliding Window Attention (AL-SWA) for capturing both global and local dependencies while adaptively adjusting attention weights based on the input window length, Spatial-Gated-enhanced Channel Mixer (SG-CM) for efficient feature aggregation from ancestors and siblings, and Latent-guided Node Occupancy Predictor (LNOP) for improving prediction accuracy of spatially adjacent octree nodes. Comprehensive experiments across both indoor and outdoor point cloud datasets demonstrate that our TopNet achieves state-of-the-art performance with fewer parameters, further advancing the reduction-efficiency boundaries of PCGC. The code is available at https://github.com/xinjiewang1995/TopNet",
    "checked": true,
    "id": "d8ed02d4c94336a466b0d2ec6f52d2f4c6360ebc",
    "semantic_title": "topnet: transformer-efficient occupancy prediction network for octree-structured point cloud geometry compression",
    "citation_count": 0,
    "authors": [
      "Xinjie Wang",
      "Yifan Zhang",
      "Ting Liu",
      "Xinpu Liu",
      "Ke Xu",
      "Jianwei Wan",
      "Yulan Guo",
      "Hanyun Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Song_MagicArticulate_Make_Your_3D_Models_Articulation-Ready_CVPR_2025_paper.html": {
    "title": "MagicArticulate: Make Your 3D Models Articulation-Ready",
    "volume": "main",
    "abstract": "With the explosive growth of 3D content creation, there is an increasing demand for automatically converting static 3D models into articulation-ready versions that support realistic animation. Traditional approaches rely heavily on manual annotation, which is both time-consuming and labor-intensive. Moreover, the lack of large-scale benchmarks has hindered the development of learning-based solutions. In this work, we present MagicArticulate, an effective framework that automatically transforms static 3D models into articulation-ready assets. Our key contributions are threefold. First, we introduce Articulation-XL, a large-scale benchmark containing over 33k 3D models with high-quality articulation annotations, carefully curated from Objaverse-XL. Second, we propose a novel skeleton generation method that formulates the task as a sequence modeling problem, leveraging an auto-regressive transformer to naturally handle varying numbers of bones or joints within skeletons and their inherent dependencies across different 3D models. Third, we predict skinning weights using a functional diffusion process that incorporates volumetric geodesic distance priors between vertices and joints. Extensive experiments demonstrate that MagicArticulate significantly outperforms existing methods across diverse object categories, achieving high-quality articulation that enables realistic animation. Project page: https://chaoyuesong.github.io/MagicArticulate",
    "checked": true,
    "id": "a46fcfaec5197c9941c0e99805742fcd2407ef97",
    "semantic_title": "magicarticulate: make your 3d models articulation-ready",
    "citation_count": 2,
    "authors": [
      "Chaoyue Song",
      "Jianfeng Zhang",
      "Xiu Li",
      "Fan Yang",
      "Yiwen Chen",
      "Zhongcong Xu",
      "Jun Hao Liew",
      "Xiaoyang Guo",
      "Fayao Liu",
      "Jiashi Feng",
      "Guosheng Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Gain_from_Neighbors_Boosting_Model_Robustness_in_the_Wild_via_CVPR_2025_paper.html": {
    "title": "Gain from Neighbors: Boosting Model Robustness in the Wild via Adversarial Perturbations Toward Neighboring Classes",
    "volume": "main",
    "abstract": "Recent approaches, such as data augmentation, adversarial training, and transfer learning, have shown potential in addressing the issue of performance degradation caused by distributional shifts. However, they typically demand careful design in terms of data or models and lack awareness of the impact of distributional shifts. In this paper, we observe that classification errors arising from distribution shifts tend to cluster near the true values, suggesting that misclassifications commonly occur in semantically similar, neighboring categories. Furthermore, robust advanced vision foundation models maintain larger inter-class distances while preserving semantic consistency, making them less vulnerable to such shifts. Building on these findings, we propose a new method called GFN (Gain From Neighbors), which uses gradient priors from neighboring classes to perturb input images and incorporates an inter-class distance-weighted loss to improve class separation. This approach encourages the model to learn more resilient features from data prone to errors, enhancing its robustness against shifts in diverse settings. In extensive experiments across various model architectures and benchmark datasets, GFN consistently demonstrated superior performance. For instance, compared to the current state-of-the-art TAPADL method, our approach achieved a higher corruption robustness of 41.4% on ImageNet-C (+2.3%), without requiring additional parameters and using only minimal data",
    "checked": true,
    "id": "40a5ece9835956a16e662fa6f8d148e050929e17",
    "semantic_title": "gain from neighbors: boosting model robustness in the wild via adversarial perturbations toward neighboring classes",
    "citation_count": 0,
    "authors": [
      "Zhou Yang",
      "Mingtao Feng",
      "Tao Huang",
      "Fangfang Wu",
      "Weisheng Dong",
      "Xin Li",
      "Guangming Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_Enhancing_Video-LLM_Reasoning_via_Agent-of-Thoughts_Distillation_CVPR_2025_paper.html": {
    "title": "Enhancing Video-LLM Reasoning via Agent-of-Thoughts Distillation",
    "volume": "main",
    "abstract": "This paper tackles the problem of video question answering (VideoQA), a task that often requires multi-step reasoning and a profound understanding of spatial-temporal dynamics. While large video-language models perform well on benchmarks, they often lack explainability and spatial-temporal grounding. In this paper, we propose **A**gent-**o**f-**T**houghts **D**istillation (**AoTD**), a method that enhances models by incorporating automatically generated Chain-of-Thoughts (CoTs) into the instruction-tuning process. Specifically, we leverage an agent-based system to decompose complex questions into sub-tasks, and address them with specialized vision models, the intermediate results are then treated as reasoning chains. We also introduce a verification mechanism using a large language model (LLM) to ensure the reliability of generated CoTs. Extensive experiments demonstrate that AoTD improves the performance on multiple-choice and open-ended benchmarks",
    "checked": true,
    "id": "6be9716bb42f7eb61e10db3fce3da58ea042b263",
    "semantic_title": "enhancing video-llm reasoning via agent-of-thoughts distillation",
    "citation_count": 1,
    "authors": [
      "Yudi Shi",
      "Shangzhe Di",
      "Qirui Chen",
      "Weidi Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_De2Gaze_Deformable_and_Decoupled_Representation_Learning_for_3D_Gaze_Estimation_CVPR_2025_paper.html": {
    "title": "De^2Gaze: Deformable and Decoupled Representation Learning for 3D Gaze Estimation",
    "volume": "main",
    "abstract": "3D Gaze estimation is a challenging task due to two main issues. First, existing methods focus on analyzing dense features (e.g., large pixel regions), which are sensitive to local noise (e.g., light spots, blurs) and result in increased computational complexity. Second, an eyeball model can correspond multiple gaze directions, and the entangled representation between gazes and models increases the learning difficulty. To address these issues, we propose De\\textsuperscript 2Gaze , a lightweight and accurate model-aware 3D gaze estimation method. In De\\textsuperscript 2 Gaze, we introduce two key innovations for deformable and decoupled representation learning. Specifically, first, we propose a deformable sparse attention mechanism that can adapt sparse sampling points to attention areas to avoid local noise influences. Second, we propose a spatial decoupling network with a dual-branch decoding architecture to disentangle invariant (e.g., eyeball radius, position) and variable (e.g., gaze, pupil, iris) features from the latent space. Compared to existing methods, De\\textsuperscript 2 Gaze requires fewer sparse features, and achieves faster convergence speed, lower computational complexity, and higher accuracy in 3D gaze estimation.Qualitative and quantitative experiments demonstrate that De\\textsuperscript 2 Gaze achieves state-of-the-art accuracy and high-quality semantic segmentation for 3D gaze estimation on the TEyeD dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunfeng Xiao",
      "Xiaowei Bai",
      "Baojun Chen",
      "Hao Su",
      "Hao He",
      "Liang Xie",
      "Erwei Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_ReCapture_Generative_Video_Camera_Controls_for_User-Provided_Videos_using_Masked_CVPR_2025_paper.html": {
    "title": "ReCapture: Generative Video Camera Controls for User-Provided Videos using Masked Video Fine-Tuning",
    "volume": "main",
    "abstract": "Recently, breakthroughs in video modeling have allowed for controllable camera trajectories in generated videos. However, these methods cannot be directly applied to user-provided videos that are not generated by a video model. In this paper, we present ReCapture, a method for generating new videos with novel camera trajectories from a single user-provided video. Our method allows us to re-generate the reference video, with all its existing scene motion, from vastly different angles and with cinematic camera motion. Notably, using our method we can also plausibly hallucinate parts of the scene that were not observable in the reference video. Our method works by (1) generating a noisy anchor video with a new camera trajectory using multiview diffusion models or depth-based point cloud rendering and then (2) regenerating the anchor video into a clean and temporally consistent reangled video using our proposed masked video fine-tuning technique",
    "checked": true,
    "id": "b742dd8f4000acba68e9628affcd2426d4ec94a3",
    "semantic_title": "recapture: generative video camera controls for user-provided videos using masked video fine-tuning",
    "citation_count": 21,
    "authors": [
      "David Junhao Zhang",
      "Roni Paiss",
      "Shiran Zada",
      "Nikhil Karnad",
      "David E. Jacobs",
      "Yael Pritch",
      "Inbar Mosseri",
      "Mike Zheng Shou",
      "Neal Wadhwa",
      "Nataniel Ruiz"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_M3-VOS_Multi-Phase_Multi-Transition_and_Multi-Scenery_Video_Object_Segmentation_CVPR_2025_paper.html": {
    "title": "M^3-VOS: Multi-Phase, Multi-Transition, and Multi-Scenery Video Object Segmentation",
    "volume": "main",
    "abstract": "Intelligent robots need to interact with diverse objects across various environments. The appearance and state of objects frequently undergo complex transformations depending on the object properties, e.g., phase transitions. However, in the vision community, segmenting dynamic objects with phase transitions is overlooked. In light of this, we introduce the concept of phase in segmentation, which categorizes real-world objects based on their visual characteristics and potential morphological and appearance changes. Then, we present a new benchmark, Multi-Phase, Multi-Transition, and Multi-Scenery Video Object Segmentation (M^3-VOS), to verify the ability of models to understand object phases, which consists of 471 high-resolution videos spanning over 10 distinct everyday scenarios. It provides dense instance mask annotations that capture both object phases and their transitions. We evaluate state-of-the-art methods on M^3-VOS, yielding several key insights. Notably, current appearance-based approaches show significant room for improvement when handling objects with phase transitions. The inherent changes in disorder suggest that the predictive performance of the forward entropy-increasing process can be improved through a reverse entropy-reducing process. These findings lead us to propose ReVOS, a new plug-and-play model that improves its performance by reversal refinement. Our data and code will be publicly available at https://zixuan-chen.github.io/M-cube-VOS.github.io/",
    "checked": false,
    "id": "9f3829a7f1bc5df28595e062cbf44c2e3ed6dba2",
    "semantic_title": "m3-vos: multi-phase, multi-transition, and multi-scenery video object segmentation",
    "citation_count": 0,
    "authors": [
      "Zixuan Chen",
      "Jiaxin Li",
      "Junxuan Liang",
      "Liming Tan",
      "Yejie Guo",
      "Cewu Lu",
      "Yong-Lu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Self-Expansion_of_Pre-trained_Models_with_Mixture_of_Adapters_for_Continual_CVPR_2025_paper.html": {
    "title": "Self-Expansion of Pre-trained Models with Mixture of Adapters for Continual Learning",
    "volume": "main",
    "abstract": "Continual learning (CL) aims to continually accumulate knowledge from a non-stationary data stream without catastrophic forgetting of learned knowledge, requiring a balance between stability and adaptability. Relying on the generalizable representation in pre-trained models (PTMs), PTM-based CL methods perform effective continual adaptation on downstream tasks by adding learnable adapters or prompts upon the frozen PTMs. However, many existing PTM-based CL methods use restricted adaptation on a fixed set of these modules to avoid forgetting, suffering from limited CL ability. Periodically adding task-specific modules results in linear model growth rate and impaired knowledge reuse. We propose Self-Expansion of pre-trained models with Modularized Adaptation (SEMA), a novel approach to enhance the control of stability-plasticity balance in PTM-based CL. SEMA automatically decides to reuse or add adapter modules on demand in CL, depending on whether significant distribution shift that cannot be handled is detected at different representation levels. We design modular adapter consisting of a functional adapter and a representation descriptor. The representation descriptors are trained as a distribution shift indicator and used to trigger self-expansion signals. For better composing the adapters, an expandable weighting router is learned jointly for mixture of adapter outputs. SEMA enables better knowledge reuse and sub-linear expansion rate. Extensive experiments demonstrate the effectiveness of the proposed self-expansion method, achieving state-of-the-art performance compared to PTM-based CL methods without memory rehearsal. Code is available at https://github.com/huiyiwang01/SEMA-CL",
    "checked": true,
    "id": "5fc2916a10545c79cf3fc25c17af6f8c4563639f",
    "semantic_title": "self-expansion of pre-trained models with mixture of adapters for continual learning",
    "citation_count": 11,
    "authors": [
      "Huiyi Wang",
      "Haodong Lu",
      "Lina Yao",
      "Dong Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kong_Dual_Prompting_Image_Restoration_with_Diffusion_Transformers_CVPR_2025_paper.html": {
    "title": "Dual Prompting Image Restoration with Diffusion Transformers",
    "volume": "main",
    "abstract": "Recent state-of-the-art image restoration methods mostly adopt latent diffusion models with U-Net backbones, yet still facing challenges in achieving high-quality restoration due to their limited capabilities. Diffusion transformers (DiTs), like SD3, are emerging as a promising alternative because of their better quality with scalability. However, previous conditional control methods for U-Net-based diffusion models, such as ControlNet, are not well-suited for DiTs. In this paper, we introduce DPIR (Dual Recent state-of-the-art image restoration methods mostly adopt latent diffusion models with U-Net backbones, yet still facing challenges in achieving high-quality restoration due to their limited capabilities. Diffusion transformers (DiTs), like SD3, are emerging as a promising alternative because of their better quality with scalability. In this paper, we introduce DPIR (Dual Prompting Image Restoration), a novel image restoration method that effectivly extracts conditional information of low-quality images from multiple perspectives. Specifically, DPIR consits of two branches: a low-quality image conditioning branch and a dual prompting control branch. The first branch utilizes a lightweight module to incorporate image priors into the DiT with high efficiency. More importantly, we believe that in image restoration, textual description alone cannot fully capture its rich visual characteristics. Therefore, a dual prompting module is designed to provide DiT with additional visual cues, capturing both global context and local appearance. The extracted global-local visual prompts as extra conditional control, alongside textual prompts to form dual prompts, greatly enhance the quality of the restoration. Extensive experimental results demonstrate that DPIR delivers superior image restoration performance",
    "checked": true,
    "id": "7d037f6bdb1204e2664683529ded24c5938a59aa",
    "semantic_title": "dual prompting image restoration with diffusion transformers",
    "citation_count": 1,
    "authors": [
      "Dehong Kong",
      "Fan Li",
      "Zhixin Wang",
      "Jiaqi Xu",
      "Renjing Pei",
      "Wenbo Li",
      "WenQi Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Brain-Inspired_Spiking_Neural_Networks_for_Energy-Efficient_Object_Detection_CVPR_2025_paper.html": {
    "title": "Brain-Inspired Spiking Neural Networks for Energy-Efficient Object Detection",
    "volume": "main",
    "abstract": "Brain-inspired spiking neural networks (SNNs) have the capability of energy-efficient processing of temporal information. However, leveraging the rich dynamic characteristics of SNNs and prior works in artificial neural networks (ANNs) to construct an effective object detection model for visual tasks remains an open question for further exploration. To develop a directly-trained , low energy consumption and high-performance multi-scale SNN model, we propose a novel interpretable object detection framework Multi-scale Spiking Detector (MSD). Initially, we propose a spiking convolutional neuron as a core component of the Optic Nerve Nucleus Block (ONNB), designed to significantly enhance the deep feature extraction capabilities of SNNs. ONNB enables direct training with improved energy efficiency, demonstrating superior performance compared to state-of-the-art ANN-to-SNN conversion and SNN techniques. In addition, we propose a Multi-scale Spiking Detection Framework to emulate the biological response and comprehension of stimuli from different objects. Wherein, spiking multi-scale fusion and the spiking detector are employed to integrate features across different depths and to detect response outcomes, respectively. Our method outperforms state-of-the-art ANN detectors, with only 7.8 M parameters and 6.43 mJ energy consumption. MSD obtains the mean average precision (mAP) of 62.0% and 66.3% on COCO and Gen1 datasets, respectively",
    "checked": false,
    "id": "dfeb8944b143b5d2e99be2efefde8045c51ef3a9",
    "semantic_title": "an fpga accelerator design of spiking neural network for energy-efficient object detection",
    "citation_count": 2,
    "authors": [
      "Ziqi Li",
      "Tao Gao",
      "Yisheng An",
      "Ting Chen",
      "Jing Zhang",
      "Yuanbo Wen",
      "Mengkun Liu",
      "Qianxi Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Medusa_A_Multi-Scale_High-order_Contrastive_Dual-Diffusion_Approach_for_Multi-View_Clustering_CVPR_2025_paper.html": {
    "title": "Medusa: A Multi-Scale High-order Contrastive Dual-Diffusion Approach for Multi-View Clustering",
    "volume": "main",
    "abstract": "Deep multi-view clustering methods utilize information from multiple views to achieve enhanced clustering results and have gained increasing popularity in recent years. Most existing methods typically focus on either inter-view or intra-view relationships, aiming to align information across views or analyze structural patterns within individual views. However, they often incorporate inter-view complementary information in a simplistic manner, while overlooking the complex, high-order relationships within multi-view data and the interactions among samples, resulting in an incomplete utilization of the rich information available. Instead, we propose a multi-scale approach that exploits all of the available information. We first introduce a dual graph diffusion module guided by a consensus graph. This module leverages inter-view information to enhance the representation of both nodes and edges within each view. Secondly, we propose a novel contrastive loss function based on hypergraphs to more effectively model and leverage complex intra-view data relationships. Finally, we propose to adaptively learn fusion weights at the sample level, which enables a more flexible and dynamic aggregation of multi-view information. Extensive experiments on eight datasets show favorable performance of the proposed method compared to state-of-the-art approaches, demonstrating its effectiveness across diverse scenarios",
    "checked": true,
    "id": "d0a24bf4a1a94823f976284c1fd2adeabec52b81",
    "semantic_title": "medusa: a multi-scale high-order contrastive dual-diffusion approach for multi-view clustering",
    "citation_count": 0,
    "authors": [
      "Liang Chen",
      "Zhe Xue",
      "Yawen Li",
      "Meiyu Liang",
      "Yan Wang",
      "Anton van den Hengel",
      "Yuankai Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_MambaOut_Do_We_Really_Need_Mamba_for_Vision_CVPR_2025_paper.html": {
    "title": "MambaOut: Do We Really Need Mamba for Vision?",
    "volume": "main",
    "abstract": "Mamba, an architecture with RNN-like token mixer of state space model (SSM), was recently introduced to address the quadratic complexity of the attention mechanism and subsequently applied to vision tasks. Nevertheless, the performance of Mamba for vision is often underwhelming when compared with convolutional and attention-based models. In this paper, we delve into the essence of Mamba, and conceptually conclude that Mamba is ideally suited for tasks with long-sequence and autoregressive characteristics. For vision tasks, as image classification on ImageNet does not align with either characteristic, we hypothesize that Mamba is not necessary for this task; Detection and segmentation tasks on COCO or ADE20K are also not autoregressive, yet they adhere to the long-sequence characteristic, so we believe it is still worthwhile to explore Mamba's potential for these tasks. To empirically verify our hypotheses, we construct a series of models named MambaOut through stacking Mamba blocks while removing their core token mixer, SSM. Experimental results strongly support our hypotheses. Specifically, our MambaOut model surpasses all visual Mamba models on ImageNet image classification, indicating that Mamba is indeed unnecessary for this task. As for detection and segmentation, MambaOut cannot match the performance of state-of-the-art visual Mamba models, demonstrating the potential of Mamba for long-sequence visual tasks",
    "checked": true,
    "id": "31fdba3a68f286894f025e734a277e2ce94dd84c",
    "semantic_title": "mambaout: do we really need mamba for vision?",
    "citation_count": 57,
    "authors": [
      "Weihao Yu",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Everything_to_the_Synthetic_Diffusion-driven_Test-time_Adaptation_via_Synthetic-Domain_Alignment_CVPR_2025_paper.html": {
    "title": "Everything to the Synthetic: Diffusion-driven Test-time Adaptation via Synthetic-Domain Alignment",
    "volume": "main",
    "abstract": "Test-time adaptation (TTA) aims to improve the performance of source-domain pre-trained models on previously unseen, shifted target domains. Traditional TTA methods primarily adapt model weights based on target data streams, making model performance sensitive to the amount and order of target data. The recently proposed diffusion-driven TTA methods mitigate this by adapting model inputs instead of weights, where an unconditional diffusion model, trained on the source domain, transforms target-domain data into a synthetic domain that is expected to approximate the source domain. However, in this paper, we reveal that although the synthetic data in diffusion-driven TTA seems indistinguishable from the source data, it is unaligned with, or even markedly different from the latter for deep networks. To address this issue, we propose a Synthetic-Domain Alignment (SDA) framework. Our key insight is to fine-tune the source model with synthetic data to ensure better alignment. Specifically, we first employ a conditional diffusion model to generate labeled samples, creating a synthetic dataset. Subsequently, we use the aforementioned unconditional diffusion model to add noise to and denoise each sample before fine-tuning. This Mix of Diffusion (MoD) process mitigates the potential domain misalignment between the conditional and unconditional models. Extensive experiments across classifiers, segmenters, and multimodal large language models (MLLMs, e.g., LLaVA) demonstrate that SDA achieves superior domain alignment and consistently outperforms existing diffusion-driven TTA methods. Our code is available at https://github.com/SHI-Labs/Diffusion-Driven-Test-Time-Adaptation-via-Synthetic-Domain-Alignment",
    "checked": true,
    "id": "0bcda9580066202cba741f0e07292ad1ea0fd7f0",
    "semantic_title": "everything to the synthetic: diffusion-driven test-time adaptation via synthetic-domain alignment",
    "citation_count": 6,
    "authors": [
      "Jiayi Guo",
      "Junhao Zhao",
      "Chaoqun Du",
      "Yulin Wang",
      "Chunjiang Ge",
      "Zanlin Ni",
      "Shiji Song",
      "Humphrey Shi",
      "Gao Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_Multi-Granularity_Class_Prototype_Topology_Distillation_for_Class-Incremental_Source-Free_Unsupervised_Domain_CVPR_2025_paper.html": {
    "title": "Multi-Granularity Class Prototype Topology Distillation for Class-Incremental Source-Free Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "This paper explores the Class-Incremental Source-Free Unsupervised Domain Adaptation (CI-SFUDA) problem, where the unlabeled target data come incrementally without access to labeled source instances. This problem poses two challenges, the interference of similar source-class knowledge in target-class representation learning and the shocks of new target knowledge to old ones. To address them, we propose the Multi-Granularity Class Prototype Topology Distillation (GROTO) algorithm, which effectively transfers the source knowledge to the class-incremental target domain. Concretely, we design the multi-granularity class prototype self-organization module and the prototype topology distillation module. First, we mine the positive classes by modeling accumulation distributions. Next, we introduce multi-granularity class prototypes to generate reliable pseudo-labels, and exploit them to promote the positive-class target feature self-organization. Second, the positive-class prototypes are leveraged to construct the topological structures of source and target feature spaces. Then, we perform the topology distillation to continually mitigate the shocks of new target knowledge to old ones. Extensive experiments demonstrate that our proposed method achieves state-of-the-art performance on three public datasets",
    "checked": true,
    "id": "45cea02f8f3285179041b8dac4aece126bf6f929",
    "semantic_title": "multi-granularity class prototype topology distillation for class-incremental source-free unsupervised domain adaptation",
    "citation_count": 1,
    "authors": [
      "Peihua Deng",
      "Jiehua Zhang",
      "Xichun Sheng",
      "Chenggang Yan",
      "Yaoqi Sun",
      "Ying Fu",
      "Liang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Danier_DepthCues_Evaluating_Monocular_Depth_Perception_in_Large_Vision_Models_CVPR_2025_paper.html": {
    "title": "DepthCues: Evaluating Monocular Depth Perception in Large Vision Models",
    "volume": "main",
    "abstract": "Large-scale pre-trained vision models are becoming increasingly prevalent, offering expressive and generalizable visual representations that benefit various downstream tasks. Recent studies on the emergent properties of these models have revealed their high-level geometric understanding, in particular in the context of depth perception. However, it remains unclear how depth perception arises in these models without explicit depth supervision provided during pre-training. To investigate this, we examine whether the monocular depth cues, similar to those used by the human visual system, emerge in these models. We introduce a new benchmark, DepthCues, designed to evaluate depth cue understanding, and present findings across 20 diverse and representative pre-trained vision models. Our analysis shows that human-like depth cues emerge in more recent larger models. We also explore enhancing depth perception in large vision models by fine-tuning on DepthCues, and find that even without dense depth supervision, this improves depth estimation. To support further research, our benchmark and evaluation code will be made publicly available for studying depth perception in vision models",
    "checked": true,
    "id": "29941d3d3b491d2a3bbba45014f5a8344707076f",
    "semantic_title": "depthcues: evaluating monocular depth perception in large vision models",
    "citation_count": 5,
    "authors": [
      "Duolikun Danier",
      "Mehmet Ayg√ºn",
      "Changjian Li",
      "Hakan Bilen",
      "Oisin Mac Aodha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_A_Polarization-Aided_Transformer_for_Image_Deblurring_via_Motion_Vector_Decomposition_CVPR_2025_paper.html": {
    "title": "A Polarization-Aided Transformer for Image Deblurring via Motion Vector Decomposition",
    "volume": "main",
    "abstract": "Effectively leveraging motion information is crucial for the image deblurring task. Existing methods typically build deep-learning models to restore a clean image by estimating blur patterns over the entire movement. This suggests that the blur caused by rotational motion components is processed together with the translational one. Exploring the movement without separation leads to limited performance for complex motion deblurring, especially rotational motion. In this paper, we propose Motion Decomposition Transformer (MDT), a transformer-based architecture augmented with polarized modules for deblurring via motion vector decomposition. MDT consists of a Motion Decomposition Module (MDM) for extracting hybrid rotation and translation features, and a Radial Stripe Attention Solver (RSAS) for sharp image reconstruction with enhanced rotational information. Specifically, the MDM uses a deformable Cartesian convolutional branch to capture translational motion, complemented by a polar-system branch to capture rotational motion. The RSAS employs radial stripe windows and angular relative positional encoding in the polar system to enhance rotational information. This design preserves translational details while keeping computational costs lower than dual-coordinate design. Experimental results on 6 image deblurring datasets show that MDT outperforms state-of-the-art methods, particularly in handling blur caused by complex motions with significant rotational components",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duosheng Chen",
      "Shihao Zhou",
      "Jinshan Pan",
      "Jinglei Shi",
      "Lishen Qu",
      "Jufeng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_SpecTRe-GS_Modeling_Highly_Specular_Surfaces_with_Reflected_Nearby_Objects_by_CVPR_2025_paper.html": {
    "title": "SpecTRe-GS: Modeling Highly Specular Surfaces with Reflected Nearby Objects by Tracing Rays in 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS), a recently emerged multi-view 3D reconstruction technique, has shown significant advantages in real-time rendering and explicit editing. However, 3DGS encounters challenges in the accurate modeling of both high-frequency view-dependent appearances and global illumination effects, including inter-reflection. This paper introduces SpecTRe-GS, which addresses these challenges and models highly Specular surfaces that reflect nearby objects through Tracing Rays in 3D Gaussian Splatting. SpecTRe-GS separately models reflections from highly specular and rough surfaces to leverage the distinctions between their reflective properties and integrates an efficient ray tracer within the 3DGS framework for querying secondary rays, thus achieving fast and accurate rendering. Also, it incorporates normal prior guidance and joint geometry optimization at various stages of the training process to enhance geometry reconstruction for undistorted reflections. Experiments on both synthetic and real-world scenes demonstrate the superiority of SpecTRe-GS compared to existing 3DGS-based methods in capturing highly specular inter-reflections and also showcase its editing applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajun Tang",
      "Fan Fei",
      "Zhihao Li",
      "Xiao Tang",
      "Shiyong Liu",
      "Youyu Chen",
      "Binxiao Huang",
      "Zhenyu Chen",
      "Xiaofei Wu",
      "Boxin Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cho_Seurat_From_Moving_Points_to_Depth_CVPR_2025_paper.html": {
    "title": "Seurat: From Moving Points to Depth",
    "volume": "main",
    "abstract": "Accurate depth estimation from monocular videos remains challenging due to ambiguities inherent in single-view geometry, as crucial depth cues like stereopsis are absent. However, humans often perceive relative depth intuitively by observing variations in the size and spacing of objects as they move. Inspired by this, we propose a novel method that infers relative depth by examining the spatial relationships and temporal evolution of a set of tracked 2D trajectories. Specifically, we use off-the-shelf point tracking models to capture 2D trajectories. Then, our approach employs spatial and temporal transformers to process these trajectories and directly infer depth changes over time. Evaluated on the TAPVid-3D benchmark, our method demonstrates robust zero-shot performance, generalizing effectively from synthetic to real-world datasets. Results indicate that our approach achieves temporally smooth, high-accuracy depth predictions across diverse domains",
    "checked": true,
    "id": "ec8fd9a83d6777d95c111a57e3c6118a0c0cb87f",
    "semantic_title": "seurat: from moving points to depth",
    "citation_count": 0,
    "authors": [
      "Seokju Cho",
      "Jiahui Huang",
      "Seungryong Kim",
      "Joon-Young Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_AuraFusion360_Augmented_Unseen_Region_Alignment_for_Reference-based_360deg_Unbounded_Scene_CVPR_2025_paper.html": {
    "title": "AuraFusion360: Augmented Unseen Region Alignment for Reference-based 360deg Unbounded Scene Inpainting",
    "volume": "main",
    "abstract": "Three-dimensional scene inpainting is crucial for applications from virtual reality to architectural visualization, yet existing methods struggle with view consistency and geometric accuracy in 360deg unbounded scenes. We present AuraFusion360, a novel reference-based method that enables high-quality object removal and hole filling in 3D scenes represented by Gaussian Splatting. Our approach introduces (1) depth-aware unseen mask generation for accurate occlusion identification, (2) Adaptive Guided Depth Diffusion, a zero-shot method for accurate initial point placement without requiring additional training, and (3) SDEdit-based detail enhancement for multi-view coherence. We also introduce 360-USID, the first comprehensive dataset for 360deg unbounded scene inpainting with ground truth. Extensive experiments demonstrate that AuraFusion360 significantly outperforms existing methods, achieving superior perceptual quality while maintaining geometric accuracy across dramatic viewpoint changes",
    "checked": false,
    "id": "efe132be55a59dce48eb5b22d8e2b3a071cd0da3",
    "semantic_title": "aurafusion360: augmented unseen region alignment for reference-based 360¬∞ unbounded scene inpainting",
    "citation_count": 2,
    "authors": [
      "Chung-Ho Wu",
      "Yang-Jung Chen",
      "Ying-Huan Chen",
      "Jie-Ying Lee",
      "Bo-Hsu Ke",
      "Chun-Wei Tuan Mu",
      "Yi-Chuan Huang",
      "Chin-Yang Lin",
      "Min-Hung Chen",
      "Yen-Yu Lin",
      "Yu-Lun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zha_Language-Guided_Image_Tokenization_for_Generation_CVPR_2025_paper.html": {
    "title": "Language-Guided Image Tokenization for Generation",
    "volume": "main",
    "abstract": "Image tokenization, the process of transforming raw image pixels into a compact low-dimensional latent representation, has proven crucial for scalable and efficient image generation. However, mainstream image tokenization methods generally have limited compression rates, making high-resolution image generation computationally expensive. To address this challenge, we propose to leverage language for efficient image tokenization, and we call our method Text-Conditioned Image Tokenization (TexTok). TexTok is a simple yet effective tokenization framework that leverages language to provide a compact, high-level semantic representation. By conditioning the tokenization process on descriptive text captions, TexTok simplifies semantic learning, allowing more learning capacity and token space to be allocated to capture fine-grained visual details, leading to enhanced reconstruction quality and higher compression rates. Compared to the conventional tokenizer without text conditioning, TexTok achieves average reconstruction FID improvements of 29.2% and 48.1% on ImageNet-256 and -512 benchmarks respectively, across varying numbers of tokens. These tokenization improvements consistently translate to 16.3% and 34.3% average improvements in generation FID. By simply replacing the tokenizer in Diffusion Transformer (DiT) with TexTok, our system can achieve a 93.5x inference speedup while still outperforming the original DiT using only 32 tokens on ImageNet-512. TexTok with a vanilla DiT generator achieves state-of-the-art FID scores of 1.46 and 1.62 on ImageNet-256 and -512 respectively. Furthermore, we demonstrate TexTok's superiority on the text-to-image generation task, effectively utilizing the off-the-shelf text captions in tokenization",
    "checked": true,
    "id": "1407d10be41d6d1de444173ec88fd622a5f31566",
    "semantic_title": "language-guided image tokenization for generation",
    "citation_count": 8,
    "authors": [
      "Kaiwen Zha",
      "Lijun Yu",
      "Alireza Fathi",
      "David A. Ross",
      "Cordelia Schmid",
      "Dina Katabi",
      "Xiuye Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiao_Img-Diff_Contrastive_Data_Synthesis_for_Multimodal_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models",
    "volume": "main",
    "abstract": "High-performance Multimodal Large Language Models (MLLMs) rely heavily on data quality. This study introduces a novel data synthesis method, leveraging insights from contrastive learning and image difference captioning to enhance fine-grained image recognition in MLLMs. By analyzing object differences in detailed regions between similar images, we challenge the model to identify both matching and distinct components. Specifically, our method initially create pairs of similar images that highlight object variations. After that, we introduce a Difference Area Generator for object differences identifying, followed by a Difference Captions Generator for differences describing. The outcome is a high-quality dataset of \"object replacement\" samples, named Img-Diff, which can be expanded as needed due to its automation. We use the generated dataset to finetune state-of-the-art (SOTA) MLLMs such as InternVL2, yielding comprehensive improvements across numerous image difference and Visual Question Answering tasks. For instance, the trained models notably surpass the SOTA models GPT-4V and Gemini on the MMVP benchmark. Additionally, we conduct thorough evaluations to confirm the dataset's diversity, quality, and robustness, presenting several insights on the synthesis of such a contrastive dataset. We release our codes and dataset to encourage further research on multimodal data synthesis and MLLMs' fundamental capabilities for image understanding",
    "checked": true,
    "id": "6b8850e5607f4d877fe512befcc368a1c62678dc",
    "semantic_title": "img-diff: contrastive data synthesis for multimodal large language models",
    "citation_count": 8,
    "authors": [
      "Qirui Jiao",
      "Daoyuan Chen",
      "Yilun Huang",
      "Bolin Ding",
      "Yaliang Li",
      "Ying Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_CocoER_Aligning_Multi-Level_Feature_by__Competition_and_Coordination_for_CVPR_2025_paper.html": {
    "title": "CocoER: Aligning Multi-Level Feature by Competition and Coordination for Emotion Recognition",
    "volume": "main",
    "abstract": "With the explosion of human-machine interaction, emotion recognition has reignited attention. Previous works focus on improving visual feature fusion and reasoning from multiple image levels. Although it is non-trivial to deduce a person's emotion by integrating multi-level feature (head, body and context), the emotion recognition results of each level is usually different from one another, which creates inconsistency in the prevailing feature alignment method and decrease recognition performance. In this work, we propose a multi-level image feature refinement method for emotion recognition (CocoER) to mitigate the impact caused by conflicting results from multi-level recognition. First, we leverage cross-level attention to improve visual feature consistency between hierarchically cropped head, body and context windows. Then, vocabulary informed alignment is incorporated into the recognition framework to produce pseudo label and guide hierarchical visual feature refinement. To effectively fuse multi-level feature, we elaborate on a competition process of eliminating irrelevant image level predictions and a coordination process to enhance the feature across all levels. Extensive experiments are executed on two popular datasets, and our method achieves state-of-the-art performance with multi-level interpretation results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuli Shen",
      "Hua Cai",
      "Weilin Shen",
      "Qing Xu",
      "Dingding Yu",
      "Weifeng Ge",
      "Xiangyang Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sur_Hyperbolic_Uncertainty-Aware_Few-Shot_Incremental_Point_Cloud_Segmentation_CVPR_2025_paper.html": {
    "title": "Hyperbolic Uncertainty-Aware Few-Shot Incremental Point Cloud Segmentation",
    "volume": "main",
    "abstract": "3D point cloud segmentation is essential across a range of applications; however, conventional methods often struggle in evolving environments, particularly when tasked with identifying novel categories under limited supervision. Few-Shot Learning (FSL) and Class Incremental Learning (CIL) have been adapted previously to address these challenges in isolation, yet the combined paradigm of Few-Shot Class Incremental Learning (FSCIL) remains largely unexplored for point cloud segmentation. To address this gap, we introduce Hyperbolic Ideal Prototypes Optimization (HIPO), a novel framework that harnesses hyperbolic embeddings for FSCIL in 3D point clouds. HIPO employs the Poincare Hyperbolic Sphere as its embedding space, integrating Ideal Prototypes enriched by CLIP-derived class semantics, to capture the hierarchical structure of 3D data. By enforcing orthogonality among prototypes and maximizing representational margins, HIPO constructs a resilient embedding space that mitigates forgetting and enables the seamless integration of new classes, thereby effectively countering overfitting. Extensive evaluations on S3DIS, ScanNetv2, and cross-dataset scenarios demonstrate HIPO's strong performance, significantly surpassing existing approaches in both in-domain and cross-dataset FSCIL tasks for 3D point cloud segmentation",
    "checked": false,
    "id": "db0f0d6564217d794cedde7c937ab0a90522185e",
    "semantic_title": "probabilistic interactive 3d segmentation with hierarchical neural processes",
    "citation_count": 0,
    "authors": [
      "Tanuj Sur",
      "Samrat Mukherjee",
      "Kaizer Rahaman",
      "Subhasis Chaudhuri",
      "Muhammad Haris Khan",
      "Biplab Banerjee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_Enhancing_Creative_Generation_on_Stable_Diffusion-based_Models_CVPR_2025_paper.html": {
    "title": "Enhancing Creative Generation on Stable Diffusion-based Models",
    "volume": "main",
    "abstract": "Recent text-to-image generative models, particularly Stable Diffusion and its distilled variants, have achieved impressive fidelity and strong text-image alignment. However, their creative generation capacity remains limited, as simply adding the term \"creative\" to prompts often fails to yield genuinely creative results. In this paper, we introduce C3 (Creative Concept Catalyst), a training-free approach designed to enhance creativity in Stable Diffusion-based models. C3 selectively amplifies features during the denoising process to foster more creative outputs. We offer practical guidelines for choosing amplification factors based on two main aspects of creativity. C3 allows user-friendly creativity control in image generation and is the first study to enhance creativity in diffusion models without extensive computational costs. We demonstrate its effectiveness across various Stable Diffusion-based models. Source codes will be publicly available",
    "checked": true,
    "id": "e546c997b12e37d84a3eda7c5f86c0d447720206",
    "semantic_title": "enhancing creative generation on stable diffusion-based models",
    "citation_count": 2,
    "authors": [
      "Jiyeon Han",
      "Dahee Kwon",
      "Gayoung Lee",
      "Junho Kim",
      "Jaesik Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_The_Devil_is_in_the_Prompts_Retrieval-Augmented_Prompt_Optimization_for_CVPR_2025_paper.html": {
    "title": "The Devil is in the Prompts: Retrieval-Augmented Prompt Optimization for Text-to-Video Generation",
    "volume": "main",
    "abstract": "The evolution of Text-to-video (T2V) generative models, trained on large-scale datasets, has been marked by significant progress. However, the sensitivity of T2V generative models to input prompts highlights the critical role of prompt design in influencing generative outcomes. Prior research has predominantly relied on Large Language Models (LLMs) to align user-provided prompts with the distribution of training prompts, albeit without tailored guidance encompassing prompt vocabulary and sentence structure nuances. To this end, we introduce RAPO, a novel Retrieval-Augmented Prompt Optimization framework. In order to address potential inaccuracies and ambiguous details generated by LLM-generated prompts. RAPO refines the naive prompts through dual optimization branches, selecting the superior prompt for T2V generation. The first branch augments user prompts with diverse modifiers extracted from a learned relational graph, refining them to align with the format of training prompts via a fine-tuned LLM. Conversely, the second branch rewrites the naive prompt using a pre-trained LLM following a well-defined instruction set. Extensive experiments demonstrate that RAPO can effectively enhance both the static and dynamic dimensions of generated videos, demonstrating the significance of prompt optimization for user-provided prompts. Project website: \\href https://whynothaha.github.io/Prompt_optimizer/RAPO.html GitHub",
    "checked": true,
    "id": "c5eb46e5872db5fe4f868eb5d6669f667d94e146",
    "semantic_title": "the devil is in the prompts: retrieval-augmented prompt optimization for text-to-video generation",
    "citation_count": 1,
    "authors": [
      "Bingjie Gao",
      "Xinyu Gao",
      "Xiaoxue Wu",
      "Yujie Zhou",
      "Yu Qiao",
      "Li Niu",
      "Xinyuan Chen",
      "Yaohui Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhuravlev_Denoising_Functional_Maps_Diffusion_Models_for_Shape_Correspondence_CVPR_2025_paper.html": {
    "title": "Denoising Functional Maps: Diffusion Models for Shape Correspondence",
    "volume": "main",
    "abstract": "Estimating correspondences between pairs of deformable shapes remains a challenging problem. Despite substantial progress, existing methods lack broad generalization capabilities and require category-specific training data. To address these limitations, we propose a fundamentally new approach to shape correspondence based on denoising diffusion models. In our method, a diffusion model learns to directly predict the functional map, a low-dimensional representation of a point-wise map between shapes. We use a large dataset of synthetic human meshes for training and employ two steps to reduce the number of functional maps that need to be learned. First, the maps refer to a template rather than shape pairs. Second, the functional map is defined in a basis of eigenvectors of the Laplacian, which is not unique due to sign ambiguity. Therefore, we introduce an unsupervised approach to select a specific basis by correcting the signs of eigenvectors based on surface features. Our model achieves competitive performance on standard human datasets, meshes with anisotropic connectivity, non-isometric humanoid shapes, as well as animals compared to existing descriptor-based and large-scale shape deformation methods",
    "checked": true,
    "id": "b45c5f9c082e69d9369ae3d30c30259883785552",
    "semantic_title": "denoising functional maps: diffusion models for shape correspondence",
    "citation_count": 1,
    "authors": [
      "Aleksei Zhuravlev",
      "Zorah L√§hner",
      "Vladislav Golyanik"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ke_ProReflow_Progressive_Reflow_with_Decomposed_Velocity_CVPR_2025_paper.html": {
    "title": "ProReflow: Progressive Reflow with Decomposed Velocity",
    "volume": "main",
    "abstract": "Diffusion models have achieved significant progress in both image and video generation while still suffering from huge computation costs. As an effective solution, rectified flow aims to rectify the diffusion process of diffusion models into a straight line for few-step and even one-step generation. However, in this paper, we suggest that the original training pipeline of reflow is not optimal and introduce two techniques to improve it. Firstly, we introduce progressive reflow, which progressively reflows the diffusion models in local timesteps until the whole diffusion progresses, reducing the difficulty of flow matching. Second, we introduce aligned v-prediction, which highlights the importance of direction matching in flow matching over magnitude matching. Experimental results on SDv1.5 and SDXL demonstrate the effectiveness of our method, for example, conducting on SDv1.5 achieves an FID of 10.70 on MSCOCO2014 validation set with only 4 sampling steps, close to our teacher model (32 DDIM steps, FID = 10.05). Our codes will be released at Github",
    "checked": true,
    "id": "12661cef7f57067e991e3984636aa4c934c0f221",
    "semantic_title": "proreflow: progressive reflow with decomposed velocity",
    "citation_count": 1,
    "authors": [
      "Lei Ke",
      "Haohang Xu",
      "Xuefei Ning",
      "Yu Li",
      "Jiajun Li",
      "Haoling Li",
      "Yuxuan Lin",
      "Dongsheng Jiang",
      "Yujiu Yang",
      "Linfeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_DnLUT_Ultra-Efficient_Color_Image_Denoising_via_Channel-Aware_Lookup_Tables_CVPR_2025_paper.html": {
    "title": "DnLUT: Ultra-Efficient Color Image Denoising via Channel-Aware Lookup Tables",
    "volume": "main",
    "abstract": "While deep neural networks have revolutionized image denoising capabilities, their deployment on edge devices remains challenging due to substantial computational and memory requirements. To this end, we present DnLUT, an ultra-efficient lookup table-based framework that achieves high-quality color image denoising with minimal resource consumption. Our key innovation lies in two complementary components: a Pairwise Channel Mixer (PCM) that effectively captures inter-channel correlations and spatial dependencies in parallel, and a novel L-shaped convolution design that maximizes receptive field coverage while minimizing storage overhead. By converting these components into optimized lookup tables post-training, DnLUT achieves remarkable efficiency - requiring only 500KB storage and 0.1% energy consumption compared to its CNN contestant DnCNN, while delivering 20x faster inference. Extensive experiments demonstrate that DnLUT outperforms all existing LUT-based methods by over 1dB in PSNR, establishing a new state-of-the-art in resource-efficient color image denoising",
    "checked": true,
    "id": "02ebf6954f203c1ce5058701d69afed73a633ca8",
    "semantic_title": "dnlut: ultra-efficient color image denoising via channel-aware lookup tables",
    "citation_count": 0,
    "authors": [
      "Sidi Yang",
      "Binxiao Huang",
      "Yulun Zhang",
      "Dahai Yu",
      "Yujiu Yang",
      "Ngai Wong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jo_Devil_is_in_the_Detail_Towards_Injecting_Fine_Details_of_CVPR_2025_paper.html": {
    "title": "Devil is in the Detail: Towards Injecting Fine Details of Image Prompt in Image Generation via Conflict-free Guidance and Stratified Attention",
    "volume": "main",
    "abstract": "While large-scale text-to-image diffusion models enable the generation of high-quality, diverse images from text prompts, these prompts struggle to capture intricate details, such as textures, preventing the user intent from being reflected. This limitation has led to efforts to generate images conditioned on user-provided images, referred to as image prompts. Recent work modifies the self-attention mechanism to impose image conditions in generated images by replacing or concatenating the keys and values from the image prompt. This enables the self-attention layer to work like a cross-attention layer, generally used to incorporate text prompts.In this paper, we identify two common issues in existing methods of modifying self-attention that hinder diffusion models from reflecting the image prompt. By addressing these issues, we propose a novel method that generates images that properly reflect the details of image prompts. First, existing approaches often neglect the importance of image prompts in classifier-free guidance, which directs the model towards the intended conditions and away from those undesirable. Specifically, current methods use image prompts as both desired and undesired conditions, causing conflicting signals. To resolve this, we propose conflict-free guidance by using image prompts only as desired conditions, ensuring that the generated image faithfully reflects the image prompt.In addition, we observe that the two most common self-attention modifications involve a trade-off between the realism of the generated image and alignment with the image prompt, achieved by selectively using keys and values from both images. Specifically, selecting more keys and values from the image prompt improves alignment, while selecting more from the generated image enhances realism. To balance both, we propose an alternative self-attention modification method, Stratified Attention, which jointly uses keys and values from both images rather than selecting between them.Through extensive experiments across three distinct image generation tasks, we demonstrate that the proposed method outperforms existing image-prompting models in faithfully reflecting the image prompt",
    "checked": true,
    "id": "dbcbbff9887b8e2f0dbf140d17a0a2047a6f8c1c",
    "semantic_title": "devil is in the detail: towards injecting fine details of image prompt in image generation via conflict-free guidance and stratified attention",
    "citation_count": 0,
    "authors": [
      "Kyungmin Jo",
      "Jooyeol Yun",
      "Jaegul Choo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_D3-Human_Dynamic_Disentangled_Digital_Human_from_Monocular_Video_CVPR_2025_paper.html": {
    "title": "D^3-Human: Dynamic Disentangled Digital Human from Monocular Video",
    "volume": "main",
    "abstract": "We introduce \\text D ^3\\text -Human , a method for reconstructing Dynamic Disentangled Digital Human geometry from monocular videos. Past monocular video human reconstruction primarily focuses on reconstructing undecoupled clothed human bodies or only reconstructing clothing, making it difficult to apply directly in applications such as animation production. The challenge in reconstructing decoupled clothing and body lies in the occlusion caused by clothing over the body. To this end, the details of the visible area and the plausibility of the invisible area must be ensured during the reconstruction process. Our proposed method combines explicit and implicit representations to model the decoupled clothed human body, leveraging the robustness of explicit representations and the flexibility of implicit representations. Specifically, we reconstruct the visible region as SDF and propose a novel human manifold signed distance field (hmSDF) to segment the visible clothing and visible body, and then merge the visible and invisible body. Extensive experimental results demonstrate that, compared with existing reconstruction schemes, \\text D ^3\\text -Human can achieve high-quality decoupled reconstruction of the human body wearing different clothing, and can be directly applied to clothing transfer and animation production",
    "checked": false,
    "id": "72caf8d5c0ec597882590c596e8d46677b7bccbf",
    "semantic_title": "d3-human: dynamic disentangled digital human from monocular video",
    "citation_count": 0,
    "authors": [
      "Honghu Chen",
      "Bo Peng",
      "Yunfan Tao",
      "Juyong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Seo_BiM-VFI_Bidirectional_Motion_Field-Guided_Frame_Interpolation_for_Video_with_Non-uniform_CVPR_2025_paper.html": {
    "title": "BiM-VFI: Bidirectional Motion Field-Guided Frame Interpolation for Video with Non-uniform Motions",
    "volume": "main",
    "abstract": "Existing Video Frame interpolation (VFI) models tend to suffer from time-to-location ambiguity when trained with video of non-uniform motions, such as accelerating, decelerating, and changing directions, which often yield blurred interpolated frames.In this paper, we propose (i) a novel motion description map, Bidirectional Motion field (BiM), to effectively describe non-uniform motions; (ii) a BiM-guided Flow Net (BiMFN) with Content-Aware Upsampling Network (CAUN) for precise optical flow estimation; and (iii) Knowledge Distillation for VFI-centric Flow supervision (KDVCF) to supervise the motion estimation of VFI model with VFI-centric teacher flows.The proposed VFI is called a Bidirectional Motion field-guided VFI (BiM-VFI) model.Extensive experiments show that our BiM-VFI model significantly surpasses the recent state-of-the-art VFI methods by 26% and 45% improvements in LPIPS and STLPIPS respectively, yielding interpolated frames with much fewer blurs at arbitrary time instances",
    "checked": true,
    "id": "cca4c5c44f1e3c064160fef5b5061f2bac0728a7",
    "semantic_title": "bim-vfi: bidirectional motion field-guided frame interpolation for video with non-uniform motions",
    "citation_count": 0,
    "authors": [
      "Wonyong Seo",
      "Jihyong Oh",
      "Munchurl Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Curriculum_Coarse-to-Fine_Selection_for_High-IPC_Dataset_Distillation_CVPR_2025_paper.html": {
    "title": "Curriculum Coarse-to-Fine Selection for High-IPC Dataset Distillation",
    "volume": "main",
    "abstract": "Dataset distillation (DD) excels in synthesizing a small number of images per class (IPC) but struggles to maintain its effectiveness in high-IPC settings. Recent works on dataset distillation demonstrate that combining distilled and real data can mitigate the effectiveness decay. However, our analysis of the combination paradigm reveals that the current one-shot and independent selection mechanism induces an incompatibility issue between distilled and real images. To address this issue, we introduce a novel curriculum coarse-to-fine selection (CCFS) method for efficient high-IPC dataset distillation. CCFS employs a curriculum selection framework for real data selection, where we leverage a coarse-to-fine strategy to select appropriate real data based on the current synthetic dataset in each curriculum. Extensive experiments validate CCFS, surpassing the state-of-the-art by +6.6% on CIFAR-10, +5.8% on CIFAR-100, and +3.4% on Tiny-ImageNet under high-IPC settings. Notably, CCFS achieves 60.2% test accuracy on ResNet-18 with a 20% compression ratio of Tiny-ImageNet, closely matching full-dataset training with only 0.3% degradation. Code: https://github.com/CYDaaa30/CCFS",
    "checked": true,
    "id": "c2a4d826bfdda42899980203e6b87b161512ccc3",
    "semantic_title": "curriculum coarse-to-fine selection for high-ipc dataset distillation",
    "citation_count": 3,
    "authors": [
      "Yanda Chen",
      "Gongwei Chen",
      "Miao Zhang",
      "Weili Guan",
      "Liqiang Nie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_BADGR_Bundle_Adjustment_Diffusion_Conditioned_by_Gradients_for_Wide-Baseline_Floor_CVPR_2025_paper.html": {
    "title": "BADGR: Bundle Adjustment Diffusion Conditioned by Gradients for Wide-Baseline Floor Plan Reconstruction",
    "volume": "main",
    "abstract": "Reconstructing precise camera poses and floor plan layouts from wide-baseline RGB panoramas is a difficult and unsolved problem. We introduce BADGR, a novel diffusion model that jointly performs reconstruction and bundle adjustment (BA) to refine poses and layouts from a coarse state, using 1D floor boundary predictions from dozens of sparsely captured images. Unlike guided diffusion models, BADGR is conditioned on dense per-column outputs from a single-step Levenberg Marquardt (LM) optimizer and is trained to predict camera and wall positions, while minimizing reprojection errors for view consistency. The objective of layout generation from denoising diffusion process complements BA optimization by providing additional learned layout-structural constraints on top of the co-visible features across images. These constraints help BADGR make plausible guesses about spatial relationships, which constrain the pose graph, such as wall adjacency and collinearity, while also learning to mitigate errors from dense boundary observations using global context. BADGR trains exclusively on 2D floor plans, simplifying data acquisition, enabling robust augmentation, and supporting a variety of input densities. Our experiments validate our method, which significantly outperforms the state-of-the-art pose and floor plan layout reconstruction with different input densities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuguang Li",
      "Ivaylo Boyadzhiev",
      "Zixuan Liu",
      "Linda Shapiro",
      "Alex Colburn"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bae_Three_Cars_Approaching_within_100m_Enhancing_Distant_Geometry_by_Tri-Axis_CVPR_2025_paper.html": {
    "title": "Three Cars Approaching within 100m! Enhancing Distant Geometry by Tri-Axis Voxel Scanning for Camera-based Semantic Scene Completion",
    "volume": "main",
    "abstract": "Camera-based Semantic Scene Completion (SSC) is gaining attentions in the 3D perception field. However, properties such as perspective and occlusion lead to the underestimation of the geometry in distant regions, posing a critical issue for safety-focused autonomous driving systems. To tackle this, we propose ScanSSC, a novel camera-based SSC model composed of a Scan Module and Scan Loss, both designed to enhance distant scenes by leveraging context from near-viewpoint scenes. The Scan Module uses axis-wise masked attention, where each axis employing a near-to-far cascade masking that enables distant voxels to capture relationships with preceding voxels. In addition, the Scan Loss computes the cross-entropy along each axis between cumulative logits and corresponding class distributions in a near-to-far direction, thereby propagating rich context-aware signals to distant voxels. Leveraging the synergy between these components, ScanSSC achieves state-of-the-art performance, with IoUs of 44.54 and 48.29, and mIoUs of 17.40 and 20.14 on the SemanticKITTI and SSCBench-KITTI-360 benchmarks",
    "checked": true,
    "id": "432adb636a421a7cd38d7aecdf9937e8e848e005",
    "semantic_title": "three cars approaching within 100m! enhancing distant geometry by tri-axis voxel scanning for camera-based semantic scene completion",
    "citation_count": 1,
    "authors": [
      "Jongseong Bae",
      "Junwoo Ha",
      "Ha Young Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MetaShadow_Object-Centered_Shadow_Detection_Removal_and_Synthesis_CVPR_2025_paper.html": {
    "title": "MetaShadow: Object-Centered Shadow Detection, Removal, and Synthesis",
    "volume": "main",
    "abstract": "Shadows are often underconsidered or even ignored in image editing applications, limiting the realism of the edited results. In this paper, we introduce MetaShadow, a three-in-one versatile framework that enables detection, removal, and controllable synthesis of shadows in natural images in an object-centered fashion. MetaShadow combines the strengths of two cooperative components: Shadow Analyzer, for object-centered shadow detection and removal, and Shadow Synthesizer, for reference-based controllable shadow synthesis. Notably, we optimize the learning of the intermediate features from Shadow Analyzer to guide Shadow Synthesizer to generate more realistic shadows that blend seamlessly with the scene. Extensive evaluations on multiple shadow benchmark datasets show significant improvements of MetaShadow over the existing state-of-the-art methods on object-centered shadow detection, removal, and synthesis. MetaShadow excels in supporting imageediting tasks such as object removal, relocation, and insertion, pushing the boundaries of object-centered image editing",
    "checked": true,
    "id": "8e615af86d62801c68ef777dd5ac21d2a8586737",
    "semantic_title": "metashadow: object-centered shadow detection, removal, and synthesis",
    "citation_count": 2,
    "authors": [
      "Tianyu Wang",
      "Jianming Zhang",
      "Haitian Zheng",
      "Zhihong Ding",
      "Scott Cohen",
      "Zhe Lin",
      "Wei Xiong",
      "Chi-Wing Fu",
      "Luis Figueroa",
      "Soo Ye Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ziliotto_TANGO_Training-free_Embodied_AI_Agents_for_Open-world_Tasks_CVPR_2025_paper.html": {
    "title": "TANGO: Training-free Embodied AI Agents for Open-world Tasks",
    "volume": "main",
    "abstract": "Large Language Models (LLMs) have demonstrated excellent capabilities in composing various modules together to create programs that can perform complex reasoning tasks on images. In this paper, we propose TANGO, an approach that extends the program composition via LLMs already observed for images, aiming to integrate those capabilities into embodied agents capable of observing and acting in the world. Specifically, by employing a simple PointGoal Navigation model combined with a memory-based exploration policy as a foundational primitive for guiding an agent through the world, we show how a single model can address diverse tasks without additional training. We task an LLM with composing the provided primitives to solve a specific task, using only a few in-context examples in the prompt. We evaluate our approach on three key Embodied AI tasks: Open-Set ObjectGoal Navigation, Multi-Modal Lifelong Navigation, and Open Embodied Question Answering, achieving state-of-the-art results without any specific fine-tuning in challenging zero-shot scenarios",
    "checked": true,
    "id": "2fef87c86ea8b318a81031befe5e09fe3706bccd",
    "semantic_title": "tango: training-free embodied ai agents for open-world tasks",
    "citation_count": 5,
    "authors": [
      "Filippo Ziliotto",
      "Tommaso Campari",
      "Luciano Serafini",
      "Lamberto Ballan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nikzad_SATA_Spatial_Autocorrelation_Token_Analysis_for_Enhancing_the_Robustness_of_CVPR_2025_paper.html": {
    "title": "SATA: Spatial Autocorrelation Token Analysis for Enhancing the Robustness of Vision Transformers",
    "volume": "main",
    "abstract": "Over the past few years, vision transformers (ViTs) have consistently demonstrated remarkable performance across various visual recognition tasks. However, attempts to enhance their robustness have yielded limited success, mainly focusing on different training strategies, input patch augmentation, or network structural enhancements. These approaches often involve extensive training and fine-tuning, which are time-consuming and resource-intensive. To tackle these obstacles, we introduce a novel approach named Spatial Autocorrelation Token Analysis (SATA). By harnessing spatial relationships between token features, SATA enhances both the representational capacity and robustness of ViT models. This is achieved through the analysis and grouping of tokens according to their spatial autocorrelation scores prior to their input into the Feed-Forward Network (FFN) block of the self-attention mechanism. Importantly, SATA seamlessly integrates into existing pre-trained ViT baselines without requiring retraining or additional fine-tuning, while concurrently improving efficiency by reducing the computational load of the FFN units. Experimental results show that the baseline ViTs enhanced with SATA not only achieve a new state-of-the-art top-1 accuracy on ImageNet-1K image classification (94.9%) but also establish new state-of-the-art performance across multiple robustness benchmarks, including ImageNet-A (top-1=63.6%), ImageNet-R (top-1=79.2%), and ImageNet-C (mCE=13.6%), all without requiring additional training or fine-tuning of baseline models. Availability: https://github.com/nick-nikzad/SATA",
    "checked": true,
    "id": "ea6aebdeecc68286d6d743c73cb46f0b26e794da",
    "semantic_title": "sata: spatial autocorrelation token analysis for enhancing the robustness of vision transformers",
    "citation_count": 0,
    "authors": [
      "Nick Nikzad",
      "Yi Liao",
      "Yongsheng Gao",
      "Jun Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_DViN_Dynamic_Visual_Routing_Network_for_Weakly_Supervised_Referring_Expression_CVPR_2025_paper.html": {
    "title": "DViN: Dynamic Visual Routing Network for Weakly Supervised Referring Expression Comprehension",
    "volume": "main",
    "abstract": "In this paper, we focus on weakly supervised referring expression comprehension (REC), and identify that the lack of fine-grained visual capability greatly limits the upper performance bound of existing methods. To address this issue, we propose a novel framework for weakly supervised REC, namely Dynamic Visual routing Network (DViN), which overcomes the visual shortcomings from the perspective of feature combination and alignment. In particular, DViN is equipped with a novel sparse routing mechanism to efficiently combine features of multiple visual encoders in a dynamic manner, thus improving the visual descriptive power. Besides, we further propose an innovative weakly supervised objective, namely Routing-based Feature Alignment (RFA), which facilitates the visual understanding of routed features through the intra-modal and inter-modal alignment. To validate DViN, we conduct extensive experiments on four REC benchmark datasets. Experiments demonstrate that DViN achieves state-of-the-art results on four benchmarks while maintaining competitive inference efficiency. Besides, the strong generalization ability of DViN is also validated on weakly supervised referring expression segmentation. Source codes are anonymously released at: https://anonymous.4open.science/r/DViN-7736",
    "checked": false,
    "id": "a4c65265f1f05966fdbb11c7706c84e63da5ae4b",
    "semantic_title": "weakmcn: multi-task collaborative network for weakly supervised referring expression comprehension and segmentation",
    "citation_count": 0,
    "authors": [
      "Xiaofu Chen",
      "Yaxin Luo",
      "Gen Luo",
      "Jiayi Ji",
      "Henghui Ding",
      "Yiyi Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Nested_Diffusion_Models_Using_Hierarchical_Latent_Priors_CVPR_2025_paper.html": {
    "title": "Nested Diffusion Models Using Hierarchical Latent Priors",
    "volume": "main",
    "abstract": "We introduce nested diffusion models, an efficient and powerful hierarchical generative framework that substantially enhances the generation quality of diffusion models, particularly for images of complex scenes. Our approach employs a series of diffusion models to progressively generate latent variables at different semantic levels. Each model in this series is conditioned on the output of the preceding higher-level models, culminating in image generation. Hierarchical latent variables guide the generation process along predefined semantic pathways, allowing our approach to capture intricate structural details. To construct these latent variables, we leverage a pre-trained visual encoder, which learns strong semantic visual representations, and modulate its capacity via dimensionality reduction and noise injection. Across multiple datasets, our system demonstrates significant enhancements in image quality for both unconditional and class/text conditional generation. Moreover, our unconditional generation system substantially outperforms the baseline conditional system. These advancements incur minimal computational overhead as the more abstract levels of our hierarchy work with lower-dimensional representations",
    "checked": true,
    "id": "5a613652d700f9a271b6d01c7d9e4223e9883300",
    "semantic_title": "nested diffusion models using hierarchical latent priors",
    "citation_count": 1,
    "authors": [
      "Xiao Zhang",
      "Ruoxi Jiang",
      "Rebecca Willett",
      "Michael Maire"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_A_Theory_of_Learning_Unified_Model_via_Knowledge_Integration_from_CVPR_2025_paper.html": {
    "title": "A Theory of Learning Unified Model via Knowledge Integration from Label Space Varying Domains",
    "volume": "main",
    "abstract": "Existing domain adaptation systems can hardly be applied to real-world problems with new classes presenting at deployment time, especially regarding source-free scenarios where multiple source domains do not share the label space despite being given a few labeled target data. To address this, we consider a challenging problem: multi-source semi-supervised open-set domain adaptation and propose a learning theory via joint error, effectively tackling strong domain shift. To generalize the algorithm into source-free cases, we introdcue a computationally efficient and architecture-flexible attention-based feature generation module. Extensive experiments on various data sets demonstrate the significant improvement of our proposed algorithm over baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dexuan Zhang",
      "Thomas Westfechtel",
      "Tatsuya Harada"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_HiLoTs_High-Low_Temporal_Sensitive_Representation_Learning_for_Semi-Supervised_LiDAR_Segmentation_CVPR_2025_paper.html": {
    "title": "HiLoTs: High-Low Temporal Sensitive Representation Learning for Semi-Supervised LiDAR Segmentation in Autonomous Driving",
    "volume": "main",
    "abstract": "LiDAR point cloud semantic segmentation plays a crucial role in autonomous driving. In recent years, semi-supervised methods have gained popularity due to their significant reduction in annotation labor and time costs. Current semi-supervised methods typically focus on point cloud spatial distribution or consider short-term temporal representations, e.g., only two adjacent frames, often overlooking the rich long-term temporal properties inherent in autonomous driving scenarios. In driving experience, we observe that nearby objects, such as roads and vehicles, remain stable while driving, whereas distant objects exhibit greater variability in category and shape. This natural phenomenon is also captured by LiDAR, which reflects lower temporal sensitivity for nearby objects and higher sensitivity for distant ones. To leverage these characteristics, we propose HiLoTs, which learns high-temporal sensitivity and low-temporal sensitivity representations from continuous LiDAR frames. These representations are further enhanced and fused using a cross-attention mechanism. Additionally, we employ a teacher-student framework to align the representations learned by the labeled and unlabeled branches, effectively utilizing the large amounts of unlabeled data. Experimental results on the SemanticKITTI and nuScenes datasets demonstrate that our proposed HiLoTs outperforms state-of-the-art semi-supervised methods, and achieves performance close to LiDAR+Camera multimodal approaches",
    "checked": true,
    "id": "5e59c75d3409557747abc43c43f3676b7377a64d",
    "semantic_title": "hilots: high-low temporal sensitive representation learning for semi-supervised lidar segmentation in autonomous driving",
    "citation_count": 0,
    "authors": [
      "R.D. Lin",
      "Pengcheng Weng",
      "Yinqiao Wang",
      "Han Ding",
      "Jinsong Han",
      "Fei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Spiking_Transformer_with_Spatial-Temporal_Attention_CVPR_2025_paper.html": {
    "title": "Spiking Transformer with Spatial-Temporal Attention",
    "volume": "main",
    "abstract": "Spike-based Transformer presents a compelling and energy-efficient alternative to traditional Artificial Neural Network (ANN)-based Transformers, achieving impressive results through sparse binary computations. However, existing spike-based transformers predominantly focus on spatial attention while neglecting crucial temporal dependencies inherent in spike-based processing, leading to suboptimal feature representation and limited performance. To address this limitation, we propose Spiking Transformer with Spatial-Temporal Attention (STAtten), a simple and straightforward architecture that efficiently integrates both spatial and temporal information in the self-attention mechanism. STAtten introduces a block-wise computation strategy that processes information in spatial-temporal chunks, enabling comprehensive feature capture while maintaining the same computational complexity as previous spatial-only approaches. Our method can be seamlessly integrated into existing spike-based transformers without architectural overhaul. Extensive experiments demonstrate that STAtten significantly improves the performance of existing spike-based transformers across both static and neuromorphic datasets, including CIFAR10/100, ImageNet, CIFAR10-DVS, and N-Caltech101",
    "checked": true,
    "id": "38492494f81d6cc96d9d233a3262b4cd0b691fe5",
    "semantic_title": "spiking transformer with spatial-temporal attention",
    "citation_count": 3,
    "authors": [
      "Donghyun Lee",
      "Yuhang Li",
      "Youngeun Kim",
      "Shiting Xiao",
      "Priyadarshini Panda"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Khan_Perceptual_Video_Compression_with_Neural_Wrapping_CVPR_2025_paper.html": {
    "title": "Perceptual Video Compression with Neural Wrapping",
    "volume": "main",
    "abstract": "Standard video codecs are rate-distortion optimization machines, where distortion is typically quantified using PSNR versus the source. However, it is now widely accepted that increasing PSNR does not necessarily translate to better visual quality. In this paper, a better balance between perception and fidelity is targeted, in order to provide for significant rate savings over state-of-the-art standards-based video codecs. Specifically, pre- and postprocessing neural networks are proposed that enhance the coding efficiency of standard video codecs when benchmarked with an array of well-established perceptual quality scores. These \"neural wrapper\" elements are end-to-end trained with a neural codec module serving as a differentiable proxy for standard video codecs. The codec proxy is jointly optimized with the pre- and post components, via a novel two-phase pretraining strategy and end-to-end iterative refinement with stop-gradient. This allows the neural pre- and postprocessor to learn to embed, remove and recover information in a codec-aware manner, thus improving its rate-quality performance. A single neural-wrapper model is thereby established and used for the entire rate-quality curve without needing any downscaling or upscaling. The trained model is tested with the AV1 and VVC standard codecs via an array of well-established objective quality scores (SSIM, MS-SSIM, VMAF, AVQT), as well as mean opinion scores (MOS) derived from ITU-T P.910 subjective testing. Experimental results show that the proposed approach improves all quality scores, with -18.5% average Bjontegaard Delta-rate (BD-rate) saving over all objective scores and MOS improvement over both standard codecs. This illustrates the significant potential of neural wrapper components over standards-based video coding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Umar Karim Khan",
      "Aaron Chadha",
      "Mohammad Ashraful Anam",
      "Yiannis Andreopoulos"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_ViKIENet_Towards_Efficient_3D_Object_Detection_with_Virtual_Key_Instance_CVPR_2025_paper.html": {
    "title": "ViKIENet: Towards Efficient 3D Object Detection with Virtual Key Instance Enhanced Network",
    "volume": "main",
    "abstract": "The sparsity of point clouds and inadequacy of semantic information pose challenges to current LiDAR-only 3D object detection methods. Recent methods alleviate these challenges by converting RGB images into virtual points via depth completion to be fused with LiDAR points. Although these methods have shown outstanding results, they often introduce significant computation overhead due to the high density of virtual points and noise due to inaccurate depth completion. Besides, they do not thoroughly leverage semantic information from images. In this work, we propose the virtual key instance enhanced network (ViKIENet), a highly efficient and effective multi-modal feature fusion framework that fuses the features of virtual key instances (VKIs) and LiDAR points through multiple stages. Our contributions include three main components: semantic key instance selection (SKIS), virtual-instance-focused fusion (VIFF), and virtual-instance-to-real attention (VIRA). We also propose the extended version ViKIENet-R with VIFF-R which includes rotationally equivariant features. Experiment results show that ViKIENet and ViKIENet-R achieve significant improvements in detection performance on the KITTI, JRDB, and nuScenes datasets compared to existing works. On the KITTI dataset, ViKIENet and ViKIENet-R operate at 22.7 and 15.0 FPS, respectively. As of CVPR submission (Nov. 15th, 2024), ViKIENet ranks first on the car detection and orientation estimation leaderboard, while ViKIENet-R ranks second (compared with officially published papers) on the 3D car detection leaderboard",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuochen Yu",
      "Bijie Qiu",
      "Andy W. H. Khong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiang_DKDM_Data-Free_Knowledge_Distillation_for_Diffusion_Models_with_Any_Architecture_CVPR_2025_paper.html": {
    "title": "DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any Architecture",
    "volume": "main",
    "abstract": "Diffusion models (DMs) have demonstrated exceptional generative capabilities across various domains, including image, video, and so on. A key factor contributing to their effectiveness is the high quantity and quality of data used during training. However, mainstream DMs now consume increasingly large amounts of data. For example, training a Stable Diffusion model requires billions of image-text pairs. This enormous data requirement poses significant challenges for training large DMs due to high data acquisition costs and storage expenses. To alleviate this data burden, we propose a novel scenario: using existing DMs as data sources to train new DMs with any architecture. We refer to this scenario as Data-Free Knowledge Distillation for Diffusion Models (DKDM), where the generative ability of DMs is transferred to new ones in a data-free manner. To tackle this challenge, we make two main contributions. First, we introduce a DKDM objective that enables the training of new DMs via distillation, without requiring access to the data. Second, we develop a dynamic iterative distillation method that efficiently extracts time-domain knowledge from existing DMs, enabling direct retrieval of training data without the need for a prolonged generative process. To the best of our knowledge, we are the first to explore this scenario. Experimental results demonstrate that our data-free approach not only achieves competitive generative performance but also, in some instances, outperforms models trained with the entire dataset",
    "checked": true,
    "id": "5aa19b8158accc50c7feda1bc1827b9c3f3fd64f",
    "semantic_title": "dkdm: data-free knowledge distillation for diffusion models with any architecture",
    "citation_count": 10,
    "authors": [
      "Qianlong Xiang",
      "Miao Zhang",
      "Yuzhang Shang",
      "Jianlong Wu",
      "Yan Yan",
      "Liqiang Nie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jia_SymDPO_Boosting_In-Context_Learning_of_Large_Multimodal_Models_with_Symbol_CVPR_2025_paper.html": {
    "title": "SymDPO: Boosting In-Context Learning of Large Multimodal Models with Symbol Demonstration Direct Preference Optimization",
    "volume": "main",
    "abstract": "As language models continue to scale, Large Language Models (LLMs) have exhibited emerging capabilities in In-Context Learning (ICL), enabling them to solve language tasks by prefixing a few in-context demonstrations (ICDs) as context. Inspired by these advancements, researchers have extended these techniques to develop Large Multimodal Models (LMMs) with ICL capabilities. However, existing LMMs face a critical issue: they often fail to effectively leverage the visual context in multimodal demonstrations and instead simply follow textual patterns. This indicates that LMMs do not achieve effective alignment between multimodal demonstrations and model outputs. To address this problem, we propose Symbol Demonstration Direct Preference Optimization (SymDPO). Specifically, SymDPO aims to break the traditional paradigm of constructing multimodal demonstrations by using random symbols to replace text answers within instances. This forces the model to carefully understand the demonstration images and establish a relationship between the images and the symbols to answer questions correctly. We validate the effectiveness of this method on multiple benchmarks, demonstrating that with SymDPO, LMMs can more effectively understand the multimodal context within examples and utilize this knowledge to answer questions better. Code is available at https://github.com/APiaoG/SymDPO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongrui Jia",
      "Chaoya Jiang",
      "Haiyang Xu",
      "Wei Ye",
      "Mengfan Dong",
      "Ming Yan",
      "Ji Zhang",
      "Fei Huang",
      "Shikun Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Stealthy_Backdoor_Attack_in_Self-Supervised_Learning_Vision_Encoders_for_Large_CVPR_2025_paper.html": {
    "title": "Stealthy Backdoor Attack in Self-Supervised Learning Vision Encoders for Large Vision Language Models",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) vision encoders learn high-quality image representations and thus have become a vital part of developing vision modality of large vision language models (LVLMs). Due to the high cost of training such encoders, pre-trained encoders are widely shared and deployed into many LVLMs, which are security-critical or bear societal significance. Under this practical scenario, we reveal a new backdoor threat that significant visual hallucinations can be induced into these LVLMs by merely compromising vision encoders. Because of the sharing and reuse of these encoders, many downstream LVLMs may inherit backdoor behaviors from encoders, leading to widespread backdoors. In this work, we propose BadVision, the first method to exploit this vulnerability in SSL vision encoders for LVLMs with novel trigger optimization and backdoor learning techniques. We evaluate BadVision on two types of SSL encoders and LVLMs across eight benchmarks. We show that BadVision effectively drives the LVLMs to attacker-chosen hallucination with over 99% attack success rate, causing a 77.6% relative visual understanding error while maintaining the stealthiness. SoTA backdoor detection methods cannot detect our attack effectively",
    "checked": true,
    "id": "0a7081dc6dcae9516231e102932a1dc6095b445b",
    "semantic_title": "stealthy backdoor attack in self-supervised learning vision encoders for large vision language models",
    "citation_count": 2,
    "authors": [
      "Zhaoyi Liu",
      "Huan Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Data-free_Universal_Adversarial_Perturbation_with_Pseudo-semantic_Prior_CVPR_2025_paper.html": {
    "title": "Data-free Universal Adversarial Perturbation with Pseudo-semantic Prior",
    "volume": "main",
    "abstract": "Data-free Universal Adversarial Perturbation (UAP) is an image-agnostic adversarial attack that deceives deep neural networks using a single perturbation generated solely from random noise without relying on data priors. However, traditional data-free UAP methods often suffer from limited transferability due to the absence of semantic content in random noise. To address this issue, we propose a novel data-free universal attack method that recursively extracts pseudo-semantic priors directly from the UAPs during training to enrich the semantic content within the data-free UAP framework. Our approach effectively leverages latent semantic information within UAPs via region sampling, enabling successful input transformations--typically ineffective in traditional data-free UAP methods due to the lack of semantic cues--and significantly enhancing black-box transferability. Furthermore, we introduce a sample reweighting technique to mitigate potential imbalances from random sampling and transformations, emphasizing hard examples less affected by the UAPs. Comprehensive experiments on ImageNet show that our method achieves state-of-the-art performance in average fooling rate by a substantial margin, notably improves attack transferability across various CNN architectures compared to existing data-free UAP methods, and even surpasses data-dependent UAP methods. Code is available at: https://github.com/ChnanChan/PSP-UAP",
    "checked": true,
    "id": "256601384b896f6e0abc96f8ce74cdf479de6642",
    "semantic_title": "data-free universal adversarial perturbation with pseudo-semantic prior",
    "citation_count": 0,
    "authors": [
      "Chanhui Lee",
      "Yeonghwan Song",
      "Jeany Son"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Debiasing_Multimodal_Large_Language_Models_via_Noise-Aware_Preference_Optimization_CVPR_2025_paper.html": {
    "title": "Debiasing Multimodal Large Language Models via Noise-Aware Preference Optimization",
    "volume": "main",
    "abstract": "Multimodal Large Language Models (MLLMs) excel in various tasks, yet often struggle with modality bias, tending to rely heavily on a single modality or prior knowledge when generating responses. In this paper, we propose a debiased preference optimization dataset, RLAIF-V-Bias, and introduce a Noise-Aware Preference Optimization (NAPO) algorithm. Specifically, we first construct the dataset by introducing perturbations to reduce the informational content of certain modalities, prompting the model to overly rely on a specific modality when generating responses. To address the inevitable noise in automatically constructed data, we combine the noise-robust Mean Absolute Error (MAE) with the Binary Cross-Entropy (BCE) in Direct Preference Optimization (DPO) using a negative Box-Cox transformation and dynamically adjust the algorithm's noise robustness based on the evaluated noise levels in the data.Extensive experiments validate our approach, demonstrating not only its effectiveness in mitigating modality bias but also its significant role in minimizing hallucinations",
    "checked": true,
    "id": "c1b2e20202fbe9989cbe02f0acde78559bb38d7b",
    "semantic_title": "debiasing multimodal large language models via noise-aware preference optimization",
    "citation_count": 1,
    "authors": [
      "Zefeng Zhang",
      "Hengzhu Tang",
      "Jiawei Sheng",
      "Zhenyu Zhang",
      "Yiming Ren",
      "Zhenyang Li",
      "Dawei Yin",
      "Duohe Ma",
      "Tingwen Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SAM2-LOVE_Segment_Anything_Model_2_in_Language-aided_Audio-Visual_Scenes_CVPR_2025_paper.html": {
    "title": "SAM2-LOVE: Segment Anything Model 2 in Language-aided Audio-Visual Scenes",
    "volume": "main",
    "abstract": "Reference Audio-Visual Segmentation (Ref-AVS) aims to provide a pixel-wise scene understanding in Language-aided Audio-Visual Scenes (LAVS). This task requires the model to continuously segment objects referred to by text and audio from a video. Previous dual-modality methods always fail due to the lack of a third modality and the existing triple-modality method struggles with spatio-temporal consistency, leading to the target shift of different frames. In this work, we introduce a novel framework, termed SAM2-LOVE, which integrates textual, audio, and visual representations into a learnable token to prompt and align SAM2 for achieving Ref-AVS in the LAVS. Technically, our approach includes a multimodal fusion module aimed at improving multimodal understanding of SAM2, as well as token propagation and accumulation strategies designed to enhance spatio-temporal consistency without forgetting historical information. We conducted extensive experiments to demonstrate that SAM2-LOVE outperforms the SOTA by 8.5% in J&F on the Ref-AVS benchmark and showcase the simplicity and effectiveness of the components. Our code will be available here",
    "checked": true,
    "id": "1f87b567685c15823bd39bd8990030f99a1e5c9a",
    "semantic_title": "sam2-love: segment anything model 2 in language-aided audio-visual scenes",
    "citation_count": 0,
    "authors": [
      "Yuji Wang",
      "Haoran Xu",
      "Yong Liu",
      "Jiaze Li",
      "Yansong Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_GIVEPose_Gradual_Intra-class_Variation_Elimination_for_RGB-based_Category-Level_Object_Pose_CVPR_2025_paper.html": {
    "title": "GIVEPose: Gradual Intra-class Variation Elimination for RGB-based Category-Level Object Pose Estimation",
    "volume": "main",
    "abstract": "Recent advances in RGBD-based category-level object pose estimation have been limited by their reliance on precise depth information, restricting their broader applicability. In response, RGB-based methods have been developed. Among these methods, geometry-guided pose regression that originated from instance-level tasks has demonstrated strong performance. However, we argue that the NOCS map is an inadequate intermediate representation for geometry-guided pose regression method, as its many-to-one correspondence with category-level pose introduces redundant instance-specific information, resulting in suboptimal results. This paper identifies the intra-class variation problem inherent in pose regression based solely on the NOCS map and proposes the Intra-class Variation-Free Consensus (IVFC) map, a novel coordinate representation generated from the category-level consensus model. By leveraging the complementary strengths of the NOCS map and the IVFC map, we introduce GIVEPose, a framework that implements Gradual Intra-class Variation Elimination for category-level object pose estimation. Extensive evaluations on both synthetic and real-world datasets demonstrate that GIVEPose significantly outperforms existing state-of-the-art RGB-based approaches, achieving substantial improvements in category-level object pose estimation. Our code is available at https://github.com/ziqin-h/GIVEPose",
    "checked": true,
    "id": "3b6eb37c314c1814b70a0e30268ac5bf92449ed3",
    "semantic_title": "givepose: gradual intra-class variation elimination for rgb-based category-level object pose estimation",
    "citation_count": 1,
    "authors": [
      "Ziqin Huang",
      "Gu Wang",
      "Chenyangguang Zhang",
      "Ruida Zhang",
      "Xiu Li",
      "Xiangyang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Camiletto_FRAME_Floor-aligned_Representation_for_Avatar_Motion_from_Egocentric_Video_CVPR_2025_paper.html": {
    "title": "FRAME: Floor-aligned Representation for Avatar Motion from Egocentric Video",
    "volume": "main",
    "abstract": "Egocentric motion capture with a head-mounted body-facing stereo camera is crucial for VR and AR applications but presents significant challenges such as heavy occlusions and limited annotated real-world data. Existing methods rely on synthetic pretraining and struggle to generate smooth and accurate predictions in real-world settings, particularly for lower limbs. Our work addresses these limitations by introducing a lightweight VR-based data collection setup with on-board, real-time 6D pose tracking. Using this setup, we collected the most extensive real-world dataset for ego-facing ego-mounted cameras to date in size and motion variability. Effectively integrating this multimodal input -- device pose and camera feeds -- is challenging due to the differing characteristics of each data source. To address this, we propose FRAME, a simple yet effective architecture that combines device pose and camera feeds for state-of-the-art body pose prediction through geometrically sound multimodal integration and can run at 300 FPS on modern hardware. Lastly, we showcase a novel training strategy to enhance the model's generalization capabilities. Our approach exploits the problem's geometric properties, yielding high-quality motion capture free from common artifacts in prior works. Qualitative and quantitative evaluations, along with extensive comparisons, demonstrate the effectiveness of our method. Data, code, and CAD designs will be available at https://vcai.mpi-inf.mpg.de/projects/FRAME/",
    "checked": true,
    "id": "03a4cab525ef61922be3212f4e887593f8f9f561",
    "semantic_title": "frame: floor-aligned representation for avatar motion from egocentric video",
    "citation_count": 0,
    "authors": [
      "Andrea Boscolo Camiletto",
      "Jian Wang",
      "Eduardo Alvarado",
      "Rishabh Dabral",
      "Thabo Beeler",
      "Marc Habermann",
      "Christian Theobalt"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sain_Sketch_Down_the_FLOPs_Towards_Efficient_Networks_for_Human_Sketch_CVPR_2025_paper.html": {
    "title": "Sketch Down the FLOPs: Towards Efficient Networks for Human Sketch",
    "volume": "main",
    "abstract": "As sketch research has collectively matured over time, its adaptation for at-mass commercialisation emerges on the immediate horizon. Despite an already mature research endeavour for photos, there is no research on the efficient inference specifically designed for sketch data. In this paper, we first demonstrate existing state-of-the-art efficient light-weight models designed for photos do not work on sketches. We then propose two sketch-specific components which work in a plug-n-play manner on any photo efficient network to adapt them to work on sketch data. We specifically chose fine-grained sketch-based image retrieval (FG-SBIR) as a demonstrator as the most recognised sketch problem with immediate commercial value. Technically speaking, we first propose a cross-modal knowledge distillation network to transfer existing photo efficient networks to be compatible with sketch, which brings down number of FLOPs and model parameters by 97.96% percent and 84.89% respectively. We then exploit the abstract trait of sketch to introduce a RL-based canvas selector that dynamically adjusts to the abstraction level which further cuts down number of FLOPs by two thirds. The end result is an overall reduction of 99.37% of FLOPs (from 40.18G to 0.254G) when compared with a full network, while retaining the accuracy (33.03% vs 32.77%) -- finally making an efficient network for the sparse sketch data that exhibit even fewer FLOPs than the best photo counterpart",
    "checked": true,
    "id": "b75057d59be562deb1dbd48490afaa9a58764df0",
    "semantic_title": "sketch down the flops: towards efficient networks for human sketch",
    "citation_count": 0,
    "authors": [
      "Aneeshan Sain",
      "Subhajit Maity",
      "Pinaki Nath Chowdhury",
      "Shubhadeep Koley",
      "Ayan Kumar Bhunia",
      "Yi-Zhe Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Generalized_Zero-Shot_Classification_via_Semantics-Free_Inter-Class_Feature_Generation_CVPR_2025_paper.html": {
    "title": "Generalized Zero-Shot Classification via Semantics-Free Inter-Class Feature Generation",
    "volume": "main",
    "abstract": "Generalized Zero-Shot Learning (GZSL) addresses the challenge of classifying unseen classes in the presence of seen classes by leveraging semantic attributes to bridge the gap for unseen classes. However, in image based disease classification, such as glioma sub-typing, distinguishing between classes using image semantic attributes can be challenging. To address this challenge, we introduce a novel GZSL method that eliminates the dependency on semantic information. Specifically, we propose that the primary of most classification in clinic is risk stratification, and classes are inherently ordered rather than purely categorical. Based on this insight, we present an inter-class feature augmentation (IFA) module, where distributions of different classes are ordered by their risk levels in a learned feature space using pre-defined joint conditional Gaussian distribution model. This ordering enables the generation of unseen class features through feature mixing of adjacent seen classes, effectively transforming the zero-shot learning problem into a supervised learning task. Our method eliminates the need for explicit semantic information, avoiding the cross-modal alignment between visual and semantic features. Moreover, the IFA module for GZSL requires no structural modifications to the existing classification models. In the experiment, both in-house and public datasets are used to evaluate our method across different tasks, including glioma subtyping, Alzheimer's disease (AD) classification and diabetic retinopathy classification. Experimental results demonstrate that our method outperforms the state-of-the-art GZSL methods with statistical significance",
    "checked": false,
    "id": "bfe41c89aa91358126823d22cca98f111ba42fec",
    "semantic_title": "data-free generalized zero-shot learning",
    "citation_count": 11,
    "authors": [
      "Libiao Chen",
      "Dong Nie",
      "Junjun Pan",
      "Jing Yan",
      "Zhenyu Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Feat2GS_Probing_Visual_Foundation_Models_with_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "Feat2GS: Probing Visual Foundation Models with Gaussian Splatting",
    "volume": "main",
    "abstract": "Given that visual foundation models (VFMs) are trained on extensive datasets but often limited to 2D images, a natural question arises: how well do they understand the 3D world? With the differences in architecture and training protocols (i.e., objectives, proxy tasks), a unified framework to fairly and comprehensively probe their 3D awareness is urgently needed. Existing works on 3D probing suggest single-view 2.5D estimation (e.g., depth and normal) or two-view sparse 2D correspondence (e.g., matching and tracking). Unfortunately, these tasks ignore texture awareness, and require 3D data as ground-truth, which limits the scale and diversity of their evaluation set. To address these issues, we introduce Feat2GS, which readout 3D Gaussians attributes from VFM features extracted from unposed images. This allows us to probe 3D awareness for geometry and texture via novel view synthesis, without requiring 3D data. Additionally, the disentanglement of 3DGS parameters - geometry (x, a, S) and texture (c) - enables separate analysis of texture and geometry awareness. Under Feat2GS, we conduct extensive experiments to probe the 3D awareness of several VFMs, and investigate the ingredients that lead to a 3D aware VFM. Building on these findings, we develop several variants that achieve state-of-the-art across diverse datasets. This makes Feat2GS useful for probing VFMs, and as a simple-yet-effective baseline for novel-view synthesis. Code and data will be made available at fanegg.github.io/Feat2GS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Chen",
      "Xingyu Chen",
      "Anpei Chen",
      "Gerard Pons-Moll",
      "Yuliang Xiu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Multi-Modal_Aerial-Ground_Cross-View_Place_Recognition_with_Neural_ODEs_CVPR_2025_paper.html": {
    "title": "Multi-Modal Aerial-Ground Cross-View Place Recognition with Neural ODEs",
    "volume": "main",
    "abstract": "Place recognition (PR) aims at retrieving the query place from a database and plays a crucial role in various applications, including navigation, autonomous driving, and augmented reality. While previous multi-modal PR works have mainly focused on the same-view scenario in which ground-view descriptors are matched with a database of ground-view descriptors during inference, the multi-modal cross-view scenario, in which ground-view descriptors are matched with aerial-view descriptors in a database, remains under-explored. We propose AGPlace, a model that effectively integrates information from multi-modal ground sensors (cameras and LiDARs) to achieve accurate aerial-ground PR. AGPlace achieves effective aerial-ground cross-view PR by leveraging a manifold-based neural ordinary differential equation (ODE) framework with a multi-domain alignment loss. It outperforms existing state-of-the-art cross-view PR models on large-scale datasets. As most existing PR models are designed for ground-ground PR, we adapt these baselines into our cross-view pipeline. Experiments demonstrate that this direct adaptation performs worse than our overall model architecture AGPlace. AGPlace represents a significant advancement in multi-modal aerial-ground PR, with promising implications for real-world applications",
    "checked": false,
    "id": "e0dcca0175d7b8a1fe5a290aed84255ee48d49dd",
    "semantic_title": "visual, spatial, geometric-preserved place recognition for cross-view and cross-modal collaborative perception",
    "citation_count": 3,
    "authors": [
      "Sijie Wang",
      "Rui She",
      "Qiyu Kang",
      "Siqi Li",
      "Disheng Li",
      "Tianyu Geng",
      "Shangshu Yu",
      "Wee Peng Tay"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wazir_Rethinking_Decoder_Design_Improving_Biomarker_Segmentation_Using_Depth-to-Space_Restoration_and_CVPR_2025_paper.html": {
    "title": "Rethinking Decoder Design: Improving Biomarker Segmentation Using Depth-to-Space Restoration and Residual Linear Attention",
    "volume": "main",
    "abstract": "Segmenting biomarkers in medical images is crucial for various biotech applications. Despite advances, Transformer and CNN based methods often struggle with variations in staining and morphology, limiting feature extraction. In medical image segmentation, where datasets often have limited sample availability, recent state-of-the-art (SOTA) methods achieve higher accuracy by leveraging pre-trained encoders, whereas end-to-end methods tend to underperform. This is due to challenges in effectively transferring rich multiscale features from encoders to decoders, as well as limitations in decoder efficiency. To address these issues, we propose an architecture that captures multi-scale local and global contextual information and a novel decoder design, which effectively integrates features from the encoder, emphasizes important channels and regions, and reconstructs spatial dimensions to enhance segmentation accuracy. Our method, compatible with various encoders, outperforms SOTA methods, as demonstrated by experiments on four datasets and ablation studies. Specifically, our method achieves absolute performance gains of 2.76% on MoNuSeg, 3.12% on DSB, 2.87% on Electron Microscopy, and 4.03% on TNBC datasets compared to existing SOTA methods. Code: https://github.com/saadwazir/MCADS-Decoder",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saad Wazir",
      "Daeyoung Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_MaDCoW_Marginal_Distortion_Correction_for_Wide-Angle_Photography_with_Arbitrary_Objects_CVPR_2025_paper.html": {
    "title": "MaDCoW: Marginal Distortion Correction for Wide-Angle Photography with Arbitrary Objects",
    "volume": "main",
    "abstract": "We introduce MaDCoW, a method for correcting marginal distortion of arbitrary objects in wide-angle photography. People often use wide-angle photography to convey natural scenes--smartphones typically default to wide-angle photography--but depicting very wide-field-of-view scenes produces distorted object appearance, particularly marginal distortion in linear projections. With MaDCoW, a user annotates regions-of-interest to correct, along with straight lines. For each region, MaDCoW solves for a local-linear perspective projection and then jointly solves for a projection for the whole photograph that minimizes distortion. We show that our method can produce good results in cases where previous methods yield visible distortions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Zhang",
      "Jia-Bin Huang",
      "Jose Echevarria",
      "Stephen DiVerdi",
      "Aaron Hertzmann"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_SynTab-LLaVA_Enhancing_Multimodal_Table_Understanding_with_Decoupled_Synthesis_CVPR_2025_paper.html": {
    "title": "SynTab-LLaVA: Enhancing Multimodal Table Understanding with Decoupled Synthesis",
    "volume": "main",
    "abstract": "Due to the limited scale of multimodal table understanding (MTU) data, model performance is constrained. A straightforward approach is to use multimodal large language models to obtain more samples, but this may cause hallucinations, generate incorrect sample pairs, and cost significantly.To address the above issues, we design a simple yet effective synthesis framework that consists of two independent steps: table image rendering and table question and answer (Q&A) pairs generation.We use table codes (HTML, LaTeX, Markdown) to synthesize images and generate Q&A pairs with large language model (LLM).This approach leverages LLM's high concurrency and low cost to boost annotation efficiency and reduce expenses. By inputting code instead of images, LLMs can directly access the content and structure of the table, reducing hallucinations in table understanding and improving the accuracy of generated Q&A pairs. Finally, we synthesize a large-scale MTU dataset, SynTab, containing 636K images and 1.8M samples costing within \\200 in US dollars. We further introduce a generalist tabular multimodal model, SynTab-LLaVA. This model not only effectively extracts local textual content within the table but also enables global modeling of relationships between cells.SynTab-LLaVA achieves SOTA performance on 21 out of 24 in-domain and out-of-domain benchmarks, demonstrating the effectiveness and generalization of our method. The Code is available at \\href https://github.com/bang123-box/SynTab-LLaVA SynTab-LLaVA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bangbang Zhou",
      "Zuan Gao",
      "Zixiao Wang",
      "Boqiang Zhang",
      "Yuxin Wang",
      "Zhineng Chen",
      "Hongtao Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Edit_Away_and_My_Face_Will_not_Stay_Personal_Biometric_CVPR_2025_paper.html": {
    "title": "Edit Away and My Face Will not Stay: Personal Biometric Defense against Malicious Generative Editing",
    "volume": "main",
    "abstract": "Recent advancements in diffusion models have made generative image editing more accessible than ever. While these developments allow users to generate creative edits with ease, they also raise significant ethical concerns, particularly regarding malicious edits to human portraits that threaten individuals' privacy and identity security. Existing general-purpose image protection methods primarily focus on generating adversarial perturbations to nullify edit effects. However, these approaches often exhibit instability to protect against diverse editing requests. In this work, we introduce a novel perspective to personal human portrait protection against malicious editing. Unlike traditional methods aiming to prevent edits from taking effect, our method, FaceLock, optimizes adversarial perturbations to ensure that original biometric information---such as facial features---is either destroyed or substantially altered post-editing, rendering the subject in the edited output biometrically unrecognizable. Our approach innovatively integrates facial recognition and visual perception factors into the perturbation optimization process, ensuring robust protection against a variety of editing attempts. Besides, we shed light on several critical issues with commonly used evaluation metrics in image editing and reveal cheating methods by which they can be easily manipulated, leading to deceptive assessments of protection. Through extensive experiments, we demonstrate that FaceLock significantly outperforms all baselines in defense performance against a wide range of malicious edits. Moreover, our method also exhibits strong robustness against purification techniques. Comprehensive ablation studies confirm the stability and broad applicability of our method across diverse diffusion-based editing algorithms. Our work not only advances the state-of-the-art in biometric defense but also sets the foundation for more secure and privacy-preserving practices in image editing",
    "checked": true,
    "id": "66167d826908a02bc675f49fba5165ccb05690a0",
    "semantic_title": "edit away and my face will not stay: personal biometric defense against malicious generative editing",
    "citation_count": 2,
    "authors": [
      "Hanhui Wang",
      "Yihua Zhang",
      "Ruizheng Bai",
      "Yue Zhao",
      "Sijia Liu",
      "Zhengzhong Tu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Any6D_Model-free_6D_Pose_Estimation_of_Novel_Objects_CVPR_2025_paper.html": {
    "title": "Any6D: Model-free 6D Pose Estimation of Novel Objects",
    "volume": "main",
    "abstract": "We introduce Any6D, a model-free framework for 6D object pose estimation that requires only a single RGB-D anchor image to estimate both the 6D pose and size of unknown objects in novel scenes. Unlike existing methods that rely on textured 3D models or multiple viewpoints, Any6D leverages a joint object alignment process to enhance 2D-3D alignment and metric scale estimation for improved pose accuracy. Our approach integrates a render-and-compare strategy to generate and refine pose hypotheses, enabling robust performance in scenarios with occlusions, non-overlapping views, diverse lighting conditions, and large cross-environment variations. We evaluate our method on five challenging datasets: REAL275, Toyota-Light, HO3D, YCBINEOAT, and LM-O, demonstrating its effectiveness in significantly outperforming state-of-the-art methods for novel object pose estimation. Project page: https://taeyeop.com/any6d",
    "checked": true,
    "id": "a08ea515555253c7791a2b00f358c7caf6dec8d0",
    "semantic_title": "any6d: model-free 6d pose estimation of novel objects",
    "citation_count": 2,
    "authors": [
      "Taeyeop Lee",
      "Bowen Wen",
      "Minjun Kang",
      "Gyuree Kang",
      "In So Kweon",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Improving_Accuracy_and_Calibration_via_Differentiated_Deep_Mutual_Learning_CVPR_2025_paper.html": {
    "title": "Improving Accuracy and Calibration via Differentiated Deep Mutual Learning",
    "volume": "main",
    "abstract": "Deep Neural Networks (DNNs) have achieved remarkable success in a variety of tasks, particularly in terms of prediction accuracy. However, in real-world scenarios, especially in safety-critical applications, accuracy alone is insufficient; reliable uncertainty estimates are essential. Modern DNNs, often trained with cross-entropy loss, tend to exhibit overconfidence, especially on ambiguous samples. Many techniques aim to improve uncertainty calibration, yet they often come at the cost of reduced accuracy or increased computational demands. To address this challenge, we propose Differentiated Deep Mutual Learning (Diff-DML), an efficient ensemble approach that simultaneously enhances accuracy and uncertainty calibration. Diff-DML draws inspiration from Deep Mutual Learning (DML) while introducing two strategies to maintain prediction diversity: (1) Differentiated Training Strategy (DTS) and (2) Diversity-Preserving Learning Objective (DPLO). Our theoretical analysis shows that Diff-DML's diversified learning framework not only leverages ensemble benefits but also avoids the loss of prediction diversity observed in traditional DML setups, which is crucial for improved calibration. Extensive evaluations on various benchmarks confirm the effectiveness of Diff-DML. For instance, on the CIFAR-100 dataset, Diff-DML on ResNet34 model achieved substantial improvements over the previous state-of-the-art method, MDCA, with absolute accuracy gains of 1.3%/3.1%, relative ECE reductions of 49.6%/43.8%, and relative classwise-ECE reductions of 7.7%/13.0%",
    "checked": false,
    "id": "b0472ef75cd1a81b6d4e83f0b8224b1cef43c244",
    "semantic_title": "mri-based deep learning model for differentiation of hepatic hemangioma and hepatoblastoma in early infancy",
    "citation_count": 3,
    "authors": [
      "Han Liu",
      "Peng Cui",
      "Bingning Wang",
      "Weipeng Chen",
      "Yupeng Zhang",
      "Jun Zhu",
      "Xiaolin Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_DrVideo_Document_Retrieval_Based_Long_Video_Understanding_CVPR_2025_paper.html": {
    "title": "DrVideo: Document Retrieval Based Long Video Understanding",
    "volume": "main",
    "abstract": "Most of the existing methods for video understanding primarily focus on videos only lasting tens of seconds, with limited exploration of techniques for handling long videos. The increased number of frames in long videos poses two main challenges: difficulty in locating key information and performing long-range reasoning. Thus, we propose DrVideo, a document-retrieval-based system designed for long video understanding. Our key idea is to convert the long-video understanding problem into a long-document understanding task so as to effectively leverage the power of large language models. Specifically, DrVideo first transforms a long video into a coarse text-based long document to initially retrieve key frames and then updates the documents with the augmented key frame information. It then employs an agent-based iterative loop to continuously search for missing information and augment the document until sufficient question-related information is gathered for making the final predictions in a chain-of-thought manner. Extensive experiments on long video benchmarks confirm the effectiveness of our method. DrVideo significantly outperforms existing LLM-based state-of-the-art methods on EgoSchema benchmark (3 minutes), MovieChat-1K benchmark (10 minutes), and the long split of Video-MME benchmark (average of 44 minutes). Code is available at https://github.com/Upper9527/DrVideo",
    "checked": true,
    "id": "61a521d47d2ea646a447d50f6a6a64ada8ea62ea",
    "semantic_title": "drvideo: document retrieval based long video understanding",
    "citation_count": 15,
    "authors": [
      "Ziyu Ma",
      "Chenhui Gou",
      "Hengcan Shi",
      "Bin Sun",
      "Shutao Li",
      "Hamid Rezatofighi",
      "Jianfei Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Infighting_in_the_Dark_Multi-Label_Backdoor_Attack_in_Federated_Learning_CVPR_2025_paper.html": {
    "title": "Infighting in the Dark: Multi-Label Backdoor Attack in Federated Learning",
    "volume": "main",
    "abstract": "Federated Learning (FL), a privacy-preserving decentralized machine learning framework, has been shown to be vulnerable to backdoor attacks. Current research primarily focuses on the Single-Label Backdoor Attack (SBA), wherein adversaries share a consistent target. However, a critical fact is overlooked: adversaries may be non-cooperative, have distinct targets, and operate independently, which exhibits a more practical scenario called Multi-Label Backdoor Attack (MBA). Unfortunately, prior works are ineffective in the MBA scenario since non-cooperative attackers exclude each other. In this work, we conduct an in-depth investigation to uncover the inherent constraints of the exclusion: similar backdoor mappings are constructed for different targets, resulting in conflicts among backdoor functions. To address this limitation, we propose Mirage, the first non-cooperative MBA strategy in FL that allows attackers to inject effective and persistent backdoors into the global model without collusion by constructing in-distribution (ID) backdoor mapping. Specifically, we introduce an adversarial adaptation method to bridge the backdoor features and the target distribution in an ID manner. Additionally, we further leverage a constrained optimization method to ensure the ID mapping survives in the global training dynamics. Extensive evaluations demonstrate that Mirage outperforms various state-of-the-art attacks and bypasses existing defenses, achieving an average ASR greater than 97% and maintaining over 90% after 900 rounds. This work aims to alert researchers to this potential threat and inspire the design of effective defense mechanisms. Code has been made open-source",
    "checked": false,
    "id": "9efcff730248c2fb843bb966fe8f07923eaec018",
    "semantic_title": "infighting in the dark: multi-labels backdoor attack in federated learning",
    "citation_count": 0,
    "authors": [
      "Ye Li",
      "Yanchao Zhao",
      "Chengcheng Zhu",
      "Jiale Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kuang_Buffer_Anytime_Zero-Shot_Video_Depth_and_Normal_from_Image_Priors_CVPR_2025_paper.html": {
    "title": "Buffer Anytime: Zero-Shot Video Depth and Normal from Image Priors",
    "volume": "main",
    "abstract": "We present Buffer Anytime, a framework for estimation of depth and normal maps (which we call geometric buffers) from video that eliminates the need for paired video--depth and video--normal training data. Instead of relying on large-scale annotated video datasets, we demonstrate high-quality video buffer estimation by leveraging single-image priors with temporal consistency constraints. Our zero-shot training strategy combines state-of-the-art image estimation models based on optical flow smoothness through a hybrid loss function, implemented via a lightweight temporal attention architecture. Applied to leading image models like Depth Anything V2 and Marigold-E2E-FT, our approach significantly improves temporal consistency while maintaining accuracy. Experiments show that our method not only outperforms image-based approaches but also achieves results comparable to state-of-the-art video models trained on large-scale paired video datasets, despite using no such paired video data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengfei Kuang",
      "Tianyuan Zhang",
      "Kai Zhang",
      "Hao Tan",
      "Sai Bi",
      "Yiwei Hu",
      "Zexiang Xu",
      "Milos Hasan",
      "Gordon Wetzstein",
      "Fujun Luan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_PSHuman_Photorealistic_Single-image_3D_Human_Reconstruction_using_Cross-Scale_Multiview_Diffusion_CVPR_2025_paper.html": {
    "title": "PSHuman: Photorealistic Single-image 3D Human Reconstruction using Cross-Scale Multiview Diffusion and Explicit Remeshing",
    "volume": "main",
    "abstract": "Photorealistic 3D human modeling is essential for various applications and has seen tremendous progress. However, existing methods for monocular full-body reconstruction, typically relying on front and/or predicted back view, still struggle with satisfactory performance due to the ill-posed nature of the problem and sophisticated self-occlusions. In this paper, we propose PSHuman, a novel framework that explicitly reconstructs human meshes utilizing priors from the multiview diffusion model. It is found that directly applying multiview diffusion on single-view human images leads to severe geometric distortions, especially on generated faces. To address it, we propose a cross-scale diffusion that models the joint probability distribution of global full-body shape and local facial characteristics, enabling identity-preserved novel-view generation without geometric distortion. Moreover, to enhance cross-view body shape consistency of varied human poses, we condition the generative model on parametric models (SMPL-X), which provide body priors and prevent unnatural views inconsistent with human anatomy. Leveraging the generated multiview normal and color images, we present SMPLX-initialized explicit human carving to recover realistic textured human meshes efficiently. Extensive experiments on CAPE and THuman2.1 demonstrate PSHuman's superiority in geometry details, texture fidelity, and generalization capability",
    "checked": true,
    "id": "28b9c8cfd17159f925fb30acd8fff6269c76f60e",
    "semantic_title": "pshuman: photorealistic single-image 3d human reconstruction using cross-scale multiview diffusion and explicit remeshing",
    "citation_count": 5,
    "authors": [
      "Peng Li",
      "Wangguandong Zheng",
      "Yuan Liu",
      "Tao Yu",
      "Yangguang Li",
      "Xingqun Qi",
      "Xiaowei Chi",
      "Siyu Xia",
      "Yan-Pei Cao",
      "Wei Xue",
      "Wenhan Luo",
      "Yike Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_LSNet_See_Large_Focus_Small_CVPR_2025_paper.html": {
    "title": "LSNet: See Large, Focus Small",
    "volume": "main",
    "abstract": "Vision network designs, including Convolutional Neural Networks and Vision Transformers, have significantly advanced the field of computer vision. Yet, their complex computations pose challenges for practical deployments, particularly in real-time applications. To tackle this issue, researchers have explored various lightweight and efficient network designs. However, existing lightweight models predominantly leverage self-attention mechanisms and convolutions for token mixing. This dependence brings limitations in effectiveness and efficiency in the perception and aggregation processes of lightweight networks, hindering the balance between performance and efficiency under limited computational budgets. In this paper, we draw inspiration from the dynamic heteroscale vision ability inherent in the efficient human vision system and propose a \"See Large, Focus Small\" strategy for lightweight vision network design. We introduce LS (Large-Small) convolution, which combines large-kernel perception and small-kernel aggregation. It can efficiently capture a wide range of perceptual information and achieve precise feature aggregation for dynamic and complex visual representations, thus enabling proficient processing of visual information. Based on LS convolution, we present LSNet, a new family of lightweight models. Extensive experiments demonstrate that LSNet achieves superior performance and efficiency over existing lightweight networks in various vision tasks. Codes and models are available at https://github.com/jameslahm/lsnet",
    "checked": true,
    "id": "64bdc3bf4cd9a7f89274d1c6a1bde74cf14ed5bc",
    "semantic_title": "lsnet: see large, focus small",
    "citation_count": 0,
    "authors": [
      "Ao Wang",
      "Hui Chen",
      "Zijia Lin",
      "Jungong Han",
      "Guiguang Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_DynamicScaler_Seamless_and_Scalable_Video_Generation_for_Panoramic_Scenes_CVPR_2025_paper.html": {
    "title": "DynamicScaler: Seamless and Scalable Video Generation for Panoramic Scenes",
    "volume": "main",
    "abstract": "The increasing demand for immersive AR/VR applications and spatial intelligence has heightened the need to generate high-quality scene-level and 360deg panoramic video. However, most video diffusion models are constrained by limited resolution and aspect ratio, which restricts their applicability to scene-level dynamic content synthesis. In this work, we propose DynamicScaler, addressing these challenges by enabling spatially scalable and panoramic dynamic scene synthesis that preserves coherence across panoramic scenes of arbitrary size. Specifically, we introduce a Offset Shifting Denoiser, facilitating efficient, synchronous, and coherent denoising panoramic dynamic scenes via a diffusion model with fixed resolution through a seamless rotating Window, which ensures seamless boundary transitions and consistency across the entire panoramic space, accommodating varying resolutions and aspect ratios. Additionally, we employ a Global Motion Guidance mechanism to ensure both local detail fidelity and global motion continuity. Extensive experiments demonstrate our method achieves superior content and motion quality in panoramic scene-level video generation, offering a training-free, efficient, and scalable solution for immersive dynamic scene creation with constant VRAM consumption regardless of the output video resolution. Project page is available at https://dynamic-scaler.pages.dev/new",
    "checked": true,
    "id": "ef6201bef3feffada2b2fa2bc1f0a465429582d6",
    "semantic_title": "dynamicscaler: seamless and scalable video generation for panoramic scenes",
    "citation_count": 4,
    "authors": [
      "Jinxiu Liu",
      "Shaoheng Lin",
      "Yinxiao Li",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Tartan_IMU_A_Light_Foundation_Model_for_Inertial_Positioning_in_CVPR_2025_paper.html": {
    "title": "Tartan IMU: A Light Foundation Model for Inertial Positioning in Robotics",
    "volume": "main",
    "abstract": "Despite recent advances in deep learning, most existing learning IMU odometry methods are trained on specific datasets, lack generalization, and are prone to overfitting, which limits their real-world application. To address these challenges, we present Tartan IMU, a foundation model designed for generalizable, IMU-based state estimation across diverse robotic platforms. Our approach consists of three-stage: First, a pre-trained foundation model leverages over 100 hours of multi-platform data to establish general motion knowledge, achieving 36% improvement in ATE over specialized models. Second, to adapt to previously unseen tasks, we employ the Low-Rank Adaptation (LoRA), allowing positive transfer with only 1.1 M trainable parameters. Finally, to support robotics deployment, we introduce online test-time adaptation, which eliminates the boundary between training and testing, allowing the model to continuously \"learn as it operates\" at 200 FPS in real-time",
    "checked": true,
    "id": "cfda737f0a7de84151a0d7d1e30d6b29be91cc3c",
    "semantic_title": "tartan imu: a light foundation model for inertial positioning in robotics",
    "citation_count": 2,
    "authors": [
      "Shibo Zhao",
      "Sifan Zhou",
      "Raphael Blanchard",
      "Yuheng Qiu",
      "Wenshan Wang",
      "Sebastian Scherer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Maeda_Event_Ellipsometer_Event-based_Mueller-Matrix_Video_Imaging_CVPR_2025_paper.html": {
    "title": "Event Ellipsometer: Event-based Mueller-Matrix Video Imaging",
    "volume": "main",
    "abstract": "Light-matter interactions modify both the intensity and polarization state of light. Changes in polarization, represented by a Mueller matrix, encode detailed scene information. Existing optical ellipsometers capture Mueller-matrix images; however, they are often limited to static scenes due to long acquisition times. Here, we introduce Event Ellipsometer, a method for acquiring Mueller-matrix images of dynamic scenes. Our imaging system employs fast-rotating quarter-wave plates (QWPs) in front of a light source and an event camera that asynchronously captures intensity changes induced by the rotating QWPs. We develop an ellipsometric-event image formation model, a calibration method, and an ellipsometric-event reconstruction method. We experimentally demonstrate that Event Ellipsometer enables Mueller-matrix imaging at 30fps, extending ellipsometry to dynamic scenes",
    "checked": true,
    "id": "c2219496f43b4ced443c11f827000170bf989e5b",
    "semantic_title": "event ellipsometer: event-based mueller-matrix video imaging",
    "citation_count": 0,
    "authors": [
      "Ryota Maeda",
      "Yunseong Moon",
      "Seung-Hwan Baek"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liao_DocLayLLM_An_Efficient_Multi-modal_Extension_of_Large_Language_Models_for_CVPR_2025_paper.html": {
    "title": "DocLayLLM: An Efficient Multi-modal Extension of Large Language Models for Text-rich Document Understanding",
    "volume": "main",
    "abstract": "Text-rich document understanding (TDU) requires comprehensive analysis of documents containing substantial textual content and complex layouts. While Multimodal Large Language Models (MLLMs) have achieved fast progress in this domain, existing approaches either demand significant computational resources or struggle with effective multi-modal integration. In this paper, we introduce DocLayLLM, an efficient multi-modal extension of LLMs specifically designed for TDU. By lightly integrating visual patch tokens and 2D positional tokens into LLMs' input and encoding the document content using the LLMs themselves, we fully take advantage of the document comprehension capability of LLMs and enhance their perception of OCR information. We have also deeply considered the role of chain-of-thought (CoT) and innovatively proposed the techniques of CoT Pre-training and CoT Annealing. Our DocLayLLM can achieve remarkable performances with lightweight training settings, showcasing its efficiency and effectiveness. Experimental results demonstrate that our DocLayLLM outperforms existing OCR-dependent methods and OCR-free competitors. Code and model are available at https://github.com/whlscut/DocLayLLM",
    "checked": true,
    "id": "7cc132254e9d6bd2049383be4890b58be36954a8",
    "semantic_title": "doclayllm: an efficient multi-modal extension of large language models for text-rich document understanding",
    "citation_count": 0,
    "authors": [
      "Wenhui Liao",
      "Jiapeng Wang",
      "Hongliang Li",
      "Chengyu Wang",
      "Jun Huang",
      "Lianwen Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_EDEN_Enhanced_Diffusion_for_High-quality_Large-motion_Video_Frame_Interpolation_CVPR_2025_paper.html": {
    "title": "EDEN: Enhanced Diffusion for High-quality Large-motion Video Frame Interpolation",
    "volume": "main",
    "abstract": "Handling complex or nonlinear motion patterns has long posed challenges for video frame interpolation. Although recent advances in diffusion-based methods offer improvements over traditional optical flow-based approaches, they still struggle to generate sharp, temporally consistent frames in scenarios with large motion. To address this limitation, we introduce EDEN, an Enhanced Diffusion for high-quality large-motion vidEo frame iNterpolation. Our approach first utilizes a transformer-based tokenizer to produce refined latent representations of the intermediate frames for diffusion models. We then enhance the diffusion transformer with temporal attention across the process and incorporate a start-end frame difference embedding to guide the generation of dynamic motion. Extensive experiments demonstrate that EDEN achieves state-of-the-art results across popular benchmarks, including nearly a 10% LPIPS reduction on DAVIS and SNU-FILM, and an 8% improvement on DAIN-HD",
    "checked": true,
    "id": "8d1e463df564ebeaef915d35e7c927384b437f56",
    "semantic_title": "eden: enhanced diffusion for high-quality large-motion video frame interpolation",
    "citation_count": 2,
    "authors": [
      "Zihao Zhang",
      "Haoran Chen",
      "Haoyu Zhao",
      "Guansong Lu",
      "Yanwei Fu",
      "Hang Xu",
      "Zuxuan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Handling_Spatial-Temporal_Data_Heterogeneity_for_Federated_Continual_Learning_via_Tail_CVPR_2025_paper.html": {
    "title": "Handling Spatial-Temporal Data Heterogeneity for Federated Continual Learning via Tail Anchor",
    "volume": "main",
    "abstract": "Federated Continual Learning (FCL) allows each client to continually update its knowledge from task streams, enhancing the applicability of federated learning in real-world scenarios. However, FCL needs to address not only spatial data heterogeneity between clients but also temporal data heterogeneity between tasks. In this paper, empirical experiments demonstrate that such input-level heterogeneity significantly affects the model's internal parameters and outputs, leading to severe spatial-temporal catastrophic forgetting of local and previous knowledge. To this end, we propose Federated Tail Anchor (FedTA) to mix trainable Tail Anchor with the frozen output features to adjust their position in the feature space, thereby overcoming parameter-forgetting and output-forgetting. Three novel components are also included: Input Enhancement for improving the performance of pre-trained models on downstream tasks; Selective Input Knowledge Fusion for fusion of heterogeneous local knowledge on the server; and Best Global Prototype Selection for finding the best anchor point for each class in the feature space. Extensive experiments demonstrate that FedTA not only outperforms existing FCL methods but also effectively preserves the relative positions of features",
    "checked": true,
    "id": "2e30d26595cea3248beeb2a7210c563f6cb8d94a",
    "semantic_title": "handling spatial-temporal data heterogeneity for federated continual learning via tail anchor",
    "citation_count": 1,
    "authors": [
      "Hao Yu",
      "Xin Yang",
      "Le Zhang",
      "Hanlin Gu",
      "Tianrui Li",
      "Lixin Fan",
      "Qiang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peng_DeSiRe-GS_4D_Street_Gaussians_for_Static-Dynamic_Decomposition_and_Surface_Reconstruction_CVPR_2025_paper.html": {
    "title": "DeSiRe-GS: 4D Street Gaussians for Static-Dynamic Decomposition and Surface Reconstruction for Urban Driving Scenes",
    "volume": "main",
    "abstract": "We present DeSiRe-GS, a self-supervised gaussian splatting representation, enabling effective static-dynamic decomposition and high-fidelity surface reconstruction in complex driving scenarios. Our approach employs a two-stage optimization pipeline of dynamic street Gaussians. In the first stage, we extract 2D motion masks based on the observation that 3D Gaussian Splatting inherently can reconstruct only the static regions in dynamic environments. These extracted 2D motion priors are then mapped into the Gaussian space in a differentiable manner, leveraging an efficient formulation of dynamic Gaussians in the second stage. Combined with the introduced geometric regularizations, our method are able to address the over-fitting issues caused by data sparsity in autonomous driving, reconstructing physically plausible Gaussians that align with object surfaces rather than floating in air. Furthermore, we introduce temporal cross-view consistency to ensure coherence across time and viewpoints, resulting in high-quality surface reconstruction. Comprehensive experiments demonstrate the efficiency and effectiveness of DeSiRe-GS, surpassing prior self-supervised arts and achieving accuracy comparable to methods relying on external 3D bounding box annotations",
    "checked": true,
    "id": "82d74ad33892c0dd61dcc94b815cb6eb20cbf48e",
    "semantic_title": "desire-gs: 4d street gaussians for static-dynamic decomposition and surface reconstruction for urban driving scenes",
    "citation_count": 6,
    "authors": [
      "Chensheng Peng",
      "Chengwei Zhang",
      "Yixiao Wang",
      "Chenfeng Xu",
      "Yichen Xie",
      "Wenzhao Zheng",
      "Kurt Keutzer",
      "Masayoshi Tomizuka",
      "Wei Zhan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_End-to-End_HOI_Reconstruction_Transformer_with_Graph-based_Encoding_CVPR_2025_paper.html": {
    "title": "End-to-End HOI Reconstruction Transformer with Graph-based Encoding",
    "volume": "main",
    "abstract": "Human-object interaction (HOI) reconstruction has garnered significant attention due to its diverse applications and the success of capturing human meshes. Existing HOI reconstruction methods often rely on explicitly modeling interactions between humans and objects. However, such a way leads to a natural conflict between 3D mesh reconstruction, which emphasizes global structure, and fine-grained contact reconstruction, which focuses on local details. To address the limitations of explicit modeling, we propose the End-to-End HOI Reconstruction Transformer with Graph-based Encoding (HOI-TG). It implicitly learns the interaction between humans and objects by leveraging self-attention mechanisms. Within the transformer architecture, we devise graph residual blocks to aggregate the topology among vertices of different spatial structures. This dual focus effectively balances global and local representations. Without bells and whistles, HOI-TG achieves state-of-the-art performance on BEHAVE and InterCap datasets. Particularly on the challenging InterCap dataset, our method improves the reconstruction results for human and object meshes by 8.9% and 8.6%, respectively",
    "checked": true,
    "id": "07f18a8165615d8b72fde83ab914d3bb2d978a7a",
    "semantic_title": "end-to-end hoi reconstruction transformer with graph-based encoding",
    "citation_count": 0,
    "authors": [
      "Zhenrong Wang",
      "Qi Zheng",
      "Sihan Ma",
      "Maosheng Ye",
      "Yibing Zhan",
      "Dongjiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_REWIND_Real-Time_Egocentric_Whole-Body_Motion_Diffusion_with_Exemplar-Based_Identity_Conditioning_CVPR_2025_paper.html": {
    "title": "REWIND: Real-Time Egocentric Whole-Body Motion Diffusion with Exemplar-Based Identity Conditioning",
    "volume": "main",
    "abstract": "We present REWIND (Real-Time Egocentric Whole-Body Motion Diffusion), a one-step diffusion model for real-time, high-fidelity human motion estimation from egocentric image inputs. While an existing method for egocentric whole-body (i.e., body and hands) motion estimation is non-real-time and acausal due to diffusion-based iterative motion refinement to capture correlations between body and hand poses, REWIND operates in a fully causal and real-time manner. To enable real-time inference, we introduce (1) cascaded body-hand denoising diffusion, which effectively models the correlation between egocentric body and hand motions in a fast, feed-forward manner, and (2) diffusion distillation, which enables high-quality motion estimation with a single denoising step. Our denoising diffusion model is based on a modified Transformer architecture, designed to causally model output motions while enhancing generalizability to unseen motion lengths. Additionally, REWIND optionally supports identity-conditioned motion estimation when identity prior is available. To this end, we propose a novel identity conditioning method based on a small set of pose exemplars of the target identity, which further enhances motion estimation quality. Through extensive experiments, we demonstrate that REWIND significantly outperforms the existing baselines both with and without exemplar-based identity conditioning",
    "checked": true,
    "id": "56c639ab6894573e7adcfe5f7a1b36c0ea9c812c",
    "semantic_title": "rewind: real-time egocentric whole-body motion diffusion with exemplar-based identity conditioning",
    "citation_count": 0,
    "authors": [
      "Jihyun Lee",
      "Weipeng Xu",
      "Alexander Richard",
      "Shih-En Wei",
      "Shunsuke Saito",
      "Shaojie Bai",
      "Te-Li Wang",
      "Minhyuk Sung",
      "Tae-Kyun Kim",
      "Jason Saragih"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Hiding_Images_in_Diffusion_Models_by_Editing_Learned_Score_Functions_CVPR_2025_paper.html": {
    "title": "Hiding Images in Diffusion Models by Editing Learned Score Functions",
    "volume": "main",
    "abstract": "Hiding data using neural networks (i.e., neural steganography) has achieved remarkable success across both discriminative classifiers and generative adversarial networks. However, the potential of data hiding in diffusion models remains relatively unexplored. Current methods exhibit limitations in achieving high extraction accuracy, model fidelity, and hiding efficiency due primarily to the entanglement of the hiding and extraction processes with multiple denoising diffusion steps. To address these, we describe a simple yet effective approach that embeds images at specific timesteps in the reverse diffusion process by editing the learned score functions. Additionally, we introduce a parameter-efficient fine-tuning method that combines gradient-based parameter selection with low-rank adaptation to enhance model fidelity and hiding efficiency. Comprehensive experiments demonstrate that our method extracts high-quality images at human-indistinguishable levels, replicates the original model behaviors at both sample and population levels, and embeds images orders of magnitude faster than prior methods. Besides, our method naturally supports multi-recipient scenarios through independent extraction channels",
    "checked": true,
    "id": "0488570d7d30f43ac4d4ce88ce8fc3887e20fd43",
    "semantic_title": "hiding images in diffusion models by editing learned score functions",
    "citation_count": 0,
    "authors": [
      "Haoyu Chen",
      "Yunqiao Yang",
      "Nan Zhong",
      "Kede Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pang_Disco4D_Disentangled_4D_Human_Generation_and_Animation_from_a_Single_CVPR_2025_paper.html": {
    "title": "Disco4D: Disentangled 4D Human Generation and Animation from a Single Image",
    "volume": "main",
    "abstract": "We present Disco4D, a novel Gaussian Splatting framework for 4D human generation and animation from a single image. Different from existing methods, Disco4D distinctively disentangles clothings (with Gaussian models) from the human body (with SMPL-X model), significantly enhancing the generation details and flexibility. It has the following technical innovations. (1) Disco4D learns to efficiently fit the clothing Gaussians over the SMPL-X Gaussians. (2) It adopts diffusion models to enhance the 3D generation process, e.g. modeling occluded parts not visible in the input image. (3) It learns an identity encoding for each clothing Gaussian to facilitate the separation and extraction of clothing assets. Furthermore, Disco4D naturally supports 4D human animation with vivid dynamics. Extensive experiments demonstrate the superiority of Disco4D on 4D human generation and animation tasks",
    "checked": true,
    "id": "2e7a0bf4dfe68ca87a0795ed5e8e34ff4601aca5",
    "semantic_title": "disco4d: disentangled 4d human generation and animation from a single image",
    "citation_count": 3,
    "authors": [
      "Hui En Pang",
      "Shuai Liu",
      "Zhongang Cai",
      "Lei Yang",
      "Tianwei Zhang",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_DoraCycle_Domain-Oriented_Adaptation_of_Unified_Generative_Model_in_Multimodal_Cycles_CVPR_2025_paper.html": {
    "title": "DoraCycle: Domain-Oriented Adaptation of Unified Generative Model in Multimodal Cycles",
    "volume": "main",
    "abstract": "Adapting generative models to specific domains presents an effective solution for satisfying specialized requirements. However, adapting to some complex domains remains challenging, especially when these domains require substantial paired data to capture the targeted distributions. Since unpaired data from a single modality, such as vision or language, is more readily available, we utilize the bidirectional mappings between vision and language learned by the unified generative model to enable training on unpaired data for domain adaptation. Specifically, we propose DoraCycle, which integrates two multimodal cycles: text-to-image-to-text and image-to-text-to-image. The model is optimized through cross-entropy loss computed at the cycle endpoints, where both endpoints share the same modality. This facilitates self-evolution of the model without reliance on annotated text-image pairs. Experimental results demonstrate that for tasks independent of paired knowledge, such as stylization, DoraCycle can effectively adapt the unified model using only unpaired data. For tasks involving new paired knowledge, such as specific identities, a combination of a small set of paired image-text examples and larger-scale unpaired data is sufficient for effective domain-oriented adaptation. The code will be released at https://github.com/showlab/DoraCycle",
    "checked": true,
    "id": "86d181c0a4f2163573af5fd1eff23dfd4dcf822d",
    "semantic_title": "doracycle: domain-oriented adaptation of unified generative model in multimodal cycles",
    "citation_count": 1,
    "authors": [
      "Rui Zhao",
      "Weijia Mao",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_WeatherGen_A_Unified_Diverse_Weather_Generator_for_LiDAR_Point_Clouds_CVPR_2025_paper.html": {
    "title": "WeatherGen: A Unified Diverse Weather Generator for LiDAR Point Clouds via Spider Mamba Diffusion",
    "volume": "main",
    "abstract": "3D scene perception demands a large amount of adverse-weather LiDAR data, yet the cost of LiDAR data collection presents a significant scaling-up challenge. To this end, a series of LiDAR simulators have been proposed. Yet, they can only simulate a single adverse weather with a single physical model, and the fidelity is quite limited. This paper presents **WeatherGen**, the first unified diverse-weather LiDAR data diffusion generation framework, significantly improving fidelity. Specifically, we first design a map-based data producer, which is capable of providing a vast amount of high-quality diverse-weather data for training purposes. Then, we utilize the diffusion-denoising paradigm to construct a diffusion model. Among them, we propose a spider mamba generator with the spider mamba scan to restore the disturbed diverse weather data gradually. The spider mamba models the feature interactions by scanning the LiDAR beam circle and central ray, excellently maintaining the physical structure of the LiDAR point cloud. Subsequently, we design a latent domain aligner following the generator to transfer real-world knowledge. Afterward, we devise a contrastive learning-based controller, which equips weather control signals with compact semantic knowledge through language supervision from CLIP, guiding the diffusion model in generating more discriminative data. Finally, we fine-tune WeatherGen with small-scale real-world data to further enhance its performance. Extensive evaluations on KITTI-360 and Seeing Through Fog demonstrate the high generation quality of WeatherGen. Through WeatherGen, we construct the mini-weather dataset, promoting the performance of the downstream task under adverse weather conditions",
    "checked": true,
    "id": "4b0264fc66e3c2ad162fa039eb24e691857ba77c",
    "semantic_title": "weathergen: a unified diverse weather generator for lidar point clouds via spider mamba diffusion",
    "citation_count": 0,
    "authors": [
      "Yang Wu",
      "Yun Zhu",
      "Kaihua Zhang",
      "Jianjun Qian",
      "Jin Xie",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qin_MUST_The_First_Dataset_and_Unified_Framework_for_Multispectral_UAV_CVPR_2025_paper.html": {
    "title": "MUST: The First Dataset and Unified Framework for Multispectral UAV Single Object Tracking",
    "volume": "main",
    "abstract": "UAV tracking faces significant challenges in real-world scenarios, such as small-size targets and occlusions, which limit the performance of RGB-based trackers. Multispectral images (MSI), which capture additional spectral information, offer a promising solution to these challenges. However, progress in this field has been hindered by the lack of relevant datasets. To address this gap, we introduce the first large-scale Multispectral UAV Single Object Tracking dataset (MUST), which includes 250 video sequences spanning diverse environments and challenges, providing a comprehensive data foundation for multispectral UAV tracking. We also propose a novel tracking framework, UNTrack, which encodes unified spectral, spatial, and temporal features from spectrum prompts, initial templates, and sequential searches. UNTrack employs an asymmetric transformer with a spectral background eliminate mechanism for optimal relationship modeling and an encoder that continuously updates the spectrum prompt to refine tracking, improving both accuracy and efficiency. Extensive experiments show that our proposed UNTrack outperforms state-of-the-art UAV trackers. We believe our dataset and framework will drive future research in this area. The dataset is available on https://github.com/q2479036243/MUST-Multispectral-UAV-Single-Object-Tracking",
    "checked": true,
    "id": "7c09fce77f682e392bae1eb02d1b81a2c14b381e",
    "semantic_title": "must: the first dataset and unified framework for multispectral uav single object tracking",
    "citation_count": 0,
    "authors": [
      "Haolin Qin",
      "Tingfa Xu",
      "Tianhao Li",
      "Zhenxiang Chen",
      "Tao Feng",
      "Jianan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhuang_IDOL_Instant_Photorealistic_3D_Human_Creation_from_a_Single_Image_CVPR_2025_paper.html": {
    "title": "IDOL: Instant Photorealistic 3D Human Creation from a Single Image",
    "volume": "main",
    "abstract": "Creating a high-fidelity, animatable 3D full-body avatar from a single image is a challenging task due to the diverse appearance and poses of humans and the limited availability of high-quality training data. To achieve fast and high-quality human reconstruction, this work rethinks the task from the perspectives of dataset, model, and representation. First, we introduce a large-scale HUman GEnerated training dataset, HuGe100K, consisting of 100K diverse, photorealistic human images with corresponding 24-view in a static pose or dynamic pose frames generated via a pose-controllable image-to-video model. Next, leveraging the diversity in views, poses, and appearances within HuGe100K, we develop a scalable feed-forward transformer model to predict a 3D human Gaussian representation in a uniform space of a given human image. This model is trained to disentangle human pose, shape, clothing geometry, and texture. Accordingly, the estimated Gaussians can be animated robustly without post-processing. We conduct comprehensive experiments to validate the effectiveness of the proposed dataset and method. Our model demonstrates the generalizable ability to efficiently reconstruct photorealistic humans in under 1 second using a single GPU. Additionally, it seamlessly supports various applications, including animation, shape, and texture editing tasks",
    "checked": true,
    "id": "97399c7dfb530190dd73da37ec3f4ea30bf177e2",
    "semantic_title": "idol: instant photorealistic 3d human creation from a single image",
    "citation_count": 11,
    "authors": [
      "Yiyu Zhuang",
      "Jiaxi Lv",
      "Hao Wen",
      "Qing Shuai",
      "Ailing Zeng",
      "Hao Zhu",
      "Shifeng Chen",
      "Yujiu Yang",
      "Xun Cao",
      "Wei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_Tightening_Robustness_Verification_of_MaxPool-based_Neural_Networks_via_Minimizing_the_CVPR_2025_paper.html": {
    "title": "Tightening Robustness Verification of MaxPool-based Neural Networks via Minimizing the Over-Approximation Zone",
    "volume": "main",
    "abstract": "The robustness of neural network classifiers is important in the safety-critical domain and can be quantified by robustness verification. At present, efficient and scalable verification techniques are always sound but incomplete, and thus, the improvement of verified robustness results is the key criterion to evaluate the performance of incomplete verification approaches. The multi-variate function MaxPool is widely adopted yet challenging to verify. In this paper, we present Ti-Lin, a robustness verifier for MaxPool-based CNNs with Tight Linear Approximation. Following the sequel of minimizing the over-approximation zone of the non-linear function of CNNs, we are the first to propose the provably neuron-wise tightest linear bounds for the MaxPool function. By our proposed linear bounds, we can certify larger robustness results for CNNs. We evaluate the effectiveness of Ti-Lin on different verification frameworks with open-sourced benchmarks, including LeNet, PointNet, and networks trained on the MNIST, CIFAR-10, Tiny ImageNet and ModelNet40 datasets. Experimental results show that Ti-Lin significantly outperforms the state-of-the-art methods across all networks with up to 78.6% improvement in terms of the certified accuracy with almost the same time consumption as the fastest tool. Our code is available at https://anonymous.4open.science/r/Ti-Lin-cvpr-72EE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Xiao",
      "Yuchen Chen",
      "Shiqing Ma",
      "Chunrong Fang",
      "Tongtong Bai",
      "Mingzheng Gu",
      "Yuxin Cheng",
      "Yanwei Chen",
      "Zhenyu Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_SketchVideo_Sketch-based_Video_Generation_and_Editing_CVPR_2025_paper.html": {
    "title": "SketchVideo: Sketch-based Video Generation and Editing",
    "volume": "main",
    "abstract": "Video generation and editing conditioned on text prompts or images have undergone significant advancements. However, challenges remain in accurately controlling global layout and geometry details solely by texts, and supporting motion control and local modification through images. In this paper, we aim to achieve sketch-based spatial and motion control for video generation and support fine-grained editing of real or synthetic videos. Based on the DiT video generation model, we propose a memory-efficient control structure with sketch control blocks that predict residual features of skipped DiT blocks. Sketches are drawn on one or two keyframes (at arbitrary time points) for easy interaction. To propagate such temporally sparse sketch conditions across all frames, we propose an inter-frame attention mechanism to analyze the relationship between the keyframes and each video frame. For sketch-based video editing, we design an additional video insertion module that maintains consistency between the newly edited content and the original video's spatial feature and dynamic motion. During inference, we use latent fusion for the accurate preservation of unedited regions. Extensive experiments demonstrate that our SketchVideo achieves superior performance in controllable video generation and editing",
    "checked": true,
    "id": "2edbb260e2fcc3a46bd01d48adeef18a11674b26",
    "semantic_title": "sketchvideo: sketch-based video generation and editing",
    "citation_count": 0,
    "authors": [
      "Feng-Lin Liu",
      "Hongbo Fu",
      "Xintao Wang",
      "Weicai Ye",
      "Pengfei Wan",
      "Di Zhang",
      "Lin Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Spitznagel_PhysicsGen_Can_Generative_Models_Learn_from_Images_to_Predict_Complex_CVPR_2025_paper.html": {
    "title": "PhysicsGen: Can Generative Models Learn from Images to Predict Complex Physical Relations?",
    "volume": "main",
    "abstract": "The image-to-image translation abilities of generative learning models have recently made significant progress in the estimation of complex (steered) mappings between image distributions. While appearance based tasks like image in-painting or style transfer have been studied at length, we propose to investigate the potential of generative models in the context of physical simulations. Providing a dataset of 300k image-pairs and baseline evaluations for three different physical simulation tasks, we propose a benchmark to investigate the following research questions: i) are generative models able to learn complex physical relations from input-output image pairs? ii) what speedups can be achieved by replacing differential equation based simulations? While baseline evaluations of different current models show the potential for high speedups (ii), these results also show strong limitations toward the physical correctness (i). This underlines the need for new methods to enforce physical correctness",
    "checked": true,
    "id": "adba20dcc4509ba2dab1ff4ae23acb57f8fce746",
    "semantic_title": "physicsgen: can generative models learn from images to predict complex physical relations?",
    "citation_count": 0,
    "authors": [
      "Martin Spitznagel",
      "Jan Vaillant",
      "Janis Keuper"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Taste_More_Taste_Better_Diverse_Data_and_Strong_Model_Boost_CVPR_2025_paper.html": {
    "title": "Taste More, Taste Better: Diverse Data and Strong Model Boost Semi-Supervised Crowd Counting",
    "volume": "main",
    "abstract": "Semi-supervised crowd counting is crucial for addressing the high annotation costs of densely populated scenes. Although several methods based on pseudo-labeling have been proposed, it remains challenging to effectively and accurately utilize unlabeled data. In this paper, we propose a novel framework called Taste More Taste Better (TMTB), which emphasizes both data and model aspects. Firstly, we explore a data augmentation technique well-suited for the crowd counting task. By inpainting the background regions, this technique can effectively enhance data diversity while preserving the fidelity of the entire scenes. Secondly, we introduce the Visual State Space Model as backbone to capture the global context information from crowd scenes, which is crucial for extremely crowded, low-light, and adverse weather scenarios. In addition to the traditional regression head for exact prediction, we employ an Anti-Noise classification head to provide less exact but more accurate supervision, since the regression head is sensitive to noise in manual annotations. We conduct extensive experiments on four benchmark datasets and show that our method outperforms state-of-the-art methods by a large margin. Code is publicly available on https://github.com/syhien/taste_more_taste_better",
    "checked": true,
    "id": "398a1f99435f912fcc6589815498bb37280d5bc1",
    "semantic_title": "taste more, taste better: diverse data and strong model boost semi-supervised crowd counting",
    "citation_count": 0,
    "authors": [
      "Maochen Yang",
      "Zekun Li",
      "Jian Zhang",
      "Lei Qi",
      "Yinghuan Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_Gaussian_Splashing_Unified_Particles_for_Versatile_Motion_Synthesis_and_Rendering_CVPR_2025_paper.html": {
    "title": "Gaussian Splashing: Unified Particles for Versatile Motion Synthesis and Rendering",
    "volume": "main",
    "abstract": "We demonstrate the feasibility of integrating physics-based animations of solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in virtual scenes reconstructed using 3DGS. Leveraging the coherence of the Gaussian Splatting and Position-Based Dynamics (PBD) in the underlying representation, we manage rendering, view synthesis, and the dynamics of solids and fluids in a cohesive manner. Similar to GaussianShader, we enhance each Gaussian kernel with an added normal, aligning the kernel's orientation with the surface normal to refine the PBD simulation. This approach effectively eliminates spiky noises that arise from rotational deformation in solids. It also allows us to integrate physically based rendering to augment the dynamic surface reflections on fluids. Consequently, our framework is capable of realistically reproducing surface highlights on dynamic fluids and facilitating interactions between scene objects and fluids from new views",
    "checked": true,
    "id": "647cd7fa76183b31175ff63c305ef2ed67c8866b",
    "semantic_title": "gaussian splashing: unified particles for versatile motion synthesis and rendering",
    "citation_count": 8,
    "authors": [
      "Yutao Feng",
      "Xiang Feng",
      "Yintong Shang",
      "Ying Jiang",
      "Chang Yu",
      "Zeshun Zong",
      "Tianjia Shao",
      "Hongzhi Wu",
      "Kun Zhou",
      "Chenfanfu Jiang",
      "Yin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Improve_Representation_for_Imbalanced_Regression_through_Geometric_Constraints_CVPR_2025_paper.html": {
    "title": "Improve Representation for Imbalanced Regression through Geometric Constraints",
    "volume": "main",
    "abstract": "In representation learning, uniformity refers to the uniform feature distribution in the latent space (i.e., unit hypersphere). Previous work has shown that improving uniformity contributes to the learning of under-represented classes. However, most of the previous work focused on classification; the representation space of imbalanced regression remains unexplored. Classification-based methods are not suitable for regression tasks because they cluster features into distinct groups without considering the continuous and ordered nature essential for regression. In a geometric aspect, we uniquely focus on ensuring uniformity in the latent space for imbalanced regression through two key losses: enveloping and homogeneity. The enveloping loss encourages the induced trace to uniformly occupy the surface of a hypersphere, while the homogeneity loss ensures smoothness, with representations evenly spaced at consistent intervals. Our method integrates these geometric principles into the data representations via a Surrogate-driven Representation Learning (SRL) framework. Experiments with real-world regression and operator learning tasks highlight the importance of uniformity in imbalanced regression and validate the efficacy of our geometry-based loss functions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijian Dong",
      "Yilei Wu",
      "Chongyao Chen",
      "Yingtian Zou",
      "Yichi Zhang",
      "Juan Helen Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_AnyDressing_Customizable_Multi-Garment_Virtual_Dressing_via_Latent_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "AnyDressing: Customizable Multi-Garment Virtual Dressing via Latent Diffusion Models",
    "volume": "main",
    "abstract": "Recent advances in garment-centric image generation from text and image prompts based on diffusion models are impressive. However, existing methods lack support for various combinations of attire, and struggle to preserve the garment details while maintaining faithfulness to the text prompts, limiting their performance across diverse scenarios. In this paper, we focus on a new task, i.e., Multi-Garment Virtual Dressing, and we propose a novel AnyDressing method for customizing characters conditioned on any combination of garments and any personalized text prompts. AnyDressing primarily comprises two primary networks named GarmentsNet and DressingNet, which are respectively dedicated to extracting detailed clothing features and generating customized images. Specifically, we propose an efficient and scalable module called Garment-Specific Feature Extractor in GarmentsNet to individually encode garment textures in parallel. This design prevents garment confusion while ensuring network efficiency. Meanwhile, we design an adaptive Dressing-Attention mechanism and a novel Instance-Level Garment Localization Learning strategy in DressingNet to accurately inject multi-garment features into their corresponding regions. This approach efficiently integrates multi-garment texture cues into generated images and further enhances text-image consistency. Additionally, we introduce a Garment-Enhanced Texture Learning strategy to improve the fine-grained texture details of garments. Thanks to our well-craft design, AnyDressing can serve as a plug-in module to easily integrate with any community control extensions for diffusion models, improving the diversity and controllability of synthesized images. Extensive experiments show that AnyDressing achieves state-of-the-art results",
    "checked": true,
    "id": "085a905534925eaddcdafd7c36ad3439f52126b2",
    "semantic_title": "anydressing: customizable multi-garment virtual dressing via latent diffusion models",
    "citation_count": 4,
    "authors": [
      "Xinghui Li",
      "Qichao Sun",
      "Pengze Zhang",
      "Fulong Ye",
      "Zhichao Liao",
      "Wanquan Feng",
      "Songtao Zhao",
      "Qian He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bahri_Spectral_Informed_Mamba_for_Robust_Point_Cloud_Processing_CVPR_2025_paper.html": {
    "title": "Spectral Informed Mamba for Robust Point Cloud Processing",
    "volume": "main",
    "abstract": "State Space Models (SSMs) have shown significant promise in Natural Language Processing (NLP) and, more recently, computer vision. This paper introduces a new methodology leveraging Mamba and Masked Autoencoder (MAE) networks for point cloud data in both supervised and self-supervised learning. We propose three key contributions to enhance Mamba's capability in processing complex point cloud structures. First, we exploit the spectrum of a graph Laplacian to capture patch connectivity, defining an isometry-invariant traversal order that is robust to viewpoints and better captures shape manifolds than traditional 3D grid-based traversals. Second, we adapt segmentation via a recursive patch partitioning strategy informed by Laplacian spectral components, allowing finer integration and segment analysis. Third, we address token placement in MAE for Mamba by restoring tokens to their original positions, which preserves essential order and improves learning. Extensive experiments demonstrate our approach's improvements in classification, segmentation, and few-shot tasks over state-of-the-art (SOTA) baselines",
    "checked": true,
    "id": "0f4f1eb47ec009543d9cb6f295ec6c86eda3f195",
    "semantic_title": "spectral informed mamba for robust point cloud processing",
    "citation_count": 3,
    "authors": [
      "Ali Bahri",
      "Moslem Yazdanpanah",
      "Mehrdad Noori",
      "Sahar Dastani",
      "Milad Cheraghalikhani",
      "Gustavo Adolfo Vargas Hakim",
      "David Osowiechi",
      "Farzad Beizaee",
      "Ismail Ben Ayed",
      "Christian Desrosiers"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Souza_Latent_Space_Imaging_CVPR_2025_paper.html": {
    "title": "Latent Space Imaging",
    "volume": "main",
    "abstract": "Digital imaging systems have traditionally relied on brute-force measurement and processing of pixels arranged on regular grids. In contrast, the human visual system performs significant data reduction from the large number of photoreceptors to the optic nerve, effectively encoding visual information into a low-bandwidth latent space representation optimized for brain processing. Inspired by this, we propose a similar approach to advance artificial vision systems. Latent Space Imaging introduces a new paradigm that combines optics and software to encode image information directly into the semantically rich latent space of a generative model. This approach substantially reduces bandwidth and memory demands during image capture and enables a range of downstream tasks focused on the latent space.We validate this principle through an initial hardware prototype based on a single-pixel camera. By implementing an amplitude modulation scheme that encodes into the generative model's latent space, we achieve compression ratios ranging from 1:100 to 1:1000 during imaging, and up to 1:16384 for downstream applications. This approach leverages the model's intrinsic linear boundaries, demonstrating the potential of latent space imaging for highly efficient imaging hardware, adaptable future applications in high-speed imaging, and task-specific cameras with significantly reduced hardware complexity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matheus Souza",
      "Yidan Zheng",
      "Kaizhang Kang",
      "Yogeshwar Nath Mishra",
      "Qiang Fu",
      "Wolfgang Heidrich"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Balanced_Direction_from_Multifarious_Choices_Arithmetic_Meta-Learning_for_Domain_Generalization_CVPR_2025_paper.html": {
    "title": "Balanced Direction from Multifarious Choices: Arithmetic Meta-Learning for Domain Generalization",
    "volume": "main",
    "abstract": "Domain generalization is proposed to address distribution shift, arising from statistical disparities between training source and unseen target domains. The widely used first-order meta-learning algorithms demonstrate strong performance for domain generalization by leveraging the gradient matching theory, which aims to establish balanced parameters across source domains to reduce overfitting to any particular domain. However, our analysis reveals that there are actually numerous directions to achieve gradient matching, with current methods representing just one possible path. These methods actually overlook another critical factor that the balanced parameters should be close to the centroid of optimal parameters of each source domain. To address this, we propose a simple yet effective arithmetic meta-learning with arithmetic-weighted gradients. This approach, while adhering to the principles of gradient matching, promotes a more precise balance by estimating the centroid between domain-specific optimal parameters. Experimental results validate the effectiveness of our strategy. Our code is available at https://github.com/zzwdx/ARITH",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiran Wang",
      "Jian Zhang",
      "Lei Qi",
      "Yinghuan Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shin_Anatomical_Consistency_and_Adaptive_Prior-informed_Transformation_for_Multi-contrast_MR_Image_CVPR_2025_paper.html": {
    "title": "Anatomical Consistency and Adaptive Prior-informed Transformation for Multi-contrast MR Image Synthesis via Diffusion Model",
    "volume": "main",
    "abstract": "Multi-contrast magnetic resonance (MR) images offer critical diagnostic information but are limited by long scan times and high cost. While diffusion models (DMs) excel in medical image synthesis, they often struggle to maintain anatomical consistency and utilize the diverse characteristics of multi-contrast MR images effectively. We propose APT, a unified diffusion model designed to generate accurate and anatomically consistent multi-contrast MR images. APT introduces a mutual information fusion module and an anatomical consistency loss to preserve critical anatomical structures across multiple contrast inputs. To enhance synthesis, APT incorporates a two-stage inference process: in the first stage, a prior codebook provides coarse anatomical structures by selecting appropriate guidance based on precomputed similarity mappings and Bezier curve transformations. The second stage applies iterative unrolling with weighted averaging to refine the initial output, enhancing fine anatomical details and ensuring structural consistency. This approach enables the preservation of both global structures and local details, resulting in realistic and diagnostically valuable synthesized images. Extensive experiments on public multi-contrast MR brain images demonstrate that our approach significantly outperforms state-of-the-art methods. The source codes are available at https://github.com/yejees/APT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yejee Shin",
      "Yeeun Lee",
      "Hanbyol Jang",
      "Geonhui Son",
      "Hyeongyu Kim",
      "Dosik Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_BlobGEN-Vid_Compositional_Text-to-Video_Generation_with_Blob_Video_Representations_CVPR_2025_paper.html": {
    "title": "BlobGEN-Vid: Compositional Text-to-Video Generation with Blob Video Representations",
    "volume": "main",
    "abstract": "Existing video generation models struggle to follow complex text prompts and synthesize multiple objects, raising the need for additional grounding input for improved controllability. In this work, we propose to decompose videos into visual primitives -- blob video representation, a general representation for controllable video generation. Based on blob conditions, we develop a blob-grounded video diffusion model named BlobGEN-Vid that allows users to control object motions and fine-grained object appearance. In particular, we introduce a masked 3D attention module that effectively improves regional consistency across frames. In addition, we introduce a learnable module to interpolate text embeddings so that users can control semantics in specific frames and obtain smooth object transitions. We show that our framework is model-agnostic and build BlobGEN-Vid based on both U-Net and DiT-based video diffusion models. Extensive experimental results show that BlobGEN-Vid achieves superior zero-shot video generation ability and state-of-the-art layout controllability on multiple benchmarks. When combined with a Large Language Model for layout planning, our framework even outperforms proprietary text-to-video generators regarding compositional accuracy",
    "checked": true,
    "id": "5e096c6fdef7b26fe9227e4f1ddf41be8d87acc3",
    "semantic_title": "blobgen-vid: compositional text-to-video generation with blob video representations",
    "citation_count": 5,
    "authors": [
      "Weixi Feng",
      "Chao Liu",
      "Sifei Liu",
      "William Yang Wang",
      "Arash Vahdat",
      "Weili Nie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_D2SP_Dynamic_Dual-Stage_Purification_Framework_for_Dual_Noise_Mitigation_in_CVPR_2025_paper.html": {
    "title": "D2SP: Dynamic Dual-Stage Purification Framework for Dual Noise Mitigation in Vision-based Affective Recognition",
    "volume": "main",
    "abstract": "The current advancements in Dynamic Facial Expression Recognition (DFER) methods mainly focus on better capturing the spatial and temporal features of facial expressions. However, DFER datasets contain a substantial amount of noisy samples, and few have addressed the issue of handling this noise. We identified two types of noise: one is caused by low-quality data resulting from factors such as occlusion, dim lighting, and blurriness; the other arises from mislabeled data due to annotation bias by annotators. Addressing the two types of noise, we have meticulously crafted a Dynamic Dual-Stage Purification (D2SP) Framework. This initiative aims to dynamically purify the DFER datasets of these two types of noise, ensuring that only high-quality and correctly labeled data is used in the training process. To mitigate low-quality samples, we introduce the Coarse-Grained Pruning (CGP) stage, which computes sample weights and prunes those low-weight samples. After CGP, the Fine-Grained Correction (FGC) stage evaluates prediction stability to correct mislabeled data. Moreover, D2SP is conceived as a general and plug-and-play framework, tailored to integrate seamlessly with prevailing DFER methods. Extensive experiments covering prevalent DFER datasets and deploying multiple benchmark methods have substantiated D2SP's ability to significantly enhance performance metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Wang",
      "Xinji Mai",
      "Zeng Tao",
      "Xuan Tong",
      "Junxiong Lin",
      "Yan Wang",
      "Jiawen Yu",
      "Shaoqi Yan",
      "Ziheng Zhou",
      "Wenqiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_PartRM_Modeling_Part-Level_Dynamics_with_Large_Cross-State_Reconstruction_Model_CVPR_2025_paper.html": {
    "title": "PartRM: Modeling Part-Level Dynamics with Large Cross-State Reconstruction Model",
    "volume": "main",
    "abstract": "As interest grows in world models that predict future states from current observations and actions, accurately modeling part-level dynamics has become increasingly relevant for various applications. Existing approaches, such as Puppet-Master, rely on fine-tuning large-scale pre-trained video diffusion models, which are impractical for real-world use due to the limitations of 2D video representation and slow processing times. To overcome these challenges, we present PartRM, a novel 4D reconstruction framework that simultaneously models appearance, geometry, and part-level motion from multi-view images of a static object. PartRM builds upon large 3D Gaussian reconstruction models, leveraging their extensive knowledge of appearance and geometry in static objects. To address data scarcity in 4D, we introduce the PartDrag-4D dataset, providing multi-view observations of part-level dynamics across over 20,000 states. We enhance the model's understanding of interaction conditions with a multi-scale drag embedding module that captures dynamics at varying granularities. To prevent catastrophic forgetting during fine-tuning, we implement a two-stage training process that focuses sequentially on motion and appearance learning. Experimental results show that PartRM establishes a new state-of-the-art in part-level motion learning and can be applied in manipulation tasks in robotics. Our code, data, and models are publicly available to facilitate future research",
    "checked": true,
    "id": "bb8fc303bfce1d61f2a3f703a0ac65cfed6be370",
    "semantic_title": "partrm: modeling part-level dynamics with large cross-state reconstruction model",
    "citation_count": 2,
    "authors": [
      "Mingju Gao",
      "Yike Pan",
      "Huan-ang Gao",
      "Zongzheng Zhang",
      "Wenyi Li",
      "Hao Dong",
      "Hao Tang",
      "Li Yi",
      "Hao Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_LaVin-DiT_Large_Vision_Diffusion_Transformer_CVPR_2025_paper.html": {
    "title": "LaVin-DiT: Large Vision Diffusion Transformer",
    "volume": "main",
    "abstract": "This paper presents the Large Vision Diffusion Transformer (LaVin-DiT), a scalable and unified foundation model designed to tackle over 20 computer vision tasks in a generative framework. Unlike existing large vision models directly adapted from natural language processing architectures, which rely on less efficient autoregressive techniques and disrupt spatial relationships essential for vision data, LaVin-DiT introduces key innovations to optimize generative performance for vision tasks. First, to address the high dimensionality of visual data, we incorporate a spatial-temporal variational autoencoder that encodes data into a continuous latent space. Second, for generative modeling, we develop a joint diffusion transformer that progressively produces vision outputs. Third, for unified multi-task training, in-context learning is implemented. Input-target pairs serve as task context, which guides the diffusion transformer to align outputs with specific tasks within the latent space. During inference, a task-specific context set and test data as queries allow LaVin-DiT to generalize across tasks without fine-tuning. Trained on extensive vision datasets, the model is scaled from 0.1B to 3.4B parameters, demonstrating substantial scalability and state-of-the-art performance across diverse vision tasks. This work introduces a novel pathway for large vision foundation models, underscoring the promising potential of diffusion transformers. The code and models are available at https://derrickwang005.github.io/LaVin-DiT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoqing Wang",
      "Xiaobo Xia",
      "Runnan Chen",
      "Dongdong Yu",
      "Changhu Wang",
      "Mingming Gong",
      "Tongliang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_DiffFNO_Diffusion_Fourier_Neural_Operator_CVPR_2025_paper.html": {
    "title": "DiffFNO: Diffusion Fourier Neural Operator",
    "volume": "main",
    "abstract": "We introduce DiffFNO, a novel diffusion framework for arbitrary-scale super-resolution strengthened by a Weighted Fourier Neural Operator (WFNO). Mode Rebalancing in WFNO effectively captures critical frequency components, significantly improving the reconstruction of high-frequency image details that are crucial for super-resolution tasks. Gated Fusion Mechanism (GFM) adaptively complements WFNO's spectral features with spatial features from an Attention-based Neural Operator (AttnNO). This enhances the network's capability to capture both global structures and local details. Adaptive Time-Step (ATS) ODE solver, a deterministic sampling strategy, accelerates inference without sacrificing output quality by dynamically adjusting integration step sizes ATS. Extensive experiments demonstrate that DiffFNO achieves state-of-the-art (SOTA) results, outperforming existing methods across various scaling factors by a margin of 2-4 dB in PSNR, including those beyond the training distribution. It also achieves this at lower inference time. Our approach sets a new standard in super-resolution, delivering both superior accuracy and computational efficiency",
    "checked": true,
    "id": "f7b5af8dffa609418b5335b48d4f6b1d4e792e8e",
    "semantic_title": "difffno: diffusion fourier neural operator",
    "citation_count": 2,
    "authors": [
      "Xiaoyi Liu",
      "Hao Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_CAP-Net_A_Unified_Network_for_6D_Pose_and_Size_Estimation_CVPR_2025_paper.html": {
    "title": "CAP-Net: A Unified Network for 6D Pose and Size Estimation of Categorical Articulated Parts from a Single RGB-D Image",
    "volume": "main",
    "abstract": "This paper tackles category-level pose estimation of ar- ticulated objects in robotic manipulation tasks and intro- duces a new benchmark dataset. While recent methods es- timate part poses and sizes at the category level, they often rely on geometric cues and complex multi-stage pipelines that first segment parts from the point cloud, followed by Normalized Part Coordinate Space (NPCS) estimation for 6D poses. These approaches overlook dense semantic cues from RGB images, leading to suboptimal accuracy, partic- ularly for objects with small parts. To address these limita- tions, we propose a single-stage Network, CAP-Net, for es- timating the 6D poses and sizes of Categorical Articulated Parts. This method combines RGB-D features to generate instance segmentation and NPCS representations for each part in an end-to-end manner. CAP-Net uses a unified net- work to simultaneously predict point-wise class labels, cen- troid offsets, and NPCS maps. A clustering algorithm then groups points of the same predicted class based on their es- timated centroid distances to isolate each part. Finally, the NPCS region of each part is aligned with the point cloud to recover its final pose and size. To bridge the sim-to-real do- main gap, we introduce the RGBD-Art dataset, the largest RGB-D articulated dataset to date, featuring photorealistic RGB images and depth noise simulated from real sensors. Experimental evaluations on the RGBD-Art dataset demon- strate that our method significantly outperforms the state- of-the-art approach. Real-world deployments of our model in robotic tasks underscore its robustness and exceptional sim-to-real transfer capabilities, confirming its substantial practical utility",
    "checked": true,
    "id": "aa5e10598b70c286e656076d802953074ac3c1cb",
    "semantic_title": "cap-net: a unified network for 6d pose and size estimation of categorical articulated parts from a single rgb-d image",
    "citation_count": 0,
    "authors": [
      "Jingshun Huang",
      "Haitao Lin",
      "Tianyu Wang",
      "Yanwei Fu",
      "Xiangyang Xue",
      "Yi Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SeCap_Self-Calibrating_and_Adaptive_Prompts_for_Cross-view_Person_Re-Identification_in_CVPR_2025_paper.html": {
    "title": "SeCap: Self-Calibrating and Adaptive Prompts for Cross-view Person Re-Identification in Aerial-Ground Networks",
    "volume": "main",
    "abstract": "When discussing the Aerial-Ground Person Re-identification (AGPReID) task, we face the main challenge of the significant appearance variations caused by different viewpoints, making identity matching difficult. To address this issue, previous methods attempt to reduce the differences between viewpoints by critical attributes and decoupling the viewpoints. While these methods can mitigate viewpoint differences to some extent, they still face two main issues: (1) difficulty in handling viewpoint diversity and (2) neglect of the contribution of local features. To effectively address these challenges, we design and implement the Self-Calibrating and Adaptive Prompt (SeCap) method for the AGPReID task. The core of this framework relies on the Prompt Re-calibration Module (PRM), which adaptively re-calibrates prompts based on the input. Combined with the Local Feature Refinement Module (LFRM), SeCap can extract view-invariant features from local features for AGPReID. Meanwhile, given the current scarcity of datasets in the AGPReID field, we further contribute two real-world Large-scale Aerial-Ground Person Re-Identification datasets, LAGPeR and G2APS-ReID. The former is collected and annotated by us independently, covering 4,231 unique identities and containing 63,841 high-quality images; the latter is reconstructed from the person search dataset G2APS. Through extensive experiments on AGPReID datasets, we demonstrate that SeCap is a feasible and effective solution for the AGPReID task",
    "checked": true,
    "id": "d239da3be29454a6addcce17fd62a3bba1c9ee63",
    "semantic_title": "secap: self-calibrating and adaptive prompts for cross-view person re-identification in aerial-ground networks",
    "citation_count": 2,
    "authors": [
      "Shining Wang",
      "Yunlong Wang",
      "Ruiqi Wu",
      "Bingliang Jiao",
      "Wenxuan Wang",
      "Peng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pippi_Zero-Shot_Styled_Text_Image_Generation_but_Make_It_Autoregressive_CVPR_2025_paper.html": {
    "title": "Zero-Shot Styled Text Image Generation, but Make It Autoregressive",
    "volume": "main",
    "abstract": "Styled Handwritten Text Generation (HTG) has recently received attention from the computer vision and document analysis communities, which have developed several solutions, either GAN- or diffusion-based, that achieved promising results. Nonetheless, these strategies fail to generalize to novel styles and have technical constraints, particularly in terms of maximum output length and training efficiency. To overcome these limitations, in this work, we propose a novel framework for text-image generation, dubbed Emuru. Our approach leverages a powerful text-image representation model (a variational autoencoder) combined with an autoregressive Transformer. Our approach enables the generation of styled text images conditioned on textual content and style examples, such as specific fonts or handwriting styles. We train our model solely on a diverse, synthetic dataset of English text rendered in over 100,000 typewritten and calligraphy fonts, which gives it the capability to reproduce unseen styles (both fonts and users' handwriting) in zero-shot. To the best of our knowledge, Emuru is the first autoregressive model for HTG, and the first designed specifically for generalization to novel styles. Moreover, our model generates images without background artifacts, which are easier to use for downstream applications. Extensive evaluation on both typewritten and handwritten, any-length text image generation scenarios demonstrates the effectiveness of our approach",
    "checked": true,
    "id": "5e3fadc38814422591cc02e65a94927479ad3040",
    "semantic_title": "zero-shot styled text image generation, but make it autoregressive",
    "citation_count": 3,
    "authors": [
      "Vittorio Pippi",
      "Fabio Quattrini",
      "Silvia Cascianelli",
      "Alessio Tonioni",
      "Rita Cucchiara"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Song_Dont_Shake_the_Wheel_Momentum-Aware_Planning_in_End-to-End_Autonomous_Driving_CVPR_2025_paper.html": {
    "title": "Don't Shake the Wheel: Momentum-Aware Planning in End-to-End Autonomous Driving",
    "volume": "main",
    "abstract": "End-to-end autonomous driving frameworks enable seamless integration of perception and planning but often rely on one-shot trajectory prediction, which may lead to unstable control and vulnerability to occlusions in single-frame perception. To address this, we propose the Momentum-Aware Driving (MomAD) framework, which introduces trajectory momentum and perception momentum to stabilize and refine trajectory predictions. MomAD comprises two core components: (1) Topological Trajectory Matching (TTM) employs Hausdorff Distance to select the optimal planning query that aligns with prior paths to ensure coherence; (2) Momentum Planning Interactor (MPI) cross-attends the selected planning query with historical queries to expand static and dynamic perception files. This enriched query, in turn, helps regenerate long-horizon trajectory and reduce collision risks. To mitigate noise arising from dynamic environments and detection errors, we introduce robust instance denoising during training, enabling the planning model to focus on critical signals and improve its robustness. We also propose a novel Trajectory Prediction Consistency (TPC) metric to quantitatively assess planning stability. Experiments on the nuScenes dataset demonstrate that MomAD achieves superior long-term consistency (>3s) compared to SOTA methods. Moreover, evaluations on the curated Turning-nuScenes shows that MomAD reduces the collision rate by 26% and improves TPC by 0.97m (33.45%) over a 6s prediction horizon, while closed-loop on Bench2Drive demonstrates an up to 16.3% improvement in success rate",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziying Song",
      "Caiyan Jia",
      "Lin Liu",
      "Hongyu Pan",
      "Yongchang Zhang",
      "Junming Wang",
      "Xingyu Zhang",
      "Shaoqing Xu",
      "Lei Yang",
      "Yadan Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Leveraging_Perturbation_Robustness_to_Enhance_Out-of-Distribution_Detection_CVPR_2025_paper.html": {
    "title": "Leveraging Perturbation Robustness to Enhance Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "Out-of-distribution (OOD) detection is the task of identifying inputs that deviate from the training data distribution. This capability is essential for the safe deployment of deep computer vision models in open-world environments. In this work, we propose a post-hoc method, Perturbation-Rectified OOD detection (PRO), based on the insight that prediction confidence for OOD inputs is more susceptible to reduction under perturbation than IND inputs. From this observation, we proposed a meta-score function that searches for local minimum scores near original inputs by applying gradient descent. This procedure enhances the separability between in-distribution (IND) and OOD samples. Importantly, the approach improves OOD detection performance without complex modifications to the underlying model architectures or training protocol. To validate our approach, we conduct extensive experiments using the OpenOOD benchmark. Our approach further pushes the limit of softmax-based OOD detection and is the leading post-hoc method for small-scale models. On a CIFAR-10 model with adversarial training, PRO effectively detects near-OOD inputs, achieving a reduction of more than 10% on FPR@95 compared to state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxi Chen",
      "Raymond A. Yeh",
      "Shaoshuai Mou",
      "Yan Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hao_Neural_Motion_Simulator_Pushing_the_Limit_of_World_Models_in_CVPR_2025_paper.html": {
    "title": "Neural Motion Simulator Pushing the Limit of World Models in Reinforcement Learning",
    "volume": "main",
    "abstract": "An embodied system must not only model the patterns of the external world but also understand its own motion dynamics. A motion dynamic model is essential for efficient skill acquisition and effective planning. In this work, we introduce the neural motion simulator (MoSim), a world model that predicts the future physical state of an embodied system based on current observations and actions. MoSim achieves state-of-the-art performance in physical state prediction and provides competitive performance across a range of downstream tasks. This works shows that when a world model is accurate enough and performs precise long-horizon predictions, it can facilitate efficient skill acquisition in imagined worlds and even enable zero-shot reinforcement learning. Furthermore, MoSim can transform any model-free reinforcement learning (RL) algorithm into a model-based approach, effectively decoupling physical environment modeling from RL algorithm development. This separation allows for independent advancements in RL algorithms and world modeling, significantly improving sample efficiency and enhancing generalization capabilities. Our findings highlight that world models for motion dynamics is a promising direction for developing more versatile and capable embodied systems",
    "checked": true,
    "id": "63df5f97fbfb22f438de2cc36f08967bdd73fd78",
    "semantic_title": "neural motion simulator pushing the limit of world models in reinforcement learning",
    "citation_count": 1,
    "authors": [
      "Chenjie Hao",
      "Weyl Lu",
      "Yifan Xu",
      "Yubei Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Aesthetic_Post-Training_Diffusion_Models_from_Generic_Preferences_with_Step-by-step_Preference_CVPR_2025_paper.html": {
    "title": "Aesthetic Post-Training Diffusion Models from Generic Preferences with Step-by-step Preference Optimization",
    "volume": "main",
    "abstract": "Generating visually appealing images is fundamental to modern text-to-image generation models. A potential solution to better aesthetics is direct preference optimization (DPO), which has been applied to diffusion models to improve general image quality including prompt alignment and aesthetics. Popular DPO methods propagate preference labels from clean image pairs to all the intermediate steps along the two generation trajectories. However, preference labels provided in existing datasets are blended with layout and aesthetic opinions, which would disagree with aesthetic preference. Even if aesthetic labels were provided (at substantial cost), it would be hard for the two-trajectory methods to capture nuanced visual differences at different steps. To improve aesthetics economically, this paper uses existing generic preference data and introduces step-by-step preference optimization (SPO) that discards the propagation strategy and allows fine-grained image details to be assessed. Specifically, at each denoising step, we 1) sample a pool of candidates by denoising from a shared noise latent, 2) use a step-aware preference model to find a suitable win-lose pair to supervise the diffusion model, and 3) randomly select one from the pool to initialize the next denoising step. This strategy ensures that diffusion models focus on the subtle, fine-grained visual differences instead of layout aspect. We find that aesthetics can be significantly enhanced by accumulating these improved minor differences. When fine-tuning Stable Diffusion v1.5 and SDXL, SPO yields significant improvements in aesthetics compared with existing DPO methods while not sacrificing image-text alignment compared with vanilla models. Moreover, SPO converges much faster than DPO methods due to the use of more correct preference labels provided by the step-aware preference model. Code and models are available at https://github.com/RockeyCoss/SPO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanhao Liang",
      "Yuhui Yuan",
      "Shuyang Gu",
      "Bohan Chen",
      "Tiankai Hang",
      "Mingxi Cheng",
      "Ji Li",
      "Liang Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Adversarial_Diffusion_Compression_for_Real-World_Image_Super-Resolution_CVPR_2025_paper.html": {
    "title": "Adversarial Diffusion Compression for Real-World Image Super-Resolution",
    "volume": "main",
    "abstract": "Real-world image super-resolution (Real-ISR) aims to reconstruct high-resolution images from low-resolution inputs degraded by complex, unknown processes. While many Stable Diffusion (SD)-based Real-ISR methods have achieved remarkable success, their slow, multi-step inference hinders practical deployment. Recent SD-based one-step networks like OSEDiff and S3Diff alleviate this issue but still incur high computational costs due to their reliance on large pretrained SD models. This paper proposes a novel Real-ISR method, AdcSR, by distilling the one-step diffusion network OSEDiff into a streamlined diffusion-GAN model under our Adversarial Diffusion Compression (ADC) framework. We meticulously examine the modules of OSEDiff, categorizing them into two types: (1) Removable (VAE encoder, prompt extractor, text encoder, etc.) and (2) Prunable (denoising UNet and VAE decoder). Since direct removal and pruning can degrade the model's generation capability, we pretrain our pruned VAE decoder to restore its ability to decode images and employ adversarial distillation to compensate for performance loss. This ADC-based diffusion-GAN hybrid design effectively reduces complexity by 73% in inference time, 78% in computation, and 74% in parameters, while preserving the model's generation capability. Experiments manifest that our proposed AdcSR achieves competitive recovery quality on both synthetic and real-world datasets, offering up to 9.3x speedup over previous one-step diffusion-based methods. Code and models are available at https://github.com/Guaishou74851/AdcSR",
    "checked": true,
    "id": "dfc694571b55ea8b8908cfec125056c6fbf04cfc",
    "semantic_title": "adversarial diffusion compression for real-world image super-resolution",
    "citation_count": 12,
    "authors": [
      "Bin Chen",
      "Gehui Li",
      "Rongyuan Wu",
      "Xindong Zhang",
      "Jie Chen",
      "Jian Zhang",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mall_DiSciPLE_Learning_Interpretable_Programs_for_Scientific_Visual_Discovery_CVPR_2025_paper.html": {
    "title": "DiSciPLE: Learning Interpretable Programs for Scientific Visual Discovery",
    "volume": "main",
    "abstract": "Visual data is used in numerous different scientific workflows ranging from remote sensing to ecology. As the amount of observation data increases, the challenge is not just to make accurate predictions but also to understand the underlying mechanisms for those predictions. Good interpretation is important in scientific workflows, as it allows for better decision-making by providing insights into the data. This paper introduces an automatic way of obtaining such interpretable-by-design models, by learning programs that interleave neural networks. We propose DiSciPLE (Discovering Scientific Programs using LLMs and Evolution) an evolutionary algorithm that leverages common sense and prior knowledge of large language models (LLMs) to create Python programs explaining visual data. Additionally, we propose two improvements: a program critic and a program simplifier to improve our method further to synthesize good programs. On three different real world problems, DiSciPLE learns state-of-the-art programs on novel tasks with no prior literature. For example, we can learn programs with 35% lower error than the closest non-interpretable baseline for population density estimation",
    "checked": true,
    "id": "7436dca1d767f13595a1d76c9e4b84a0023dd6ec",
    "semantic_title": "disciple: learning interpretable programs for scientific visual discovery",
    "citation_count": 1,
    "authors": [
      "Utkarsh Mall",
      "Cheng Perng Phoo",
      "Mia Chiquier",
      "Bharath Hariharan",
      "Kavita Bala",
      "Carl Vondrick"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_SOLAMI_Social_Vision-Language-Action_Modeling_for_Immersive_Interaction_with_3D_Autonomous_CVPR_2025_paper.html": {
    "title": "SOLAMI: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters",
    "volume": "main",
    "abstract": "Human beings are social animals. How to equip 3D autonomous characters with similar social intelligence that can perceive, understand and interact with humans remains an open yet foundamental problem. In this paper, we introduce SOLAMI, the first end-to-end Social vision-Language-Action (VLA) Modeling framework for Immersive interaction with 3D autonomous characters. Specifically, SOLAMI builds 3D autonomous characters from three aspects: 1) Social VLA Architecture: We propose a unified social VLA framework to generate multimodal response (speech and motion) based on the user's multimodal input to drive the character for social interaction. 2) Interactive Multimodal Data: We present SynMSI, a synthetic multimodal social interaction dataset generated by an automatic pipeline using only existing motion datasets to address the issue of data scarcity. 3) Immersive VR Interface: We develop a VR interface that enables users to immersively interact with these characters driven by various architectures. Extensive quantitative experiments and user studies demonstrate that our framework leads to more precise and natural character responses (in both speech and motion) that align with user expectations with lower latency",
    "checked": true,
    "id": "8ce9302cf87ca20aba04bc33ca8f37e59bbd5554",
    "semantic_title": "solami: social vision-language-action modeling for immersive interaction with 3d autonomous characters",
    "citation_count": 6,
    "authors": [
      "Jianping Jiang",
      "Weiye Xiao",
      "Zhengyu Lin",
      "Huaizhong Zhang",
      "Tianxiang Ren",
      "Yang Gao",
      "Zhiqian Lin",
      "Zhongang Cai",
      "Lei Yang",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_EntropyMark_Towards_More_Harmless_Backdoor_Watermark_via_Entropy-based_Constraint_for_CVPR_2025_paper.html": {
    "title": "EntropyMark: Towards More Harmless Backdoor Watermark via Entropy-based Constraint for Open-source Dataset Copyright Protection",
    "volume": "main",
    "abstract": "High-quality open-source datasets are essential for advancing deep neural networks. However, the unauthorized commercial use of these datasets has raised significant concerns about copyright protection. One promising approach is backdoor watermark-based dataset ownership verification (BW-DOV), in which dataset protectors implant specific backdoors into illicit models through dataset watermarking, enabling the tracing of these models through abnormal prediction behaviors. Unfortunately, the targeted nature of these BW-DOV methods can be maliciously exploited, potentially leading to harmful side effects. While existing harmless methods attempt to mitigate these risks, watermarked datasets can still negatively affect prediction results, partially compromising dataset functionality. In this paper, we propose a more harmless backdoor watermark, called EntropyMark, which improves prediction confidence without altering the final prediction results. For this purpose, an entropy-based constraint is introduced to regulate the probability distribution. Specifically, we design an iterative clean-label dataset watermarking framework. Our framework employs gradient matching and adaptive data selection to optimize backdoor injection. In parallel, we introduce a hypothesis test method grounded in entropy inconsistency to verify dataset ownership. Extensive experiments on benchmark datasets demonstrate the effectiveness, transferability, and defense resistance of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Sun",
      "Rui Wang",
      "Zixuan Zhu",
      "Lihua Jing",
      "Yuanfang Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_Adaptive_Markup_Language_Generation_for_Contextually-Grounded_Visual_Document_Understanding_CVPR_2025_paper.html": {
    "title": "Adaptive Markup Language Generation for Contextually-Grounded Visual Document Understanding",
    "volume": "main",
    "abstract": "Visual Document Understanding has become essential with the increase of text-rich visual content. This field poses significant challenges due to the need for effective integration of visual perception and textual comprehension, particularly across diverse document types with complex layouts. Moreover, existing fine-tuning datasets for this domain often fall short in providing the detailed contextual information for robust understanding, leading to hallucinations and limited comprehension of spatial relationships among visual elements. To address these challenges, we propose an innovative pipeline that utilizes adaptive generation of markup languages, such as Markdown, JSON, HTML, and TiKZ, to build highly structured document representations and deliver contextually-grounded responses. We introduce two fine-grained structured datasets: DocMark-Pile, comprising approximately 3.8M pretraining data pairs for document parsing, and DocMark-Instruct, featuring 624k fine-tuning data annotations for grounded instruction following.Extensive experiments demonstrate that our proposed model significantly outperforms existing state-of-the-art MLLMs across a range of visual document understanding benchmarks, facilitating advanced reasoning and comprehension capabilities in complex visual scenarios",
    "checked": true,
    "id": "859d187ca4985c42fdea4909d614e2be0785a270",
    "semantic_title": "adaptive markup language generation for contextually-grounded visual document understanding",
    "citation_count": 1,
    "authors": [
      "Han Xiao",
      "Yina Xie",
      "Guanxin Tan",
      "Yinghao Chen",
      "Rui Hu",
      "Ke Wang",
      "Aojun Zhou",
      "Hao Li",
      "Hao Shao",
      "Xudong Lu",
      "Peng Gao",
      "Yafei Wen",
      "Xiaoxin Chen",
      "Shuai Ren",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_BARD-GS_Blur-Aware_Reconstruction_of_Dynamic_Scenes_via_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "BARD-GS: Blur-Aware Reconstruction of Dynamic Scenes via Gaussian Splatting",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) has shown remarkable potential for static scene reconstruction, and recent advancements have extended its application to dynamic scenes. However, the quality of reconstructions depends heavily on high-quality input images and precise camera poses, which is not that trivial to fulfill in the real-world scenarios. Capturing dynamic scenes with handheld monocular cameras, for instance, typically involves simultaneous movement of both the camera and objects within a single exposure. This combined motion frequently results in image blur that existing methods cannot adequately handle. To address these challenges, we introduce BARD-GS, a novel approach for robust dynamic scene reconstruction that effectively handles blurry inputs and imprecise camera poses. Our method comprises two main components: 1) camera motion deblurring and 2) object motion deblurring. By explicitly decomposing motion blur into camera motion blur and object motion blur and modeling them separately, we achieve significantly improved rendering results in dynamic regions. In addition, we collect a real-world motion blur dataset of dynamic scenes to evaluate our approach. Extensive experiments demonstrate that BARD-GS effectively reconstructs high-quality dynamic scenes under realistic conditions, significantly outperforming existing methods",
    "checked": true,
    "id": "62543038d0e7b399bb39bf85c8ab11c149330dc4",
    "semantic_title": "bard-gs: blur-aware reconstruction of dynamic scenes via gaussian splatting",
    "citation_count": 3,
    "authors": [
      "Yiren Lu",
      "Yunlai Zhou",
      "Disheng Liu",
      "Tuo Liang",
      "Yu Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hong_SALAD_Skeleton-aware_Latent_Diffusion_for_Text-driven_Motion_Generation_and_Editing_CVPR_2025_paper.html": {
    "title": "SALAD: Skeleton-aware Latent Diffusion for Text-driven Motion Generation and Editing",
    "volume": "main",
    "abstract": "Text-driven motion generation has advanced significantly with the rise of denoising diffusion models. However, previous methods often oversimplify representations for the skeletal joints, temporal frames, and textual words, limiting their ability to fully capture the information within each modality and their interactions. Moreover, when using pre-trained models for downstream tasks, such as editing, they typically require additional efforts, including manual interventions, optimization, or fine-tuning. In this paper, we introduce a skeleton-aware latent diffusion (SALAD), a model that explicitly captures the intricate inter-relationships between joints, frames, and words. Furthermore, by leveraging cross-attention maps produced during the generation process, we enable the attention-based zero-shot text-driven motion editing using a pre-trained SALAD model, requiring no additional user input beyond text prompts. Our approach significantly outperforms previous methods in terms of text-motion alignment without compromising generation quality, and demonstrates practical versatility by providing diverse editing capabilities beyond generation. Code is available at project page",
    "checked": true,
    "id": "387a3698b591ce32a21604c05d1977848f70911c",
    "semantic_title": "salad: skeleton-aware latent diffusion for text-driven motion generation and editing",
    "citation_count": 4,
    "authors": [
      "Seokhyeon Hong",
      "Chaelin Kim",
      "Serin Yoon",
      "Junghyun Nam",
      "Sihun Cha",
      "Junyong Noh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Towards_Universal_AI-Generated_Image_Detection_by_Variational_Information_Bottleneck_Network_CVPR_2025_paper.html": {
    "title": "Towards Universal AI-Generated Image Detection by Variational Information Bottleneck Network",
    "volume": "main",
    "abstract": "The rapid advancement of generative models has significantly improved the quality of generated images. Meanwhile, it challenges information authenticity and credibility. Current generated image detection methods based on large-scale pre-trained multimodal models have achieved impressive results. Although these models provide abundant features, the authentication task-related features are often submerged. Consequently, those authentication task-irrelated features cause models to learn superficial biases, thereby harming their generalization performance across different model genera (e.g., GANs and Diffusion Models). To this end, we proposed VIB-Net, which uses Variational Information Bottlenecks to enforce authentication task-related feature learning. We tested and analyzed the proposed method and existing methods on samples generated by 17 different generative models. Compared to SOTA methods, VIB-Net achieved a 5.55% improvement in mAP and a 9.33% increase in accuracy. Notably, in generalization tests on unseen generative models from different series, VIB-Net improved mAP by 12.48% and accuracy by 23.59% over SOTA methods. The code is available at https://github.com/oceanzhf/VIBAIGCDetect",
    "checked": true,
    "id": "e9317ec76a0df353928f014a3ca727eb502e97e6",
    "semantic_title": "towards universal ai-generated image detection by variational information bottleneck network",
    "citation_count": 2,
    "authors": [
      "Haifeng Zhang",
      "Qinghui He",
      "Xiuli Bi",
      "Weisheng Li",
      "Bo Liu",
      "Bin Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_HSI_A_Holistic_Style_Injector_for_Arbitrary_Style_Transfer_CVPR_2025_paper.html": {
    "title": "HSI: A Holistic Style Injector for Arbitrary Style Transfer",
    "volume": "main",
    "abstract": "Attention-based arbitrary style transfer methods have gained significant attention recently due to their impressive ability to synthesize style details. However, the point-wise matching within the attention mechanism may overly focus on local patterns such that neglect the remarkable global features of style images. Additionally, when processing large images, the quadratic complexity of the attention mechanism will bring high computational load. To alleviate above problems, we propose Holistic Style Injector (HSI), a novel attention-style transformation module to deliver artistic expression of target style. Specifically, HSI performs stylization only based on global style representation that is more in line with the characteristics of style transfer, to avoid generating local disharmonious patterns in stylized images. Moreover, we propose a dual relation learning mechanism inside the HSI to dynamically render images by leveraging semantic similarity in content and style, ensuring the stylized images preserve the original content and improve style fidelity. Note that the proposed HSI achieves linear computational complexity because it establishes feature mapping through element-wise multiplication rather than matrix multiplication. Qualitative and quantitative results demonstrate that our method outperforms state-of-the-art approaches in both effectiveness and efficiency",
    "checked": true,
    "id": "caa67b35ce6854389957793e16533df75b11c9af",
    "semantic_title": "hsi: a holistic style injector for arbitrary style transfer",
    "citation_count": 0,
    "authors": [
      "Shuhao Zhang",
      "Hui Kang",
      "Yang Liu",
      "Fang Mei",
      "Hongjuan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chang_LookingGlass_Generative_Anamorphoses_via_Laplacian_Pyramid_Warping_CVPR_2025_paper.html": {
    "title": "LookingGlass: Generative Anamorphoses via Laplacian Pyramid Warping",
    "volume": "main",
    "abstract": "Anamorphosis refers to a category of images that are intentionally distorted, making them unrecognizable when viewed directly. Their true form only reveals itself when seen from a specific viewpoint, which can be through some catadioptric device like a mirror or a lens. While the construction of these mathematical devices can be traced back to as early as the 17th century, they are only interpretable when viewed from a specific vantage point and tend to lose meaning when seen normally. In this paper, we revisit these famous optical illusions with a generative twist. With the help of latent rectified flow models, we propose a method to create anamorphic images that still retain a valid interpretation when viewed directly. To this end, we introduce Laplacian Pyramid Warping, a frequency-aware image warping technique key to generating high-quality visuals. Our work extends Visual Anagrams (Geng et al. 2024) to latent space models and to a wider range of spatial transforms, enabling the creation of novel generative perceptual illusions",
    "checked": true,
    "id": "1086671036364274970c7617135442ac9a4058ad",
    "semantic_title": "lookingglass: generative anamorphoses via laplacian pyramid warping",
    "citation_count": 0,
    "authors": [
      "Pascal Chang",
      "Sergio Sancho",
      "Jingwei Tang",
      "Markus Gross",
      "Vinicius Azevedo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_V2V3D_View-to-View_Denoised_3D_Reconstruction_for_Light_Field_Microscopy_CVPR_2025_paper.html": {
    "title": "V2V3D: View-to-View Denoised 3D Reconstruction for Light Field Microscopy",
    "volume": "main",
    "abstract": "Light field microscopy (LFM) has gained significant attention due to its ability to capture snapshot-based, large-scale 3D fluorescence images. However, existing LFM reconstruction algorithms are highly sensitive to sensor noise or require hard-to-get ground-truth annotated data for training. To address these challenges, this paper introduces V2V3D, an unsupervised view2view-based framework that establishes a new paradigm for joint optimization of image denoising and 3D reconstruction in a unified architecture. We assume that the LF images are derived from a consistent 3D signal, with the noise in each view being independent. This enables V2V3D to incorporate the principle of noise2noise for effective denoising. To enhance the recovery of high-frequency details, we propose a novel wave-optics-based feature alignment technique, which transforms the point spread function, used for forward propagation in wave optics, into convolution kernels specifically designed for feature alignment. Moreover, we introduce an LFM dataset containing LF images and their corresponding 3D intensity volumes. Extensive experiments demonstrate that our approach achieves high computational efficiency and outperforms the other state-of-the-art methods. These advancements position V2V3D as a promising solution for 3D imaging under challenging conditions. Our code and dataset will be publicly accessible at https://joey1998hub.github.io/V2V3D/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayin Zhao",
      "Zhenqi Fu",
      "Tao Yu",
      "Hui Qiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_DiN_Diffusion_Model_for_Robust_Medical_VQA_with_Semantic_Noisy_CVPR_2025_paper.html": {
    "title": "DiN: Diffusion Model for Robust Medical VQA with Semantic Noisy Labels",
    "volume": "main",
    "abstract": "Medical Visual Question Answering (Med-VQA) systems benefit the interpretation of medical images containing critical clinical information. However, the challenge of noisy labels and limited high-quality datasets remains underexplored. To address this, we establish the first benchmark for noisy labels in Med-VQA by simulating human mislabeling with semantically designed noise types. More importantly, we introduce the DiN framework, which leverages a diffusion model to handle noisy labels in Med-VQA. Unlike the dominant classification-based VQA approaches that directly predict answers, our Answer Diffuser (AD) module employs a coarse-to-fine process, refining answer candidates with a diffusion model for improved accuracy. The Answer Condition Generator (ACG) further enhances this process by generating task-specific conditional information via integrating answer embeddings with fused image-question features. To address label noise, our Noisy Label Refinement(NLR) module introduces a robust loss function and dynamic answer adjustment to further boost the performance of the AD module. Our DiN framework consistently outperforms existing methods across multiple benchmarks with varying noise levels",
    "checked": true,
    "id": "144783f08621cee07e222ed4ca9b18ba50e786d9",
    "semantic_title": "din: diffusion model for robust medical vqa with semantic noisy labels",
    "citation_count": 1,
    "authors": [
      "Erjian Guo",
      "Zhen Zhao",
      "Zicheng Wang",
      "Tong Chen",
      "Yunyi Liu",
      "Luping Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Splatter-360_Generalizable_360_Gaussian_Splatting_for_Wide-baseline_Panoramic_Images_CVPR_2025_paper.html": {
    "title": "Splatter-360: Generalizable 360 Gaussian Splatting for Wide-baseline Panoramic Images",
    "volume": "main",
    "abstract": "Wide-baseline panoramic images are frequently used in applications like VR and simulations to minimize capturing labor costs and storage needs. However, synthesizing novel views from these panoramic images in real time remains a significant challenge, especially due to panoramic imagery's high resolution and inherent distortions. Although existing 3D Gaussian splatting (3DGS) methods can produce photo-realistic views under narrow baselines, they often overfit the training views when dealing with wide-baseline panoramic images due to the difficulty in learning precise geometry from sparse 360^ \\circ views. This paper presents Splatter-360, a novel end-to-end generalizable 3DGS framework designed to handle wide-baseline panoramic images. Unlike previous approaches, Splatter-360 performs multi-view matching directly in the spherical domain by constructing a spherical cost volume through a spherical sweep algorithm, enhancing the network's depth perception and geometry estimation. Additionally, we introduce a 3D-aware bi-projection encoder to mitigate the distortions inherent in panoramic images and integrate cross-view attention to improve feature interactions across multiple viewpoints. This enables robust 3D-aware feature representations and real-time rendering capabilities. Experimental results on the HM3D and Replica demonstrate that Splatter-360 significantly outperforms state-of-the-art NeRF and 3DGS methods (e.g., PanoGRF, MVSplat, DepthSplat, and HiSplat) in both synthesis quality and generalization performance for wide-baseline panoramic images. Code and trained models are available at https://3d-aigc.github.io/Splatter-360/",
    "checked": false,
    "id": "1af2084341a26fb321bebddcccf9493e43c9e207",
    "semantic_title": "splatter-360: generalizable 360¬∞ gaussian splatting for wide-baseline panoramic images",
    "citation_count": 7,
    "authors": [
      "Zheng Chen",
      "Chenming Wu",
      "Zhelun Shen",
      "Chen Zhao",
      "Weicai Ye",
      "Haocheng Feng",
      "Errui Ding",
      "Song-Hai Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_ShowMak3r_Compositional_TV_Show_Reconstruction_CVPR_2025_paper.html": {
    "title": "ShowMak3r: Compositional TV Show Reconstruction",
    "volume": "main",
    "abstract": "Reconstructing dynamic radiance fields from video clips is challenging, especially when entertainment videos like TV shows are given. Many challenges make the reconstruction difficult due to (1) actors occluding with each other and having diverse facial expressions, (2) cluttered stages, and (3) small baseline views or sudden shot changes. To address these issues, we present ShowMak3r, a comprehensive reconstruction pipeline that allows the editing of scenes like how video clips are made in a production control room. In ShowMak3r, a 3DLocator module locates recovered actors on the stage using depth prior, and estimates unseen human poses via interpolation. The proposed ShotMatcher module then tracks the actors under shot changes. Furthermore, ShowMak3r introduces a face-fitting network that dynamically recovers the actors' expressions. Experiments on Sitcoms3D dataset show that our pipeline can reassemble TV show scenes with new cameras at different timestamps. We also demonstrate that ShowMak3r enables interesting applications such as synthetic shot-making, actor relocation, insertion, deletion, and pose manipulation. Project page : https://nstar1125.github.io/showmak3r",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangmin Kim",
      "Seunguk Do",
      "Jaesik Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ling_CADRef_Robust_Out-of-Distribution_Detection_via_Class-Aware_Decoupled_Relative_Feature_Leveraging_CVPR_2025_paper.html": {
    "title": "CADRef: Robust Out-of-Distribution Detection via Class-Aware Decoupled Relative Feature Leveraging",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) have been widely criticized for their overconfidence when dealing with out-of-distribution (OOD) samples, highlighting the critical need for effective OOD detection to ensure the safe deployment of DNNs in real-world settings. Existing post-hoc OOD detection methods primarily enhance the discriminative power of logit-based approaches by reshaping sample features, yet they often neglect critical information inherent in the features themselves. In this paper, we propose the \\underline C lass-\\underline A ware \\underline Re lative \\underline F eature-based method (CARef), which utilizes the error between a sample's feature and its class-aware average feature as a discriminative criterion. To further refine this approach, we introduce the \\underline C lass-\\underline A ware \\underline D ecoupled \\underline Re lative \\underline F eature-based method (CADRef), which decouples sample features based on the alignment of signs between the relative feature and corresponding model weights, enhancing the discriminative capabilities of CARef.Extensive experimental results across multiple datasets and models demonstrate that both proposed methods exhibit effectiveness and robustness in OOD detection compared to state-of-the-art methods. Specifically, our two methods outperform the best baseline by 2.82% and 3.27% in AUROC, with improvements of 4.03% and 6.32% in FPR95, respectively",
    "checked": true,
    "id": "fe67d07f5224d9d0d00da5fe7a9f8bd8e2710ac6",
    "semantic_title": "cadref: robust out-of-distribution detection via class-aware decoupled relative feature leveraging",
    "citation_count": 1,
    "authors": [
      "Zhiwei Ling",
      "Yachen Chang",
      "Hailiang Zhao",
      "Xinkui Zhao",
      "Kingsum Chow",
      "Shuiguang Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_S3-Face_SSS-Compliant_Facial_Reflectance_Estimation_via_Diffusion_Priors_CVPR_2025_paper.html": {
    "title": "S^3-Face: SSS-Compliant Facial Reflectance Estimation via Diffusion Priors",
    "volume": "main",
    "abstract": "Recent 3D face reconstruction methods have made remarkable advancements, yet achieving high-quality facial reflectance from monocular input remains challenging. Existing methods rely on the light-stage captured data to learn facial reflectance models. However, limited subject diversity in these datasets poses challenges in achieving good generalization and broad applicability. This motivates us to explore whether the extensive priors captured in recent generative diffusion models (e.g., Stable Diffusion) can enable more generalizable facial reflectance estimation as these models have been pre-trained on large-scale internet image collections containing rich visual patterns. In this paper, we introduce the use of Stable Diffusion as a prior for facial reflectance estimation, achieving robust results with minimal captured data for fine-tuning. We present S^3-Face, a comprehensive framework capable of producing SSS-compliant skin reflectance from in-the-wild images. Our method adopts a two-stage training approach: in the first stage, DSN-Net is trained to predict diffuse albedo, specular albedo, and normal maps from in-the-wild images using a novel joint reflectance attention module. In the second stage, HM-Net is trained to generate hemoglobin and melanin maps based on the diffuse albedo predicted in the first stage, yielding SSS-compliant and detailed reflectance maps. Extensive experiments demonstrate that our method achieves strong generalization and produces high-fidelity, SSS-compliant facial reflectance reconstructions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Ren",
      "Jiankang Deng",
      "Yuhao Cheng",
      "Wenhan Zhu",
      "Yichao Yan",
      "Xiaokang Yang",
      "Stefanos Zafeiriou",
      "Chao Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_FSBench_A_Figure_Skating_Benchmark_for_Advancing_Artistic_Sports_Understanding_CVPR_2025_paper.html": {
    "title": "FSBench: A Figure Skating Benchmark for Advancing Artistic Sports Understanding",
    "volume": "main",
    "abstract": "Figure skating, known as the \"Art on Ice,\" is among the most artistic sports, challenging to understand due to its blend of technical elements (like jumps and spins) and overall artistic expression. Existing figure skating datasets mainly focus on single tasks, such as action recognition or scoring, lacking comprehensive annotations for both technical and artistic evaluation. Current sports research is largely centered on ball games, with limited relevance to artistic sports like figure skating. To address this, we introduce FSAnno, a large-scale dataset advancing artistic sports understanding through figure skating. FSAnno includes an open-access training and test dataset, alongside a benchmark dataset, FSBench, for fair model evaluation. FSBench consists of FSBench-Text, with multiple-choice questions and explanations, and FSBench-Motion, containing multimodal data and Question and Answer (QA) pairs, supporting tasks from technical analysis to performance commentary. Initial tests on FSBench reveal significant limitations in existing models' understanding of artistic sports. We hope FSBench will become a key tool for evaluating and enhancing model comprehension of figure skating. All data, models, and more details are available at: https://github.com/Moomin-Fin/Ano",
    "checked": true,
    "id": "fd85290f9c79790b21c976eb88e1cb695258e78a",
    "semantic_title": "fsbench: a figure skating benchmark for advancing artistic sports understanding",
    "citation_count": 0,
    "authors": [
      "Rong Gao",
      "Xin Liu",
      "Zhuozhao Hu",
      "Bohao Xing",
      "Baiqiang Xia",
      "Zitong Yu",
      "Heikki K√§lvi√§inen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cai_Keep_the_Balance_A_Parameter-Efficient_Symmetrical_Framework_for_RGBX_Semantic_CVPR_2025_paper.html": {
    "title": "Keep the Balance: A Parameter-Efficient Symmetrical Framework for RGB+X Semantic Segmentation",
    "volume": "main",
    "abstract": "Multimodal semantic segmentation is a critical challenge in computer vision, with early methods suffering from high computational costs and limited transferability due to full fine-tuning of RGB-based pre-trained parameters. Recent studies, while leveraging additional modalities as supplementary prompts to RGB, still predominantly rely on RGB, which restricts the full potential of other modalities. To address these issues, we propose a novel symmetric parameter-efficient fine-tuning framework for multimodal segmentation, featuring with a modality-aware prompting and adaptation scheme, to simultaneously adapt the capabilities of a powerful pre-trained model to both RGB and X modalities. Furthermore, prevalent approaches use the global cross-modality correlations of attention mechanism for modality fusion, which inadvertently introduces noise across modalities. To mitigate this noise, we propose a dynamic sparse cross-modality fusion module to facilitate effective and efficient cross-modality fusion. To further strengthen the above two modules, we propose a training strategy that leverages accurately predicted dual-modality results to self-teach the single-modality outcomes. In comprehensive experiments, we demonstrate that our method outperforms previous state-of-the-art approaches across six multimodal segmentation scenarios with minimal computation cost",
    "checked": true,
    "id": "8ff0764099bb3931d41e2919c0c2d918e107eb3c",
    "semantic_title": "keep the balance: a parameter-efficient symmetrical framework for rgb+x semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Jiaxin Cai",
      "Jingze Su",
      "Qi Li",
      "Wenjie Yang",
      "Shu Wang",
      "Tiesong Zhao",
      "Shengfeng He",
      "Wenxi Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VideoDirector_Precise_Video_Editing_via_Text-to-Video_Models_CVPR_2025_paper.html": {
    "title": "VideoDirector: Precise Video Editing via Text-to-Video Models",
    "volume": "main",
    "abstract": "Despite the typical inversion-then-editing paradigm using text-to-image (T2I) models has demonstrated promising results, directly extending it to text-to-video (T2V) models still suffers severe artifacts such as color flickering and content distortion. Consequently, current video editing methods primarily rely on T2I models, which inherently lack temporal-coherence generative ability, often resulting in inferior editing results. In this paper, we attribute the failure of the typical editing paradigm to: 1) Tightly Spatial-temporal Coupling. The vanilla pivotal-based inversion strategy struggles to disentangle spatial-temporal information in the video diffusion model; 2) Complicated Spatial-temporal Layout. The vanilla cross-attention control strategy is deficient in preserving the unedited content. To address these limitations, we propose a spatial-temporal decoupled guidance (STDG) and multi-frame null-text optimization strategy to provide pivotal temporal cues for more precise pivotal inversion. Furthermore, we introduce a self-attention control strategy to maintain higher fidelity for precise partial content editing. Experimental results demonstrate that our method (termed VideoDirector) effectively harnesses the powerful temporal generation capabilities of T2V models, producing edited videos with state-of-the-art performance in accuracy, motion smoothness, realism, and fidelity to unedited content",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yukun Wang",
      "Longguang Wang",
      "Zhiyuan Ma",
      "Qibin Hu",
      "Kai Xu",
      "Yulan Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lai_LLM-driven_Multimodal_and_Multi-Identity_Listening_Head_Generation_CVPR_2025_paper.html": {
    "title": "LLM-driven Multimodal and Multi-Identity Listening Head Generation",
    "volume": "main",
    "abstract": "Generating natural listener responses in conversational scenarios is crucial for creating engaging digital humans and avatars. Recent work has shown that large language models (LLMs) can be effectively leveraged for this task, demonstrating remarkable capabilities in generating contextually appropriate listener behaviors. However, current LLM-based methods face two critical limitations: they rely solely on speech content, overlooking other crucial communication signals, and they entangle listener identity with response generation, compromising output fidelity and generalization. In this work, we present a novel framework that addresses these limitations while maintaining the advantages of LLMs. Our approach introduces a Multimodal-LM architecture that jointly processes speech content, acoustics, and speaker emotion, capturing the full spectrum of communication cues. Additionally, we propose an identity disentanglement strategy using instance normalization and adaptive instance normalization in a VQ-VAE framework, enabling high-fidelity listening head synthesis with flexible identity control. Extensive experiments demonstrate that our method significantly outperforms existing approaches in terms of response naturalness and fidelity, while enabling effective identity control without retraining",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiwen Lai",
      "Weizhi Zhong",
      "Yipeng Qin",
      "Xiaohang Ren",
      "Baoyuan Wang",
      "Guanbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Towards_Understanding_How_Knowledge_Evolves_in_Large_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "Towards Understanding How Knowledge Evolves in Large Vision-Language Models",
    "volume": "main",
    "abstract": "Large Vision-Language Models (LVLMs) are gradually becoming the foundation for many artificial intelligence applications. However, understanding their internal working mechanisms has continued to puzzle researchers, which in turn limits the further enhancement of their capabilities. In this paper, we seek to investigate how multimodal knowledge evolves and eventually induces natural languages in LVLMs. We design a series of novel strategies for analyzing internal knowledge within LVLMs, and delve into the evolution of multimodal knowledge from three levels, including single token probabilities, token probability distributions, and feature encodings. In this process, we identify two key nodes in knowledge evolution: the critical layers and the mutation layers, dividing the evolution process into three stages: rapid evolution, stabilization, and mutation. Our research is the first to reveal the trajectory of knowledge evolution in LVLMs, providing a fresh perspective for understanding their underlying mechanisms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sudong Wang",
      "Yunjian Zhang",
      "Yao Zhu",
      "Jianing Li",
      "Zizhe Wang",
      "Yanwei Liu",
      "Xiangyang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kumar_A_Unified_Resilient_and_Explainable_Adversarial_Patch_Detector_CVPR_2025_paper.html": {
    "title": "A Unified, Resilient, and Explainable Adversarial Patch Detector",
    "volume": "main",
    "abstract": "Deep Neural Networks (DNNs), backbone architecture in `almost' every computer vision task, are vulnerable to adversarial attacks, particularly physical out-of-distribution (OOD) adversarial patches. Existing defense models often struggle with interpreting these attacks in ways that align with human visual perception. Our proposed AdvPatchXAI approach introduces a generalized, robust, and explainable defense algorithm designed to defend DNNs against physical adversarial threats. AdvPatchXAI employs a novel patch decorrelation loss that reduces feature redundancy and enhances the distinctiveness of patch representations, enabling better generalization across unseen adversarial scenarios. It learns prototypical parts self-supervised, enhancing interpretability and correlation with human vision. The model utilizes a sparse linear layer for classification, making the decision process globally interpretable through a set of learned prototypes and locally explainable by pinpointing relevant prototypes within an image. Our comprehensive evaluation shows that AdvPatchXAI closes the \"semantic\" gap between latent space and pixel space and effectively handles unseen adversarial patches even perturbed with unseen corruptions, thereby significantly advancing DNN robustness in practical settings(https://github.com/tbvl22/Unified-resilient-and-Explainable-Adversarial-Patch-detector)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vishesh Kumar",
      "Akshay Agarwal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_VISTA_Enhancing_Long-Duration_and_High-Resolution_Video_Understanding_by_Video_Spatiotemporal_CVPR_2025_paper.html": {
    "title": "VISTA: Enhancing Long-Duration and High-Resolution Video Understanding by Video Spatiotemporal Augmentation",
    "volume": "main",
    "abstract": "Current large multimodal models (LMMs) face significant challenges in processing and comprehending long-duration or high-resolution videos, which is mainly due to the lack of high-quality datasets. To address this issue from a data-centric perspective, we propose VISTA, a simple yet effective video spatiotemporal augmentation framework that synthesizes long-duration and high-resolution video instruction-following pairs from existing video-caption datasets. VISTA spatially and temporally combines videos to create new synthetic videos with extended durations and enhanced resolutions, and subsequently produces question-answer pairs pertaining to these newly synthesized videos. Based on this paradigm, we develop seven video augmentation methods and curate VISTA-400K, a video instruction-following dataset aimed at enhancing long-duration and high-resolution video understanding. Finetuning various video LMMs on our data resulted in an average improvement of 3.3% across four challenging benchmarks for long-video understanding. Furthermore, we introduce the first comprehensive high-resolution video understanding benchmark HRVideoBench, on which our finetuned models achieve a 6.5% performance gain. These results highlight the effectiveness of our framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiming Ren",
      "Huan Yang",
      "Jie Min",
      "Cong Wei",
      "Wenhu Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiang_Structured_3D_Latents_for_Scalable_and_Versatile_3D_Generation_CVPR_2025_paper.html": {
    "title": "Structured 3D Latents for Scalable and Versatile 3D Generation",
    "volume": "main",
    "abstract": "We introduce a novel 3D generation method for versatile and high-quality 3D asset creation.The cornerstone is a unified Structured LATent (SLAT) representation which allows decoding to different output formats, such as Radiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a sparsely-populated 3D grid with dense multiview visual features extracted from a powerful vision foundation model, comprehensively capturing both structural (geometry) and textural (appearance) information while maintaining flexibility during decoding.We employ rectified flow transformers tailored for SLAT as our 3D generation models and train models with up to 2 billion parameters on a large 3D asset dataset of 500K diverse objects. Our model generates high-quality results with text or image conditions, significantly surpassing existing methods, including recent ones at similar scales. We showcase flexible output format selection and local 3D editing capabilities which were not offered by previous models. Code, model, and data will be released",
    "checked": true,
    "id": "5666d551bc9f86e2f379ede8b8ffddbe4d1d53a8",
    "semantic_title": "structured 3d latents for scalable and versatile 3d generation",
    "citation_count": 211,
    "authors": [
      "Jianfeng Xiang",
      "Zelong Lv",
      "Sicheng Xu",
      "Yu Deng",
      "Ruicheng Wang",
      "Bowen Zhang",
      "Dong Chen",
      "Xin Tong",
      "Jiaolong Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kawana_GA3CE_Unconstrained_3D_Gaze_Estimation_with_Gaze-Aware_3D_Context_Encoding_CVPR_2025_paper.html": {
    "title": "GA3CE: Unconstrained 3D Gaze Estimation with Gaze-Aware 3D Context Encoding",
    "volume": "main",
    "abstract": "We propose a novel 3D gaze estimation approach that learns spatial relationships between the subject and objects in the scene, and outputs 3D gaze direction. Our method targets unconstrained settings, including cases where close-up views of the subject's eyes are unavailable, such as when the subject is distant or facing away. Previous approaches typically rely on either 2D appearance alone or incorporate limited spatial cues using depth maps in the non-learnable post-processing step. Estimating 3D gaze direction from 2D observations in these scenarios is challenging; variations in subject pose, scene layout, and gaze direction, combined with differing camera poses, yield diverse 2D appearances and 3D gaze directions even when targeting the same 3D scene. To address this issue, we propose GA3CE: Gaze-Aware 3D Context Encoding. Our method represents subject and scene using 3D poses and object positions, treating them as 3D context to learn spatial relationships in 3D space. Inspired by human vision, we align this context in an egocentric space, significantly reducing spatial complexity. Furthermore, we propose D^3 (direction-distance-decomposed) positional encoding to better capture the spatial relationship between 3D context and gaze direction in direction and distance space. Experiments demonstrate substantial improvements, reducing mean angle error by 13%-37% compared to leading baselines on benchmark datasets in single-frame settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuki Kawana",
      "Shintaro Shiba",
      "Quan Kong",
      "Norimasa Kobori"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qiu_Self-Cross_Diffusion_Guidance_for_Text-to-Image_Synthesis_of_Similar_Subjects_CVPR_2025_paper.html": {
    "title": "Self-Cross Diffusion Guidance for Text-to-Image Synthesis of Similar Subjects",
    "volume": "main",
    "abstract": "Diffusion models achieved unprecedented fidelity and diversity for synthesizing image, video, 3D assets, etc. However, subject mixing is an unresolved issue for diffusion-based image synthesis, particularly for synthesizing multiple similar-looking subjects. We propose Self-Cross Diffusion Guidance to penalize the overlap between cross-attention maps and the aggregated self-attention map. Compared to previous methods based on self-attention or cross-attention alone, our guidance is more effective in eliminating subject mixing. What's more, our guidance addresses subject mixing for all relevant patches beyond the most discriminant one, e.g., the beak of a bird. For each subject, we aggregate self-attention maps of patches with higher cross-attention values. Thus, the aggregated self-attention map forms a region that the whole subject attends to. Our training-free method boosts the performance of both Unet-based and Transformer-based diffusion models such as the Stable Diffusion series. We also release a similar subjects dataset (SSD), a challenging benchmark, and utilize GPT-4o for automatic and reliable evaluation. Extensive qualitative and quantitative results demonstrate the effectiveness of our self-cross diffusion guidance",
    "checked": true,
    "id": "917cfbdb225f9f6f45ae5c7be1fc4ca5741a0886",
    "semantic_title": "self-cross diffusion guidance for text-to-image synthesis of similar subjects",
    "citation_count": 3,
    "authors": [
      "Weimin Qiu",
      "Jieke Wang",
      "Meng Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yao_RigGS_Rigging_of_3D_Gaussians_for_Modeling_Articulated_Objects_in_CVPR_2025_paper.html": {
    "title": "RigGS: Rigging of 3D Gaussians for Modeling Articulated Objects in Videos",
    "volume": "main",
    "abstract": "This paper considers the problem of modeling articulated objects captured in 2D videos to enable novel view synthesis, while also being easily editable, drivable, and reposable. To tackle this challenging problem, we propose RigGS, a new paradigm that leverages 3D Gaussian representation and skeleton-based motion representation to model dynamic objects without utilizing additional template priors. Specifically, we first propose skeleton-aware node-controlled deformation, which deforms a canonical 3D Gaussian representation over time to initialize the modeling process, producing candidate skeleton nodes that are further simplified into a sparse 3D skeleton according to their motion and semantic information. Subsequently, based on the resulting skeleton, we design learnable skin deformations and pose-dependent detailed deformations, thereby easily deforming the 3D Gaussian representation to generate new actions and render further high-quality images from novel views. Extensive experiments demonstrate that our method can generate realistic new actions easily for objects and achieve high-quality rendering",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Yao",
      "Zhi Deng",
      "Junhui Hou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Noise_Modeling_in_One_Hour_Minimizing_Preparation_Efforts_for_Self-supervised_CVPR_2025_paper.html": {
    "title": "Noise Modeling in One Hour: Minimizing Preparation Efforts for Self-supervised Low-Light RAW Image Denoising",
    "volume": "main",
    "abstract": "Noise synthesis is a promising solution for addressing the data shortage problem in data-driven low-light RAW image denoising. However, accurate noise synthesis methods often necessitate labor-intensive calibration and profiling procedures during preparation, preventing them from landing to practice at scale. This work introduces a practically simple noise synthesis pipeline based on detailed analyses of noise properties and extensive justification of widespread techniques. Compared to other approaches, our proposed pipeline eliminates the cumbersome system gain calibration and signal-independent noise profiling steps, reducing the preparation time for noise synthesis from days to hours. Meanwhile, our method exhibits strong denoising performance, showing an up to 0.54dB PSNR improvement over the current state-of-the-art noise synthesis technique",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feiran Li",
      "Haiyang Jiang",
      "Daisuke Iso"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Adv-CPG_A_Customized_Portrait_Generation_Framework_with_Facial_Adversarial_Attacks_CVPR_2025_paper.html": {
    "title": "Adv-CPG: A Customized Portrait Generation Framework with Facial Adversarial Attacks",
    "volume": "main",
    "abstract": "Recent Customized Portrait Generation (CPG) methods, taking a facial image and a textual prompt as inputs, have attracted substantial attention. Although these methods generate high-fidelity portraits, they fail to prevent the generated portraits from being tracked and misused by malicious face recognition systems. To address this, this paper proposes a Customized Portrait Generation framework with facial Adversarial attacks (Adv-CPG). Specifically, to achieve facial privacy protection, we devise a lightweight local ID encryptor and an encryption enhancer. They implement progressive double-layer encryption protection by directly injecting the target identity and adding additional identity guidance, respectively. Furthermore, to accomplish fine-grained and personalized portrait generation, we develop a multi-modal image customizer capable of generating controlled fine-grained facial features. To the best of our knowledge, Adv-CPG is the first study that introduces facial adversarial attacks into CPG. Extensive experiments demonstrate the superiority of Adv-CPG, e.g., the average attack success rate of the proposed Adv-CPG is 28.1% and 2.86% higher compared to the SOTA noise-based attack methods and unconstrained attack methods, respectively",
    "checked": true,
    "id": "432428e09747b490ec2bd6a8bc5aecc160f06c0c",
    "semantic_title": "adv-cpg: a customized portrait generation framework with facial adversarial attacks",
    "citation_count": 9,
    "authors": [
      "Junying Wang",
      "Hongyuan Zhang",
      "Yuan Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mehrab_Fish-Vista_A_Multi-Purpose_Dataset_for_Understanding__Identification_of_Traits_CVPR_2025_paper.html": {
    "title": "Fish-Vista: A Multi-Purpose Dataset for Understanding & Identification of Traits from Images",
    "volume": "main",
    "abstract": "We introduce Fish-Visual Trait Analysis (Fish-Vista), the first organismal image dataset designed for the analysis of visual traits of aquatic species directly from images using machine learning and computer vision methods. Fish-Vista contains 69,269 annotated images spanning 4,316 fish species, curated and organized to serve three downstream tasks: species classification, trait identification, and trait segmentation. Our work makes two key contributions. First, we provide a fully reproducible data processing pipeline to process fish images sourced from various museum collections, contributing to the advancement of AI in biodiversity science. We annotate the images with carefully curated labels from biological databases and manual annotations to create an AI-ready dataset of visual traits. Second, our work offers fertile grounds for researchers to develop novel methods for a variety of problems in computer vision such as handling long-tailed distributions, out-of-distribution generalization, learning with weak labels, explainable AI, and segmenting small objects. Dataset and code for Fish-Vista are available at https://github.com/Imageomics/Fish-Vista",
    "checked": true,
    "id": "b427d30b218c3325cb801f9ba6b3a5336cf5d953",
    "semantic_title": "fish-vista: a multi-purpose dataset for understanding & identification of traits from images",
    "citation_count": 6,
    "authors": [
      "Kazi Sajeed Mehrab",
      "M. Maruf",
      "Arka Daw",
      "Abhilash Neog",
      "Harish Babu Manogaran",
      "Mridul Khurana",
      "Zhenyang Feng",
      "Bahadir Altintas",
      "Yasin Bakis",
      "Elizabeth G Campolongo",
      "Matthew J Thompson",
      "Xiaojun Wang",
      "Hilmar Lapp",
      "Tanya Berger-Wolf",
      "Paula Mabee",
      "Henry Bart",
      "Wei-Lun Chao",
      "Wasila M Dahdul",
      "Anuj Karpatne"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_High_Dynamic_Range_Video_Compression_A_Large-Scale_Benchmark_Dataset_and_CVPR_2025_paper.html": {
    "title": "High Dynamic Range Video Compression: A Large-Scale Benchmark Dataset and A Learned Bit-depth Scalable Compression Algorithm",
    "volume": "main",
    "abstract": "Recently, learned video compression (LVC) is undergoing a period of rapid development. However, due to absence of large and high-quality high dynamic range (HDR) video training data, LVC on HDR video is still unexplored. In this paper, we are the first to collect a large-scale HDR video benchmark dataset, named HDRVD2K, featuring huge quantity, diverse scenes and multiple motion types. HDRVD2K fills gaps of video training data and facilitate the development of LVC on HDR videos. Based on HDRVD2K, we further propose the first learned bit-depth scalable video compression (LBSVC) network for HDR videos by effectively exploiting bit-depth redundancy between videos of multiple dynamic ranges. To achieve this, we first propose a compression-friendly bit-depth enhancement module (BEM) to effectively predict original HDR videos based on compressed tone-mapped low dynamic range (LDR) videos and dynamic range prior, instead of reducing redundancy only through spatio-temporal predictions. Our method greatly improves the reconstruction quality and compression performance on HDR videos. Extensive experiments demonstrate the effectiveness of HDRVD2K on learned HDR video compression and great compression performance of our proposed LBSVC network",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyi Tian",
      "Feifeng Wang",
      "Shiwei Wang",
      "Zihao Zhou",
      "Yao Zhu",
      "Liquan Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lei_OffsetOPT_Explicit_Surface_Reconstruction_without_Normals_CVPR_2025_paper.html": {
    "title": "OffsetOPT: Explicit Surface Reconstruction without Normals",
    "volume": "main",
    "abstract": "Neural surface reconstruction has been dominated by implicit representations with marching cubes for explicit surface extraction. However, those methods typically require high-quality normals for accurate reconstruction. We propose OffsetOPT, a method that reconstructs explicit surfaces directly from 3D point clouds and eliminates the need for point normals. The approach comprises two stages: first, we train a neural network to predict surface triangles based on local point geometry, given uniformly distributed training point clouds. Next, we apply the frozen network to reconstruct surfaces from unseen point clouds by optimizing a per-point offset to maximize the accuracy of triangle predictions. Compared to state-of-the-art methods, OffsetOPT not only excels at reconstructing overall surfaces but also significantly preserves sharp surface features. We demonstrate its accuracy on popular benchmarks, including small-scale shapes and large-scale open surfaces",
    "checked": true,
    "id": "de057de3da9b4d471a7f3bcc3667315bfb876d36",
    "semantic_title": "offsetopt: explicit surface reconstruction without normals",
    "citation_count": 0,
    "authors": [
      "Huan Lei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/So_PCM__Picard_Consistency_Model_for_Fast_Parallel_Sampling_of_CVPR_2025_paper.html": {
    "title": "PCM : Picard Consistency Model for Fast Parallel Sampling of Diffusion Models",
    "volume": "main",
    "abstract": "Recently, diffusion models have achieved significant advances in vision, text, and robotics. However, they still face slow generation speeds due to sequential denoising processes. To address this, a parallel sampling method based on Picard iteration was introduced, effectively reducing sequential steps while ensuring exact convergence to the original output. Nonetheless, Picard iteration does not guarantee faster convergence, which can still result in slow generation in practice. In this work, we propose a new parallelization scheme, the Picard Consistency Model (PCM), which significantly reduces the number of generation steps in Picard iteration. Inspired by the consistency model, PCM is directly trained to predict the fixed-point solution, or the final output, at any stage of the convergence trajectory. Additionally, we introduce a new concept called model switching, which addresses PCM's limitations and ensures exact convergence. Extensive experiments demonstrate that PCM achieves up to a 2.71x speedup over sequential sampling and a 1.77x speedup over Picard iteration across various tasks, including image generation and robotic control",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhyuk So",
      "Jiwoong Shin",
      "Chaeyeon Jang",
      "Eunhyeok Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jang_CoMapGS_Covisibility_Map-based_Gaussian_Splatting_for_Sparse_Novel_View_Synthesis_CVPR_2025_paper.html": {
    "title": "CoMapGS: Covisibility Map-based Gaussian Splatting for Sparse Novel View Synthesis",
    "volume": "main",
    "abstract": "We propose Covisibility Map-based Gaussian Splatting (CoMapGS), designed to recover underrepresented sparse regions in sparse novel view synthesis. CoMapGS addresses both high- and low-uncertainty regions by constructing covisibility maps, enhancing initial point clouds, and applying uncertainty-aware weighted supervision using a proximity classifier. Our contributions are threefold: (1) CoMapGS reframes novel view synthesis by leveraging covisibility maps as a core component to address region-specific uncertainty; (2) Enhanced initial point clouds for both low- and high-uncertainty regions compensate for sparse COLMAP-derived point clouds, improving reconstruction quality and benefiting few-shot 3DGS methods; (3) Adaptive supervision with covisibility-score-based weighting and proximity classification achieves consistent performance gains across scenes with varying sparsity scores derived from covisibility maps. Experimental results demonstrate that CoMapGS outperforms state-of-the-art methods on datasets including Mip-NeRF 360 and LLFF",
    "checked": true,
    "id": "6c05a56b3cef3286eabd6061da922278c40b44a0",
    "semantic_title": "comapgs: covisibility map-based gaussian splatting for sparse novel view synthesis",
    "citation_count": 2,
    "authors": [
      "Youngkyoon Jang",
      "Eduardo P√©rez-Pellitero"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Karageorgiou_Any-Resolution_AI-Generated_Image_Detection_by_Spectral_Learning_CVPR_2025_paper.html": {
    "title": "Any-Resolution AI-Generated Image Detection by Spectral Learning",
    "volume": "main",
    "abstract": "Recent works have established that AI models introduce spectral artifacts into generated images and propose approaches for learning to capture them using labeled data. However, the significant differences in such artifacts among different generative models hinder these approaches from generalizing to generators not seen during training. In this work, we build upon the key idea that the spectral distribution of real images constitutes both an invariant and highly discriminative pattern for AI-generated image detection. To model this under a self-supervised setup, we employ masked spectral learning using the pretext task of frequency reconstruction. Since generated images constitute out-of-distribution samples for this model, we propose spectral reconstruction similarity to capture this divergence. Moreover, we introduce spectral context attention, which enables our approach to efficiently capture subtle spectral inconsistencies in images of any resolution. Our spectral AI-generated image detection approach (SPAI) achieves a 5.5% absolute improvement in AUC over the previous state-of-the-art across 13 recent generative approaches, while exhibiting robustness against common online perturbations. Code is available on https://mever-team.github.io/spai",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dimitrios Karageorgiou",
      "Symeon Papadopoulos",
      "Ioannis Kompatsiaris",
      "Efstratios Gavves"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Alvar_DivPrune_Diversity-based_Visual_Token_Pruning_for_Large_Multimodal_Models_CVPR_2025_paper.html": {
    "title": "DivPrune: Diversity-based Visual Token Pruning for Large Multimodal Models",
    "volume": "main",
    "abstract": "Large Multimodal Models (LMMs) have emerged as powerful models capable of understanding various data modalities, including text, images, and videos. LMMs encode both text and visual data into tokens that are then combined and processed by an integrated Large Language Model (LLM). Including visual tokens substantially increases the total token count, often by thousands. The increased input length for LLM significantly raises the complexity of inference, resulting in high latency in LMMs. To address this issue, token pruning methods, which remove part of the visual tokens, are proposed. The existing token pruning methods either require extensive calibration and fine-tuning or rely on suboptimal importance metrics which results in increased redundancy among the retained tokens. In this paper, we first formulate token pruning as Max-Min Diversity Problem (MMDP) where the goal is to select a subset such that the diversity among the selected tokens is maximized. Then, we solve the MMDP to obtain the selected subset and prune the rest. The proposed method, DivPrune, reduces redundancy and achieves the highest diversity of the selected tokens. By ensuring high diversity, the selected tokens better represent the original tokens, enabling effective performance even at high pruning ratios without requiring fine-tuning. Extensive experiments with various LMMs show that DivPrune achieves state-of-the-art accuracy over 16 image- and video-language datasets. Additionally, DivPrune reduces both the end-to-end latency and GPU memory usage for the tested models. The code is available here",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saeed Ranjbar Alvar",
      "Gursimran Singh",
      "Mohammad Akbari",
      "Yong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Training_Data_Provenance_Verification_Did_Your_Model_Use_Synthetic_Data_CVPR_2025_paper.html": {
    "title": "Training Data Provenance Verification: Did Your Model Use Synthetic Data from My Generative Model for Training?",
    "volume": "main",
    "abstract": "High-quality open-source text-to-image models have lowered the threshold for obtaining photorealistic images significantly, but also face potential risks of misuse. Specifically, suspects may use synthetic data generated by these generative models to train models for specific tasks without permission, when lacking real data resources especially. Protecting these generative models is crucial for the well-being of their owners. In this work, we propose the first method to this important yet unresolved issue, called Training data Provenance Verification (TrainProVe). The rationale behind TrainProVe is grounded in the principle of generalization error bound, which suggests that, for two models with the same task, if the distance between their training data distributions is smaller, their generalization ability will be closer. We validate the efficacy of TrainProVe across four text-to-image models (Stable Diffusion v1.4, latent consistency model, PixArt-\\alpha, and Stable Cascade). The results show that TrainProVe achieves a verification accuracy of over 99% in determining the provenance of suspicious model training data, surpassing all previous methods. Code is available at https://github.com/xieyc99/TrainProVe",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuechen Xie",
      "Jie Song",
      "Huiqiong Wang",
      "Mingli Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wei_3D-AVS_LiDAR-based_3D_Auto-Vocabulary_Segmentation_CVPR_2025_paper.html": {
    "title": "3D-AVS: LiDAR-based 3D Auto-Vocabulary Segmentation",
    "volume": "main",
    "abstract": "Open-vocabulary segmentation methods offer promising capabilities in detecting unseen object categories, but the category must be aware and needs to be provided by a human, either via a text prompt or pre-labeled datasets, thus limiting their scalability. We propose 3D-AVS, a method for Auto-Vocabulary Segmentation of 3D point clouds for which the vocabulary is unknown and auto-generated for each input at runtime, thus eliminating the human in the loop and typically providing a substantially larger vocabulary for richer annotations. 3D-AVS first recognizes semantic entities from image or point cloud data and then segments all points with the automatically generated vocabulary. Our method incorporates both image-based and point-based recognition, enhancing robustness under challenging lighting conditions where geometric information from LiDAR is especially valuable. Our point-based recognition features a Sparse Masked Attention Pooling (SMAP) module to enrich the diversity of recognized objects. To address the challenges of evaluating unknown vocabularies and avoid annotation biases from label synonyms, hierarchies, or semantic overlaps, we introduce the annotation-free Text-Point Semantic Similarity (TPSS) metric for assessing generated vocabulary quality. Our evaluations on nuScenes and ScanNet200 demonstrate 3D-AVS's ability to generate semantic classes with accurate point-wise segmentations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijie Wei",
      "Osman √úlger",
      "Fatemeh Karimi Nejadasl",
      "Theo Gevers",
      "Martin R. Oswald"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_STOP_Integrated_Spatial-Temporal_Dynamic_Prompting_for_Video_Understanding_CVPR_2025_paper.html": {
    "title": "STOP: Integrated Spatial-Temporal Dynamic Prompting for Video Understanding",
    "volume": "main",
    "abstract": "Pre-trained on tremendous image-text pairs, vision-language models like CLIP have demonstrated promising zero-shot generalization across numerous image-based tasks. However, extending these capabilities to video tasks remains challenging due to limited labeled video data and high training costs. Recent video prompting methods attempt to adapt CLIP for video tasks by introducing learnable prompts, but they typically rely on a single static prompt for all video sequences, overlooking the diverse temporal dynamics and spatial variations that exist across frames. This limitation significantly hinders the model's ability to capture essential temporal information for effective video understanding. To address this, we propose an integrated Spatial-TempOral dynamic Prompting (STOP) model which consists of two complementary modules, the intra-frame spatial prompting and inter-frame temporal prompting. Our intra-frame spatial prompts are designed to adaptively highlight discriminative regions within each frame by leveraging intra-frame attention and temporal variation, allowing the model to focus on areas with substantial temporal dynamics and capture fine-grained spatial details. Additionally, to highlight the varying importance of frames for video understanding, we further introduce inter-frame temporal prompts, dynamically inserting prompts between frames with high temporal variance as measured by frame similarity. This enables the model to prioritize key frames and enhances its capacity to understand temporal dependencies across sequences. Extensive experiments on various video benchmarks demonstrate that STOP consistently achieves superior performance against state-of-the-art methods. The code is available at https://github.com/zhoujiahuan1991/CVPR2025-STOP",
    "checked": true,
    "id": "a51f8f0642b869f6bfb45bae6b873874e0c91209",
    "semantic_title": "stop: integrated spatial-temporal dynamic prompting for video understanding",
    "citation_count": 5,
    "authors": [
      "Zichen Liu",
      "Kunlun Xu",
      "Bing Su",
      "Xu Zou",
      "Yuxin Peng",
      "Jiahuan Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_TimeTracker_Event-based_Continuous_Point_Tracking_for_Video_Frame_Interpolation_with_CVPR_2025_paper.html": {
    "title": "TimeTracker: Event-based Continuous Point Tracking for Video Frame Interpolation with Non-linear Motion",
    "volume": "main",
    "abstract": "Video frame interpolation (VFI) that leverages the bio-inspired event cameras as guidance has recently shown better performance and memory efficiency than the frame-based methods, thanks to the event cameras' advantages, such as high temporal resolution. A hurdle for event-based VFI is how to effectively deal with non-linear motion, caused by the dynamic changes in motion direction and speed within the scene. Existing methods either use events to estimate sparse optical flow or fuse events with image features to estimate dense optical flow. Unfortunately, motion errors often degrade the VFI quality as the continuous motion cues from events do not align with the dense spatial information of images in the temporal dimension. In this paper, we find that object motion is continuous in space, tracking local regions over continuous time enables more accurate identification of spatiotemporal feature correlations. In light of this, we propose a novel continuous point tracking-based VFI framework, named TimeTracker. Specifically, we first design a Scene-Aware Region Segmentation (SARS) module to divide the scene into similar patches. Then, a Continuous Trajectory guided Motion Estimation (CTME) module is proposed to track the continuous motion trajectory of each patch through events. Finally, intermediate frames at any given time are generated through global motion optimization and frame refinement. Moreover, we collect a real-world dataset that features fast non-linear motion. Extensive experiments show that our method outperforms prior arts in both motion estimation and frame interpolation quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyue Liu",
      "Jinghan Xu",
      "Yi Chang",
      "Hanyu Zhou",
      "Haozhi Zhao",
      "Lin Wang",
      "Luxin Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Improving_the_Training_of_Data-Efficient_GANs_via_Quality_Aware_Dynamic_CVPR_2025_paper.html": {
    "title": "Improving the Training of Data-Efficient GANs via Quality Aware Dynamic Discriminator Rejection Sampling",
    "volume": "main",
    "abstract": "Data-Efficient Generative Adversarial Nets (DE-GANs) have become more and more popular in recent years. Existing methods apply data augmentation, noise injection and pre-trained models to maximumly increase the number of training samples thus improving the training of DE-GANs. However, none of these methods considers the sample quality during training, which can also significantly influence the training of DE-GANs. Focusing on sample quality during training, in this paper, we are the first to incorporate discriminator rejection sampling (DRS) into the training process and introduce a novel method, called quality aware dynamic discriminator rejection sampling (QADDRS). Specifically, QADDRS consists of two steps: (1) the sample quality aware step, which aims to obtain the sorted critic scores, i.e., the ordered discriminator outputs, on real/fake samples in the current training stage; (2) the dynamic rejection step that obtains dynamic rejection number N, where N is controlled by the overfitting degree of discriminator (D) during training. When updating the parameters of D, the N high critic score real samples and the N low critic score fake samples in the minibatch are rejected dynamically based on the overfitting degree of D. As a result, QADDRS can avoid D becoming overly confident in distinguishing both real and fake samples, thereby alleviating the overfitting of D issue during training. Extensive experiments on several datasets demonstrate that integrating QADDRS into different DE-GANs can achieve better performance and deliver state-of-the-art results. Codes are available at https://github.com/zzhang05/QADDRS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyu Zhang",
      "Yang Hua",
      "Guanxiong Sun",
      "Hui Wang",
      "Se√°n McLoone"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Shading_Meets_Motion_Self-supervised_Indoor_3D_Reconstruction_Via_Simultaneous_Shape-from-Shading_CVPR_2025_paper.html": {
    "title": "Shading Meets Motion: Self-supervised Indoor 3D Reconstruction Via Simultaneous Shape-from-Shading and Structure-from-Motion",
    "volume": "main",
    "abstract": "Scene reconstruction has a wide range of applications in computer vision and robotics. To build practical constraints and feature Scene reconstruction has a wide range of applications in computer vision and robotics. To build practical constraints and feature correspondences, rich textures and distinguished gradient variations are particularly required in classic and learning-based SfM. When building low-texture regions with repeated patterns, especially mostly-white indoor rooms, there is a significant drop in performance. In this work, we propose Shading-SfM-Net, a novel framework for simultaneously learning a shape-from-shading network based on the inverse rendering constraint and a structure-from-motion framework based on warped keypoint and geometric consistency, to improve structure-from-motion and surface reconstruction for low-texture indoor scenes. Shading-SfM-Net tightly incorporates the surface shape consistency and 3D geometric registration loss in order to dig into their mutual information and further overcome the instability on flat regions. We evaluate the proposed framework on texture-less indoor scenes (NYUv2 and ScanNet), and show that by simultaneously learning shading, motion and shape, our pipeline is able to achieve state-of-the-art performance with superior generalization capability for unseen texture-less datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoyu Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bhattacharjee_Believing_is_Seeing_Unobserved_Object_Detection_using_Generative_Models_CVPR_2025_paper.html": {
    "title": "Believing is Seeing: Unobserved Object Detection using Generative Models",
    "volume": "main",
    "abstract": "Can objects that are not visible in an image---but are in the vicinity of the camera---be detected? This study introduces the novel tasks of 2D, 2.5D and 3D unobserved object detection for predicting the location of nearby objects that are occluded or lie outside the image frame. We adapt several state-of-the-art pre-trained generative models to address this task, including 2D and 3D diffusion models and vision-language models, and show that they can be used to infer the presence of objects that are not directly observed. To benchmark this task, we propose a suite of metrics that capture different aspects of performance. Our empirical evaluation on indoor scenes from the RealEstate10k and NYU Depth v2 datasets demonstrate results that motivate the use of generative models for the unobserved object detection task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Subhransu S. Bhattacharjee",
      "Dylan Campbell",
      "Rahul Shome"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_MotionStone_Decoupled_Motion_Intensity_Modulation_with_Diffusion_Transformer_for_Image-to-Video_CVPR_2025_paper.html": {
    "title": "MotionStone: Decoupled Motion Intensity Modulation with Diffusion Transformer for Image-to-Video Generation",
    "volume": "main",
    "abstract": "The image-to-video (I2V) generation is conditioned on the static image, which has been enhanced recently by the motion intensity as an additional control signal. These motion-aware models are appealing to generate diverse motion patterns, yet there lacks a reliable motion estimator for training such models on large-scale video set in the wild. Traditional metrics, e.g., SSIM or optical flow, are hard to generalize to arbitrary videos, while, it is very tough for human annotators to label the abstract motion intensity neither. Furthermore, the motion intensity shall reveal both local object motion and global camera movement, which has not been studied before. This paper addresses the challenge with a new motion estimator, capable of measuring the decoupled motion intensities of objects and cameras in video. We leverage the contrastive learning on randomly paired videos and distinguish the video with greater motion intensity. Such a paradigm is friendly for annotation and easy to scale up to achieve stable performance on motion estimation. We then present a new I2V model, named MotionStone, developed with the decoupled motion estimator. Experimental results demonstrate the stability of the proposed motion estimator and the state-of-the-art performance of MotionStone on I2V generation. These advantages warrant the decoupled motion estimator to serve as a general plug-in enhancer for both data processing and video generation training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuwei Shi",
      "Biao Gong",
      "Xi Chen",
      "Dandan Zheng",
      "Shuai Tan",
      "Zizheng Yang",
      "Yuyuan Li",
      "Jingwen He",
      "Kecheng Zheng",
      "Jingdong Chen",
      "Ming Yang",
      "Yinqiang Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pan_NLPrompt_Noise-Label_Prompt_Learning_for_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "NLPrompt: Noise-Label Prompt Learning for Vision-Language Models",
    "volume": "main",
    "abstract": "The emergence of vision-language foundation models, such as CLIP, has revolutionized image-text representation, enabling a broad range of applications via prompt learning. Despite its promise, real-world datasets often contain noisy labels that can degrade prompt learning performance. In this paper, we demonstrate that using mean absolute error (MAE) loss in prompt learning, named PromptMAE, significantly enhances robustness against noisy labels while maintaining high accuracy. Though MAE is straightforward and recognized for its robustness, it is rarely used in noisy-label learning due to its slow convergence and poor performance outside prompt learning scenarios. To elucidate the robustness of PromptMAE, we leverage feature learning theory to show that MAE can suppress the influence of noisy samples, thereby improving the signal-to-noise ratio and enhancing overall robustness. Additionally, we introduce PromptOT, a prompt-based optimal transport data purification method to enhance the robustness further. PromptOT employs text encoder representations in vision-language models as prototypes to construct an optimal transportation matrix. This matrix effectively partitions datasets into clean and noisy subsets, allowing for the application of cross-entropy loss to the clean subset and MAE loss to the noisy subset. Our Noise-Label Prompt Learning method, named NLPrompt, offers a simple and efficient approach that leverages the expressive representation and precise alignment capabilities of vision-language models for robust prompt learning. We validate NLPrompt through extensive experiments across various noise settings, demonstrating significant performance improvements",
    "checked": true,
    "id": "136bafd5c49715f8c4644e664805103132eb8503",
    "semantic_title": "nlprompt: noise-label prompt learning for vision-language models",
    "citation_count": 2,
    "authors": [
      "Bikang Pan",
      "Qun Li",
      "Xiaoying Tang",
      "Wei Huang",
      "Zhen Fang",
      "Feng Liu",
      "Jingya Wang",
      "Jingyi Yu",
      "Ye Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fiche_MEGA_Masked_Generative_Autoencoder_for_Human_Mesh_Recovery_CVPR_2025_paper.html": {
    "title": "MEGA: Masked Generative Autoencoder for Human Mesh Recovery",
    "volume": "main",
    "abstract": "Human Mesh Recovery (HMR) from a single RGB image is a highly ambiguous problem, as an infinite set of 3D interpretations can explain the 2D observation equally well. Nevertheless, most HMR methods overlook this issue and make a single prediction without accounting for this ambiguity. A few approaches generate a distribution of human meshes, enabling the sampling of multiple predictions; however, none of them is competitive with the latest single-output model when making a single prediction. This work proposes a new approach based on masked generative modeling. By tokenizing the human pose and shape, we formulate the HMR task as generating a sequence of discrete tokens conditioned on an input image. We introduce MEGA, a MaskEd Generative Autoencoder trained to recover human meshes from images and partial human mesh token sequences. Given an image, our flexible generation scheme allows us to predict a single human mesh in deterministic mode or to generate multiple human meshes in stochastic mode. Experiments on in-the-wild benchmarks show that MEGA achieves state-of-the-art performance in deterministic and stochastic modes, outperforming single-output and multi-output approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gu√©nol√© Fiche",
      "Simon Leglaive",
      "Xavier Alameda-Pineda",
      "Francesc Moreno-Noguer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_PBR-NeRF_Inverse_Rendering_with_Physics-Based_Neural_Fields_CVPR_2025_paper.html": {
    "title": "PBR-NeRF: Inverse Rendering with Physics-Based Neural Fields",
    "volume": "main",
    "abstract": "We tackle the ill-posed inverse rendering problem in 3D reconstruction with a Neural Radiance Field (NeRF) approach informed by Physics-Based Rendering (PBR) theory, named PBR-NeRF. Our method addresses a key limitation in most NeRF and 3D Gaussian Splatting approaches: they estimate view-dependent appearance without modeling scene materials and illumination. To address this limitation, we present an inverse rendering (IR) model capable of jointly estimating scene geometry, materials, and illumination. Our model builds upon recent NeRF-based IR approaches, but crucially introduces two novel physics-based priors that better constrain the IR estimation. Our priors are rigorously formulated as intuitive loss terms and achieve state-of-the-art material estimation without compromising novel view synthesis quality. Our method is easily adaptable to other inverse rendering and 3D reconstruction frameworks that require material estimation. We demonstrate the importance of extending current neural rendering approaches to fully model scene properties beyond geometry and view-dependent appearance. Code is publicly available at: https://github.com/s3anwu/pbrnerf",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sean Wu",
      "Shamik Basu",
      "Tim Broedermann",
      "Luc Van Gool",
      "Christos Sakaridis"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Muthukumar_Disentangling_Safe_and_Unsafe_Image_Corruptions_via_Anisotropy_and_Locality_CVPR_2025_paper.html": {
    "title": "Disentangling Safe and Unsafe Image Corruptions via Anisotropy and Locality",
    "volume": "main",
    "abstract": "State-of-the-art machine learning systems are vulnerable to small perturbations to their input, where _small_ is defined according to a threat model that assigns a positive threat to each perturbation. Most prior works define a task-agnostic, isotropic, and global threat, like the l_p norm, where the magnitude of the perturbation fully determines the degree of the threat and neither the direction of the attack nor its position in space matter. However, common corruptions in computer vision, such as blur, compression, or occlusions, are not well captured by such treat models. This paper proposes a novel threat model called \\texttt Projected Displacement (PD) to study robustness beyond existing isotropic and global threat models. The proposed threat model measures the threat of a perturbation via its alignment with _unsafe directions_, defined as directions in the input space along which a perturbation of sufficient magnitude changes the ground truth class label. Unsafe directions are identified locally for each input based on observed training data. In this way, the PD-threat model exhibits anisotropy and locality. The PD-threat model is computationally efficient and can be easily integrated into existing robustness pipelines. Experiments on Imagenet-1k data indicate that, for any input, the set of perturbations with small PD threat includes _safe_ perturbations of large l_p norm that preserve the true label, such as noise, blur and compression, while simultaneously excluding _unsafe_ perturbations that alter the true label. Unlike perceptual threat models based on embeddings of large-vision models, the PD-threat model can be readily computed for arbitrary classification tasks without pre-training or finetuning. Further additional task information such as sensitivity to image regions or concept hierarchies can be easily integrated into the assessment of threat and thus the PD threat model presents practitioners a flexible, task-driven threat specification that alleviates the limitations of l_p-threat models",
    "checked": true,
    "id": "04aa16b41d39af8c5298246ed31c75d42dd0ff8d",
    "semantic_title": "disentangling safe and unsafe image corruptions via anisotropy and locality",
    "citation_count": 0,
    "authors": [
      "Ramchandran Muthukumar",
      "Ambar Pal",
      "Jeremias Sulam",
      "Rene Vidal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Prometheus_3D-Aware_Latent_Diffusion_Models_for_Feed-Forward_Text-to-3D_Scene_Generation_CVPR_2025_paper.html": {
    "title": "Prometheus: 3D-Aware Latent Diffusion Models for Feed-Forward Text-to-3D Scene Generation",
    "volume": "main",
    "abstract": "In this work, we introduce Prometheus, a 3D-aware latent diffusion model for text-to-3D generation at both object and scene levels in seconds. We formulate 3D scene generation as multi-view, feed-forward, pixel-aligned 3D Gaussian generation within the latent diffusion paradigm. To ensure generalizability, we build our model upon pre-trained text-to-image generation model with only minimal adjustments and further train it using a large number of images from both single-view and multi-view datasets. Furthermore, we introduce an RGB-D latent space into 3D Gaussian generation to disentangle appearance and geometry information, enabling efficient feed-forward generation of 3D Gaussians with better fidelity and geometry. Extensive experimental results demonstrate the effectiveness of our method in both feed-forward 3D Gaussian reconstruction and text-to-3D generation",
    "checked": true,
    "id": "699aeed32ca75d19ded2e2b6d23e32ea63578c87",
    "semantic_title": "prometheus: 3d-aware latent diffusion models for feed-forward text-to-3d scene generation",
    "citation_count": 8,
    "authors": [
      "Yuanbo Yang",
      "Jiahao Shao",
      "Xinyang Li",
      "Yujun Shen",
      "Andreas Geiger",
      "Yiyi Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qin_No_Pains_More_Gains_Recycling_Sub-Salient_Patches_for_Efficient_High-Resolution_CVPR_2025_paper.html": {
    "title": "No Pains, More Gains: Recycling Sub-Salient Patches for Efficient High-Resolution Image Recognition",
    "volume": "main",
    "abstract": "Over the last decade, many notable methods have emerged to tackle the computational resource challenge of the high resolution image recognition (HRIR). They typically focus on identifying and aggregating a few salient regions for classification, discarding sub-salient areas for low training consumption. Nevertheless, many HRIR tasks necessitate the exploration of wider regions to model objects and contexts, which limits their performance in such scenarios. To address this issue, we present a DBPS strategy to enable training with more patches at low consumption. Specifically, in addition to a fundamental buffer that stores the embeddings of most salient patches, DBPS further employs an auxiliary buffer to recycle those sub-salient ones. To reduce the computational cost associated with gradients of sub-salient patches, these patches are primarily used in the forward pass to provide sufficient information for classification. Meanwhile, only the gradients of the salient patches are back-propagated to update the entire network. Moreover, we design a Multiple Instance Learning (MIL) architecture that leverages aggregated information from salient patches to filter out uninformative background within sub-salient patches for better accuracy. Besides, we introduce the random patch drop to accelerate training process and uncover informative regions. Experiment results demonstrate the superiority of our method in terms of both accuracy and training consumption against other advanced methods. The code is available in the https://github.com/Qinrong-NKU/DBPS",
    "checked": true,
    "id": "b456bac2fd24f79f2656a6ae8d10d001e9c5c524",
    "semantic_title": "no pains, more gains: recycling sub-salient patches for efficient high-resolution image recognition",
    "citation_count": 0,
    "authors": [
      "Rong Qin",
      "Xin Liu",
      "Xingyu Liu",
      "Jiaxuan Liu",
      "Jinglei Shi",
      "Liang Lin",
      "Jufeng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Benny_SphereUFormer_A_U-Shaped_Transformer_for_Spherical_360_Perception_CVPR_2025_paper.html": {
    "title": "SphereUFormer: A U-Shaped Transformer for Spherical 360 Perception",
    "volume": "main",
    "abstract": "This paper proposes a novel method for omnidirectional 360\\degree perception. Most common previous methods relied on equirectangular projection. This representation is easily applicable to 2D operation layers but introduces distortions into the image. Other methods attempted to remove the distortions by maintaining a sphere representation but relied on complicated convolution kernels that failed to show competitive results. In this work, we introduce a transformer-based architecture that, by incorporating a novel \"Spherical Local Self-Attention\" and other spherically-oriented modules, successfully operates in the spherical domain and outperforms the state-of-the-art in 360\\degree perception benchmarks for depth estimation and semantic segmentation. Our code is available at https://github.com/yanivbenny/sphere_uformer",
    "checked": true,
    "id": "59f67e77949a4cf72993ceecd355b435e1c15b2b",
    "semantic_title": "sphereuformer: a u-shaped transformer for spherical 360 perception",
    "citation_count": 1,
    "authors": [
      "Yaniv Benny",
      "Lior Wolf"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Advancing_Generalizable_Tumor_Segmentation_with_Anomaly-Aware_Open-Vocabulary_Attention_Maps_and_CVPR_2025_paper.html": {
    "title": "Advancing Generalizable Tumor Segmentation with Anomaly-Aware Open-Vocabulary Attention Maps and Frozen Foundation Diffusion Models",
    "volume": "main",
    "abstract": "We explore Generalizable Tumor Segmentation, aiming to train a single model for zero-shot tumor segmentation across diverse anatomical regions. Existing methods face limitations related to segmentation quality, scalability, and the range of applicable imaging modalities. In this paper, we uncover the potential of the internal representations within frozen medical foundation diffusion models as highly efficient zero-shot learners for tumor segmentation by introducing a novel framework named DiffuGTS. DiffuGTS creates anomaly-aware open-vocabulary attention maps based on text prompts to enable generalizable anomaly segmentation without being restricted by a predefined training category list. To further improve and refine anomaly segmentation masks, DiffuGTS leverages the diffusion model, transforming pathological regions into high-quality pseudo-healthy counterparts through latent space inpainting, and applies a novel pixel-level and feature-level residual learning approach, resulting in segmentation masks with significantly enhanced quality and generalization. Comprehensive experiments on four datasets and seven tumor categories demonstrate the superior performance of our method, surpassing current state-of-the-art models across multiple zero-shot settings. Codes are available at https://github.com/Yankai96/DiffuGTS",
    "checked": true,
    "id": "6c9e7e8374023b8743388235ebf61eb37ab72004",
    "semantic_title": "advancing generalizable tumor segmentation with anomaly-aware open-vocabulary attention maps and frozen foundation diffusion models",
    "citation_count": 0,
    "authors": [
      "Yankai Jiang",
      "Peng Zhang",
      "Donglin Yang",
      "Yuan Tian",
      "Hai Lin",
      "Xiaosong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Towards_Generalizable_Scene_Change_Detection_CVPR_2025_paper.html": {
    "title": "Towards Generalizable Scene Change Detection",
    "volume": "main",
    "abstract": "While current state-of-the-art Scene Change Detection (SCD) approaches achieve impressive results in well-trained research data, they become unreliable under unseen environments and different temporal conditions; in-domain performance drops from 77.6% to 8.0% in a previously unseen environment and to 4.6% under a different temporal condition---calling for generalizable SCD and benchmark. In this work, we propose the Generalizable Scene Change Detection Framework (GeSCF), which addresses unseen domain performance and temporal consistency---to meet the growing demand for anything SCD. Our method leverages the pre-trained Segment Anything Model (SAM) in a zero-shot manner. For this, we design Initial Pseudo-mask Generation and Geometric-Semantic Mask Matching---seamlessly turning user-guided prompt and single-image based segmentation into scene change detection for a pair of inputs without guidance. Furthermore, we define the Generalizable Scene Change Detection (GeSCD) benchmark along with novel metrics and an evaluation protocol to facilitate SCD research in generalizability. In the process, we introduce the ChangeVPR dataset, a collection of challenging image pairs with diverse environmental scenarios---including urban, suburban, and rural settings. Extensive experiments across various datasets demonstrate that GeSCF achieves an average performance gain of 19.2% on existing SCD datasets and 30.0% on the ChangeVPR dataset, nearly doubling the prior art performance. We believe our work can lay a solid foundation for robust and generalizable SCD research",
    "checked": true,
    "id": "67671edfb0eb14f5eff33ae902ca44c105c22471",
    "semantic_title": "towards generalizable scene change detection",
    "citation_count": 2,
    "authors": [
      "Jae-Woo Kim",
      "Ue-Hwan Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Beyond_Clean_Training_Data_A_Versatile_and_Model-Agnostic_Framework_for_CVPR_2025_paper.html": {
    "title": "Beyond Clean Training Data: A Versatile and Model-Agnostic Framework for Out-of-Distribution Detection with Contaminated Training Data",
    "volume": "main",
    "abstract": "In real-world AI applications, training datasets are often contaminated, containing a mix of in-distribution (ID) and out-of-distribution (OOD) samples without labels. This contamination poses a significant challenge for developing and training OOD detection models, as nearly all existing methods assume access to a clean training dataset of only ID samples--a condition rarely met in real-world scenarios. Customizing each existing OOD detection method to handle such contamination is impractical, given the vast number of diverse methods designed for clean data. To address this issue, we propose a universal, model-agnostic framework that integrates with nearly all existing OOD detection methods, enabling training on contaminated datasets while achieving high OOD detection accuracy on test datasets. Additionally, our framework provides an accurate estimation of the unknown proportion of OOD samples within the training dataset--an important and distinct challenge in its own right. Our approach introduces a novel dynamic weighting function and transition mechanism within an iterative training structure, enabling both reliable estimation of the OOD sample proportion of the training data and precise OOD detection on test data. Extensive evaluations across diverse datasets, including ImageNet-1k, demonstrate that our framework accurately estimates OOD sample proportions of training data and substantially enhances OOD detection accuracy on test data",
    "checked": true,
    "id": "05947f46d794b1c160bf0c2e4ee01bc401b39788",
    "semantic_title": "beyond clean training data: a versatile and model-agnostic framework for out-of-distribution detection with contaminated training data",
    "citation_count": 0,
    "authors": [
      "Yuchuan Li",
      "Jae-Mo Kang",
      "Il-Min Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Incomplete_Multi-modal_Brain_Tumor_Segmentation_via_Learnable_Sorting_State_Space_CVPR_2025_paper.html": {
    "title": "Incomplete Multi-modal Brain Tumor Segmentation via Learnable Sorting State Space Model",
    "volume": "main",
    "abstract": "Brain tumor segmentation plays a crucial role in clinical diagnosis, yet the frequent unavailability of certain MRI modalities poses a significant challenge. In this paper, we introduce the Learnable Sorting State Space Model (LS3M), a novel framework designed to maximize the utilization of available modalities for brain tumor segmentation. LS3M excels at efficiently modeling long-range dependencies based on the Mamba design, while incorporating differentiable permutation matrices that reorder input sequences based on modality-specific characteristics. This dynamic reordering ensures that critical spatial inductive biases and long-range semantic correlations inherent in 3D brain MRI are preserved, which is crucial for imcomplete multi-modal brain tumor segmentation.Once the input sequences are reordered using the generated permutation matrix, the Series State Space Model (S3M) block models the relationships between them, capturing both local and long-range dependencies. This enables effective representation of intra-modal and inter-modal relationships, significantly improving segmentation accuracy. Extensive experiments on the BraTS2018 and BraTS2020 datasets demonstrate that LS3M outperforms existing methods, offering a robust solution for brain tumor segmentation, particularly in scenarios with missing modalities",
    "checked": true,
    "id": "b8d978f72167f62a03ff3e12c982fb4ff5638b53",
    "semantic_title": "incomplete multi-modal brain tumor segmentation via learnable sorting state space model",
    "citation_count": 0,
    "authors": [
      "Zheyu Zhang",
      "Yayuan Lu",
      "Feipeng Ma",
      "Yueyi Zhang",
      "Huanjing Yue",
      "Xiaoyan Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_FedAWA_Adaptive_Optimization_of_Aggregation_Weights_in_Federated_Learning_Using_CVPR_2025_paper.html": {
    "title": "FedAWA: Adaptive Optimization of Aggregation Weights in Federated Learning Using Client Vectors",
    "volume": "main",
    "abstract": "Federated Learning (FL) has emerged as a promising framework for distributed machine learning, enabling collaborative model training without sharing local data, thereby preserving privacy and enhancing security. However, data heterogeneity resulting from differences across user behaviors, preferences, and device characteristics poses a significant challenge for federated learning. Most previous works overlook the adjustment of aggregation weights, relying solely on dataset size for weight assignment, which often leads to unstable convergence and reduced model performance. Recently, several studies have sought to refine aggregation strategies by incorporating dataset characteristics and model alignment. However, adaptively adjusting aggregation weights while ensuring data security--without requiring additional proxy data--remains a significant challenge. In this work, we propose Federated learning with Adaptive Weight Aggregation (FedAWA), a novel method that adaptively adjusts aggregation weights based on client vectors during the learning process. The client vector captures the direction of model updates, reflecting local data variations, and is used to optimize the aggregation weight without requiring additional datasets or violating privacy. By assigning higher aggregation weights to local models whose updates align closely with the global optimization direction, FedAWA enhances the stability and generalization of the global model. Extensive experiments under diverse scenarios demonstrate the superiority of our method, providing a promising solution to the challenges of data heterogeneity in federated learning",
    "checked": true,
    "id": "16c0620ed99a84f715fd128f9640f2d5157fce0b",
    "semantic_title": "fedawa: adaptive optimization of aggregation weights in federated learning using client vectors",
    "citation_count": 3,
    "authors": [
      "Changlong Shi",
      "He Zhao",
      "Bingjie Zhang",
      "Mingyuan Zhou",
      "Dandan Guo",
      "Yi Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_FreeUV_Ground-Truth-Free_Realistic_Facial_UV_Texture_Recovery_via_Cross-Assembly_Inference_CVPR_2025_paper.html": {
    "title": "FreeUV: Ground-Truth-Free Realistic Facial UV Texture Recovery via Cross-Assembly Inference Strategy",
    "volume": "main",
    "abstract": "Recovering high-quality 3D facial textures from single-view 2D images is a challenging task, especially under constraints of limited data and complex facial details such as makeup, wrinkles, and occlusions. In this paper, we introduce FreeUV, a novel ground-truth-free UV texture recovery framework that eliminates the need for annotated or synthetic UV data. FreeUV leverages pre-trained stable diffusion model alongside a Cross-Assembly inference strategy to fulfill this objective. In FreeUV, separate networks are trained independently to focus on realistic appearance and structural consistency, and these networks are combined during inference to generate coherent textures. Our approach accurately captures intricate facial features and demonstrates robust performance across diverse poses and occlusions. Extensive experiments validate FreeUV's effectiveness, with results surpassing state-of-the-art methods in both quantitative and qualitative metrics. Additionally, FreeUV enables new applications, including local editing, facial feature interpolation, and multi-view texture recovery. By reducing data requirements, FreeUV offers a scalable solution for generating high-fidelity 3D facial textures suitable for real-world scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingchao Yang",
      "Takafumi Taketomi",
      "Yuki Endo",
      "Yoshihiro Kanamori"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_HarmonySet_A_Comprehensive_Dataset_for_Understanding_Video-Music_Semantic_Alignment_and_CVPR_2025_paper.html": {
    "title": "HarmonySet: A Comprehensive Dataset for Understanding Video-Music Semantic Alignment and Temporal Synchronization",
    "volume": "main",
    "abstract": "This paper introduces HarmonySet, a comprehensive dataset designed to advance video-music understanding. HarmonySet consists of 48,328 diverse video-music pairs, annotated with detailed information on rhythmic synchronization, emotional alignment, thematic coherence, and cultural relevance. We propose a multi-step human-machine collaborative framework for efficient annotation, combining human insights with machine-generated descriptions to identify key transitions and assess alignment across multiple dimensions. Additionally, we introduce a novel evaluation framework with tasks and metrics to assess the multi-dimensional alignment of video and music, including rhythm, emotion, theme, and cultural context. Our extensive experiments demonstrate that HarmonySet, along with the proposed evaluation framework, significantly improves the ability of multimodal models to capture and analyze the intricate relationships between video and music. Project page: https://harmonyset.github.io/",
    "checked": true,
    "id": "34288ad84ff79159fb32c0bb5fb56ed6da5f0ce4",
    "semantic_title": "harmonyset: a comprehensive dataset for understanding video-music semantic alignment and temporal synchronization",
    "citation_count": 2,
    "authors": [
      "Zitang Zhou",
      "Ke Mei",
      "Yu Lu",
      "Tianyi Wang",
      "Fengyun Rao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Meng_Rethinking_Diffusion_for_Text-Driven_Human_Motion_Generation_Redundant_Representations_Evaluation_CVPR_2025_paper.html": {
    "title": "Rethinking Diffusion for Text-Driven Human Motion Generation: Redundant Representations, Evaluation, and Masked Autoregression",
    "volume": "main",
    "abstract": "Since 2023, Vector Quantization (VQ)-based discrete generation methods have rapidly dominated human motion generation, primarily surpassing diffusion-based continuous generation methods in standard performance metrics. However, VQ-based methods have inherent limitations. Representing continuous motion data as limited discrete tokens leads to inevitable information loss, reduces the diversity of generated motions, and restricts their ability to function effectively as motion priors or generation guidance. In contrast, the continuous space generation nature of diffusion-based methods makes them well-suited to address these limitations and with even potential for model scalability. In this work, we systematically investigate why current VQ-based methods perform well and explore the limitations of existing diffusion-based methods from the perspective of motion data representation and distribution. Drawing on these insights, we preserve the inherent strengths of a diffusion-based human motion generation model and gradually optimize it with inspiration from VQ-based approaches. Our approach introduces a human motion diffusion model enabled to perform masked autoregression, optimized with a reformed data representation and distribution. Additionally, we propose a more robust evaluation method to assess different approaches. Extensive experiments on various datasets demonstrate our method outperforms previous methods and achieves state-of-the-art performances",
    "checked": true,
    "id": "2283ebef7384996b31e112735996aa76e54c4890",
    "semantic_title": "rethinking diffusion for text-driven human motion generation: redundant representations, evaluation, and masked autoregression",
    "citation_count": 13,
    "authors": [
      "Zichong Meng",
      "Yiming Xie",
      "Xiaogang Peng",
      "Zeyu Han",
      "Huaizu Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_StyleMaster_Stylize_Your_Video_with_Artistic_Generation_and_Translation_CVPR_2025_paper.html": {
    "title": "StyleMaster: Stylize Your Video with Artistic Generation and Translation",
    "volume": "main",
    "abstract": "Style control has been popular in video generation models. Existing methods often generate videos far from the given style, cause content leakage, and struggle to transfer one video to the desired style. Our first observation is that the style extraction stage matters, whereas existing methods emphasize global style but ignore local textures. In order to bring texture features while preventing content leakage, we filter content-related patches while retaining style ones based on prompt-patch similarity; for global style extraction, we generate a paired style dataset through model illusion to facilitate contrastive learning, which greatly enhances the absolute style consistency. Moreover, to fill in the image-to-video gap, we train a lightweight motion adapter on still videos, which implicitly enhances stylization extent, and enables our image-trained model to be seamlessly applied to videos. Benefited from these efforts, our approach, StyleMaster, not only achieves significant improvement in both style resemblance and temporal coherence, but also can easily generalize to video style transfer with a gray tile ControlNet. Extensive experiments and visualizations demonstrate that StyleMaster significantly outperforms competitors, effectively generating high-quality stylized videos that align with textual content and closely resemble the style of reference images",
    "checked": true,
    "id": "0b0847aeb0eca4a26f838b0969b35c26ff8d8cc2",
    "semantic_title": "stylemaster: stylize your video with artistic generation and translation",
    "citation_count": 10,
    "authors": [
      "Zixuan Ye",
      "Huijuan Huang",
      "Xintao Wang",
      "Pengfei Wan",
      "Di Zhang",
      "Wenhan Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Unsupervised_Continual_Domain_Shift_Learning_with_Multi-Prototype_Modeling_CVPR_2025_paper.html": {
    "title": "Unsupervised Continual Domain Shift Learning with Multi-Prototype Modeling",
    "volume": "main",
    "abstract": "In real-world applications, deep neural networks may encounter constantly changing environments, where the test data originates from continually shifting unlabeled target domains. This problem, known as Unsupervised Continual Domain Shift Learning (UCDSL), poses practical difficulties. Existing methods for UCDSL aim to learn domain-invariant representations for all target domains. However, due to the existence of adaptivity gap, the invariant representation may theoretically lead to large joint errors. To overcome the limitation, we propose a novel UCDSL method, called Multi-Prototype Modeling (MPM). Our model comprises two key components: (1) Multi-Prototype Learning (MPL) for acquiring domain-specific representations using multiple domain-specific prototypes. MPL achieves domain-specific error minimization instead of enforcing feature alignment across different domains. (2) Bi-Level Graph Enhancer (BiGE) for enhancing domain-level and category-level representations, resulting in more accurate predictions. We provide theoretical and empirical analysis to demonstrate the effectiveness of our proposed method. We evaluate our approach on multiple benchmark datasets and show that our model surpasses state-of-the-art methods across all datasets, highlighting its effectiveness and robustness in handling unsupervised continual domain shift learning. Codes will be publicly accessible",
    "checked": true,
    "id": "b44a9cd2c563db0b1925d7c8c6690b24763f5bc6",
    "semantic_title": "unsupervised continual domain shift learning with multi-prototype modeling",
    "citation_count": 1,
    "authors": [
      "Haopeng Sun",
      "Yingwei Zhang",
      "Lumin Xu",
      "Sheng Jin",
      "Ping Luo",
      "Chen Qian",
      "Wentao Liu",
      "Yiqiang Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_OmniGuard_Hybrid_Manipulation_Localization_via_Augmented_Versatile_Deep_Image_Watermarking_CVPR_2025_paper.html": {
    "title": "OmniGuard: Hybrid Manipulation Localization via Augmented Versatile Deep Image Watermarking",
    "volume": "main",
    "abstract": "With the rapid growth of generative AI and its widespread application in image editing, new risks have emerged regarding the authenticity and integrity of digital content. Existing versatile watermarking approaches suffer from trade-offs between tamper localization precision and visual quality. Constrained by the limited flexibility of previous framework, their localized watermark must remain fixed across all images. Under AIGC-editing, their copyright extraction accuracy is also unsatisfactory. To address these challenges, we propose OmniGuard, a novel augmented versatile watermarking approach that integrates proactive embedding with passive, blind extraction for robust copyright protection and tamper localization. OmniGuard employs a hybrid forensic framework that enables flexible localization watermark selection and introduces a degradation-aware tamper extraction network for precise localization under challenging conditions. Additionally, a lightweight AIGC-editing simulation layer is designed to enhance robustness across global and local editing. Extensive experiments show that OmniGuard achieves superior fidelity, robustness, and flexibility. Compared to the recent state-of-the-art approach EditGuard, our method outperforms it by 4.25dB in PSNR of the container image, 20.7% in F1-Score under noisy conditions, and 14.8% in average bit accuracy",
    "checked": true,
    "id": "0dd9e2400c0e6b77de42c671516a1d77c5b2e78c",
    "semantic_title": "omniguard: hybrid manipulation localization via augmented versatile deep image watermarking",
    "citation_count": 8,
    "authors": [
      "Xuanyu Zhang",
      "Zecheng Tang",
      "Zhipei Xu",
      "Runyi Li",
      "Youmin Xu",
      "Bin Chen",
      "Feng Gao",
      "Jian Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fogel_Open-Canopy_Towards_Very_High_Resolution_Forest_Monitoring_CVPR_2025_paper.html": {
    "title": "Open-Canopy: Towards Very High Resolution Forest Monitoring",
    "volume": "main",
    "abstract": "Estimating canopy height and its changes at meter resolution from satellite imagery is a significant challenge in computer vision with critical environmental applications. However, the lack of open-access datasets at this resolution hinders the reproducibility and evaluation of models. We introduce Open-Canopy, the first open-access, country-scale benchmark for very high-resolution (1.5 m) canopy height estimation, covering over 87,000 km2 across France with 1.5 m resolution satellite imagery and aerial LiDAR data. Additionally, we present Open-Canopy-, a benchmark for canopy height change detection between images from different years at tree level--a challenging task for current computer vision models. We evaluate state-of-the-art architectures on these benchmarks, highlighting significant challenges and opportunities for improvement. Our datasets and code are publicly available at https://github.com/fajwel/Open-Canopy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fajwel Fogel",
      "Yohann Perron",
      "Nikola Besic",
      "Laurent Saint-Andr√©",
      "Agn√®s Pellissier-Tanon",
      "Martin Schwartz",
      "Thomas Boudras",
      "Ibrahim Fayad",
      "Alexandre d'Aspremont",
      "Loic Landrieu",
      "Philippe Ciais"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_ClearSight_Visual_Signal_Enhancement_for_Object_Hallucination_Mitigation_in_Multimodal_CVPR_2025_paper.html": {
    "title": "ClearSight: Visual Signal Enhancement for Object Hallucination Mitigation in Multimodal Large Language Models",
    "volume": "main",
    "abstract": "Contrastive decoding strategies are widely used to mitigate object hallucinations in multimodal large language models (MLLMs). By reducing over-reliance on language priors, these strategies ensure that generated content remains closely grounded in visual inputs, producing contextually accurate outputs. Since contrastive decoding requires no additional training or external tools, it offers both computational efficiency and versatility, making it highly attractive. However, these methods present two main limitations: (1) bluntly suppressing language priors can compromise coherence and accuracy of generated content, and (2) processing contrastive inputs adds computational load, significantly slowing inference speed. To address these challenges, we propose Visual Amplification Fusion (VAF), a plug-and-play technique that enhances attention to visual signals within the model's middle layers, where modality fusion predominantly occurs. This approach enables more effective capture of visual features, reducing the model's bias toward language modality. Experimental results demonstrate that VAF significantly reduces hallucinations across various MLLMs without affecting inference speed, while maintaining coherence and accuracy in generated outputs",
    "checked": true,
    "id": "ecc51ce52ca524be17616a9c0dc8a051a2996ad7",
    "semantic_title": "clearsight: visual signal enhancement for object hallucination mitigation in multimodal large language models",
    "citation_count": 2,
    "authors": [
      "Hao Yin",
      "Guangzong Si",
      "Zilei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sehwag_Stretching_Each_Dollar_Diffusion_Training_from_Scratch_on_a_Micro-Budget_CVPR_2025_paper.html": {
    "title": "Stretching Each Dollar: Diffusion Training from Scratch on a Micro-Budget",
    "volume": "main",
    "abstract": "As scaling laws in generative AI push performance, they simultaneously concentrate the development of these models among actors with large computational resources. With a focus on text-to-image (T2I) generative models, we aim to unlock this bottleneck by demonstrating very low-cost training of large-scale T2I diffusion transformer models. As the computational cost of transformers increases with the number of patches in each image, we propose randomly masking up to 75% of the image patches during training. We propose a deferred masking strategy that preprocesses all patches using a patch-mixer before masking, thus significantly reducing the performance degradation with masking, making it superior to model downscaling in reducing computational cost. We also incorporate the latest improvements in transformer architecture, such as the use of mixture-of-experts layers, to improve performance and further identify the critical benefit of using synthetic images in micro-budget training. Finally, using only 37M publicly available real and synthetic images, we train a 1.16 billion parameter sparse transformer with only 1,890 USD economical cost and achieve a 12.7 FID in zero-shot generation on the COCO dataset. Notably, our model achieves competitive performance across both automated and human-centric evaluations, as well as high-quality generations, while incurring 118xlower costs than Stable Diffusion models and 14xlower costs than the current state-of-the-art approach, which costs \\28,400. We also further investigate the influence of synthetic images on performance and demonstrate that micro-budget training on only synthetic images is sufficient for achieving high-quality data generation. Our end-to-end training pipeline and model checkpoints are available at https://github.com/SonyResearch/micro_diffusion",
    "checked": true,
    "id": "fbc91231aab3327f2ff271e84178e6ee4c38e48a",
    "semantic_title": "stretching each dollar: diffusion training from scratch on a micro-budget",
    "citation_count": 13,
    "authors": [
      "Vikash Sehwag",
      "Xianghao Kong",
      "Jingtao Li",
      "Michael Spranger",
      "Lingjuan Lyu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xue_Guiding_Human-Object_Interactions_with_Rich_Geometry_and_Relations_CVPR_2025_paper.html": {
    "title": "Guiding Human-Object Interactions with Rich Geometry and Relations",
    "volume": "main",
    "abstract": "Human-object interaction (HOI) synthesis is crucial for creating immersive and realistic experiences for applications such as virtual reality. Existing methods often rely on simplified object representations, such as the object's centroid or the nearest point to a human, to achieve physically plausible motions. However, these approaches may overlook geometric complexity, resulting in suboptimal interaction fidelity. To address this limitation, we introduce ROG, a novel diffusion-based framework that models the spatiotemporal relationships inherent in HOIs with rich geometric detail. For efficient object representation, we select boundary-focused and fine-detail key points from the object mesh, ensuring a comprehensive depiction of the object's geometry. This representation is used to construct an interactive distance field (IDF), capturing the robust HOI dynamics. Furthermore, we develop a diffusion-based relation model that integrates spatial and temporal attention mechanisms, enabling a better understanding of intricate HOI relationships. This relation model refines the generated motion's IDF, guiding the motion generation process to produce relation-aware and semantically aligned movements. Experimental evaluations demonstrate that ROG significantly outperforms state-of-the-art methods in the realism and semantic accuracy of synthesized HOIs. This paper's code will be released",
    "checked": true,
    "id": "409d41cc91d10a19c08652f7348e8d586879c8cf",
    "semantic_title": "guiding human-object interactions with rich geometry and relations",
    "citation_count": 4,
    "authors": [
      "Mengqing Xue",
      "Yifei Liu",
      "Ling Guo",
      "Shaoli Huang",
      "Changxing Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_TacoDepth_Towards_Efficient_Radar-Camera_Depth_Estimation_with_One-stage_Fusion_CVPR_2025_paper.html": {
    "title": "TacoDepth: Towards Efficient Radar-Camera Depth Estimation with One-stage Fusion",
    "volume": "main",
    "abstract": "Radar-Camera depth estimation aims to predict dense and accurate metric depth by fusing input images and Radar data. Model efficiency is crucial for this task in pursuit of real-time processing on autonomous vehicles and robotic platforms. However, due to the sparsity of Radar returns, the prevailing methods adopt multi-stage frameworks with intermediate quasi-dense depth, which are time-consuming and not robust. To address these challenges, we propose TacoDepth, an efficient and accurate Radar-Camera depth estimation model with one-stage fusion. Specifically, the graph-based Radar structure extractor and the pyramid-based Radar fusion module are designed to capture and integrate the graph structures of Radar point clouds, delivering superior model efficiency and robustness without relying on the intermediate depth results. Moreover, TacoDepth can be flexible for different inference modes, providing a better balance of speed and accuracy. Extensive experiments are conducted to demonstrate the efficacy of our method. Compared with the previous state-of-the-art approach, TacoDepth improves depth accuracy and processing speed by 12.8% and 91.8%. Our work provides a new perspective on efficient Radar-Camera depth estimation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiran Wang",
      "Jiaqi Li",
      "Chaoyi Hong",
      "Ruibo Li",
      "Liusheng Sun",
      "Xiao Song",
      "Zhe Wang",
      "Zhiguo Cao",
      "Guosheng Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Taketsugu_Physical_Plausibility-aware_Trajectory_Prediction_via_Locomotion_Embodiment_CVPR_2025_paper.html": {
    "title": "Physical Plausibility-aware Trajectory Prediction via Locomotion Embodiment",
    "volume": "main",
    "abstract": "Humans can predict future human trajectories even from momentary observations by using human pose-related cues. However, previous Human Trajectory Prediction (HTP) methods leverage the pose cues implicitly, resulting in implausible predictions. To address this, we propose Locomotion Embodiment, a framework that explicitly evaluates the physical plausibility of the predicted trajectory by locomotion generation under the laws of physics. While the plausibility of locomotion is learned with an indifferentiable physics simulator, it is replaced by our differentiable Locomotion Value function to train an HTP network in a data-driven manner. In particular, our proposed Embodied Locomotion loss is beneficial for efficiently training a stochastic HTP network using multiple heads. Furthermore, the Locomotion Value filter is proposed to filter out implausible trajectories at inference. Experiments demonstrate that our method enhances even the state-of-the-art HTP methods across diverse datasets and problem settings. Our code is available at: https://github.com/ImIntheMiddle/EmLoco",
    "checked": true,
    "id": "d66a698fc38c2a5dcfbcfb250d9099323c53eba8",
    "semantic_title": "physical plausibility-aware trajectory prediction via locomotion embodiment",
    "citation_count": 5,
    "authors": [
      "Hiromu Taketsugu",
      "Takeru Oba",
      "Takahiro Maeda",
      "Shohei Nobuhara",
      "Norimichi Ukita"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_CADDreamer_CAD_Object_Generation_from_Single-view_Images_CVPR_2025_paper.html": {
    "title": "CADDreamer: CAD Object Generation from Single-view Images",
    "volume": "main",
    "abstract": "The field of diffusion-based 3D generation has experienced tremendous progress in recent times. However, existing 3D generative models often produce overly dense and unstructured meshes, which are in stark contrast to the compact, structured and clear-edged CAD models created by human modelers. We introduce CADDreamer, a novel method for generating CAD objects from a single image. This method proposes a primitive-aware multi-view diffusion model, which perceives both local geometry and high-level structural semantics during the generation process. We encode primitive semantics into the color domain, and enforce the strong priors in pre-trained diffusion models to align with the well-defined primitives. As a result, we can infer multi-view normal maps and semantic maps from a single image, thereby reconstructing a mesh with primitive labels. Correspondingly, we propose a set of fitting and optimization methods to deal with the inevitable noise and distortion in generated primitives, ultimately producing a complete and seamless Boundary Representation (B-rep) of a Computer-Aided Design (CAD) model. Experimental results demonstrate that our method can effectively recover high-quality CAD objects from single-view images. Compared to existing 3D generation methods, the models produced by CADDreamer are compact in representation, clear in structure, sharp in boundaries, and watertight in topology",
    "checked": true,
    "id": "d89bc61f4521308340ffc34cd0e48c4d91b685f1",
    "semantic_title": "caddreamer: cad object generation from single-view images",
    "citation_count": 6,
    "authors": [
      "Yuan Li",
      "Cheng Lin",
      "Yuan Liu",
      "Xiaoxiao Long",
      "Chenxu Zhang",
      "Ningna Wang",
      "Xin Li",
      "Wenping Wang",
      "Xiaohu Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Vision-Language_Model_IP_Protection_via_Prompt-based_Learning_CVPR_2025_paper.html": {
    "title": "Vision-Language Model IP Protection via Prompt-based Learning",
    "volume": "main",
    "abstract": "Vision-language models (VLMs) like CLIP (Contrastive Language-Image Pre-Training) have seen remarkable success in visual recognition, highlighting the increasing need to safeguard the intellectual property (IP) of well-trained models. Effective IP protection extends beyond ensuring authorized usage; it also necessitates restricting model deployment to authorized data domains, particularly when the model is fine-tuned for specific target domains. However, current IP protection methods often rely solely on the visual backbone, which may lack sufficient semantic richness. To bridge this gap, we introduce IP-CLIP, a lightweight IP protection strategy tailored to CLIP, employing a prompt-based learning approach. By leveraging the frozen visual backbone of CLIP, we extract both image style and content information, incorporating them into the learning of IP prompt. This strategy acts as a robust barrier, effectively preventing the unauthorized transfer of features from authorized domains to unauthorized ones. Additionally, we propose a style-enhancement branch that constructs feature banks for both authorized and unauthorized domains. This branch integrates self-enhanced and cross-domain features, further strengthening IP-CLIP's capability to block features from unauthorized domains. Finally, we present new three metrics designed to better balance the performance degradation of authorized and unauthorized domains. Comprehensive experiments in various scenarios demonstrate its promising potential for application in IP protection tasks for VLMs",
    "checked": true,
    "id": "e8bd21bada5a93a05ad7f90b54b1875068c1f63a",
    "semantic_title": "vision-language model ip protection via prompt-based learning",
    "citation_count": 0,
    "authors": [
      "Lianyu Wang",
      "Meng Wang",
      "Huazhu Fu",
      "Daoqiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bai_Wheres_the_Liability_in_the_Generative_Era_Recovery-based_Black-Box_Detection_CVPR_2025_paper.html": {
    "title": "Where's the Liability in the Generative Era? Recovery-based Black-Box Detection of AI-Generated Content",
    "volume": "main",
    "abstract": "The recent proliferation of photorealistic images created by generative models has sparked both excitement and concern, as these images are increasingly indistinguishable from real ones to the human eye. While offering new creative and commercial possibilities, the potential for misuse, such as in misinformation and fraud, highlights the need for effective detection methods. Current detection approaches often rely on access to model weights or require extensive collections of real image datasets, limiting their scalability and practical application in real-world scenarios. In this work, we introduce a novel black-box detection framework that requires only API access, sidestepping the need for model weights or large auxiliary datasets. Our approach leverages a corrupt-and-recover strategy: by masking part of an image and assessing the model's ability to reconstruct it, we measure the likelihood that the image was generated by the model itself. For black-box models that do not support masked-image inputs, we incorporate a cost-efficient surrogate model trained to align with the target model's distribution, enhancing detection capability. Our framework demonstrates strong performance, outperforming baseline methods by 4.31% in mean average precision across eight diffusion model variant datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyue Bai",
      "Yiyou Sun",
      "Wei Cheng",
      "Haifeng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Kiss3DGen_Repurposing_Image_Diffusion_Models_for_3D_Asset_Generation_CVPR_2025_paper.html": {
    "title": "Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation",
    "volume": "main",
    "abstract": "Diffusion models have achieved great success in generating 2D images. However, the quality and generalizability of 3D content generation remain limited. State-of-the-art methods often require large-scale 3D assets for training, which are challenging to collect. In this work, we introduce Kiss3DGen (Keep It Simple and Straightforward in 3D Generation), an efficient framework for generating, editing, and enhancing 3D objects by repurposing a well-trained 2D image diffusion model for 3D generation. Specifically, we fine-tune a diffusion model to generate \"3D Bundle Image\", a tiled representation composed of multi-view images and their corresponding normal maps. The normal maps are then used to reconstruct a 3D mesh, and the multi-view images provide texture mapping, resulting in a complete 3D model. This simple method effectively transforms the 3D generation problem into a 2D image generation task, maximizing the utilization of knowledge in pretrained diffusion models. Furthermore, we demonstrate that our Kiss3DGen model is compatible with various diffusion model techniques, enabling advanced features such as 3D editing, mesh and texture enhancement, etc. Through extensive experiments, we demonstrate the effectiveness of our approach, showcasing its ability to produce high-quality 3D models efficiently",
    "checked": true,
    "id": "cc6c64b4fec00e5371b38d7c4dfa10a1b23c76fe",
    "semantic_title": "kiss3dgen: repurposing image diffusion models for 3d asset generation",
    "citation_count": 5,
    "authors": [
      "Jiantao Lin",
      "Xin Yang",
      "Meixi Chen",
      "Yingjie Xu",
      "Dongyu Yan",
      "Leyi Wu",
      "Xinli Xu",
      "Lie Xu",
      "Shunsi Zhang",
      "Ying-Cong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mantri_DiTASK_Multi-Task_Fine-Tuning_with_Diffeomorphic_Transformations_CVPR_2025_paper.html": {
    "title": "DiTASK: Multi-Task Fine-Tuning with Diffeomorphic Transformations",
    "volume": "main",
    "abstract": "Pre-trained Vision Transformers now serve as powerful tools for computer vision. Yet, efficiently adapting them for multiple tasks remains a challenge that arises from the need to modify the rich hidden representations encoded by the learned weight matrices, without inducing interference between tasks. Current parameter-efficient methods like LoRA, which apply low-rank updates, force tasks to compete within constrained subspaces, ultimately degrading performance. We introduce DiTASK, a novel Diffeomorphic Multi-Task Fine-Tuning approach that maintains pre-trained representations by preserving weight matrix singular vectors, while enabling task-specific adaptations through neural diffeomorphic transformations of the singular values. By following this approach, DiTASK enables both shared and task-specific feature modulations with minimal added parameters. Our theoretical analysis shows that DiTASK achieves full-rank updates during optimization, preserving the geometric structure of pre-trained features, and establishing a new paradigm for efficient multi-task learning (MTL). Our experiments on PASCAL MTL and NYUD show that DiTASK achieves state-of-the-art performance across four dense prediction tasks, using 75% fewer parameters than existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krishna Sri Ipsit Mantri",
      "Carola-Bibiane Sch√∂nlieb",
      "Bruno Ribeiro",
      "Chaim Baskin",
      "Moshe Eliasof"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xi_OW-OVD_Unified_Open_World_and_Open_Vocabulary_Object_Detection_CVPR_2025_paper.html": {
    "title": "OW-OVD: Unified Open World and Open Vocabulary Object Detection",
    "volume": "main",
    "abstract": "Open world perception expands traditional closed-set frameworks, which assume a predefined set of known categories, to encompass dynamic real-world environments. Open World Object Detection (OWOD) and Open Vocabulary Object Detection (OVD) are two main research directions, each addressing unique challenges in dynamic environments. However, existing studies often focus on only one of these tasks, leaving the combined challenges of OWOD and OVD largely underexplored. In this paper, we propose a novel detector, OW-OVD, which inherits the zero-shot generalization capability of OVD detectors while incorporating the ability to actively detect unknown objects and progressively optimize performance through incremental learning, as seen in OWOD detectors. To achieve this, we start with a standard OVD detector and adapt it for OWOD tasks. For attribute selection, we propose the Visual Similarity Attribute Selection (VSAS) method, which identifies the most generalizable attributes by computing similarity distributions across annotated and unannotated regions. Additionally, to ensure the diversity of attributes, we incorporate a similarity constraint in the iterative process. Finally, to preserve the standard inference process of OVD, we propose the Hybrid Attribute-Uncertainty Fusion (HAUF) method. This method combines attribute similarity with known class uncertainty to infer the likelihood of an object belonging to an unknown class. We validated the effectiveness of OW-OVD through evaluations on two OWOD benchmarks, M-OWODB and S-OWODB. The results demonstrate that OW-OVD outperforms existing state-of-the-art models, achieving a +15.3 improvement in unknown object recall (U-Recall) and a +15.5 increase in unknown class average precision (U-mAP). Our code is available at: https://github.com/xxyzll/OW_OVD",
    "checked": true,
    "id": "15c53a098da6e6d2cfe5d74ac3a18f866183393a",
    "semantic_title": "ow-ovd: unified open world and open vocabulary object detection",
    "citation_count": 1,
    "authors": [
      "Xing Xi",
      "Yangyang Huang",
      "Ronghua Luo",
      "Yu Qiu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Improving_Diffusion_Inverse_Problem_Solving_with_Decoupled_Noise_Annealing_CVPR_2025_paper.html": {
    "title": "Improving Diffusion Inverse Problem Solving with Decoupled Noise Annealing",
    "volume": "main",
    "abstract": "Diffusion models have recently achieved success in solving Bayesian inverse problems with learned data priors. Current methods build on top of the diffusion sampling process, where each denoising step makes small modifications to samples from the previous step. However, this process struggles to correct errors from earlier sampling steps, leading to worse performance in complicated nonlinear inverse problems, such as phase retrieval. To address this challenge, we propose a new method called Decoupled Annealing Posterior Sampling (DAPS) that relies on a novel noise annealing process. Specifically, we decouple consecutive steps in a diffusion sampling trajectory, allowing them to vary considerably from one another while ensuring their time-marginals anneal to the true posterior as we reduce noise levels. This approach enables the exploration of a larger solution space, improving the success rate for accurate reconstructions. We demonstrate that DAPS significantly improves sample quality and stability across multiple image restoration tasks, particularly in complicated nonlinear inverse problems",
    "checked": true,
    "id": "26820c6371feb72c6487d3131e64152dee99614a",
    "semantic_title": "improving diffusion inverse problem solving with decoupled noise annealing",
    "citation_count": 51,
    "authors": [
      "Bingliang Zhang",
      "Wenda Chu",
      "Julius Berner",
      "Chenlin Meng",
      "Anima Anandkumar",
      "Yang Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_AvatarArtist_Open-Domain_4D_Avatarization_CVPR_2025_paper.html": {
    "title": "AvatarArtist: Open-Domain 4D Avatarization",
    "volume": "main",
    "abstract": "This work focuses on open-domain 4D avatarization, with the purpose of creating a 4D avatar from a portrait image in an arbitrary style. We select parametric triplanes as the intermediate 4D representation, and propose a practical training paradigm that takes advantage of both generative adversarial networks (GANs) and diffusion models. Our design stems from the observation that 4D GANs excel at bridging images and triplanes without supervision yet usually face challenges in handling diverse data distributions. A robust 2D diffusion prior emerges as the solution, assisting the GAN in transferring its expertise across various domains. The synergy between these experts permits the construction of a multi-domain image-triplane dataset, which drives the development of a general 4D avatar creator. Extensive experiments suggest that our model, termed AvatarArtist, is capable of producing high-quality 4D avatars with strong robustness to various source image domains. The code, the data, and the models will be made publicly available to facilitate future studies",
    "checked": true,
    "id": "23e637f6076535fd266ecc4048338cf058a4a8f8",
    "semantic_title": "avatarartist: open-domain 4d avatarization",
    "citation_count": 7,
    "authors": [
      "Hongyu Liu",
      "Xuan Wang",
      "Ziyu Wan",
      "Yue Ma",
      "Jingye Chen",
      "Yanbo Fan",
      "Yujun Shen",
      "Yibing Song",
      "Qifeng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DesignDiffusion_High-Quality_Text-to-Design_Image_Generation_with_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "DesignDiffusion: High-Quality Text-to-Design Image Generation with Diffusion Models",
    "volume": "main",
    "abstract": "In this paper, we present DesignDiffusion, a simple yet effective framework for the novel task of synthesizing design images from textual descriptions. A primary challenge lies in generating accurate and style-consistent textual and visual content. Existing works in a related task of visual text generation often focus on generating text within given specific regions, which limits the creativity of generation models, resulting in style or color inconsistencies between textual and visual elements if applied to design image generation. To address this issue, we propose an end-to-end, one-stage diffusion-based framework that avoids intricate components like position and layout modeling. Specifically, the proposed framework directly synthesizes textual and visual design elements from user prompts. It utilizes a distinctive character embedding derived from the visual text to enhance the input prompt, along with a character localization loss for enhanced supervision during text generation. Furthermore, we employ a self-play Direct Preference Optimization fine-tuning strategy to improve the quality and accuracy of the synthesized visual text. Extensive experiments demonstrate that DesignDiffusion achieves state-of-the-art performance in design image generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhendong Wang",
      "Jianmin Bao",
      "Shuyang Gu",
      "Dong Chen",
      "Wengang Zhou",
      "Houqiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liao_Using_Powerful_Prior_Knowledge_of_Diffusion_Model_in_Deep_Unfolding_CVPR_2025_paper.html": {
    "title": "Using Powerful Prior Knowledge of Diffusion Model in Deep Unfolding Networks for Image Compressive Sensing",
    "volume": "main",
    "abstract": "Recently, Deep Unfolding Networks (DUNs) have achieved impressive reconstruction quality in the field of image Compressive Sensing (CS) by unfolding iterative optimization algorithms into neural networks. The reconstruction quality of DUNs depends on the learned prior knowledge, so introducing stronger prior knowledge can further improve reconstruction quality. On the other hand, pre-trained diffusion models contain powerful prior knowledge and have a solid theoretical foundation and strong scalability, but it requires a large number of iterative steps to achieve reconstruction. In this paper, we propose to use the powerful prior knowledge of pre-trained diffusion model in DUNs to achieve high-quality reconstruction with less steps for image CS. Specifically, we first design an iterative optimization algorithm named Diffusion Message Passing (DMP), which embeds a pre-trained diffusion model into each iteration process of DMP. Then, we deeply unfold the DMP algorithm into a neural network named DMP-DUN. The proposed DMP-DUN can use lightweight neural networks to achieve mapping from measurement data to the intermediate steps of the reverse diffusion process and directly approximate the divergence of the diffusion model, thereby further improving reconstruction efficiency. Extensive experiments show that our proposed DMP-DUN achieves state-of-the-art performance and requires at least only 2 steps to reconstruct the image. Codes are available at https://github.com/FengodChen/DMP-DUN-CVPR2025",
    "checked": true,
    "id": "9ccd10109958ed52731b1d5c05abc4d9ec92ab93",
    "semantic_title": "using powerful prior knowledge of diffusion model in deep unfolding networks for image compressive sensing",
    "citation_count": 1,
    "authors": [
      "Chen Liao",
      "Yan Shen",
      "Dan Li",
      "Zhongli Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Koala-36M_A_Large-scale_Video_Dataset_Improving_Consistency_between_Fine-grained_Conditions_CVPR_2025_paper.html": {
    "title": "Koala-36M: A Large-scale Video Dataset Improving Consistency between Fine-grained Conditions and Video Content",
    "volume": "main",
    "abstract": "With the continuous progress of visual generation technologies, the scale of video datasets has grown exponentially. The quality of these datasets plays a pivotal role in the performance of video generation models. We assert that temporal splitting, detailed captions, and video quality filtering are three crucial determinants of dataset quality. However, existing datasets exhibit various limitations in these areas. To address these challenges, we introduce Koala-36M, a large-scale, high-quality video dataset featuring accurate temporal splitting, detailed captions, and superior video quality. The essence of our approach lies in improving the consistency between fine-grained conditions and video content. Specifically, we employ a linear classifier on probability distributions to enhance the accuracy of transition detection, ensuring better temporal consistency. We then provide structured captions for the splitted videos, with an average length of 200 words, to improve text-video alignment. Additionally, we develop a Video Training Suitability Score (VTSS) that integrates multiple sub-metrics, allowing us to filter high-quality videos from the original corpus. Finally, we incorporate several metrics into the training process of the generation model, further refining the fine-grained conditions. Our experiments demonstrate the effectiveness of our data processing pipeline and the quality of the proposed Koala-36M dataset. Our dataset and code have been released at https://koala36m.github.io/",
    "checked": false,
    "id": "6f004592aa9d3baa772f75a347720c1f2c6b5774",
    "semantic_title": "koala-36m : a large-scale video dataset improving consistency between fine-grained conditions and video content",
    "citation_count": 50,
    "authors": [
      "Qiuheng Wang",
      "Yukai Shi",
      "Jiarong Ou",
      "Rui Chen",
      "Ke Lin",
      "Jiahao Wang",
      "Boyuan Jiang",
      "Haotian Yang",
      "Mingwu Zheng",
      "Xin Tao",
      "Fei Yang",
      "Pengfei Wan",
      "Di Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhuang_VASparse_Towards_Efficient_Visual_Hallucination_Mitigation_via_Visual-Aware_Token_Sparsification_CVPR_2025_paper.html": {
    "title": "VASparse: Towards Efficient Visual Hallucination Mitigation via Visual-Aware Token Sparsification",
    "volume": "main",
    "abstract": "Large Vision-Language Models (LVLMs) may produce outputs that are unfaithful to reality, also known as visual hallucinations (VH), which significantly impedes their real-world usage. To alleviate VH, various decoding strategies have been proposed to enhance visual information. However, many of these methods may require secondary decoding and rollback, which significantly reduces inference speed. In this work, we propose an efficient plug-and-play decoding algorithm via Visual-Aware Sparsification (VASparse) from the perspective of token sparsity for mitigating VH. VASparse is inspired by empirical observations: (1) the sparse activation of attention in LVLMs, and (2) visual-agnostic tokens sparsification exacerbates VH. Based on these insights, we propose a novel token sparsification strategy that balances efficiency and trustworthiness. Specifically, VASparse implements a visual-aware token selection strategy during decoding to reduce redundant tokens while preserving visual context effectively. Additionally, we innovatively introduce a sparse-based visual contrastive decoding method to recalibrate the distribution of hallucinated outputs without the time overhead associated with secondary decoding. Subsequently, VASparse recalibrates attention scores to penalize attention sinking of LVLMs towards text tokens. Extensive experiments across four popular benchmarks confirm the effectiveness of VASparse in mitigating VH across different LVLM families without requiring additional training or post-processing. Impressively, VASparse achieves state-of-the-art performance for mitigating VH while maintaining competitive decoding speed. Code is available at https://github.com/mengchuang123/VASparse-github",
    "checked": true,
    "id": "90de34816d7b5aec17f8bf048b75de7d257cbaac",
    "semantic_title": "vasparse: towards efficient visual hallucination mitigation via visual-aware token sparsification",
    "citation_count": 2,
    "authors": [
      "Xianwei Zhuang",
      "Zhihong Zhu",
      "Yuxin Xie",
      "Liming Liang",
      "Yuexian Zou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Miller_SPARC_Score_Prompting_and_Adaptive_Fusion_for_Zero-Shot_Multi-Label_Recognition_CVPR_2025_paper.html": {
    "title": "SPARC: Score Prompting and Adaptive Fusion for Zero-Shot Multi-Label Recognition in Vision-Language Models",
    "volume": "main",
    "abstract": "Zero-shot multi-label recognition (MLR) with Vision-Language Models (VLMs) faces significant challenges without training data, model tuning, or architectural modifications. Existing approaches require prompt tuning or architectural adaptations, limiting zero-shot applicability. Our work proposes a novel solution treating VLMs as black boxes, leveraging scores without training data or ground truth. We make two contributions. First, we find that VLM scores suffer from image- and prompt-specific biases, and that simple standardization is surprisingly effective at removing these and boosting MLR performance. And second, we introduce compound prompts grounded in realistic object combinations. Our analysis reveals \"AND\"/\"OR\" signal ambiguities that cause maximum compound scores to be surprisingly suboptimal compared to second-highest scores. We introduce an adaptive fusion method to address this issue. Our method enhances other zero-shot approaches, consistently improving their results. Experiments show superior mean Average Precision (mAP) compared to methods requiring training data, achieved through refined object ranking for robust zero-shot MLR. Code can be found at https://github.com/kjmillerCURIS/SPARC",
    "checked": true,
    "id": "6bf0fe65df87abeac6e1b774643d27e6e85af959",
    "semantic_title": "sparc: score prompting and adaptive fusion for zero-shot multi-label recognition in vision-language models",
    "citation_count": 1,
    "authors": [
      "Kevin Miller",
      "Aditya Gangrade",
      "Samarth Mishra",
      "Kate Saenko",
      "Venkatesh Saligrama"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_UniGoal_Towards_Universal_Zero-shot_Goal-oriented_Navigation_CVPR_2025_paper.html": {
    "title": "UniGoal: Towards Universal Zero-shot Goal-oriented Navigation",
    "volume": "main",
    "abstract": "In this paper, we propose a general framework for universal zero-shot goal-oriented navigation. Existing zero-shot methods build inference framework upon large language models (LLM) for specific tasks, which differs a lot in overall pipeline and fails to generalize across different types of goal. Towards the aim of universal zero-shot navigation, we propose a uniform graph representation to unify different goals, including object category, instance image and text description. We also convert the observation of agent into an online maintained scene graph. With this consistent scene and goal representation, we preserve most structural information compared with pure text and are able to leverage LLM for explicit graph-based reasoning.Specifically, we conduct graph matching between the scene graph and goal graph at each time instant and propose different strategies to generate long-term goal of exploration according to different matching states. The agent first iteratively searches subgraph of goal when zero-matched. With partial matching, the agent then utilizes coordinate projection and anchor pair alignment to infer the goal location. Finally scene graph correction and goal verification are applied for perfect matching. We also present a blacklist mechanism to enable robust switch between stages.Extensive experiments on several benchmarks show that our UniGoal achieves state-of-the-art zero-shot performance on three studied navigation tasks with a single model, even outperforming task-specific zero-shot methods and supervised universal methods",
    "checked": true,
    "id": "01bde49e75eaa46baa2619fb239fdcde9dc47a31",
    "semantic_title": "unigoal: towards universal zero-shot goal-oriented navigation",
    "citation_count": 15,
    "authors": [
      "Hang Yin",
      "Xiuwei Xu",
      "Linqing Zhao",
      "Ziwei Wang",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qiu_Noise-Consistent_Siamese-Diffusion_for_Medical_Image_Synthesis_and_Segmentation_CVPR_2025_paper.html": {
    "title": "Noise-Consistent Siamese-Diffusion for Medical Image Synthesis and Segmentation",
    "volume": "main",
    "abstract": "Deep learning has revolutionized medical image segmentation, yet its full potential remains constrained by the paucity of annotated datasets. While diffusion models have emerged as a promising approach for generating synthetic image-mask pairs to augment these datasets, they paradoxically suffer from the same data scarcity challenges they aim to mitigate. Traditional mask-only models frequently yield low-fidelity images due to their inability to adequately capture morphological intricacies, which can critically compromise the robustness and reliability of segmentation models. To alleviate this limitation, we introduce Siamese-Diffusion, a novel dual-component model comprising Mask-Diffusion and Image-Diffusion. During training, a Noise Consistency Loss is introduced between these components to enhance the morphological fidelity of Mask-Diffusion in the parameter space. During sampling, only Mask-Diffusion is used, ensuring diversity and scalability. Comprehensive experiments demonstrate the superiority of our method. Siamese-Diffusion boosts SANet's mDice and mIoU by 3.6% and 4.4% on the Polyps, while UNet improves by 1.52% and 1.64% on the ISIC2018",
    "checked": true,
    "id": "88ecda31867c681925d4fe52d01ded7c13a9f6be",
    "semantic_title": "noise-consistent siamese-diffusion for medical image synthesis and segmentation",
    "citation_count": 10,
    "authors": [
      "Kunpeng Qiu",
      "Zhiqiang Gao",
      "Zhiying Zhou",
      "Mingjie Sun",
      "Yongxin Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Song_DefectFill_Realistic_Defect_Generation_with_Inpainting_Diffusion_Model_for_Visual_CVPR_2025_paper.html": {
    "title": "DefectFill: Realistic Defect Generation with Inpainting Diffusion Model for Visual Inspection",
    "volume": "main",
    "abstract": "Developing effective visual inspection models remains challenging due to the scarcity of defect data. While image generation models have been used to synthesize defect images, producing highly realistic defects remains difficult. We propose DefectFill, a novel method for realistic defect generation that requires only a few reference defect images. It leverages a fine-tuned inpainting diffusion model, optimized with our custom loss functions incorporating defect, object, and attention terms. It enables precise capture of detailed, localized defect features and their seamless integration into defect-free objects. Additionally, our Low-Fidelity Selection method further enhances the defect sample quality. Experiments show that DefectFill generates high-quality defect images, enabling visual inspection models to achieve state-of-the-art performance on the MVTec AD dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaewoo Song",
      "Daemin Park",
      "Kanghyun Baek",
      "Sangyub Lee",
      "Jooyoung Choi",
      "Eunji Kim",
      "Sungroh Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Less_is_More_Efficient_Image_Vectorization_with_Adaptive_Parameterization_CVPR_2025_paper.html": {
    "title": "Less is More: Efficient Image Vectorization with Adaptive Parameterization",
    "volume": "main",
    "abstract": "Image vectorization aims to convert raster images to vector ones, allowing for easy scaling and editing.Existing works mainly rely on preset parameters (i.e., a fixed number of paths and control points), ignoring the complexity of the image and posing significant challenges to practical applications.We demonstrate that such an assumption is often incorrect, as the preset paths or control points may be neither essential nor enough to achieve accurate and editable vectorization results.Based on this key insight, in this paper, we propose AdaVec, an efficient image vectorization method with adaptive parametrization, where the paths and control points can be adjusted dynamically based on the complexity of the input raster image.In particular, we first decompose the input raster image into a set of pure-colored layers that are aligned with human perception.For each layer with varying shape complexity, we propose a novel allocation mechanism to adaptively adjust the control point distribution.We further adopt a differentiable rendering process to compose and optimize the shape and color parameters of each layer iteratively.Extensive experiments demonstrate that AdaVec outperforms the baselines qualitatively and quantitatively, in terms of computational efficiency, vectorization accuracy, and editing flexibility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaibo Zhao",
      "Liang Bao",
      "Yufei Li",
      "Xu Su",
      "Ke Zhang",
      "Xiaotian Qiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_FedMIA_An_Effective_Membership_Inference_Attack_Exploiting_All_for_One_CVPR_2025_paper.html": {
    "title": "FedMIA: An Effective Membership Inference Attack Exploiting \"All for One\" Principle in Federated Learning",
    "volume": "main",
    "abstract": "Federated Learning (FL) is a promising approach for training machine learning models on decentralized data while preserving privacy. However, privacy risks, particularly Membership Inference Attacks (MIAs), which aim to determine whether a specific data point belongs to a target client's training set, remain a significant concern. Existing methods for implementing MIAs in FL primarily analyze updates from the target client, focusing on metrics such as loss, gradient norm, and gradient difference. However, these methods fail to leverage updates from non-target clients, potentially underutilizing available information.In this paper, we first formulate a one-tailed likelihood-ratio hypothesis test based on the likelihood of updates from non-target clients. Building upon this formulation, we introduce a three-stage Membership Inference Attack (MIA) method, called FedMIA, which follows the \"all for one\"--leveraging updates from all clients across multiple communication rounds to enhance MIA effectiveness. Both theoretical analysis and extensive experimental results demonstrate that FedMIA outperforms existing MIAs in both classification and generative tasks. Additionally, it can be integrated as an extension to existing methods and is robust against various defense strategies, Non-IID data, and different federated structures. Our code is available in https://github.com/Liar-Mask/FedMIA",
    "checked": true,
    "id": "3256c83b21f8feb344667711a25b8b596553f8de",
    "semantic_title": "fedmia: an effective membership inference attack exploiting \"all for one\" principle in federated learning",
    "citation_count": 2,
    "authors": [
      "Gongxi Zhu",
      "Donghao Li",
      "Hanlin Gu",
      "Yuan Yao",
      "Lixin Fan",
      "Yuxing Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Erase_Diffusion_Empowering_Object_Removal_Through_Calibrating_Diffusion_Pathways_CVPR_2025_paper.html": {
    "title": "Erase Diffusion: Empowering Object Removal Through Calibrating Diffusion Pathways",
    "volume": "main",
    "abstract": "Erase inpainting, or object removal, aims to precisely remove target objects within masked regions while preserving the overall consistency of the surrounding content. Despite diffusion-based methods have made significant strides in the field of image inpainting, challenges remain regarding the emergence of unexpected objects or artifacts. We assert that the inexact diffusion pathways established by existing standard optimization paradigms constrain the efficacy of object removal. To tackle these challenges, we propose a novel Erase Diffusion, termed EraDiff, aimed at unleashing the potential power of standard diffusion in the context of object removal. In contrast to standard diffusion, the EraDiff adapts both the optimization paradigm and the network to improve the coherence and elimination of the erasure results.We first introduce a Chain-Rectifying Optimization (CRO) paradigm, a sophisticated diffusion process specifically designed to align with the objectives of erasure. This paradigm establishes innovative diffusion transition pathways that simulate the gradual elimination of objects during optimization, allowing the model to accurately capture the intent of object removal. Furthermore, to mitigate deviations caused by artifacts during the sampling pathways, we develop a simple yet effective Self-Rectifying Attention (SRA) mechanism. The SRA calibrates the sampling pathways by altering self-attention activation, allowing the model to effectively bypass artifacts while further enhancing the coherence of the generated content. With this design, our proposed EraDiff achieves state-of-the-art performance on the OpenImages V5 dataset and demonstrates significant superiority in real-world scenarios",
    "checked": true,
    "id": "b49dd3d9f6578aef778a3f082725965c5b5d81e7",
    "semantic_title": "erase diffusion: empowering object removal through calibrating diffusion pathways",
    "citation_count": 2,
    "authors": [
      "Yi Liu",
      "Hao Zhou",
      "Benlei Cui",
      "Wenxiang Shang",
      "Ran Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chowdhury_Prompt-CAM_Making_Vision_Transformers_Interpretable_for_Fine-Grained_Analysis_CVPR_2025_paper.html": {
    "title": "Prompt-CAM: Making Vision Transformers Interpretable for Fine-Grained Analysis",
    "volume": "main",
    "abstract": "We present a simple approach to make pre-trained Vision Transformers (ViTs) interpretable for fine-grained analysis, aiming to identify and localize the traits that distinguish visually similar categories, such as bird species. Pre-trained ViTs, such as DINO, have demonstrated remarkable capabilities in extracting localized, discriminative features. However, saliency maps like Grad-CAM often fail to identify these traits, producing blurred, coarse heatmaps that highlight entire objects instead. We propose a novel approach, Prompt Class Attention Map (Prompt-CAM), to address this limitation. Prompt-CAM learns class-specific prompts for a pre-trained ViT and uses the corresponding outputs for classification. To correctly classify an image, the true-class prompt must attend to unique image patches not present in other classes' images (i.e., traits). As a result, the true class's multi-head attention maps reveal traits and their locations. Implementation-wise, Prompt-CAM is almost a \"free lunch,\" requiring only a modification to the prediction head of Visual Prompt Tuning (VPT). This makes Prompt-CAM easy to train and apply, in stark contrast to other interpretable methods that require designing specific models and training processes. Extensive empirical studies on a dozen datasets from various domains (e.g., birds, fishes, insects, fungi, flowers, food, and cars) validate the superior interpretation capability of Prompt-CAM. The source code and demo are available at https://github.com/Imageomics/Prompt_CAM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arpita Chowdhury",
      "Dipanjyoti Paul",
      "Zheda Mai",
      "Jianyang Gu",
      "Ziheng Zhang",
      "Kazi Sajeed Mehrab",
      "Elizabeth G. Campolongo",
      "Daniel Rubenstein",
      "Charles V. Stewart",
      "Anuj Karpatne",
      "Tanya Berger-Wolf",
      "Yu Su",
      "Wei-Lun Chao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cao_Instruction-based_Image_Manipulation_by_Watching_How_Things_Move_CVPR_2025_paper.html": {
    "title": "Instruction-based Image Manipulation by Watching How Things Move",
    "volume": "main",
    "abstract": "This paper introduces a novel dataset construction pipeline that samples pairs of frames from videos and uses multimodal large language models (MLLMs) to generate editing instructions for training instruction-based image manipulation models. Video frames inherently preserve the identity of subjects and scenes, ensuring consistent content preservation during editing. Additionally, video data captures diverse, natural dynamics--such as non-rigid subject motion and complex camera movements--that are difficult to model otherwise, making it an ideal source for scalable dataset construction. Using this approach, we create a new dataset to train InstructMove, a model capable of instruction-based complex manipulations that are difficult to achieve with synthetically generated datasets. Our model demonstrates state-of-the-art performance in tasks such as adjusting subject poses, rearranging elements, and altering camera perspectives",
    "checked": true,
    "id": "32edab3b925a8f5f9ba41418b19af246704d9610",
    "semantic_title": "instruction-based image manipulation by watching how things move",
    "citation_count": 5,
    "authors": [
      "Mingdeng Cao",
      "Xuaner Zhang",
      "Yinqiang Zheng",
      "Zhihao Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Morimitsu_DPFlow_Adaptive_Optical_Flow_Estimation_with_a_Dual-Pyramid_Framework_CVPR_2025_paper.html": {
    "title": "DPFlow: Adaptive Optical Flow Estimation with a Dual-Pyramid Framework",
    "volume": "main",
    "abstract": "Optical flow estimation is essential for video processing tasks, such as restoration and action recognition. The quality of videos is constantly increasing, with current standards reaching 8K resolution. However, optical flow methods are usually designed for low resolution and do not generalize to large inputs due to their rigid architectures. They adopt downscaling or input tiling to reduce the input size, causing a loss of details and global information. There is also a lack of optical flow benchmarks to judge the actual performance of existing methods on high-resolution samples. Previous works only conducted qualitative high-resolution evaluations on hand-picked samples. This paper fills this gap in optical flow estimation in two ways. We propose DPFlow, an adaptive optical flow architecture capable of generalizing up to 8K resolution inputs while trained with only low-resolution samples. We also introduce Kubric-NK, a new benchmark for evaluating optical flow methods with input resolutions ranging from 1K to 8K. Our high-resolution evaluation pushes the boundaries of existing methods and reveals new insights about their generalization capabilities. Extensive experimental results show that DPFlow achieves state-of-the-art results on the MPI-Sintel, KITTI 2015, Spring, and other high-resolution benchmarks. The code and dataset are available at https://github.com/hmorimitsu/ptlflow/tree/main/ptlflow/models/dpflow",
    "checked": true,
    "id": "4cef76d5b81830de17d13e419d14e9f8ad454002",
    "semantic_title": "dpflow: adaptive optical flow estimation with a dual-pyramid framework",
    "citation_count": 4,
    "authors": [
      "Henrique Morimitsu",
      "Xiaobin Zhu",
      "Roberto M. Cesar",
      "Xiangyang Ji",
      "Xu-Cheng Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_DocSAM_Unified_Document_Image_Segmentation_via_Query_Decomposition_and_Heterogeneous_CVPR_2025_paper.html": {
    "title": "DocSAM: Unified Document Image Segmentation via Query Decomposition and Heterogeneous Mixed Learning",
    "volume": "main",
    "abstract": "Document image segmentation is crucial in document analysis and recognition but remains challenging due to the heterogeneity of document formats and diverse segmentation tasks. Existing methods often treat these tasks separately, leading to limited generalization and resource wastage.This paper introduces DocSAM, a transformer-based unified framework for various document image segmentation tasks, including document layout analysis, multi-granularity text segmentation, and table structure recognition by modelling these tasks as a combination of instance and semantic segmentation.Specifically, DocSAM uses a Sentence BERT to map category names from each dataset into semantic queries of the same dimension as instance queries. These queries interact through attention mechanisms and are cross-attended with image features to predict instance and semantic segmentation masks. To predict instance categories, instance queries are dot-producted with semantic queries, and scores are normalized using softmax.As a result, DocSAM can be jointly trained on heterogeneous datasets, enhancing robustness and generalization while reducing computing and storage resources. Comprehensive evaluations show that DocSAM outperforms existing methods in accuracy, efficiency, and adaptability, highlighting its potential for advancing document image understanding and segmentation in various applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao-Hui Li",
      "Fei Yin",
      "Cheng-Lin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Ferret_An_Efficient_Online_Continual_Learning_Framework_under_Varying_Memory_CVPR_2025_paper.html": {
    "title": "Ferret: An Efficient Online Continual Learning Framework under Varying Memory Constraints",
    "volume": "main",
    "abstract": "In the realm of high-frequency data streams, achieving real-time learning within varying memory constraints is paramount. This paper presents Ferret, a comprehensive framework designed to enhance online accuracy of Online Continual Learning (OCL) algorithms while dynamically adapting to varying memory budgets.Ferret employs a fine-grained pipeline parallelism strategy combined with an iterative gradient compensation algorithm, ensuring seamless handling of high-frequency data with minimal latency, and effectively counteracting the challenge of stale gradients in parallel training. To adapt to varying memory budgets, its automated model partitioning and pipeline planning optimizes performance regardless of memory limitations. Extensive experiments across 20 benchmarks and 5 integrated OCL algorithms show Ferret's remarkable efficiency, achieving up to 3.7x lower memory overhead to reach the same online accuracy compared to competing methods.Furthermore, Ferret consistently outperforms these methods across diverse memory budgets, underscoring its superior adaptability. These findings position Ferret as a premier solution for efficient and adaptive OCL framework in real-time environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Zhou",
      "Yuxin Tian",
      "Jindi Lv",
      "Mingjia Shi",
      "Yuanxi Li",
      "Qing Ye",
      "Shuhao Zhang",
      "Jiancheng Lv"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hyung_Spatiotemporal_Skip_Guidance_for_Enhanced_Video_Diffusion_Sampling_CVPR_2025_paper.html": {
    "title": "Spatiotemporal Skip Guidance for Enhanced Video Diffusion Sampling",
    "volume": "main",
    "abstract": "Diffusion models have emerged as a powerful tool for generating high-quality images, videos, and 3D content. While sampling guidance techniques like CFG improve quality, they reduce diversity and motion. Autoguidance mitigates these issues but demands extra weak model training, limiting its practicality for large-scale models. In this work, we introduce Spatiotemporal Skip Guidance (STG), a simple training-free sampling guidance method for enhancing transformer-based video diffusion models. STG employs an implicit weak model via self-perturbation, avoiding the need for external models or additional training. By selectively skipping spatiotemporal layers, STG produces an aligned, degraded version of the original model to boost sample quality without compromising diversity or dynamic degree. Our contributions include: (1) introducing STG as an efficient, high-performing guidance technique for video diffusion models, (2) eliminating the need for auxiliary models by simulating a weak model through layer skipping, and (3) ensuring quality-enhanced guidance without compromising sample diversity or dynamics unlike CFG. For additional results, visit https://junhahyung.github.io/STGuidance",
    "checked": true,
    "id": "7e73f27507f41486a8d6d499783582d827f40008",
    "semantic_title": "spatiotemporal skip guidance for enhanced video diffusion sampling",
    "citation_count": 7,
    "authors": [
      "Junha Hyung",
      "Kinam Kim",
      "Susung Hong",
      "Min-Jung Kim",
      "Jaegul Choo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_VidComposition_Can_MLLMs_Analyze_Compositions_in_Compiled_Videos_CVPR_2025_paper.html": {
    "title": "VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?",
    "volume": "main",
    "abstract": "The advancement of Multimodal Large Language Models (MLLMs) has enabled significant progress in multimodal understanding, expanding their capacity to analyze video content. However, existing evaluation benchmarks for MLLMs primarily focus on abstract video comprehension, lacking a detailed assessment of their ability to understand video compositions, the nuanced interpretation of how visual elements combine and interact within highly compiled video contexts. We introduce VidComposition, a new benchmark specifically designed to evaluate the video composition understanding capabilities of MLLMs using carefully curated compiled videos and cinematic-level annotations.VidComposition includes 982 videos with 1706 multiple-choice questions, covering various compositional aspects such as camera movement, angle, shot size, narrative structure, character actions and emotions, etc. Our comprehensive evaluation of 33 open-source and proprietary MLLMs reveals a significant performance gap between human and model capabilities. This highlights the limitations of current MLLMs in understanding complex, compiled video compositions and offers insights into areas for further improvement. Our benchmark is publicly available at https://yunlong10.github.io/VidComposition/",
    "checked": true,
    "id": "50b900f9d4cf428c97253a1f2289d5628d798bba",
    "semantic_title": "vidcomposition: can mllms analyze compositions in compiled videos?",
    "citation_count": 12,
    "authors": [
      "Yunlong Tang",
      "Junjia Guo",
      "Hang Hua",
      "Susan Liang",
      "Mingqian Feng",
      "Xinyang Li",
      "Rui Mao",
      "Chao Huang",
      "Jing Bi",
      "Zeliang Zhang",
      "Pooyan Fazli",
      "Chenliang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_SAR3D_Autoregressive_3D_Object_Generation_and_Understanding_via_Multi-scale_3D_CVPR_2025_paper.html": {
    "title": "SAR3D: Autoregressive 3D Object Generation and Understanding via Multi-scale 3D VQVAE",
    "volume": "main",
    "abstract": "Autoregressive models have demonstrated remarkable success across various fields, from large language models (LLMs) to large multimodal models (LMMs) and 2D content generation, moving closer to artificial general intelligence (AGI). Despite these advances, applying autoregressive approaches to 3D object generation and understanding remains largely unexplored. This paper introduces Scale AutoRegressive 3D (SAR3D), a novel framework that leverages a multi-scale 3D vector-quantized variational autoencoder (VQVAE) to tokenize 3D objects for efficient autoregressive generation and detailed understanding. By predicting the next scale in a multi-scale latent representation instead of the next single token, SAR3D reduces generation time significantly, achieving fast 3D object generation in just 0.82 seconds on an A6000 GPU. Additionally, given the tokens enriched with hierarchical 3D-aware information, we finetune a pretrained LLM on them, enabling multimodal comprehension of 3D content.Our experiments show that SAR3D surpasses current 3D generation methods in both speed and quality and allows LLMs to interpret and caption 3D models comprehensively",
    "checked": true,
    "id": "c298686fcac0262bb3bb46f1a6932a9b3e4c95cd",
    "semantic_title": "sar3d: autoregressive 3d object generation and understanding via multi-scale 3d vqvae",
    "citation_count": 7,
    "authors": [
      "Yongwei Chen",
      "Yushi Lan",
      "Shangchen Zhou",
      "Tengfei Wang",
      "Xingang Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jin_Dual-Interrelated_Diffusion_Model_for_Few-Shot_Anomaly_Image_Generation_CVPR_2025_paper.html": {
    "title": "Dual-Interrelated Diffusion Model for Few-Shot Anomaly Image Generation",
    "volume": "main",
    "abstract": "The performance of anomaly inspection in industrial manufacturing is constrained by the scarcity of anomaly data. To overcome this challenge, researchers have started employing anomaly generation approaches to augment the anomaly dataset. However, existing anomaly generation methods suffer from limited diversity in the generated anomalies and struggle to achieve a seamless blending of this anomaly with the original image. Moreover, the generated mask is usually not aligned with the generated anomaly. In this paper, we overcome these challenges from a new perspective, simultaneously generating a pair of the overall image and the corresponding anomaly part. We propose DualAnoDiff, a novel diffusion-based few-shot anomaly image generation model, which can generate diverse and realistic anomaly images by using a dual-interrelated diffusion model, where one of them is employed to generate the whole image while the other one generates the anomaly part. Moreover, we extract background and shape information to mitigate the distortion and blurriness phenomenon in few-shot image generation. Extensive experiments demonstrate the superiority of our proposed model over state-of-the-art methods in terms of diversity, realism and the accuracy of mask. Overall, our approach significantly improves the performance of downstream anomaly inspection tasks, including anomaly detection, anomaly localization, and anomaly classification tasks. Code will be made available",
    "checked": true,
    "id": "ffae095d0aff2a712638d12c646aee5fdc51e911",
    "semantic_title": "dual-interrelated diffusion model for few-shot anomaly image generation",
    "citation_count": 12,
    "authors": [
      "Ying Jin",
      "Jinlong Peng",
      "Qingdong He",
      "Teng Hu",
      "Jiafu Wu",
      "Hao Chen",
      "Haoxuan Wang",
      "Wenbing Zhu",
      "Mingmin Chi",
      "Jun Liu",
      "Yabiao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tu_ODE_Open-Set_Evaluation_of_Hallucinations_in_Multimodal_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "ODE: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models",
    "volume": "main",
    "abstract": "Hallucination poses a persistent challenge for multimodal large language models (MLLMs). However, existing benchmarks for evaluating hallucinations are generally static, which may overlook the potential risk of data contamination. To address this issue, we propose ODE, an open-set, dynamic protocol designed to evaluate object hallucinations in MLLMs at both the existence and attribute levels. ODE employs a graph-based structure to represent real-world object concepts, their attributes, and the distributional associations between them. This structure facilitates the extraction of concept combinations based on diverse distributional criteria, generating varied samples for structured queries that evaluate hallucinations in both generative and discriminative tasks. Through the generation of new samples, dynamic concept combinations, and varied distribution frequencies, ODE mitigates the risk of data contamination and broadens the scope of evaluation. This protocol is applicable to both general and specialized scenarios, including those with limited data. Experimental results demonstrate the effectiveness of our protocol, revealing that MLLMs exhibit higher hallucination rates when evaluated with ODE-generated samples, which indicates potential data contamination. Furthermore, these generated samples aid in analyzing hallucination patterns and fine-tuning models, offering an effective approach to mitigating hallucinations in MLLMs",
    "checked": true,
    "id": "ac7cc880f897626ee2d2d5a5c40180d551f4e0f8",
    "semantic_title": "ode: open-set evaluation of hallucinations in multimodal large language models",
    "citation_count": 3,
    "authors": [
      "Yahan Tu",
      "Rui Hu",
      "Jitao Sang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Self-Supervised_Learning_for_Color_Spike_Camera_Reconstruction_CVPR_2025_paper.html": {
    "title": "Self-Supervised Learning for Color Spike Camera Reconstruction",
    "volume": "main",
    "abstract": "Spike camera is a kind of neuromorphic camera with ultra-high temporal resolution, which can capture dynamic scenes by continuously firing spike signals. To capture color information, a color filter array (CFA) is employed on the sensor of the spike camera, resulting in Bayer-pattern spike streams. How to restore high-quality color images from the binary spike signals remains challenging. In this paper, we propose a motion-guided reconstruction method for spike cameras with CFA, utilizing color layout and estimated motion information. Specifically, we develop a joint motion estimation pipeline for the Bayer-pattern spike stream, exploiting the motion consistency of channels. We propose to estimate the missing pixels of each color channel according to temporally neighboring pixels of the corresponding color along the motion trajectory. As the spike signals are read out at discrete time points, there is quantization noise that impacts the image quality. Thus, we analyze the correlation of the noise in spatial and temporal domains and propose a self-supervised network utilizing a masked spike encoder to handle the noise. Experiments on real-world captured Bayer-pattern spike streams show that our method can restore color images with better visual quality, compared with state-of-the-art methods. The source codes are available at https://github.com/csycdong/SSL-CSC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanchen Dong",
      "Ruiqin Xiong",
      "Xiaopeng Fan",
      "Zhaofei Yu",
      "Yonghong Tian",
      "Tiejun Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huy_Interactive_Medical_Image_Analysis_with_Concept-based_Similarity_Reasoning_CVPR_2025_paper.html": {
    "title": "Interactive Medical Image Analysis with Concept-based Similarity Reasoning",
    "volume": "main",
    "abstract": "The ability to interpret and intervene model decisions is important for the adoption of computer-aided diagnosis methods in clinical workflows. Recent concept-based methods link the model predictions with interpretable concepts and modify their activation scores to interact with the model. However, these concepts are at the image level, which hinders the model from pinpointing the exact patches the concepts are activated. Alternatively, prototype-based methods learn representations from training image patches and compare these with test image patches, using the similarity scores for final class prediction. However, interpreting the underlying concepts of these patches can be challenging and often necessitates post-hoc guesswork. To address this issue, this paper introduces the novel Concept-based Similarity Reasoning network (CSR), which offers (i) patch-level prototype with intrinsic concept interpretation, and (ii) spatial interactivity. First, the proposed CSR provides localized explanation by grounding prototypes of each concept on image regions. Second, our model introduces novel spatial-level interaction, allowing doctors to engage directly with specific image areas, making it an intuitive and transparent tool for medical imaging. CSR improves upon prior state-of-the-art interpretable methods by up to 4.% across three biomedical datasets",
    "checked": true,
    "id": "6a95909e870ea3ba0e6739182f75455f50f30ea1",
    "semantic_title": "interactive medical image analysis with concept-based similarity reasoning",
    "citation_count": 4,
    "authors": [
      "Ta Duc Huy",
      "Sen Kim Tran",
      "Phan Nguyen",
      "Nguyen Hoang Tran",
      "Tran Bao Sam",
      "Anton van den Hengel",
      "Zhibin Liao",
      "Johan W. Verjans",
      "Minh-Son To",
      "Vu Minh Hieu Phan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_From_Elements_to_Design_A_Layered_Approach_for_Automatic_Graphic_CVPR_2025_paper.html": {
    "title": "From Elements to Design: A Layered Approach for Automatic Graphic Design Composition",
    "volume": "main",
    "abstract": "In this work, we investigate automatic design composition from multimodal graphic elements. Although recent studies have developed various generative models for graphic design, they usually face the following limitations: they only focus on certain subtasks and are far from achieving the design composition task; they do not consider the hierarchical information of graphic designs during the generation process. To tackle these issues, we introduce the layered design principle into Large Multimodal Models (LMMs) and propose a novel approach, called LaDeCo, to accomplish this challenging task. Specifically, LaDeCo first performs layer planning for a given element set, dividing the input elements into different semantic layers according to their contents. Based on the planning results, it subsequently predicts element attributes that control the design composition in a layer-wise manner, and includes the rendered image of previously generated layers into the context. With this insightful design, LaDeCo decomposes the difficult task into smaller manageable steps, making the generation process smoother and clearer. The experimental results demonstrate the effectiveness of LaDeCo in design composition. Furthermore, we show that LaDeCo enables some interesting applications in graphic design, such as resolution adjustment, design decoration, design variation, etc. In addition, it even outperforms the specialized models in some design subtasks without any task-specific training",
    "checked": true,
    "id": "aeb6bd1ce4d53dd789801a60d5740300e420da56",
    "semantic_title": "from elements to design: a layered approach for automatic graphic design composition",
    "citation_count": 0,
    "authors": [
      "Jiawei Lin",
      "Shizhao Sun",
      "Danqing Huang",
      "Ting Liu",
      "Ji Li",
      "Jiang Bian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nguyen_h-Edit_Effective_and_Flexible_Diffusion-Based_Editing_via_Doobs_h-Transform_CVPR_2025_paper.html": {
    "title": "h-Edit: Effective and Flexible Diffusion-Based Editing via Doob's h-Transform",
    "volume": "main",
    "abstract": "We introduce a theoretical framework for diffusion-based image editing by formulating it as a reverse-time bridge modeling problem. This approach modifies the backward process of a pretrained diffusion model to construct a bridge that converges to an implicit distribution associated with the editing target at time 0. Building on this framework, we propose h-Edit, a novel editing method that utilizes Doob's h-transform and Langevin Monte Carlo to decompose the update of an intermediate edited sample into two components: a \"reconstruction\" term and an \"editing\" term. This decomposition provides flexibility, allowing the reconstruction term to be computed via existing inversion techniques and enabling the combination of multiple editing terms to handle complex editing tasks. To our knowledge, h-Edit is the first training-free method capable of performing simultaneous text-guided and reward-model-based editing. Extensive experiments, both quantitative and qualitative, show that h-Edit outperforms state-of-the-art baselines in terms of editing effectiveness and faithfulness",
    "checked": true,
    "id": "e96e619638eb6444618c0721dce2af43f330c24e",
    "semantic_title": "h-edit: effective and flexible diffusion-based editing via doob's h-transform",
    "citation_count": 2,
    "authors": [
      "Toan Nguyen",
      "Kien Do",
      "Duc Kieu",
      "Thin Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Heo_Masking_meets_Supervision_A_Strong_Learning_Alliance_CVPR_2025_paper.html": {
    "title": "Masking meets Supervision: A Strong Learning Alliance",
    "volume": "main",
    "abstract": "Pre-training with random masked inputs has emerged as a novel trend in self-supervised training. However, supervised learning still faces a challenge in adopting masking augmentations, primarily due to unstable training. In this paper, we propose a novel way to involve masking augmentations dubbed Masked Sub-branch (MaskSub). MaskSub consists of the main-branch and sub-branch, the latter being a part of the former. The main-branch undergoes conventional training recipes, while the sub-branch merits intensive masking augmentations, during training. MaskSub tackles the challenge by mitigating adverse effects through a relaxed loss function similar to a self-distillation loss. Our analysis shows that MaskSub improves performance, with the training loss converging faster than in standard training, which suggests our method stabilizes the training process. We further validate MaskSub across diverse training scenarios and models, including DeiT-III training, MAE finetuning, CLIP finetuning, BERT training, and hierarchical architectures (ResNet and Swin Transformer). Our results show that MaskSub consistently achieves impressive performance gains across all the cases. MaskSub provides a practical and effective solution for introducing additional regularization under various training recipes. Code available at https://github.com/naver-ai/augsub",
    "checked": true,
    "id": "886d6ce09ad3804c29d0754c02265f00fb5ff26b",
    "semantic_title": "masking meets supervision: a strong learning alliance",
    "citation_count": 3,
    "authors": [
      "Byeongho Heo",
      "Taekyung Kim",
      "Sangdoo Yun",
      "Dongyoon Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_DI-PCG_Diffusion-based_Efficient_Inverse_Procedural_Content_Generation_for_High-quality_3D_CVPR_2025_paper.html": {
    "title": "DI-PCG: Diffusion-based Efficient Inverse Procedural Content Generation for High-quality 3D Asset Creation",
    "volume": "main",
    "abstract": "Procedural Content Generation (PCG) is powerful in creating high-quality 3D contents, yet controlling it to produce desired shapes is difficult and often requires extensive parameter tuning. Inverse Procedural Content Generation aims to automatically find the best parameters under the input condition. However, existing sampling-based and neural network-based methods still suffer from numerous sample iterations or limited controllability. In this work, we present DI-PCG, a novel and efficient method for Inverse PCG from general image conditions. At its core is a lightweight diffusion transformer model, where PCG parameters are directly treated as the denoising target and the observed images as conditions to control parameter generation. DI-PCG is efficient and effective. With only 7.6M network parameters and 30 GPU hours to train, it demonstrates superior performance in recovering parameters accurately, and generalizing well to in-the-wild images. Quantitative and qualitative experiment results validate the effectiveness of DI-PCG in inverse PCG and image-to-3D generation tasks. DI-PCG offers a promising approach for efficient inverse PCG and represents a valuable exploration step towards a 3D generation path that models how to construct a 3D asset using parametric models",
    "checked": true,
    "id": "951b4df132215c39486f98b107feeb0afb9829d6",
    "semantic_title": "di-pcg: diffusion-based efficient inverse procedural content generation for high-quality 3d asset creation",
    "citation_count": 3,
    "authors": [
      "Wang Zhao",
      "Yan-Pei Cao",
      "Jiale Xu",
      "Yuejiang Dong",
      "Ying Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_SALOVA_Segment-Augmented_Long_Video_Assistant_for_Targeted_Retrieval_and_Routing_CVPR_2025_paper.html": {
    "title": "SALOVA: Segment-Augmented Long Video Assistant for Targeted Retrieval and Routing in Long-Form Video Analysis",
    "volume": "main",
    "abstract": "Despite advances in Large Multi-modal Models, applying them to long and untrimmed video content remains challenging due to limitations in context length and substantial memory overhead. These constraints often lead to significant information loss and reduced relevance in the model responses. With the exponential growth of video data across web platforms, understanding long-form video is crucial for advancing generalized intelligence. In this paper, we introduce SALOVA: Segment-Augmented LOng Video Assistant, a novel video-LLM framework designed to enhance the comprehension of lengthy video content through targeted retrieval process. We address two main challenges to achieve it: (i) We present the SceneWalk dataset, a high-quality collection of 87.8K long videos, each densely captioned at the segment level to enable models to capture scene continuity and maintain rich descriptive context. (ii) We develop robust architectural designs integrating dynamic routing mechanism and spatio-temporal projector to efficiently retrieve and process relevant video segments based on user queries. Our framework mitigates the limitations of current video-LMMs by allowing for precise identification and retrieval of relevant video segments in response to queries, thereby improving the contextual relevance of the generated responses. Through extensive experiments, SALOVA demonstrates enhanced capability in processing complex long-form videos, showing significant capability to maintain contextual integrity across extended sequences",
    "checked": true,
    "id": "cf5898e70519a4823249ccec2e0509d8136a30eb",
    "semantic_title": "salova: segment-augmented long video assistant for targeted retrieval and routing in long-form video analysis",
    "citation_count": 4,
    "authors": [
      "Junho Kim",
      "Hyunjun Kim",
      "Hosu Lee",
      "Yong Man Ro"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_Notes-guided_MLLM_Reasoning_Enhancing_MLLM_with_Knowledge_and_Visual_Notes_CVPR_2025_paper.html": {
    "title": "Notes-guided MLLM Reasoning: Enhancing MLLM with Knowledge and Visual Notes for Visual Question Answering",
    "volume": "main",
    "abstract": "The knowledge-based visual question answering (KB-VQA) task involves using external knowledge about the image to assist reasoning. Building on the impressive performance of multimodal large language model (MLLM), recent methods have commenced leveraging MLLM as an implicit knowledge base for reasoning. However, the direct employment of MLLM with raw external knowledge might result in reasoning errors due to misdirected knowledge information. Additionally, MLLM may lack fine-grained perception of visual features, which can result in hallucinations during reasoning. To address these challenges, we propose Notes-guided MLLM Reasoning (NoteMR), a novel framework that guides MLLM in better reasoning by utilizing knowledge notes and visual notes. Specifically, we initially obtain explicit knowledge from an external knowledge base. Then, this explicit knowledge, combined with images, is used to assist the MLLM in generating knowledge notes. These notes are designed to filter explicit knowledge and identify relevant internal implicit knowledge within the MLLM. We then identify highly correlated regions between the images and knowledge notes, retaining them as image notes to enhance the model's fine-grained perception, thereby mitigating MLLM induced hallucinations. Finally, both notes are fed into the MLLM, enabling a more comprehensive understanding of the image-question pair and enhancing the model's reasoning capabilities. Our method achieves state-of-the-art performance on the OK-VQA and A-OKVQA datasets, demonstrating its robustness and effectiveness across diverse VQA scenarios",
    "checked": true,
    "id": "9a0225c78ed7f4ffecdbd5762860959d57479262",
    "semantic_title": "notes-guided mllm reasoning: enhancing mllm with knowledge and visual notes for visual question answering",
    "citation_count": 0,
    "authors": [
      "Wenlong Fang",
      "Qiaofeng Wu",
      "Jing Chen",
      "Yun Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Are_Spatial-Temporal_Graph_Convolution_Networks_for_Human_Action_Recognition_Over-Parameterized_CVPR_2025_paper.html": {
    "title": "Are Spatial-Temporal Graph Convolution Networks for Human Action Recognition Over-Parameterized?",
    "volume": "main",
    "abstract": "Spatial-temporal graph convolutional networks (ST-GCNs) showcase impressive performance in skeleton-based human action recognition (HAR). However, despite the development of numerous models, their recognition performance does not differ significantly after aligning the input settings. With this observation, we hypothesize that ST-GCNs are over-parameterized for HAR, a conjecture subsequently confirmed through experiments employing the lottery ticket hypothesis. Additionally, a novel sparse ST-GCNs generator is proposed, which trains a sparse architecture from a randomly initialized dense network while maintaining comparable performance levels to the dense components. Moreover, we generate multi-level sparsity ST-GCNs by integrating sparse structures at various sparsity levels and demonstrate that the assembled model yields a significant enhancement in HAR performance. Thorough experiments on four datasets, including NTU-RGB+D 60(120), Kinetics-400, and FineGYM, demonstrate that the proposed sparse ST-GCNs can achieve comparable performance to their dense components. Even with 95% fewer parameters, the sparse ST-GCNs exhibit a degradation of <1% in top-1 accuracy. Meanwhile, the multi-level sparsity ST-GCNs, which require only 66% of the parameters of the dense ST-GCNs, demonstrate an improvement of >1% in top-1 accuracy. The code is available at https://github.com/davelailai/Sparse-ST-GCN",
    "checked": true,
    "id": "8a909479fcc0a3e05f8f18e14f2563cca87ad2e3",
    "semantic_title": "are spatial-temporal graph convolution networks for human action recognition over-parameterized?",
    "citation_count": 0,
    "authors": [
      "Jianyang Xie",
      "Yitian Zhao",
      "Yanda Meng",
      "He Zhao",
      "Anh Nguyen",
      "Yalin Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_DA-VPT_Semantic-Guided_Visual_Prompt_Tuning_for_Vision_Transformers_CVPR_2025_paper.html": {
    "title": "DA-VPT: Semantic-Guided Visual Prompt Tuning for Vision Transformers",
    "volume": "main",
    "abstract": "Visual Prompt Tuning (VPT) has become a promising solution for Parameter-Efficient Fine-Tuning (PEFT) approach for Vision Transformer (ViT) models by partially fine-tuning learnable tokens while keeping most model parameters frozen. Recent research has explored modifying the connection structures of the prompts. However, the fundamental correlation and distribution between the prompts and image tokens remain unexplored. In this paper, we leverage metric learning techniques to investigate how the distribution of prompts affects fine-tuning performance. Specifically, we propose a novel framework, Distribution Aware Visual Prompt Tuning (DA-VPT), to guide the distributions of the prompts by learning the distance metric from their class-related semantic data. Our method demonstrates that the prompts can serve as an effective bridge to share semantic information between image patches and the class token. We extensively evaluated our approach on popular benchmarks in both recognition and segmentation tasks. The results demonstrate that our approach enables more effective and efficient fine-tuning of ViT models by leveraging semantic information to guide the learning of the prompts, leading to improved performance on various downstream vision tasks",
    "checked": true,
    "id": "3cf4f7bb9b08b18ab8118616f36c92c5d62c9ead",
    "semantic_title": "da-vpt: semantic-guided visual prompt tuning for vision transformers",
    "citation_count": 1,
    "authors": [
      "Li Ren",
      "Chen Chen",
      "Liqiang Wang",
      "Kien Hua"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_Towards_Lossless_Implicit_Neural_Representation_via_Bit_Plane_Decomposition_CVPR_2025_paper.html": {
    "title": "Towards Lossless Implicit Neural Representation via Bit Plane Decomposition",
    "volume": "main",
    "abstract": "We quantify the upper bound on the size of the implicit neural representation (INR) model from a digital perspective. The upper bound of the model size increases exponentially as the required bit-precision increases. To this end, we present a bit-plane decomposition method that makes INR predict bit-planes, producing the same effect as reducing the upper bound of the model size. We validate our hypothesis that reducing the upper bound leads to faster convergence with constant model size. Our method achieves lossless representation in 2D image and audio fitting, even for high bit-depth signals, such as 16-bit, which was previously unachievable. We pioneered the presence of bit bias, which INR prioritizes as the most significant bit (MSB). We expand the application of the INR task to bit depth expansion, lossless image compression, and extreme network quantization. Our source code is available at https://github.com/WooKyoungHan/LosslessINR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woo Kyoung Han",
      "Byeonghun Lee",
      "Hyunmin Cho",
      "Sunghoon Im",
      "Kyong Hwan Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dastani_Spectral_State_Space_Model_for_Rotation-Invariant_Visual_Representation_Learning_CVPR_2025_paper.html": {
    "title": "Spectral State Space Model for Rotation-Invariant Visual Representation Learning",
    "volume": "main",
    "abstract": "State Space Models (SSMs) have recently emerged as an alternative to Vision Transformers (ViTs) due to their unique ability of modeling global relationships with linear complexity. SSMs are specifically designed to capture spatially proximate relationships of image patches. However, they fail to identify relationships between conceptually related yet not adjacent patches. This limitation arises from the non-causal nature of image data, which lacks inherent directional relationships. Additionally, current vision-based SSMs are highly sensitive to transformations such as rotation. Their predefined scanning directions depend on the original image orientation, which can cause the model to produce inconsistent patch-processing sequences after rotation.To address these limitations, we introduce Spectral VMamba, a novel approach that effectively captures the global structure within an image by leveraging spectral information derived from the graph Laplacian of image patches. Through spectral decomposition, our approach encodes patch relationships independently of image orientation, achieving rotation invariance with the aid of our Rotational Feature Normalizer (RFN) module. Our experiments on classification tasks show that Spectral VMamba outperforms the leading SSM models in vision, such as VMamba, while maintaining invariance to rotations and a providing a similar runtime efficiency",
    "checked": true,
    "id": "e702e1470c8590c74867594d6dced833b3edb186",
    "semantic_title": "spectral state space model for rotation-invariant visual representation learning",
    "citation_count": 1,
    "authors": [
      "Sahar Dastani",
      "Ali Bahri",
      "Moslem Yazdanpanah",
      "Mehrdad Noori",
      "David Osowiechi",
      "Gustavo Adolfo Vargas Hakim",
      "Farzad Beizaee",
      "Milad Cheraghalikhani",
      "Arnab Kumar Mondal",
      "Herve Lombaert",
      "Christian Desrosiers"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_iSegMan_Interactive_Segment-and-Manipulate_3D_Gaussians_CVPR_2025_paper.html": {
    "title": "iSegMan: Interactive Segment-and-Manipulate 3D Gaussians",
    "volume": "main",
    "abstract": "The efficient rendering and explicit nature of 3DGS promote the advancement of 3D scene manipulation.However, existing methods typically encounter challenges in controlling the manipulation region and are unable to furnish the user with interactive feedback, which inevitably leads to unexpected results.Intuitively, incorporating interactive 3D segmentation tools can compensate for this deficiency. Nevertheless, existing segmentation frameworks impose a pre-processing step of scene-specific parameter training, which limits the efficiency and flexibility of scene manipulation.To deliver a 3D region control module that is well-suited for scene manipulation with reliable efficiency, we propose **i**nteractive **Seg**ment-and-**Man**ipulate 3D Gaussians (**iSegMan**), an interactive segmentation and manipulation framework that only requires simple 2D user interactions in any view.To propagate user interactions to other views, we propose Epipolar-guided Interaction Propagation (**EIP**), which innovatively exploits epipolar constraint for efficient and robust interaction matching.To avoid scene-specific training to maintain efficiency, we further propose the novel Visibility-based Gaussian Voting (**VGV**), which obtains 2D segmentations from SAM and models the region extraction as a voting game between 2D Pixels and 3D Gaussians based on Gaussian visibility.Taking advantage of the efficient and precise region control of EIP and VGV, we put forth a **Manipulation Toolbox** to implement various functions on selected regions, enhancing the controllability, flexibility and practicality of scene manipulation.Extensive results on 3D scene manipulation and segmentation tasks fully demonstrate the significant advantages of iSegMan",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yian Zhao",
      "Wanshi Xu",
      "Ruochong Zheng",
      "Pengchong Qiao",
      "Chang Liu",
      "Jie Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_BlueLM-V-3B_Algorithm_and_System_Co-Design_for_Multimodal_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "BlueLM-V-3B: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices",
    "volume": "main",
    "abstract": "The emergence and growing popularity of multimodal large language models (MLLMs) have significant potential to enhance various aspects of daily life, from improving communication to facilitating learning and problem-solving. Mobile phones, as essential daily companions, represent the most effective and accessible deployment platform for MLLMs, enabling seamless integration into everyday tasks. However, deploying MLLMs on mobile phones presents challenges due to limitations in memory size and computational capability, making it difficult to achieve smooth and real-time processing without extensive optimization. In this paper, we present BlueLM-V-3B, an algorithm and system co-design approach specifically tailored for the efficient deployment of MLLMs on mobile platforms. To be specific, we redesign the dynamic resolution scheme adopted by mainstream MLLMs and implement system optimization for hardware-aware deployment to optimize model inference on mobile phones. BlueLM-V-3B boasts the following key highlights: (1) Small Size: BlueLM-V-3B features a language model with 2.7B parameters and a vision encoder with 400M parameters. (2) Fast Speed: BlueLM-V-3B achieves a generation speed of 24.4 token/s on the MediaTek Dimensity 9300 processor with 4-bit LLM weight quantization. (3) Strong Performance: BlueLM-V-3B has attained the highest average score of 66.1 on the OpenCompass benchmark among models with <= 4B parameters and surpassed a series of models with much larger parameter sizes (e.g., MiniCPM-V-2.6, InternVL2-8B)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xudong Lu",
      "Yinghao Chen",
      "Cheng Chen",
      "Hui Tan",
      "Boheng Chen",
      "Yina Xie",
      "Rui Hu",
      "Guanxin Tan",
      "Renshou Wu",
      "Yan Hu",
      "Yi Zeng",
      "Lei Wu",
      "Liuyang Bian",
      "Zhaoxiong Wang",
      "Long Liu",
      "Yanzhou Yang",
      "Han Xiao",
      "Aojun Zhou",
      "Yafei Wen",
      "Xiaoxin Chen",
      "Shuai Ren",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Unraveling_Normal_Anatomy_via_Fluid-Driven_Anomaly_Randomization_CVPR_2025_paper.html": {
    "title": "Unraveling Normal Anatomy via Fluid-Driven Anomaly Randomization",
    "volume": "main",
    "abstract": "Data-driven machine learning has made significant strides in medical image analysis. However, most existing methods are tailored to specific modalities and assume a particular resolution (often isotropic). This limits their generalizability in clinical settings, where variations in scan appearance arise from differences in sequence parameters, resolution, and orientation. Furthermore, most general-purpose models are designed for healthy subjects and suffer from performance degradation when pathology is present. We introduce UNA (Unraveling Normal Anatomy), the first modality-agnostic learning approach for normal brain anatomy reconstruction that can handle both healthy scans and cases with pathology. We propose a fluid-driven anomaly randomization method that generates an unlimited number of realistic pathology profiles on-the-fly. UNA is trained on a combination of synthetic and real data, and can be applied directly to real images with potential pathology without the need for fine-tuning. We demonstrate UNA's effectiveness in reconstructing healthy brain anatomy and showcase its direct application to anomaly detection, using both simulated and real images from 3D healthy and stroke datasets, including CT and MRI scans. By bridging the gap between healthy and diseased images, UNA enables the use of general-purpose models on diseased images, opening up new opportunities for large-scale analysis of uncurated clinical images in the presence of pathology",
    "checked": true,
    "id": "cdae5828062e06b3d560f0dde0b465336cd1e650",
    "semantic_title": "unraveling normal anatomy via fluid-driven anomaly randomization",
    "citation_count": 3,
    "authors": [
      "Peirong Liu",
      "Ana Lawry Aguila",
      "Juan E. Iglesias"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Taming_Teacher_Forcing_for_Masked_Autoregressive_Video_Generation_CVPR_2025_paper.html": {
    "title": "Taming Teacher Forcing for Masked Autoregressive Video Generation",
    "volume": "main",
    "abstract": "We introduce MAGI, a hybrid video generation framework that combines masked modeling for intra-frame generation with causal modeling for next-frame generation. Our key innovation, Complete Teacher Forcing (CTF), conditions masked frames on complete observation frames rather than masked ones (namely Masked Teacher Forcing, MTF), enabling a smooth transition from token-level (patch-level) to frame-level autoregressive generation. CTF significantly outperforms MTF, achieving a 23% improvement in FVD scores on first-frame conditioned video prediction. To address issues like exposure bias, we employ targeted training strategies, setting a new benchmark in autoregressive video generation. Experiments show that MAGI can generate long, coherent video sequences exceeding 100 frames, even when trained on as few as 16 frames, highlighting its potential for scalable, high-quality video generation",
    "checked": true,
    "id": "ae66295a839e6e4dc71c9cde6da9bc28061ddc8a",
    "semantic_title": "taming teacher forcing for masked autoregressive video generation",
    "citation_count": 12,
    "authors": [
      "Deyu Zhou",
      "Quan Sun",
      "Yuang Peng",
      "Kun Yan",
      "Runpei Dong",
      "Duomin Wang",
      "Zheng Ge",
      "Nan Duan",
      "Xiangyu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_UniRestore_Unified_Perceptual_and_Task-Oriented_Image_Restoration_Model_Using_Diffusion_CVPR_2025_paper.html": {
    "title": "UniRestore: Unified Perceptual and Task-Oriented Image Restoration Model Using Diffusion Prior",
    "volume": "main",
    "abstract": "Image restoration aims to recover content from inputs degraded by various factors, such as adverse weather, blur, and noise. Perceptual Image Restoration (PIR) methods improve visual quality but often do not support downstream tasks effectively. On the other hand, Task-oriented Image Restoration (TIR) methods focus on enhancing image utility for high-level vision tasks, sometimes compromising visual quality. This paper introduces UniRestore, a unified image restoration model that bridges the gap between PIR and TIR by using a diffusion prior. The diffusion prior is designed to generate images that align with human visual quality preferences, but these images are often unsuitable for TIR scenarios. To solve this limitation, UniRestore utilizes encoder features from an autoencoder to adapt the diffusion prior to specific tasks. We propose a Complementary Feature Restoration Module (CFRM) to reconstruct degraded encoder features and a Task Feature Adapter (TFA) module to facilitate adaptive feature fusion in the decoder. This design allows UniRestore to optimize images for both human perception and downstream task requirements, addressing discrepancies between visual quality and functional needs. Integrating these modules also enhances UniRestore's adapability and efficiency across diverse tasks. Extensive expertments demonstrate the superior performance of UniRestore in both PIR and TIR scenarios",
    "checked": true,
    "id": "f4498685e25c1d8da85a3dd110a94054a1db71f9",
    "semantic_title": "unirestore: unified perceptual and task-oriented image restoration model using diffusion prior",
    "citation_count": 5,
    "authors": [
      "I-Hsiang Chen",
      "Wei-Ting Chen",
      "Yu-Wei Liu",
      "Yuan-Chun Chiang",
      "Sy-Yen Kuo",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Edelstein_Sharp-It_A_Multi-view_to_Multi-view_Diffusion_Model_for_3D_Synthesis_CVPR_2025_paper.html": {
    "title": "Sharp-It: A Multi-view to Multi-view Diffusion Model for 3D Synthesis and Manipulation",
    "volume": "main",
    "abstract": "Advancements in text-to-image diffusion models have led to significant progress in fast 3D content creation. One common approach is to generate a set of multi-view images of an object, and then reconstruct it into a 3D model. However, this approach bypasses the use of a native 3D representation of the object and is hence prone to geometric artifacts and limited in controllability and manipulation capabilities. An alternative approach involves native 3D generative models that directly produce 3D representations. These models, however, are typically limited in their resolution, resulting in lower quality 3D objects. In this work, we bridge the quality gap between methods that directly generate 3D representations and ones that reconstruct 3D objects from multi-view images. We introduce a multi-view to multi-view diffusion model called Sharp-It, which takes a 3D consistent set of multi-view images rendered from a low-quality object and enriches its geometric details and texture. The diffusion model operates on the multi-view set in parallel, in the sense that it shares features across the generated views. A high-quality 3D model can then be reconstructed from the enriched multi-view set. By leveraging the advantages of both 2D and 3D approaches, our method offers an efficient and controllable method for high-quality 3D content creation. We demonstrate that Sharp-It enables various 3D applications, such as fast synthesis, editing, and controlled generation, while attaining high-quality assets",
    "checked": true,
    "id": "be5b7cb14ae8dd3f248eac09927396371f6fce4a",
    "semantic_title": "sharp-it: a multi-view to multi-view diffusion model for 3d synthesis and manipulation",
    "citation_count": 1,
    "authors": [
      "Yiftach Edelstein",
      "Or Patashnik",
      "Dana Cohen-Bar",
      "Lihi Zelnik-Manor"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_URWKV_Unified_RWKV_Model_with_Multi-state_Perspective_for_Low-light_Image_CVPR_2025_paper.html": {
    "title": "URWKV: Unified RWKV Model with Multi-state Perspective for Low-light Image Restoration",
    "volume": "main",
    "abstract": "Existing low-light image enhancement (LLIE) and joint LLIE and deblurring (LLIE-deblur) models have made strides in addressing predefined degradations, yet they are often constrained by dynamically coupled degradations. To address these challenges, we introduce a Unified Receptance Weighted Key Value (URWKV) model with multi-state perspective, enabling flexible and effective degradation restoration for low-light images. Specifically, we customize the core URWKV block to perceive and analyze complex degradations by leveraging multiple intra- and inter-stage states. First, inspired by the pupil mechanism in the human visual system, we propose Luminance-adaptive Normalization (LAN) that adjusts normalization parameters based on rich inter-stage states, allowing for adaptive, scene-aware luminance modulation. Second, we aggregate multiple intra-stage states through exponential moving average approach, effectively capturing subtle variations while mitigating information loss inherent in the single-state mechanism. To reduce the degradation effects commonly associated with conventional skip connections, we propose the State-aware Selective Fusion (SSF) module, which dynamically aligns and integrates multi-state features across encoder stages, selectively fusing contextual information. In comparison to state-of-the-art models, our URWKV model achieves superior performance on various benchmarks, while requiring significantly fewer parameters and computational resources. Code is available at: https://github.com/FZU-N/URWKV",
    "checked": true,
    "id": "2dccf575dfd8b6298e41b9cddb923f302dfde6ac",
    "semantic_title": "urwkv: unified rwkv model with multi-state perspective for low-light image restoration",
    "citation_count": 2,
    "authors": [
      "Rui Xu",
      "Yuzhen Niu",
      "Yuezhou Li",
      "Huangbiao Xu",
      "Wenxi Liu",
      "Yuzhong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Revisiting_Backdoor_Attacks_against_Large_Vision-Language_Models_from_Domain_Shift_CVPR_2025_paper.html": {
    "title": "Revisiting Backdoor Attacks against Large Vision-Language Models from Domain Shift",
    "volume": "main",
    "abstract": "Instruction tuning enhances large vision-language models (LVLMs) but increases their vulnerability to backdoor attacks due to their open design. Unlike prior studies in static settings, this paper explores backdoor attacks in LVLM instruction tuning across mismatched training and testing domains. We introduce a new evaluation dimension, backdoor domain generalization, to assess attack robustness under visual and text domain shifts. Our findings reveal two insights: (1) backdoor generalizability improves when distinctive trigger patterns are independent of specific data domains or model architectures, and (2) the competitive interaction between trigger patterns and clean semantic regions, where guiding the model to predict triggers enhances attack generalizability. Based on these insights, we propose a multimodal attribution backdoor attack (MABA) that injects domain-agnostic triggers into critical areas using attributional interpretation. Experiments with OpenFlamingo, Blip-2, and Otter show that MABA significantly boosts the attack success rate of generalization by 36.4% over the unimodal attack, achieving a 97% success rate at a 0.2% poisoning rate. This study reveals limitations in current evaluations and highlights how enhanced backdoor generalizability poses a security threat to LVLMs, even without test data access",
    "checked": true,
    "id": "2a76b2797949fda3102a83213c6446e6716e002e",
    "semantic_title": "revisiting backdoor attacks against large vision-language models from domain shift",
    "citation_count": 11,
    "authors": [
      "Siyuan Liang",
      "Jiawei Liang",
      "Tianyu Pang",
      "Chao Du",
      "Aishan Liu",
      "Mingli Zhu",
      "Xiaochun Cao",
      "Dacheng Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ding_Condensing_Action_Segmentation_Datasets_via_Generative_Network_Inversion_CVPR_2025_paper.html": {
    "title": "Condensing Action Segmentation Datasets via Generative Network Inversion",
    "volume": "main",
    "abstract": "This work presents the first condensation approach for procedural video datasets used in temporal action segmentation. We propose a condensation framework that leverages generative prior learned from the dataset and network inversion to condense data into compact latent codes with significant storage reduced across temporal and channel aspects. Orthogonally, we propose sampling diverse and representative action sequences to minimize video-wise redundancy. Our evaluation on standard benchmarks demonstrates consistent effectiveness in condensing TAS datasets and achieving competitive performances. Specifically, on the Breakfast dataset, our approach reduces storage by over 500xwhile retaining 83% of the performance compared to training with the full dataset. Furthermore, when applied to a downstream incremental learning task, it yields superior performance compared to the state-of-the-art",
    "checked": true,
    "id": "6eea6a64f59f0faee5a6ea2b2635e3a99c65c7d6",
    "semantic_title": "condensing action segmentation datasets via generative network inversion",
    "citation_count": 1,
    "authors": [
      "Guodong Ding",
      "Rongyu Chen",
      "Angela Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kwon_TCFG_Tangential_Damping_Classifier-free_Guidance_CVPR_2025_paper.html": {
    "title": "TCFG: Tangential Damping Classifier-free Guidance",
    "volume": "main",
    "abstract": "Diffusion models have achieved remarkable success in text-to-image synthesis, largely attributed to the use of classifier-free guidance (CFG), which enables high-quality, condition-aligned image generation. CFG combines the conditional score (e.g., text-conditioned) with the unconditional score to control the output. However, the unconditional score is in charge of estimating the transition between manifolds of adjacent timesteps from x_t to x_ t-1 , which may inadvertently interfere with the trajectory toward the specific condition. In this work, we introduce a novel approach that leverages a geometric perspective on the unconditional score to enhance CFG performance when conditional scores are available. Specifically, we propose a method that filters the singular vectors of both conditional and unconditional scores using singular value decomposition. This filtering process aligns the unconditional score with the conditional score, thereby refining the sampling trajectory to stay closer to the manifold. Our approach improves image quality with negligible additional computation. We provide deeper insights into the score function behavior in diffusion models and present a practical technique for achieving more accurate and contextually coherent image synthesis",
    "checked": true,
    "id": "996b79731a0ccfab2f03f5401312741ed678a310",
    "semantic_title": "tcfg: tangential damping classifier-free guidance",
    "citation_count": 1,
    "authors": [
      "Mingi Kwon",
      "Shin seong Kim",
      "Jaeseok Jeong",
      "Yi Ting Hsiao",
      "Youngjung Uh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_MatAnyone_Stable_Video_Matting_with_Consistent_Memory_Propagation_CVPR_2025_paper.html": {
    "title": "MatAnyone: Stable Video Matting with Consistent Memory Propagation",
    "volume": "main",
    "abstract": "Auxiliary-free human video matting methods, which rely solely on input frames, often struggle with complex or ambiguous backgrounds. To tackle this, we propose MatAnyone, a practical framework designed for target-assigned video matting. Specifically, building on a memory-based framework, we introduce a consistent memory propagation module via region-adaptive memory fusion, which adaptively combines memory from the previous frame. This ensures stable semantic consistency in core regions while maintaining fine details along object boundaries. For robust training, we present a larger, high-quality, and diverse dataset for video matting. Additionally, we incorporate a novel training strategy that efficiently leverages large-scale segmentation data, further improving matting stability. With this new network design, dataset, and training strategy, MatAnyone delivers robust, accurate video matting in diverse real-world scenarios, outperforming existing methods. The code and model will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peiqing Yang",
      "Shangchen Zhou",
      "Jixin Zhao",
      "Qingyi Tao",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cai_Can_Generative_Video_Models_Help_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "Can Generative Video Models Help Pose Estimation?",
    "volume": "main",
    "abstract": "Pairwise pose estimation from images with little or no overlap is an open challenge in computer vision. Existing methods, even those trained on large-scale datasets, struggle in these scenarios due to the lack of identifiable correspondences or visual overlap. Inspired by the human ability to infer spatial relationships from diverse scenes, we propose a novel approach, InterPose, that leverages the rich priors encoded within pre-trained generative video models. We propose to use a video model to hallucinate intermediate frames between two input images, effectively creating a dense, visual transition, which significantly simplifies the problem of pose estimation. Since current video models can still produce implausible motion or inconsistent geometry, we introduce a self-consistency score that evaluates the consistency of pose predictions from sampled videos. We demonstrate that our approach generalizes among three state-of-the-art video models and show consistent improvements over the state-of-the-art DUSt3R baseline on four diverse datasets encompassing indoor, outdoor, and object-centric scenes. Our findings suggest a promising avenue for improving pose estimation models by leveraging large generative models trained on vast amounts of video data, which is more readily available than 3D data. See our project page for results: Inter-Pose.github.io",
    "checked": true,
    "id": "eba6a54df251ec0a818f02d8570eb25b3a146f1a",
    "semantic_title": "can generative video models help pose estimation?",
    "citation_count": 1,
    "authors": [
      "Ruojin Cai",
      "Jason Y. Zhang",
      "Philipp Henzler",
      "Zhengqi Li",
      "Noah Snavely",
      "Ricardo Martin-Brualla"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deitke_Molmo_and_PixMo_Open_Weights_and_Open_Data_for_State-of-the-Art_CVPR_2025_paper.html": {
    "title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models",
    "volume": "main",
    "abstract": "Today's most advanced vision-language models (VLMs) remain proprietary. The strongest open-weight models rely heavily on synthetic data from proprietary VLMs to achieve good performance, effectively distilling these closed VLMs into open ones. As a result, the community has been missing foundational knowledge about how to build performant VLMs from scratch. We present Molmo, a new family of VLMs that are state-of-the-art in their class of openness. Our key contribution is a collection of new datasets called PixMo, including a dataset of highly detailed image captions for pre-training, a free-form image Q&A dataset for fine-tuning, and an innovative 2D pointing dataset, all collected without the use of external VLMs. The success of our approach relies on careful modeling choices, a well-tuned training pipeline, and, most critically, the quality of our newly collected datasets. Our best-in-class 72B model not only outperforms others in the class of open weight and data models, but also outperforms larger proprietary models including Claude 3.5 Sonnet, and Gemini 1.5 Pro and Flash, second only to GPT-4o based on both academic benchmarks and on a large human evaluation. Our model weights, new datasets, and source code are available at https://molmo.allenai.org/blog",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matt Deitke",
      "Christopher Clark",
      "Sangho Lee",
      "Rohun Tripathi",
      "Yue Yang",
      "Jae Sung Park",
      "Mohammadreza Salehi",
      "Niklas Muennighoff",
      "Kyle Lo",
      "Luca Soldaini",
      "Jiasen Lu",
      "Taira Anderson",
      "Erin Bransom",
      "Kiana Ehsani",
      "Huong Ngo",
      "YenSung Chen",
      "Ajay Patel",
      "Mark Yatskar",
      "Chris Callison-Burch",
      "Andrew Head",
      "Rose Hendrix",
      "Favyen Bastani",
      "Eli VanderBilt",
      "Nathan Lambert",
      "Yvonne Chou",
      "Arnavi Chheda",
      "Jenna Sparks",
      "Sam Skjonsberg",
      "Michael Schmitz",
      "Aaron Sarnat",
      "Byron Bischoff",
      "Pete Walsh",
      "Chris Newell",
      "Piper Wolters",
      "Tanmay Gupta",
      "Kuo-Hao Zeng",
      "Jon Borchardt",
      "Dirk Groeneveld",
      "Crystal Nam",
      "Sophie Lebrecht",
      "Caitlin Wittlif",
      "Carissa Schoenick",
      "Oscar Michel",
      "Ranjay Krishna",
      "Luca Weihs",
      "Noah A. Smith",
      "Hannaneh Hajishirzi",
      "Ross Girshick",
      "Ali Farhadi",
      "Aniruddha Kembhavi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_DriveGPT4-V2_Harnessing_Large_Language_Model_Capabilities_for_Enhanced_Closed-Loop_Autonomous_CVPR_2025_paper.html": {
    "title": "DriveGPT4-V2: Harnessing Large Language Model Capabilities for Enhanced Closed-Loop Autonomous Driving",
    "volume": "main",
    "abstract": "Multimodal large language models (MLLMs) possess the ability to comprehend visual images or videos, and show impressive reasoning ability thanks to the vast amounts of pretrained knowledge, making them highly suitable for autonomous driving applications. Unlike the previous work, DriveGPT4-V1, which focused on open-loop tasks, this study explores the capabilities of LLMs in enhancing closed-loop autonomous driving. DriveGPT4-V2 processes camera images and vehicle states as input to generate low-level control signals for end-to-end vehicle operation. A multi-view visual tokenizer (MV-VT) is employed enabling DriveGPT4-V2 to perceive the environment with an extensive range while maintaining critical details. The model architecture has been refined to improve decision prediction and inference speed. To further enhance the performance, an additional expert LLM is trained for online imitation learning. The expert LLM, sharing a similar structure with DriveGPT4-V2, can access privileged information about surrounding objects for more robust and reliable predictions. Experimental results show that DriveGPT4-V2 outperforms all baselines on the challenging CARLA Longest6 benchmark. The code and data of DriveGPT4-V2 will be publicly available",
    "checked": true,
    "id": "2e53d676a3ed157f964e76b7b5ee2a1d69e7dcc6",
    "semantic_title": "drivegpt4-v2: harnessing large language model capabilities for enhanced closed-loop autonomous driving",
    "citation_count": 2,
    "authors": [
      "Zhenhua Xu",
      "Yan Bai",
      "Yujia Zhang",
      "Zhuoling Li",
      "Fei Xia",
      "Kwan-Yee K. Wong",
      "Jianqiang Wang",
      "Hengshuang Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_High-Fidelity_Lightweight_Mesh_Reconstruction_from_Point_Clouds_CVPR_2025_paper.html": {
    "title": "High-Fidelity Lightweight Mesh Reconstruction from Point Clouds",
    "volume": "main",
    "abstract": "Recently, learning signed distance functions (SDFs) from point clouds has become popular for reconstruction. To ensure accuracy, most methods require using high-resolution Marching Cubes for surface extraction. However, this results in redundant mesh elements, making the mesh inconvenient to use. To solve the problem, we propose an adaptive meshing method to extract resolution-adaptive meshes based on surface curvature, enabling the recovery of high-fidelity lightweight meshes. Specifically, we first use point-based representation to perceive implicit surfaces and calculate surface curvature. A vertex generator is designed to produce curvature-adaptive vertices with any specified number on the implicit surface, preserving the overall structure and high-curvature features. Then we develop a Delaunay meshing algorithm to generate meshes from vertices, ensuring geometric fidelity and correct topology. In addition, to obtain accurate SDFs for adaptive meshing and achieve better lightweight reconstruction, we design a hybrid representation combining feature grid and feature tri-plane for better detail capture. Experiments demonstrate that our method can generate high-quality lightweight meshes from point clouds. Compared with methods from various categories, our approach achieves superior results, especially in capturing more details with fewer elements",
    "checked": true,
    "id": "41e0aa355cf58187b2c66ae8691f029a20c1cd33",
    "semantic_title": "high-fidelity lightweight mesh reconstruction from point clouds",
    "citation_count": 0,
    "authors": [
      "Chen Zhang",
      "Wentao Wang",
      "Ximeng Li",
      "Xinyao Liao",
      "Wanjuan Su",
      "Wenbing Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_MDP_Multidimensional_Vision_Model_Pruning_with_Latency_Constraint_CVPR_2025_paper.html": {
    "title": "MDP: Multidimensional Vision Model Pruning with Latency Constraint",
    "volume": "main",
    "abstract": "Current structural pruning methods face two significant limitations: (i) they often limit pruning to finer-grained levels like channels, making aggressive parameter reduction challenging, and (ii) they focus heavily on parameter and FLOP reduction, with existing latency-aware methods frequently relying on simplistic, suboptimal linear models that fail to generalize well to transformers, where multiple interacting dimensions impact latency. In this paper, we address both limitations by introducing Multi-Dimensional Pruning(MDP), a novel paradigm that jointly optimizes across a variety of pruning granularities--including channels, query/key, heads, embeddings, and blocks. MDP employs an advanced latency modeling technique to accurately capture latency variations across all prunable dimensions, achieving an optimal balance between latency and accuracy. By reformulating pruning as a Mixed-Integer Nonlinear Program (MINLP), MDP efficiently identifies the optimal pruned structure across all prunable dimensions while respecting latency constraints. This versatile framework supports both CNNs and transformers. Extensive experiments demonstrate that MDP significantly outperforms previous methods, especially at high pruning ratios. On ImageNet, MDP achieves a 28% speed increase with a +1.4 Top-1 accuracy improvement over prior work like HALP for ResNet50 pruning. Against the latest transformer pruning method, Isomorphic, MDP delivers an additional 37% acceleration with a +0.7 Top-1 accuracy improvement",
    "checked": true,
    "id": "e499768e171c5d9e3794927ad171c37c60f99b90",
    "semantic_title": "mdp: multidimensional vision model pruning with latency constraint",
    "citation_count": 0,
    "authors": [
      "Xinglong Sun",
      "Barath Lakshmanan",
      "Maying Shen",
      "Shiyi Lan",
      "Jingde Chen",
      "Jose M. Alvarez"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_OSDFace_One-Step_Diffusion_Model_for_Face_Restoration_CVPR_2025_paper.html": {
    "title": "OSDFace: One-Step Diffusion Model for Face Restoration",
    "volume": "main",
    "abstract": "Diffusion models have demonstrated impressive performance in face restoration. Yet, their multi-step inference process remains computationally intensive, limiting their applicability in real-world scenarios. Moreover, existing methods often struggle to generate face images that are harmonious, realistic, and consistent with the subject's identity. In this work, we propose OSDFace, a novel one-step diffusion model for face restoration. Specifically, we propose a visual representation embedder (VRE) to better capture prior information and understand the input face. In VRE, low-quality faces are processed by a visual tokenizer and subsequently embedded with a vector-quantized dictionary to generate visual prompts. Additionally, we incorporate a facial identity loss derived from face recognition to further ensure identity consistency. We further employ a generative adversarial network (GAN) as a guidance model to encourage distribution alignment between the restored face and the ground truth. Experimental results demonstrate that OSDFace surpasses current state-of-the-art (SOTA) methods in both visual quality and quantitative metrics, generating high-fidelity, natural face images with high identity consistency. The code and model will be released at https://github.com/jkwang28/OSDFace",
    "checked": true,
    "id": "e0548d4b0facfc925eceb95e57a23e9356137737",
    "semantic_title": "osdface: one-step diffusion model for face restoration",
    "citation_count": 8,
    "authors": [
      "Jingkai Wang",
      "Jue Gong",
      "Lin Zhang",
      "Zheng Chen",
      "Xing Liu",
      "Hong Gu",
      "Yutong Liu",
      "Yulun Zhang",
      "Xiaokang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gargiulo_Task_Singular_Vectors_Reducing_Task_Interference_in_Model_Merging_CVPR_2025_paper.html": {
    "title": "Task Singular Vectors: Reducing Task Interference in Model Merging",
    "volume": "main",
    "abstract": "Task Arithmetic has emerged as a simple yet effective method to merge models without additional training. However, by treating entire networks as flat parameter vectors, it overlooks key structural information and is susceptible to task interference. In this paper, we study task vectors at the layer level, focusing on task layer matrices and their singular value decomposition. In particular, we concentrate on the resulting singular vectors, which we refer to as Task Singular Vectors (TSV). Recognizing that layer task matrices are often low-rank, we propose TSV-Compress, a simple procedure that compresses them to 10% of their original size while retaining 99% of accuracy. We further leverage this low-rank space to define a new measure of task interference based on the interaction of singular vectors from different tasks. Building on these findings, we introduce TSV-Merge, a novel model merging approach that combines compression with interference reduction, significantly outperforming existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antonio Andrea Gargiulo",
      "Donato Crisostomi",
      "Maria Sofia Bucarelli",
      "Simone Scardapane",
      "Fabrizio Silvestri",
      "Emanuele Rodol√†"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Corsetti_Functionality_Understanding_and_Segmentation_in_3D_Scenes_CVPR_2025_paper.html": {
    "title": "Functionality Understanding and Segmentation in 3D Scenes",
    "volume": "main",
    "abstract": "Understanding functionalities in 3D scenes involves interpreting natural language descriptions to locate functional interactive objects, such as handles and buttons, in a 3D environment. Functionality understanding is highly challenging, as it requires both world knowledge to interpret language and spatial perception to identify fine-grained objects. For example, given a task like \"turn on the ceiling light\", an embodied AI agent must infer that it needs to locate the light switch, even though the switch is not explicitly mentioned in the task description.To date, no dedicated methods have been developed for this problem. In this paper, we introduce Fun3DU, the first approach designed for functionality understanding in 3D scenes. Fun3DU uses a language model to parse the task description through Chain-of-Thought reasoning in order to identify the object of interest. The identified object is segmented across multiple views of the captured scene by using a VLM. The segmentation results from each view are lifted in 3D and aggregated into the point cloud using geometric information. Fun3DU is training-free, relying entirely on pre-trained models. We evaluate Fun3DU on SceneFun3D, the most recent and only dataset to benchmark this task, which comprises over 3000 task descriptions on 230 scenes. Our method significantly outperforms state-of-the-art open-vocabulary 3D segmentation approaches. Project page: https://tev-fbk.github.io/fun3du",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaime Corsetti",
      "Francesco Giuliari",
      "Alice Fasoli",
      "Davide Boscaini",
      "Fabio Poiesi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guang_Dragin3D_Image_Editing_by_Dragging_in_3D_Space_CVPR_2025_paper.html": {
    "title": "Dragin3D: Image Editing by Dragging in 3D Space",
    "volume": "main",
    "abstract": "Interactive drag editing of images is a valuable task that has gained considerable attention for its precision and controllability. However, existing approaches have primarily focused on manipulating the shape or movement of objects in 2D plane. We propose to extend this drag-based editing task to 3D space. Firstly, we utilize the trajectory of two points to represent the rotational trajectory of the object. Gaussian maps of a circle and a square are centered at these two points, respectively. We use distinct shapes to ensure that symmetric views produce different object representations. Secondly, we introduce a lightweight mapping network to embed the object features into two Gaussian maps to obtain a continuous control condition that guides the model in learning the correspondence between the trajectory and the object. Finally, to overcome the limitations of current 3D object reconstruction datasets, which typically consist of object maps with transparent backgrounds, we affix random backgrounds to them. This modification helps improve the model's ability to ignore background interference when editing real images with complex backgrounds. Experiments demonstrate that our approach successfully achieves object rotation within the drag framework and demonstrates strong generalization to real-world images",
    "checked": true,
    "id": "08f91a5bd6abd59f676888d5dabbd672a1f3e7e8",
    "semantic_title": "dragin3d: image editing by dragging in 3d space",
    "citation_count": 2,
    "authors": [
      "Weiran Guang",
      "Xiaoguang Gu",
      "Mengqi Huang",
      "Zhendong Mao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MMTL-UniAD_A_Unified_Framework_for_Multimodal_and_Multi-Task_Learning_in_CVPR_2025_paper.html": {
    "title": "MMTL-UniAD: A Unified Framework for Multimodal and Multi-Task Learning in Assistive Driving Perception",
    "volume": "main",
    "abstract": "Advanced driver assistance systems require a comprehensive understanding of the driver's mental/physical state and traffic context but existing works often neglect the potential benefits of joint learning between these tasks. This paper proposes MMTL-UniAD, a unified multi-modal multi-task learning framework that simultaneously recognizes driver behavior (e.g., looking around, talking), driver emotion (e.g., anxiety, happiness), vehicle behavior (e.g., parking, turning), and traffic context (e.g., traffic jam, traffic smooth). A key challenge is avoiding negative transfer between tasks, which can impair learning performance. To address this, we introduce two key components into the framework: one is the multi-axis region attention network to extract global context-sensitive features, and the other is the dual-branch multimodal embedding to learn multimodal embeddings from both task-shared and task-specific features. The former uses a multi-attention mechanism to extract task-relevant features, mitigating negative transfer caused by task-unrelated features. The latter employs a dual-branch structure to adaptively adjust task-shared and task-specific parameters, enhancing cross-task knowledge transfer while reducing task conflicts. We assess MMTL-UniAD on the AIDE dataset, using a series of ablation studies, and show that it outperforms state-of-the-art methods across all four tasks. The code is available on https://github.com/Wenzhuo-Liu/MMTL-UniAD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenzhuo Liu",
      "Wenshuo Wang",
      "Yicheng Qiao",
      "Qiannan Guo",
      "Jiayin Zhu",
      "Pengfei Li",
      "Zilong Chen",
      "Huiming Yang",
      "Zhiwei Li",
      "Lening Wang",
      "Tiao Tan",
      "Huaping Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_T2V-CompBench_A_Comprehensive_Benchmark_for_Compositional_Text-to-video_Generation_CVPR_2025_paper.html": {
    "title": "T2V-CompBench: A Comprehensive Benchmark for Compositional Text-to-video Generation",
    "volume": "main",
    "abstract": "Text-to-video (T2V) generative models have advanced significantly, yet their ability to compose different objects, attributes, actions, and motions into a video remains unexplored. Previous text-to-video benchmarks also neglect this important ability for evaluation. In this work, we conduct the first systematic study on compositional text-to-video generation. We propose T2V-CompBench, the first benchmark tailored for compositional text-to-video generation. T2V-CompBench encompasses diverse aspects of compositionality, including consistent attribute binding, dynamic attribute binding, spatial relationships, motion binding, action binding, object interactions, and generative numeracy. We further carefully design evaluation metrics of multimodal large language model (MLLM)-based, detection-based, and tracking-based metrics, which can better reflect the compositional text-to-video generation quality of seven proposed categories with 1400 text prompts. The effectiveness of the proposed metrics is verified by correlation with human evaluations. We also benchmark various text-to-video generative models and conduct in-depth analysis across different models and various compositional categories. We find that compositional text-to-video generation is highly challenging for current models, and we hope our attempt could shed light on future research in this direction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiyue Sun",
      "Kaiyi Huang",
      "Xian Liu",
      "Yue Wu",
      "Zihan Xu",
      "Zhenguo Li",
      "Xihui Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sehgal_Self-Evolving_Visual_Concept_Library_using_Vision-Language_Critics_CVPR_2025_paper.html": {
    "title": "Self-Evolving Visual Concept Library using Vision-Language Critics",
    "volume": "main",
    "abstract": "We study the problem of building a visual concept library for visual recognition. Building effective visual concept libraries is challenging, as manual definition is labor-intensive, while relying solely on LLMs for concept generation can result in concepts that lack discriminative power or fail to account for the complex interactions between them. Our approach, ESCHER, takes a library learning perspective to iteratively discover and improve visual concepts. ESCHER uses a vision-language model (VLM) as a critic to iteratively refine the concept library, including accounting for interactions between concepts and how they affect downstream classifiers. By leveraging the in-context learning abilities of LLMs and the history of performance using various concepts, ESCHER dynamically improves its concept generation strategy based on the VLM critic's feedback. Finally, ESCHER does not require any human annotations, and is thus an automated plug-and-play framework. We empirically demonstrate the ability of ESCHER to learn a concept library for zero-shot, few-shot, and fine-tuning visual classification tasks. This work represents, to our knowledge, the first application of concept library learning to real-world visual tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Atharva Sehgal",
      "Patrick Yuan",
      "Ziniu Hu",
      "Yisong Yue",
      "Jennifer J. Sun",
      "Swarat Chaudhuri"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fini_Multimodal_Autoregressive_Pre-training_of_Large_Vision_Encoders_CVPR_2025_paper.html": {
    "title": "Multimodal Autoregressive Pre-training of Large Vision Encoders",
    "volume": "main",
    "abstract": "We introduce a novel method for pre-training of large-scale vision encoders. Building on recent advancements in autoregressive pre-training of vision models, we extend this framework to a multimodal setting, i.e., images and text. In this paper, we present AIMV2, a family of generalist vision encoders characterized by a straightforward pre-training process, scalability, and remarkable performance across a range of downstream tasks. This is achieved by pairing the vision encoder with a multimodal decoder that autoregressively generates raw image patches and text tokens. Our encoders excel not only in multimodal evaluations but also in vision benchmarks such as localization, grounding, and classification. Notably, our AIMV2-3B encoder achieves 89.5% accuracy on ImageNet-1k with a frozen trunk. Fur- thermore, AIMV2 consistently outperforms state-of-the-art contrastive models (e.g., CLIP, SigLIP) in multimodal im- age understanding across diverse settings",
    "checked": true,
    "id": "f6aeb1921d39ebe47750b9fe3e77a4c264b8fb91",
    "semantic_title": "multimodal autoregressive pre-training of large vision encoders",
    "citation_count": 43,
    "authors": [
      "Enrico Fini",
      "Mustafa Shukor",
      "Xiujun Li",
      "Philipp Dufter",
      "Michal Klein",
      "David Haldimann",
      "Sai Aitharaju",
      "Victor G. Turrisi da Costa",
      "Louis B√©thune",
      "Zhe Gan",
      "Alexander Toshev",
      "Marcin Eichner",
      "Moin Nabi",
      "Yinfei Yang",
      "Joshua Susskind",
      "Alaaeldin El-Nouby"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_AKiRa_Augmentation_Kit_on_Rays_for_Optical_Video_Generation_CVPR_2025_paper.html": {
    "title": "AKiRa: Augmentation Kit on Rays for Optical Video Generation",
    "volume": "main",
    "abstract": "Recent advances in text-conditioned video diffusion have greatly improved video quality. However, these methods offer limited or sometimes no control to users on camera aspects, including dynamic camera motion, zoom, distorted lens and focus shifts. These motion and optical aspects are crucial for adding controllability and cinematic elements to generation frameworks, ultimately resulting in visual content that draws focus, enhances mood, and guides emotions according to filmmakers' controls. In this paper, we aim to close the gap between controllable video generation and camera optics. To achieve this, we propose AKiRa (Augmentation Kit on Rays), a novel augmentation framework that builds and trains a camera adapter with a complex camera model over an existing video generation backbone. It enables fine-tuned control over camera motion as well as complex optical parameters (focal length, distortion, aperture) to achieve cinematic effects such as zoom, fisheye effect, and bokeh. Extensive experiments demonstrate AKiRa's effectiveness in combining and composing camera optics while outperforming all state-of-the-art methods. This work sets a new landmark in controlled and optically enhanced video generation, paving the way for future camera diffusion methods",
    "checked": true,
    "id": "797c7dd6bba79c65ccf7b524ed86c92485a58b65",
    "semantic_title": "akira: augmentation kit on rays for optical video generation",
    "citation_count": 7,
    "authors": [
      "Xi Wang",
      "Robin Courant",
      "Marc Christie",
      "Vicky Kalogeiton"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhong_Towards_Stable_and_Storage-efficient_Dataset_Distillation_Matching_Convexified_Trajectory_CVPR_2025_paper.html": {
    "title": "Towards Stable and Storage-efficient Dataset Distillation: Matching Convexified Trajectory",
    "volume": "main",
    "abstract": "The rapid evolution of deep learning and large language models has led to an exponential growth in the demand for training data, prompting the development of Dataset Distillation methods to address the challenges of managing large datasets. Among these, Matching Training Trajectories (MTT) has been a prominent approach, which replicates the training trajectory of an expert network on real data with a synthetic dataset. However, our investigation found that this method suffers from three significant limitations: 1. Instability of expert trajectory generated by Stochastic Gradient Descent (SGD); 2. Low convergence speed of the distillation process; 3. High storage consumption of the expert trajectory. To address these issues, we offer a new perspective on understanding the essence of Dataset Distillation and MTT through a simple transformation of the objective function, and introduce a novel method called Matching Convexified Trajectory (MCT), which aims to provide better guidance for the student trajectory. MCT creates convex combinations of expert trajectories by selecting a few expert models, guiding student networks to converge quickly and stably. This trajectory is not only easier to store, but also enables continuous sampling strategies during the distillation process, ensuring thorough learning and fitting of the entire expert trajectory. The comprehensive experiment of three public datasets verified that MCT is superior to the traditional MTT method",
    "checked": true,
    "id": "5ca96c678e27c14b4e636458c6b069b4ec2b8dc0",
    "semantic_title": "towards stable and storage-efficient dataset distillation: matching convexified trajectory",
    "citation_count": 2,
    "authors": [
      "Wenliang Zhong",
      "Haoyu Tang",
      "Qinghai Zheng",
      "Mingzhu Xu",
      "Yupeng Hu",
      "Weili Guan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Radman_TSAM_Temporal_SAM_Augmented_with_Multimodal_Prompts_for_Referring_Audio-Visual_CVPR_2025_paper.html": {
    "title": "TSAM: Temporal SAM Augmented with Multimodal Prompts for Referring Audio-Visual Segmentation",
    "volume": "main",
    "abstract": "Referring audio-visual segmentation (Ref-AVS) aims to segment objects within audio-visual scenes using multimodal cues embedded in text expressions. While the Segment Anything Model (SAM) has revolutionized visual segmentation, its applicability to Ref-AVS, where multimodal cues act as novel prompts, remains unexplored. SAM's limitation to single-frame segmentation also hinders its ability to capture essential temporal context needed for multi-frame audio-visual segmentation. To address this gap, we propose TSAM, a novel extension of SAM designed to leverage multimodal cues for precise segmentation in dynamic audio-visual scenes. TSAM enhances SAM's image encoder with a temporal modeling branch, enabling spatio-temporal learning and deep multimodal fusion across video frames, while retaining SAM's pre-trained knowledge. Additionally, TSAM replaces SAM's user-interactive prompting mechanism with sparse and dense data-driven prompts, enabling more effective integration of audio-visual inputs and reference text expressions. Extensive experiments on the Ref-AVS dataset demonstrate TSAM's superiority over state-of-the-art methods. The results illustrate its effectiveness in segmenting objects in dynamic audio-visual scenes using text-based multimodal cues and its strong generalization to unseen objects",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abduljalil Radman",
      "Jorma Laaksonen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_TFCustom_Customized_Image_Generation_with_Time-Aware_Frequency_Feature_Guidance_CVPR_2025_paper.html": {
    "title": "TFCustom: Customized Image Generation with Time-Aware Frequency Feature Guidance",
    "volume": "main",
    "abstract": "Subject-driven image personalization has seen notable advancements, especially with the advent of the ReferenceNet paradigm. ReferenceNet excels in integrating image reference features, making it highly applicable in creative and commercial settings. However, current implementations of ReferenceNet primarily operate as latent-level feature extractors, which limit their potential. This constraint hinders the provision of appropriate features to the denoising backbone across different timesteps, leading to suboptimal image consistency. In this paper, we revisit the extraction of reference features and propose TFCustom, a model framework designed to focus on reference image features at different temporal steps and frequency levels. Specifically, we firstly propose synchronized ReferenceNet to extract reference image features while simultaneously optimizing noise injection and denoising for the reference image. We also propose a time-aware frequency feature refinement module that leverages high- and low-frequency filters, combined with time embeddings, to adaptively select the degree of reference feature injection. Additionally, to enhance the similarity between reference objects and the generated image, we introduce a novel reward-based loss that encourages greater alignment between the reference and generated images. Experimental results demonstrate state-of-the-art performance in both multi-object and single-object reference generation, with significant improvements in texture and textual detail generation over existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mushui Liu",
      "Dong She",
      "Jingxuan Pang",
      "Qihan Huang",
      "Jiacheng Ying",
      "Wanggui He",
      "Yuanlei Hou",
      "Siming Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Boosting_Point-Supervised_Temporal_Action_Localization_through_Integrating_Query_Reformation_and_CVPR_2025_paper.html": {
    "title": "Boosting Point-Supervised Temporal Action Localization through Integrating Query Reformation and Optimal Transport",
    "volume": "main",
    "abstract": "Point-supervised Temporal Action Localization poses significant challenges due to the difficulty of identifying complete actions with a single-point annotation per action. Existing methods typically employ Multiple Instance Learning, which struggles to capture global temporal context and requires heuristic post-processing. In research on fully-supervised tasks, DETR-based structures have effectively addressed these limitations. However, it is nontrivial to merely adapt DETR to this task, encountering two major bottlenecks. (1) How to integrate point label information into the model and (2) How to select optimal decoder proposals for training in the absence of complete action segment annotations. To address this issue, we introduce an end-to-end framework by integrating Query Reformation and Optimal Transport (QROT). Specifically, we encode point labels through a set of semantic consensus queries, enabling effective focus on action-relevant snippets. Furthermore, we integrate an optimal transport mechanism to generate high-quality pseudo labels. These pseudo-labels facilitate precise proposals selection based on Hungarian algorithm, significantly enhancing localization accuracy in point-supervised settings. Extensive experiments on the THUMOS14 and ActivityNet-v1.3 datasets demonstrate that our method outperforms existing MIL-based approaches, offering more stable and accurate temporal action localization in point-level supervision",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengnan Liu",
      "Le Wang",
      "Sanping Zhou",
      "Kun Xia",
      "Xiaolong Sun",
      "Gang Hua"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Koley_SketchFusion_Learning_Universal_Sketch_Features_through_Fusing_Foundation_Models_CVPR_2025_paper.html": {
    "title": "SketchFusion: Learning Universal Sketch Features through Fusing Foundation Models",
    "volume": "main",
    "abstract": "While foundation models have revolutionised computer vision, their effectiveness for sketch understanding remains limited by the unique challenges of abstract, sparse visual inputs. Through systematic analysis, we uncover two fundamental limitations: Stable Diffusion (SD) struggles to extract meaningful features from abstract sketches (unlike its success with photos), and exhibits a pronounced frequency-domain bias that suppresses essential low-frequency components needed for sketch understanding. Rather than costly retraining, we address these limitations by strategically combining SD with CLIP, whose strong semantic understanding naturally compensates for SD's spatial-frequency biases. By dynamically injecting CLIP features into SD's denoising process and adaptively aggregating features across semantic levels, our method achieves state-of-the-art performance in sketch retrieval (+3.35%), recognition (+1.06%), segmentation (+29.42%), and correspondence learning (+21.22%), demonstrating the first truly universal sketch feature representation in the era of foundation models",
    "checked": true,
    "id": "bcac8bfc09d088bde8ad37f631fc2270ef26a396",
    "semantic_title": "sketchfusion: learning universal sketch features through fusing foundation models",
    "citation_count": 0,
    "authors": [
      "Subhadeep Koley",
      "Tapas Kumar Dutta",
      "Aneeshan Sain",
      "Pinaki Nath Chowdhury",
      "Ayan Kumar Bhunia",
      "Yi-Zhe Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Bridging_the_Vision-Brain_Gap_with_an_Uncertainty-Aware_Blur_Prior_CVPR_2025_paper.html": {
    "title": "Bridging the Vision-Brain Gap with an Uncertainty-Aware Blur Prior",
    "volume": "main",
    "abstract": "Can our brain signals faithfully reflect the original visual stimuli, even including high-frequency details? Although human perceptual and cognitive capacities enable us to process and remember visual information, these abilities are constrained by several factors, such as limited attentional resources and finite capacity of visual memory. When visual stimuli are processed by human visual system into brain signals, some information is inevitably lost, leading to a discrepancy known as the System GAP. Additionally, perceptual and cognitive dynamics, along with technical noise in signal acquisition, degrade the fidelity of brain signals relative to the visual stimuli, known as the Random GAP. When encoded brain representations are directly aligned with the corresponding pretrained image features, the System GAP and Random GAP between paired data challenge the model, requiring it to bridge these gaps. However, due to limited paired data, these gaps are difficult for the model to learn, leading to overfitting and poor generalization to new data. To address these GAPs, we propose a simple yet effective approach called the Uncertainty-aware Blur Prior (UBP). It estimates the uncertainty within the paired data, reflecting the mismatch between brain signals and visual stimuli. Based on uncertainty, UBP dynamically blurs the high-frequency details of the original images, reducing the impact of mismatch and improving alignment. Our method achieves a top-1 accuracy of 50.9% and a top-5 accuracy of 79.7% on the zero-shot brain-to-image retrieval task, surpassing previous state-of-the-art methods. Code is available at https://github.com/HaitaoWuTJU/Uncertainty-aware-Blur-Prior",
    "checked": true,
    "id": "fe4a99fdfb8c7d2d26fda5d5b19b9b8e949928f9",
    "semantic_title": "bridging the vision-brain gap with an uncertainty-aware blur prior",
    "citation_count": 2,
    "authors": [
      "Haitao Wu",
      "Qing Li",
      "Changqing Zhang",
      "Zhen He",
      "Xiaomin Ying"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Invisible_Backdoor_Attack_against_Self-supervised_Learning_CVPR_2025_paper.html": {
    "title": "Invisible Backdoor Attack against Self-supervised Learning",
    "volume": "main",
    "abstract": "Self-supervised learning (SSL) models are vulnerable to backdoor attacks. Existing backdoor attacks that are effective in SSL often involve noticeable triggers, like colored patches or visible noise, which are vulnerable to human inspection. This paper proposes an imperceptible and effective backdoor attack against self-supervised models. We first find that existing imperceptible triggers designed for supervised learning are less effective in compromising self-supervised models. We then identify this ineffectiveness is attributed to the overlap in distributions between the backdoor and augmented samples used in SSL. Building on this insight, we design an attack using optimized triggers disentangled with the augmented transformation in the SSL, while remaining imperceptible to human vision. Experiments on five datasets and six SSL algorithms demonstrate our attack is highly effective and stealthy. It also has strong resistance to existing backdoor defenses. Our code can be found at https://github.com/Zhang-Henry/INACTIVE",
    "checked": true,
    "id": "cd6f250135a7db133cb6647fe0327d0b400e260e",
    "semantic_title": "invisible backdoor attack against self-supervised learning",
    "citation_count": 3,
    "authors": [
      "Hanrong Zhang",
      "Zhenting Wang",
      "Boheng Li",
      "Fulin Lin",
      "Tingxu Han",
      "Mingyu Jin",
      "Chenlu Zhan",
      "Mengnan Du",
      "Hongwei Wang",
      "Shiqing Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chae-Yeon_Perceptually_Accurate_3D_Talking_Head_Generation_New_Definitions_Speech-Mesh_Representation_CVPR_2025_paper.html": {
    "title": "Perceptually Accurate 3D Talking Head Generation: New Definitions, Speech-Mesh Representation, and Evaluation Metrics",
    "volume": "main",
    "abstract": "Recent advancements in speech-driven 3D talking head generation have made significant progress in lip synchronization. However, existing models still struggle to capture the perceptual alignment between varying speech characteristics and corresponding lip movements. In this work, we claim that three criteria--Temporal Synchronization, Lip Readability, and Expressiveness--are crucial for achieving perceptually accurate lip movements. Motivated by our hypothesis that a desirable representation space exists to meet these three criteria, we introduce a speech-mesh synchronized representation that captures intricate correspondences between speech signals and 3D face meshes. We found that our learned representation exhibits desirable characteristics, and we plug it into existing models as a perceptual loss to better align lip movements to the given speech. In addition, we utilize this representation as a perceptual metric and introduce two other physically grounded lip synchronization metrics to assess how well the generated 3D talking heads align with these three criteria. Experiments show that training 3D talking head generation models with our perceptual loss significantly improve all three aspects of perceptually accurate lip synchronization. Codes and datasets are available at https://perceptual-3d-talking-head.github.io/",
    "checked": true,
    "id": "79ac360f4014001847a687d4c424a13727e93628",
    "semantic_title": "perceptually accurate 3d talking head generation: new definitions, speech-mesh representation, and evaluation metrics",
    "citation_count": 3,
    "authors": [
      "Lee Chae-Yeon",
      "Oh Hyun-Bin",
      "Han EunGi",
      "Kim Sung-Bin",
      "Suekyeong Nam",
      "Tae-Hyun Oh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_BWFormer_Building_Wireframe_Reconstruction_from_Airborne_LiDAR_Point_Cloud_with_CVPR_2025_paper.html": {
    "title": "BWFormer: Building Wireframe Reconstruction from Airborne LiDAR Point Cloud with Transformer",
    "volume": "main",
    "abstract": "In this paper, we present BWFormer, a novel Transformer-based model for building wireframe reconstruction from airborne LiDAR point cloud. The problem is solved in a ground-up manner here by detecting the building corners in 2D, lifting and connecting them in 3D space afterwards with additional data augmentation.Due to the 2.5D characteristic of the airborne LiDAR point cloud, we simplify the problem by projecting the points on the ground plane to produce a 2D height map. With the height map, a heat map is first generated with pixel-wise corner likelihood to predict the possible 2D corners.Then, 3D corners are predicted by a Transformer-based network with extra height embedding initialization. This 2D-to-3D corner detection strategy reduces the search space significantly. To recover the topological connections among the corners, edges are finally predicted from the height map with the proposed edge attention mechanism, which extracts holistic features and preserves local details simultaneously. In addition, due to the limited datasets in the field and the irregularity of the point clouds, a conditional latent diffusion model for LiDAR scanning simulation is utilized for data augmentation. BWFormer surpasses other state-of-the-art methods, especially in reconstruction completeness. Our code is available at: https://github.com/3dv-casia/BWformer/",
    "checked": true,
    "id": "305c6147d3ae0b4dad4f7d4740d2a03c6a17d6ac",
    "semantic_title": "bwformer: building wireframe reconstruction from airborne lidar point cloud with transformer",
    "citation_count": 0,
    "authors": [
      "Yuzhou Liu",
      "Lingjie Zhu",
      "Hanqiao Ye",
      "Shangfeng Huang",
      "Xiang Gao",
      "Xianwei Zheng",
      "Shuhan Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Diffusion-4K_Ultra-High-Resolution_Image_Synthesis_with_Latent_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent Diffusion Models",
    "volume": "main",
    "abstract": "In this paper, we present Diffusion-4K, a novel framework for direct ultra-high-resolution image synthesis using text-to-image diffusion models. The core advancements include: (1) Aesthetic-4K Benchmark: addressing the absence of a publicly available 4K image synthesis dataset, we construct Aesthetic-4K, a comprehensive benchmark for ultra-high-resolution image generation. We curated a high-quality 4K dataset with carefully selected images and captions generated by GPT-4o. Additionally, we introduce GLCM Score and compression ratio metrics to evaluate fine details, combined with holistic measures such as FID, Aesthetics and CLIPScore for a comprehensive assessment of ultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose a wavelet-based fine-tuning approach for direct training with photorealistic 4K images, applicable to various latent diffusion models, demonstrating its effectiveness in synthesizing highly detailed 4K images. Consequently, Diffusion-4K achieves impressive performance in high-quality image synthesis and text prompt adherence, especially when powered by modern large-scale diffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental results from our benchmark demonstrate the superiority of Diffusion-4K in ultra-high-resolution image synthesis. Code is available at https://github.com/zhang0jhon/diffusion-4k",
    "checked": true,
    "id": "bf232b405cad758317ddf0114f4822c5cec23efb",
    "semantic_title": "diffusion-4k: ultra-high-resolution image synthesis with latent diffusion models",
    "citation_count": 8,
    "authors": [
      "Jinjin Zhang",
      "Qiuyu Huang",
      "Junjie Liu",
      "Xiefan Guo",
      "Di Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_AffordDP_Generalizable_Diffusion_Policy_with_Transferable_Affordance_CVPR_2025_paper.html": {
    "title": "AffordDP: Generalizable Diffusion Policy with Transferable Affordance",
    "volume": "main",
    "abstract": "Diffusion-based policies have shown impressive performance in robotic manipulation tasks while struggling with out-of-domain distributions. Recent efforts attempted to enhance generalization by improving the visual feature encoding for diffusion policy. However, their generalization is typically limited to the same category with similar appearances. Our key insight is that leveraging affordances--manipulation priors that define \"where\" and \"how\" an agent interacts with an object--can substantially enhance generalization to entirely unseen object instances and categories. We introduce the Diffusion Policy with transferable Affordance (AffordDP), designed for generalizable manipulation across novel categories. AffordDP models affordances through 3D contact points and post-contact trajectories, capturing the essential static and dynamic information for complex tasks. The transferable affordance from in-domain data to unseen objects is achieved by estimating a 6D transformation matrix using foundational vision models and point cloud registration techniques. More importantly, we incorporate affordance guidance during diffusion sampling that can refine action sequence generation. This guidance directs the generated action to gradually move towards the desired manipulation for unseen objects while keeping the generated action within the manifold of action space. Experimental results from both simulated and real-world environments demonstrate that AffordDP consistently outperforms previous diffusion-based methods, successfully generalizing to unseen instances and categories where others fail",
    "checked": true,
    "id": "d1d94cc5fc86a52e4b1afd3e4038ba3cdbccf57e",
    "semantic_title": "afforddp: generalizable diffusion policy with transferable affordance",
    "citation_count": 10,
    "authors": [
      "Shijie Wu",
      "Yihang Zhu",
      "Yunao Huang",
      "Kaizhen Zhu",
      "Jiayuan Gu",
      "Jingyi Yu",
      "Ye Shi",
      "Jingya Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kumbong_HMAR_Efficient_Hierarchical_Masked_Auto-Regressive_Image_Generation_CVPR_2025_paper.html": {
    "title": "HMAR: Efficient Hierarchical Masked Auto-Regressive Image Generation",
    "volume": "main",
    "abstract": "Visual AutoRegressive modeling (VAR) shows promise in bridging the speed and quality gap between autoregressive image models and diffusion models. VAR reformulates autoregressive modeling by decomposing an image into successive resolution scales. During inference, an image is generated by predicting all the tokens in the next (higher-resolution) scale, conditioned on all tokens in all previous (lower-resolutions) scales. However, this formulation suffers from reduced image quality due to parallel generation of all tokens in a resolution scale; has sequence lengths scaling superlinearly in image resolution; and requires retraining to change the resolution sampling schedule.We introduce \\underline H ierarchical \\underline M asked \\underline A uto\\underline R egressive modeling (HMAR), a new image generation algorithm that alleviates these issues using next-scale prediction and masked prediction to generate high-quality images with fast sampling. HMAR reformulates next-scale prediction as a Markovian process, wherein prediction of each resolution scale is conditioned only on tokens in its immediate predecessor instead of the tokens in all predecessor resolutions. When predicting a resolution scale, HMAR uses a controllable multi-step masked generation procedure to generate a subset of the tokens in each step. On ImageNet 256 x 256 and 512 x 512 benchmarks, HMAR models match or outperform parameter-matched VAR, diffusion, and autoregressive baselines. We develop efficient IO-aware block-sparse attention kernels that allow HMAR to achieve faster training and inference times over VAR by over 2.5xand 1.75 xrespectively, as well as over 3 xlower inference memory footprint. Finally, the Markovian formulation of HMAR yields additional flexibility over VAR; we show that its sampling schedule can be changed without further training, and it can be applied to image editing tasks in a zero-shot manner",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hermann Kumbong",
      "Xian Liu",
      "Tsung-Yi Lin",
      "Ming-Yu Liu",
      "Xihui Liu",
      "Ziwei Liu",
      "Daniel Y. Fu",
      "Christopher Re",
      "David W. Romero"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_OmniDrive_A_Holistic_Vision-Language_Dataset_for_Autonomous_Driving_with_Counterfactual_CVPR_2025_paper.html": {
    "title": "OmniDrive: A Holistic Vision-Language Dataset for Autonomous Driving with Counterfactual Reasoning",
    "volume": "main",
    "abstract": "The advances in vision-language models (VLMs) have led to a growing interest in autonomous driving to leverage their strong reasoning capabilities. However, extending these capabilities from 2D to full 3D understanding is crucial for real-world applications. To address this challenge, we propose OmniDrive, a holistic vision-language dataset that aligns agent models with 3D driving tasks through counterfactual reasoning. This approach enhances decision-making by evaluating potential scenarios and their outcomes, similar to human drivers considering alternative actions. Our counterfactual-based synthetic data annotation process generates large-scale, high-quality datasets, providing denser supervision signals that bridge planning trajectories and language-based reasoning. Futher, we explore two advanced OmniDrive-Agent frameworks, namely Omni-L and Omni-Q, to assess the importance of vision-language alignment versus 3D perception, revealing critical insights into designing effective LLM-agents. Significant improvements on the DriveLM Q&A benchmark and nuScenes open-loop planning demonstrate the effectiveness of our dataset and methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shihao Wang",
      "Zhiding Yu",
      "Xiaohui Jiang",
      "Shiyi Lan",
      "Min Shi",
      "Nadine Chang",
      "Jan Kautz",
      "Ying Li",
      "Jose M. Alvarez"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cui_DKC_Differentiated_Knowledge_Consolidation_for_Cloth-Hybrid_Lifelong_Person_Re-identification_CVPR_2025_paper.html": {
    "title": "DKC: Differentiated Knowledge Consolidation for Cloth-Hybrid Lifelong Person Re-identification",
    "volume": "main",
    "abstract": "Lifelong person re-identification (LReID) aims to match the same person using sequentially collected data. However, due to the long-term nature of lifelong learning, the inevitable changes in human clothes prevent the model from relying on unified discriminative information (e.g., clothing style) to match the same person in the streaming data, demanding differentiated cloth-irrelevant information. Unfortunately, existing LReID methods typically fail to leverage such knowledge resulting in the exacerbation of catastrophic forgetting issues. Therefore, in this paper, we focus on a challenging practical task called Cloth-Hybrid Lifelong Person Re-identification (CH-LReID), which requires matching the same person wearing different clothes using sequentially collected data. A Differentiated Knowledge Consolidation (DKC) framework is designed to unify and balance distinct knowledge across streaming data. The core idea is to adaptively balance differentiated knowledge and compatibly consolidate cloth-relevant and cloth-irrelevant information. To this end, a Differentiated Knowledge Transfer (DKT) module and a Latent Knowledge Consolidation (LKC) module are designed to adaptively discover differentiated new knowledge, while eliminating the derived domain shift of old knowledge via reconstructing the old latent feature space, respectively. Then, to further alleviate the catastrophic conflict between differentiated new and old knowledge, we further propose a Dual-level Distribution Alignment (DDA) module to align the distribution of discriminative knowledge at both the instance level and the fine-grained level. Extensive experiments on multiple benchmarks demonstrate the superiority of our method against existing methods in both CH-LReID and traditional LReID tasks. The source code of this paper is available at https://github.com/PKU-ICST-MIPL/DKC-CVPR2025",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Cui",
      "Jiahuan Zhou",
      "Yuxin Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Salar_Enhancing_Facial_Privacy_Protection_via_Weakening_Diffusion_Purification_CVPR_2025_paper.html": {
    "title": "Enhancing Facial Privacy Protection via Weakening Diffusion Purification",
    "volume": "main",
    "abstract": "The rapid growth of social media has led to the widespread sharing of individual portrait images, which pose serious privacy risks due to the capabilities of automatic face recognition (AFR) systems for mass surveillance. Hence, protecting facial privacy against unauthorized AFR systems is essential. Inspired by the generation capability of the emerging diffusion models, recent methods employ diffusion models to generate adversarial face images for privacy protection. However, they suffer from the diffusion purification effect, leading to a low protection success rate (PSR). In this paper, we first propose learning unconditional embeddings to increase the learning capacity for adversarial modifications and then use them to guide the modification of the adversarial latent code to weaken the diffusion purification effect. Moreover, we integrate an identity-preserving structure to maintain structural consistency between the original and generated images, allowing human observers to recognize the generated image as having the same identity as the original. Extensive experiments conducted on two public datasets, i.e., CelebA-HQ and LADN, demonstrate the superiority of our approach. The protected faces generated by our method outperform those produced by existing facial privacy protection approaches in terms of transferability and natural appearance. The code is available at https://github.com/parham1998/Facial-Privacy-Protection",
    "checked": true,
    "id": "4e8de21560ed183d351150555c03cab12280d1c0",
    "semantic_title": "enhancing facial privacy protection via weakening diffusion purification",
    "citation_count": 3,
    "authors": [
      "Ali Salar",
      "Qing Liu",
      "Yingli Tian",
      "Guoying Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_ORIDa_Object-centric_Real-world_Image_Composition_Dataset_CVPR_2025_paper.html": {
    "title": "ORIDa: Object-centric Real-world Image Composition Dataset",
    "volume": "main",
    "abstract": "Object compositing, the task of placing and harmonizing objects in images of diverse visual scenes, has become an important task in computer vision with the rise of generative models.However, existing datasets lack the diversity and scale required to comprehensively explore real-world scenarios comprehensively. We introduce ORIDa (Object-centric Real-world Image Composition Dataset), a large-scale, real-captured dataset containing over 30,000 images featuring 200 unique objects, each of which is presented across varied positions and scenes. ORIDa has two types of data: factual-counterfactual sets and factual-only scenes. The factual-counterfactual sets consist of four factual images showing an object in different positions within a scene and a single counterfactual (or background) image of the scene without the object, resulting in five images per scene. The factual-only scenes include a single image containing an object in a specific context, expanding the variety of environments. To our knowledge, ORIDa is the first publicly available dataset with its scale and complexity for real-world image composition. Extensive analysis and experiments highlight the value of ORIDa as a resource for advancing further research in object compositing",
    "checked": true,
    "id": "02e57ba127c626f46d626a9f963727e9ad4768a8",
    "semantic_title": "orida: object-centric real-world image composition dataset",
    "citation_count": 1,
    "authors": [
      "Jinwoo Kim",
      "Sangmin Han",
      "Jinho Jeong",
      "Jiwoo Choi",
      "Dongyeoung Kim",
      "Seon Joo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MeGA_Hybrid_Mesh-Gaussian_Head_Avatar_for_High-Fidelity_Rendering_and_Head_CVPR_2025_paper.html": {
    "title": "MeGA: Hybrid Mesh-Gaussian Head Avatar for High-Fidelity Rendering and Head Editing",
    "volume": "main",
    "abstract": "Creating high-fidelity head avatars from multi-view videos is essential for many AR/VR applications. However, current methods often struggle to achieve high-quality renderings across all head components (e.g., skin vs. hair) due to the limitations of using one single representation for elements with varying characteristics. In this paper, we introduce a Hybrid Mesh-Gaussian Head Avatar (MeGA) that models different head components with more suitable representations. Specifically, we employ an enhanced FLAME mesh for the facial representation and predict a UV displacement map to provide per-vertex offsets for improved personalized geometric details. To achieve photorealistic rendering, we use deferred neural rendering to obtain facial colors and decompose neural textures into three meaningful parts. For hair modeling, we first build a static canonical hair using 3D Gaussian Splatting. A rigid transformation and an MLP-based deformation field are further applied to handle complex dynamic expressions. Combined with our occlusion-aware blending, MeGA generates higher-fidelity renderings for the whole head and naturally supports diverse downstream tasks. Experiments on the NeRSemble dataset validate the effectiveness of our designs, outperforming previous state-of-the-art methods and enabling versatile editing capabilities, including hairstyle alteration and texture editing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Wang",
      "Di Kang",
      "Heyi Sun",
      "Shenhan Qian",
      "Zixuan Wang",
      "Linchao Bao",
      "Song-Hai Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dombrowski_Image_Generation_Diversity_Issues_and_How_to_Tame_Them_CVPR_2025_paper.html": {
    "title": "Image Generation Diversity Issues and How to Tame Them",
    "volume": "main",
    "abstract": "Generative methods have reached a level of quality that is almost indistinguishable from real data. However, while individual samples may appear unique, generative models often exhibit limitations in covering the full data distribution. Unlike quality issues, diversity problems within generative models are not easily detected by simply observing single images or generated datasets, which means we need a specific measure to assess the diversity of these models. In this paper, we draw attention to the current lack of diversity in generative models and the inability of common metrics to measure this. We achieve this by framing diversity as an image retrieval problem, where we measure how many real images can be retrieved using synthetic data as queries. This yields the Image Retrieval Score (IRS), an interpretable, hyperparameter-free metric that quantifies the diversity of a generative model's output. IRS requires only a subset of synthetic samples and provides a statistical measure of confidence. Our experiments indicate that current feature extractors commonly used in generative model assessment are inadequate for evaluating diversity effectively. Consequently, we perform an extensive search for the best feature extractors to assess diversity. Evaluation reveals that current diffusion models converge to limited subsets of the real distribution, with no current state-of-the-art models superpassing 77% of the diversity of the training data. To address this limitation, we introduce Diversity-Aware Diffusion Models (DiADM), a novel approach that improves diversity of unconditional diffusion models without loss of image quality. We do this by disentangling diversity from image quality by using a diversity aware module that uses pseudo-unconditional features as input. We provide a Python package offering unified feature extraction and metric computation to further facilitate the evaluation of generative models https://github.com/MischaD/beyondfid",
    "checked": true,
    "id": "361b8ae37d90f110fcc744ba842e7e645c5f8fed",
    "semantic_title": "image generation diversity issues and how to tame them",
    "citation_count": 3,
    "authors": [
      "Mischa Dombrowski",
      "Weitong Zhang",
      "Sarah Cechnicka",
      "Hadrien Reynaud",
      "Bernhard Kainz"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kumari_Annotation_Ambiguity_Aware_Semi-Supervised_Medical_Image_Segmentation_CVPR_2025_paper.html": {
    "title": "Annotation Ambiguity Aware Semi-Supervised Medical Image Segmentation",
    "volume": "main",
    "abstract": "Despite the remarkable progress of deep learning-based methods in medical image segmentation, their use in clinical practice remains limited for two main reasons. First, obtaining a large medical dataset with precise annotations to train segmentation models is challenging. Secondly, most current segmentation techniques generate a single deterministic segmentation mask for each image. However, in real-world scenarios, there is often significant uncertainty regarding what defines the \"correct\" segmentation, and various expert annotators might provide different segmentations for the same image. To tackle both of these problems, we propose Annotation Ambiguity Aware Semi-Supervised Medical Image Segmentation (AmbiSSL). AmbiSSL combines a small amount of multi-annotator labeled data and a large set of unlabeled data to generate diverse and plausible segmentation maps. Our method consists of three key components: (1) The Diverse Pseudo-Label Generation (DPG) module utilizes multiple decoders, created by performing randomized pruning on the original backbone decoder. These pruned decoders enable the generation of a diverse pseudo-label set; (2) a Semi-Supervised Latent Distribution Learning (SSLDL) module constructs a common latent space by utilizing both ground truth annotations and pseudo-label set; and (3) a Cross-Decoder Supervision (CDS) module, which enables pruned decoders to guide each other's learning. We evaluated the proposed method on two publicly available datasets. Extensive experiments demonstrate that AmbiSSL can generate diverse segmentation maps using only a small amount of labeled data and abundant unlabeled data, offering a more practical solution for medical image segmentation by reducing reliance on large labeled datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suruchi Kumari",
      "Pravendra Singh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Effective_Cloud_Removal_for_Remote_Sensing_Images_by_an_Improved_CVPR_2025_paper.html": {
    "title": "Effective Cloud Removal for Remote Sensing Images by an Improved Mean-Reverting Denoising Model with Elucidated Design Space",
    "volume": "main",
    "abstract": "Cloud removal (CR) remains a challenging task in remote sensing image processing. Although diffusion models (DM) exhibit strong generative capabilities, their direct applications to CR are suboptimal, as they generate cloudless images from random noise, ignoring inherent information in cloudy inputs. To overcome this drawback, we develop a new CR model EMRDM based on mean-reverting diffusion models (MRDMs) to establish a direct diffusion process between cloudy and cloudless images. Compared to current MRDMs, EMRDM offers a modular framework with updatable modules and an elucidated design space, based on a reformulated forward process and a new ordinary differential equation (ODE)-based backward process. Leveraging our framework, we redesign key MRDM modules to boost CR performance, including restructuring the denoiser via a preconditioning technique, reorganizing the training process, and improving the sampling process by introducing deterministic and stochastic samplers. To achieve multi-temporal CR, we further develop a denoising network for simultaneously denoising sequential images. Experiments on mono-temporal and multi-temporal datasets demonstrate the superior performance of EMRDM. Our code is available at https://github.com/Ly403/EMRDM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Liu",
      "Wengen Li",
      "Jihong Guan",
      "Shuigeng Zhou",
      "Yichao Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Taubner_CAP4D_Creating_Animatable_4D_Portrait_Avatars_with_Morphable_Multi-View_Diffusion_CVPR_2025_paper.html": {
    "title": "CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models",
    "volume": "main",
    "abstract": "Reconstructing photorealistic and dynamic portrait avatars from images is essential to many applications including advertising, visual effects, and virtual reality. Depending on the application, avatar reconstruction involves different capture setups and constraints -- for example, visual effects studios use camera arrays to capture hundreds of reference images, while content creators may seek to animate a single portrait image downloaded from the internet. As such, there is a large and heterogeneous ecosystem of methods for avatar reconstruction. Techniques based on multi-view stereo or neural rendering achieve the highest quality results, but require hundreds of reference images. Recent generative models produce convincing avatars from a single reference image, but visual fidelity lags behind multi-view techniques. Here, we present CAP4D: an approach that uses a morphable multi-view diffusion model to reconstruct photoreal 4D (dynamic 3D) portrait avatars from any number of reference images (i.e., one to 100) and animate and render them in real time. Our approach demonstrates state-of-the-art performance for single-, few-, and multi-image 4D portrait avatar reconstruction, and takes steps to bridge the gap in visual fidelity between single-image and multi-view reconstruction techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Taubner",
      "Ruihang Zhang",
      "Mathieu Tuli",
      "David B. Lindell"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hong_Comprehensive_Information_Bottleneck_for_Unveiling_Universal_Attribution_to_Interpret_Vision_CVPR_2025_paper.html": {
    "title": "Comprehensive Information Bottleneck for Unveiling Universal Attribution to Interpret Vision Transformers",
    "volume": "main",
    "abstract": "The feature attribution method reveals the contribution of input variables to the decision-making process to provide an attribution map for explanation. Existing methods grounded on the information bottleneck principle compute information in a specific layer to obtain attributions, compressing the features by injecting noise via a parametric damping ratio. However, the attribution obtained in a specific layer neglects evidence of the decision-making process distributed across layers. In this paper, we introduce a comprehensive information bottleneck (CoIBA), which discovers the relevant information in each targeted layer to explain the decision-making process. Our core idea is applying information bottleneck in multiple targeted layers to estimate the comprehensive information by sharing a parametric damping ratio across the layers. Leveraging this shared ratio complements the over-compressed information to discover the omitted clues of the decision by sharing the relevant information across the targeted layers. We suggest the variational approach to fairly reflect the relevant information of each layer by upper bounding layer-wise information. Therefore, CoIBA guarantees that the discarded activation is unnecessary in every targeted layer to make a decision. The extensive experimental results demonstrate the enhancement in faithfulness of the feature attributions provided by CoIBA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jung-Ho Hong",
      "Ho-Joong Kim",
      "Kyu-Sung Jeon",
      "Seong-Whan Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_OpticalNet_An_Optical_Imaging_Dataset_and_Benchmark_Beyond_the_Diffraction_CVPR_2025_paper.html": {
    "title": "OpticalNet: An Optical Imaging Dataset and Benchmark Beyond the Diffraction Limit",
    "volume": "main",
    "abstract": "Optical imaging capable of resolving nanoscale features would revolutionize scientific research and engineering applications across biomedicine, smart manufacturing, and semiconductor quality control. However, due to the physical phenomenon of diffraction, the optical resolution is limited to approximately half the wavelength of light, which impedes the observation of subwavelength objects such as the native state coronavirus, typically smaller than 200 nm. Fortunately, deep learning methods have shown remarkable potential in uncovering underlying patterns within data, promising to overcome the diffraction limit by revealing the mapping pattern between diffraction images and their corresponding ground truth object images. However, the absence of suitable datasets has hindered progress in this field--collecting high-quality optical data of subwavelength objects is highly difficult as these objects are inherently invisible under conventional microscopy, making it impossible to perform standard visual calibration and drift correction. Therefore, we provide the first general optical imaging dataset based on the \"building block\" concept for challenging the diffraction limit. Drawing an analogy to modular construction principles, we construct a comprehensive optical imaging dataset comprising subwavelength fundamental elements, i.e., small square units that can be assembled into larger and more complex objects. We then frame the task as an image-to-image translation task and evaluate various vision methods. Experimental results validate our \"building block\" concept, demonstrating that models trained on basic square units can effectively generalize to realistic, more complex unseen objects. Most importantly, by highlighting this underexplored AI-for-science area and its potential, we aspire to advance optical science by fostering collaboration with the vision and machine learning communities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benquan Wang",
      "Ruyi An",
      "Jin-Kyu So",
      "Sergei Kurdiumov",
      "Eng Aik Chan",
      "Giorgio Adamo",
      "Yuhan Peng",
      "Yewen Li",
      "Bo An"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Dataset_Distillation_with_Neural_Characteristic_Function_A_Minmax_Perspective_CVPR_2025_paper.html": {
    "title": "Dataset Distillation with Neural Characteristic Function: A Minmax Perspective",
    "volume": "main",
    "abstract": "Dataset distillation has emerged as a powerful approach for reducing data requirements in deep learning. Among various methods, distribution matching-based approaches stand out for their balance of computational efficiency and strong performance. However, existing distance metrics used in distribution matching often fail to accurately capture distributional differences, leading to unreliable measures of discrepancy. In this paper, we reformulate dataset distillation as a minmax optimization problem and introduce Neural Characteristic Function Discrepancy (NCFD), a comprehensive and theoretically grounded metric for measuring distributional differences. NCFD leverages the Characteristic Function (CF) to encapsulate full distributional information, employing a neural network to optimize the sampling strategy for the CF's frequency arguments, thereby maximizing the discrepancy to enhance distance estimation. Simultaneously, we minimize the difference between real and synthetic data under this optimized NCFD measure. Our approach, termed Neural Characteristic Function Matching (NCFM), inherently aligns the phase and amplitude of neural features in the complex plane for both real and synthetic data, achieving a balance between realism and diversity in synthetic samples. Experiments demonstrate that our method achieves significant performance gains over state-of-the-art methods on both low- and high-resolution datasets. Notably, we achieve a 22.9% accuracy boost on ImageSquawk. Our method also reduces GPU memory usage by over 300x and achieves 20x faster processing speeds compared to state-of-the-art methods. To the best of our knowledge, this is the first work to achieve lossless compression of CIFAR-100 on a single NVIDIA 2080 Ti GPU using only 2.3 GB of memory. The code for this work is publicly available at: https://github.com/gszfwsb/NCFM",
    "checked": true,
    "id": "ed343a29ef8e1152cbde39147f4af93c8c31c885",
    "semantic_title": "dataset distillation with neural characteristic function: a minmax perspective",
    "citation_count": 15,
    "authors": [
      "Shaobo Wang",
      "Yicun Yang",
      "Zhiyuan Liu",
      "Chenghao Sun",
      "Xuming Hu",
      "Conghui He",
      "Linfeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hong_Free-viewpoint_Human_Animation_with_Pose-correlated_Reference_Selection_CVPR_2025_paper.html": {
    "title": "Free-viewpoint Human Animation with Pose-correlated Reference Selection",
    "volume": "main",
    "abstract": "Diffusion-based human animation aims to animate a human character based on a source human image as well as driving signals such as a sequence of poses. Leveraging the generative capacity of diffusion model, existing approaches are able to generate high-fidelity poses, but struggle with significant viewpoint changes, especially in zoom-in/zoom-out scenarios where camera-character distance varies. This limits the applications such as cinematic shot type plan or camera control. We propose a pose-correlated reference selection diffusion network, supporting substantial viewpoint variations in human animation. Our key idea is to enable the network to utilize multiple reference images as input, since significant viewpoint changes often lead to missing appearance details on the human body. To eliminate the computational cost, we first introduce a novel pose correlation module to compute similarities between non-aligned target and source poses, and then propose an adaptive reference selection strategy, utilizing the attention map to identify key regions for animation generation. To train our model, we curated a large dataset from public TED talks featuring varied shots of the same character, helping the model learn synthesis for different perspectives. Our experimental results show that with the same number of reference images, our model performs favorably compared to the current SOTA methods under large viewpoint change. We further show that the adaptive reference selection is able to choose the most relevant reference regions to generate humans under free viewpoints",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fa-Ting Hong",
      "Zhan Xu",
      "Haiyang Liu",
      "Qinjie Lin",
      "Luchuan Song",
      "Zhixin Shu",
      "Yang Zhou",
      "Duygu Ceylan",
      "Dan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_CORE4D_A_4D_Human-Object-Human_Interaction_Dataset_for_Collaborative_Object_REarrangement_CVPR_2025_paper.html": {
    "title": "CORE4D: A 4D Human-Object-Human Interaction Dataset for Collaborative Object REarrangement",
    "volume": "main",
    "abstract": "Understanding how humans cooperatively rearrange household objects is critical for VR/AR and human-robot interaction. However, in-depth studies on modeling these behaviors are under-researched due to the lack of relevant datasets. We fill this gap by presenting CORE4D, a novel large-scale 4D human-object-human interaction dataset focusing on collaborative object rearrangement, which encompasses diverse compositions of various object geometries, collaboration modes, and 3D scenes. With 1K human-object-human motion sequences captured in the real world, we enrich CORE4D by contributing an iterative collaboration retargeting strategy to augment motions to a variety of novel objects. Leveraging this approach, CORE4D comprises a total of 11K collaboration sequences spanning 3K real and virtual object shapes. Benefiting from extensive motion patterns provided by CORE4D, we benchmark two tasks aiming at generating human-object interaction: human-object motion forecasting and interaction synthesis. Extensive experiments demonstrate the effectiveness of our collaboration retargeting strategy and indicate that CORE4D has posed new challenges to existing human-object interaction generation methodologies",
    "checked": true,
    "id": "eb885c05e45a1aa0be7bafb32732c03f3c6030e6",
    "semantic_title": "core4d: a 4d human-object-human interaction dataset for collaborative object rearrangement",
    "citation_count": 18,
    "authors": [
      "Yun Liu",
      "Chengwen Zhang",
      "Ruofan Xing",
      "Bingda Tang",
      "Bowen Yang",
      "Li Yi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_PillarHist_A_Quantization-aware_Pillar_Feature_Encoder_based_on_Height-aware_Histogram_CVPR_2025_paper.html": {
    "title": "PillarHist: A Quantization-aware Pillar Feature Encoder based on Height-aware Histogram",
    "volume": "main",
    "abstract": "Real-time and high-performance 3D object detection plays a critical role in autonomous driving and robotics. Recent pillar-based 3D object detectors have gained significant attention due to their compact representation and low computational overhead, making them suitable for onboard deployment and quantization. However, existing pillar-based detectors still suffer from information loss along height dimension and large numerical distribution difference during pillar feature encoding (PFE), which severely limits their performance and quantization potential. To address above issue, we first unveil the importance of different input information during PFE and identify the height dimension as a key factor in enhancing 3D detection performance. Motivated by this observation, we propose a height-aware pillar feature encoder, called PillarHist. Specifically, PillarHist statistics the discrete distribution of points at different heights within one pillar. This simple yet effective design greatly preserves the information along the height dimension while significantly reducing the computation overhead of the PFE. Meanwhile, PillarHist also constrains the arithmetic distribution of PFE input to a stable range, making it quantization-friendly. Notably, PillarHist operates exclusively within the PFE stage to enhance performance, enabling seamless integration into existing pillar-based methods without introducing complex operations. Extensive experiments show the effectiveness of PillarHist in terms of both efficiency and performance",
    "checked": true,
    "id": "b61c0af51e38b789da964d271f9bc692f1e8471e",
    "semantic_title": "pillarhist: a quantization-aware pillar feature encoder based on height-aware histogram",
    "citation_count": 5,
    "authors": [
      "Sifan Zhou",
      "Zhihang Yuan",
      "Dawei Yang",
      "Xing Hu",
      "Jian Qian",
      "Ziyu Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wilson_POp-GS_Next_Best_View_in_3D-Gaussian_Splatting_with_P-Optimality_CVPR_2025_paper.html": {
    "title": "POp-GS: Next Best View in 3D-Gaussian Splatting with P-Optimality",
    "volume": "main",
    "abstract": "In this paper, we present a novel algorithm for quantifying uncertainty and information gained within 3D Gaussian Splatting (3D-GS) through P-Optimality. While 3D-GS has proven to be a useful world model with high-quality rasterizations, it does not natively quantify uncertainty or information, posing a challenge for real-world applications such as 3D-GS SLAM. We propose to quantify information gain in 3D-GS by reformulating the problem through the lens of optimal experimental design, which is a classical solution widely used in literature. By restructuring information quantification of 3D-GS through optimal experimental design, we arrive at multiple solutions, of which T-Optimality and D-Optimality perform the best quantitatively and qualitatively as measured on two popular datasets. Additionally, we propose a block diagonal covariance approximation which provides a measure of correlation at the expense of a greater computation cost",
    "checked": true,
    "id": "c20a975e390ee5320bfe2b613913b923456805a5",
    "semantic_title": "pop-gs: next best view in 3d-gaussian splatting with p-optimality",
    "citation_count": 0,
    "authors": [
      "Joey Wilson",
      "Marcelino Almeida",
      "Sachit Mahajan",
      "Martin Labrie",
      "Maani Ghaffari",
      "Omid Ghasemalizadeh",
      "Min Sun",
      "Cheng-Hao Kuo",
      "Arnab Sen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Empowering_Vector_Graphics_with_Consistently_Arbitrary_Viewing_and_View-dependent_Visibility_CVPR_2025_paper.html": {
    "title": "Empowering Vector Graphics with Consistently Arbitrary Viewing and View-dependent Visibility",
    "volume": "main",
    "abstract": "This work presents a novel text-to-vector graphics generation approach, Dream3DVG, allowing for arbitrary viewpoint viewing, progressive detail optimization, and view-dependent occlusion awareness. Our approach is a dual-branch optimization framework, consisting of an auxiliary 3D Gaussian Splatting optimization branch and a 3D vector graphics optimization branch. The introduced 3DGS branch can bridge the domain gaps between text prompts and vector graphics with more consistent guidance. Moreover, 3DGS allows for progressive detail control by scheduling classifier-free guidance, facilitating guiding vector graphics with coarse shapes at the initial stages and finer details at later stages. We also improve the view-dependent occlusions by devising a visibility-awareness rendering module. Extensive results on 3D sketches and 3D iconographies, demonstrate the superiority of the method on different abstraction levels of details, cross-view consistency, and occlusion-aware stroke culling",
    "checked": true,
    "id": "01939c04d3e872c28226422f1eb15afdaa7ba969",
    "semantic_title": "empowering vector graphics with consistently arbitrary viewing and view-dependent visibility",
    "citation_count": 1,
    "authors": [
      "Yidi Li",
      "Jun Xiao",
      "Zhengda Lu",
      "Yiqun Wang",
      "Haiyong Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_Semantic_and_Expressive_Variations_in_Image_Captions_Across_Languages_CVPR_2025_paper.html": {
    "title": "Semantic and Expressive Variations in Image Captions Across Languages",
    "volume": "main",
    "abstract": "Most vision-language models today are primarily trained on English image-text pairs, with non-English pairs often filtered out. Evidence from cross-cultural psychology suggests that this approach will bias models against perceptual modes exhibited by people who speak other (non-English) languages. We investigate semantic and expressive variation in image captions across different languages; we analyze both human-annotated datasets and model-produced captions. By analyzing captions across seven languages (English, French, German, Russian, Chinese, Japanese, Korean) in high-quality image captioning datasets (Crossmodal and Visual Genome), we find that multilingual caption sets tend to provide richer visual descriptions than monolingual (including English-only) ones; multilingual sets contain 46.0% more objects, 66.1% more relationships, and 66.8% more attributes. We observe the same results with multilingual captions produced by LLaVA and the Google Vertex API: for example, compared to monolingual captions, they cover 21.9% more objects,18.8% more relations, and 20.1% more attributes. These suggest that, across a large number of samples, different languages bias people and models to focus on different visual concepts. Finally, we show that models trained on image-text data in one language perform distinctly better on that language's test set. Our work points towards the potential value of training vision models on multilingual data sources to widen the range/variation of descriptive information those models are exposed to",
    "checked": false,
    "id": "64e16687f2356973dad1f2b4d0b56a40d86a8345",
    "semantic_title": "semantic and expressive variation in image captions across languages",
    "citation_count": 4,
    "authors": [
      "Andre Ye",
      "Sebastin Santy",
      "Jena D. Hwang",
      "Amy X. Zhang",
      "Ranjay Krishna"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_ATP-LLaVA_Adaptive_Token_Pruning_for_Large_Vision_Language_Models_CVPR_2025_paper.html": {
    "title": "ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models",
    "volume": "main",
    "abstract": "Large Vision Language Models (LVLMs) have achieved significant success across multi-modal tasks. However, the computational cost of processing long visual tokens can be prohibitively expensive on resource-limited devices. Previous methods have identified redundancy in visual tokens within the Large Language Model (LLM) decoder layers and have mitigated this by pruning tokens using a pre-defined or fixed ratio, thereby reducing computational overhead. Nonetheless, we observe that the impact of pruning ratio varies across different LLM layers and instances (image-prompt pairs). Therefore, it is essential to develop a layer-wise and instance-wise vision token pruning strategy to balance computational cost and model performance effectively. We propose ATP-LLaVA, a novel approach that adaptively determines instance-specific token pruning ratios for each LLM layer. Specifically, we introduce an Adaptive Token Pruning (ATP) module, which computes the importance score and pruning threshold based on input instance adaptively. The ATP module can be seamlessly integrated between any two LLM layers with negligible computational overhead. Additionally, we develop a Spatial Augmented Pruning (SAP) strategy that prunes visual tokens with both token redundancy and spatial modeling perspectives. Our approach reduces the average token count by 75% while maintaining performance, with only a minimal 1.9% degradation across seven widely used benchmarks",
    "checked": true,
    "id": "134a66cc64344b9ef913e0f25c2fab6f4ff92c94",
    "semantic_title": "atp-llava: adaptive token pruning for large vision language models",
    "citation_count": 19,
    "authors": [
      "Xubing Ye",
      "Yukang Gan",
      "Yixiao Ge",
      "Xiao-Ping Zhang",
      "Yansong Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mi_ADD_Attribution-Driven_Data_Augmentation_Framework_for_Boosting_Image_Super-Resolution_CVPR_2025_paper.html": {
    "title": "ADD: Attribution-Driven Data Augmentation Framework for Boosting Image Super-Resolution",
    "volume": "main",
    "abstract": "Data augmentation (DA) stands out as a powerful technique to enhance the generalization capabilities of deep neural networks across diverse tasks. However, in low-level vision tasks, DA remains rudimentary (i.e., vanilla DA), facing a critical bottleneck due to information loss. In this paper, we introduce a novel Calibrated Attribution Maps (CAM) to generate saliency masks, followed by two saliency-based DA methods-- Attribution-Driven Data augmentation (ADD) and ADD+--designed to address this issue. CAM leverages integrated gradients and incorporates two key innovations: a global feature detector and calibrated integrated gradients. Based on CAM and the proposed methods, we have two new insights for low-level vision tasks: (1) increasing pixel diversity, as seen in vanilla DA, can improve performance, and (2) focusing on salient features while minimizing the impact of irrelevant pixels, as seen in saliency-based DA, more effectively enhances model performance. Additionally, we find and highlight the key guiding principle for designing saliency-based DA: a wider spectrum of degradation patterns. Extensive experiments demonstrate the compatibility and consistency of our method, as well as the significant performance improvement across various SR tasks and networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ze-Yu Mi",
      "Yu-Bin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_DIFFER_Disentangling_Identity_Features_via_Semantic_Cues_for_Clothes-Changing_Person_CVPR_2025_paper.html": {
    "title": "DIFFER: Disentangling Identity Features via Semantic Cues for Clothes-Changing Person Re-ID",
    "volume": "main",
    "abstract": "Clothes-changing person re-identification (CC-ReID) aims to recognize individuals under different clothing scenarios. Current CC-ReID approaches either concentrate on modeling body shape using additional modalities including silhouette, pose, and body mesh, potentially causing the model to overlook other critical biometric traits such as gender, age, and style, or they incorporate supervision through additional labels that the model tries to disregard or emphasize, such as clothing or personal attributes. However, these annotations are discrete in nature and do not capture comprehensive descriptions. In this work, we propose DIFFER: Disentangle Identity Features From Entangled Representations, a novel adversarial learning method that leverages textual descriptions to disentangle identity features. Recognizing that image features inherently mix inseparable information, DIFFER introduces NBDetach, a mechanism designed for feature disentanglement by leveraging the separable nature of text descriptions as supervision. It partitions the feature space into distinct subspaces and, through gradient reversal layers, effectively separates identity-related features from non-biometric features. We evaluate DIFFER on 4 different benchmark datasets (LTCC, PRCC, CelebreID-Light, and CCVID) to demonstrate its effectiveness and provide state-of-the-art performance across all the benchmarks. DIFFER consistently outperforms the baseline method, with improvements in top-1 accuracy of 3.6% on LTCC, 3.4% on PRCC, 2.5% on CelebReID-Light, and 1% on CCVID. Our code can be found at https://github.com/xliangp/DIFFER.git",
    "checked": true,
    "id": "cf53c21d47b9c7719e90e81159048c5240fb5f31",
    "semantic_title": "differ: disentangling identity features via semantic cues for clothes-changing person re-id",
    "citation_count": 4,
    "authors": [
      "Xin Liang",
      "Yogesh S Rawat"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ferens_HyperPose_Hypernetwork-Infused_Camera_Pose_Localization_and_an_Extended_Cambridge_Landmarks_CVPR_2025_paper.html": {
    "title": "HyperPose: Hypernetwork-Infused Camera Pose Localization and an Extended Cambridge Landmarks Dataset",
    "volume": "main",
    "abstract": "In this work, we propose HyperPose, which utilizes hypernetworks in absolute camera pose regressors. The inherent appearance variations in natural scenes, attributable to environmental conditions, perspective, and lighting, induce a significant domain disparity between the training and test datasets. This disparity degrades the precision of contemporary localization networks. To mitigate this, we advocate for incorporating hypernetworks into single-scene and multiscene camera pose regression models. During inference, the hypernetwork dynamically computes adaptive weights for the localization regression heads based on the particular input image, effectively narrowing the domain gap. Using indoor and outdoor datasets, we evaluate the HyperPose methodology across multiple established absolute pose regression architectures. In particular, we introduce and share the Extended Cambridge Landmarks (ECL), which is a novel localization dataset, based on the Cambridge Landmarks dataset, showing it in multiple seasons with significantly varying appearance conditions. Our empirical experiments demonstrate that HyperPose yields notable performance enhancements for both single- and multi-scene architectures. We have made our source code, pre-trained models, and ECL dataset openly available",
    "checked": true,
    "id": "6d28405111d348e10340ec7157e57eca030b7bf5",
    "semantic_title": "hyperpose: hypernetwork-infused camera pose localization and an extended cambridge landmarks dataset",
    "citation_count": 0,
    "authors": [
      "Ron Ferens",
      "Yosi Keller"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Critic-V_VLM_Critics_Help_Catch_VLM_Errors_in_Multimodal_Reasoning_CVPR_2025_paper.html": {
    "title": "Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning",
    "volume": "main",
    "abstract": "Vision-language models (VLMs) have shown remarkable advancements in multimodal reasoning tasks. However, they still often generate inaccurate or irrelevant responses due to issues like hallucinated image understandings or unrefined reasoning paths. To address these challenges, we introduce Critic-V, a novel framework inspired by the Actor-Critic paradigm to boost the reasoning capability of VLMs. This framework decouples the reasoning process and critic process by integrating two independent components: the Reasoner, which generates reasoning paths based on visual and textual inputs, and the Critic, which provides constructive critique to refine these paths. In this approach, the Reasoner generates reasoning responses according to text prompts, which can evolve iteratively as a policy based on feedback from the Critic. This interaction process was theoretically driven by a reinforcement learning framework where the Critic offers natural language critiques instead of scalar rewards, enabling more nuanced feedback to boost the Reasoner's capability on complex reasoning tasks. The Critic model is trained using Direct Preference Optimization (DPO), leveraging a preference dataset of critiques ranked by Rule-based Reward (RBR) to enhance its critic capabilities. Evaluation results show that the Critic-V framework significantly outperforms existing methods, including GPT-4V, on 5 out of 8 benchmarks, especially regarding reasoning accuracy and efficiency. Combining a dynamic text-based policy for the Reasoner and constructive feedback from the preference-optimized Critic enables a more reliable and context-sensitive multimodal reasoning process. Our approach provides a promising solution to enhance the reliability of VLMs, improving their performance in real-world reasoning-heavy multimodal applications such as autonomous driving and embodied intelligence",
    "checked": true,
    "id": "8690c869ab7e18cdbd4f17e38a41727c16b28221",
    "semantic_title": "critic-v: vlm critics help catch vlm errors in multimodal reasoning",
    "citation_count": 12,
    "authors": [
      "Di Zhang",
      "Jingdi Lei",
      "Junxian Li",
      "Xunzhi Wang",
      "Yujie Liu",
      "Zonglin Yang",
      "Jiatong Li",
      "Weida Wang",
      "Suorong Yang",
      "Jianbo Wu",
      "Peng Ye",
      "Wanli Ouyang",
      "Dongzhan Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wei_Mono3DVLT_Monocular-Video-Based_3D_Visual_Language_Tracking_CVPR_2025_paper.html": {
    "title": "Mono3DVLT: Monocular-Video-Based 3D Visual Language Tracking",
    "volume": "main",
    "abstract": "Visual-Language Tracking (VLT) is emerging as a promising paradigm to bridge the human-machine performance gap. For single objects, VLT broadens the problem scope to text-driven video comprehension. Yet, this direction is still confined to 2D spatial extents, currently lacking the ability to deal with 3D tracking in the confines of monocular video. Unfortunately, advances in 3D tracking mainly rely on expensive sensor inputs, e.g., point clouds, depth measurements, radar. Absence of language counterpart for the outputs of these mildly democratized sensors in the literature also hinders VLT expansion to 3D tracking. Addressing that, we make the first attempt towards extending VLT to 3D tracking based on monocular video. We present a comprehensive framework, introducing (i) the Monocular-Video-based 3D Visual Language Tracking (Mono3DVLT) task, (ii) a large-scale dataset for the task, called Mono3DVLT-V2X, and (iii) a customized neural model for the task. Our dataset is carefully curated, leveraging a Large Langauge Model (LLM) followed by human verification, composing natural language descriptions for 79,158 video sequences aiming at single object tracking, providing 2D and 3D bounding box annotations. Our neural model, termed Mono3DVLT-MT, is the first targeted approach for the Mono3DVLT task. Comprising the pipeline of multi-modal feature extractor, visual-language encoder, tracking decoder and a tracking head, our model sets a strong baseline for the task on Mono3DVLT-V2X. Experimental results show that our method significantly outperforms existing techniques on the Mono3DVLT-V2X dataset. Our dataset and code are available in https://github.com/hongkai-wei/Mono3DVLT",
    "checked": true,
    "id": "320f902fd301f97f01c6aa36f443420892d5da83",
    "semantic_title": "mono3dvlt: monocular-video-based 3d visual language tracking",
    "citation_count": 0,
    "authors": [
      "Hongkai Wei",
      "Yang Yang",
      "Shijie Sun",
      "Mingtao Feng",
      "Xiangyu Song",
      "Qi Lei",
      "Hongli Hu",
      "Rong Wang",
      "Huansheng Song",
      "Naveed Akhtar",
      "Ajmal Saeed Mian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qi_Towards_Universal_Dataset_Distillation_via_Task-Driven_Diffusion_CVPR_2025_paper.html": {
    "title": "Towards Universal Dataset Distillation via Task-Driven Diffusion",
    "volume": "main",
    "abstract": "Dataset distillation (DD) condenses key information from large-scale datasets into smaller synthetic datasets, reducing storage and computational costs for training networks. However, recent research has primarily focused on image classification tasks, with limited expansion to detection and segmentation. Two key challenges remain: (i) Task Optimization Heterogeneity, where existing methods focus on class-level information and fail to address the diverse needs of detection and segmentation and (ii) Inflexible Image Generation, where current generation methods rely on global updates for single-class targets and lack localized optimization for specific object regions.To address these challenges, we propose a universal dataset distillation framework, named UniDD, a task-driven diffusion model for diverse DD tasks, as illustrated in Fig.1. Our approach operates in two stages: Universal Task Knowledge Mining, which captures task-relevant information through task-specific proxy model training, and Universal Task-Driven Diffusion, where these proxies guide the diffusion process to generate task-specific synthetic images.Extensive experiments across ImageNet-1K, Pascal VOC, and MS COCO demonstrate that UniDD consistently outperforms state-of-the-art methods. In particular, on ImageNet-1K with IPC-10, UniDD surpasses previous diffusion-based methods by 6.1%, while also reducing deployment costs",
    "checked": true,
    "id": "cbd137bc868da7005da1fdbb71dd9776bc7c4232",
    "semantic_title": "towards universal dataset distillation via task-driven diffusion",
    "citation_count": 1,
    "authors": [
      "Ding Qi",
      "Jian Li",
      "Junyao Gao",
      "Shuguang Dou",
      "Ying Tai",
      "Jianlong Hu",
      "Bo Zhao",
      "Yabiao Wang",
      "Chengjie Wang",
      "Cairong Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Parametric_Point_Cloud_Completion_for_Polygonal_Surface_Reconstruction_CVPR_2025_paper.html": {
    "title": "Parametric Point Cloud Completion for Polygonal Surface Reconstruction",
    "volume": "main",
    "abstract": "Existing polygonal surface reconstruction methods heavily depend on input completeness and struggle with incomplete point clouds. We argue that while current point cloud completion techniques may recover missing points, they are not optimized for polygonal surface reconstruction, where the parametric representation of underlying surfaces remains overlooked. To address this gap, we introduce parametric completion, a novel paradigm for point cloud completion, which recovers parametric primitives instead of individual points to convey high-level geometric structures. Our presented approach, PaCo, enables high-quality polygonal surface reconstruction by leveraging plane proxies that encapsulate both plane parameters and inlier points, proving particularly effective in challenging scenarios with highly incomplete data. Comprehensive evaluations of our approach on the ABC dataset establish its effectiveness with superior performance and set a new standard for polygonal surface reconstruction from incomplete data. Project page: https://parametric-completion.github.io",
    "checked": true,
    "id": "61a1e263d3250c1fc0675091added8931eaea555",
    "semantic_title": "parametric point cloud completion for polygonal surface reconstruction",
    "citation_count": 0,
    "authors": [
      "Zhaiyu Chen",
      "Yuqing Wang",
      "Liangliang Nan",
      "Xiao Xiang Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_SyncSDE_A_Probabilistic_Framework_for_Diffusion_Synchronization_CVPR_2025_paper.html": {
    "title": "SyncSDE: A Probabilistic Framework for Diffusion Synchronization",
    "volume": "main",
    "abstract": "There have been many attempts to leverage multiple diffusion models for collaborative generation, extending beyond the original domain. A prominent approach involves synchronizing multiple diffusion trajectories by mixing the estimated scores to artificially correlate the generation processes. However, existing methods rely on naive heuristics, such as averaging, without considering task specificity. These approaches do not clarify why such methods work and often fail when a heuristic suitable for one task is blindly applied to others. In this paper, we present a probabilistic framework for analyzing why diffusion synchronization works and reveal where heuristics should be focused--modeling correlations between multiple trajectories and adapting them to each specific task. We further identify optimal correlation models per task, achieving better results than previous approaches that apply a single heuristic across all tasks without justification",
    "checked": true,
    "id": "425429e059532a7a5876d387cae1a7bedfa6a00e",
    "semantic_title": "syncsde: a probabilistic framework for diffusion synchronization",
    "citation_count": 0,
    "authors": [
      "Hyunjun Lee",
      "Hyunsoo Lee",
      "Sookwan Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MaRI_Material_Retrieval_Integration_across_Domains_CVPR_2025_paper.html": {
    "title": "MaRI: Material Retrieval Integration across Domains",
    "volume": "main",
    "abstract": "Accurate material retrieval is critical for creating realistic 3D assets. Existing methods rely on datasets that capture shape-invariant and lighting-varied representations of materials, which are scarce and face challenges due to limited diversity and inadequate real-world generalization. Most current approaches adopt traditional image search techniques. They fall short in capturing the unique properties of material spaces, leading to suboptimal performance in retrieval tasks. Addressing these challenges, we introduce MaRI, a framework designed to bridge the feature space gap between synthetic and real-world materials. MaRI constructs a shared embedding space that harmonizes visual and material attributes through a contrastive learning strategy by jointly training a image and a material encoder, bringing similar materials and images closer while separating dissimilar pairs within the feature space. To support this, we construct a comprehensive dataset comprising high-quality synthetic materials rendered with controlled shape variations and diverse lighting conditions, along with real-world materials processed and standardized using material transfer techniques. Extensive experiments demonstrate the superior performance, accuracy, and generalization capabilities of MaRI across diverse and complex material retrieval tasks, outperforming existing methods",
    "checked": true,
    "id": "0b890e0260ce01c4dce1ca5c9f8b795af32e400c",
    "semantic_title": "mari: material retrieval integration across domains",
    "citation_count": 3,
    "authors": [
      "Jianhui Wang",
      "Zhifei Yang",
      "Yangfan He",
      "Huixiong Zhang",
      "Yuxuan Chen",
      "Jingwei Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_MCCD_Multi-Agent_Collaboration-based_Compositional_Diffusion_for_Complex_Text-to-Image_Generation_CVPR_2025_paper.html": {
    "title": "MCCD: Multi-Agent Collaboration-based Compositional Diffusion for Complex Text-to-Image Generation",
    "volume": "main",
    "abstract": "Diffusion models have shown excellent performance in text-to-image generation. However, existing methods often suffer from performance bottlenecks when dealing with complex prompts involving multiple objects, characteristics, and relations. Therefore, we propose a Multi-agent Collaboration-based Compositional Diffusion (MCCD) for text-to-image generation for complex scenes. Specifically, we design a multi-agent collaboration based scene parsing module that generates an agent system containing multiple agents with different tasks using MLLMs to adequately extract multiple scene elements. In addition, Hierarchical Compositional diffusion utilizes Gaussian mask and filtering to achieve the refinement of bounding box regions and highlights objects through region enhancement for accurate and high-fidelity generation of complex scenes. Comprehensive experiments demonstrate that our MCCD significantly improves the performance of the baseline models in a training-free manner, which has a large advantage in complex scene generation. The code will be open-source on github",
    "checked": true,
    "id": "f8b50a476e3e1ed2499f853bcc3e16acf93bca85",
    "semantic_title": "mccd: multi-agent collaboration-based compositional diffusion for complex text-to-image generation",
    "citation_count": 3,
    "authors": [
      "Mingcheng Li",
      "Xiaolu Hou",
      "Ziyang Liu",
      "Dingkang Yang",
      "Ziyun Qian",
      "Jiawei Chen",
      "Jinjie Wei",
      "Yue Jiang",
      "Qingyao Xu",
      "Lihua Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Dual_Semantic_Guidance_for_Open_Vocabulary_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Dual Semantic Guidance for Open Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "Open-vocabulary semantic segmentation aims to enable models to segment arbitrary categories. Currently, though pre-trained Vision-Language Models (VLMs) like CLIP have established a robust foundation for this task by learning to match text and image representations from large-scale data, their lack of pixel-level recognition necessitates further fine-tuning. Most existing methods leverage text as a guide to achieve pixel-level recognition. However, the inherent biases in text semantic descriptions and the lack of pixel-level supervisory information make it challenging to fine-tune CLIP-based models effectively. This paper considers leveraging image-text data to simultaneously capture the semantic information contained in both image and text, thereby constructing Dual Semantic Guidance and corresponding pixel-level pseudo annotations. Particularly, the visual semantic guidance is enhanced via explicitly exploring foreground regions and minimizing the influence of background. The dual semantic guidance is then jointly utilized to fine-tune CLIP-based segmentation models, achieving decent fine-grained recognition capabilities. As the comprehensive evaluation shows, our method outperforms state-of-art results with large margins, on eight commonly used datasets with/without background",
    "checked": true,
    "id": "cbfd33c94657a0682c089b322e8c5d2f7e51c113",
    "semantic_title": "dual semantic guidance for open vocabulary semantic segmentation",
    "citation_count": 1,
    "authors": [
      "Zhengyang Wang",
      "Tingliang Feng",
      "Fan Lyu",
      "Fanhua Shang",
      "Wei Feng",
      "Liang Wan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Blum_CroCoDL_Cross-device_Collaborative_Dataset_for_Localization_CVPR_2025_paper.html": {
    "title": "CroCoDL: Cross-device Collaborative Dataset for Localization",
    "volume": "main",
    "abstract": "Accurate localization plays a pivotal role in the autonomy of systems operating in unfamiliar environments, particularly when interaction with humans is expected. High-accuracy visual localization systems encompass various components, such as image retrievers, feature extractors, matchers, reconstruction and pose estimation methods. This complexity translates to the necessity of robust evaluation settings and pipelines. However, existing datasets and benchmarks primarily focus on single-agent scenarios, overlooking the critical issue of cross-device localization. Different agents with different sensors will show their own specific strengths and weaknesses, and the data they have available varies substantially. This work addresses this gap by enhancing an existing augmented reality visual localization benchmark with data from legged robots, and evaluating human-robot, cross-device mapping and localization. Our contributions extend beyond device diversity and include high environment variability, spanning ten distinct locations ranging from disaster sites to art exhibitions. Each scene in our dataset features recordings from robot agents, hand-held and head-mounted devices, and high-accuracy ground truth LiDAR scanners, resulting in a comprehensive multi-agent dataset and benchmark. This work represents a significant advancement in the field of visual localization benchmarking, with key insights into the performance of cross-device localization methods across diverse settings",
    "checked": true,
    "id": "c382f4d4f6f64d895fe9ee5e44e0160e35a31d46",
    "semantic_title": "crocodl: cross-device collaborative dataset for localization",
    "citation_count": 1,
    "authors": [
      "Hermann Blum",
      "Alessandro Mercurio",
      "Joshua O'Reilly",
      "Tim Engelbracht",
      "Mihai Dusmanu",
      "Marc Pollefeys",
      "Zuria Bauer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Q-Bench-Video_Benchmark_the_Video_Quality_Understanding_of_LMMs_CVPR_2025_paper.html": {
    "title": "Q-Bench-Video: Benchmark the Video Quality Understanding of LMMs",
    "volume": "main",
    "abstract": "With the rising interest in research on Large Multi-modal Models (LMMs) for video understanding, many studies have emphasized general video comprehension capabilities, neglecting the systematic exploration into video quality understanding. To address this oversight, we introduce Q-Bench-Video in this paper, a new benchmark specifically designed to evaluate LMMs' proficiency in discerning video quality. a) To ensure video source diversity, Q-Bench-Video encompasses videos from natural scenes, AI-generated Content (AIGC), and Computer Graphics (CG). b) Building on the traditional multiple-choice questions format with the Yes-or-No and What-How categories, we include Open-ended questions to better evaluate complex scenarios. Additionally, we incorporate the video pair quality comparison question to enhance comprehensiveness. c) Beyond the traditional Technical, Aesthetic, and Temporal distortions, we have expanded our evaluation aspects to include the dimension of AIGC distortions, which addresses the increasing demand for video generation. Finally, we collect a total of 2,378 question-answer pairs and test them on 12 open-source & 5 proprietary LMMs. Our findings indicate that while LMMs have a foundational understanding of video quality, their performance remains incomplete and imprecise, with a notable discrepancy compared to human performance. Through Q-Bench-Video, we seek to catalyze community interest, stimulate further research, and unlock the untapped potential of LMMs to close the gap in video quality understanding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zicheng Zhang",
      "Ziheng Jia",
      "Haoning Wu",
      "Chunyi Li",
      "Zijian Chen",
      "Yingjie Zhou",
      "Wei Sun",
      "Xiaohong Liu",
      "Xiongkuo Min",
      "Weisi Lin",
      "Guangtao Zhai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Glossy_Object_Reconstruction_with_Cost-effective_Polarized_Acquisition_CVPR_2025_paper.html": {
    "title": "Glossy Object Reconstruction with Cost-effective Polarized Acquisition",
    "volume": "main",
    "abstract": "The challenge of image-based 3D reconstruction for glossy objects lies in separating diffuse and specular components on glossy surfaces from captured images, a task complicated by the ambiguity in discerning lighting conditions and material properties using RGB data alone. While state-of-the-art methods rely on tailored and/or high-end equipment for data acquisition, which can be cumbersome and time-consuming, this work introduces a scalable polarization-aided approach that employs cost-effective acquisition tools. By attaching a linear polarizer to readily available RGB cameras, multi-view polarization images can be captured without the need for advance calibration or precise measurements of the polarizer angle, substantially reducing system construction costs. The proposed approach represents polarimetric BRDF, Stokes vectors, and polarization states of object surfaces as neural implicit fields. These fields, combined with the polarizer angle, are retrieved by optimizing the rendering loss of input polarized images. By leveraging fundamental physical principles for the implicit representation of polarization rendering, our method demonstrates superiority over existing techniques through experiments in public datasets and real captured images on both reconstruction and novel view synthesis",
    "checked": true,
    "id": "ec42e5bbfabcc0f108b348d0f13fcbcd152b0bfd",
    "semantic_title": "glossy object reconstruction with cost-effective polarized acquisition",
    "citation_count": 0,
    "authors": [
      "Bojian Wu",
      "Yifan Peng",
      "Ruizhen Hu",
      "Xiaowei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Generalizable_Object_Keypoint_Localization_from_Generative_Priors_CVPR_2025_paper.html": {
    "title": "Generalizable Object Keypoint Localization from Generative Priors",
    "volume": "main",
    "abstract": "Generalizable object keypoint localization is a fundamental computer vision task in understanding the object structure. It is challenging for existing keypoint localization methods because their limited training data cannot provide generalizable shape and semantic cues, leading to inferior performance and generalization capability. Instead of relying on large scale training data, this work tackles this challenge by exploiting the rich priors from large generative models. We propose a data-efficient generalizable localization method named GenLoc. GenLoc extracts the generative priors from a pre-trained image generation model by calculating the correlation map between image latent feature and condition embedding. Those priors are hence optimized with our proposed heatmap expectation loss to perform object keypoint localization. Benefited by the rich knowledge of generative priors in understanding of object semantics and structures, GenLoc achieves superior performance on various object keypoint localization benchmarks. It shows more substantial performance enhancements in cross-domain, few-shot and zero-shot evaluation settings, e.g., getting 20%+ AP enhancement over CLAMP in various zero-shot settings",
    "checked": true,
    "id": "75805757bb729124cf74d52dd170a33cb143aa42",
    "semantic_title": "generalizable object keypoint localization from generative priors",
    "citation_count": 0,
    "authors": [
      "Dongkai Wang",
      "Jiang Duan",
      "Liangjian Wen",
      "Shiyu Xuan",
      "Hao Chen",
      "Shiliang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qin_CLIP_is_Almost_All_You_Need_Towards_Parameter-Efficient_Scene_Text_CVPR_2025_paper.html": {
    "title": "CLIP is Almost All You Need: Towards Parameter-Efficient Scene Text Retrieval without OCR",
    "volume": "main",
    "abstract": "Scene Text Retrieval (STR) seeks to identify all images containing a given query string. Existing methods typically rely on an explicit Optical Character Recognition (OCR) process of text spotting or localization, which is susceptible to complex pipelines and accumulated errors. To settle this, we resort to the Contrastive Language-Image Pre-training (CLIP) models, which have demonstrated the capacity to perceive and understand scene text, making it possible to achieve strictly OCR-free STR. From the perspective of parameter-efficient transfer learning, a lightweight visual position adapter is proposed to provide a positional information complement for the visual encoder. Besides, we introduce a visual context dropout technique to improve the alignment of local visual features. A novel, parameter-free cross-attention mechanism transfers the contrastive relationship between images and text to that between visual tokens and text, producing a rich cross-modal representation, which can be utilized for efficient reranking with a linear classifier. The resulting model, CAYN, which proves that CLIP is Almost all You Need for STR with no more than 0.50M additional parameters required, achieves new state-of-the-art performance on the STR task, with 92.46%/89.49%/85.98% mAP on the SVT/IIIT-STR/TTR datasets. Our findings demonstrate that CLIP can serve as a reliable and efficient solution for OCR-free STR",
    "checked": true,
    "id": "655e36453d4869655267a223b4393564bede930b",
    "semantic_title": "clip is almost all you need: towards parameter-efficient scene text retrieval without ocr",
    "citation_count": 0,
    "authors": [
      "Xugong Qin",
      "Peng Zhang",
      "Jun Jie Ou Yang",
      "Gangyan Zeng",
      "Yubo Li",
      "Yuanyuan Wang",
      "Wanqian Zhang",
      "Pengwen Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Casarin_L-SWAG_Layer-Sample_Wise_Activation_with_Gradients_Information_for_Zero-Shot_NAS_CVPR_2025_paper.html": {
    "title": "L-SWAG: Layer-Sample Wise Activation with Gradients Information for Zero-Shot NAS on Vision Transformers",
    "volume": "main",
    "abstract": "Training-free Neural Architecture Search (NAS) efficiently identifies high-performing neural networks using zero-cost (ZC) proxies. Unlike multi-shot and one-shot NAS approaches, ZC-NAS is both (i) time-efficient, eliminating the need for model training, and (ii) interpretable, with proxy designs often theoretically grounded. Despite rapid developments in the field, current SOTA ZC proxies are typically constrained to well-established convolutional search spaces. With the rise of Large Language Models shaping the future of deep learning, this work extends ZC proxy applicability to Vision Transformers (ViTs). We present a new benchmark using the Autoformer search space evaluated on 6 distinct tasks, and propose Layer-Sample Wise Activation with Gradients information (L-SWAG), a novel, generalizable metric that characterises both convolutional and transformer architectures across 14 tasks. Additionally, previous works highlighted how different proxies contain complementary information, motivating the need for a ML model to identify useful combinations. To further enhance ZC-NAS, we therefore introduce LIBRA-NAS (Low Information gain and Bias Re-Alignment), a method that strategically combines proxies to best represent a specific benchmark. Integrated into the NAS search, LIBRA-NAS outperforms evolution and gradient-based NAS techniques by identifying an architecture with a 17.0% test error on ImageNet1k in just 0.1 GPU days",
    "checked": true,
    "id": "bdb7cae8fa9f658d47868db34cf3fe40d27e75e0",
    "semantic_title": "l-swag: layer-sample wise activation with gradients information for zero-shot nas on vision transformers",
    "citation_count": 1,
    "authors": [
      "Sofia Casarin",
      "Sergio Escalera",
      "Oswald Lanz"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Commonsense_Video_Question_Answering_through_Video-Grounded_Entailment_Tree_Reasoning_CVPR_2025_paper.html": {
    "title": "Commonsense Video Question Answering through Video-Grounded Entailment Tree Reasoning",
    "volume": "main",
    "abstract": "This paper proposes the first video-grounded entailment tree reasoning method for commonsense video question answering (VQA). Despite the remarkable progress of large visual-language models (VLMs), there are growing concerns that they learn spurious correlations between videos and likely answers, reinforced by their black-box nature and remaining benchmarking biases. Our method explicitly grounds VQA tasks to video fragments in four steps: entailment tree construction, video-language entailment verification, tree reasoning, and dynamic tree expansion. A vital benefit of the method is its generalizability to current video- and image-based VLMs across reasoning types.To support fair evaluation, we devise a de-biasing procedure based on large-language models that rewrite VQA benchmark answer sets to enforce model reasoning. Systematic experiments on existing and de-biased benchmarks highlight the impact of our method components across benchmarks, VLMs, and reasoning types",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huabin Liu",
      "Filip Ilievski",
      "Cees G. M. Snoek"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Frank_What_Makes_a_Good_Dataset_for_Knowledge_Distillation_CVPR_2025_paper.html": {
    "title": "What Makes a Good Dataset for Knowledge Distillation?",
    "volume": "main",
    "abstract": "Knowledge distillation (KD) has been a popular and effective method for model compression. One important assumption of KD is that the teacher's original dataset will also be available when training the student. However, in situations such as continual learning and distilling large models trained on company-withheld datasets, having access to the original data may not always be possible. This leads practitioners towards utilizing other sources of supplemental data, which could yield mixed results. One must then ask: \"what makes a good dataset for transferring knowledge from teacher to student?\" Many would assume that only real in-domain imagery is viable, but is that the only option? In this work, we explore multiple possible surrogate distillation datasets and demonstrate that many different datasets, even unnatural synthetic imagery, can serve as a suitable alternative in KD. From examining these alternative datasets, we identify and present various criteria describing what makes a good dataset for distillation. Source code is available at https://github.com/osu-cvl/good-kd-dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Logan Frank",
      "Jim Davis"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Lifelong_Knowledge_Editing_for_Vision_Language_Models_with_Low-Rank_Mixture-of-Experts_CVPR_2025_paper.html": {
    "title": "Lifelong Knowledge Editing for Vision Language Models with Low-Rank Mixture-of-Experts",
    "volume": "main",
    "abstract": "Model editing aims to correct inaccurate knowledge, update outdated information, and incorporate new data into Large Language Models (LLMs) without the need for retraining. This task poses challenges in lifelong scenarios where edits must be continuously applied for real-world applications. While some editors demonstrate strong robustness for lifelong editing in pure LLMs, Vision LLMs (VLLMs), which incorporate an additional vision modality, are not directly adaptable to existing LLM editors. In this paper, we propose LiveEdit, a lifelong vision language model edit to bridge the gap between lifelong LLM editing and VLLMs. We begin by training an editing expert generator to independently produce low-rank experts for each editing instance, with the goal of correcting the relevant responses of the VLLM. A hard filtering mechanism is developed to utilize visual semantic knowledge, thereby coarsely eliminating visually irrelevant experts for input queries during the inference stage of the post-edited model. Finally, to integrate visually relevant experts, we introduce a soft routing mechanism based on textual semantic relevance to achieve multi-expert fusion. For evaluation, we establish a benchmark for lifelong VLLM editing. Extensive experiments demonstrate that LiveEdit offers significant advantages in lifelong VLLM editing scenarios. Further experiments validate the rationality and effectiveness of each module design in LiveEdit",
    "checked": true,
    "id": "89eb7afe3e5c4054a6b1b9a1fdf8d799215dcd89",
    "semantic_title": "lifelong knowledge editing for vision language models with low-rank mixture-of-experts",
    "citation_count": 2,
    "authors": [
      "Qizhou Chen",
      "Chengyu Wang",
      "Dakan Wang",
      "Taolin Zhang",
      "Wangyue Li",
      "Xiaofeng He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gong_Rectification-specific_Supervision_and_Constrained_Estimator_for_Online_Stereo_Rectification_CVPR_2025_paper.html": {
    "title": "Rectification-specific Supervision and Constrained Estimator for Online Stereo Rectification",
    "volume": "main",
    "abstract": "Online stereo rectification is critical for autonomous vehicles and robots in dynamic environments, where factors such as vibration, temperature fluctuations, and mechanical stress can affect rectification accuracy and severely degrade downstream stereo depth estimation. Current dominant approaches for online stereo rectification involve estimating relative camera poses in real time to derive rectification homographies. However, they do not directly optimize for rectification constraints. Additionally, the general-purpose correspondence matchers used in these methods are not trained for rectification, while training of these matchers typically requires ground-truth correspondences which are not available in stereo rectification datasets. To address these limitations, we propose a matching-based stereo rectification framework that is directly optimized for rectification and does not require ground-truth correspondence annotations for training. We assume intrinsics are known as they are generally available on modern devices and are relatively stable. Our framework incorporates a rectification-constrained estimator and applies multi-level, rectification-specific supervision that trains the matcher network for rectification without relying on ground-truth correspondences. Additionally, we create a new rectification dataset with ground-truth optical flow annotations, eliminating bias from evaluation metrics used in prior work that relied on pretrained keypoint matching or optical flow models. Extensive experiments show that our approach outperforms both state-of-the-art matching-based and matching-free methods in vertical flow metric by 10.7% on the Carla-Flowguided dataset and 21.3% on the Semi-Truck Highway dataset, offering superior rectification accuracy",
    "checked": true,
    "id": "d901aa62510f0ca48a6228f62b0d0e39f3676d15",
    "semantic_title": "rectification-specific supervision and constrained estimator for online stereo rectification",
    "citation_count": 0,
    "authors": [
      "Rui Gong",
      "Kim-Hui Yap",
      "Weide Liu",
      "Xulei Yang",
      "Jun Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Long_Shape_and_Texture_What_Influences_Reliable_Optical_Flow_Estimation_CVPR_2025_paper.html": {
    "title": "Shape and Texture: What Influences Reliable Optical Flow Estimation?",
    "volume": "main",
    "abstract": "Recent methods have made significant progress in optical flow estimation. However, the evaluation of these methods focuses mainly on improved accuracy in benchmarks and often overlooks the analysis of network robustness, which may be important in safety-critical scenarios such as autonomous driving. In this paper, we propose a novel method for robustness evaluation by modifying data from original benchmarks. Unlike previous benchmarks that focus on complex scenes, we propose to modify shape and texture of objects from the original images in order to analyze the sensitivity to these changes observed in the output. Our aim is to identify common failure cases of state-of-the-art (SOTA) methods to evaluate their robustness and understand their behaviors. We show that: Optical flow methods are more sensitive to shape changes than to texture changes; and optical flow methods tend to \"remember\" objects seen during training and may \"ignore\" the motion of unseen objects. Our experimental results and findings provide a more in-depth understanding of the behavior of recent optical flow methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Libo Long",
      "Xiao Hu",
      "Jochen Lang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_PartGen_Part-level_3D_Generation_and_Reconstruction_with_Multi-view_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "PartGen: Part-level 3D Generation and Reconstruction with Multi-view Diffusion Models",
    "volume": "main",
    "abstract": "Text- or image-to-3D generators and 3D scanners can now produce 3D assets with high-quality shapes and textures, but as single, fused entities lacking meaningful structure. In contrast, most applications and creative workflows require 3D assets to be composed of distinct, meaningful parts that can be independently manipulated. To bridge this gap, we introduce PartGen, a novel approach for generating, from text, images, or unstructured 3D objects, 3D objects composed of meaningful parts. Our method leverages a multi-view diffusion model to extract plausible and view-consistent part segmentations from multiple views of a 3D object, dividing it into meaningful components. A second multi-view diffusion model then processes each part individually, filling in occlusions and generating completed views, which are subsequently passed to a 3D reconstruction network. The completion process ensures that the reconstructed parts integrate cohesively by considering the context of the entire object, compensating for missing information caused by occlusions and, in extreme cases, hallucinating entirely invisible parts based on contextual cues. We evaluate PartGen on both generated and real 3D assets, demonstrating significant improvements over segmentation and part completion baselines. We also showcase downstream applications such as text-guided 3D part editing",
    "checked": true,
    "id": "cc5edf846dcaa15e45bcf76c930f63d8bf5f4ba2",
    "semantic_title": "partgen: part-level 3d generation and reconstruction with multi-view diffusion models",
    "citation_count": 16,
    "authors": [
      "Minghao Chen",
      "Roman Shapovalov",
      "Iro Laina",
      "Tom Monnier",
      "Jianyuan Wang",
      "David Novotny",
      "Andrea Vedaldi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_FedCALM_Conflict-aware_Layer-wise_Mitigation_for_Selective_Aggregation_in_Deeper_Personalized_CVPR_2025_paper.html": {
    "title": "FedCALM: Conflict-aware Layer-wise Mitigation for Selective Aggregation in Deeper Personalized Federated Learning",
    "volume": "main",
    "abstract": "Server aggregation conflict is a key challenge in personalized federated learning (PFL). While existing PFL methods have achieved significant progress with shallow base models (e.g., four-layer CNNs), they often overlook the negative impacts of deeper base models on personalization mechanisms. In this paper, we identify the phenomenon of deep model degradation in PFL, where as base model depth increases, the model becomes more sensitive to local client data distributions, thereby exacerbating server aggregation conflicts and ultimately reducing overall model performance. Moreover, we show that these conflicts manifest in insufficient global average updates and mutual constraints between clients. Motivated by our analysis, we proposed a two-stage conflict-aware layer-wise mitigation algorithm (FedCALM), which first constructs a conflict-free global update to alleviate negative conflicts, and then maximizes the benefits of all clients through a conflict-aware strategy. Notably, our method naturally leads to a selective mechanism that balances the tradeoff between clients involved in aggregation and the tolerance for conflicts. Consequently, it can boost the positive contribution to the clients even with the greatest conflicts with the global update. Extensive experiments across multiple datasets and deeper base models demonstrate that FedCALM outperforms four state-of-the-art (SOTA) methods by up to 9.88% and seamlessly integrates into existing PFL methods with performance improvements of up to 9.01%",
    "checked": true,
    "id": "61cf21955d583438ee692cfe013b730597cd036d",
    "semantic_title": "fedcalm: conflict-aware layer-wise mitigation for selective aggregation in deeper personalized federated learning",
    "citation_count": 0,
    "authors": [
      "Hao Zheng",
      "Zhigang Hu",
      "Liu Yang",
      "Meiguang Zheng",
      "Aikun Xu",
      "Boyu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jayasundara_SINR_Sparsity_Driven_Compressed_Implicit_Neural_Representations_CVPR_2025_paper.html": {
    "title": "SINR: Sparsity Driven Compressed Implicit Neural Representations",
    "volume": "main",
    "abstract": "Implicit Neural Representations (INRs) are increasingly recognized as a versatile data modality for representing discretized signals, offering benefits such as infinite query resolution and reduced storage requirements. Existing signal compression approaches for INRs typically employ one of two strategies: 1. direct quantization with entropy coding of the trained INR; 2. deriving a latent code on top of the INR through a learnable transformation. Thus, their performance is heavily dependent on the quantization and entropy coding schemes employed. In this paper, we introduce SINR, an innovative compression algorithm that leverages the patterns in the vector spaces formed by weights of INRs. We compress these vector spaces using a high-dimensional sparse code within a dictionary. Further analysis reveals that the atoms of the dictionary used to generate the sparse code do not need to be learned or transmitted to successfully recover the INR weights. We demonstrate that the proposed approach can be integrated with any existing INR-based signal compression technique. Our results indicate that SINR achieves substantial reductions in storage requirements for INRs across various configurations, outperforming conventional INR-based compression baselines. Furthermore, SINR maintains high-quality decoding across diverse data modalities, including images, occupancy fields, and Neural Radiance Fields",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dhananjaya Jayasundara",
      "Sudarshan Rajagopalan",
      "Yasiru Ranasinghe",
      "Trac D. Tran",
      "Vishal M. Patel"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qu_CaricatureBooth_Data-Free_Interactive_Caricature_Generation_in_a_Photo_Booth_CVPR_2025_paper.html": {
    "title": "CaricatureBooth: Data-Free Interactive Caricature Generation in a Photo Booth",
    "volume": "main",
    "abstract": "We present CaricatureBooth, a system that transforms caricature creation into a simple interactive experience -- as easy as using a photo booth! A key challenge in caricature generation is two-fold: the scarcity of high-quality caricature data and the difficulty in enabling precise creative control over the exaggeration process while maintaining identity. Prior approaches either require large-scale caricature and photo data or lack intuitive mechanisms for users to guide the deformation without losing identity. We address the data scarcity by synthesising training data through Thin Plate Spline (TPS) deformation of standard face images. For creative control, we design a Bezier curve interface where users can easily manipulate facial features, with these edits then driving TPS transformations at inference time. When combined with a pre-trained ID-preserving diffusion model, our system maintains both identity preservation and creative flexibility. Through extensive experiments, we demonstrate that CaricatureBooth achieves state-of-the-art quality while making the joy of caricature creation as accessible as taking a photo -- just walk in and walk out with your personalised caricature! Code is available at https://github.com/WinKawaks/CaricatureBooth",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyu Qu",
      "Yunqi Miao",
      "Zhensong Zhang",
      "Jifei Song",
      "Jiankang Deng",
      "Yi-Zhe Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_FlexGS_Train_Once_Deploy_Everywhere_with_Many-in-One_Flexible_3D_Gaussian_CVPR_2025_paper.html": {
    "title": "FlexGS: Train Once, Deploy Everywhere with Many-in-One Flexible 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "3D Gaussian splatting (3DGS) has enabled various applications in 3D scene representation and novel view synthesis due to its efficient rendering capabilities. However, 3DGS demands significant GPU memory, limiting its use on devices with restricted computational resources. Previous approaches have focused on pruning less important Gaussians, effectively compressing 3DGS but often requiring a fine-tuning stage and lacking adaptability for the specific memory needs of different devices. In this work, we present an elastic inference method for 3DGS. Given an input for the desired model size, our method selects and transforms a subset of Gaussians, achieving substantial rendering performance without additional fine-tuning. We introduce a tiny learnable module that controls Gaussian selection based on the input percentage, along with a transformation module that adjusts the selected Gaussians to complement the performance of the reduced model. Comprehensive experiments on ZipNeRF, MipNeRF and Tanks&Temples scenes demonstrate the effectiveness of our approach. Code will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hengyu Liu",
      "Yuehao Wang",
      "Chenxin Li",
      "Ruisi Cai",
      "Kevin Wang",
      "Wuyang Li",
      "Pavlo Molchanov",
      "Peihao Wang",
      "Zhangyang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Generalizing_Deepfake_Video_Detection_with_Plug-and-Play_Video-Level_Blending_and_Spatiotemporal_CVPR_2025_paper.html": {
    "title": "Generalizing Deepfake Video Detection with Plug-and-Play: Video-Level Blending and Spatiotemporal Adapter Tuning",
    "volume": "main",
    "abstract": "Three key challenges hinder the development of current deepfake video detection: (1) Temporal features can be complex and diverse: how can we identify general temporal artifacts to enhance model generalization? (2) Spatiotemporal models often lean heavily on one type of artifact and ignore the other: how can we ensure balanced learning from both? (3) Videos are naturally resource-intensive: how can we tackle efficiency without compromising accuracy? This paper attempts to tackle the three challenges jointly. First, inspired by the notable generality of using image-level blending data for image forgery detection, we investigate whether and how video-level blending can be effective in video. We then perform a thorough analysis and identify a previously underexplored temporal forgery artifact: Facial Feature Drift (FFD), which commonly exists across different forgeries. To reproduce FFD, we then propose a novel Video-level Blending data (VB), where VB is implemented by blending the original image and its warped version frame-by-frame, serving as a hard negative sample to mine more general artifacts. Second, we carefully design a lightweight Spatiotemporal Adapter (StA) to equip a pre-trained image model with the ability to capture both spatial and temporal features jointly and efficiently. StA is designed with two-stream 3D-Conv with varying kernel sizes, allowing it to process spatial and temporal features separately. This eliminates the need to design a new deepfake-specific video architecture from scratch. Extensive experiments validate the effectiveness of the proposed methods; and show our approach can generalize well to previously unseen forgery videos",
    "checked": true,
    "id": "219e0b8a33f602f6cb6650497ddda866789e916e",
    "semantic_title": "generalizing deepfake video detection with plug-and-play: video-level blending and spatiotemporal adapter tuning",
    "citation_count": 23,
    "authors": [
      "Zhiyuan Yan",
      "Yandan Zhao",
      "Shen Chen",
      "Mingyi Guo",
      "Xinghe Fu",
      "Taiping Yao",
      "Shouhong Ding",
      "Yunsheng Wu",
      "Li Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_ManipTrans_Efficient_Dexterous_Bimanual_Manipulation_Transfer_via_Residual_Learning_CVPR_2025_paper.html": {
    "title": "ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning",
    "volume": "main",
    "abstract": "Human hands play a central role in interacting, motivating increasing research in dexterous robotic manipulation. Data-driven embodied AI algorithms demand precise, large-scale, human-like manipulation sequences, which are challenging to obtain with conventional reinforcement learning or real-world teleoperation. To address this, we introduce ManipTrans, a novel two-stage method for efficiently transferring human bimanual skills to dexterous robotic hands in simulation. ManipTrans first pre-trains a generalist trajectory imitator to mimic hand motion, then fine-tunes a specific residual module under interaction constraints, enabling efficient learning and accurate execution of complex bimanual tasks. Experiments show that ManipTrans surpasses state-of-the-art methods in success rate, fidelity, and efficiency. Leveraging ManipTrans, we transfer multiple hand-object datasets to robotic hands, creating DexManipNet, a large-scale dataset featuring previously unexplored tasks like pen capping and bottle unscrewing. DexManipNet comprises 3.3K episodes of robotic manipulation and is easily extensible, facilitating further policy training for dexterous hands and enabling real-world deployments",
    "checked": true,
    "id": "d50a6b10f4d115a137c3933fb79a49790f503206",
    "semantic_title": "maniptrans: efficient dexterous bimanual manipulation transfer via residual learning",
    "citation_count": 18,
    "authors": [
      "Kailin Li",
      "Puhao Li",
      "Tengyu Liu",
      "Yuyang Li",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Precise_Fast_and_Low-cost_Concept_Erasure_in_Value_Space__CVPR_2025_paper.html": {
    "title": "Precise, Fast, and Low-cost Concept Erasure in Value Space: Orthogonal Complement Matters",
    "volume": "main",
    "abstract": "The success of text-to-image generation enabled by diffusion models has imposed an urgent need to erase unwanted concepts, e.g., copyrighted, offensive, and unsafe ones, from the pre-trained models in a precise, timely, and low-cost manner. The twofold demand of concept erasure requires a precise removal of the target concept during generation (i.e., erasure efficacy), while a minimal impact on non-target content generation (i.e., prior preservation). Existing methods are either computationally costly or face challenges in maintaining an effective balance between erasure efficacy and prior preservation. To improve, we propose a precise, fast, and low-cost concept erasure method, called Adaptive Vaule Decomposer (AdaVD), which is training-free. This method is grounded in a classical linear algebraic orthogonal complement operation, implemented in the value space of each cross-attention layer within the UNet of diffusion models. An effective shift factor is designed to adaptively navigate the erasure strength, enhancing prior preservation without sacrificing erasure efficacy. Extensive experimental results show that the proposed AdaVD is effective at both single and multiple concept erasure, showing a 2- to 10-fold improvement in prior preservation as compared to the second best, meanwhile achieving the best or near best erasure efficacy, when comparing with both training-based and training-free state of the arts. AdaVD supports a series of diffusion models and downstream image generation tasks, with code to be publicly available",
    "checked": true,
    "id": "e74b997e990e5d0eaff9bb68237ed89dbd98dc3b",
    "semantic_title": "precise, fast, and low-cost concept erasure in value space: orthogonal complement matters",
    "citation_count": 6,
    "authors": [
      "Yuan Wang",
      "Ouxiang Li",
      "Tingting Mu",
      "Yanbin Hao",
      "Kuien Liu",
      "Xiang Wang",
      "Xiangnan He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_HOIGen-1M_A_Large-scale_Dataset_for_Human-Object_Interaction_Video_Generation_CVPR_2025_paper.html": {
    "title": "HOIGen-1M: A Large-scale Dataset for Human-Object Interaction Video Generation",
    "volume": "main",
    "abstract": "Text-to-video (T2V) generation has made tremendous progress in generating complicated scenes based on texts. However, human-object interaction (HOI) often cannot be precisely generated by current T2V models due to the lack of large-scale videos with accurate captions for HOI. To address this issue, we introduce HOIGen-1M, the first large-scale dataset for HOI Generation, consisting of over one million high-quality videos collected from diverse sources. In particular, to guarantee the high quality of videos, we first design an efficient framework to automatically curate HOI videos using the powerful multimodal large language models (MLLMs), and then the videos are further cleaned by human annotators. Moreover, to obtain accurate textual captions for HOI videos, we design a novel video description method based on a Mixture-of-Multimodal-Experts (MoME) strategy that not only generates expressive captions but also eliminates the hallucination by individual MLLM. Furthermore, due to the lack of an evaluation framework for generated HOI videos, we propose two new metrics to assess the quality of generated videos in a coarse-to-fine manner. Extensive experiments reveal that current T2V models struggle to generate high-quality HOI videos and confirm that our HOIGen-1M dataset is instrumental for improving HOI video generation",
    "checked": true,
    "id": "0a9b0eeb6e6cb1bfa3f4cc74f95010ca384dcc41",
    "semantic_title": "hoigen-1m: a large-scale dataset for human-object interaction video generation",
    "citation_count": 4,
    "authors": [
      "Kun Liu",
      "Qi Liu",
      "Xinchen Liu",
      "Jie Li",
      "Yongdong Zhang",
      "Jiebo Luo",
      "Xiaodong He",
      "Wu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_T2ISafety_Benchmark_for_Assessing_Fairness_Toxicity_and_Privacy_in_Image_CVPR_2025_paper.html": {
    "title": "T2ISafety: Benchmark for Assessing Fairness, Toxicity, and Privacy in Image Generation",
    "volume": "main",
    "abstract": "Text-to-image (T2I) models have rapidly advanced, enabling the generation of high-quality images from text prompts across various domains. However, these models present notable safety concerns, including the risk of generating harmful, biased, or private content. Current research on assessing T2I safety remains in its early stages. While some efforts have been made to evaluate models on specific safety dimensions, many critical risks remain unexplored. To address this gap, we introduce T2ISafety, a safety benchmark that evaluates T2I models across three key domains: toxicity, fairness, and bias. We build a detailed hierarchy of 12 tasks and 44 categories based on these three domains, and meticulously collect 70K corresponding prompts. Based on this taxonomy and prompt set, we build a large-scale T2I dataset with 68K manually annotated images and train an evaluator capable of detecting critical risks that previous work has failed to identify, including risks that even ultra-large proprietary models like GPTs cannot correctly detect. We evaluate 15 prominent diffusion models on T2ISafety and reveal several concerns including persistent issues with racial fairness, a tendency to generate toxic content, and significant variation in privacy protection across the models, even with defense methods like concept erasing",
    "checked": true,
    "id": "26e14ae20eb349c2b53d7a32b40be11a3f18a2bf",
    "semantic_title": "t2isafety: benchmark for assessing fairness, toxicity, and privacy in image generation",
    "citation_count": 14,
    "authors": [
      "Lijun Li",
      "Zhelun Shi",
      "Xuhao Hu",
      "Bowen Dong",
      "Yiran Qin",
      "Xihui Liu",
      "Lu Sheng",
      "Jing Shao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hahn_Order-One_Rolling_Shutter_Cameras_CVPR_2025_paper.html": {
    "title": "Order-One Rolling Shutter Cameras",
    "volume": "main",
    "abstract": "Rolling shutter (RS) cameras dominate consumer and smartphone markets. Several methods for computing the absolute pose of RS cameras have appeared in the last 20 years, but the relative pose problem has not been fully solved yet. We provide a unified theory for the important class of order-one rolling shutter (RS_1) cameras. These cameras generalize the perspective projection to RS cameras, projecting a generic space point to exactly one image point via a rational map. We introduce a new back-projection RS camera model, characterize RS_1 cameras, construct explicit parameterizations of such cameras, and determine the image of a space line. We classify all minimal problems for solving the relative camera pose problem with linear RS_1 cameras and discover new practical cases. Finally, we show how the theory can be used to explain RS models previously used for absolute pose computation",
    "checked": true,
    "id": "5e02461e4fbd636bfe8ebc08098a8747781c6145",
    "semantic_title": "order-one rolling shutter cameras",
    "citation_count": 4,
    "authors": [
      "Marvin Anas Hahn",
      "Kathl√©n Kohn",
      "Orlando Marigliano",
      "Tomas Pajdla"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Animate_and_Sound_an_Image_CVPR_2025_paper.html": {
    "title": "Animate and Sound an Image",
    "volume": "main",
    "abstract": "This paper addresses a promising yet underexplored task, Image-to-Sounding-Video (I2SV) generation, which animates a static image and generates synchronized sound simultaneously. Despite advances in video and audio generation models, challenges remain to develop a unified model for generating naturally sounding videos. In this work, we propose a novel approach that leverages two separate pretrained diffusion models and makes vision and audio influence each other during generation based on the Diffusion Transformer (DiT) architecture. First, the individual video and audio pretrained generation models are decomposed into input, output, and expert sub-modules. We propose using a unified joint DiT block to integrate the expert sub-modules to effectively model the interaction between the two modalities, resulting in high-quality I2SV generation. Then, we introduce a joint classifier-free guidance technique to boost the performance during joint generation. Finally, we conduct extensive experiments on three popular benchmark datasets, and in both objective and subjective evaluation our method surpass all the baseline methods in almost all metrics. Case studies show our generated sounding videos are high quality and synchronized between video and audio",
    "checked": true,
    "id": "c307f31bdf2b9f1e6d6cc4acaed8d4bcffd73882",
    "semantic_title": "animate and sound an image",
    "citation_count": 0,
    "authors": [
      "Xihua Wang",
      "Ruihua Song",
      "Chongxuan Li",
      "Xin Cheng",
      "Boyuan Li",
      "Yihan Wu",
      "Yuyue Wang",
      "Hongteng Xu",
      "Yunfeng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Miao_Shining_Yourself_High-Fidelity_Ornaments_Virtual_Try-on_with_Diffusion_Model_CVPR_2025_paper.html": {
    "title": "Shining Yourself: High-Fidelity Ornaments Virtual Try-on with Diffusion Model",
    "volume": "main",
    "abstract": "While virtual try-on for clothes and shoes with diffusion models has gained attraction, virtual try-on for ornaments, such as bracelets, rings, earrings, and necklaces, remains largely unexplored. Due to the intricate tiny patterns and repeated geometric sub-structures in most ornaments, it is much more difficult to guarantee identity and appearance consistency under large pose and scale variances between ornaments and models. This paper proposes the task of virtual try-on for ornaments and presents a method to improve the geometric and appearance preservation of ornament virtual try-ons. Specifically, we estimate an accurate wearing mask to improve the alignments between ornaments and models in an iterative scheme alongside the denoising process. To preserve structure details, we further regularize attention layers to map the reference ornament mask to the wearing mask in an implicit way. Experiment results demonstrate that our method successfully wears ornaments from reference images onto target models, handling substantial differences in scale and pose while preserving identity and achieving realistic visual effects",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingmao Miao",
      "Zhanpeng Huang",
      "Rui Han",
      "Zibin Wang",
      "Chenhao Lin",
      "Chao Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zeng_Foveated_Instance_Segmentation_CVPR_2025_paper.html": {
    "title": "Foveated Instance Segmentation",
    "volume": "main",
    "abstract": "Instance segmentation is essential for augmented reality and virtual reality (AR/VR) as it enables precise object recognition and interaction, enhancing the integration of virtual and real-world elements for an immersive experience. However, the high computational overhead of segmentation limits its application on resource-constrained AR/VR devices, causing large processing latency and degrading user experience. In contrast to conventional scenarios, AR/VR users typically focus on only a few regions within their field of view before shifting perspective, allowing segmentation to be concentrated on gaze-specific areas. This insight drives the need for efficient segmentation methods that prioritize processing instance of interest, reducing computational load and enhancing real-time performance. In this paper, we present a foveated instance segmentation(FovealSeg) framework that leverages real-time user gaze data to perform instance segmentation exclusively on instance of interest, resulting in substantial computational savings. Evaluation results show that FSNet achieves an IoU of 0.56 on ADE20K and 0.54 on LVIS, notably outperforming the baseline. The code is available at https://github.com/SAI-Lab-NYU/Foveated-Instance-Segmentation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyi Zeng",
      "Wenxuan Liu",
      "Tianhua Xia",
      "Jinhui Chen",
      "Ziyun Li",
      "Sai Qian Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Binyamin_Make_It_Count_Text-to-Image_Generation_with_an_Accurate_Number_of_CVPR_2025_paper.html": {
    "title": "Make It Count: Text-to-Image Generation with an Accurate Number of Objects",
    "volume": "main",
    "abstract": "Despite the unprecedented success of text-to-image diffusion models, controlling the number of depicted objects using text is surprisingly hard. This is important for various applications from technical documents, to children's books to illustrating cooking recipes. Generating object-correct counts is fundamentally challenging because the generative model needs to keep a sense of separate identity for every instance of the object, even if several objects look identical or overlap, and then carry out a global computation implicitly during generation. It is still unknown if such representations exist. To address count-correct generation, we first identify features within the diffusion model that can carry the object identity information. We then use them to separate and count instances of objects during the denoising process and detect over-generation and under-generation. We fix the latter by training a model that predicts both the shape and location of a missing object, based on the layout of existing ones, and show how it can be used to guide denoising with correct object count. Our approach, CountGen, does not depend on external source to determine object layout, but rather uses the prior from the diffusion model itself, creating prompt-dependent and seed-dependent layouts. Evaluated on two benchmark datasets, we find that CountGen strongly outperforms the count-accuracy of existing baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lital Binyamin",
      "Yoad Tewel",
      "Hilit Segev",
      "Eran Hirsch",
      "Royi Rassin",
      "Gal Chechik"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Choe_Universal_Domain_Adaptation_for_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Universal Domain Adaptation for Semantic Segmentation",
    "volume": "main",
    "abstract": "Unsupervised domain adaptation for semantic segmentation (UDA-SS) aims to transfer knowledge from labeled synthetic data (source) to unlabeled real-world data (target). Traditional UDA-SS methods work on the assumption that the category settings between the source and target domains are known in advance. However, in real-world scenarios, the category settings of the source and target are unknown due to the lack of labels in the target, resulting in the existence of target-private or source-private classes. Traditional UDA-SS methods struggle with this change, leading to negative transfer and performance degradation. To address these issues, we propose Universal Domain Adaptation for Semantic Segmentation (UniDA-SS) for the first time to achieve good performance even when the category settings of source and target are unknown. We defined the problem in the UniDA-SS scenario as that the confidence score of common Unsupervised domain adaptation for semantic segmentation (UDA-SS) aims to transfer knowledge from labeled source data to unlabeled target data. However, traditional UDA-SS methods assume that category settings between source and target domains are known, which is unrealistic in real-world scenarios. This leads to performance degradation if private private classes exist. To address this limitation, we propose Universal Domain Adaptation for Semantic Segmentation (UniDA-SS), achieving robust adaptation even without prior knowledge of category settings. We define the problem in the UniDA-SS scenario as low confidence scores of common classes in the target domain, which leads to confusion with private classes. To solve this problem, we propose UniMAP: UniDA-SS with Image Ma tching and Prototype-based Distinction, a novel framework composed of two key components. First, Domain-Specific Prototype-based Distinction (DSPD) divides each class into two domain-specific prototypes, enabling finer separation of domain-specific features and enhancing the identification of common classes across domains. Second, Target-based Image Matching (TIM) selects a source image containing the most common-class pixels based on the target pseudo-label and pairs it in a batch to promote effective learning of common classes. We also introduce a new UniDA-SS benchmark and demonstrate through various experiments that UniMAP significantly outperforms baselines. The code is available at https://github.com/KU-VGI/UniDA-SS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seun-An Choe",
      "Keon-Hee Park",
      "Jinwoo Choi",
      "Gyeong-Moon Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Thirgood_HyperGS_Hyperspectral_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "HyperGS: Hyperspectral 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "We introduce HyperGS, a novel framework for Hyperspectral Novel View Synthesis (HNVS), based on a new latent 3D Gaussian Splatting (3DGS) technique. Our approach enables simultaneous spatial and spectral renderings by encoding material properties from multi-view 3D hyperspectral datasets. HyperGS reconstructs high-fidelity views from arbitrary perspectives with improved accuracy and speed, outperforming currently existing methods. To address the challenges of high-dimensional data, we perform view synthesis in a learned latent space, incorporating a pixel-wise adaptive density function and a pruning technique for increased training stability and efficiency. Additionally, we introduce the first HNVS benchmark, implementing a number of new baselines based on recent SOTA RGB-NVS techniques, alongside the small number of prior works on HNVS. We demonstrate HyperGS's robustness through extensive evaluation of real and simulated hyperspectral scenes with a 14dB accuracy improvement upon previously published models",
    "checked": true,
    "id": "40388591287f5c708d28cd4be5555a71202d8a68",
    "semantic_title": "hypergs: hyperspectral 3d gaussian splatting",
    "citation_count": 4,
    "authors": [
      "Christopher Thirgood",
      "Oscar Mendez",
      "Erin Ling",
      "Jon Storey",
      "Simon Hadfield"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Emphasizing_Discriminative_Features_for_Dataset_Distillation_in_Complex_Scenarios_CVPR_2025_paper.html": {
    "title": "Emphasizing Discriminative Features for Dataset Distillation in Complex Scenarios",
    "volume": "main",
    "abstract": "Dataset distillation has demonstrated strong performance on simple datasets like CIFAR, MNIST, and TinyImageNet but struggles to achieve similar results in more complex scenarios. In this paper, we propose EDF (emphasizes the discriminative features), a dataset distillation method that enhances key discriminative regions in synthetic images using Grad-CAM activation maps. Our approach is inspired by a key observation: in simple datasets, high-activation areas typically occupy most of the image, whereas in complex scenarios, the size of these areas is much smaller. Unlike previous methods that treat all pixels equally when synthesizing images, EDF uses Grad-CAM activation maps to enhance high-activation areas. From a supervision perspective, we downplay supervision signals produced by lower trajectory-matching losses, as they contain common patterns. Additionally, to help the DD community better explore complex scenarios, we build the Complex Dataset Distillation (Comp-DD) benchmark by meticulously selecting sixteen subsets, eight easy and eight hard, from ImageNet-1K. In particular, EDF consistently outperforms SOTA results in complex scenarios, such as ImageNet-1K subsets. Hopefully, more researchers will be inspired and encouraged to improve the practicality and efficacy of DD. Our code and benchmark have been made public at NUS-HPC-AI-Lab/EDF",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Wang",
      "Zekai Li",
      "Zhi-Qi Cheng",
      "Samir Khaki",
      "Ahmad Sajedi",
      "Ramakrishna Vedantam",
      "Konstantinos N Plataniotis",
      "Alexander Hauptmann",
      "Yang You"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_LMO_Linear_Mamba_Operator_for_MRI_Reconstruction_CVPR_2025_paper.html": {
    "title": "LMO: Linear Mamba Operator for MRI Reconstruction",
    "volume": "main",
    "abstract": "Interpretability and consistency have long been crucial factors in MRI reconstruction. While interpretability has been significantly innovated with the emerging deep unfolding networks, current solutions still suffer from inconsistency issues and produce inferior anatomical structures. Especially in out-of-distribution cases, e.g., when the acceleration rate (AR) varies, the generalization performance is often catastrophic. To counteract the dilemma, we propose an innovative Linear Mamba Operator (LMO) to ensure consistency and generalization, while still enjoying desirable interpretability. Theoretically, we argue that mapping between function spaces, rather than between signal instances, provides a solid foundation of high generalization. Technically, LMO achieves a good balance between global integration facilitated by a state space model that scans the whole function domain, and local integration engaged with an appealing property of continuous-discrete equivalence. On that basis, learning holistic features can be guaranteed, tapping the potential of maximizing data consistency. Quantitative and qualitative results demonstrate that LMO significantly outperforms other state-of-the-arts. More importantly, LMO is the unique model that, with AR changed, achieves retraining performance without retraining steps. Codes are available at https://github.com/ZhengJianwei2/LMO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Li",
      "Jiawei Jiang",
      "Jie Wu",
      "Kaihao Yu",
      "Jianwei Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_AnomalyNCD_Towards_Novel_Anomaly_Class_Discovery_in_Industrial_Scenarios_CVPR_2025_paper.html": {
    "title": "AnomalyNCD: Towards Novel Anomaly Class Discovery in Industrial Scenarios",
    "volume": "main",
    "abstract": "Recently, multi-class anomaly classification has garnered increasing attention. Previous methods directly cluster anomalies but often struggle due to the lack of anomaly-prior knowledge. Acquiring this knowledge faces two issues: the non-prominent and weak-semantics anomalies. In this paper, we propose AnomalyNCD, a multi-class anomaly classification network compatible with different anomaly detection methods. To address the non-prominence of anomalies, we design main element binarization (MEBin) to obtain anomaly-centered images, ensuring anomalies are learned while avoiding the impact of incorrect detections. Next, to learn anomalies with weak semantics, we design mask-guided representation learning, which focuses on isolated anomalies guided by masks and reduces confusion from erroneous inputs through corrected pseudo labels. Finally, to enable flexible classification at both region and image levels, we develop a region merging strategy that determines the overall image category based on the classified anomaly regions. Our method outperforms the state-of-the-art works on the MVTec AD and MTD datasets. Compared with the current methods, AnomalyNCD combined with zero-shot anomaly detection method achieves a 10.8% F1 gain, 8.8% NMI gain, and 9.5% ARI gain on MVTec AD, and 12.8% F1 gain, 5.7% NMI gain, and 10.8% ARI gain on MTD. Code is available at https://github.com/HUST-SLOW/AnomalyNCD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziming Huang",
      "Xurui Li",
      "Haotian Liu",
      "Feng Xue",
      "Yuzhe Wang",
      "Yu Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Schmidt_Segment_This_Thing_Foveated_Tokenization_for_Efficient_Point-Prompted_Segmentation_CVPR_2025_paper.html": {
    "title": "Segment This Thing: Foveated Tokenization for Efficient Point-Prompted Segmentation",
    "volume": "main",
    "abstract": "This paper presents Segment This Thing (STT), a new efficient image segmentation model designed to produce a single segment given a single point prompt. Instead of following prior work and increasing efficiency by decreasing model size, we gain efficiency by foveating input images. Given an image and a point prompt, we extract a crop centered on the prompt and apply a novel variable-resolution patch tokenization in which patches are downsampled at a rate that increases with increased distance from the prompt. This approach yields far fewer image tokens than uniform patch tokenization. As a result we can drastically reduce the computational cost of segmentation without reducing model size. Furthermore, the foveation focuses the model on the region of interest, a potentially useful inductive bias. We show that our Segment This Thing model is more efficient than prior work while remaining competitive on segmentation benchmarks. It can easily run at interactive frame rates on consumer hardware and is thus a promising tool for augmented reality or robotics applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanner Schmidt",
      "Richard Newcombe"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Task-Specific_Gradient_Adaptation_for_Few-Shot_One-Class_Classification_CVPR_2025_paper.html": {
    "title": "Task-Specific Gradient Adaptation for Few-Shot One-Class Classification",
    "volume": "main",
    "abstract": "Optimization-based meta-learning methods for few-shot one-class classification (FS-OCC) aim to fine-tune a meta-trained model to classify the positive and negative samples using only a few positive samples by adaptation. However, recent approaches primarily focus on adjusting existing meta-learning algorithms for FS-OCC, while overlooking issues stemming from the misalignment between the cross-entropy loss and OCC tasks during adaptation. This misalignment, combined with the limited availability of one-class samples and the restricted diversity of task-specific adaptation, can significantly exacerbate the adverse effects of gradient instability and generalization. To address these challenges, we propose a novel Task-Specific Gradient Adaptation (TSGA) for FS-OCC. Without extra supervision, TSGA learns to generate appropriate, stable gradients by leveraging label prediction and feature representation details of one-class samples and refines the adaptation process by recalibrating task-specific gradients and regularization terms. We evaluate TSGA on three challenging datasets and a real-world CNC Milling Machine application and demonstrate consistent improvements over baseline methods. Furthermore, we illustrate the critical impact of gradient instability and task-agnostic adaptation. Notably, TSGA achieves state-of-the-art results by effectively addressing these issues",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunlong Li",
      "Xiabi Liu",
      "Liyuan Pan",
      "Yuchen Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Song_TraF-Align_Trajectory-aware_Feature_Alignment_for_Asynchronous_Multi-agent_Perception_CVPR_2025_paper.html": {
    "title": "TraF-Align: Trajectory-aware Feature Alignment for Asynchronous Multi-agent Perception",
    "volume": "main",
    "abstract": "Cooperative perception presents significant potential for enhancing the sensing capabilities of individual vehicles, however, inter-agent latency remains a critical challenge. Latencies cause misalignments in both spatial and semantic features, complicating the fusion of real-time observations from the ego vehicle with delayed data from others. To address these issues, we propose TraF-Align, a novel framework that learns the flow path of features by predicting the feature-level trajectory of objects from past observations up to the ego vehicle's current time. By generating temporally ordered sampling points along these paths, TraF-Align directs attention from the current-time query to relevant historical features along each trajectory, supporting the reconstruction of current-time features and promoting semantic interaction across multiple frames. This approach corrects spatial misalignment and ensures semantic consistency across agents, effectively compensating for motion and achieving coherent feature fusion. Experiments on two real-world datasets, V2V4Real and DAIR-V2X-Seq, show that TraF-Align sets a new benchmark for asynchronous cooperative perception",
    "checked": true,
    "id": "a5819666ebd7e65a2c52de3b684a16b1ca5addb9",
    "semantic_title": "traf-align: trajectory-aware feature alignment for asynchronous multi-agent perception",
    "citation_count": 4,
    "authors": [
      "Zhiying Song",
      "Lei Yang",
      "Fuxi Wen",
      "Jun Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Aiello_DreamCache_Finetuning-Free_Lightweight_Personalized_Image_Generation_via_Feature_Caching_CVPR_2025_paper.html": {
    "title": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation via Feature Caching",
    "volume": "main",
    "abstract": "Personalized image generation requires text-to-image generative models that capture the core features of a reference subject to allow for controlled generation across different contexts. Existing methods face challenges due to complex training requirements, high inference costs, limited flexibility, or a combination of these issues. In this paper, we introduce DreamCache, a scalable approach for efficient and high-quality personalized image generation. By caching a small number of reference image features from a subset of layers and a single timestep of the pretrained diffusion denoiser, DreamCache enables dynamic modulation of the generated image features through lightweight, trained conditioning adapters. DreamCache achieves state-of-the-art image and text alignment, utilizing an order of magnitude fewer extra parameters, and is both more computationally effective and versatile than existing models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emanuele Aiello",
      "Umberto Michieli",
      "Diego Valsesia",
      "Mete Ozay",
      "Enrico Magli"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_3D_Gaussian_Inpainting_with_Depth-Guided_Cross-View_Consistency_CVPR_2025_paper.html": {
    "title": "3D Gaussian Inpainting with Depth-Guided Cross-View Consistency",
    "volume": "main",
    "abstract": "When performing 3D inpainting using novel-view rendering methods like Neural Radiance Field (NeRF) or 3D Gussian Splatting (3DGS), how to achieve texture and geometry consistency across camera views has been a challenge. In this paper, we propose a framework of 3D Gaussian Inpainting with Depth-Guided Cross-View Consistency (3DGIC) for cross-view consistent 3D inpainting. Guided by the rendered depth information from each training view, our 3DGIC exploits background pixels visible across different views for updating the inpainting mask, allowing us to refine the 3DGS for inpainting purposes. Through extensive experiments on benchmark datasets, we confirm that our 3DGIC outperforms current state-of-the-art 3D inpainting methods quantitatively and qualitatively",
    "checked": true,
    "id": "0f62bdb807989029547e24e4dd2eeb507137653e",
    "semantic_title": "3d gaussian inpainting with depth-guided cross-view consistency",
    "citation_count": 6,
    "authors": [
      "Sheng-Yu Huang",
      "Zi-Ting Chou",
      "Yu-Chiang Frank Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kang_Your_Large_Vision-Language_Model_Only_Needs_A_Few_Attention_Heads_CVPR_2025_paper.html": {
    "title": "Your Large Vision-Language Model Only Needs A Few Attention Heads For Visual Grounding",
    "volume": "main",
    "abstract": "Visual grounding seeks to localize the image region corresponding to a free-form text description. Recently, the strong multimodal capabilities of Large Vision-Language Models (LVLMs) have driven substantial improvements in visual grounding, though they inevitably require fine-tuning and additional model components to explicitly generate bounding boxes or segmentation masks. However, we discover that a few attention heads in frozen LVLMs demonstrate strong visual grounding capabilities. We refer to these heads, which consistently capture object locations related to text semantics, as localization heads. Using localization heads, we introduce a straightforward and effective training-free visual grounding framework that utilizes text-to-image attention maps from localization heads to identify the target objects. Surprisingly, only three out of thousands of attention heads are sufficient to achieve competitive localization performance compared to existing LVLM-based visual grounding methods that require fine-tuning. Our findings suggest that LVLMs can innately ground objects based on a deep comprehension of the text-image relationship, as they implicitly focus on relevant image regions to generate informative text outputs",
    "checked": true,
    "id": "5ac09c2d06683c72af2c19a2b02af817be1d2a40",
    "semantic_title": "your large vision-language model only needs a few attention heads for visual grounding",
    "citation_count": 10,
    "authors": [
      "Seil Kang",
      "Jinyeong Kim",
      "Junhyeok Kim",
      "Seong Jae Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_FlexUOD_The_Answer_to_Real-world_Unsupervised_Image_Outlier_Detection_CVPR_2025_paper.html": {
    "title": "FlexUOD: The Answer to Real-world Unsupervised Image Outlier Detection",
    "volume": "main",
    "abstract": "How many outliers are within an unlabeled and contaminated dataset? Despite a series of unsupervised outlier detection (UOD) approaches have been proposed, they cannot correctly answer this critical question, resulting in their performance instability across various real-world (varying contamination factor) scenarios. To address this problem, we propose FlexUOD, with a novel contamination factor estimation perspective. FlexUOD not only achieves its remarkable robustness but also is a general and plug-and-play framework, which can significantly improve the performance of existing UOD methods. Extensive experiments demonstrate that FlexUOD achieves state-of-the-art results as well as high efficacy on diverse evaluation benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhonghang Liu",
      "Kun Zhou",
      "Changshuo Wang",
      "Wen-Yan Lin",
      "Jiangbo Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mane_Ges3ViG__Incorporating_Pointing_Gestures_into_Language-Based_3D_Visual_Grounding_CVPR_2025_paper.html": {
    "title": "Ges3ViG : Incorporating Pointing Gestures into Language-Based 3D Visual Grounding for Embodied Reference Understanding",
    "volume": "main",
    "abstract": "3-Dimensional Embodied Reference Understanding (3DERU) combines a language description and an accompanying pointing gesture to identify the most relevant target object in a 3D scene. Although prior work has explored pure language-based 3D grounding, there has been limited exploration of 3D-ERU, which also incorporates human pointing gestures. To address this gap, we introduce a data augmentation framework-Imputer, and use it to curate a new benchmark dataset-ImputeRefer for 3D-ERU, by incorporating human pointing gestures into existing 3D scene datasets that only contain language instructions. We also propose Ges3ViG, a novel model for 3D-ERU that achieves 30% improvement in accuracy as compared to other 3DERU models and 9% compared to other purely language-based 3D grounding models. Our code and dataset are available at https://github.com/AtharvMane/Ges3ViG",
    "checked": false,
    "id": "e07a5ac3ed21688c8efbb0aeecd78c4e2d32ab26",
    "semantic_title": "ges3vig: incorporating pointing gestures into language-based 3d visual grounding for embodied reference understanding",
    "citation_count": 1,
    "authors": [
      "Atharv Mahesh Mane",
      "Dulanga Weerakoon",
      "Vigneshwaran Subbaraju",
      "Sougata Sen",
      "Sanjay E. Sarma",
      "Archan Misra"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shim_Focusing_on_Tracks_for_Online_Multi-Object_Tracking_CVPR_2025_paper.html": {
    "title": "Focusing on Tracks for Online Multi-Object Tracking",
    "volume": "main",
    "abstract": "Multi-object tracking (MOT) is a critical task in computer vision, requiring the accurate identification and continuous tracking of multiple objects across video frames. However, current state-of-the-art methods mainly rely on a global optimization technique and multi-stage cascade association strategy, and those approaches often overlook the specific characteristics of assignment task in MOT and useful detection results that may represent occluded objects. To address these challenges, we propose a novel Track-Focused Online Multi-Object Tracker (TrackTrack) with two key strategies: Track-Perspective-Based Association (TPA) and Track-Aware Initialization (TAI). The TPA strategy associates each track with the most suitable detection result by choosing the one with the minimum distance from all available detection results in a track-perspective manner. On the other hand, TAI precludes the generation of spurious tracks in the track-aware aspect by suppressing track initialization of detection results that heavily overlap with current active tracks and more confident detection results. Extensive experiments on MOT17, MOT20, and DanceTrack demonstrate that our TrackTrack outperforms current state-of-the-art trackers, offering improved robustness and accuracy across diverse and challenging tracking scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyujin Shim",
      "Kangwook Ko",
      "Yujin Yang",
      "Changick Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hoffmann_Floxels_Fast_Unsupervised_Voxel_Based_Scene_Flow_Estimation_CVPR_2025_paper.html": {
    "title": "Floxels: Fast Unsupervised Voxel Based Scene Flow Estimation",
    "volume": "main",
    "abstract": "Scene flow estimation is a foundational task for many robotic applications, including robust dynamic object detection, automatic labeling, and sensor synchronization. Two types of approaches to the problem have evolved: 1) Supervised and 2) optimization-based methods. Supervised methods are fast during inference and achieve high-quality results, however, they are limited by the need for large amounts of labeled training data and are susceptible to domain gaps. In contrast, unsupervised test-time optimization methods do not face the problem of domain gaps but usually suffer from substantial runtime, exhibit artifacts, or fail to converge to the right solution. In this work, we mitigate several limitations of existing optimization-based methods. To this end, we 1) introduce a simple voxel grid-based model that improves over the standard MLP-based formulation in multiple dimensions and 2) introduce a new multi-frame loss formulation. 3) We combine both contributions in our new method, termed Floxels. On the Argoverse 2 benchmark, Floxels is surpassed only by EulerFlow among unsupervised methods while achieving comparable performance at a fraction of the computational cost. Floxels achieves a massive speedup of more than 60-140x over EulerFlow, reducing the runtime from a day to 10 minutes per sequence. Over the faster but low-quality baseline, NSFP, Floxels achieves a speedup of 14x",
    "checked": true,
    "id": "15930fddb5bf0622bb87ac68919522d6c2e5d57b",
    "semantic_title": "floxels: fast unsupervised voxel based scene flow estimation",
    "citation_count": 3,
    "authors": [
      "David T. Hoffmann",
      "Syed Haseeb Raza",
      "Hanqiu Jiang",
      "Denis Tananaev",
      "Steffen Klingenhoefer",
      "Martin Meinke"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_LiveCC_Learning_Video_LLM_with_Streaming_Speech_Transcription_at_Scale_CVPR_2025_paper.html": {
    "title": "LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale",
    "volume": "main",
    "abstract": "Recent video large language models (Video LLMs) often depend on costly human annotations or proprietary APIs (e.g., GPT-4o) to produce training data, which limits their training at scale. In this paper, we explore large-scale training for Video LLM with cheap automatic speech recognition (ASR) transcripts. Specifically, we propose a novel streaming training approach that densely interleaves the ASR words and video frames according to their timestamps. Compared to previous studies in vision-language representation with ASR, our method naturally fits the streaming characteristics of ASR, thus enabling the model to learn temporally-aligned, fine-grained vision-language modeling. To support the training algorithm, we introduce a data pipeline for YouTube videos and their closed captions (CC), resulting in \\texttt Live-CC-10M pre-training set and \\texttt Live-WhisperX-408K high-quality supervised fine-tuning (SFT) set. Remarkably, even without SFT, the pre-trained model \\texttt LiveCC-7B demonstrates significant improvements in general video QA and exhibits a new capability in real-time video commentary. To evaluate this, we carefully design a new benchmark \\texttt LiveSports-3K , using LLM-as-a-judge to measure the free-form commentary. Experiments show our final model \\texttt LiveCC-7B can surpass LLaVA-Video-72B in commentary quality even working in a real-time mode. Meanwhile, it achieves state-of-the-art results at the 7B scale on popular benchmarks such as VideoMME, demonstrating its broad generalizability. All resources of this paper have been released at \\href https://showlab.github.io/livecc showlab.github.io/livecc",
    "checked": true,
    "id": "3e5288e6035bfc5d8a4f60afd7e28bea421e7884",
    "semantic_title": "livecc: learning video llm with streaming speech transcription at scale",
    "citation_count": 1,
    "authors": [
      "Joya Chen",
      "Ziyun Zeng",
      "Yiqi Lin",
      "Wei Li",
      "Zejun Ma",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Identity-preserving_Distillation_Sampling_by_Fixed-Point_Iterator_CVPR_2025_paper.html": {
    "title": "Identity-preserving Distillation Sampling by Fixed-Point Iterator",
    "volume": "main",
    "abstract": "Score distillation sampling (SDS) demonstrates a powerful capability for text-conditioned 2D image and 3D object generation by distilling the knowledge from learned score functions. However, SDS often suffers from blurriness caused by noisy gradients. When SDS meets the image editing, such degradations can be reduced by adjusting bias shifts using reference pairs, but the de-biasing techniques are still corrupted by erroneous gradients. To this end, we introduce Identity-preserving Distillation Sampling (IDS), which compensates for the gradient leading to undesired changes in the results. Based on the analysis that these errors come from the text-conditioned scores, a new regularization technique, called fixed-point iterative regularization (FPR), is proposed to modify the score itself, driving the preservation of the identity even including poses and structures. Thanks to a self-correction by FPR, the proposed method provides clear and unambiguous representations corresponding to the given prompts in image-to-image editing and editable neural radiance field (NeRF). The structural consistency between the source and the edited data is obviously maintained compared to other state-of-the-art methods",
    "checked": true,
    "id": "fea5cac66119a83e7a8944316c69a91781292386",
    "semantic_title": "identity-preserving distillation sampling by fixed-point iterator",
    "citation_count": 1,
    "authors": [
      "SeonHwa Kim",
      "Jiwon Kim",
      "Soobin Park",
      "Donghoon Ahn",
      "Jiwon Kang",
      "Seungryong Kim",
      "Kyong Hwan Jin",
      "Eunju Cha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Long_Progressive_Focused_Transformer_for_Single_Image_Super-Resolution_CVPR_2025_paper.html": {
    "title": "Progressive Focused Transformer for Single Image Super-Resolution",
    "volume": "main",
    "abstract": "Transformer-based methods have achieved remarkable results in image super-resolution tasks because they can capture non-local dependencies in low-quality input images. However, this feature-intensive modeling approach is computationally expensive because it calculates the similarities between numerous features that are irrelevant to the query features when obtaining attention weights. These unnecessary similarity calculations not only degrade the reconstruction performance but also introduce significant computational overhead. How to accurately identify the features that are important to the current query features and avoid similarity calculations between irrelevant features remains an urgent problem. To address this issue, we propose a novel and effective Progressive Focused Transformer (PFT) that links all isolated attention maps in the network through Progressive Focused Attention (PFA) to focus attention on the most important tokens. PFA not only enables the network to capture more critical similar features, but also significantly reduces the computational cost of the overall network by filtering out irrelevant features before calculating similarities. Extensive experiments demonstrate the effectiveness of the proposed method, achieving state-of-the-art performance on various single image super-resolution benchmarks",
    "checked": true,
    "id": "98e6347ae9bddde64a3f58034ca476233a8b4a07",
    "semantic_title": "progressive focused transformer for single image super-resolution",
    "citation_count": 1,
    "authors": [
      "Wei Long",
      "Xingyu Zhou",
      "Leheng Zhang",
      "Shuhang Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ouali_VladVA_Discriminative_Fine-tuning_of_LVLMs_CVPR_2025_paper.html": {
    "title": "VladVA: Discriminative Fine-tuning of LVLMs",
    "volume": "main",
    "abstract": "Contrastively-trained Vision-Language Models (VLMs) like CLIP have become the de facto approach for discriminative vision-language representation learning. However, these models have limited language understanding, often exhibiting a \"bag of words\" behavior. At the same time, Large Vision-Language Models (LVLMs), which combine vision encoders with LLMs, have been shown to be capable of detailed vision-language reasoning, yet their autoregressive nature renders them less suitable for discriminative tasks. In this work, we propose to combine \"the best of both worlds\": a new training approach for discriminative fine-tuning of LVLMs that results in strong discriminative and compositional capabilities. Essentially, our approach converts a generative LVLM into a discriminative one, unlocking its capability for powerful image-text discrimination combined with enhanced language understanding. Our contributions include (1) A carefully designed training/optimization framework that utilizes image-text pairs of variable length and granularity for training the model with both contrastive and next-token prediction losses. This is accompanied by ablation studies that justify the necessity of our framework's components. (2) A parameter-efficient adaptation method using a combination of soft prompting and LoRA adapters. (3) Significant improvements over state-of-the-art CLIP-like models of similar size, including standard image-text retrieval benchmarks and notable gains in compositionality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yassine Ouali",
      "Adrian Bulat",
      "Alexandros Xenos",
      "Anestis Zaganidis",
      "Ioannis Maniadis Metaxas",
      "Brais Martinez",
      "Georgios Tzimiropoulos"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Anagnostidis_FlexiDiT_Your_Diffusion_Transformer_Can_Easily_Generate_High-Quality_Samples_with_CVPR_2025_paper.html": {
    "title": "FlexiDiT: Your Diffusion Transformer Can Easily Generate High-Quality Samples with Less Compute",
    "volume": "main",
    "abstract": "Despite their remarkable performance, modern Diffusion Transformers (DiTs) are hindered by substantial resource requirements during inference, stemming from the fixed and large amount of compute needed for each denoising step. In this work, we revisit the conventional static paradigm that allocates a fixed compute budget per denoising iteration and propose a dynamic strategy instead. Our simple and sample-efficient framework enables pre-trained DiT models to be converted into flexible ones --- dubbed FlexiDiT --- allowing them to process inputs at varying compute budgets. We demonstrate how a single flexible model can generate images without any drop in quality, while reducing the required FLOPs by more than 40% compared to their static counterparts, for both class-conditioned and text-conditioned image generation. Our method is general and agnostic to input and conditioning modalities. We show how our approach can be readily extended for video generation, where FlexiDiT models generate samples with up to 75% less compute without compromising performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sotiris Anagnostidis",
      "Gregor Bachmann",
      "Yeongmin Kim",
      "Jonas Kohler",
      "Markos Georgopoulos",
      "Artsiom Sanakoyeu",
      "Yuming Du",
      "Albert Pumarola",
      "Ali Thabet",
      "Edgar Sch√∂nfeld"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Potamias_WiLoR_End-to-end_3D_Hand_Localization_and_Reconstruction_in-the-wild_CVPR_2025_paper.html": {
    "title": "WiLoR: End-to-end 3D Hand Localization and Reconstruction in-the-wild",
    "volume": "main",
    "abstract": "In recent years, 3D hand pose estimation methods have garnered significant attention due to their extensive applications in human-computer interaction, virtual reality, and robotics. In contrast, there has been a notable gap in hand detection pipelines, posing significant challenges in constructing effective real-world multi-hand reconstruction systems. In this work, we present a data-driven pipeline for efficient multi-hand reconstruction in the wild. The proposed pipeline is composed of two components: a real-time fully convolutional hand localization and a high-fidelity transformer-based 3D hand reconstruction model. To tackle the limitations of previous methods and build a robust and stable detection network, we introduce a large-scale dataset with over than 2M in-the-wild hand images with diverse lighting, illumination, and occlusion conditions. Our approach outperforms previous methods in both efficiency and accuracy on popular 2D and 3D benchmarks. Finally, we showcase the effectiveness of our pipeline to achieve smooth 3D hand tracking from monocular videos, without utilizing any temporal components. Code, models, and dataset are available at https://rolpotamias.github.io/WiLoR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rolandos Alexandros Potamias",
      "Jinglei Zhang",
      "Jiankang Deng",
      "Stefanos Zafeiriou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_HumanMM_Global_Human_Motion_Recovery_from_Multi-shot_Videos_CVPR_2025_paper.html": {
    "title": "HumanMM: Global Human Motion Recovery from Multi-shot Videos",
    "volume": "main",
    "abstract": "In this paper, we present a novel framework designed to reconstruct long-sequence 3D human motion in the world coordinates from in-the-wild videos with multiple shot transitions. Such long-sequence in-the-wild motions are highly valuable to applications such as motion generation and motion understanding, but are of great challenge to be recovered due to abrupt shot transitions, partial occlusions, and dynamic backgrounds presented in such videos. Existing methods primarily focus on single-shot videos, where continuity is maintained within a single camera view, or simplify multi-shot alignment in camera space only. In this work, we tackle the challenges by integrating an enhanced camera pose estimation with Human Motion Recovery (HMR) by incorporating a shot transition detector and a robust alignment module for accurate pose and orientation continuity across shots. By leveraging a custom motion integrator, we effectively mitigate the problem of foot sliding and ensure temporal consistency in human pose. Extensive evaluations on our created multi-shot dataset from public 3D human datasets demonstrate the robustness of our method in reconstructing realistic human motion in world coordinates",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhong Zhang",
      "Guanlin Wu",
      "Ling-Hao Chen",
      "Zhuokai Zhao",
      "Jing Lin",
      "Xiaoke Jiang",
      "Jiamin Wu",
      "Zhuoheng Li",
      "Hao Frank Yang",
      "Haoqian Wang",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kee_Removing_Reflections_from_RAW_Photos_CVPR_2025_paper.html": {
    "title": "Removing Reflections from RAW Photos",
    "volume": "main",
    "abstract": "We describe a system to remove real-world reflections from images for consumer photography. Our system operates on linear (RAW) photos, and accepts an optional contextual photo looking in the opposite direction (e.g., the \"selfie\" camera on a mobile device). This optional photo disambiguates what should be considered the reflection. The system is trained solely on synthetic mixtures of real RAW photos, which we combine using a reflection simulation that is photometrically and geometrically accurate. Our system comprises a base model that accepts the captured photo and optional context photo as input, and runs at 256p, followed by an up-sampling model that transforms 256p images to full resolution. The system produces preview images at 1K in 4.5-6.5s on a MacBook or iPhone 14 Pro. We show SOTA results on RAW photos that were captured in the field to embody typical consumer photos, and show that training on RAW simulation data improves performance more than the architectural variations among prior works",
    "checked": true,
    "id": "d6c20d405d7c4549ead7fb6473f3408b46f26797",
    "semantic_title": "removing reflections from raw photos",
    "citation_count": 4,
    "authors": [
      "Eric Kee",
      "Adam Pikielny",
      "Kevin Blackburn-Matzen",
      "Marc Levoy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Koleilat_BiomedCoOp_Learning_to_Prompt_for_Biomedical_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "BiomedCoOp: Learning to Prompt for Biomedical Vision-Language Models",
    "volume": "main",
    "abstract": "Recent advancements in vision-language models (VLMs), such as CLIP, have demonstrated substantial success in self-supervised representation learning for vision tasks. However, effectively adapting VLMs to downstream applications remains challenging, as their accuracy often depends on time-intensive and expertise-demanding prompt engineering, while full model fine-tuning is costly. This is particularly true for biomedical images, which, unlike natural images, typically suffer from limited annotated datasets, unintuitive image contrasts, and nuanced visual features. Recent prompt learning techniques, such as Context Optimization (CoOp) intend to tackle these issues, but still fall short in generalizability. Meanwhile, explorations in prompt learning for biomedical image analysis are still highly limited. In this work, we propose BiomedCoOp, a novel prompt learning framework that enables efficient adaptation of BiomedCLIP for accurate and highly generalizable few-shot biomedical image classification. Our approach achieves effective prompt context learning by leveraging semantic consistency with average prompt ensembles from Large Language Models (LLMs) and knowledge distillation with a statistics-based prompt selection strategy. We conducted comprehensive validation of our proposed framework on 11 medical datasets across 9 modalities and 10 organs against existing state-of-the-art methods, demonstrating significant improvements in both accuracy and generalizability. The code is publicly available at https://github.com/HealthX-Lab/BiomedCoOp",
    "checked": true,
    "id": "594cc99c46439c2d8ae5e6129585ce4a9a2459e1",
    "semantic_title": "biomedcoop: learning to prompt for biomedical vision-language models",
    "citation_count": 6,
    "authors": [
      "Taha Koleilat",
      "Hojat Asgariandehkordi",
      "Hassan Rivaz",
      "Yiming Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_AMR-Transformer_Enabling_Efficient_Long-range_Interaction_for_Complex_Neural_Fluid_Simulation_CVPR_2025_paper.html": {
    "title": "AMR-Transformer: Enabling Efficient Long-range Interaction for Complex Neural Fluid Simulation",
    "volume": "main",
    "abstract": "Accurately and efficiently simulating complex fluid dynamics is a challenging task that has traditionally relied on computationally intensive methods. Neural network-based approaches, such as convolutional and graph neural networks, have partially alleviated this burden by enabling efficient local feature extraction. However, they struggle to capture long-range dependencies due to limited receptive fields, and Transformer-based models, while providing global context, incur prohibitive computational costs. To tackle these challenges, we propose AMR-Transformer, an efficient and accurate neural CFD-solving pipeline that integrates a novel adaptive mesh refinement scheme with a Navier-Stokes constraint-aware fast pruning module. This design encourages long-range interactions between simulation cells and facilitates the modeling of global fluid wave patterns, such as turbulence and shockwaves. Experiments show that our approach achieves significant gains in efficiency while preserving critical details, making it suitable for high-resolution physical simulations with long-range dependencies. On CFDBench, PDEBench and a new shockwave dataset, our pipeline demonstrates up to an order-of-magnitude improvement in accuracy over baseline models. Additionally, compared to ViT, our approach achieves a reduction in FLOPs of up to 60 times",
    "checked": true,
    "id": "72b4ffa783a86b79e790efce0b1e78eb369a3963",
    "semantic_title": "amr-transformer: enabling efficient long-range interaction for complex neural fluid simulation",
    "citation_count": 1,
    "authors": [
      "Zeyi Xu",
      "Jinfan Liu",
      "Kuangxu Chen",
      "Ye Chen",
      "Zhangli Hu",
      "Bingbing Ni"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chharia_MV-SSM_Multi-View_State_Space_Modeling_for_3D_Human_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "MV-SSM: Multi-View State Space Modeling for 3D Human Pose Estimation",
    "volume": "main",
    "abstract": "While significant progress has been made in single-view 3D human pose estimation, multi-view 3D human pose estimation remains challenging, particularly in terms of generalizing to new camera configurations. Existing attention-based transformers often struggle to accurately model the spatial arrangement of keypoints, especially in occluded scenarios. Additionally, they tend to overfit specific camera arrangements and visual scenes from training data, resulting in substantial performance drops in new settings. In this study, we introduce a novel Multi-View State Space Modeling framework, named MV-SSM, for robustly estimating 3D human keypoints. We explicitly model the joint spatial sequence at two distinct levels: the feature level from multi-view images and the person keypoint level. We propose a Projective State Space (PSS) block to learn a generalized representation of joint spatial arrangements using state space modeling. Moreover, we modify Mamba's traditional scanning into an effective Grid Token-guided Bidirectional Scanning (GTBS), which is integral to the PSS block. Multiple experiments demonstrate that MV-SSM achieves strong generalization, outperforming state-of-the-art methods: +10.8 on AP25 on the challenging three-camera setting in CMU Panoptic, +7.0 on AP25 on varying camera arrangements, and +15.3 PCP on Campus A1 in cross-dataset evaluations. Project Website: https://aviralchharia.github.io/MV-SSM",
    "checked": true,
    "id": "f42ef9a41f6f4b15ec3eed1b83e8851f0bd8a96a",
    "semantic_title": "mv-ssm: multi-view state space modeling for 3d human pose estimation",
    "citation_count": 0,
    "authors": [
      "Aviral Chharia",
      "Wenbo Gou",
      "Haoye Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nguyen_HyperGLM_HyperGraph_for_Video_Scene_Graph_Generation_and_Anticipation_CVPR_2025_paper.html": {
    "title": "HyperGLM: HyperGraph for Video Scene Graph Generation and Anticipation",
    "volume": "main",
    "abstract": "Multimodal LLMs have advanced vision-language tasks but still struggle with understanding video scenes. To bridge this gap, Video Scene Graph Generation (VidSGG) has emerged to capture multi-object relationships across video frames. However, prior methods rely on pairwise connections, limiting their ability to handle complex multi-object interactions and reasoning. To this end, we propose Multimodal LLMs on a Scene HyperGraph (HyperGLM), promoting reasoning about multi-way interactions and higher-order relationships. Our approach uniquely integrates entity scene graphs, which capture spatial relationships between objects, with a procedural graph that models their causal transitions, forming a unified HyperGraph. Significantly, HyperGLM enables reasoning by injecting this unified HyperGraph into LLMs. Additionally, we introduce a new Video Scene Graph Reasoning (VSGR) dataset featuring 1.9M frames from third-person, egocentric, and drone views and supports five tasks: Scene Graph Generation, Scene Graph Anticipation, Video Question Answering, Video Captioning, and Relation Reasoning. Empirically, HyperGLM consistently outperforms state-of-the-art methods across five tasks, effectively modeling and reasoning complex relationships in diverse video scenes",
    "checked": true,
    "id": "de8e54fd449e87935d1eacb86c24afcbfc2b2cd2",
    "semantic_title": "hyperglm: hypergraph for video scene graph generation and anticipation",
    "citation_count": 3,
    "authors": [
      "Trong-Thuan Nguyen",
      "Pha Nguyen",
      "Jackson Cothren",
      "Alper Yilmaz",
      "Khoa Luu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Astruc_AnySat_One_Earth_Observation_Model_for_Many_Resolutions_Scales_and_CVPR_2025_paper.html": {
    "title": "AnySat: One Earth Observation Model for Many Resolutions, Scales, and Modalities",
    "volume": "main",
    "abstract": "Geospatial models must adapt to the diversity of Earth observation data in terms of resolutions, scales, and modalities. However, existing approaches expect fixed input configurations, which limits their practical applicability. We propose AnySat, a multimodal model based on joint embedding predictive architecture (JEPA) and scale-adaptive spatial encoders, allowing us to train a single model on highly heterogeneous data in a self-supervised manner. To demonstrate the advantages of this unified approach, we compile GeoPlex, a collection of 5 multimodal datasets with varying characteristics and 11 distinct sensors. We then train a single powerful model on these diverse datasets simultaneously. Once fine-tuned or probed, we achieve state-of-the-art results on the test sets of GeoPlex and for 6 external datasets across various environment monitoring tasks: land cover mapping, tree species identification, crop type classification, change detection, climate type classification, and segmentation of flood, burn scar, and deforestation. Our code and models are available at https://github.com/gastruc/AnySat",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillaume Astruc",
      "Nicolas Gonthier",
      "Cl√©ment Mallet",
      "Loic Landrieu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_FSFM_A_Generalizable_Face_Security_Foundation_Model_via_Self-Supervised_Facial_CVPR_2025_paper.html": {
    "title": "FSFM: A Generalizable Face Security Foundation Model via Self-Supervised Facial Representation Learning",
    "volume": "main",
    "abstract": "This work asks: with abundant, unlabeled real faces, how to learn a robust and transferable facial representation that boosts various face security tasks with respect to generalization performance? We make the first attempt and propose a self-supervised pretraining framework to learn fundamental representations of real face images, FSFM, that leverages the synergy between masked image modeling (MIM) and instance discrimination (ID). We explore various facial masking strategies for MIM and present a simple yet powerful CRFR-P masking, which explicitly forces the model to capture meaningful intra-region Consistency and challenging inter-region Coherency. Furthermore, we devise an ID network that naturally couples with MIM to establish underlying local-to-global Correspondence through tailored self-distillation. These three learning objectives, namely 3C, empower encoding both local features and global semantics of real faces. After pretraining, a vanilla ViT serves as a universal vision Foundation Model for downstream Face Security tasks: cross-dataset deepfake detection, cross-domain face anti-spoofing, and unseen diffusion facial forgery detection. Extensive experiments on 10 public datasets demonstrate that our model transfers better than supervised pretraining, visual and facial self-supervised learning arts, and even outperforms task-specialized SOTA methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaojian Wang",
      "Feng Lin",
      "Tong Wu",
      "Zhenguang Liu",
      "Zhongjie Ba",
      "Kui Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Niu_OVO-Bench_How_Far_is_Your_Video-LLMs_from_Real-World_Online_Video_CVPR_2025_paper.html": {
    "title": "OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?",
    "volume": "main",
    "abstract": "Temporal Awareness, the ability to reason dynamically based on the timestamp when a question is raised, is the key distinction between offline and online video LLMs. Unlike offline models, which rely on complete videos for static, post hoc analysis, online models process video streams incrementally and dynamically adapt their responses based on the timestamp at which the question is posed. Despite its significance, temporal awareness has not been adequately evaluated in existing benchmarks. To fill this gap, we present OVO-Bench (Online-VideO-Benchmark), a novel video benchmark that emphasizes the importance of timestamps for advanced online video understanding capability benchmarking. OVO-Bench evaluates the ability of video LLMs to reason and respond to events occurring at specific timestamps under three distinct scenarios: (1) Backward tracing: trace back to past events to answer the question. (2) Real-time understanding: understand and respond to events as they unfold at the current timestamp. (3) Forward active responding: delay the response until sufficient future information becomes available to answer the question accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videos and approximately human-curated 2,800 fine-grained meta-annotations with precise timestamps. We combine automated generation pipelines with human curation. With these high-quality samples, we further developed an evaluation pipeline to systematically query video LLMs along the video timeline. Evaluations of nine Video-LLMs reveal that, despite advancements on traditional benchmarks, current models struggle with online video understanding, showing a significant gap compared to human agents. We hope OVO-Bench will drive progress in video LLMs and inspire future research in online video reasoning. Our benchmark and code can be accessed at https://joeleelyf.github.io/OVO-Bench",
    "checked": true,
    "id": "6f00bdbb2082ec80ce0ae701f4c8c4ed52607a41",
    "semantic_title": "ovo-bench: how far is your video-llms from real-world online video understanding?",
    "citation_count": 14,
    "authors": [
      "Junbo Niu",
      "Yifei Li",
      "Ziyang Miao",
      "Chunjiang Ge",
      "Yuanhang Zhou",
      "Qihao He",
      "Xiaoyi Dong",
      "Haodong Duan",
      "Shuangrui Ding",
      "Rui Qian",
      "Pan Zhang",
      "Yuhang Zang",
      "Yuhang Cao",
      "Conghui He",
      "Jiaqi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_AlignMamba_Enhancing_Multimodal_Mamba_with_Local_and_Global_Cross-modal_Alignment_CVPR_2025_paper.html": {
    "title": "AlignMamba: Enhancing Multimodal Mamba with Local and Global Cross-modal Alignment",
    "volume": "main",
    "abstract": "Cross-modal alignment is crucial for multimodal representation fusion due to the inherent heterogeneity between modalities. While Transformer-based methods have shown promising results in modeling inter-modal relationships, their quadratic computational complexity limits their applicability to long-sequence or large-scale data. Although recent Mamba-based approaches achieve linear complexity, their sequential scanning mechanism poses fundamental challenges in comprehensively modeling cross-modal relationships. To address this limitation, we propose AlignMamba, an efficient and effective method for multimodal fusion. Specifically, grounded in Optimal Transport, we introduce a local cross-modal alignment module that explicitly learns token-level correspondences between different modalities. Moreover, we propose a global cross-modal alignment loss based on Maximum Mean Discrepancy to implicitly enforce the consistency between different modal distributions. Finally, the unimodal representations after local and global alignment are passed to the Mamba backbone for further cross-modal interaction and multimodal fusion. Extensive experiments on complete and incomplete multimodal fusion tasks demonstrate the effectiveness and efficiency of the proposed method. For instance, on the CMU-MOSI dataset, AlignMamba improves classification accuracy by 0.9%, reduces GPU memory usage by 20.3%, and decreases inference time by 83.3%",
    "checked": true,
    "id": "8da057282a39efda365c6f8d094efe5efddc3f1e",
    "semantic_title": "alignmamba: enhancing multimodal mamba with local and global cross-modal alignment",
    "citation_count": 9,
    "authors": [
      "Yan Li",
      "Yifei Xing",
      "Xiangyuan Lan",
      "Xin Li",
      "Haifeng Chen",
      "Dongmei Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Blurry-Edges_Photon-Limited_Depth_Estimation_from_Defocused_Boundaries_CVPR_2025_paper.html": {
    "title": "Blurry-Edges: Photon-Limited Depth Estimation from Defocused Boundaries",
    "volume": "main",
    "abstract": "Extracting depth information from photon-limited, defocused images is challenging because depth from defocus (DfD) relies on accurate estimation of defocus blur, which is fundamentally sensitive to image noise. We present a novel approach to robustly measure object depths from photon-limited images along the defocused boundaries. It is based on a new image patch representation, Blurry-Edges, that explicitly stores and visualizes a rich set of low-level patch information, including boundaries, color, and smoothness. We develop a deep neural network architecture that predicts the Blurry-Edges representation from a pair of differently defocused images, from which depth can be calculated using a closed-form DfD relation we derive. The experimental results on synthetic and real data show that our method achieves the highest depth estimation accuracy on photon-limited images compared to a broad range of state-of-the-art DfD methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Xu",
      "Charles James Wagner",
      "Junjie Luo",
      "Qi Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_VideoComp_Advancing_Fine-Grained_Compositional_and_Temporal_Alignment_in_Video-Text_Models_CVPR_2025_paper.html": {
    "title": "VideoComp: Advancing Fine-Grained Compositional and Temporal Alignment in Video-Text Models",
    "volume": "main",
    "abstract": "We introduce VideoComp, a benchmark and learning framework for advancing video-text compositionality understanding, aimed at improving vision-language models (VLMs) in fine-grained temporal alignment. Unlike existing benchmarks focused on static image-text compositionality or isolated single-event videos, our benchmark targets alignment in continuous multi-event videos. Leveraging video-text datasets with temporally localized event captions (e.g. ActivityNet-Captions, YouCook2), we construct two compositional benchmarks, ActivityNet-Comp and YouCook2-Comp. We create challenging negative samples with subtle temporal disruptions such as reordering, action word replacement, partial captioning, and combined disruptions. These benchmarks comprehensively test models' compositional sensitivity across extended, cohesive video-text sequences. To improve model performance, we propose a hierarchical pairwise preference loss that strengthens alignment with temporally accurate pairs and gradually penalizes increasingly disrupted ones, encouraging fine-grained compositional learning. To mitigate the limited availability of densely annotated video data, we introduce a pretraining strategy that concatenates short video-caption pairs to simulate multi-event sequences. We evaluate video-text foundational models and large multimodal models (LMMs) on our benchmark, identifying both strengths and areas for improvement in compositionality. Overall, our work provides a comprehensive framework for evaluating and enhancing model capabilities in achieving fine-grained, temporally coherent video-text alignment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dahun Kim",
      "AJ Piergiovanni",
      "Ganesh Mallya",
      "Anelia Angelova"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_One_Model_for_ALL_Low-Level_Task_Interaction_Is_a_Key_CVPR_2025_paper.html": {
    "title": "One Model for ALL: Low-Level Task Interaction Is a Key to Task-Agnostic Image Fusion",
    "volume": "main",
    "abstract": "Advanced image fusion methods mostly prioritise high-level missions, where task interaction struggles with semantic gaps, requiring complex bridging mechanisms. In contrast, we propose to leverage low-level vision tasks from digital photography fusion, allowing for effective feature interaction through pixel-level supervision. This new paradigm provides strong guidance for unsupervised multimodal fusion without relying on abstract semantics, enhancing task-shared feature learning for broader applicability. Owning to the hybrid image features and enhanced universal representations, the proposed GIFNet supports diverse fusion tasks, achieving high performance across both seen and unseen scenarios with a single model. Uniquely, experimental results reveal that our framework also supports single-modality enhancement, offering superior flexibility for practical applications. Our code will be available at https://github.com/AWCXV/GIFNet",
    "checked": true,
    "id": "70b1d147de624af9ae68c0b3cc96d322fb30bb97",
    "semantic_title": "one model for all: low-level task interaction is a key to task-agnostic image fusion",
    "citation_count": 4,
    "authors": [
      "Chunyang Cheng",
      "Tianyang Xu",
      "Zhenhua Feng",
      "Xiaojun Wu",
      "Zhangyong Tang",
      "Hui Li",
      "Zeyang Zhang",
      "Sara Atito",
      "Muhammad Awais",
      "Josef Kittler"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shao_MICAS_Multi-grained_In-Context_Adaptive_Sampling_for_3D_Point_Cloud_Processing_CVPR_2025_paper.html": {
    "title": "MICAS: Multi-grained In-Context Adaptive Sampling for 3D Point Cloud Processing",
    "volume": "main",
    "abstract": "Point cloud processing (PCP) encompasses tasks like reconstruction, denoising, registration, and segmentation, each often requiring specialized models to address unique task characteristics. While in-context learning (ICL) has shown promise across tasks by using a single model with task-specific demonstration prompts, its application to PCP reveals significant limitations. We identify inter-task and intra-task sensitivity issues in current ICL methods for PCP, which we attribute to inflexible sampling strategies lacking context adaptation at the point and prompt levels. To address these challenges, we propose MICAS, an advanced ICL framework featuring a multi-grained adaptive sampling mechanism tailored for PCP. MICAS introduces two core components: task-adaptive point sampling, which leverages inter-task cues for point-level sampling, and query-specific prompt sampling, which selects optimal prompts per query to mitigate intra-task sensitivity. To our knowledge, this is the first approach to introduce adaptive sampling tailored to the unique requirements of point clouds within an ICL framework. Extensive experiments show that MICAS not only efficiently handles various PCP tasks but also significantly outperforms existing methods. Notably, it achieves a remarkable 4.1% improvement in the part segmentation task and delivers consistent gains across various PCP applications",
    "checked": true,
    "id": "58df491f0a62356edc6f33371a09eb1a3fa22c85",
    "semantic_title": "micas: multi-grained in-context adaptive sampling for 3d point cloud processing",
    "citation_count": 1,
    "authors": [
      "Feifei Shao",
      "Ping Liu",
      "Zhao Wang",
      "Yawei Luo",
      "Hongwei Wang",
      "Jun Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zanella_Can_Text-to-Video_Generation_help_Video-Language_Alignment_CVPR_2025_paper.html": {
    "title": "Can Text-to-Video Generation help Video-Language Alignment?",
    "volume": "main",
    "abstract": "Recent video-language alignment models are trained on sets of videos, each with an associated positive caption and a negative caption generated by large language models. A problem with this procedure is that negative captions may introduce linguistic biases, i.e., concepts are seen only as negatives and never associated with a video. While a solution would be to collect videos for the negative captions, existing databases lack the fine-grained variations needed to cover all possible negatives. In this work, we study whether synthetic videos can help to overcome this issue. Our preliminary analysis with multiple generators shows that, while promising on some tasks, synthetic videos harm the performance of the model on others. We hypothesize this issue is linked to noise (semantic and visual) in the generated videos and develop a method, SynViTA, that accounts for those. SynViTA dynamically weights the contribution of each synthetic video based on how similar its target caption is w.r.t. the real counterpart. Moreover, a semantic consistency loss makes the model focus on fine-grained differences across captions, rather than differences in video appearance. Experiments show that, on average, SynViTA improves over existing methods on VideoCon test sets and SSv2-Temporal, SSv2-Events, and ATP-Hard benchmarks, being a first promising step for using synthetic videos when learning video-language models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Zanella",
      "Massimiliano Mancini",
      "Willi Menapace",
      "Sergey Tulyakov",
      "Yiming Wang",
      "Elisa Ricci"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xing_GoalFlow_Goal-Driven_Flow_Matching_for_Multimodal_Trajectories_Generation_in_End-to-End_CVPR_2025_paper.html": {
    "title": "GoalFlow: Goal-Driven Flow Matching for Multimodal Trajectories Generation in End-to-End Autonomous Driving",
    "volume": "main",
    "abstract": "We propose GoalFlow, an end-to-end autonomous driving method for generating high-quality multimodal trajectories. In autonomous driving scenarios, there is rarely a single suitable trajectory. Recent methods have increasingly focused on modeling multimodal trajectory distributions. However, they suffer from trajectory selection complexity and reduced trajectory quality due to high trajectory divergence and inconsistencies between guidance and scene information. To address these issues, we introduce GoalFlow, a novel method that effectively constrains the generative process to produce high-quality, multimodal trajectories. To resolve the trajectory divergence problem inherent in diffusion-based methods, GoalFlow constrains the generated trajectories by introducing a goal point. GoalFlow establishes a novel scoring mechanism that selects the most appropriate goal point from the candidate points based on scene information. Furthermore, GoalFlow employs an efficient generative method, Flow Matching, to generate multimodal trajectories, and incorporates a refined scoring mechanism to select the optimal trajectory from the candidates. Our experimental results, validated on the Navsim, demonstrate that GoalFlow achieves state-of-the-art performance, delivering robust multimodal trajectories for autonomous driving. GoalFlow achieved PDMS of 90.3, significantly surpassing other methods. Compared with other diffusion-policy-based methods, our approach requires only a single denoising step to obtain excellent performance. The code is available at https://github.com/YvanYin/GoalFlow",
    "checked": true,
    "id": "89c18187b210b713aee8c84bc1c9f47acea06781",
    "semantic_title": "goalflow: goal-driven flow matching for multimodal trajectories generation in end-to-end autonomous driving",
    "citation_count": 20,
    "authors": [
      "Zebin Xing",
      "Xingyu Zhang",
      "Yang Hu",
      "Bo Jiang",
      "Tong He",
      "Qian Zhang",
      "Xiaoxiao Long",
      "Wei Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_GuardSplat_Efficient_and_Robust_Watermarking_for_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "GuardSplat: Efficient and Robust Watermarking for 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) has recently created impressive 3D assets for various applications. However, considering security, capacity, invisibility, and training efficiency, the copyright of 3DGS assets is not well protected as existing watermarking methods are unsuited for its rendering pipeline. In this paper, we propose GuardSplat, an innovative and efficient framework for watermarking 3DGS assets. Specifically, 1) We propose a CLIP-guided pipeline for optimizing the message decoder with minimal costs. The key objective is to achieve high-accuracy extraction by leveraging CLIP's aligning capability and rich representations, demonstrating exceptional capacity and efficiency. 2) We tailor a Spherical-Harmonic-aware (SH-aware) Message Embedding module for 3DGS, seamlessly embedding messages into the SH features of each 3D Gaussian while preserving the original 3D structure. This enables watermarking 3DGS assets with minimal fidelity trade-offs and prevents malicious users from removing the watermarks from the model files, meeting the demands for invisibility and security. 3) We present an Anti-distortion Message Extraction module to improve robustness against various distortions. Experiments demonstrate that GuardSplat outperforms state-of-the-art and achieves fast optimization speed",
    "checked": true,
    "id": "ac8dce2b761d4107e4c076de29566f2a40ae28f6",
    "semantic_title": "guardsplat: efficient and robust watermarking for 3d gaussian splatting",
    "citation_count": 1,
    "authors": [
      "Zixuan Chen",
      "Guangcong Wang",
      "Jiahao Zhu",
      "Jianhuang Lai",
      "Xiaohua Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Weakly_Supervised_Contrastive_Adversarial_Training_for_Learning_Robust_Features_from_CVPR_2025_paper.html": {
    "title": "Weakly Supervised Contrastive Adversarial Training for Learning Robust Features from Semi-supervised Data",
    "volume": "main",
    "abstract": "Existing adversarial training (AT) methods often suffer from incomplete perturbation, meaning that not all non-robust features are perturbed when generating adversarial examples (AEs). This results in residual correlations between non-robust features and labels, leading to suboptimal learning of robust features. However, achieving complete perturbation--perturbing as many non-robust features as possible--is challenging due to the difficulty in distinguishing robust and non-robust features and the sparsity of labeled data. To address these challenges, we propose a novel approach called Weakly Supervised Contrastive Adversarial Training (WSCAT). WSCAT ensures complete perturbation for improved learning of robust features by disrupting correlations between non-robust features and labels through complete AE generation over partially labeled data, grounded in information theory. Extensive theoretical analysis and comprehensive experiments on widely adopted benchmarks validate the superiority of WSCAT. Our code is available at https://github.com/zhang-lilin/WSCAT",
    "checked": true,
    "id": "10910fdd1ab387163f88f49e811cf81500c3b0ec",
    "semantic_title": "weakly supervised contrastive adversarial training for learning robust features from semi-supervised data",
    "citation_count": 0,
    "authors": [
      "Lilin Zhang",
      "Chengpei Wu",
      "Ning Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yuan_From_Poses_to_Identity_Training-Free_Person_Re-Identification_via_Feature_Centralization_CVPR_2025_paper.html": {
    "title": "From Poses to Identity: Training-Free Person Re-Identification via Feature Centralization",
    "volume": "main",
    "abstract": "Person re-identification (ReID) aims to extract accurate identity representation features. However, during feature extraction, individual samples are inevitably affected by noise (background, occlusions, and model limitations). Considering that features from the same identity follow a normal distribution around identity centers after training, we propose a Training-Free Feature Centralization ReID framework (Pose2ID) by aggregating the same identity features to reduce individual noise and enhance the stability of identity representation, which preserves the feature's original distribution for following strategies such as re-ranking. Specifically, to obtain samples of the same identity, we introduce two components: Identity-Guided Pedestrian Generation: by leveraging identity features to guide the generation process, we obtain high-quality images with diverse poses, ensuring identity consistency even in complex scenarios such as infrared, and occlusion. Neighbor Feature Centralization: it explores each sample's potential positive samples from its neighborhood. Experiments demonstrate that our generative model exhibits strong generalization capabilities and maintains high identity consistency. With the Feature Centralization framework, we achieve impressive performance even with an ImageNet pre-trained model without ReID training, reaching mAP/Rank-1 of 52.81/78.92 on Market1501. Moreover, our method sets new state-of-the-art results across standard, cross-modality, and occluded ReID tasks, showcasing strong adaptability",
    "checked": true,
    "id": "53c8037d626d4700fcf2465b5161a322d22632c2",
    "semantic_title": "from poses to identity: training-free person re-identification via feature centralization",
    "citation_count": 4,
    "authors": [
      "Chao Yuan",
      "Guiwei Zhang",
      "Changxiao Ma",
      "Tianyi Zhang",
      "Guanglin Niu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Edstedt_ColabSfM_Collaborative_Structure-from-Motion_by_Point_Cloud_Registration_CVPR_2025_paper.html": {
    "title": "ColabSfM: Collaborative Structure-from-Motion by Point Cloud Registration",
    "volume": "main",
    "abstract": "Structure-from-Motion (SfM) is the task of estimating 3D structure and camera poses from images. We define Collaborative SfM (ColabSfM) as sharing distributed SfM reconstructions. Sharing maps requires estimating a joint reference frame, which is typically referred to as registration. However, there is a lack of scalable methods and training datasets for registering SfM reconstructions. In this paper, we tackle this challenge by proposing the scalable task of point cloud registration for SfM reconstructions. We find that current registration methods cannot register SfM point clouds when trained on existing datasets. To this end, we propose a SfM registration dataset generation pipeline, leveraging partial reconstructions from synthetically generated camera trajectories for each scene. Finally, we propose a simple but impactful neural refiner on top of the SotA registration method RoITr that yields significant improvements, which we call RefineRoITr. Our extensive experimental evaluation shows that our proposed pipeline and model enables ColabSfM. Code is available at https://github.com/EricssonResearch/ColabSfM",
    "checked": true,
    "id": "2ba449740ddfa79ec154441c32245dbcb479d363",
    "semantic_title": "colabsfm: collaborative structure-from-motion by point cloud registration",
    "citation_count": 0,
    "authors": [
      "Johan Edstedt",
      "Andr√© Mateus",
      "Alberto Jaenal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Parikh_RoadSocial_A_Diverse_VideoQA_Dataset_and_Benchmark_for_Road_Event_CVPR_2025_paper.html": {
    "title": "RoadSocial: A Diverse VideoQA Dataset and Benchmark for Road Event Understanding from Social Video Narratives",
    "volume": "main",
    "abstract": "We introduce RoadSocial, a large-scale, diverse VideoQA dataset tailored for generic road event understanding from social media narratives. Unlike existing datasets limited by regional bias, viewpoint bias and expert-driven annotations, RoadSocial captures the global complexity of road events with varied geographies, camera viewpoints (CCTV, handheld, drones) and rich social discourse. Our scalable semi-automatic annotation framework leverages Text LLMs and Video LLMs to generate comprehensive question-answer pairs across 12 challenging QA tasks, pushing the boundaries of road event understanding. RoadSocial is derived from social media videos spanning 14M frames and 414K social comments, resulting in a dataset with 13.2K videos, 674 tags and 260K high-quality QA pairs. We evaluate 18 Video LLMs (open-source and proprietary, driving-specific and general-purpose) on our road event understanding benchmark. We also demonstrate RoadSocial's utility in improving road event understanding capabilities of general-purpose Video LLMs",
    "checked": true,
    "id": "88be2f7842df0fa98e8ecc1c18b09d512b7748da",
    "semantic_title": "roadsocial: a diverse videoqa dataset and benchmark for road event understanding from social video narratives",
    "citation_count": 3,
    "authors": [
      "Chirag Parikh",
      "Deepti Rawat",
      "Rakshitha R. T.",
      "Tathagata Ghosh",
      "Ravi Kiran Sarvadevabhatla"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MangaNinja_Line_Art_Colorization_with_Precise_Reference_Following_CVPR_2025_paper.html": {
    "title": "MangaNinja: Line Art Colorization with Precise Reference Following",
    "volume": "main",
    "abstract": "Derived from diffusion models, MangaNinja specializes in the task of reference-guided line art colorization. We incorporate two thoughtful designs to ensure precise character detail transcription, including a patch shuffling module to facilitate correspondence learning between the reference color image and the target line art, and a point-driven control scheme to enable fine-grained color matching. Experiments on a self-collected benchmark demonstrate the superiority of our model over current solutions in terms of precise colorization. We further showcase the potential of the proposed interactive point control in handling challenging cases (*e.g.*, extreme poses and shadows), cross-character colorization, multi-reference harmonization, *etc.*, beyond the reach of existing algorithms",
    "checked": true,
    "id": "08478cb36374b6d214b64168c28789c53fbbca12",
    "semantic_title": "manganinja: line art colorization with precise reference following",
    "citation_count": 9,
    "authors": [
      "Zhiheng Liu",
      "Ka Leong Cheng",
      "Xi Chen",
      "Jie Xiao",
      "Hao Ouyang",
      "Kai Zhu",
      "Yu Liu",
      "Yujun Shen",
      "Qifeng Chen",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Curreli_Nonisotropic_Gaussian_Diffusion_for_Realistic_3D_Human_Motion_Prediction_CVPR_2025_paper.html": {
    "title": "Nonisotropic Gaussian Diffusion for Realistic 3D Human Motion Prediction",
    "volume": "main",
    "abstract": "Probabilistic human motion prediction aims to forecast multiple possible future movements from past observations. While current approaches report high diversity and realism, they often generate motions with undetected limb stretching and jitter. To address this, we introduce SkeletonDiffusion, a latent diffusion model that embeds an explicit inductive bias on the human body within its architecture and training. We present a nonisotropic Gaussian diffusion formulation that aligns with the natural kinematic structure of the human skeleton and models relationships between body parts. Results show that our approach outperforms isotropic alternatives, consistently generating realistic predictions while avoiding artifacts such as limb distortion. Additionally, we identify a limitation in commonly used diversity metrics, which may favor models that produce inconsistent limb lengths within the same sequence. SkeletonDiffusion sets a new benchmark on three real-world datasets, outperforming various baselines across multiple evaluation metrics. We release the code on our \\href https://ceveloper.github.io/publications/skeletondiffusion/ project page",
    "checked": true,
    "id": "daac6cf71bcbf4eeca56283c992acc7b1aae2c70",
    "semantic_title": "nonisotropic gaussian diffusion for realistic 3d human motion prediction",
    "citation_count": 1,
    "authors": [
      "Cecilia Curreli",
      "Dominik Muhle",
      "Abhishek Saroha",
      "Zhenzhang Ye",
      "Riccardo Marin",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Is_Your_World_Simulator_a_Good_Story_Presenter_A_Consecutive_CVPR_2025_paper.html": {
    "title": "Is Your World Simulator a Good Story Presenter? A Consecutive Events-Based Benchmark for Future Long Video Generation",
    "volume": "main",
    "abstract": "The current state-of-the-art video generative models can produce commercial-grade videos with highly realistic details. However, they still struggle to coherently present multiple sequential events in specific short stories, which is foreseeable an essential capability for future long video generation scenarios. For example, top T2V generative models still fail to generate a video of the short simple story \"how to put an elephant into a refrigerator.\" While existing detail-oriented benchmarks primarily focus on fine-grained metrics like aesthetic quality and spatial-temporal consistency, they fall short of evaluating models' abilities to handle event-level story presentation. To address this gap, we introduce StoryEval, a story-oriented benchmark specifically designed to assess text-to-video (T2V) models' story-completion capabilities. StoryEval features 423 prompts spanning 7 classes, each representing short stories composed of 2-4 consecutive events. We employ Vision-Language Models, such as GPT-4o and LLaVA-OV-Chat-72B, to verify the completion of each event in the generated videos, applying a unanimous voting method to enhance reliability. Our methods ensure high alignment with human evaluations, and the evaluation of 11 models reveals its challenge, with none exceeding an average story-completion rate of 50%. StoryEval provides a new benchmark for advancing T2V models and highlights the challenges and opportunities in developing next-generation solutions for coherent story-driven video generation. Project website is available at https://ypwang61.github.io/project/StoryEval",
    "checked": true,
    "id": "f4b8e6895a83493ff0893160f240993069e2f0db",
    "semantic_title": "is your world simulator a good story presenter? a consecutive events-based benchmark for future long video generation",
    "citation_count": 4,
    "authors": [
      "Yiping Wang",
      "Xuehai He",
      "Kuan Wang",
      "Luyao Ma",
      "Jianwei Yang",
      "Shuohang Wang",
      "Simon Shaolei Du",
      "Yelong Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_LookCloser_Frequency-aware_Radiance_Field_for_Tiny-Detail_Scene_CVPR_2025_paper.html": {
    "title": "LookCloser: Frequency-aware Radiance Field for Tiny-Detail Scene",
    "volume": "main",
    "abstract": "Humans perceive and comprehend their surroundings through information spanning multiple frequencies. In immersive scenes, people naturally scan their environment to grasp its overall structure while examining fine details of objects that capture their attention. However, current NeRF frameworks primarily focus on modeling either high-frequency local views or the broad structure of scenes with low-frequency information, limited to balance both. We introduce FA-NeRF, a novel frequency-aware framework for view synthesis that simultaneously captures the overall scene structure and high-definition details within a single NeRF model. To achieve this, we propose a 3D frequency quantification method that analyzes the scene's frequency distribution, enabling frequency-aware rendering. Our framework incorporates a frequency grid for fast convergence and querying, a frequency-aware feature re-weighting strategy to balance features across different frequency contents. Extensive experiments show that our method significantly outperforms existing approaches in modeling entire scenes while preserving fine details",
    "checked": true,
    "id": "bd7d51799bcf7ffd5333a8751d105efcae993112",
    "semantic_title": "lookcloser: frequency-aware radiance field for tiny-detail scene",
    "citation_count": 1,
    "authors": [
      "Xiaoyu Zhang",
      "Weihong Pan",
      "Chong Bao",
      "Xiyu Zhang",
      "Xiaojun Xiang",
      "Hanqing Jiang",
      "Hujun Bao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cseke_PICO_Reconstructing_3D_People_In_Contact_with_Objects_CVPR_2025_paper.html": {
    "title": "PICO: Reconstructing 3D People In Contact with Objects",
    "volume": "main",
    "abstract": "Recovering 3D Human-Object Interaction (HOI) from single color images is challenging due to depth ambiguities, occlusions, and the huge variation in object shape and appearance. Thus, past work requires controlled settings such as known object shapes and contacts, and tackles only limited object classes. Instead, we need methods that generalize to natural images and novel object classes. We tackle this in two main ways:(1) We collect PICO-db, a new dataset of natural images uniquely paired with dense 3D contact correspondences on both body and object meshes. To this end, we use images from the recent DAMON dataset that are paired with annotated contacts, but only on a canonical 3D body. In contrast, we seek contact labels on both the body and the object. To infer these given an image, we retrieve an appropriate 3D object mesh from a database by leveraging vision foundation models. Then, we project DAMON's body contact patches onto the object via a novel method needing only 2 clicks per patch. This minimal human input establishes rich contact correspondences between bodies and objects. (2) We exploit our new dataset in a novel render-and-compare fitting method, called PICO-fit, to recover 3D body and object meshes in interaction. PICO-fit infers contact for the SMPL-X body, retrieves a likely 3D object mesh and contact from PICO-db for that object, and uses the contact to iteratively fit the 3D body and object meshes to image evidence via optimization. Uniquely, PICO-fit works well for many object categories that no existing method can tackle. This is crucial for scaling HOI understanding in the wild. Our data and code are available at https://pico.is.tue.mpg.de",
    "checked": true,
    "id": "6e2977de7af56175fb5e3ef0df01449249faf50f",
    "semantic_title": "pico: reconstructing 3d people in contact with objects",
    "citation_count": 3,
    "authors": [
      "Alp√°r Cseke",
      "Shashank Tripathi",
      "Sai Kumar Dwivedi",
      "Arjun S. Lakshmipathy",
      "Agniv Chatterjee",
      "Michael J. Black",
      "Dimitrios Tzionas"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liao_Convex_Relaxation_for_Robust_Vanishing_Point_Estimation_in_Manhattan_World_CVPR_2025_paper.html": {
    "title": "Convex Relaxation for Robust Vanishing Point Estimation in Manhattan World",
    "volume": "main",
    "abstract": "Determining the vanishing points (VPs) in a Manhattan world, as a fundamental task in many 3D vision applications, consists of jointly inferring the line-VP association and locating each VP. Existing methods are, however, either sub-optimal solvers or pursuing global optimality at a significant cost of computing time. In contrast to prior works, we introduce convex relaxation techniques to solve this task for the first time. Specifically, we employ a \"soft\" association scheme, realized via a truncated multi-selection error, that allows for joint estimation of VPs' locations and line-VP associations. This approach leads to a primal problem that can be reformulated into a quadratically constrained quadratic programming (QCQP) problem, which is then relaxed into a convex semidefinite programming (SDP) problem. To solve this SDP problem efficiently, we present a globally optimal outlier-robust iterative solver (called GlobustVP), which independently searches for one VP and its associated lines in each iteration, treating other lines as outliers. After each independent update of all VPs, the mutual orthogonality between the three VPs in a Manhattan world is reinforced via local refinement. Extensive experiments on both synthetic and real-world data demonstrate that GlobustVP achieves a favorable balance between efficiency, robustness, and global optimality compared to previous works. The code is publicly available at github.com/wu-cvgl/GlobustVP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bangyan Liao",
      "Zhenjun Zhao",
      "Haoang Li",
      "Yi Zhou",
      "Yingping Zeng",
      "Hao Li",
      "Peidong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Linguistics-aware_Masked_Image_Modeling_for_Self-supervised_Scene_Text_Recognition_CVPR_2025_paper.html": {
    "title": "Linguistics-aware Masked Image Modeling for Self-supervised Scene Text Recognition",
    "volume": "main",
    "abstract": "Text images are unique in their dual nature, encompassing both visual and linguistic information. The visual component encompasses structural and appearance-based features, while the linguistic dimension incorporates contextual and semantic elements. In scenarios with degraded visual quality, linguistic patterns serve as crucial supplements for comprehension, highlighting the necessity of integrating both aspects for robust scene text recognition (STR). Contemporary STR approaches often use language models or semantic reasoning modules to capture linguistic features, typically requiring large-scale annotated datasets. Self-supervised learning, which lacks annotations, presents challenges in disentangling linguistic features related to the global context. Typically, sequence contrastive learning emphasizes the alignment of local features, while masked image modeling (MIM) tends to exploit local structures to reconstruct visual patterns, resulting in limited linguistic knowledge. In this paper, we propose a Linguistics-aware Masked Image Modeling (LMIM) approach, which channels the linguistic information into the decoding process of MIM through a separate branch. Specifically, we design a linguistics alignment module to extract vision-independent features as linguistic guidance using inputs with different visual appearances. As features extend beyond mere visual structures, LMIM must consider the global context to achieve reconstruction. Extensive experiments on various benchmarks quantitatively demonstrate our state-of-the-art performance, and attention visualizations qualitatively show the simultaneous capture of both visual and linguistic information. The code is available at https://github.com/zhangyifei01/LMIM",
    "checked": true,
    "id": "b923cf54549b83ef735763da10e00aaa9a5354fe",
    "semantic_title": "linguistics-aware masked image modeling for self-supervised scene text recognition",
    "citation_count": 6,
    "authors": [
      "Yifei Zhang",
      "Chang Liu",
      "Jin Wei",
      "Xiaomeng Yang",
      "Yu Zhou",
      "Can Ma",
      "Xiangyang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_FruitNinja_3D_Object_Interior_Texture_Generation_with_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "FruitNinja: 3D Object Interior Texture Generation with Gaussian Splatting",
    "volume": "main",
    "abstract": "In the real world, objects reveal internal textures when sliced or cut, yet this behavior is not well-studied in 3D generation tasks today. For example, slicing a virtual 3D watermelon should reveal flesh and seeds. Given that no available dataset captures an object's full internal structure and collecting data from all slices is impractical, generative methods become the obvious approach. However, current 3D generation and inpainting methods often focus on visible appearance and overlook internal textures. To bridge this gap, we introduce FruitNinja, the first method to generate internal textures for 3D objects undergoing geometric and topological changes. Our approach produces objects via 3D Gaussian Splatting (3DGS) with both surface and interior textures synthesized, enabling real-time slicing and rendering without additional optimization. FruitNinja leverages a pre-trained diffusion model to progressively inpaint cross-sectional views and applies voxel-grid-based smoothing to achieve cohesive textures throughout the object. Our OpaqueAtom GS strategy overcomes 3DGS limitations by employing densely distributed opaque Gaussians, avoiding biases toward larger particles that destabilize training and sharp color transitions for fine-grained textures. Experimental results show that FruitNinja substantially outperforms existing approaches, showcasing unmatched visual quality in real-time rendered internal views across arbitrary geometry manipulations. Project page: https://fanguw.github.io/FruitNinja3D",
    "checked": true,
    "id": "db23d16b15b463b946536b9436eefa98b3f2c219",
    "semantic_title": "fruitninja: 3d object interior texture generation with gaussian splatting",
    "citation_count": 1,
    "authors": [
      "Fangyu Wu",
      "Yuhao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Scaling_up_Image_Segmentation_across_Data_and_Tasks_CVPR_2025_paper.html": {
    "title": "Scaling up Image Segmentation across Data and Tasks",
    "volume": "main",
    "abstract": "Traditional segmentation models, while effective in isolated tasks, often fail to generalize to more complex and open-ended segmentation problems, such as free-form, open-vocabulary, and in-the-wild scenarios. To bridge this gap, we propose to scale up image segmentation across diverse datasets and tasks such that the knowledge across different tasks and datasets can be integrated while improving the generalization ability. QueryMeldNet, a novel segmentation framework, is introduced and designed to scale seamlessly across both data size and task diversity. It is built upon a dynamic object query mechanism called query meld, which fuses different types of queries using cross-attention. This hybrid approach enables the model to balance between instance- and stuff-level segmentation, providing enhanced scalability for handling diverse object types. We further enhance scalability by leveraging synthetic data-generating segmentation masks and captions for pixel-level and open-vocabulary tasks-drastically reducing the need for costly human annotations. By training on multiple datasets and tasks at scale, QueryMeldNet continuously improves performance as the volume and diversity of data and tasks increase. It exhibits strong generalization capabilities, boosting performance in open-set segmentation tasks SeginW by 7 points. These advancements mark a key step toward universal, scalable segmentation models capable of addressing the demands of real-world applications",
    "checked": true,
    "id": "bea191c12aa18e1d590177795347457be32ef9bc",
    "semantic_title": "scaling up image segmentation across data and tasks",
    "citation_count": 0,
    "authors": [
      "Pei Wang",
      "Zhaowei Cai",
      "Hao Yang",
      "Ashwin Swaminathan",
      "R. Manmatha",
      "Stefano Soatto"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Take_the_Bull_by_the_Horns_Learning_to_Segment_Hard_CVPR_2025_paper.html": {
    "title": "Take the Bull by the Horns: Learning to Segment Hard Samples",
    "volume": "main",
    "abstract": "Medical image segmentation is vital for clinical applications, with hard samples playing a key role in segmentation accuracy. We propose an effective image segmentation framework that includes mechanisms for identifying and segmenting hard samples. It derives a novel image segmentation paradigm: 1) Learning to identify hard samples: automatically selecting inherent hard samples from different datasets, and 2) Learning to segment hard samples: achieving the segmentation of hard samples through effective feature augmentation on dedicated networks. We name our method `Learning to Segment hard samples' (L2S). The hard sample identification module comprises a backbone model and a classifier, which dynamically uncovers inherent dataset patterns. The hard sample segmentation module utilizes the diffusion process for feature augmentation and incorporates a more sophisticated segmentation network to achieve precise segmentation. We justify our motivation through solid theoretical analysis and extensive experiments. Evaluations across various modalities show that our L2S outperforms other SOTA methods, particularly by substantially improving the segmentation accuracy of hard samples. On ISIC dataset, our L2S improves the Dice score on hard samples and overall segmentation by 8.97% and 1.01%, respectively, compared to SOTA methods. The code is available at \\href https://github.com/TqlYuanGie/L2S https://github.com/TqlYuanGie/L2S",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Guo",
      "Jingyu Kong",
      "Yu Wang",
      "Yuping Duan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_MIMO_A_Medical_Vision_Language_Model_with_Visual_Referring_Multimodal_CVPR_2025_paper.html": {
    "title": "MIMO: A Medical Vision Language Model with Visual Referring Multimodal Input and Pixel Grounding Multimodal Output",
    "volume": "main",
    "abstract": "Currently, medical vision language models are widely used in medical vision question answering tasks. However, existing models are confronted with two issues: for input, the model only relies on text instructions and lacks direct understanding of visual clues in the image; for output, the model only gives text answers and lacks connection with key areas in the image. To address these issues, we propose a unified medical vision language model MIMO, with visual referring Multimodal Input and pixel grounding Multimodal Output. MIMO can not only combine visual clues and textual instructions to understand complex medical images and semantics, but can also ground medical terminologies in textual output within the image. To overcome the scarcity of relevant data in the medical field, we propose MIMOSeg, a comprehensive medical multimodal dataset including 895K samples. MIMOSeg is constructed from four different perspectives, covering basic instruction following and complex question answering with multimodal input and multimodal output. We conduct experiments on several downstream medical multimodal tasks. Extensive experimental results verify that MIMO can uniquely combine visual referring and pixel grounding capabilities, which are not available in previous models. Our project can be found in https://github.com/pkusixspace/MIMO",
    "checked": true,
    "id": "be14dea67bfc5d77e009363702c07545ae567f11",
    "semantic_title": "mimo: a medical vision language model with visual referring multimodal input and pixel grounding multimodal output",
    "citation_count": 2,
    "authors": [
      "Yanyuan Chen",
      "Dexuan Xu",
      "Yu Huang",
      "Songkun Zhan",
      "Hanpin Wang",
      "Dongxue Chen",
      "Xueping Wang",
      "Meikang Qiu",
      "Hang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kayabasi_Bias_for_Action_Video_Implicit_Neural_Representations_with_Bias_Modulation_CVPR_2025_paper.html": {
    "title": "Bias for Action: Video Implicit Neural Representations with Bias Modulation",
    "volume": "main",
    "abstract": "We propose a new continuous video modeling framework based on implicit neural representations (INRs) called ActINR. At the core of our approach is the observation that INRs can be considered as a learnable dictionary, with the shapes of the basis functions governed by the weights of the INR, and their locations governed by the biases. Given compact non-linear activation functions, we hypothesize that an INR's biases are suitable to capture motion across images, and facilitate compact representations for video sequences. Using these observations, we design ActINR to share INR weights across frames of a video sequence, while using unique biases for each frame. We further model the biases as the output of a separate INR conditioned on time index to promote smoothness. By training the video INR and this bias INR together, we demonstrate unique capabilities, including 10x video slow motion, 4x spatial super resolution along with 2x slow motion, denoising, and video inpainting. ActINR performs remarkably well across numerous video processing tasks (often achieving more than 6dB improvement), setting a new standard for continuous modeling of videos",
    "checked": true,
    "id": "d63a95b93d8e5e0828b4dccf6335cdd0d794eb23",
    "semantic_title": "bias for action: video implicit neural representations with bias modulation",
    "citation_count": 1,
    "authors": [
      "Alper Kayabasi",
      "Anil Kumar Vadathya",
      "Guha Balakrishnan",
      "Vishwanath Saragadam"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Bridging_Past_and_Future_End-to-End_Autonomous_Driving_with_Historical_Prediction_CVPR_2025_paper.html": {
    "title": "Bridging Past and Future: End-to-End Autonomous Driving with Historical Prediction and Planning",
    "volume": "main",
    "abstract": "End-to-end autonomous driving unifies tasks in a differentiable framework, enabling planning-oriented optimization and attracting growing attention.Current methods aggregate historical information either through dense historical bird's-eye-view (BEV) features or by querying a sparse memory bank, following paradigms inherited from detection.However, we argue that these paradigms either omit historical information in motion planning or fail to align with its multi-step nature, which requires predicting or planning multiple future time steps. In line with the philosophy of \"future is a continuation of past\", we propose **BridgeAD**, which reformulates motion and planning queries as multi-step queries to differentiate the queries for each future time step. This design enables the effective use of historical prediction and planning by applying them to the appropriate parts of the end-to-end system based on the time steps, which improves both perception and motion planning. Specifically, historical queries for the current frame are combined with perception, while queries for future frames are integrated with motion planning. In this way, we bridge the gap between past and future by aggregating historical insights at every time step, enhancing the overall coherence and accuracy of the end-to-end autonomous driving pipeline. Extensive experiments on the nuScenes dataset in both open-loop and closed-loop settings demonstrate that BridgeAD achieves state-of-the-art performance. We will make our code and models publicly available",
    "checked": true,
    "id": "939784d0f5d874b360c9f1532a403536f7025ffe",
    "semantic_title": "bridging past and future: end-to-end autonomous driving with historical prediction and planning",
    "citation_count": 8,
    "authors": [
      "Bozhou Zhang",
      "Nan Song",
      "Xin Jin",
      "Li Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_Blood_Flow_Speed_Estimation_with_Optical_Coherence_Tomography_Angiography_Images_CVPR_2025_paper.html": {
    "title": "Blood Flow Speed Estimation with Optical Coherence Tomography Angiography Images",
    "volume": "main",
    "abstract": "Estimating blood flow speed is essential in many medical and physiological applications, yet it is extremely challenging due to complex vascular structure and flow dynamics, particularly for cerebral cortex regions. Existing techniques, such as Optical Doppler Tomography (ODT), generally require complex hardware control and signal processing, and still suffer from inherent system-level artifacts. To address these challenges, we propose a new learning-based approach named OCTA-Flow, which directly estimates vascular blood flow speed from Optical Coherence Tomography Angiography (OCTA) images that are commonly used for vascular structure analysis. OCTA-Flow employs several novel components to achieve this goal. First, using an encoder-decoder architecture, OCTA-Flow leverages ODT data as pseudo labels during training, thus bypassing the difficulty of collecting ground truth data. Second, to capture the relationship between vessels of varying scales and their flow speed, we design an Adaptive Window Fusion module that employs multiscale window attention. Third, to mitigate ODT artifacts, we incorporate a Conditional Random Field Decoder that promotes smoothness and consistency in the estimated blood flow. Together, these innovations enable OCTA-Flow to effectively produce accurate flow estimation, suppress the artifacts in ODT, and enhance practicality, benefiting from the established techniques of OCTA data acquisition. The code and data are available at https://github.com/Spritea/OCTA-Flow",
    "checked": true,
    "id": "c07bfbe3a10948beef21cb04082c8343f47cf949",
    "semantic_title": "blood flow speed estimation with optical coherence tomography angiography images",
    "citation_count": 0,
    "authors": [
      "Wensheng Cheng",
      "Zhenghong Li",
      "Jiaxiang Ren",
      "Hyomin Jeong",
      "Congwu Du",
      "Yingtian Pan",
      "Haibin Ling"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_DreamTrack_Dreaming_the_Future_for_Multimodal_Visual_Object_Tracking_CVPR_2025_paper.html": {
    "title": "DreamTrack: Dreaming the Future for Multimodal Visual Object Tracking",
    "volume": "main",
    "abstract": "Aiming to achieve class-agnostic perception in visual object tracking, current trackers commonly formulate tracking as a one-shot detection problem with the template-matching architecture. Despite the success, severe environmental variations in long-term tracking raise challenges to generalizing the tracker in novel situations. Temporal trackers try to fix it by preserving the time-validity of target information with historical predictions, e.g., updating the template. However, solely transmitting the previous observations instead of learning from them leads to an inferior capability of understanding the tracking scenario from past experience, which is critical for the generalization in new frames. To address this issue, we reformulate temporal learning in visual tracking as a History-to-Future process and propose a novel tracking framework DreamTrack. Our DreamTrack learns the temporal dynamics from past observations to dream the future variations of the environment, which boosts the generalization with the extended future information from history. Considering the uncertainty of future variation, multimodal prediction is designed to infer the target trajectory of each possible future situation. The experiments demonstrate that our DreamTrack achieves leading performance with real-time inference speed. In particular, DreamTrack obtains SUC scores of 76.6%/87.9% on LaSOT/TrackingNet, surpassing all recent SOTA trackers",
    "checked": true,
    "id": "77fe3f7c6249d1dc11440f1bc682b58c9282c9f5",
    "semantic_title": "dreamtrack: dreaming the future for multimodal visual object tracking",
    "citation_count": 0,
    "authors": [
      "Mingzhe Guo",
      "Weiping Tan",
      "Wenyu Ran",
      "Liping Jing",
      "Zhipeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_OmniStyle_Filtering_High_Quality_Style_Transfer_Data_at_Scale_CVPR_2025_paper.html": {
    "title": "OmniStyle: Filtering High Quality Style Transfer Data at Scale",
    "volume": "main",
    "abstract": "In this paper, we introduce OmniStyle-1M, a large-scale paired style transfer dataset comprising over one million content-style-stylized image triplets across 1,000 diverse style categories, each enhanced with textual descriptions and instruction prompts. We show that OmniStyle-1M can not only improve the generalization of style transfer models through supervised training but also facilitates precise control over target stylization. Especially, to ensure the quality of the dataset, we introduce OmniFilter, a comprehensive style transfer quality assessment framework, which filters high-quality triplets based on content preservation, style consistency, and aesthetic appeal. Building upon this foundation, we propose OmniStyle, a framework based on the Diffusion Transformer (DiT) architecture designed for high-quality and efficient style transfer. This framework supports both instruction-guided and image-guided style transfer, generating high resolution outputs with exceptional detail. Extensive qualitative and quantitative evaluations demonstrate OmniStyle's superior performance compared to existing approaches, highlighting its efficiency and versatility. OmniStyle-1M and its accompanying methodologies provide a significant contribution to advancing high-quality style transfer, offering a valuable resource for the research community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Wang",
      "Ruiqi Liu",
      "Jiang Lin",
      "Fei Liu",
      "Zili Yi",
      "Yilin Wang",
      "Rui Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jagpal_EIDT-V_Exploiting_Intersections_in_Diffusion_Trajectories_for_Model-Agnostic_Zero-Shot_Training-Free_CVPR_2025_paper.html": {
    "title": "EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation",
    "volume": "main",
    "abstract": "Zero-shot, training-free, image-based text-to-video generation is an emerging area that aims to generate videos using existing image-based diffusion models. Current methods in this space require specific architectural changes to image-generation models, which limit their adaptability and scalability.In contrast to such methods, we provide a model-agnostic approach. We use intersections in diffusion trajectories, working only with the latent values. We could not obtain localized frame-wise coherence and diversity using only the intersection of trajectories. Thus, we instead use a grid-based approach. An in-context trained LLM is used to generate coherent frame-wise prompts; another is used to identify differences between frames. Based on these, we obtain a CLIP-based attention mask that controls the timing of switching the prompts for each grid cell. Earlier switching results in higher variance, while later switching results in more coherence. Therefore, Our approach can ensure appropriate control between coherence and variance for the frames.Our approach results in state-of-the-art performance while being more flexible when working with diverse image-generation models. The empirical analysis using quantitative metrics and user studies confirms our model's superior temporal consistency, visual fidelity and user satisfaction, thus providing a novel way to obtain training-free, image-based text-to-video generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diljeet Jagpal",
      "Xi Chen",
      "Vinay P. Namboodiri"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/An_Cross-View_Completion_Models_are_Zero-shot_Correspondence_Estimators_CVPR_2025_paper.html": {
    "title": "Cross-View Completion Models are Zero-shot Correspondence Estimators",
    "volume": "main",
    "abstract": "In this work, we analyze new aspects of cross-view completion, mainly through the analogy of cross-view completion and traditional self-supervised correspondence learning algorithms. Based on our analysis, we reveal that the cross-attention map of Croco-v2, best reflects this correspondence information compared to other correlations from the encoder or decoder features. We further verify the effectiveness of the cross-attention map by evaluating on both zero-shot and supervised dense geometric correspondence and multi-frame depth estimation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Honggyu An",
      "Jin Hyeon Kim",
      "Seonghoon Park",
      "Jaewoo Jung",
      "Jisang Han",
      "Sunghwan Hong",
      "Seungryong Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Multi-party_Collaborative_Attention_Control_for_Image_Customization_CVPR_2025_paper.html": {
    "title": "Multi-party Collaborative Attention Control for Image Customization",
    "volume": "main",
    "abstract": "The rapid development of diffusion models has fueled a growing demand for customized image generation. However, current customization methods face several limitations: 1) typically accept either image or text conditions alone; 2) customization in complex visual scenarios often leads to subject leakage or confusion; 3) image-conditioned outputs tend to suffer from inconsistent backgrounds; and 4) high computational costs. To address these issues, this paper introduces Multi-party Collaborative Attention Control (MCA-Ctrl), a tuning-free approach that enables high-quality image customization under both text and complex visual conditions. Specifically, MCA-Ctrl leverages two key operations within the self-attention layer to coordinate multiple parallel diffusion processes and guide the target image generation. This approach allows MCA-Ctrl to capture the content and appearance of specific subjects while maintaining semantic consistency with the conditional input. Additionally, to mitigate subject leakage and confusion issues common in complex visual scenarios, we introduce a Subject Localization Module that extracts precise subject and editable image layers based on user instructions. Extensive quantitative and human evaluation experiments demonstrate that MCA-Ctrl outperforms previous methods in zero-shot image customization, effectively addressing the aforementioned issues",
    "checked": true,
    "id": "145579a9e0d660a717a8547e55e5a90b3ce14710",
    "semantic_title": "multi-party collaborative attention control for image customization",
    "citation_count": 2,
    "authors": [
      "Han Yang",
      "Chuanguang Yang",
      "Qiuli Wang",
      "Zhulin An",
      "Weilun Feng",
      "Libo Huang",
      "Yongjun Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Reproducible_Vision-Language_Models_Meet_Concepts_Out_of_Pre-Training_CVPR_2025_paper.html": {
    "title": "Reproducible Vision-Language Models Meet Concepts Out of Pre-Training",
    "volume": "main",
    "abstract": "Contrastive Language-Image Pre-training (CLIP) models as a milestone of modern multimodal intelligence, its generalization mechanism grasped massive research interests in the community. While existing studies limited in the scope of pre-training knowledge, hardly underpinned its generalization to countless open-world concepts absent from the pre-training regime. This paper dives into such Out-of-Pre-training (OOP) generalization problem from a holistic perspective. We propose LAION-Beyond benchmark to isolate the evaluation of OOP concepts from pre-training knowledge, with regards to OpenCLIP and its reproducible variants derived from LAION datasets. Empirical analysis evidences that despite image features of OOP concepts born with significant category margins, their zero-shot transfer significantly fails due to the poor image-text alignment. To this, we elaborate the \"name-tuning\" methodology with its theoretical merits in terms of OOP generalization, then propose few-shot name learning (FSNL) and zero-shot name learning (ZSNL) algorithms to achieve OOP generalization in a data-efficient manner. Their superiority have been further verified in our comprehensive experiments",
    "checked": true,
    "id": "4633d43f1f2427df3ef8481304c31067ad4f6eda",
    "semantic_title": "reproducible vision-language models meet concepts out of pre-training",
    "citation_count": 1,
    "authors": [
      "Ziliang Chen",
      "Xin Huang",
      "Xiaoxuan Fan",
      "Keze Wang",
      "Yuyu Zhou",
      "Quanlong Guan",
      "Liang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yariv_Through-The-Mask_Mask-based_Motion_Trajectories_for_Image-to-Video_Generation_CVPR_2025_paper.html": {
    "title": "Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation",
    "volume": "main",
    "abstract": "We consider the task of Image-to-Video (I2V) generation, which involves transforming static images into realistic video sequences based on a textual description. While recent advancements produce photorealistic outputs, they frequently struggle to create videos with accurate and consistent object motion, especially in multi-object scenarios. To address these limitations, we propose a two-stage compositional framework that decomposes I2V generation into: (i) An explicit intermediate representation generation stage, followed by (ii) A video generation stage that is conditioned on this representation. Our key innovation is the introduction of a mask-based motion trajectory as an intermediate representation, that captures both semantic object information and motion, enabling an expressive but compact representation of motion and semantics. To incorporate the learned representation in the second stage, we utilize object-level attention objectives. Specifically, we consider a spatial, per-object, masked-cross attention objective, integrating object-specific prompts into corresponding latent space regions and a masked spatio-temporal self-attention objective, ensuring frame-to-frame consistency for each object. We evaluate our method on challenging benchmarks with multi-object and high-motion scenarios and empirically demonstrate that the proposed method achieves state-of-the-art results in temporal coherence, motion realism, and text-prompt faithfulness. Additionally, we introduce SA-V-128, a new challenging benchmark for single-object and multi-object I2V generation, and demonstrate our method's superiority on this benchmark. Project page is available at https://guyyariv.github.io/TTM/",
    "checked": true,
    "id": "44eac98880108aef441ee8ca0c0edac9a8d53b67",
    "semantic_title": "through-the-mask: mask-based motion trajectories for image-to-video generation",
    "citation_count": 3,
    "authors": [
      "Guy Yariv",
      "Yuval Kirstain",
      "Amit Zohar",
      "Shelly Sheynin",
      "Yaniv Taigman",
      "Yossi Adi",
      "Sagie Benaim",
      "Adam Polyak"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MAGE__Single_Image_to_Material-Aware_3D_via_the_Multi-View_CVPR_2025_paper.html": {
    "title": "MAGE : Single Image to Material-Aware 3D via the Multi-View G-Buffer Estimation Model",
    "volume": "main",
    "abstract": "With advances in deep learning models and the availability of large-scale 3D datasets, we have recently witnessed significant progress in single-view 3D reconstruction. However, existing methods often fail to reconstruct physically based material properties given a single image, limiting their applicability in complicated scenarios. This paper presents a novel approach (named MAGE) for generating 3D geometry with realistic decomposed material properties given a single image as input. Our method leverages inspiration from traditional computer graphics deferred rendering pipelines to introduce a multi-view G-buffer estimation model. The proposed model estimates G-buffers for various views as multi-domain images, including XYZ coordinates, normals, albedo, roughness, and metallic properties from a single-view RGB image. To address the inherent ambiguity and inconsistency in generating G-buffers simultaneously, we also formulate a deterministic network from the pretrained diffusion models and propose a lighting response loss that enforces consistency across these domains using PBR principles. Finally, we propose a large-scale synthetic dataset rich in material diversity for our model training. Experimental results demonstrate the effectiveness of our method in producing high-quality 3D meshes with rich material properties. Our code and dataset can be found at https://www.whyy.site/paper/mage",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyuan Wang",
      "Zhenwei Wang",
      "Xiaoxiao Long",
      "Cheng Lin",
      "Gerhard Hancke",
      "Rynson W.H. Lau"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tai_Segment_Anything_Even_Occluded_CVPR_2025_paper.html": {
    "title": "Segment Anything, Even Occluded",
    "volume": "main",
    "abstract": "Amodal instance segmentation, which aims to detect and segment both visible and invisible parts of objects in images, plays a crucial role in various applications, including autonomous driving, robotic manipulation, and scene understanding. While existing methods require training both front-end detectors and mask decoders jointly, this approach lacks flexibility and fails to leverage the strengths of pre-existing modal detectors. To address this limitation, we propose SAMEO, a novel framework that adapts the Segment Anything Model (SAM) as a versatile mask decoder capable of interfacing with various front-end detectors to enable mask prediction even for partially occluded objects. Acknowledging the constraints of limited amodal segmentation datasets, we introduce Amodal-LVIS, a large-scale synthetic dataset comprising 300K images derived from the modal LVIS and LVVIS datasets. This dataset significantly expands the training data available for amodal segmentation research. Our experimental results demonstrate that our approach, when trained on the newly extended dataset, including Amodal-LVIS, achieves remarkable zero-shot performance on both COCOA-cls and D2SA benchmarks, highlighting its potential for generalization to unseen scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei-En Tai",
      "Yu-Lin Shih",
      "Cheng Sun",
      "Yu-Chiang Frank Wang",
      "Hwann-Tzong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Banerjee_HOT3D_Hand_and_Object_Tracking_in_3D_from_Egocentric_Multi-View_CVPR_2025_paper.html": {
    "title": "HOT3D: Hand and Object Tracking in 3D from Egocentric Multi-View Videos",
    "volume": "main",
    "abstract": "We introduce HOT3D, a publicly available dataset for egocentric hand and object tracking in 3D. The dataset offers over 833 minutes (3.7M+ images) of recordings that feature 19 subjects interacting with 33 diverse rigid objects. In addition to simple pick-up, observe, and put-down actions, the subjects perform actions typical for a kitchen, office, and living room environment. The recordings include multiple synchronized data streams containing egocentric multi-view RGB/monochrome images, eye gaze signal, scene point clouds, and 3D poses of cameras, hands, and objects. The dataset is recorded with two headsets from Meta: Project Aria, which is a research prototype of AI glasses, and Quest 3, a virtual-reality headset that has shipped millions of units. Ground-truth poses were obtained by a motion-capture system using small optical markers attached to hands and objects. Hand annotations are provided in the UmeTrack and MANO formats, and objects are represented by 3D meshes with PBR materials obtained by an in-house scanner. In our experiments, we demonstrate the effectiveness of multi-view egocentric data for three popular tasks: 3D hand tracking, model-based 6DoF object pose estimation, and 3D lifting of unknown in-hand objects. The evaluated multi-view methods, whose benchmarking is uniquely enabled by HOT3D, significantly outperform their single-view counterparts",
    "checked": true,
    "id": "37202f8275411c21ebb0c3e4b0ce5466f9fd4800",
    "semantic_title": "hot3d: hand and object tracking in 3d from egocentric multi-view videos",
    "citation_count": 21,
    "authors": [
      "Prithviraj Banerjee",
      "Sindi Shkodrani",
      "Pierre Moulon",
      "Shreyas Hampali",
      "Shangchen Han",
      "Fan Zhang",
      "Linguang Zhang",
      "Jade Fountain",
      "Edward Miller",
      "Selen Basol",
      "Richard Newcombe",
      "Robert Wang",
      "Jakob Julian Engel",
      "Tomas Hodan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_DELT_A_Simple_Diversity-driven_EarlyLate_Training_for_Dataset_Distillation_CVPR_2025_paper.html": {
    "title": "DELT: A Simple Diversity-driven EarlyLate Training for Dataset Distillation",
    "volume": "main",
    "abstract": "Recent advances in dataset distillation have led to solutions in two main directions. The conventional batch-to-batch matching mechanism is ideal for small-scale datasets and includes bi-level optimization methods on models and syntheses, such as FRePo, RCIG, and RaT-BPTT, as well as other methods like distribution matching, gradient matching, and weight trajectory matching. Conversely, batch-to-global matching typifies decoupled methods, which are particularly advantageous for large-scale datasets. This approach has garnered substantial interest within the community, as seen in SRe^2L, G-VBSM, WMDD, and CDA. A primary challenge with the second approach is the lack of diversity among syntheses within each class since samples are optimized independently and the same global supervision signals are reused across different synthetic images. In this study, we propose a new Diversity-driven EarlyLate Training (DELT) scheme to enhance the diversity of images in batch-to-global matching with less computation. Our approach is conceptually simple yet effective, it partitions predefined IPC samples into smaller subtasks and employs local optimizations to distill each subset into distributions from distinct phases, reducing the uniformity induced by the unified optimization process. These distilled images from the subtasks demonstrate effective generalization when applied to the entire task. We conduct extensive experiments on CIFAR, Tiny-ImageNet, ImageNet-1K, and its sub-datasets. Our approach outperforms the previous state-of-the-art by 2~5% on average across different datasets and IPCs (images per class), increasing diversity per class by more than 5% while reducing synthesis time by up to 39.3% for enhancing the training efficiency",
    "checked": true,
    "id": "a57d40b2e3c035a31c529137c62cfa16c5f09bab",
    "semantic_title": "delt: a simple diversity-driven earlylate training for dataset distillation",
    "citation_count": 2,
    "authors": [
      "Zhiqiang Shen",
      "Ammar Sherif",
      "Zeyuan Yin",
      "Shitong Shao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_MESC-3DMining_Effective_Semantic_Cues_for_3D_Reconstruction_from_a_Single_CVPR_2025_paper.html": {
    "title": "MESC-3D:Mining Effective Semantic Cues for 3D Reconstruction from a Single Image",
    "volume": "main",
    "abstract": "Reconstructing 3D shapes from a single image plays an important role in computer vision. Many methods have been proposed and achieve impressive performance. However, existing methods mainly focus on extracting semantic information from images and then simply concatenating it with 3D point clouds without further exploring the concatenated semantics. As a result, these entangled semantic features significantly hinder the reconstruction performance. In this paper, we propose a novel single-image 3D reconstruction method called Mining Effective Semantic Cues for 3D Reconstruction from a Single Image (MESC-3D), which can actively mine effective semantic cues from entangled features. Specifically, we design an Effective Semantic Mining Module, which incorporates a point-selection map establish connections between point clouds and semantic attributes, enabling the point clouds to autonomously select the necessary information. Furthermore, to address the potential insufficiencies in semantic information from a single image, such as occlusions, inspired by the human ability to represent 3D objects using prior knowledge drawn from daily experiences, we introduce a 3D Semantic Prior Learning Module. This module employs contrastive learning paradigm along with a learnable text prompt to incorporates semantic understanding of spatial structures, enabling the model to interpret and reconstruct 3D objects with greater accuracy and realism, closely mirroring human perception of complex 3D environments. Extensive evaluations show that our method achieves significant improvements in reconstruction quality and robustness compared to prior works. Additionally, further experiments validate the strong generalization capabilities and excels in zero-shot preformance on unseen classes. Code is available at https://github.com/QINGQINGLE/MESC-3D",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaoming Li",
      "Qing Cai",
      "Songqi Kong",
      "Runqing Tan",
      "Heng Tong",
      "Shiji Qiu",
      "Yongguo Jiang",
      "Zhi Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ji_RoboBrain_A_Unified_Brain_Model_for_Robotic_Manipulation_from_Abstract_CVPR_2025_paper.html": {
    "title": "RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete",
    "volume": "main",
    "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have shown remarkable capabilities across various multimodal contexts. However, their application in robotic scenarios, particularly for long-horizon manipulation tasks, reveals significant limitations. These limitations arise from the current MLLMs lacking three essential robotic brain capabilities: Planning Capability, which involves decomposing complex manipulation instructions into manageable sub-tasks; Affordance Perception, the ability to recognize and interpret the affordances of interactive objects; and Trajectory Prediction, the foresight to anticipate the complete manipulation trajectory necessary for successful execution. To enhance the robotic brain's core capabilities from abstract to concrete, we introduce ShareRobot, a high-quality heterogeneous dataset that labels multi-dimensional information such as task planning, object affordance, and end-effector trajectory. ShareRobot's diversity and accuracy have been meticulously refined by three human annotators. Building on this dataset, we developed RoboBrain, an MLLM-based model that combines robotic and general multi-modal data, utilizes a multi-stage training strategy, and incorporates long videos and high-resolution images to improve its robotic manipulation capabilities.Extensive experiments demonstrate that RoboBrain achieves state-of-the-art performance across various obotic tasks, highlighting its potential to advance robotic brain capabilities",
    "checked": true,
    "id": "c1fa31734bf75df4cdd1a6a04f63307d9817ed9a",
    "semantic_title": "robobrain: a unified brain model for robotic manipulation from abstract to concrete",
    "citation_count": 35,
    "authors": [
      "Yuheng Ji",
      "Huajie Tan",
      "Jiayu Shi",
      "Xiaoshuai Hao",
      "Yuan Zhang",
      "Hengyuan Zhang",
      "Pengwei Wang",
      "Mengdi Zhao",
      "Yao Mu",
      "Pengju An",
      "Xinda Xue",
      "Qinghang Su",
      "Huaihai Lyu",
      "Xiaolong Zheng",
      "Jiaming Liu",
      "Zhongyuan Wang",
      "Shanghang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Advancing_Multiple_Instance_Learning_with_Continual_Learning_for_Whole_Slide_CVPR_2025_paper.html": {
    "title": "Advancing Multiple Instance Learning with Continual Learning for Whole Slide Imaging",
    "volume": "main",
    "abstract": "Advances in medical imaging and deep learning have propelled progress in whole slide image (WSI) analysis, with multiple instance learning (MIL) showing promise for efficient and accurate diagnostics. However, conventional MIL models often lack adaptability to evolving datasets, as they rely on static training that cannot incorporate new information without extensive retraining. Applying continual learning (CL) to MIL models is a possible solution, but often sees limited improvements. In this paper, we analyze CL in the context of attention MIL models and find that the model forgetting is mainly concentrated in the attention layers of the MIL model. Using the results of this analysis we propose two components for improving CL on MIL:Attention Knowledge Distillation (AKD) and the Pseudo-Bag Memory Pool (PMP). AKD mitigates catastrophic forgetting by focusing on retaining attention layer knowledge between learning sessions, while PMP reduces the memory footprint by selectively storing only the most informative patches, or \"pseudo-bags\" from WSIs. Experimental evaluations demonstrate that our method significantly improves both accuracy and memory efficiency on diverse WSI datasets, outperforming current state-of-the-art CL methods. This work provides a foundation for CL in large-scale, weakly annotated clinical datasets, paving the way for more adaptable and resilient diagnostic models",
    "checked": true,
    "id": "9a44e1b26b2d8ee5d90db7c0e232c442fca90db5",
    "semantic_title": "advancing multiple instance learning with continual learning for whole slide imaging",
    "citation_count": 0,
    "authors": [
      "Xianrui Li",
      "Yufei Cui",
      "Jun Li",
      "Antoni B. Chan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Beyond_Image_Classification_A_Video_Benchmark_and_Dual-Branch_Hybrid_Discrimination_CVPR_2025_paper.html": {
    "title": "Beyond Image Classification: A Video Benchmark and Dual-Branch Hybrid Discrimination Framework for Compositional Zero-Shot Learning",
    "volume": "main",
    "abstract": "Human reasoning naturally combines concepts to identify unseen compositions, a capability that Compositional Zero-Shot Learning (CZSL) aims to replicate in machine learning models. However, we observe that focusing solely on typical image classification tasks in CZSL may limit models' compositional generalization potential. To address this, we introduce C-EgoExo, a video-based benchmark, along with a compositional action recognition task to enable more comprehensive evaluations. Inspired by human reasoning processes, we propose a Dual-branch Hybrid Discrimination (DHD) framework, featuring two branches that decode visual inputs in distinct observation sequences. Through a cross-attention mechanism and a contextual dependency encoder, DHD effectively mitigates challenges posed by conditional variance. We further design a Copula-based orthogonal decoding loss to counteract contextual interference in primitive decoding. Our approach demonstrates outstanding performance across diverse CZSL tasks, excelling in both image-based and video-based modalities and in attribute-object and action-object compositions, setting a new benchmark for CZSL evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyao Jiang",
      "Haodong Jing",
      "Yongqiang Ma",
      "Nanning Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_ABBSPO_Adaptive_Bounding_Box_Scaling_and_Symmetric_Prior_based_Orientation_CVPR_2025_paper.html": {
    "title": "ABBSPO: Adaptive Bounding Box Scaling and Symmetric Prior based Orientation Prediction for Detecting Aerial Image Objects",
    "volume": "main",
    "abstract": "Weakly supervised Oriented Object Detection (WS-OOD) has gained attention as a cost-effective alternative to fully supervised methods, providing efficiency and high accuracy. Among weakly supervised approaches, horizontal bounding box (HBox) supervised OOD stands out for its ability to directly leverage existing HBox annotations while achieving the highest accuracy under weak supervision settings. This paper introduces adaptive bounding box scaling and symmetry-prior-based orientation prediction, called ABBSPO that is a framework for WS-OOD. Our ABBSPO addresses the limitations of previous HBox-supervised OOD methods, which compare ground truth (GT) HBoxes directly with predicted RBoxes' minimum circumscribed rectangles, often leading to inaccuracies. To overcome this, we propose: (i) Adaptive Bounding Box Scaling (ABBS) that appropriately scales the GT HBoxes to optimize for the size of each predicted RBox, ensuring more accurate prediction for RBoxes' scales; and (ii) a Symmetric Prior Angle (SPA) loss that uses the inherent symmetry of aerial objects for self-supervised learning, addressing the issue in previous methods where learning fails if they consistently make incorrect predictions for all three augmented views (original, rotated, and flipped). Extensive experimental results demonstrate that our ABBSPO achieves state-of-the-art results, outperforming existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woojin Lee",
      "Hyugjae Chang",
      "Jaeho Moon",
      "Jaehyup Lee",
      "Munchurl Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Decoupled_Distillation_to_Erase_A_General_Unlearning_Method_for_Any_CVPR_2025_paper.html": {
    "title": "Decoupled Distillation to Erase: A General Unlearning Method for Any Class-centric Tasks",
    "volume": "main",
    "abstract": "In this work, we present DEcoupLEd Distillation To Erase (DELETE), a general and strong unlearning method for any class-centric tasks. To derive this, we first propose a theoretical framework to analyze the general form of unlearning loss and decompose it into forgetting and retention terms. Through the theoretical framework, we point out that a class of previous methods could be mainly formulated as a loss that implicitly optimizes the forgetting term while lacking supervision for the retention term, disturbing the distribution of pre-trained model and struggling to adequately preserve knowledge of the remaining classes. To address it, we refine the retention term using \"dark knowledge\" and propose a mask distillation unlearning method. By applying a mask to separate forgetting logits from retention logits, our approach optimizes both the forgetting and refined retention components simultaneously, retaining knowledge of the remaining classes while ensuring thorough forgetting of the target class. Without access to the remaining data or intervention (i.e., used in some works), we achieve state-of-the-art performance across various benchmarks. What's more, DELETE is a general solution that can be applied to various downstream tasks, including face recognition, backdoor defense, and semantic segmentation with great performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Zhou",
      "Dian Zheng",
      "Qijie Mo",
      "Renjie Lu",
      "Kun-Yu Lin",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu-Hang_TAET_Two-Stage_Adversarial_Equalization_Training_on_Long-Tailed__Distributions_CVPR_2025_paper.html": {
    "title": "TAET: Two-Stage Adversarial Equalization Training on Long-Tailed Distributions",
    "volume": "main",
    "abstract": "Adversarial robustness remains a significant challenge in deploying deep neural networks for real-world applications. While adversarial training is widely acknowledged as a promising defense strategy, most existing studies primarily focus on balanced datasets, neglecting the fact that real-world data often exhibit a long-tailed distribution, which introduces substantial challenges to robustness. In this paper, we provide an in-depth analysis of adversarial training in the context of long-tailed distributions and identify the limitations of the current state-of-the-art method, AT-BSL, in achieving robust performance under such conditions. To address these challenges, we propose a novel training framework, TAET, which incorporates an initial stabilization phase followed by a stratified, equalization adversarial training phase. Furthermore, prior work on long-tailed robustness has largely overlooked a crucial evaluation metric--Balanced Accuracy. To fill this gap, we introduce the concept of Balanced Robustness, a comprehensive metric that measures robustness specifically under long-tailed distributions. Extensive experiments demonstrate that our method outperforms existing advanced defenses, yielding significant improvements in both memory and computational efficiency. We believe this work represents a substantial step forward in tackling robustness challenges in real-world applications. Our paper code can be found at https://github.com/BuhuiOK/TAET-Two-Stage-Adversarial-Equalization-Training-on-Long-Tailed-Distributions",
    "checked": true,
    "id": "5470d3e9d24ed99c751c686604722241e7e14a8b",
    "semantic_title": "taet: two-stage adversarial equalization training on long-tailed distributions",
    "citation_count": 1,
    "authors": [
      "Wang Yu-Hang",
      "Junkang Guo",
      "Aolei Liu",
      "Kaihao Wang",
      "Zaitong Wu",
      "Zhenyu Liu",
      "Wenfei Yin",
      "Jian Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xue_Few-shot_Personalized_Scanpath_Prediction_CVPR_2025_paper.html": {
    "title": "Few-shot Personalized Scanpath Prediction",
    "volume": "main",
    "abstract": "A personalized model for scanpath prediction provides insights into the visual preferences and attention patterns of individual subjects. However, existing methods for training scanpath prediction models are data-intensive and cannot be effectively personalized to new individuals with only a few available examples. In this paper, we propose few-shot personalized scanpath prediction task (FS-PSP) and a novel method to address it, which aims to predict scanpaths for an unseen subject using minimal support data of that subject's scanpath behavior. The key to our method's adaptability is the Subject-Embedding Network (SE-Net), specifically designed to capture unique, individualized representations for each subject's scanpaths. SE-Net generates subject embeddings that effectively distinguish between subjects while minimizing variability among scanpaths from the same individual. The personalized scanpath prediction model is then conditioned on these subject embeddings to produce accurate, personalized results. Experiments on multiple eye-tracking datasets demonstrate that our method excels in FS-PSP settings and does not require any fine-tuning steps at test time. Code is available at: https://github.com/cvlab-stonybrook/few-shot-scanpath",
    "checked": true,
    "id": "d3ae70f6b31c274087eb24754fe1d1523847cd3b",
    "semantic_title": "few-shot personalized scanpath prediction",
    "citation_count": 0,
    "authors": [
      "Ruoyu Xue",
      "Jingyi Xu",
      "Sounak Mondal",
      "Hieu Le",
      "Greg Zelinsky",
      "Minh Hoai",
      "Dimitris Samaras"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kang_Do_Your_Best_and_Get_Enough_Rest_for_Continual_Learning_CVPR_2025_paper.html": {
    "title": "Do Your Best and Get Enough Rest for Continual Learning",
    "volume": "main",
    "abstract": "According to the forgetting curve theory, we can enhance memory retention by learning extensive data and taking adequate rest. This means that in order to effectively retain new knowledge, it is essential to learn it thoroughly and ensure sufficient rest so that our brain can memorize without forgetting. The main takeaway from this theory is that learning extensive data at once necessitates sufficient rest before learning the same data again. This aspect of human long-term memory retention can be effectively utilized to address the continual learning of neural networks. Retaining new knowledge for a long period of time without catastrophic forgetting is the critical problem of continual learning. Therefore, based on Ebbinghaus' theory, we introduce the view-batch model that adjusts the learning schedules to optimize the recall interval between retraining the same samples. The proposed view-batch model allows the network to get enough rest to learn extensive knowledge from the same samples with a recall interval of sufficient length. To this end, we specifically present two approaches: 1) a replay method that guarantees the optimal recall interval, and 2) a self-supervised learning that acquires extensive knowledge from a single training sample at a time. We empirically show that these approaches of our method are aligned with the forgetting curve theory, which can enhance long-term memory. In our experiments, we also demonstrate that our method significantly improves many state-of-the-art continual learning methods in various protocols and scenarios. We open-source this project at https://github.com/hankyul2/ViewBatchModel",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hankyul Kang",
      "Gregor Seifer",
      "Donghyun Lee",
      "Jongbin Ryu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Enhancing_Few-Shot_Class-Incremental_Learning_via_Training-Free_Bi-Level_Modality_Calibration_CVPR_2025_paper.html": {
    "title": "Enhancing Few-Shot Class-Incremental Learning via Training-Free Bi-Level Modality Calibration",
    "volume": "main",
    "abstract": "Few-shot Class-Incremental Learning (FSCIL) challenges models to adapt to new classes with limited samples, presenting greater difficulties than traditional classincremental learning. While existing approaches rely heavily on visual models and require additional training during base or incremental phases, we propose a training-free framework that leverages pre-trained visual-language models like CLIP. At the core of our approach is a novel Bilevel Modality Calibration (BiMC) strategy. Our framework initially performs intra-modal calibration, combining LLM-generated fine-grained category descriptions with visual prototypes from the base session to achieve precise classifier estimation. This is further complemented by inter-modal calibration that fuses pre-trained linguistic knowledge with task-specific visual priors to mitigate modality-specific biases. To enhance prediction robustness, we introduce additional metrics and strategies that maximize the utilization of limited data. Extensive experimental results demonstrate that our approach significantly outperforms existing methods. Code is available at: https://github.com/yychen016/BiMC",
    "checked": true,
    "id": "199d747dbd807a4ce6f29b68b75f86428ceb8071",
    "semantic_title": "enhancing few-shot class-incremental learning via training-free bi-level modality calibration",
    "citation_count": 0,
    "authors": [
      "Yiyang Chen",
      "Tianyu Ding",
      "Lei Wang",
      "Jing Huo",
      "Yang Gao",
      "Wenbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cabon_MUSt3R_Multi-view_Network_for_Stereo_3D_Reconstruction_CVPR_2025_paper.html": {
    "title": "MUSt3R: Multi-view Network for Stereo 3D Reconstruction",
    "volume": "main",
    "abstract": "DUSt3R introduced a novel paradigm in geometric computer vision by proposing a model that can provide dense and unconstrained Stereo 3D Reconstruction of arbitrary image collections with no prior information about camera calibration nor viewpoint poses. Under the hood, however, DUSt3R processes image pairs, regressing local 3D reconstructions that need to be aligned in a global coordinate system. The number of pairs, growing quadratically, is an inherent limitation that becomes especially concerning for robust and fast optimization in the case of large image collections. In this paper, we propose an extension of DUSt3R from pairs to multiple views, that addresses all aforementioned concerns. Indeed, we propose a Multi-view Network for Stereo 3D Reconstruction, or MUSt3R, that modifies the DUSt3R architecture by making it symmetric and extending it to directly predict 3D structure for all views in a common coordinate frame. Second, we entail the model with a multi-layer memory mechanism which allows to reduce the computational complexity and to scale the reconstruction to large collections, inferring thousands of 3D pointmaps at high frame-rates with limited added complexity. The framework is designed to perform 3D reconstruction both offline and online, and hence can be seamlessly applied to SfM and visual SLAM scenarios showing state-of-the-art performance on various 3D downstream tasks, including uncalibrated Visual Odometry, relative camera pose, scale and focal estimation, 3D reconstruction and multi-view depth estimation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yohann Cabon",
      "Lucas Stoffl",
      "Leonid Antsfeld",
      "Gabriela Csurka",
      "Boris Chidlovskii",
      "Jerome Revaud",
      "Vincent Leroy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Hybrid-Level_Instruction_Injection_for_Video_Token_Compression_in_Multi-modal_Large_CVPR_2025_paper.html": {
    "title": "Hybrid-Level Instruction Injection for Video Token Compression in Multi-modal Large Language Models",
    "volume": "main",
    "abstract": "Recent Multi-modal Large Language Models (MLLMs) have been challenged by the computational overhead resulting from massive video frames, often alleviated through compression strategies. However, the visual content is not equally contributed to user instructions, existing strategies (e.g., average pool) inevitably lead to the loss of potentially useful information. To tackle this, we propose the Hybrid-level Instruction Injection Strategy for Conditional Token Compression in MLLMs (HICom), utilizing the instruction as a condition to guide the compression from both local and global levels. This encourages the compression to retain the maximum amount of user-focused information while reducing visual tokens to minimize computational burden. Specifically, the instruction condition is injected into the grouped visual tokens at the local level and the learnable tokens at the global level, and we conduct the attention mechanism to complete the conditional compression. From the hybrid-level compression, the instruction-relevant visual parts are highlighted while the temporal-spatial structure is also preserved for easier understanding of LLMs. To further unleash the potential of HICom, we introduce a new conditional pre-training stage with our proposed dataset HICom-248K. Experiments show that our HICom can obtain distinguished video understanding ability with fewer tokens, increasing the performance by 2.43% average on three multiple-choice QA benchmarks and saving 78.8% tokens compared with the SOTA method. The code is available at https://github.com/lntzm/HICom",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihang Liu",
      "Chen-Wei Xie",
      "Pandeng Li",
      "Liming Zhao",
      "Longxiang Tang",
      "Yun Zheng",
      "Chuanbin Liu",
      "Hongtao Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Mamba4D_Efficient_4D_Point_Cloud_Video_Understanding_with_Disentangled_Spatial-Temporal_CVPR_2025_paper.html": {
    "title": "Mamba4D: Efficient 4D Point Cloud Video Understanding with Disentangled Spatial-Temporal State Space Models",
    "volume": "main",
    "abstract": "Point cloud videos can faithfully capture real-world spatial geometries and temporal dynamics, which are essential for enabling intelligent agents to understand the dynamically changing world. However, designing an effective 4D backbone remains challenging, mainly due to the irregular and unordered distribution of points and temporal inconsistencies across frames. Also, recent transformer-based 4D backbones commonly suffer from large computational costs due to their quadratic complexity, particularly for long video sequences. To address these challenges, we propose a novel point cloud video understanding backbone purely based on the State Space Models (SSMs). Specifically, we first disentangle space and time in 4D video sequences and then establish the spatio-temporal correlation with the unified spatial-temporal Mamba blocks. The Intra-frame Spatial Mamba module is developed to encode locally similar geometric structures within a certain temporal stride. Subsequently, locally correlated tokens are delivered to the Inter-frame Temporal Mamba module, which integrates long-term point features across the entire video with linear complexity. Our proposed Mamba4D achieves competitive performance on the MSR-Action3D action recognition (+10.4% accuracy), HOI4D action segmentation (+0.7 F1 Score), and Synthia4D semantic segmentation (+0.19 mIoU) datasets. Mamba4D also has a significant efficiency improvement, especially for long video sequences, with 87.5% GPU memory reduction and 5.36 times speed-up. Codes are released at https://github.com/IRMVLab/Mamba4D",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiuming Liu",
      "Jinru Han",
      "Lihao Liu",
      "Angelica I. Aviles-Rivero",
      "Chaokang Jiang",
      "Zhe Liu",
      "Hesheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Mamba_as_a_Bridge_Where_Vision_Foundation_Models_Meet_Vision_CVPR_2025_paper.html": {
    "title": "Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation",
    "volume": "main",
    "abstract": "Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained traction in Domain Generalized Semantic Segmentation (DGSS) due to their strong generalization capabilities. However, existing DGSS methods often rely exclusively on either VFMs or VLMs, overlooking their complementary strengths. VFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g., CLIP) provide robust text alignment but struggle with coarse granularity. Despite their complementary strengths, effectively integrating VFMs and VLMs with attention mechanisms is challenging, as the increased patch tokens complicate long-sequence modeling. To address this, we propose MFuser, a novel Mamba-based fusion framework that efficiently combines the strengths of VFMs and VLMs while maintaining linear scalability in sequence length. MFuser consists of two key components: MVFuser, which acts as a co-adapter to jointly fine-tune the two models by capturing both sequential and spatial dynamics; and MTEnhancer, a hybrid attention-Mamba module that refines text embeddings by incorporating image priors. Our approach achieves precise feature locality and strong text alignment without incurring significant computational overhead. Extensive experiments demonstrate that MFuser significantly outperforms state-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and 71.87 mIoU on real-to-real benchmarks. The code is available at https://github.com/devinxzhang/MFuser",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Zhang",
      "Robby T. Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Vision-Guided_Action_Enhancing_3D_Human_Motion_Prediction_with_Gaze-informed_Affordance_CVPR_2025_paper.html": {
    "title": "Vision-Guided Action: Enhancing 3D Human Motion Prediction with Gaze-informed Affordance in 3D Scenes",
    "volume": "main",
    "abstract": "Recent advances in human motion prediction (HMP) have shifted focus from isolated motion data to integrating human-scene correlations. In particular, the latest methods leverage human gaze points, using their spatial coordinates to indicate intent--where a person might move within a 3D environment. Despite promising trajectory results, these methods often produce inaccurate poses by overlooking the semantic implications of gaze, specifically the affordances of observed objects, which indicate the possible interactions. To address this, we propose GAP3DS, an affordance-aware HMP model that utilizes gaze-informed object affordances to improve HMP in complex 3D environments. GAP3DS incorporates a gaze-guided affordance learner to identify relevant objects in the scene and infer their affordances based on human gaze, thus contextualizing future human-object interactions. This affordance information, enriched with visual features and gaze data, conditions the generation of multiple human-object interaction poses, which are subsequently decoded into final motion predictions. Extensive experiments on two real-world datasets demonstrate that GAP3DS outperforms state-of-the-art methods in both trajectory and pose accuracy, producing more physically consistent and contextually grounded predictions. For more details and code, please refer to the project page",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting Yu",
      "Yi Lin",
      "Jun Yu",
      "Zhenyu Lou",
      "Qiongjie Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_LOGICZSL_Exploring_Logic-induced_Representation_for_Compositional_Zero-shot_Learning_CVPR_2025_paper.html": {
    "title": "LOGICZSL: Exploring Logic-induced Representation for Compositional Zero-shot Learning",
    "volume": "main",
    "abstract": "Compositional zero-shot learning (CZSL) aims to recognize unseen attribute-object compositions by learning the primitive concepts (*i.e.*, attribute and object) from the training set. While recent works achieve impressive results in CZSL by leveraging large vision-language models like CLIP, they ignore the rich semantic relationships between primitive concepts and their compositions. In this work, we propose LOGICZSL, a novel logic-induced learning framework to explicitly model the semantic relationships. Our logic-induced learning framework formulates the relational knowledge constructed from large language models as a set of logic rules, and grounds them onto the training data. Our logic-induced losses are complementary to the widely used CZSL losses, therefore can be employed to inject the semantic information into any existing CZSL methods. Extensive experimental results show that our method brings significant performance improvements across diverse datasets (*i.e.*, CGQA, UT-Zappos50K, MIT-States) with strong CLIP-based methods and settings (*i.e.*, Close World, Open World)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Wu",
      "Xiankai Lu",
      "Hao Hu",
      "Yongqin Xian",
      "Jianbing Shen",
      "Wenguan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zeng_ChainHOI_Joint-based_Kinematic_Chain_Modeling_for_Human-Object_Interaction_Generation_CVPR_2025_paper.html": {
    "title": "ChainHOI: Joint-based Kinematic Chain Modeling for Human-Object Interaction Generation",
    "volume": "main",
    "abstract": "We propose ChainHOI, a novel approach for text-driven human-object interaction (HOI) generation that explicitly models interactions at both the joint and kinetic chain levels. Unlike existing methods that implicitly model interactions using full-body poses as tokens, we argue that explicitly modeling joint-level interactions is more natural and effective for generating realistic HOIs, as it directly captures the geometric and semantic relationships between joints, rather than modeling interactions in the latent pose space. To this end, ChainHOI introduces a novel joint graph to capture potential interactions with objects, and a Generative Spatiotemporal Graph Convolution Network to explicitly model interactions at the joint level. Furthermore, we propose a Kinematics-based Interaction Module that explicitly models interactions at the kinetic chain level, ensuring more realistic and biomechanically coherent motions. Evaluations on two public datasets demonstrate that ChainHOI significantly outperforms previous methods, generating more realistic, and semantically consistent HOIs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ling-An Zeng",
      "Guohong Huang",
      "Yi-Lin Wei",
      "Shengbo Gu",
      "Yu-Ming Tang",
      "Jingke Meng",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pitawela_CLOC_Contrastive_Learning_for_Ordinal_Classification_with_Multi-Margin_N-pair_Loss_CVPR_2025_paper.html": {
    "title": "CLOC: Contrastive Learning for Ordinal Classification with Multi-Margin N-pair Loss",
    "volume": "main",
    "abstract": "In ordinal classification, misclassifying neighboring ranks is common, yet the consequences of these errors are not the same.For example, misclassifying benign tumor categories is less consequential, compared to an error at the pre-cancerous to cancerous threshold, which could profoundly influence treatment choices. Despite this, existing ordinal classification methods do not account for the varying importance of these margins, treating all neighboring classes as equally significant. To address this limitation, we propose CLOC, a new margin-based contrastive learning method for ordinal classification that learns an ordered representation based on the optimization of multiple margins with a novel multi-margin n-pair loss (MMNP).CLOC enables flexible decision boundaries across key adjacent categories, facilitating smooth transitions between classes and reducing the risk of overfitting to biases present in the training data.We provide empirical discussion regarding the properties of MMNP and show experimental results on five real-world image datasets (Adience, Historical Colour Image Dating, Knee Osteoarthritis, Indian Diabetic Retinopathy Image, and Breast Carcinoma Subtyping) and one synthetic dataset simulating clinical decision bias.Our results demonstrate that CLOC outperforms existing ordinal classification methods and show the interpretability and controllability of CLOC in learning meaningful, ordered representations that align with clinical and practical needs",
    "checked": true,
    "id": "7700b46ac3e76407652c19707bb612adf959dbe7",
    "semantic_title": "cloc: contrastive learning for ordinal classification with multi-margin n-pair loss",
    "citation_count": 2,
    "authors": [
      "Dileepa Pitawela",
      "Gustavo Carneiro",
      "Hsiang-Ting Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_Universal_Actions_for_Enhanced_Embodied_Foundation_Models_CVPR_2025_paper.html": {
    "title": "Universal Actions for Enhanced Embodied Foundation Models",
    "volume": "main",
    "abstract": "Training on diverse, internet-scale data is a key factor in the success of recent large foundation models. Yet, using the same recipe for building embodied agents has faced noticeable difficulties. Despite the availability of many crowd-sourced embodied datasets, their action spaces often exhibit significant heterogeneity due to distinct physical embodiment and control interfaces for different robots, causing substantial challenges in developing embodied foundation models using cross-embodiment data. In this paper, we introduce UniAct, a new embodied foundation modeling framework operating in the Universal Action Space. Our learned universal actions capture the generic behaviors across diverse robots by exploiting their shared structural features, and enable enhanced cross-domain data utilization and cross-embodiment generalizations by eliminating the notorious heterogeneity. Moreover, the universal actions can be efficiently translated back to heterogeneous actionable commands by simply adding embodiment-specific details, from which fast adaptation to new robots becomes simple and straightforward. Our 0.5B instantiation of UniAct outperforms 14X larger SOTA embodied foundation models in extensive evaluations on various real-world and simulation robots, showcasing exceptional cross-embodiment control and adaptation capability, highlighting the crucial benefit of adopting universal actions",
    "checked": true,
    "id": "1801573190814440f436be7f8dec7c4ddede6445",
    "semantic_title": "universal actions for enhanced embodied foundation models",
    "citation_count": 22,
    "authors": [
      "Jinliang Zheng",
      "Jianxiong Li",
      "Dongxiu Liu",
      "Yinan Zheng",
      "Zhihao Wang",
      "Zhonghong Ou",
      "Yu Liu",
      "Jingjing Liu",
      "Ya-Qin Zhang",
      "Xianyuan Zhan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_ObjectMover_Generative_Object_Movement_with_Video_Prior_CVPR_2025_paper.html": {
    "title": "ObjectMover: Generative Object Movement with Video Prior",
    "volume": "main",
    "abstract": "Simple as it seems, moving an object to another location within an image is, in fact, a challenging image-editing task that requires re-harmonizing the lighting, adjusting the pose based on perspective, accurately filling occluded regions, and ensuring coherent synchronization of shadows and reflections while maintaining the object identity. In this paper, we present ObjectMover, a generative model that can perform object movement in highly challenging scenes. Our key insight is that we model this task as a sequence-to-sequence problem and fine-tune a video generation model to leverage its knowledge of consistent object generation across video frames. We show that with this approach, our model is able to adjust to complex real-world scenarios, handling extreme lighting harmonization and object effect movement. As large-scale data for object movement are unavailable, we construct a data generation pipeline using a modern game engine to synthesize high-quality data pairs. We further propose a multi-task learning strategy that enables training on real-world video data to improve the model generalization. Through extensive experiments, we demonstrate that ObjectMover achieves outstanding results and adapts well to real-world scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Yu",
      "Tianyu Wang",
      "Soo Ye Kim",
      "Paul Guerrero",
      "Xi Chen",
      "Qing Liu",
      "Zhe Lin",
      "Xiaojuan Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_FaithDiff_Unleashing_Diffusion_Priors_for_Faithful_Image_Super-resolution_CVPR_2025_paper.html": {
    "title": "FaithDiff: Unleashing Diffusion Priors for Faithful Image Super-resolution",
    "volume": "main",
    "abstract": "Faithful image super-resolution (SR) not only needs to recover images that appear realistic, similar to image generation tasks, but also requires that the restored images maintain fidelity and structural consistency with the input. To this end, we propose a simple and effective method, named FaithDiff, to fully harness the impressive power of latent diffusion models (LDMs) for faithful image SR. In contrast to existing diffusion-based SR methods that freeze the diffusion model pre-trained on high-quality images, we propose to unleash the diffusion prior to identify useful information and recover faithful structures. As there exists a significant gap between the features of degraded inputs and the noisy latent from the diffusion model, we then develop an effective alignment module to explore useful features from degraded inputs to align well with the diffusion process. Considering the indispensable roles and interplay of the encoder and diffusion model in LDMs, we jointly fine-tune them in a unified optimization framework, facilitating the encoder to extract useful features that coincide with diffusion process. Extensive experimental results demonstrate that FaithDiff outperforms state-of-the-art methods, providing high-quality and faithful SR results",
    "checked": true,
    "id": "a50689d9ffa6e996e221b34fa9459148e6841a81",
    "semantic_title": "faithdiff: unleashing diffusion priors for faithful image super-resolution",
    "citation_count": 4,
    "authors": [
      "Junyang Chen",
      "Jinshan Pan",
      "Jiangxin Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MLLM-as-a-Judge_for_Image_Safety_without_Human_Labeling_CVPR_2025_paper.html": {
    "title": "MLLM-as-a-Judge for Image Safety without Human Labeling",
    "volume": "main",
    "abstract": "Image content safety has become a significant challenge with the rise of visual media on online platforms. Meanwhile, in the age of AI-generated content (AIGC), many image generation models are capable of producing harmful content, such as images containing sexual or violent material. Thus, it becomes crucial to identify such unsafe images based on established safety rules. Pre-trained Multimodal Large Language Models (MLLMs) offer potential in this regard, given their strong pattern recognition abilities. Existing approaches typically fine-tune MLLMs with human-labeled datasets, which however brings a series of drawbacks. First, relying on human annotators to label data following intricate and detailed guidelines is both expensive and labor-intensive. Furthermore, users of safety judgment systems may need to frequently update safety rules, making fine-tuning on human-based annotation more challenging. This raises the research question: Can we detect unsafe images by querying MLLMs in a zero-shot setting using a predefined safety constitution (a set of safety rules)? Our research showed that simply querying pre-trained MLLMs does not yield satisfactory results. This lack of effectiveness stems from factors such as the subjectivity of safety rules, the complexity of lengthy constitutions, and the inherent biases in the models. To address these challenges, we propose a MLLM-based method includes objectifying safety rules, assessing the relevance between rules and images, making quick judgments based on debiased token probabilities with logically complete yet simplified precondition chains for safety rules, and conducting more in-depth reasoning with cascaded chain-of-thought processes if necessary. Experiment results demonstrate that our method is highly effective for zero-shot image safety judgment tasks",
    "checked": true,
    "id": "57655f3d72dbe52a2591c7a5dad9134fcffa85d6",
    "semantic_title": "mllm-as-a-judge for image safety without human labeling",
    "citation_count": 8,
    "authors": [
      "Zhenting Wang",
      "Shuming Hu",
      "Shiyu Zhao",
      "Xiaowen Lin",
      "Felix Juefei-Xu",
      "Zhuowei Li",
      "Ligong Han",
      "Harihar Subramanyam",
      "Li Chen",
      "Jianfa Chen",
      "Nan Jiang",
      "Lingjuan Lyu",
      "Shiqing Ma",
      "Dimitris N. Metaxas",
      "Ankit Jain"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bodrito_A_New_Statistical_Model_of_Star_Speckles_for_Learning_to_CVPR_2025_paper.html": {
    "title": "A New Statistical Model of Star Speckles for Learning to Detect and Characterize Exoplanets in Direct Imaging Observations",
    "volume": "main",
    "abstract": "The search for exoplanets is an active field in astronomy, with direct imaging as one of the most challenging methods due to faint exoplanet signals buried within stronger residual starlight. Successful detection requires advanced image processing to separate the exoplanet signal from this nuisance component. This paper presents a novel statistical model that captures nuisance fluctuations using a multi-scale approach, leveraging problem symmetries and a joint spectral channel representation grounded in physical principles. Our model integrates into an interpretable, end-to-end learnable framework for simultaneous exoplanet detection and flux estimation. The proposed algorithm is evaluated against the state of the art using datasets from the SPHERE instrument operating at the Very Large Telescope (VLT). It significantly improves the precision-recall trade-off, notably on challenging datasets that are otherwise unusable by astronomers. The proposed approach is computationally efficient, robust to varying data quality, and well suited for large-scale observational surveys",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Th√©o Bodrito",
      "Olivier Flasseur",
      "Julien Mairal",
      "Jean Ponce",
      "Maud Langlois",
      "Anne-Marie Lagrange"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_Scene-agnostic_Pose_Regression_for_Visual_Localization_CVPR_2025_paper.html": {
    "title": "Scene-agnostic Pose Regression for Visual Localization",
    "volume": "main",
    "abstract": "Absolute Pose Regression (APR) predicts 6D camera poses but lacks the adaptability to unknown environments without retraining, while Relative Pose Regression (RPR) generalizes better yet requires a large image retrieval database. Visual Odometry (VO) generalizes well in unseen environments but suffers from accumulated error in open trajectories. To address this dilemma, we introduce a new task, Scene-agnostic Pose Regression (SPR), which can achieve accurate pose regression in a flexible way while eliminating the need for retraining or databases. To benchmark SPR, we created a large-scale dataset, 360SPR, with over 200K photorealistic panoramas, 3.6M pinhole images and camera poses in 270 scenes at three different sensor heights. Furthermore, a SPR-Mamba model is initially proposed to address SPR in a dual-branch manner. Extensive experiments and studies demonstrate the effectiveness of our SPR paradigm, dataset, and model. In the unknown scenes of both 360SPR and 360Loc datasets, our method consistently outperforms APR, RPR and VO. The dataset and code are available at SPR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junwei Zheng",
      "Ruiping Liu",
      "Yufan Chen",
      "Zhenfang Chen",
      "Kailun Yang",
      "Jiaming Zhang",
      "Rainer Stiefelhagen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Damblon_Learning_to_Filter_Outlier_Edges_in_Global_SfM_CVPR_2025_paper.html": {
    "title": "Learning to Filter Outlier Edges in Global SfM",
    "volume": "main",
    "abstract": "We present a novel approach to enhance camera pose estimation in global Structure-from-Motion (SfM) frameworks by filtering inaccurate pose graph edges - representing relative translation estimates - before applying translation averaging. In SfM, pose graph vertices represent images, and edges represent relative poses (rotations and translations) between cameras. We reformulate the edge filtering problem as a vertex filtering in the dual graph, specifically, a line graph where vertices correspond to edges in the original graph and edges correspond to cameras. Utilizing this representation, we frame the problem as a binary classification over nodes in the dual graph. To identify outlier edges, we employ a Transformer-based architecture. To overcome the challenge of memory overflow caused by converting to a line graph, we introduce a clustering-based graph processing approach, enabling our method to be applied to arbitrarily large pose graphs. Our method outperforms existing relative translation filtering techniques in terms of camera position accuracy and can be seamlessly integrated with other filters. The code is available at https://github.com/DmblnNicole/LFOE-GlobalSfM",
    "checked": true,
    "id": "717eeeea9431d0e424dc44f2b29c69a816d92bcc",
    "semantic_title": "learning to filter outlier edges in global sfm",
    "citation_count": 0,
    "authors": [
      "Nicole Damblon",
      "Marc Pollefeys",
      "Daniel Barath"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pei_Divide_and_Conquer_Heterogeneous_Noise_Integration_for_Diffusion-based_Adversarial_Purification_CVPR_2025_paper.html": {
    "title": "Divide and Conquer: Heterogeneous Noise Integration for Diffusion-based Adversarial Purification",
    "volume": "main",
    "abstract": "Existing diffusion-based purification methods aim to disrupt adversarial perturbations by introducing a certain amount of noise through a forward diffusion process, followed by a reverse process to recover clean examples. However, this approach is fundamentally flawed: the uniform operation of the forward process across all pixels compromises normal pixels while attempting to combat adversarial perturbations, resulting in the target model producing incorrect predictions. Simply relying on low-intensity noise is insufficient for effective defense. To address this critical issue, we implement a heterogeneous purification strategy grounded in the interpretability of neural networks. Our method decisively applies higher-intensity noise to specific pixels that the target model focuses on while the remaining pixels are subjected to only low-intensity noise. This requirement motivates us to redesign the sampling process of the diffusion model, allowing for the effective removal of varying noise levels. Furthermore, to evaluate our method against strong adaptative attack, our proposed method sharply reduces time cost and memory usage through a single-step resampling. The empirical evidence from extensive experiments across three datasets demonstrates that our method outperforms most current adversarial training and purification techniques by a substantial margin",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaozheng Pei",
      "Shaojie Lyu",
      "Gong Chen",
      "Ke Ma",
      "Qianqian Xu",
      "Yingfei Sun",
      "Qingming Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_SEC-PromptSEmantic_Complementary_Prompting_for_Few-Shot_Class-Incremental_Learning_CVPR_2025_paper.html": {
    "title": "SEC-Prompt:SEmantic Complementary Prompting for Few-Shot Class-Incremental Learning",
    "volume": "main",
    "abstract": "Few-shot class-incremental learning (FSCIL) presents a significant challenge in machine learning, requiring models to integrate new classes from limited examples while preserving performance on previously learned classes. Recently, prompt-based CIL approaches leverage ample data to train prompts, effectively mitigating catastrophic forgetting. However, these methods do not account for the semantic features embedded in prompts, exacerbating the plasticity-stability dilemma in few-shot incremental learning. In this paper, we propose a novel and simple framework named SEmantic Complementary Prompt(SEC-Prompt), which learns two sets of semantically complementary prompts based on an adaptive query: discriminative prompts(D-Prompt) and non-discriminative prompts(ND-Prompt). D-Prompt enhances the separation of class-specific feature distributions by strengthening key discriminative features, while ND-Prompt balances non-discriminative information to promote generalization to novel classes. To efficiently learn high-quality knowledge from limited samples, we leverage ND-Prompt for data augmentation to increase sample diversity and introduce Prompt Clustering Loss to prevent noise contamination in D-Prompt, ensuring robust discriminative feature learning and improved generalization. Our experimental results showcase state-of-the-art performance across three benchmark datasets, including CIFAR100, ImageNet-R and CUB datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Liu",
      "Meng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_LiMoE_Mixture_of_LiDAR_Representation_Learners_from_Automotive_Scenes_CVPR_2025_paper.html": {
    "title": "LiMoE: Mixture of LiDAR Representation Learners from Automotive Scenes",
    "volume": "main",
    "abstract": "LiDAR data pretraining offers a promising approach to leveraging large-scale, readily available datasets for enhanced data utilization. However, existing methods predominantly focus on sparse voxel representation, overlooking the complementary attributes provided by other LiDAR representations. In this work, we propose LiMoE, a framework that integrates the Mixture of Experts (MoE) paradigm into LiDAR data representation learning to synergistically combine multiple representations, such as range images, sparse voxels, and raw points. Our approach consists of three stages: i) Image-to-LiDAR Pretraining, which transfers prior knowledge from images to point clouds across different representations; ii) Contrastive Mixture Learning (CML), which uses MoE to adaptively activate relevant attributes from each representation and distills these mixed features into a unified 3D network; iii) Semantic Mixture Supervision (SMS), which combines semantic logits from multiple representations to boost downstream segmentation performance. Extensive experiments across eleven large-scale LiDAR datasets demonstrate our effectiveness and superiority. The code has been made publicly accessible",
    "checked": true,
    "id": "638426ba8994c2d9a5508e7ed7b80f37f5a2a2a5",
    "semantic_title": "limoe: mixture of lidar representation learners from automotive scenes",
    "citation_count": 8,
    "authors": [
      "Xiang Xu",
      "Lingdong Kong",
      "Hui Shuai",
      "Liang Pan",
      "Ziwei Liu",
      "Qingshan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_CoT-VLA_Visual_Chain-of-Thought_Reasoning_for_Vision-Language-Action_Models_CVPR_2025_paper.html": {
    "title": "CoT-VLA: Visual Chain-of-Thought Reasoning for Vision-Language-Action Models",
    "volume": "main",
    "abstract": "Vision-language-action models (VLAs) have shown potential in leveraging pretrained vision-language models and diverse robot demonstrations for learning generalizable sensorimotor control. While this paradigm effectively utilizes large-scale data from both robotic and non-robotic sources, current VLAs primarily focus on direct input--output mappings, lacking the intermediate reasoning steps crucial for complex manipulation tasks. As a result, existing VLAs lack temporal planning or reasoning capabilities. In this paper, we introduce a method that incorporates explicit visual chain-of-thought (CoT) reasoning into vision-language-action models (VLAs) by predicting future image frames autoregressively as visual goals before generating a short action sequence to achieve these goals. We introduce CoT-VLA, a state-of-the-art 7B VLA that can understand and generate visual and action tokens. Our experimental results demonstrate that CoT-VLA achieves strong performance, outperforming the state-of-the-art VLA model by 17% in real-world manipulation tasks and 6% in simulation benchmarks. Videos are available at: https://cot-vla.github.io/",
    "checked": true,
    "id": "2d7f3a99e916fc80ff890d109699f9682253e66d",
    "semantic_title": "cot-vla: visual chain-of-thought reasoning for vision-language-action models",
    "citation_count": 84,
    "authors": [
      "Qingqing Zhao",
      "Yao Lu",
      "Moo Jin Kim",
      "Zipeng Fu",
      "Zhuoyang Zhang",
      "Yecheng Wu",
      "Zhaoshuo Li",
      "Qianli Ma",
      "Song Han",
      "Chelsea Finn",
      "Ankur Handa",
      "Tsung-Yi Lin",
      "Gordon Wetzstein",
      "Ming-Yu Liu",
      "Donglai Xiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_WAVE_Weight_Templates_for_Adaptive_Initialization_of_Variable-sized_Models_CVPR_2025_paper.html": {
    "title": "WAVE: Weight Templates for Adaptive Initialization of Variable-sized Models",
    "volume": "main",
    "abstract": "The growing complexity of model parameters underscores the significance of pre-trained models. However, deployment constraints often necessitate models of varying sizes, exposing limitations in the conventional pre-training and fine-tuning paradigm, particularly when target model sizes are incompatible with pre-trained ones. To address this challenge, we propose WAVE, a novel approach that reformulates variable-sized model initialization from a multi-task perspective, where initializing each model size is treated as a distinct task. WAVE employs shared, size-agnostic weight templates alongside size-specific weight scalers to achieve consistent initialization across various model sizes. These weight templates, constructed within the Learngene framework, integrate knowledge from pre-trained models through a distillation process constrained by Kronecker-based rules. Target models are then initialized by concatenating and weighting these templates, with adaptive connection rules established by lightweight weight scalers, whose parameters are learned from minimal training data. Extensive experiments demonstrate the efficiency of WAVE, achieving state-of-the-art performance in initializing models of various depth and width. The knowledge encapsulated in weight templates is also task-agnostic, allowing for seamless transfer across diverse downstream datasets. Code will be made available at https://github.com/fu-feng/WAVE",
    "checked": true,
    "id": "28e31dafa6c16a13afc53962eabedafd91ab3c73",
    "semantic_title": "wave: weight templates for adaptive initialization of variable-sized models",
    "citation_count": 6,
    "authors": [
      "Fu Feng",
      "Yucheng Xie",
      "Jing Wang",
      "Xin Geng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cui_Forensics_Adapter_Adapting_CLIP_for_Generalizable_Face_Forgery_Detection_CVPR_2025_paper.html": {
    "title": "Forensics Adapter: Adapting CLIP for Generalizable Face Forgery Detection",
    "volume": "main",
    "abstract": "We describe the Forensics Adapter, an adapter network designed to transform CLIP into an effective and generalizable face forgery detector. Although CLIP is highly versatile, adapting it for face forgery detection is non-trivial as forgery-related knowledge is entangled with a wide range of unrelated knowledge. Existing methods treat CLIP merely as a feature extractor, lacking task-specific adaptation, which limits their effectiveness. To address this, we introduce an adapter to learn face forgery traces -- the blending boundaries unique to forged faces, guided by task-specific objectives. Then we enhance the CLIP visual tokens with a dedicated interaction strategy that communicates knowledge across CLIP and the adapter. Since the adapter is alongside CLIP, its versatility is highly retained, naturally ensuring strong generalizability in face forgery detection. With only 5.7M trainable parameters, our method achieves a significant performance boost, improving by approximately 7% on average across five standard datasets. We believe the proposed method can serve as a baseline for future CLIP-based face forgery detection methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinjie Cui",
      "Yuezun Li",
      "Ao Luo",
      "Jiaran Zhou",
      "Junyu Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_KAC_Kolmogorov-Arnold_Classifier_for_Continual_Learning_CVPR_2025_paper.html": {
    "title": "KAC: Kolmogorov-Arnold Classifier for Continual Learning",
    "volume": "main",
    "abstract": "Continual learning requires models to train continuously across consecutive tasks without forgetting. Most existing methods utilize linear classifiers, which struggle to maintain a stable classification space while learning new tasks. Inspired by the success of Kolmogorov-Arnold Networks (KAN) in preserving learning stability during simple continual regression tasks, we set out to explore their potential in more complex continual learning scenarios. In this paper, we introduce the Kolmogorov-Arnold Classifier (KAC), a novel classifier developed for continual learning based on the KAN structure. We delve into the impact of KAN's spline functions and introduce Radial Basis Functions (RBF) for improved compatibility with continual learning. We replace linear classifiers with KAC in several recent approaches and conduct experiments across various continual learning benchmarks, all of which demonstrate performance improvements, highlighting the effectiveness and robustness of KAC in continual learning",
    "checked": true,
    "id": "1ffca1b04477b2d75afe7c521ba482c7434656e1",
    "semantic_title": "kac: kolmogorov-arnold classifier for continual learning",
    "citation_count": 4,
    "authors": [
      "Yusong Hu",
      "Zichen Liang",
      "Fei Yang",
      "Qibin Hou",
      "Xialei Liu",
      "Ming-Ming Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_PI-HMR_Towards_Robust_In-bed_Temporal_Human_Shape_Reconstruction_with_Contact_CVPR_2025_paper.html": {
    "title": "PI-HMR: Towards Robust In-bed Temporal Human Shape Reconstruction with Contact Pressure Sensing",
    "volume": "main",
    "abstract": "Long-term in-bed monitoring benefits automatic and real-time health management within healthcare, and the advancement of human shape reconstruction technologies further enhances the representation and visualization of users' activity patterns. However, existing technologies are primarily based on visual cues, facing serious challenges in non-light-of-sight and privacy-sensitive in-bed scenes. Pressure-sensing bedsheets offer a promising solution for real-time motion reconstruction. Yet, limited exploration in model designs and data have hindered its further development. To tackle these issues, we propose a general framework that bridges gaps in data annotation and model design. Firstly, we introduce SMPLify-IB, an optimization method that overcomes the depth ambiguity issue in top-view scenarios through gravity constraints, enabling generating high-quality 3D human shape annotations for in-bed datasets. Then we present PI-HMR, a temporal-based human shape estimator to regress meshes from pressure sequences. By integrating multi-scale feature fusion with high-pressure distribution and spatial position priors, PI-HMR outperforms SOTA methods with 17.01mm Mean-Per-Joint-Error decrease. This work provides a whole tool-chain to support the development of in-bed monitoring with pressure contact sensing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyu Wu",
      "Yufan Xiong",
      "Mengting Niu",
      "Fangting Xie",
      "Quan Wan",
      "Qijun Ying",
      "Boyan Liu",
      "Xiaohui Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_BOOTPLACE_Bootstrapped_Object_Placement_with_Detection_Transformers_CVPR_2025_paper.html": {
    "title": "BOOTPLACE: Bootstrapped Object Placement with Detection Transformers",
    "volume": "main",
    "abstract": "In this paper, we tackle the copy-paste image-to-image composition problem with a focus on object placement learning. Prior methods have leveraged generative models to reduce the reliance for dense supervision. However, this often limits their capacity to model complex data distributions. Alternatively, transformer networks with a sparse contrastive loss have been explored, but their over-relaxed regularization often leads to imprecise object placement. We introduce BOOTPLACE, a novel paradigm that formulates object placement as a placement-by-detection problem. Our approach begins by identifying suitable regions of interest for object placement. This is achieved by training a specialized detection transformer on object-subtracted backgrounds, enhanced with multi-object supervisions. It then semantically associates each target compositing object with detected regions based on their complementary characteristics. Through a boostrapped training approach applied to randomly object-subtracted images, our model enforces meaningful placements through extensive paired data augmentation. Experimental results on established benchmarks demonstrate BOOTPLACE's superior performance in object repositioning, markedly surpassing state-of-the-art baselines on Cityscapes and OPA datasets with notable improvements in IOU scores. Additional ablation studies further showcase the compositionality and generalizability of our approach, supported by user study evaluations. Code is available at https://github.com/RyanHangZhou/BOOTPLACE",
    "checked": true,
    "id": "0576923d97a7459ba11fa8d89b71f885e86ec509",
    "semantic_title": "bootplace: bootstrapped object placement with detection transformers",
    "citation_count": 0,
    "authors": [
      "Hang Zhou",
      "Xinxin Zuo",
      "Rui Ma",
      "Li Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Long_CheckManual_A_New_Challenge_and_Benchmark_for_Manual-based_Appliance_Manipulation_CVPR_2025_paper.html": {
    "title": "CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation",
    "volume": "main",
    "abstract": "Correct use of electrical appliances has significantly improved human life quality. Unlike simple tools that can be manipulated with common sense, different parts of electrical appliances have specific functions defined by manufacturers. If we want the robot to heat bread by microwave, we should enable them to review the microwave's manual first. From the manual, it can learn about component functions, interaction methods, and representative task steps about appliances. However, previous manual-related works remain limited to question-answering tasks while existing manipulation researchers ignore the manual's important role and fail to comprehend multi-page manuals. In this paper, we propose the first manual-based appliance manipulation benchmark CheckManual. Specifically, we design a large model-assisted human-revised data generation pipeline to create manuals based on CAD appliance models. With these manuals, we establish novel manual-based manipulation challenges, metrics, and simulator environments for model performance evaluation. Furthermore, we propose the first manual-based manipulation planning model ManualPlan to set up a group of baselines for the CheckManual benchmark",
    "checked": true,
    "id": "8577257105588a84298bc719d68f0a8e82e8e4c5",
    "semantic_title": "checkmanual: a new challenge and benchmark for manual-based appliance manipulation",
    "citation_count": 0,
    "authors": [
      "Yuxing Long",
      "Jiyao Zhang",
      "Mingjie Pan",
      "Tianshu Wu",
      "Taewhan Kim",
      "Hao Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_CXPMRG-Bench_Pre-training_and_Benchmarking_for_X-ray_Medical_Report_Generation_on_CVPR_2025_paper.html": {
    "title": "CXPMRG-Bench: Pre-training and Benchmarking for X-ray Medical Report Generation on CheXpert Plus Dataset",
    "volume": "main",
    "abstract": "X-ray image-based medical report generation (MRG) is a pivotal area in artificial intelligence that can significantly reduce diagnostic burdens and patient wait times. Despite significant progress, we believe that the task has reached a bottleneck due to the limited benchmark datasets and the existing large models' insufficient capability enhancements in this specialized domain. Specifically, the recently released CheXpert Plus dataset lacks comparative evaluation algorithms and their results, providing only the dataset itself. This situation makes the training, evaluation, and comparison of subsequent algorithms challenging. Thus, we conduct a comprehensive benchmarking of existing mainstream X-ray report generation models and large language models (LLMs), on the CheXpert Plus dataset. We believe that the proposed benchmark can provide a solid comparative basis for subsequent algorithms and serve as a guide for researchers to quickly grasp the state-of-the-art models in this field. More importantly, we propose a large model for the X-ray image report generation using a multi-stage pre-training strategy, including self-supervised autoregressive generation and Xray-report contrastive learning, and supervised fine-tuning. Extensive experimental results indicate that the autoregressive pre-training based on Mamba effectively encodes X-ray images, and the image-text contrastive pre-training further aligns the feature spaces, achieving better experimental results",
    "checked": true,
    "id": "d207c3902daf3df63281c9522c5ecc09fed16aae",
    "semantic_title": "cxpmrg-bench: pre-training and benchmarking for x-ray medical report generation on chexpert plus dataset",
    "citation_count": 11,
    "authors": [
      "Xiao Wang",
      "Fuling Wang",
      "Yuehang Li",
      "Qingchuan Ma",
      "Shiao Wang",
      "Bo Jiang",
      "Jin Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dang_FASTer_Focal_token_Acquiring-and-Scaling_Transformer_for_Long-term_3D_Objection_Detection_CVPR_2025_paper.html": {
    "title": "FASTer: Focal token Acquiring-and-Scaling Transformer for Long-term 3D Objection Detection",
    "volume": "main",
    "abstract": "Recent top-performing temporal 3D detectors based on Lidars have increasingly adopted region-based paradigms. They first generate coarse proposals, followed by encoding and fusing regional features. However, indiscriminate sampling and fusion often overlook the varying contributions of individual points and lead to exponentially increased complexity as the number of input frames grows. Moreover, arbitrary result-level concatenation limits the global information extraction. In this paper, we propose a Focal Token Acquring-and-Scaling Transformer (FASTer), which dynamically selects focal tokens and condenses token sequences in an adaptive and lightweight manner. Emphasizing the contribution of individual tokens, we propose a simple but effective Adaptive Scaling mechanism to capture geometric contexts while sifting out focal points. Adaptively storing and processing only focal points in historical frames dramatically reduces the overall complexity. Furthermore, a novel Grouped Hierarchical Fusion strategy is proposed, progressively performing sequence scaling and Intra-Group Fusion operations to facilitate the exchange of global spatial and temporal information. Experiments on the Waymo Open Dataset demonstrate that our FASTer significantly outperforms other state-of-the-art detectors in both performance and efficiency while also exhibiting improved flexibility and robustness. The code is available at https://github.com/MSunDYY/FASTer.git",
    "checked": false,
    "id": "c8198ce2d6fb1a442bc9eaaa1f25bafcc06a6d3b",
    "semantic_title": "faster: focal token acquiring-and-scaling transformer for long-term 3d object detection",
    "citation_count": 0,
    "authors": [
      "Chenxu Dang",
      "ZaiPeng Duan",
      "Pei An",
      "Xinmin Zhang",
      "Xuzhong Hu",
      "Jie Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_SEEN-DA_SEmantic_ENtropy_guided_Domain-aware_Attention_for_Domain_Adaptive_Object_CVPR_2025_paper.html": {
    "title": "SEEN-DA: SEmantic ENtropy guided Domain-aware Attention for Domain Adaptive Object Detection",
    "volume": "main",
    "abstract": "Domain adaptive object detection (DAOD) aims to generalize detectors trained on an annotated source domain to an unlabelled target domain. Traditional works focus on aligning visual features between domains to extract domain-invariant knowledge, and recent VLM-based DAOD methods leverage semantic information provided by the textual encoder to supplement domain-specific features for each domain.However, they overlook the role of semantic information in guiding the learning of visual features that are beneficial for adaptation.To solve the problem, we propose semantic entropy to quantify the semantic information contained in visual features, and design SEmantic ENtropy guided Domain-aware Attention (SEEN-DA) to adaptively refine visual features with the semantic information of two domains.Semantic entropy reflects the importance of features based on semantic information, which can serve as attention to select discriminative visual features and suppress semantically irrelevant redundant information.Guided by semantic entropy, we introduce domain-aware attention modules into the visual encoder in SEEN-DA.It utilizes an inter-domain attention branch to extract domain-invariant features and eliminate redundant information, and an intra-domain attention branch to supplement the domain-specific semantic information discriminative on each domain.Comprehensive experiments validate the effectiveness of SEEN-DA, demonstrating significant improvements in cross-domain object detection performance",
    "checked": true,
    "id": "259e26384b60a972519aad1c82900b74914506e6",
    "semantic_title": "seen-da: semantic entropy guided domain-aware attention for domain adaptive object detection",
    "citation_count": 0,
    "authors": [
      "Haochen Li",
      "Rui Zhang",
      "Hantao Yao",
      "Xin Zhang",
      "Yifan Hao",
      "Xinkai Song",
      "Shaohui Peng",
      "Yongwei Zhao",
      "Chen Zhao",
      "Yanjun Wu",
      "Ling Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Event-Equalized_Dense_Video_Captioning_CVPR_2025_paper.html": {
    "title": "Event-Equalized Dense Video Captioning",
    "volume": "main",
    "abstract": "Dense video captioning aims to localize and caption all events in arbitrary untrimmed videos. Although previous methods have achieved appealing results, they still face the issue of temporal bias, i.e, models tend to focus more on events with certain temporal characteristics. Specifically, 1) the temporal distribution of events in training datasets is uneven. Models trained on these datasets will pay less attention to out-of-distribution events. 2) long-duration events have more frame features than short ones and will attract more attention. To address this, we argue that events, with varying temporal characteristics, should be treated equally when it comes to dense video captioning. Intuitively, different events tend to have distinct visual differences due to varied camera views, backgrounds, or subjects. Inspired by that, we intend to utilize visual features to have an approximate perception of possible events and pay equal attention to them. In this paper, we introduce a simple but effective framework, called Event-Equalized Dense Video Captioning(E^2DVC) to overcome the temporal bias and treat all possible events equally. Specifically, an event perception module(EPM) is proposed to do uneven clustering on visual frame features to generate pseudo-events. We enforce the model's attention to these pseudo-events through the pseudo-event initialization module(PEI). A novel event-enhanced encoder(EEE) is also devised to enhance the model's ability to explore frame-frame and frame-event relationships. Experimental results validate the effectiveness of the proposed methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangyi Wu",
      "Pengna Li",
      "Jingwen Fu",
      "Yizhe Li",
      "Yang Wu",
      "Yuhan Liu",
      "Jinjun Wang",
      "Sanping Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ha_Geometry-guided_Online_3D_Video_Synthesis_with_Multi-View_Temporal_Consistency_CVPR_2025_paper.html": {
    "title": "Geometry-guided Online 3D Video Synthesis with Multi-View Temporal Consistency",
    "volume": "main",
    "abstract": "We introduce a novel geometry-guided online video view synthesis method with enhanced view and temporal consistency. Traditional approaches achieve high-quality synthesis from dense multi-view camera setups but require significant computational resources. In contrast, selective-input methods reduce this cost but often compromise quality, leading to multi-view and temporal inconsistencies such as flickering artifacts. Our method addresses this challenge to deliver efficient, high-quality novel-view synthesis with view and temporal consistency. The key innovation of our approach lies in using global geometry to guide an image-based rendering pipeline. To accomplish this, we progressively refine depth maps using color difference masks across time. These depth maps are then accumulated through truncated signed distance fields in the synthesized view's image space. This depth representation is view and temporally consistent, and is used to guide a pre-trained blending network that fuses multiple forward-rendered input-view images. Thus, the network is encouraged to output geometrically consistent synthesis results across multiple views and time. Our approach achieves consistent, high-quality video synthesis, while running efficiently in an online manner",
    "checked": true,
    "id": "d3142acf91d9994ac2f143ff0340a972ab17c630",
    "semantic_title": "geometry-guided online 3d video synthesis with multi-view temporal consistency",
    "citation_count": 0,
    "authors": [
      "Hyunho Ha",
      "Lei Xiao",
      "Christian Richardt",
      "Thu Nguyen-Phuoc",
      "Changil Kim",
      "Min H. Kim",
      "Douglas Lanman",
      "Numair Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_EDCFlow_Exploring_Temporally_Dense_Difference_Maps_for_Event-based_Optical_Flow_CVPR_2025_paper.html": {
    "title": "EDCFlow: Exploring Temporally Dense Difference Maps for Event-based Optical Flow Estimation",
    "volume": "main",
    "abstract": "Recent learning-based methods for event-based optical flow estimation utilize cost volumes for pixel matching but suffer from redundant computations and limited scalability to higher resolutions for flow refinement. In this work, we take advantage of the complementarity between temporally dense feature differences of adjacent event frames and cost volume and present a lightweight event-based optical flow network (EDCFlow) to achieve high-quality flow estimation at a higher resolution. Specifically, an attention-based multi-scale temporal feature difference layer is developed to capture diverse motion patterns at high resolution in a computation-efficient manner. An adaptive fusion of high-resolution difference motion features and low-resolution correlation motion features is performed to enhance motion representation and model generalization. Notably, EDCFlow can serve as a plug-and-play refinement module for RAFT-like event-based methods to enhance flow details. Extensive experiments demonstrate that EDCFlow achieves better performance with lower complexity compared to existing methods, offering superior generalization. Codes and models will be available at here",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daikun Liu",
      "Lei Cheng",
      "Teng Wang",
      "Changyin Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Point2RBox-v2_Rethinking_Point-supervised_Oriented_Object_Detection_with_Spatial_Layout_Among_CVPR_2025_paper.html": {
    "title": "Point2RBox-v2: Rethinking Point-supervised Oriented Object Detection with Spatial Layout Among Instances",
    "volume": "main",
    "abstract": "With the rapidly increasing demand for oriented object detection (OOD), recent research involving weakly-supervised detectors for learning OOD from point annotations has gained great attention. In this paper, we rethink this challenging task setting with the layout among instances and present Point2RBox-v2. At the core are three principles: 1) Gaussian overlap loss. It learns an upper bound for each instance by treating objects as 2D Gaussian distributions and minimizing their overlap. 2) Voronoi watershed loss. It learns a lower bound for each instance through watershed on Voronoi tessellation. 3) Consistency loss. It learns the size/rotation variation between two output sets with respect to an input image and its augmented view. Supplemented by a few devised techniques, e.g. edge loss and copy-paste, the detector is further enhanced. To our best knowledge, Point2RBox-v2 is the first approach to explore the spatial layout among instances for learning point-supervised OOD. Our solution is elegant and lightweight, yet it is expected to give a competitive performance especially in densely packed scenes: 62.61%/86.15%/34.71% on DOTA/HRSC/FAIR1M",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Yu",
      "Botao Ren",
      "Peiyuan Zhang",
      "Mingxin Liu",
      "Junwei Luo",
      "Shaofeng Zhang",
      "Feipeng Da",
      "Junchi Yan",
      "Xue Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mehri_LibraGrad_Balancing_Gradient_Flow_for_Universally_Better_Vision_Transformer_Attributions_CVPR_2025_paper.html": {
    "title": "LibraGrad: Balancing Gradient Flow for Universally Better Vision Transformer Attributions",
    "volume": "main",
    "abstract": "Why do gradient-based explanations struggle with Transformers, and how can we improve them? We identify gradient flow imbalances in Transformers that violate FullGrad-completeness, a critical property for attribution faithfulness that CNNs naturally possess. To address this issue, we introduce LibraGrad--a theoretically grounded post-hoc approach that corrects gradient imbalances through pruning and scaling of backward paths, without changing the forward pass or adding computational overhead. We evaluate LibraGrad using three metric families: Faithfulness, which quantifies prediction changes under perturbations of the most and least relevant features; Completeness Error, which measures attribution conservation relative to model outputs; and Segmentation AP, which assesses alignment with human perception. Extensive experiments across 8 architectures, 4 model sizes, and 5 datasets show that LibraGrad universally enhances gradient-based methods, outperforming existing white-box methods--including Transformer-specific approaches--across all metrics. We demonstrate superior qualitative results through two complementary evaluations: precise text-prompted region highlighting on CLIP models and accurate class discrimination between co-occurring animals on ImageNet-finetuned models--two settings on which existing methods often struggle. LibraGrad is effective even on the attention-free MLP-Mixer architecture, indicating potential for extension to other modern architectures. Our code is freely available at https://nightmachinery.github.io/LibraGrad/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Faridoun Mehri",
      "Mahdieh Soleymani Baghshah",
      "Mohammad Taher Pilehvar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Blind_Bitstream-corrupted_Video_Recovery_via_Metadata-guided_Diffusion_Model_CVPR_2025_paper.html": {
    "title": "Blind Bitstream-corrupted Video Recovery via Metadata-guided Diffusion Model",
    "volume": "main",
    "abstract": "Bitstream-corrupted video recovery aims to fill in realistic video content due to bitstream corruption during video storage or transmission. Most existing methods typically assume that the predefined masks of the corrupted regions are known in advance. However, manually annotating these masks is laborious and time-consuming, limiting the applicability of existing methods in real-world scenarios. Therefore, we expect to relax this assumption by defining a new blind video recovery setting where the recovery of corrupted regions does not rely on predefined masks. There are two significant challenges in this setting: (i) without predefined masks, how accurately can a model identify the regions requiring recovery? (ii) how to recover contents from extensive and irregular regions, especially when large portions of frames are severely degraded? To address these challenges, we introduce a Metadata-Guided Diffusion Model, dubbed M-GDM. To enable a diffusion model focusing on the corrupted regions, we leverage intrinsic video metadata as a corruption indicator and design a dual-stream metadata encoder. This encoder first embeds the motion vectors and frame types of a video separately and then merges them into a unified metadata representation. The metadata representation will interact with the corrupted latent feature through cross-attention mechanisms at each diffusion step. Meanwhile, to preserve the intact regions, we propose a prior-driven mask predictor that generates pseudo masks for the corrupted regions by leveraging the metadata prior and diffusion prior. These pseudo masks enable the separation and recombination of intact and recovered regions through hard masking. However, imperfections in pseudo mask predictions and hard masking processes often result in boundary artifacts. Thus, we introduce a post-refinement module that refines the hard-masked outputs, enhancing the consistency between intact and recovered regions. Extensive experiment results validate the effectiveness of our method and demonstrate its superiority in the blind video recovery task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyun Wang",
      "Hu Zhang",
      "Xin Shen",
      "Dadong Wang",
      "Xin Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Mind_the_Trojan_Horse_Image_Prompt_Adapter_Enabling_Scalable_and_CVPR_2025_paper.html": {
    "title": "Mind the Trojan Horse: Image Prompt Adapter Enabling Scalable and Deceptive Jailbreaking",
    "volume": "main",
    "abstract": "Recently, the Image Prompt Adapter (IP-Adapter) has been increasingly integrated into text-to-image diffusion models (T2I-DMs) to improve controllability. However, in this paper, we reveal that T2I-DMs equipped with the IP-Adapter (T2I-IP-DMs) enable a new jailbreak attack named the hijacking attack. We demonstrate that, by uploading imperceptible image-space adversarial examples (AEs), the adversary can hijack massive benign users to jailbreak an Image Generation Service (IGS) driven by T2I-IP-DMs and mislead the public to discredit the service provider. Worse still, the IP-Adapter's dependency on open-source image encoders reduces the knowledge required to craft AEs. Extensive experiments verify the technical feasibility of the hijacking attack. In light of the revealed threat, we investigate several existing defenses and explore combining the IP-Adapter with adversarially trained models to overcome existing defenses' limitations. Our code is available at https://github.com/fhdnskfbeuv/attackIPA",
    "checked": true,
    "id": "39d9959da0c28e99f83718cc40dfa11716132912",
    "semantic_title": "mind the trojan horse: image prompt adapter enabling scalable and deceptive jailbreaking",
    "citation_count": 1,
    "authors": [
      "Junxi Chen",
      "Junhao Dong",
      "Xiaohua Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jang_Lost_in_Translation_Found_in_Context_Sign_Language_Translation_with_CVPR_2025_paper.html": {
    "title": "Lost in Translation, Found in Context: Sign Language Translation with Contextual Cues",
    "volume": "main",
    "abstract": "Our objective is to translate continuous sign language into spoken language text. Inspired by the way human interpreters rely on context for accurate translation, we incorporate additional contextual cues together with the signing video, into a new translation framework. Specifically, besides visual sign recognition features that encode the input video, we integrate complementary textual information from (i) captions describing the background show, (ii) translation of previous sentences, as well as (iii) pseudo-glosses transcribing the signing. These are automatically extracted and inputted along with the visual features to a pre-trained large language model (LLM), which we fine-tune to generate spoken language translations in text form. Through extensive ablation studies, we show the positive contribution of each input cue to the translation performance. We train and evaluate our approach on BOBSL -- the largest British Sign Language dataset currently available. We show that our contextual approach significantly enhances the quality of the translations compared to previously reported results on BOBSL, and also to state-of-the-art methods that we implement as baselines. Furthermore, we demonstrate the generality of our approach by applying it also to How2Sign, an American Sign Language dataset, and achieve competitive results",
    "checked": true,
    "id": "579ddc45b1d3cc849323da23f358b832788e71a5",
    "semantic_title": "lost in translation, found in context: sign language translation with contextual cues",
    "citation_count": 5,
    "authors": [
      "Youngjoon Jang",
      "Haran Raajesh",
      "Liliane Momeni",
      "G√ºl Varol",
      "Andrew Zisserman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_CoCoGaussian_Leveraging_Circle_of_Confusion_for_Gaussian_Splatting_from_Defocused_CVPR_2025_paper.html": {
    "title": "CoCoGaussian: Leveraging Circle of Confusion for Gaussian Splatting from Defocused Images",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) has attracted significant attention for its high-quality novel view rendering, inspiring research to address real-world challenges. While conventional methods depend on sharp images for accurate scene reconstruction, real-world scenarios are often affected by defocus blur due to finite depth of field, making it essential to account for realistic 3D scene representation. In this study, we propose CoCoGaussian, a Circle of Confusion-aware Gaussian Splatting that enables precise 3D scene representation using only defocused images. CoCoGaussian addresses the challenge of defocus blur by modeling the Circle of Confusion (CoC) through a physically grounded approach based on the principles of photographic defocus. Exploiting 3D Gaussians, we compute the CoC diameter from depth and learnable aperture information, generating multiple Gaussians to precisely capture the CoC shape. Furthermore, we introduce a learnable scaling factor to enhance robustness and provide more flexibility in handling unreliable depth in scenes with reflective or refractive surfaces. Experiments on both synthetic and real-world datasets demonstrate that CoCoGaussian achieves state-of-the-art performance across multiple benchmarks",
    "checked": true,
    "id": "b0c3b5977eedf90d1f66a6b661de83b3a5ccd814",
    "semantic_title": "cocogaussian: leveraging circle of confusion for gaussian splatting from defocused images",
    "citation_count": 1,
    "authors": [
      "Jungho Lee",
      "Suhwan Cho",
      "Taeoh Kim",
      "Ho-Deok Jang",
      "Minhyeok Lee",
      "Geonho Cha",
      "Dongyoon Wee",
      "Dogyoon Lee",
      "Sangyoun Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pan_Semantic_and_Sequential_Alignment_for_Referring_Video_Object_Segmentation_CVPR_2025_paper.html": {
    "title": "Semantic and Sequential Alignment for Referring Video Object Segmentation",
    "volume": "main",
    "abstract": "Referring video object segmentation (RVOS) seeks to segment the objects within a video referred by linguistic expressions. Existing RVOS solutions follow a \"fuse then select\" paradigm: establishing semantic correlation between visual and linguistic feature, and performing frame-level query interaction to select the instance mask per frame with instance segmentation module. This paradigm overlooks the challenge of semantic gap between the linguistic descriptor and the video object as well as the underlying clutters in the video. This paper proposes a novel Semantic and Sequential Alignment (SSA) paradigm to handle these challenges. We first insert a lightweight adapter after the vision language model (VLM) to perform the semantic alignment. Then, prior to selecting mask per frame, we exploit the trajectory-to-instance enhancement for each frame via sequential alignment. This paradigm leverages the visual-language alignment inherent in VLM during adaptation and tries to capture global information by ensembling trajectories. This helps understand videos and the corresponding descriptors by mitigating the discrepancy with intricate activity semantics, particularly when facing occlusion or similar interference. SSA demonstrates competitive performance while maintaining fewer learnable parameters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feiyu Pan",
      "Hao Fang",
      "Fangkai Li",
      "Yanyu Xu",
      "Yawei Li",
      "Luca Benini",
      "Xiankai Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Synchronized_Video-to-Audio_Generation_via_Mel_Quantization-Continuum_Decomposition_CVPR_2025_paper.html": {
    "title": "Synchronized Video-to-Audio Generation via Mel Quantization-Continuum Decomposition",
    "volume": "main",
    "abstract": "Video-to-audio generation is essential for synthesizing realistic audio tracks that synchronize effectively with silent videos.Following the perspective of extracting essential signals from videos that can precisely control the mature text-to-audio generative diffusion models, this paper presents how to balance the representation of mel-spectrograms in terms of completeness and complexity through a new approach called Mel Quantization-Continuum Decomposition (Mel-QCD).We decompose the mel-spectrogram into three distinct types of signals, employing quantization or continuity to them, we can effectively predict them from video by a devised video-to-all (V2X) predictor.Then, the predicted signals are recomposed and fed into a ControlNet, along with a textual inversion design, to control the audio generation process.Our proposed Mel-QCD method demonstrates state-of-the-art performance across eight metrics, evaluating dimensions such as quality, synchronization, and semantic consistency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juncheng Wang",
      "Chao Xu",
      "Cheng Yu",
      "Lei Shang",
      "Zhe Hu",
      "Shujun Wang",
      "Liefeng Bo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Continual_SFT_Matches_Multimodal_RLHF_with_Negative_Supervision_CVPR_2025_paper.html": {
    "title": "Continual SFT Matches Multimodal RLHF with Negative Supervision",
    "volume": "main",
    "abstract": "Multimodal RLHF usually happens after supervised finetuning (SFT) stage to continually improve vision-language models' (VLMs) comprehension. Conventional wisdom holds its superiority over continual SFT during this preference alignment stage. In this paper, we observe that the inherent value of multimodal RLHF lies in its negative supervision, the logit of the rejected responses. We thus propose a novel negative supervised finetuning (nSFT) approach that fully excavates these information resided. Our nSFT disentangles this negative supervision in RLHF paradigm, and continually aligns VLMs with a simple SFT loss. This is more memory efficient than multimodal RLHF where 2 (e.g., DPO) or 4 (e.g., PPO) large VLMs are strictly required. The effectiveness of nSFT is rigorously proved by comparing it with various multimodal RLHF approaches, across different dataset sources, base VLMs and evaluation metrics. Besides, fruitful of ablations are provided to support our hypothesis. Code will be found in https://github.com/Kevinz-code/nSFT/",
    "checked": true,
    "id": "359eb1ab4ad2368203695a3e86daf0cc80f43d62",
    "semantic_title": "continual sft matches multimodal rlhf with negative supervision",
    "citation_count": 2,
    "authors": [
      "Ke Zhu",
      "Yu Wang",
      "Yanpeng Sun",
      "Qiang Chen",
      "Jiangjiang Liu",
      "Gang Zhang",
      "Jingdong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Semantic-guided_Cross-Modal_Prompt_Learning_for_Skeleton-based_Zero-shot_Action_Recognition_CVPR_2025_paper.html": {
    "title": "Semantic-guided Cross-Modal Prompt Learning for Skeleton-based Zero-shot Action Recognition",
    "volume": "main",
    "abstract": "Skeleton-based human action recognition is promising due to its privacy preservation, robustness to visual challenges, and computational efficiency. Especially, the practical necessity to recognize unseen actions has led to increased interest in zero-shot skeleton-based action recognition (ZSSAR). Existing ZSSAR approaches often rely on manually crafted action descriptions or visual assumptions to enhance knowledge transfer, which is limited in flexibility and prone to inaccuracies and noise. To overcome this, we introduce Semantic-guided Cross-Modal Prompt Learning (SCoPLe), a novel framework that replaces manual guidance with data-driven prompt learning for refinement and alignment of skeletal and textual features. Specifically, we introduce a dual-stream language prompting module that preserves the original semantic context from the pre-trained text encoder while still effectively tuning its ouput for ZSSAR task adaptation. We also introduce a joint-shaped prompting module that learns tuning for skeleton features and incorporate an adaptive visual representation sampler that leverages text semantics to strengthen the cross-modal prompting interactions during skeleton-to-text embedding projection. Experimental results on the NTU-RGB+D and PKU-MMD datasets demonstrate the state-of-the-art performance of our method in both ZSSAR and generalized ZSSAR scenarios",
    "checked": true,
    "id": "cf3671aa28841f77b7506335df280d5a34f9b4ac",
    "semantic_title": "semantic-guided cross-modal prompt learning for skeleton-based zero-shot action recognition",
    "citation_count": 0,
    "authors": [
      "Anqi Zhu",
      "Jingmin Zhu",
      "James Bailey",
      "Mingming Gong",
      "Qiuhong Ke"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_FATE_Full-head_Gaussian_Avatar_with_Textural_Editing_from_Monocular_Video_CVPR_2025_paper.html": {
    "title": "FATE: Full-head Gaussian Avatar with Textural Editing from Monocular Video",
    "volume": "main",
    "abstract": "Reconstructing high-fidelity, animatable 3D head avatars from effortlessly captured monocular videos is a pivotal yet formidable challenge. Although significant progress has been made in rendering performance and manipulation capabilities, notable challenges remain, including incomplete reconstruction and inefficient Gaussian representation. To address these challenges, we introduce FATE -- a novel method for reconstructing an editable full-head avatar from a single monocular video. FATE integrates a sampling-based densification strategy to ensure optimal positional distribution of points, improving rendering efficiency. A neural baking technique is introduced to convert discrete Gaussian representations into continuous attribute maps, facilitating intuitive appearance editing. Furthermore, we propose a universal completion framework to recover non-frontal appearance, culminating in a 360^\\circ-renderable 3D head avatar. FATE outperforms previous approaches in both qualitative and quantitative evaluations, achieving state-of-the-art performance. To the best of our knowledge, FATE is the first animatable and 360^\\circ full-head monocular reconstruction method for a 3D head avatar. Project page and code are available at this \\href https://zjwfufu.github.io/FATE-page/ link",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Zhang",
      "Zijian Wu",
      "Zhiyang Liang",
      "Yicheng Gong",
      "Dongfang Hu",
      "Yao Yao",
      "Xun Cao",
      "Hao Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jia_ChatGen_Automatic_Text-to-Image_Generation_From_FreeStyle_Chatting_CVPR_2025_paper.html": {
    "title": "ChatGen: Automatic Text-to-Image Generation From FreeStyle Chatting",
    "volume": "main",
    "abstract": "Despite the significant advancements in text-to-image (T2I) generative models, users often face a trial-and-error challenge in practical scenarios. This challenge arises from the complexity and uncertainty of tedious steps such as crafting suitable prompts, selecting appropriate models, and configuring specific arguments, making users resort to labor-intensive attempts for desired images. This paper proposes Automatic T2I generation, which aims to automate these tedious steps, allowing users to simply describe their needs in a freestyle chatting way. To systematically study this problem, we first introduce ChatGenBench, a novel benchmark designed for Automatic T2I. It features high-quality paired data with diverse freestyle inputs, enabling comprehensive evaluation of automatic T2I models across all steps. Additionally, recognizing Automatic T2I as a complex multi-step reasoning task, we propose ChatGen-Evo, a multi-stage evolution strategy that progressively equips models with essential automation skills. Through extensive evaluation across step-wise accuracy and image quality, ChatGen-Evo significantly enhances performance over various baselines. Our evaluation also uncovers valuable insights for advancing automatic T2I. All our data, code and models will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengyou Jia",
      "Changliang Xia",
      "Zhuohang Dang",
      "Weijia Wu",
      "Hangwei Qian",
      "Minnan Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hassan_GEM_A_Generalizable_Ego-Vision_Multimodal_World_Model_for_Fine-Grained_Ego-Motion_CVPR_2025_paper.html": {
    "title": "GEM: A Generalizable Ego-Vision Multimodal World Model for Fine-Grained Ego-Motion, Object Dynamics, and Scene Composition Control",
    "volume": "main",
    "abstract": "We present GEM, a Generalizable Ego-vision Multimodal world model that predicts future frames using a reference frame, sparse features, human poses, and ego-trajectories. Hence, our model has precise control over object dynamics, ego-agent motion and human poses. GEM generates paired RGB and depth outputs for richer spatial understanding. We introduce autoregressive noise schedules to enable stable long-horizon generations. Our dataset is comprised of 4000+ hours of multimodal data across domains like autonomous driving, egocentric human activities, and drone flights. Pseudo-labels are used to get depth maps, ego-trajectories, and human poses. We use a comprehensive evaluation framework, including a new Control of Object Manipulation (COM) metric, to assess controllability. Experiments show GEM excels at generating diverse, controllable scenarios and temporal consistency over long generations. Code, models, and datasets are fully open-sourced",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mariam Hassan",
      "Sebastian Stapf",
      "Ahmad Rahimi",
      "Pedro M B Rezende",
      "Yasaman Haghighi",
      "David Br√ºggemann",
      "Isinsu Katircioglu",
      "Lin Zhang",
      "Xiaoran Chen",
      "Suman Saha",
      "Marco Cannici",
      "Elie Aljalbout",
      "Botao Ye",
      "Xi Wang",
      "Aram Davtyan",
      "Mathieu Salzmann",
      "Davide Scaramuzza",
      "Marc Pollefeys",
      "Paolo Favaro",
      "Alexandre Alahi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_VEU-Bench_Towards_Comprehensive_Understanding_of_Video_Editing_CVPR_2025_paper.html": {
    "title": "VEU-Bench: Towards Comprehensive Understanding of Video Editing",
    "volume": "main",
    "abstract": "Widely shared videos on the internet are often edited. Recently, although Video Large Language Models (Vid-LLMs) have made great progress in general video understanding tasks, their capabilities in video editing understanding (VEU) tasks remain unexplored. To address this gap, in this paper, we introduce VEU-Bench (Video Editing Understanding Benchmark), a comprehensive benchmark that categorizes video editing components across various dimensions, from intra-frame features like shot size to inter-shot attributes such as cut types and transitions. Unlike previous video editing understanding benchmarks that focus mainly on editing element classification, VEU-Bench encompasses 19 fine-grained tasks across three stages: recognition, reasoning, and judging. To enhance the annotation of VEU automatically, we built an annotation pipeline integrated with an ontology-based knowledge base. Through extensive experiments with 11 state-of-the-art Vid-LLMs, our findings reveal that current Vid-LLMs face significant challenges in VEU tasks, with some performing worse than random choice. To alleviate this issue, we develop Oscars(Named after the Academy Awards.), a VEU expert model fine-tuned on the curated VEU-Bench dataset. It outperforms existing open-source Vid-LLMs on VEU-Bench by over 28.3% in accuracy and achieves performance comparable to commercial models like GPT-4o. We also demonstrate that incorporating VEU data significantly enhances the performance of Vid-LLMs on general video understanding benchmarks, with an average improvement of 8.3% across nine reasoning tasks. The code and data will be made available",
    "checked": true,
    "id": "339bd281eae5d8abc02627045212780808604b0e",
    "semantic_title": "veu-bench: towards comprehensive understanding of video editing",
    "citation_count": 1,
    "authors": [
      "Bozheng Li",
      "Yongliang Wu",
      "Yi Lu",
      "Jiashuo Yu",
      "Licheng Tang",
      "Jiawang Cao",
      "Wenqing Zhu",
      "Yuyang Sun",
      "Jay Wu",
      "Wenbo Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Decouple_Distortion_from_Perception_Region_Adaptive_Diffusion_for_Extreme-low_Bitrate_CVPR_2025_paper.html": {
    "title": "Decouple Distortion from Perception: Region Adaptive Diffusion for Extreme-low Bitrate Perception Image Compression",
    "volume": "main",
    "abstract": "Leveraging the generative power of diffusion models, generative image compression has achieved impressive perceptual fidelity even at extremely low bitrates. However, current methods often neglect the non-uniform complexity of images, limiting their ability to balance global perceptual quality with local texture consistency and to allocate coding resources efficiently. To address this, we introduce the Map-guided Masking Realism Image Diffusion Codec(MRIDC), designed to optimize the trade-off between local distortion and global perceptual quality in extreme-low bitrate compression. MRIDC integrates a vector-quantized image encoder with a diffusion-based decoder. On the encoding side, we propose a Map-guided Latent Masking(MLM) module, which selectively masks elements in the latent space based on prior information, allowing adaptive resource allocation aligned with image complexity. On the decoding side, masked latents are completed using the Bidirectional Prediction Controllable Generation(BPCG) module, which guides the constrained generation process within the diffusion model to reconstruct the image. Experimental results show that MRIDC achieves state-of-the-art perceptual compression quality at extremely low bitrates, effectively preserving feature consistency in key regions and advancing the rate-distortion-perception performance curve, establishing new benchmarks in balancing compression efficiency with visual fidelity",
    "checked": true,
    "id": "682f981aaccaafac825a9871cd3a50243cb57d42",
    "semantic_title": "decouple distortion from perception: region adaptive diffusion for extreme-low bitrate perception image compression",
    "citation_count": 0,
    "authors": [
      "Jinchang Xu",
      "Shaokang Wang",
      "Jintao Chen",
      "Zhe Li",
      "Peidong Jia",
      "Fei Zhao",
      "Guoqing Xiang",
      "Zhijian Hao",
      "Shanghang Zhang",
      "Xiaodong Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nguyen_YoChameleon_Personalized_Vision_and_Language_Generation_CVPR_2025_paper.html": {
    "title": "Yo'Chameleon: Personalized Vision and Language Generation",
    "volume": "main",
    "abstract": "Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into powerful tools with millions of users. However, they remain generic models and lack personalized knowledge of specific user concepts. Previous work has explored personalization for text generation, yet it remains unclear how these methods can be adapted to new modalities, such as image generation. In this paper, we introduce Yo'Chameleon, the first attempt to study personalization for large multimodal models. Given 3-5 images of a particular concept, Yo'Chameleon leverages soft-prompt tuning to embed subject-specific information to (i) answer questions about the subject and (ii) recreate pixel-level details to produce images of the subject in new contexts. Yo'Chameleon is trained with (i) a self-prompting optimization mechanism to balance performance across multiple modalities, and (ii) a \"soft-positive\" image generation approach to enhance image quality in a few-shot setting. Our qualitative and quantitative analyses reveal that Yo'Chameleon can learn concepts more efficiently using fewer tokens and effectively encode visual attributes, outperforming prompting baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thao Nguyen",
      "Krishna Kumar Singh",
      "Jing Shi",
      "Trung Bui",
      "Yong Jae Lee",
      "Yuheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Du_PatchVSR_Breaking_Video_Diffusion_Resolution_Limits_with_Patch-wise_Video_Super-Resolution_CVPR_2025_paper.html": {
    "title": "PatchVSR: Breaking Video Diffusion Resolution Limits with Patch-wise Video Super-Resolution",
    "volume": "main",
    "abstract": "Pre-trained video generation models hold great potential for generative video super-resolution (VSR). However, adapting them for full-size VSR, as most existing methods do, suffers from unnecessary intensive full-attention computation and fixed output resolution. To overcome these limitations, we make the first exploration into utilizing video diffusion priors for patch-wise VSR.This is non-trivial because pre-trained video diffusion models are not native for patch-level detail generation. To mitigate this challenge, we propose an innovative approach, called PatchVSR, which integrates a dual-stream adapter for conditional guidance. The patch branch extracts features from input patches to maintain content fidelity while the global branch extracts context features from the resized full video to bridge the generation gap caused byincomplete semantics of patches.Particularly, we also inject the patch's location information into the model to better contextualize patch synthesis within the global video frame.Experiments demonstrate that our method can synthesize high-fidelity, high-resolution details at the patch level. A tailor-made multi-patch joint modulation is proposed to ensure visual consistency across individually enhanced patches. Due to the flexibility of our patch-based paradigm, we can achieve highly competitive 4K VSR based on a 512x512 resolution base model, with extremely high efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shian Du",
      "Menghan Xia",
      "Chang Liu",
      "Xintao Wang",
      "Jing Wang",
      "Pengfei Wan",
      "Di Zhang",
      "Xiangyang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dalva_FluxSpace_Disentangled_Semantic_Editing_in_Rectified_Flow_Models_CVPR_2025_paper.html": {
    "title": "FluxSpace: Disentangled Semantic Editing in Rectified Flow Models",
    "volume": "main",
    "abstract": "Rectified flow models have emerged as a dominant approach in image generation, showcasing impressive capabilities in high-quality image synthesis. However, despite their effectiveness in visual generation, rectified flow models often struggle with disentangled editing of images. This limitation prevents the ability to perform precise, attribute-specific modifications without affecting unrelated aspects of the image. In this paper, we introduce FluxSpace, a domain-agnostic image editing method leveraging a representation space with the ability to control the semantics of images generated by rectified flow transformers, such as Flux. By leveraging the representations learned by the transformer blocks within the rectified flow models, we propose a set of semantically interpretable representations that enable a wide range of image editing tasks, from fine-grained image editing to artistic creation. This work offers a scalable and effective image editing approach, along with its disentanglement capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusuf Dalva",
      "Kavana Venkatesh",
      "Pinar Yanardag"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hahn_Scene-Centric_Unsupervised_Panoptic_Segmentation_CVPR_2025_paper.html": {
    "title": "Scene-Centric Unsupervised Panoptic Segmentation",
    "volume": "main",
    "abstract": "Unsupervised panoptic segmentation aims to partition an image into semantically meaningful regions and distinct object instances without training on manually annotated data. In contrast to prior work on unsupervised panoptic scene understanding, we eliminate the need for object-centric training data, enabling the unsupervised understanding of complex scenes. To that end, we present the first unsupervised panoptic method that directly trains on scene-centric imagery. In particular, we propose an approach to obtain high-resolution panoptic pseudo labels on complex scene-centric data, combining visual representations, depth, and motion cues. Utilizing both pseudo-label training and a panoptic self-training strategy yields a novel approach that accurately predicts panoptic segmentation of complex scenes without requiring any human annotations. Our approach significantly improves panoptic quality, e.g., surpassing the recent state of the art in unsupervised panoptic segmentation on Cityscapes by 9.4% points in PQ",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Oliver Hahn",
      "Christoph Reich",
      "Nikita Araslanov",
      "Daniel Cremers",
      "Christian Rupprecht",
      "Stefan Roth"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Touch2Shape_Touch-Conditioned_3D_Diffusion_for_Shape_Exploration_and_Reconstruction_CVPR_2025_paper.html": {
    "title": "Touch2Shape: Touch-Conditioned 3D Diffusion for Shape Exploration and Reconstruction",
    "volume": "main",
    "abstract": "Diffusion models have made breakthroughs in 3D generation tasks. Current 3D diffusion models focus on reconstructing target shape from images or a set of partial observations. While excelling in global context understanding, they struggle to capture the local details of complex shapes and limited to the occlusion and lighting conditions. To overcome these limitations, we utilize tactile images to capture the local 3D information and propose a Touch2Shape model, which leverages a touch-conditioned diffusion model to explore and reconstruct the target shape from touch. For shape reconstruction, we have developed a touch embedding module to condition the diffusion model in creating a compact representation and a touch shape fusion module to refine the reconstructed shape. For shape exploration, we combine the diffusion model with reinforcement learning to train a policy. This involves using the generated latent vector from the diffusion model to guide the touch exploration policy training through a novel reward design. Experiments validate the reconstruction quality thorough both qualitatively and quantitative analysis, and our touch exploration policy further boosts reconstruction performance",
    "checked": true,
    "id": "40cd5b9ac7b1e57684daa1fe9fed9d208561968d",
    "semantic_title": "touch2shape: touch-conditioned 3d diffusion for shape exploration and reconstruction",
    "citation_count": 0,
    "authors": [
      "Yuanbo Wang",
      "Zhaoxuan Zhang",
      "Jiajin Qiu",
      "Dilong Sun",
      "Zhengyu Meng",
      "Xiaopeng Wei",
      "Xin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_VITED_Video_Temporal_Evidence_Distillation_CVPR_2025_paper.html": {
    "title": "VITED: Video Temporal Evidence Distillation",
    "volume": "main",
    "abstract": "We investigate complex video question answering via chain-of-evidence reasoning --- identifying sequences of temporal spans from multiple relevant parts of the video, together with visual evidence within them.Existing models struggle with multi-step reasoning as they uniformly sample a fixed number of frames, which can miss critical evidence distributed nonuniformly throughout the video. Moreover, they lack the ability to temporally localize such evidence in the broader context of the full video, which is required for answering complex questions. We propose a framework to enhance existing VideoQA datasets with evidence reasoning chains, automatically constructed by searching for optimal intervals of interest in the video with supporting evidence, that maximizes the likelihood of answering a given question.We train our model (ViTED) to generate these evidence chains directly, enabling it to both localize evidence windows as well as perform multi-step reasoning across them in long-form video content.We show the value of our evidence-distilled models on a suite of long video QA benchmarks where we outperform state-of-the-art approaches that lack evidence reasoning capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Lu",
      "Yale Song",
      "William Wang",
      "Lorenzo Torresani",
      "Tushar Nagarajan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Adversarial_Domain_Prompt_Tuning_and_Generation_for_Single_Domain_Generalization_CVPR_2025_paper.html": {
    "title": "Adversarial Domain Prompt Tuning and Generation for Single Domain Generalization",
    "volume": "main",
    "abstract": "Single domain generalization (SDG) aims to learn a robust model, which could perform well on many unseen domains while there is only one single domain available for training. One of the promising directions for achieving single-domain generalization is to generate out-of-domain (OOD) training data through data augmentation or image generation. Given the rapid advancements in AI-generated content (AIGC), this paper is the first to propose leveraging powerful pre-trained text-to-image (T2I) foundation models to create the training data. However, manually designing textual prompts to generate images for all possible domains is often impractical, and some domain characteristics may be too abstract to describe with words. To address these challenges, we propose a novel Progressive Adversarial Prompt Tuning (PAPT) framework for pre-trained diffusion models. Instead of relying on static textual domains, our approach learns two sets of abstract prompts as conditions for the diffusion model: one that captures domain-invariant category information and another that models domain-specific styles. This adversarial learning mechanism enables the T2I model to generate images in various domain styles while preserving key categorical features. Extensive experiments demonstrate the effectiveness of the proposed method, achieving superior performances to state-of-the-art single-domain generalization approaches",
    "checked": true,
    "id": "d7e2dc2d298bc9ed7d584563f96929c7692cb048",
    "semantic_title": "adversarial domain prompt tuning and generation for single domain generalization",
    "citation_count": 1,
    "authors": [
      "Zhipeng Xu",
      "De Cheng",
      "Xinyang Jiang",
      "Nannan Wang",
      "Dongsheng Li",
      "Xinbo Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Garcia_Learning_Physics_From_Video_Unsupervised_Physical_Parameter_Estimation_for_Continuous_CVPR_2025_paper.html": {
    "title": "Learning Physics From Video: Unsupervised Physical Parameter Estimation for Continuous Dynamical Systems",
    "volume": "main",
    "abstract": "Extracting physical dynamical system parameters from recorded observations is key in natural science. Current methods for automatic parameter estimation from video train supervised deep networks on large datasets. Such datasets require labels, which are difficult to acquire. While some unsupervised techniques--which depend on frame prediction--exist, they suffer from long training times, initialization instabilities, only consider motion-based dynamical systems, and are evaluated mainly on synthetic data. In this work, we propose an unsupervised method to estimate the physical parameters of known, continuous governing equations from single videos suitable for different dynamical systems beyond motion and robust to initialization. Moreover, we remove the need for frame prediction by implementing a KL-divergence-based loss function in the latent space, which avoids convergence to trivial solutions and reduces model size and compute. We first evaluate our model on synthetic data, as commonly done. After which, we take the field closer to reality by recording Delfys75: our own real-world dataset of 75 videos for five different types of dynamical systems to evaluate our method and others. Our method compares favorably to others. Code and data are available online: https://github.com/Alejandro-neuro/Learning_physics_from_video",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alejandro Casta√±eda Garcia",
      "Jan Warchocki",
      "Jan van Gemert",
      "Daan Brinks",
      "Nergis Tomen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cao_Temporal_Score_Analysis_for_Understanding_and_Correcting_Diffusion_Artifacts_CVPR_2025_paper.html": {
    "title": "Temporal Score Analysis for Understanding and Correcting Diffusion Artifacts",
    "volume": "main",
    "abstract": "Visual artifacts remain a persistent challenge in diffusion models, even with training on massive datasets. Current solutions primarily rely on supervised detectors, yet lack understanding of why these artifacts occur in the first place. In our analysis, we identify three distinct phases in the diffusion generative process: Profiling, Mutation, and Refinement. Artifacts typically emerge during the Mutation phase, where certain regions exhibit anomalous score dynamics over time, causing abrupt disruptions in the normal evolution pattern. This temporal nature explains why existing methods focusing only on spatial uncertainty of the final output fail at effective artifact localization. Based on these insights, we propose ASCED (Abnormal Score Correction for Enhancing Diffusion), that detects artifacts by monitoring abnormal score dynamics during the diffusion process, with a trajectory-aware on-the-fly mitigation strategy that appropriate generation of noise in the detected areas. Unlike most existing methods that apply post hoc corrections, e.g., by applying a noising-denoising scheme after generation, our mitigation strategy operates seamlessly within the existing diffusion process. Extensive experiments demonstrate that our proposed approach effectively reduces artifacts across diverse domains, matching or surpassing existing supervised methods without additional training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Cao",
      "Zengqun Zhao",
      "Ioannis Patras",
      "Shaogang Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qu_ProAPO_Progressively_Automatic_Prompt_Optimization_for_Visual_Classification_CVPR_2025_paper.html": {
    "title": "ProAPO: Progressively Automatic Prompt Optimization for Visual Classification",
    "volume": "main",
    "abstract": "Vision-language models (VLMs) have made significant progress in image classification by training with large-scale paired image-text data. Their performances largely depend on the prompt quality. While recent methods show that visual descriptions generated by large language models (LLMs) enhance the generalization of VLMs, class-specific prompts may be inaccurate or lack discrimination due to the hallucination in LLMs. In this paper, we aim to find visually discriminative prompts for fine-grained categories with minimal supervision and no human-in-the-loop. An evolution-based algorithm is proposed to progressively optimize language prompts from task-specific templates to class-specific descriptions. Unlike optimizing templates, the search space shows an explosion in class-specific candidate prompts. This increases prompt generation costs, iterative times, and the overfitting problem. To this end, we first introduce several simple yet effective edit-based and evolution-based operations to generate diverse candidate prompts by one-time query of LLMs. Then, two sampling strategies are proposed to find a better initial search point and reduce traversed categories, saving iteration costs. Moreover, we apply a novel fitness score with entropy constraints to mitigate overfitting. In a challenging one-shot image classification setting, our method outperforms existing textual prompt-based methods and improves LLM-generated description methods across 13 datasets. Meanwhile, we demonstrate that our optimal prompts improve adapter-based methods and transfer effectively across different backbones",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyan Qu",
      "Gaopeng Gou",
      "Jiamin Zhuang",
      "Jing Yu",
      "Kun Song",
      "Qihao Wang",
      "Yili Li",
      "Gang Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Petrov_ShapeWords_Guiding_Text-to-Image_Synthesis_with_3D_Shape-Aware_Prompts_CVPR_2025_paper.html": {
    "title": "ShapeWords: Guiding Text-to-Image Synthesis with 3D Shape-Aware Prompts",
    "volume": "main",
    "abstract": "We introduce ShapeWords, an approach for synthesizing images based on 3D shape guidance and text prompts.ShapeWords incorporates target 3D shape information within specialized tokens embedded together with the input text, effectively blending 3D shape awareness with textual context to guide the image synthesis process. Unlike conventional shape guidance methods that rely on depth maps restricted to fixed viewpoints and often overlook full 3D structure or textual context, ShapeWords generates diverse yet consistent images that reflect both the target shape's geometry and the textual description. Experimental results show that ShapeWords produces images that are more text-compliant, aesthetically plausible, while also maintaining 3D shape awareness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dmitry Petrov",
      "Pradyumn Goyal",
      "Divyansh Shivashok",
      "Yuanming Tao",
      "Melinos Averkiou",
      "Evangelos Kalogerakis"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Auto-Encoded_Supervision_for_Perceptual_Image_Super-Resolution_CVPR_2025_paper.html": {
    "title": "Auto-Encoded Supervision for Perceptual Image Super-Resolution",
    "volume": "main",
    "abstract": "This work tackles the fidelity objective in the perceptual super-resolution (SR) task. Specifically, we address the shortcomings of pixel-level \\mathcal L _\\text p loss (\\mathcal L _\\text pix ) in the GAN-based SR framework. Since \\mathcal L _\\text pix is known to have a trade-off relationship against perceptual quality, prior methods often multiply a small scale factor or utilize low-pass filters. However, this work shows that these circumventions fail to address the fundamental factor that induces blurring. Accordingly, we focus on two points: 1) precisely discriminating the subcomponent of \\mathcal L _\\text pix that contributes to blurring, and 2) guiding reconstruction only based on the factor that is free from this trade-off relationship. We show that this can be achieved in a surprisingly simple manner, with an Auto-Encoder (AE) pretrained using \\mathcal L _\\text pix . Based on this insight, we propose the Auto-Encoded Supervision for Optimal Penalization loss (\\mathcal L _\\text AESOP ), a novel loss function that measures distance in the AE space (the space after the decoder, not the bottleneck), rather than in the raw pixel space. By simply substituting \\mathcal L _\\text pix with \\mathcal L _\\text AESOP , we can provide effective reconstruction guidance without compromising perceptual quality. Designed for simplicity, our method enables easy integration into existing SR frameworks. Extensive experiments demonstrate the effectiveness of AESOP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "MinKyu Lee",
      "Sangeek Hyun",
      "Woojin Jun",
      "Jae-Pil Heo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chinchure_Black_Swan_Abductive_and_Defeasible_Video_Reasoning_in_Unpredictable_Events_CVPR_2025_paper.html": {
    "title": "Black Swan: Abductive and Defeasible Video Reasoning in Unpredictable Events",
    "volume": "main",
    "abstract": "The commonsense reasoning capabilities of vision-language models (VLMs), especially in abductive reasoning and defeasible reasoning, remain poorly understood. Most benchmarks focus on typical visual scenarios, making it difficult to discern whether model performance stems from keen perception and reasoning skills, or reliance on pure statistical recall. We argue that by focusing on atypical events in videos, clearer insights can be gained on the core capabilities of VLMs. Explaining and understanding such out-of-distribution events requires models to extend beyond basic pattern recognition and regurgitation of their prior knowledge. To this end, we introduce BlackSwanSuite, a benchmark for evaluating VLMs' ability to reason about unexpected events through abductive and defeasible tasks. Our tasks artificially limit the amount of visual information provided to models while questioning them about hidden unexpected events, or provide new visual information that could change an existing hypothesis about the event. We curate a comprehensive benchmark suite comprising over 3,800 MCQ, 4,900 generative and 6,700 yes/no questions, spanning 1,655 videos. After extensively evaluating various state-of-the-art VLMs, including GPT-4o and Gemini 1.5 Pro, as well as open-source VLMs such as LLaVA-Video, we find significant performance gaps of up to 32% from humans on these tasks. Our findings reveal key limitations in current VLMs, emphasizing the need for enhanced model architectures and training strategies. Our data and leaderboard is available at blackswan.cs.ubc.ca",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Chinchure",
      "Sahithya Ravi",
      "Raymond Ng",
      "Vered Shwartz",
      "Boyang Li",
      "Leonid Sigal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Burgert_Go-with-the-Flow_Motion-Controllable_Video_Diffusion_Models_Using_Real-Time_Warped_Noise_CVPR_2025_paper.html": {
    "title": "Go-with-the-Flow: Motion-Controllable Video Diffusion Models Using Real-Time Warped Noise",
    "volume": "main",
    "abstract": "Generative modeling aims to transform random noise into structured outputs. In this work, we enhance video diffusion models by allowing motion control via structured latent noise sampling. This is achieved by just a change in data: we pre-process training videos to yield structured noise. Consequently, our method is agnostic to diffusion model design, requiring no changes to model architectures or training pipelines. Specifically, we propose a novel noise warping algorithm, fast enough to run in real time, that replaces random temporal Gaussianity with correlated warped noise derived from optical flow fields, while preserving the spatial Gaussianity. The efficiency of our algorithm enables us to fine-tune modern video diffusion base models using warped noise with minimal overhead, and provide a one-stop solution for a wide range of user-friendly motion control: local object motion control, global camera movement control, and motion transfer. The harmonization between temporal coherence and spatial Gaussianity in our warped noise leads to effective motion control while maintaining per-frame pixel quality. Extensive experiments and user studies demonstrate the advantages of our method, making it a robust and scalable approach for controlling motion in video diffusion models. Please see our project webpage: https://eyeline-research.github.io/Go-with-the-Flow/ ; source code and checkpoints are available on GitHub: https://github.com/Eyeline-Research/Go-with-the-Flow",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryan Burgert",
      "Yuancheng Xu",
      "Wenqi Xian",
      "Oliver Pilarski",
      "Pascal Clausen",
      "Mingming He",
      "Li Ma",
      "Yitong Deng",
      "Lingxiao Li",
      "Mohsen Mousavi",
      "Michael Ryoo",
      "Paul Debevec",
      "Ning Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gan_Silence_is_Golden_Leveraging_Adversarial_Examples_to_Nullify_Audio_Control_CVPR_2025_paper.html": {
    "title": "Silence is Golden: Leveraging Adversarial Examples to Nullify Audio Control in LDM-based Talking-Head Generation",
    "volume": "main",
    "abstract": "Advances in talking-head animation based on Latent Diffusion Models (LDM) enable the creation of highly realistic, synchronized videos. These fabricated videos are indistinguishable from real ones, increasing the risk of potential misuse for scams, political manipulation, and misinformation. Hence, addressing these ethical concerns has become a pressing issue in AI security. Recent proactive defense studies focused on countering LDM-based models by adding perturbations to portraits. However, these methods are ineffective at protecting reference portraits from advanced image-to-video animation. The limitations are twofold: 1) they fail to prevent images from being manipulated by audio signals, and 2) diffusion-based purification techniques can effectively eliminate protective perturbations. To address these challenges, we propose Silencer, a two-stage method designed to proactively protect the privacy of portraits. First, a nullifying loss is proposed to ignore audio control in talking-head generation. Second, we apply anti-purification loss in LDM to optimize the inverted latent feature to generate robust perturbations. Extensive experiments demonstrate the effectiveness of Silencer in proactively protecting portrait privacy. We hope this work will raise awareness among the AI security community regarding critical ethical issues related to talking-head generation techniques. Code: https://github.com/yuangan/Silencer",
    "checked": true,
    "id": "a006958dd1f4ac3041b4c832cbde38eb5dd6141d",
    "semantic_title": "silence is golden: leveraging adversarial examples to nullify audio control in ldm-based talking-head generation",
    "citation_count": 0,
    "authors": [
      "Yuan Gan",
      "Jiaxu Miao",
      "Yunze Wang",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_Iterative_Predictor-Critic_Code_Decoding_for_Real-World_Image_Dehazing_CVPR_2025_paper.html": {
    "title": "Iterative Predictor-Critic Code Decoding for Real-World Image Dehazing",
    "volume": "main",
    "abstract": "We propose a novel Iterative Predictor-Critic Code Decoding framework for real-world image dehazing, abbreviated as IPC-Dehaze, which leverages the high-quality codebook prior encapsulated in a pre-trained VQGAN. Apart from previous codebook-based methods that rely on one-shot decoding, our method utilizes high-quality codes obtained in the previous iteration to guide the prediction of the Code-Predictor in the subsequent iteration, improving code prediction accuracy and ensuring stable dehazing performance. Our idea stems from the observations that 1) the degradation of hazy images varies with haze density and scene depth, and 2) clear regions play crucial cues in restoring dense haze regions. However, it is nontrivial to progressively refine the obtained codes in subsequent iterations, owing to the difficulty in determining which codes should be retained or replaced at each iteration. Another key insight of our study is to propose Code-Critic to capture interrelations among codes. The Code-Critic is used to evaluate code correlations and then resample a set of codes with the highest mask scores, i.e., a higher score indicates that the code is more likely to be rejected, which helps retain more accurate codes and predict difficult ones. Extensive experiments demonstrate the superiority of our method over state-of-the-art methods in real-world dehazing. Our code will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Fu",
      "Siyu Liu",
      "Zikun Liu",
      "Chun-Le Guo",
      "Hyunhee Park",
      "Ruiqi Wu",
      "Guoqing Wang",
      "Chongyi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_RNG_Relightable_Neural_Gaussians_CVPR_2025_paper.html": {
    "title": "RNG: Relightable Neural Gaussians",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) has shown impressive results for the novel view synthesis task, where lighting is assumed to be fixed. However, creating relightable 3D assets, especially for objects with ill-defined shapes (fur, fabric, etc.), remains a challenging task. The decomposition between light, geometry, and material is ambiguous, especially if either smooth surface assumptions or surface-based analytical shading models do not apply. We propose Relightable Neural Gaussians (RNG), a novel 3DGS-based framework that enables the relighting of objects with both hard surfaces or soft boundaries, while avoiding assumptions on the shading model. We condition the radiance at each point on both view and light directions. We also introduce a shadow cue, as well as a depth refinement network to improve shadow accuracy. Finally, we propose a hybrid forward-deferred fitting strategy to balance geometry and appearance quality. Our method achieves significantly faster training (1.3 hours) and rendering (60 frames per second) compared to a prior method based on neural radiance fields and produces higher-quality shadows than a concurrent 3DGS-based method",
    "checked": true,
    "id": "a5e742c34c0360d565256a15b595f2dec21e2550",
    "semantic_title": "rng: relightable neural gaussians",
    "citation_count": 4,
    "authors": [
      "Jiahui Fan",
      "Fujun Luan",
      "Jian Yang",
      "Milos Hasan",
      "Beibei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_Towards_Realistic_Example-based_Modeling_via_3D_Gaussian_Stitching_CVPR_2025_paper.html": {
    "title": "Towards Realistic Example-based Modeling via 3D Gaussian Stitching",
    "volume": "main",
    "abstract": "Using parts of existing models to rebuild new models, commonly termed as example-based modeling, is a classical methodology in the realm of computer graphics. Previous works mostly focus on shape composition, making them very hard to use for realistic composition of 3D objects captured from real-world scenes. This leads to combining multiple NeRFs into a single 3D scene to achieve seamless appearance blending. However, the current SeamlessNeRF method struggles to achieve interactive editing and harmonious stitching for real-world scenes due to its gradient-based strategy and grid-based representation. To this end, we present an example-based modeling method that combines multiple Gaussian fields in a point-based representation using sample-guided synthesis. Specifically, as for composition, we create a GUI to segment and transform multiple fields in real time, easily obtaining a semantically meaningful composition of models represented by 3D Gaussian Splatting (3DGS). For texture blending, due to the discrete and irregular nature of 3DGS, straightforwardly applying gradient propagation as SeamlssNeRF is not supported. Thus, a novel sampling-based cloning method is proposed to harmonize the blending while preserving the original rich texture and content. Our workflow consists of three steps: 1) real-time segmentation and transformation of a Gaussian model using a well-tailored GUI, 2) KNN analysis to identify boundary points in the intersecting area between the source and target models, and 3) two-phase optimization of the target model using sampling-based cloning and gradient constraints. Extensive experimental results validate that our approach significantly outperforms previous works in terms of realistic synthesis, demonstrating its practicality",
    "checked": true,
    "id": "37f8a4c86e2f4afb7a986020ae89a3ea163cfa03",
    "semantic_title": "towards realistic example-based modeling via 3d gaussian stitching",
    "citation_count": 5,
    "authors": [
      "Xinyu Gao",
      "Ziyi Yang",
      "Bingchen Gong",
      "Xiaoguang Han",
      "Sipeng Yang",
      "Xiaogang Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Safaei_Filter_Images_First_Generate_Instructions_Later_Pre-Instruction_Data_Selection_for_CVPR_2025_paper.html": {
    "title": "Filter Images First, Generate Instructions Later: Pre-Instruction Data Selection for Visual Instruction Tuning",
    "volume": "main",
    "abstract": "Visual instruction tuning (VIT) for large vision-language models (LVLMs) requires training on expansive datasets of image-instruction pairs, which can be costly. Recent efforts in VIT data selection aim to select a small subset of high-quality image-instruction pairs, reducing VIT runtime while maintaining performance comparable to full-scale training. However, a major challenge often overlooked is that generating instructions from unlabeled images for VIT is highly expensive. Most existing VIT datasets rely heavily on human annotations or paid services like the GPT API, which limits users with constrained resources from creating VIT datasets for custom applications. To address this, we introduce Pre-Instruction Data Selection (PreSel), a more practical data selection paradigm that directly selects the most beneficial unlabeled images and generates instructions only for the selected images. PreSel first estimates the relative importance of each vision task within VIT datasets to derive task-wise sampling budgets. It then clusters image features within each task, selecting the most representative images with the budget. This approach reduces computational overhead for both instruction generation during VIT data formation and LVLM fine-tuning. By generating instructions for only 15% of the images, PreSel achieves performance comparable to full-data VIT on the LLaVA-1.5 and Vision-Flan datasets. The link to our project page: https://bardisafa.github.io/PreSel",
    "checked": true,
    "id": "1098bd3e0317c7b3d63e95c25668f44258c5c606",
    "semantic_title": "filter images first, generate instructions later: pre-instruction data selection for visual instruction tuning",
    "citation_count": 5,
    "authors": [
      "Bardia Safaei",
      "Faizan Siddiqui",
      "Jiacong Xu",
      "Vishal M. Patel",
      "Shao-Yuan Lo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ballas_Gradient-Guided_Annealing_for_Domain_Generalization_CVPR_2025_paper.html": {
    "title": "Gradient-Guided Annealing for Domain Generalization",
    "volume": "main",
    "abstract": "Domain Generalization (DG) research has gained considerable traction as of late, since the ability to generalize to unseen data distributions is a requirement that eludes even state-of-the-art training algorithms. In this paper we observe that the initial iterations of model training play a key role in domain generalization effectiveness, since the loss landscape may be significantly different across the training and test distributions, contrary to the case of i.i.d. data. Conflicts between gradients of the loss components of each domain lead the optimization procedure to undesirable local minima that do not capture the domain-invariant features of the target classes. We propose alleviating domain conflicts in model optimization, by iteratively annealing the parameters of a model in the early stages of training and searching for points where gradients align between domains. By discovering a set of parameter values where gradients are updated towards the same direction for each data distribution present in the training set, the proposed Gradient-Guided Annealing (GGA) algorithm encourages models to seek out minima that exhibit improved robustness against domain shifts. The efficacy of GGA is evaluated on five widely accepted and challenging image classification domain generalization benchmarks, where its use alone is able to establish highly competitive or even state-of-the-art performance. Moreover, when combined with previously proposed domain-generalization algorithms it is able to consistently improve their effectiveness by significant margins",
    "checked": true,
    "id": "85bbb7998007df7781c2b68c56b0126c7fbd253c",
    "semantic_title": "gradient-guided annealing for domain generalization",
    "citation_count": 3,
    "authors": [
      "Aristotelis Ballas",
      "Christos Diou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kong_Generative_Sparse-View_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "Generative Sparse-View Gaussian Splatting",
    "volume": "main",
    "abstract": "Novel view synthesis from limited observations remains a significant challenge due to the lack of information in under-sampled regions, often resulting in noticeable artifacts. We introduce Generative Sparse-view Gaussian Splatting (GS-GS), a general pipeline designed to enhance the rendering quality of 3D/4D Gaussian Splatting (GS) when training views are sparse. Our method generates unseen views using generative models, specifically leveraging pre-trained image diffusion models to iteratively refine view consistency and hallucinate additional images at pseudo views. This approach improves 3D/4D scene reconstruction by explicitly enforcing semantic correspondences during the generation of unseen views, thereby enhancing geometric consistency--unlike purely generative methods that often fail to maintain view consistency. Extensive evaluations on various 3D/4D datasets--including Blender, LLFF, Mip-NeRF360, and Neural 3D Video--demonstrate that our GS-GS outperforms existing state-of-the-art methods in rendering quality without sacrificing efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyang Kong",
      "Xingyi Yang",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Burgess_MicroVQA_A_Multimodal_Reasoning_Benchmark_for_Microscopy-Based_Scientific_Research_CVPR_2025_paper.html": {
    "title": "MicroVQA: A Multimodal Reasoning Benchmark for Microscopy-Based Scientific Research",
    "volume": "main",
    "abstract": "Scientific research demands sophisticated reasoning over multimodal data, a challenge especially prevalent in biology. Despite recent advances in multimodal large language models (MLLMs) for AI-assisted research, existing multimodal reasoning benchmarks only target up to college-level difficulty, while research-level benchmarks emphasize lower-level perception, falling short of the complex multimodal reasoning needed for scientific discovery. To bridge this gap, we introduce MicroVQA, a visual-question answering (VQA) benchmark designed to assess three reasoning capabilities vital in research workflows: expert image understanding, hypothesis generation, and experiment proposal. MicroVQA consists of 1,042 multiple-choice questions (MCQs) curated by biology experts across diverse microscopy modalities, ensuring VQA samples represent real scientific practice. In constructing the benchmark, we find that standard MCQ generation methods induce language shortcuts, motivating a new two-stage pipeline: an optimized LLM prompt structures question-answer pairs into MCQs; then, an agent-based 'RefineBot' updates them to remove shortcuts. Benchmarking on state-of-the-art MLLMs reveal a peak performance of 53%; models with smaller LLMs only slightly underperform top models, suggesting that language-based reasoning is less challenging than multimodal reasoning; and tuning with scientific articles enhances performance. Expert analysis of chain-of-thought responses shows that perception errors are the most frequent, followed by knowledge errors and then overgeneralization errors. These insights highlight the challenges in multimodal scientific reasoning, showing MicroVQA is a valuable resource advancing AI-driven biomedical research. MicroVQA is available at https://huggingface.co/datasets/jmhb/microvqa and project at https://jmhb0.github.io/microvqa",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "James Burgess",
      "Jeffrey J Nirschl",
      "Laura Bravo-S√°nchez",
      "Alejandro Lozano",
      "Sanket Rajan Gupte",
      "Jesus G. Galaz-Montoya",
      "Yuhui Zhang",
      "Yuchang Su",
      "Disha Bhowmik",
      "Zachary Coman",
      "Sarina M Hasan",
      "Alexandra Johannesson",
      "William D. Leineweber",
      "Malvika G Nair",
      "Ridhi Yarlagadda",
      "Connor Zuraski",
      "Wah Chiu",
      "Sarah Cohen",
      "Jan N. Hansen",
      "Manuel D Leonetti",
      "Chad Liu",
      "Emma Lundberg",
      "Serena Yeung-Levy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Generative_Inbetweening_through_Frame-wise_Conditions-Driven_Video_Generation_CVPR_2025_paper.html": {
    "title": "Generative Inbetweening through Frame-wise Conditions-Driven Video Generation",
    "volume": "main",
    "abstract": "Generative inbetweening aims to generate intermediate frame sequences by utilizing two key frames as input. Although remarkable progress has been made in video generation models, generative inbetweening still faces challenges in maintaining temporal stability due to the ambiguous interpolation path between two key frames. This issue becomes particularly severe when there is a large motion gap between input frames. In this paper, we propose a straightforward yet highly effective Frame-wise Conditions-driven Video Generation (FCVG) method that significantly enhances the temporal stability of interpolated video frames. Specifically, our FCVG provides an explicit condition for each frame, making it much easier to identify the interpolation path between two input frames and thus ensuring temporally stable production of visually plausible video frames. To achieve this, we suggest extracting matched lines from two input frames that can then be easily interpolated frame by frame, serving as frame-wise conditions seamlessly integrated into existing video generation models. In extensive evaluations covering diverse scenarios such as natural landscapes, complex human poses, camera movements and animations, existing methods often exhibit incoherent transitions across frames. In contrast, our FCVG demonstrates the capability to generate temporally stable videos using both linear and non-linear interpolation curves. Our project page and code are available at https://fcvg-inbetween.github.io/",
    "checked": true,
    "id": "1d59482e3769d047299f71920a04042c5243ea69",
    "semantic_title": "generative inbetweening through frame-wise conditions-driven video generation",
    "citation_count": 4,
    "authors": [
      "Tianyi Zhu",
      "Dongwei Ren",
      "Qilong Wang",
      "Xiaohe Wu",
      "Wangmeng Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhong_DexGrasp_Anything_Towards_Universal_Robotic_Dexterous_Grasping_with_Physics_Awareness_CVPR_2025_paper.html": {
    "title": "DexGrasp Anything: Towards Universal Robotic Dexterous Grasping with Physics Awareness",
    "volume": "main",
    "abstract": "A dexterous hand capable of grasping any object is essential for the development of general-purpose embodied intelligent robots. However, due to the high degree of freedom in dexterous hands and the vast diversity of objects, generating high-quality, usable grasping poses in a robust manner is a significant challenge. In this paper, we introduce DexGrasp Anything, a method that effectively integrates physical constraints into both the training and sampling phases of a diffusion-based generative model, achieving state-of-the-art performance across nearly all open datasets. Additionally, we present a new dexterous grasping dataset containing over 3.4 million diverse grasping poses for more than 15k different objects, demonstrating its potential to advance universal dexterous grasping. The code of our method and our dataset will be publicly released soon",
    "checked": true,
    "id": "67b81a7a869a209bfaf3ce6e1ab12f9776cc04ed",
    "semantic_title": "dexgrasp anything: towards universal robotic dexterous grasping with physics awareness",
    "citation_count": 10,
    "authors": [
      "Yiming Zhong",
      "Qi Jiang",
      "Jingyi Yu",
      "Yuexin Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kong_CustAny_Customizing_Anything_from_A_Single_Example_CVPR_2025_paper.html": {
    "title": "CustAny: Customizing Anything from A Single Example",
    "volume": "main",
    "abstract": "Recent advances in diffusion-based text-to-image models have simplified creating high-fidelity images, but preserving the identity (ID) of specific elements, like a personal dog, is still challenging.Object customization, using reference images and textual descriptions, is key to addressing this issue. Current object customization methods are either object-specific, requiring extensive fine-tuning, or object-agnostic, offering zero-shot customization but limited to specialized domains. The primary issue of promoting zero-shot object customization from specific domains to the general domain is to establish a large-scale general ID dataset for model pre-training, which is time-consuming and labor-intensive. In this paper, we propose a novel pipeline to construct a large dataset of general objects and build the Multi-Category ID-Consistent (MC-IDC) dataset, featuring 315k text-image samples across 10k categories. With the help of MC-IDC, we introduce Customizing Anything (CustAny), a zero-shot framework that maintains ID fidelity and supports flexible text editing for general objects. CustAny features three key components: a general ID extraction module, a dual-level ID injection module, and an ID-aware decoupling module, allowing it to customize any object from a single reference image and text prompt. Experiments demonstrate that CustAny outperforms existing methods in both general object customization and specialized domains like human customization and virtual try-on. Our contributions include a large-scale dataset, the CustAny framework and novel ID processing to advance this field. The official project page is in https://lingjiekong-fdu.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingjie Kong",
      "Kai Wu",
      "Chengming Xu",
      "Xiaobin Hu",
      "Wenhui Han",
      "Jinlong Peng",
      "Donghao Luo",
      "Mengtian Li",
      "Jiangning Zhang",
      "Chengjie Wang",
      "Yanwei Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zeng_Vision-Language_Gradient_Descent-driven_All-in-One_Deep_Unfolding_Networks_CVPR_2025_paper.html": {
    "title": "Vision-Language Gradient Descent-driven All-in-One Deep Unfolding Networks",
    "volume": "main",
    "abstract": "Dynamic image degradations, including noise, blur and lighting inconsistencies, pose significant challenges in image restoration, often due to sensor limitations or adverse environmental conditions. Existing Deep Unfolding Networks (DUNs) offer stable restoration performance but require manual selection of degradation matrices for each degradation type, limiting their adaptability across diverse scenarios.To address this issue, we propose the Vision-Language-guided Unfolding Network (VLU-Net), a unified DUN framework for handling multiple degradation types simultaneously.VLU-Net leverages a Vision-Language Model (VLM) refined on degraded image-text pairs to align image features with degradation descriptions, selecting the appropriate transform for target degradation.By integrating an automatic VLM-based gradient estimation strategy into the Proximal Gradient Descent (PGD) algorithm, VLU-Net effectively tackles complex multi-degradation restoration tasks while maintaining interpretability. Furthermore, we design a hierarchical feature unfolding structure to enhance VLU-Net framework, efficiently synthesizing degradation patterns across various levels.VLU-Net is the first all-in-one DUN framework and outperforms current leading one-by-one and all-in-one end-to-end methods by 3.74 dB on the SOTS dehazing dataset and 1.70 dB on the Rain100L deraining dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haijin Zeng",
      "Xiangming Wang",
      "Yongyong Chen",
      "Jingyong Su",
      "Jie Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_3D-LLaVA_Towards_Generalist_3D_LMMs_with_Omni_Superpoint_Transformer_CVPR_2025_paper.html": {
    "title": "3D-LLaVA: Towards Generalist 3D LMMs with Omni Superpoint Transformer",
    "volume": "main",
    "abstract": "Current 3D Large Multimodal Models (3D LMMs) have shown tremendous potential in 3D-vision-based dialogue and reasoning. However, how to further enhance 3D LMMs to achieve fine-grained scene understanding and facilitate flexible human-agent interaction remains a challenging problem. In this work, we introduce 3D-LLaVA, a simple yet highly powerful 3D LMM designed to act as an intelligent assistant in comprehending, reasoning, and interacting with the 3D world. Unlike existing top-performing methods that rely on complicated pipelines--such as offline multi-view feature extraction or additional task-specific heads--3D-LLaVA adopts a minimalist design with integrated architecture and only takes point clouds as input. At the core of 3D-LLaVA is a new Omni Superpoint Transformer (OST), which integrates three functionalities: (1) a visual feature selector that converts and selects visual tokens, (2) a visual prompt encoder that embeds interactive visual prompts into the visual token space, and (3) a referring mask decoder that produces 3D masks based on text description. This versatile OST is empowered by the hybrid pretraining to obtain perception priors and leveraged as the visual connector that bridges the 3D data to the LLM. After performing unified instruction tuning, our 3D-LLaVA reports impressive results on various benchmarks. The code and model will be released at https://github.com/djiajunustc/3D-LLaVA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajun Deng",
      "Tianyu He",
      "Li Jiang",
      "Tianyu Wang",
      "Feras Dayoub",
      "Ian Reid"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_Event-based_Video_Super-Resolution_via_State_Space_Models_CVPR_2025_paper.html": {
    "title": "Event-based Video Super-Resolution via State Space Models",
    "volume": "main",
    "abstract": "Exploiting temporal correlations is crucial for video super-resolution (VSR). Recent approaches enhance this by incorporating event cameras. In this paper, we introduce MamEVSR, a Mamba-based network for event-based VSR that leverages the selective state space model, Mamba. MamEVSR stands out by offering global receptive field coverage with linear computational complexity, thus addressing the limitations of convolutional neural networks and Transformers. The key components of MamEVSR include: (1) The interleaved Mamba (iMamba) block, which interleaves tokens from adjacent frames and applies multidirectional selective state space modeling, enabling efficient feature fusion and propagation across bi-directional frames while maintaining linear complexity. (2) The crossmodality Mamba (cMamba) block facilitates further interaction and aggregation between event information and the output from the iMamba block. The cMamba block can leverage complementary spatio-temporal information from both modalities and allows MamEVSR to capture finermotion details. Experimental results show that the proposed MamEVSR achieves superior performance on various datasets quantitatively and qualitatively",
    "checked": true,
    "id": "b8efd598ac0304530b715bad325ded40c8353bc0",
    "semantic_title": "event-based video super-resolution via state space models",
    "citation_count": 6,
    "authors": [
      "Zeyu Xiao",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ji_PoseTraj_Pose-Aware_Trajectory_Control_in_Video_Diffusion_CVPR_2025_paper.html": {
    "title": "PoseTraj: Pose-Aware Trajectory Control in Video Diffusion",
    "volume": "main",
    "abstract": "Recent advancements in trajectory-guided video generation have achieved notable progress. However, existing models still face challenges in generating object motions with potentially changing 6D poses under wide-range rotations, due to limited 3D understanding. To address this problem, we introduce PoseTraj, a pose-aware video dragging model for generating 3D-aligned motion from 2D trajectories. Our method adopts a novel two-stage pose-aware pretraining framework, improving 3D understanding across diverse trajectories. Specifically, we propose a large-scale synthetic dataset PoseTraj-10k, containing 10k videos of objects following rotational trajectories, and enhance the model perception of object pose changes by incorporating 3D bounding boxes as intermediate supervision signals. Following this, we fine-tune the trajectory-controlling module on real-world videos, applying an additional camera-disentanglement module to further refine motion accuracy. Experiments on various benchmark datasets demonstrate that our method not only excels in 3D pose-aligned dragging for rotational trajectories but also outperforms existing baselines in trajectory accuracy and video quality",
    "checked": true,
    "id": "882c06129d64c151be6d9d2ee82f46a2ba605b97",
    "semantic_title": "posetraj: pose-aware trajectory control in video diffusion",
    "citation_count": 0,
    "authors": [
      "Longbin Ji",
      "Lei Zhong",
      "Pengfei Wei",
      "Changjian Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hermosilla_Masked_Scene_Modeling_Narrowing_the_Gap_Between_Supervised_and_Self-Supervised_CVPR_2025_paper.html": {
    "title": "Masked Scene Modeling: Narrowing the Gap Between Supervised and Self-Supervised Learning in 3D Scene Understanding",
    "volume": "main",
    "abstract": "Self-supervised learning has transformed 2D computer vision by enabling models trained on large, unannotated datasets to provide versatile off-the-shelf features that perform similarly to models trained with labels. However, in 3D scene understanding, self-supervised methods are typically only used as a weight initialization step for task-specific fine-tuning, limiting their utility for general-purpose feature extraction. This paper aims to address this shortcoming by proposing a robust evaluation protocol specifically designed to assess the quality of self-supervised features for 3D scene understanding. Our protocol uses multi-resolution feature sampling of hierarchical models to create rich point-level representations that capture the semantic capabilities of the model and, hence, are suitable for evaluation with linear probing and nearest-neighbor methods. Furthermore, we introduce the first self-supervised model that performs similarly to supervised models when only off-the-shelf features are used in a linear probing setup. In particular, our model is trained natively in 3D with a novel self-supervised approach based on a Masked Scene Modeling objective, which reconstructs deep features of masked patches in a bottom-up manner and is specifically tailored to hierarchical 3D models. Our experiments not only demonstrate that our method achieves competitive performance to supervised models, but also surpasses existing self-supervised approaches by a large margin",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pedro Hermosilla",
      "Christian Stippel",
      "Leon Sick"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jang_VL2Lite_Task-Specific_Knowledge_Distillation_from_Large_Vision-Language_Models_to_Lightweight_CVPR_2025_paper.html": {
    "title": "VL2Lite: Task-Specific Knowledge Distillation from Large Vision-Language Models to Lightweight Networks",
    "volume": "main",
    "abstract": "Deploying high-performing neural networks in resource-constrained environments poses a significant challenge due to the computational demands of large-scale models. We introduce VL2Lite, a knowledge distillation framework designed to enhance the performance of lightweight neural networks in image classification tasks by leveraging the rich representational knowledge from Vision-Language Models (VLMs). VL2Lite directly integrates multi-modal knowledge from VLMs into compact models during training, effectively compensating for the limited computational and modeling capabilities of smaller networks. By transferring high-level features and complex data representations, our approach improves the accuracy and efficiency of image classification tasks without increasing computational overhead during inference. Experimental evaluations demonstrate that VL2Lite achieves up to a 7% improvement in classification performance across various datasets. This method addresses the challenge of deploying accurate models in environments with constrained computational resources, offering a balanced solution between model complexity and operational efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinseong Jang",
      "Chunfei Ma",
      "Byeongwon Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Boost_the_Inference_with_Co-training_A_Depth-guided_Mutual_Learning_Framework_CVPR_2025_paper.html": {
    "title": "Boost the Inference with Co-training: A Depth-guided Mutual Learning Framework for Semi-supervised Medical Polyp Segmentation",
    "volume": "main",
    "abstract": "Semi-supervised polyp segmentation has made significant progress in recent years as a potential solution for computer-assisted treatment. Since depth images can provide extra information other than RGB images to help segment these problematic areas, depth-assisted polyp segmentation has gained much attention. However, the utilization of depth information is still worth studying. The existing RGB-D segmentation methods rely on depth data in the inference stage, limiting their clinical applications. To tackle this problem, we propose a semi-supervised polyp segmentation framework based on the mean teacher architecture. We establish an auxiliary student network with depth images as input in the training stage, and we propose a depth-guided cross-modal mutual learning strategy to promote the learning of complementary information between different student networks. Meanwhile, we use the high-confidence pseudo-labels generated by the auxiliary student network to guide the learning progress of the main student network from different perspectives. Our model does not need depth data in the inference phase. In addition, we introduce a depth-guided patch augmentation method to improve the model's learning performance in difficult regions of unlabeled polyp images. Experimental results show that our method achieves state-of-the-art performance under different label conditions on five polyp datasets. The code is available at https://github.com/pingchuan/RD-Net",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Li",
      "Zihao Zhu",
      "Yuxiang Zhang",
      "Yifan Chen",
      "Zhibin Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_VidHalluc_Evaluating_Temporal_Hallucinations_in_Multimodal_Large_Language_Models_for_CVPR_2025_paper.html": {
    "title": "VidHalluc: Evaluating Temporal Hallucinations in Multimodal Large Language Models for Video Understanding",
    "volume": "main",
    "abstract": "Multimodal large language models (MLLMs) have recently shown significant advancements in video understanding, excelling in content reasoning and instruction-following tasks. However, hallucination, where models generate inaccurate or misleading content, remains underexplored in the video domain. Building on the observation that MLLM visual encoders often fail to distinguish visually different yet semantically similar video pairs, we introduce VidHalluc, the largest benchmark designed to examine hallucinations in MLLMs for video understanding. It consists of 5,002 videos, paired to highlight cases prone to hallucinations. VidHalluc assesses hallucinations across three critical dimensions: (1) action, (2) temporal sequence, and (3) scene transition. Comprehensive testing shows that most MLLMs are vulnerable to hallucinations across these dimensions. Furthermore, we propose DINO-HEAL, a training-free method that reduces hallucinations by incorporating spatial saliency from DINOv2 to reweight visual features during inference. Our results show that DINO-HEAL consistently improves performance on VidHalluc, achieving an average improvement of 3.02% in mitigating hallucinations across all tasks. Both the VidHalluc benchmark and DINO-HEAL code are available at https://people-robots.github.io/vidhalluc",
    "checked": true,
    "id": "24a48ef14c8eb4e571e3f4ae9b37936060a3fb06",
    "semantic_title": "vidhalluc: evaluating temporal hallucinations in multimodal large language models for video understanding",
    "citation_count": 14,
    "authors": [
      "Chaoyu Li",
      "Eun Woo Im",
      "Pooyan Fazli"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gan_StageDesigner_Artistic_Stage_Generation_for_Scenography_via_Theater_Scripts_CVPR_2025_paper.html": {
    "title": "StageDesigner: Artistic Stage Generation for Scenography via Theater Scripts",
    "volume": "main",
    "abstract": "In this work, we introduce StageDesigner, the first comprehensive framework for artistic stage generation using large language models combined with layout-controlled diffusion models. Given the professional requirements of stage scenography, StageDesigner simulates the workflows of seasoned artists to generate immersive 3D stage scenes. Specifically, our approach is divided into three primary modules: Script Analysis, which extracts thematic and spatial cues from input scripts; Foreground Generation, which constructs and arranges essential 3D objects; and Background Generation, which produces a harmonious background aligned with the narrative atmosphere and maintains spatial coherence by managing occlusions between foreground and background elements. Furthermore, we introduce the StagePro-V1 dataset, a dedicated dataset with 276 unique stage scenes spanning different historical styles and annotated with scripts, images, and detailed 3D layouts, specifically tailored for this task. Finally, evaluations using both standard and newly proposed metrics, along with extensive user studies, demonstrate the effectiveness of StageDesigner, showcasing its ability to produce visually and thematically cohesive stages that meet both artistic and spatial coherence standards. Project can be found at: https://deadsmither5.github.io/2025/01/03/StageDesigner/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoxing Gan",
      "Mengtian Li",
      "Ruhua Chen",
      "Zhongxia Ji",
      "Sichen Guo",
      "Huanling Hu",
      "Guangnan Ye",
      "Zuo Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_From_Laboratory_to_Real_World_A_New_Benchmark_Towards_Privacy-Preserved_CVPR_2025_paper.html": {
    "title": "From Laboratory to Real World: A New Benchmark Towards Privacy-Preserved Visible-Infrared Person Re-Identification",
    "volume": "main",
    "abstract": "Aiming to match pedestrian images captured under varying lighting conditions, visible-infrared person re-identification (VI-ReID) has drawn intensive research attention and achieved promising results. However, in real-world surveillance contexts, data is distributed across multiple devices/entities, raising privacy and ownership concerns that make existing centralized training impractical for VI-ReID. To tackle these challenges, we propose L2RW, a benchmark that brings VI-ReID closer to real-world applications. The rationale of L2RW is that integrating decentralized training into VI-ReID can address privacy concerns in scenarios with limited data-sharing regulation. Specifically, we design protocols and corresponding algorithms for different privacy sensitivity levels. In our new benchmark, we ensure the model training is done in the conditions that: 1) data from each camera remains completely isolated, or 2) different data entities (e.g., data controllers of a certain region) can selectively share the data. In this way, we simulate scenarios with strict privacy constraints which is closer to real-world conditions. Intensive experiments with various server-side federated algorithms are conducted, showing the feasibility of decentralized VI-ReID training. Notably, when evaluated in unseen domains (i.e., new data entities), our L2RW, trained with isolated data (privacy-preserved), achieves performance comparable to SOTAs trained with shared data (privacy-unrestricted). We hope this work offers a novel research entry for deploying VI-ReID that fits real-world scenarios and can benefit the community",
    "checked": true,
    "id": "4e1e68663c4ec8c8d2cb33cffe8bed64d41d3402",
    "semantic_title": "from laboratory to real world: a new benchmark towards privacy-preserved visible-infrared person re-identification",
    "citation_count": 1,
    "authors": [
      "Yan Jiang",
      "Hao Yu",
      "Xu Cheng",
      "Haoyu Chen",
      "Zhaodong Sun",
      "Guoying Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sang_4Deform_Neural_Surface_Deformation_for_Robust_Shape_Interpolation_CVPR_2025_paper.html": {
    "title": "4Deform: Neural Surface Deformation for Robust Shape Interpolation",
    "volume": "main",
    "abstract": "Generating realistic intermediate shapes between non-rigidly deformed shapes is a challenging task in computer vision, especially with unstructured data (e.g., point clouds) where temporal consistency across frames is lacking, and topologies are changing. Most interpolation methods are designed for structured data (i.e., meshes) and do not apply to real-world point clouds. In contrast, our approach leverages neural implicit representation (NIR) to enable free-topology changing shape deformation. Unlike previous mesh-based methods, which model learns vertex-based deformation fields, our method learns a continuous velocity field in Euclidean space, making it suitable for less structured data such as point clouds.Additionally, our method does not require intermediate-shape supervision during training; instead, we incorporate physical and geometrical constraints to regularize the velocity field. We reconstruct intermediate surfaces using a modified level-set equation, directly linking our NIR with the velocity field. Experiments show that our method significantly outperforms previous NIR approaches across various scenarios (e.g., noisy, partial, topology-changing, non-isometric shapes) and, for the first time, enables new applications like 4D Kinect sequence upsampling and real-world high-resolution mesh deformation",
    "checked": true,
    "id": "10ae0116a68d575d090eb8114a8b1779827406dd",
    "semantic_title": "4deform: neural surface deformation for robust shape interpolation",
    "citation_count": 3,
    "authors": [
      "Lu Sang",
      "Zehranaz Canfes",
      "Dongliang Cao",
      "Riccardo Marin",
      "Florian Bernard",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Astermark_Dense_Match_Summarization_for_Faster_Two-view_Estimation_CVPR_2025_paper.html": {
    "title": "Dense Match Summarization for Faster Two-view Estimation",
    "volume": "main",
    "abstract": "In this paper, we speed up robust two-view relative pose from dense correspondences. Previous work has shown that dense matchers can significantly improve both accuracy and robustness in the resulting pose. However, the large number of matches comes with a significantly increased runtime during robust estimation in RANSAC. To avoid this, we propose an efficient match summarization scheme which provides comparable accuracy to using the full set of dense matches, while having 10-100x faster runtime. We validate our approach on standard benchmark datasets together with multiple state-of-the-art dense matchers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonathan Astermark",
      "Anders Heyden",
      "Viktor Larsson"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Align-A-Video_Deterministic_Reward_Tuning_of_Image_Diffusion_Models_for_Consistent_CVPR_2025_paper.html": {
    "title": "Align-A-Video: Deterministic Reward Tuning of Image Diffusion Models for Consistent Video Editing",
    "volume": "main",
    "abstract": "Due to control limitations in the denoising process and the lack of training, zero-shot video editing methods often struggle to meet user instructions, resulting in generated videos that are visually unappealing and fail to fully satisfy expectations. To address this problem, we propose Align-A-Video, a video editing pipeline that incorporates human feedback through reward fine-tuning. Our approach consists of two key steps: 1) Deterministic Reward Fine-tuning. To reduce optimization costs for expected noise distributions, we propose a deterministic reward tuning strategy. This method improves tuning stability by increasing sample determinism, allowing the tuning process to be completed in minutes; 2) Feature Propagation Across Frames. We optimize a selected anchor frame and propagate its features to the remaining frames, improving both visual quality and semantic fidelity. This approach avoids temporal consistency degradation from reward optimization. Extensive qualitative and quantitative experiments confirm the effectiveness of using reward fine-tuning in Align-A-Video, significantly improving the overall quality of generated videos",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengzhi Wang",
      "Yingkang Zhong",
      "Jiangchuan Mu",
      "Kai Wu",
      "Mingliang Xiong",
      "Wen Fang",
      "Mingqing Liu",
      "Hao Deng",
      "Bin He",
      "Gang Li",
      "Qingwen Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Interpreting_Object-level_Foundation_Models_via_Visual_Precision_Search_CVPR_2025_paper.html": {
    "title": "Interpreting Object-level Foundation Models via Visual Precision Search",
    "volume": "main",
    "abstract": "Advances in multimodal pre-training have propelled object-level foundation models, such as Grounding DINO and Florence-2, in tasks like visual grounding and object detection. However, interpreting these models' decisions has grown increasingly challenging. Existing interpretable attribution methods for object-level task interpretation have notable limitations: (1) gradient-based methods lack precise localization due to visual-textual fusion in foundation models, and (2) perturbation-based methods produce noisy saliency maps, limiting fine-grained interpretability. To address these, we propose a Visual Precision Search method that generates accurate attribution maps with fewer regions. Our method bypasses internal model parameters to overcome attribution issues from multimodal fusion, dividing inputs into sparse sub-regions and using consistency and collaboration scores to accurately identify critical decision-making regions. We also conducted a theoretical analysis of the boundary guarantees and scope of applicability of our method. Experiments on RefCOCO, MS COCO, and LVIS show our approach enhances object-level task interpretability over SOTA for Grounding DINO and Florence-2 across various evaluation metrics, with faithfulness gains of 23.7%, 31.6%, and 20.1% on MS COCO, LVIS, and RefCOCO for Grounding DINO, and 50.7% and 66.9% on MS COCO and RefCOCO for Florence-2. Additionally, our method can interpret failures in visual grounding and object detection tasks, surpassing existing methods across multiple evaluation metrics. The code is released at https://github.com/RuoyuChen10/VPS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruoyu Chen",
      "Siyuan Liang",
      "Jingzhi Li",
      "Shiming Liu",
      "Maosen Li",
      "Zhen Huang",
      "Hua Zhang",
      "Xiaochun Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mo_Foley-Flow_Coordinated_Video-to-Audio_Generation_with_Masked_Audio-Visual_Alignment_and_Dynamic_CVPR_2025_paper.html": {
    "title": "Foley-Flow: Coordinated Video-to-Audio Generation with Masked Audio-Visual Alignment and Dynamic Conditional Flows",
    "volume": "main",
    "abstract": "Coordinated audio generation based on video inputs typically requires a strict audio-visual (AV) alignment, where both semantics and rhythmics of the generated audio segments shall correspond to those in the video frames. Previous studies leverage a two-stage design where the AV encoders are firstly aligned via contrastive learning, then the encoded video representations guide the audio generation process. We observe that both contrastive learning and global video guidance are effective in aligning overall AV semantics while limiting temporally rhythmic synchronization. In this work, we propose Foley-Flow to first align unimodal AV encoders via masked modeling training, where the masked audio segments are recovered under the guidance of the corresponding video segments. After training, the AV encoders which are separately pretrained using only unimodal data are aligned with semantic and rhythmic consistency. Then, we develop a dynamic conditional flow for the final audio generation. Built upon the efficient velocity flow generation framework, our dynamic conditional flow utilizes temporally varying video features as the dynamic condition to guide corresponding audio segment generations. To this end, we extract coherent semantic and rhythmic representations during masked AV alignment, and use this representation of video segments to guide audio generation temporally. Our audio results are evaluated on the standard benchmarks and largely surpass existing results under several metrics. The superior performance indicates that Foley-Flow is effective in generating coordinated audios that are both semantically and rhythmically coherent to various video sequences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shentong Mo",
      "Yibing Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_LION-FS_Fast__Slow_Video-Language_Thinker_as_Online_Video_Assistant_CVPR_2025_paper.html": {
    "title": "LION-FS: Fast & Slow Video-Language Thinker as Online Video Assistant",
    "volume": "main",
    "abstract": "First-person video assistants are highly anticipated to enhance our daily life through online video dialogue. However, existing online video assistants often sacrifice assistant efficacy for real-time efficiency by processing low-frame-rate videos with coarse-grained visual features. To overcome the trade-off between efficacy and efficiency, we propose \"**F**ast & **S**low Video-Language Thinker\" as on**LI**ne vide**O** assista**N**t, **LION-FS**, achieving real-time, proactive, temporally accurate, and contextually precise responses. LION-FS adopts a two-stage optimization strategy: **1) Fast Path: Routing-Based Response Determination** evaluates frame-by-frame whether a immediate response is necessary. To enhance responses determination accuracy and handle higher frame-rate inputs efficiently, we employ Token Aggregation Routing to dynamically fuse spatiotemporal features without increasing token numbers, while utilizing Token Dropping Routing to eliminate redundant features, and **2) Slow Path: Multi-granularity Keyframe Augmentation** optimizes keyframes during response generation. To provide comprehensive and detailed responses beyond atomic actions constrained by training data, fine-grained spatial features and human-environment interaction features are extracted through multi-granular pooling. They are further integrated into a meticulously designed multimodal Thinking Template to guide more precise response generation. Comprehensive evaluations on online video tasks demonstrate that LION-FS achieves state-of-the-art efficacy and efficiency. The codes will be released soon",
    "checked": true,
    "id": "07ce01c89c54726b4dba3ca4fa770f01e88452dc",
    "semantic_title": "lion-fs: fast & slow video-language thinker as online video assistant",
    "citation_count": 11,
    "authors": [
      "Wei Li",
      "Bing Hu",
      "Rui Shao",
      "Leyang Shen",
      "Liqiang Nie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_CARE_Transformer_Mobile-Friendly_Linear_Visual_Transformer_via_Decoupled_Dual_Interaction_CVPR_2025_paper.html": {
    "title": "CARE Transformer: Mobile-Friendly Linear Visual Transformer via Decoupled Dual Interaction",
    "volume": "main",
    "abstract": "Recently, large efforts have been made to design efficient linear-complexity visual Transformers. However, current linear attention models are generally unsuitable to be deployed in resource-constrained mobile devices, due to suffering from either few efficiency gains or significant accuracy drops. In this paper, we propose a new deCoupled duAl-interactive lineaR attEntion (CARE) mechanism, revealing that features' decoupling and interaction can fully unleash the power of linear attention. We first propose an asymmetrical feature decoupling strategy that asymmetrically decouples the learning process for local inductive bias and long-range dependencies, thereby preserving sufficient local and global information while effectively enhancing the efficiency of models. Then, a dynamic memory unit is employed to maintain critical information along the network pipeline. Moreover, we design a dual interaction module to effectively facilitate interaction between local inductive bias and long-range information as well as among features at different layers. By adopting a decoupled learning way and fully exploiting complementarity across features, our method can achieve both high efficiency and accuracy. Extensive experiments on ImageNet-1K, COCO, and ADE20K datasets demonstrate the effectiveness of our approach, e.g., achieving 78.4/82.1% top-1 accuracy on ImagegNet-1K at the cost of only 0.7/1.9 GMACs. Codes will be released on github",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Zhou",
      "Qingshan Xu",
      "Jiequan Cui",
      "Junbao Zhou",
      "Jing Zhang",
      "Richang Hong",
      "Hanwang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wasserman_Paint_by_Inpaint_Learning_to_Add_Image_Objects_by_Removing_CVPR_2025_paper.html": {
    "title": "Paint by Inpaint: Learning to Add Image Objects by Removing Them First",
    "volume": "main",
    "abstract": "Image editing has advanced significantly with the introduction of text-conditioned diffusion models. Despite this progress, seamlessly adding objects to images based on textual instructions without requiring user-provided input masks remains a challenge. We address this by leveraging the insight that removing objects (Inpaint) is significantly simpler than its inverse process of adding them (Paint), attributed to inpainting models that benefit from segmentation mask guidance. Capitalizing on this realization, by implementing an automated and extensive pipeline, we curate a filtered large-scale image dataset containing pairs of images and their corresponding object-removed versions. Using these pairs, we train a diffusion model to inverse the inpainting process, effectively adding objects into images. Unlike other editing datasets, ours features natural target images instead of synthetic ones while ensuring source-target consistency by construction. Additionally, we utilize a large Vision-Language Model to provide detailed descriptions of the removed objects and a Large Language Model to convert these descriptions into diverse, natural-language instructions. Our quantitative and qualitative results show that the trained model surpasses existing models in both object addition and general editing tasks. Visit our project page for the released dataset and trained models: https://rotsteinnoam.github.io/Paint-by-Inpaint/",
    "checked": true,
    "id": "92754021a0f4836cd7985ba98eeb72eb96c4c2b7",
    "semantic_title": "paint by inpaint: learning to add image objects by removing them first",
    "citation_count": 22,
    "authors": [
      "Navve Wasserman",
      "Noam Rotstein",
      "Roy Ganz",
      "Ron Kimmel"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_Motion-Grounded_Video_Reasoning_Understanding_and_Perceiving_Motion_at_Pixel_Level_CVPR_2025_paper.html": {
    "title": "Motion-Grounded Video Reasoning: Understanding and Perceiving Motion at Pixel Level",
    "volume": "main",
    "abstract": "In this paper, we introduce Motion-Grounded Video Reasoning, a new motionunderstanding task that requires generating visual answers (video segmentationmasks) according to the input question, and hence needs implicit spatiotemporalreasoning and grounding. This task extends existing spatiotemporal groundingwork focusing on explicit action/motion grounding, to a more general format byenabling implicit reasoning via questions. To facilitate the development of the newtask, we collect a large-scale dataset called GROUNDMORE, which comprises1,715 video clips, 249K object masks that are deliberately designed with 4 questiontypes (Causal, Sequential, Counterfactual, and Descriptive) for benchmarkingdeep and comprehensive motion reasoning abilities. GROUNDMORE uniquelyrequires models to generate visual answers, providing a more concrete and visuallyinterpretable response than plain texts. It evaluates models on both spatiotemporalgrounding and reasoning, fostering to address complex challenges in motion-relatedvideo reasoning, temporal perception, and pixel-level understanding. Furthermore,we introduce a novel baseline model named Motion-Grounded Video ReasoningAssistant (MORA). MORA incorporates the multimodal reasoning ability from theMultimodal LLM, the pixel-level perception capability from the grounding model(SAM), and the temporal perception ability from a lightweight localization head.MORA achieves respectable performance on GROUNDMORE outperforming thebest existing visual grounding baseline model by an average of 21.5% relatively.We hope this novel and challenging task will pave the way for future advancementsin robust and general motion understanding via video reasoning segmentation",
    "checked": true,
    "id": "9d90fde1e08d652f180c39fdfbce5ed835e985ab",
    "semantic_title": "motion-grounded video reasoning: understanding and perceiving motion at pixel level",
    "citation_count": 5,
    "authors": [
      "Andong Deng",
      "Tongjia Chen",
      "Shoubin Yu",
      "Taojiannan Yang",
      "Lincoln Spencer",
      "Yapeng Tian",
      "Ajmal Saeed Mian",
      "Mohit Bansal",
      "Chen Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zha_PMA_Towards_Parameter-Efficient_Point_Cloud_Understanding_via_Point_Mamba_Adapter_CVPR_2025_paper.html": {
    "title": "PMA: Towards Parameter-Efficient Point Cloud Understanding via Point Mamba Adapter",
    "volume": "main",
    "abstract": "Applying pre-trained models to assist point cloud understanding has recently become a mainstream paradigm in 3D perception. However, existing application strategies are straightforward, utilizing only the final output of the pre-trained model for various task heads. It neglects the rich complementary information in the intermediate layer, thereby failing to fully unlock the potential of pre-trained models. To overcome this limitation, we propose an orthogonal solution: Point Mamba Adapter (PMA), which constructs an ordered feature sequence from all layers of the pre-trained model and leverages Mamba to fuse all complementary semantics, thereby promoting comprehensive point cloud understanding. Constructing this ordered sequence is non-trivial due to the inherent isotropy of 3D space. Therefore, we further propose a geometry-constrained gate prompt generator (G2PG) shared across different layers, which applies shared geometric constraints to the output gates of the Mamba and dynamically optimizes the spatial order, thus enabling more effective integration of multi-layer information. Extensive experiments conducted on challenging point cloud datasets across various tasks demonstrate that our PMA elevates the capability for point cloud understanding to a new level by fusing diverse complementary intermediate features. Code is available at https://github.com/zyh16143998882/PMA",
    "checked": true,
    "id": "b1f6b9c1d6419e79cf41aa816b6601516ffb09f8",
    "semantic_title": "pma: towards parameter-efficient point cloud understanding via point mamba adapter",
    "citation_count": 1,
    "authors": [
      "Yaohua Zha",
      "Yanzi Wang",
      "Hang Guo",
      "Jinpeng Wang",
      "Tao Dai",
      "Bin Chen",
      "Zhihao Ouyang",
      "Xue Yuerong",
      "Ke Chen",
      "Shu-Tao Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_All-directional_Disparity_Estimation_for_Real-world_QPD_Images_CVPR_2025_paper.html": {
    "title": "All-directional Disparity Estimation for Real-world QPD Images",
    "volume": "main",
    "abstract": "Quad Photodiode (QPD) sensors represent an evolution by providing four sub-views, whereas dual-pixel (DP) sensors are limited to two sub-views. In addition to enhancing auto-focus performance, QPD sensors also enable disparity estimation in horizontal and vertical directions. However, the characteristics of QPD sensors, including uneven illumination across sub-views and the narrow baseline, render algorithm design difficult. Furthermore, effectively utilizing the two-directional disparity of QPD sensors remains a challenge. The scarcity of QPD disparity datasets also limits the development of learning-based methods. In this work, we address these challenges by first proposing a DPNet for DP disparity estimation. Specifically, we design an illumination-invariant module to reduce the impact of illumination, followed by a coarse-to-fine module to estimate sub-pixel disparity. Building upon the DPNet, we further propose a QuadNet, which integrates the two-directional disparity via an edge-aware fusion module. To facilitate the evaluation of our approaches, we propose the first QPD disparity dataset QPD2K, comprising 2,100 real-world QPD images and corresponding disparity maps. Experiments demonstrate that our approaches achieve state-of-the-art performance in DP and QPD disparity estimation",
    "checked": true,
    "id": "9e6ed4dfa7df6d36e4ac947e39c77cb4a3d5c55c",
    "semantic_title": "all-directional disparity estimation for real-world qpd images",
    "citation_count": 0,
    "authors": [
      "Hongtao Yu",
      "Shaohui Song",
      "Lihu Sun",
      "Wenkai Su",
      "Xiaodong Yang",
      "Chengming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jeong_LC-Mamba_Local_and_Continuous_Mamba_with_Shifted_Windows_for_Frame_CVPR_2025_paper.html": {
    "title": "LC-Mamba: Local and Continuous Mamba with Shifted Windows for Frame Interpolation",
    "volume": "main",
    "abstract": "In this paper, we propose LC-Mamba, a Mamba-based model that captures fine-grained spatiotemporal information in video frames, addressing limitations in current interpolation methods and enhancing performance. The main contributions are as follows: First, we apply a shifted local window technique to reduce historical decay and enhance local spatial features, allowing multiscale capture of detailed motion between frames. Second, we introduce a Hilbert curve-based selective state scan to maintain continuity across window boundaries, preserving spatial correlations both within and between windows. Third, we extend the Hilbert curve to enable voxel-level scanning to effectively capture spatiotemporal characteristics between frames. The proposed LC-Mamba achieves competitive results, with a PSNR of 36.53 dB on Vimeo-90k, outperforming prior models by +0.03 dB. The code and models are publicly available at https://github.com/Miinuuu/LC-Mamba.git",
    "checked": true,
    "id": "85b28ac1b20b9bc7b6a61f96356dcbd951709778",
    "semantic_title": "lc-mamba: local and continuous mamba with shifted windows for frame interpolation",
    "citation_count": 0,
    "authors": [
      "Min Wu Jeong",
      "Chae Eun Rhee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kang_Zero-Shot_Head_Swapping_in_Real-World_Scenarios_CVPR_2025_paper.html": {
    "title": "Zero-Shot Head Swapping in Real-World Scenarios",
    "volume": "main",
    "abstract": "With growing demand in media and social networks for personalized images, the need for advanced head-swapping techniques--integrating an entire head from the head image with the body from the body image--has increased. However, traditional head-swapping methods heavily rely on face-centered cropped data with primarily frontal-facing views, which limits their effectiveness in real-world applications. Additionally, their masking methods, designed to indicate regions requiring editing, are optimized for these types of dataset but struggle to achieve seamless blending in complex situations, such as when the original data includes features like long hair extending beyond the masked area. To overcome these limitations and enhance adaptability in diverse and complex scenarios, we propose a novel head swapping method, HID, that is robust to images including the full head and the upper body, and handles from frontal to side views, while automatically generating context-aware masks. For automatic mask generation, we introduce the IOMask, which enables seamless blending of the head and body, effectively addressing integration challenges. We further introduce the hair injection module to capture hair details with greater precision. Our experiments demonstrate that the proposed approach achieves state-of-the-art performance in head swapping, providing visually consistent and realistic results across a wide range of challenging conditions",
    "checked": true,
    "id": "807a0c3d301f4d00355cd9fc33224d611ac3c0cb",
    "semantic_title": "zero-shot head swapping in real-world scenarios",
    "citation_count": 0,
    "authors": [
      "Taewoong Kang",
      "Sohyun Jeong",
      "Hyojin Jang",
      "Jaegul Choo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ouasfi_Toward_Robust_Neural_Reconstruction_from_Sparse_Point_Sets_CVPR_2025_paper.html": {
    "title": "Toward Robust Neural Reconstruction from Sparse Point Sets",
    "volume": "main",
    "abstract": "We consider the challenging problem of learning Signed Distance Functions (SDF) from sparse and noisy 3D point clouds. In contrast to recent methods that depend on smoothness priors, our method, rooted in a distributionally robust optimization (DRO) framework, incorporates a regularization term that leverages samples from the uncertainty regions of the model to improve the learned SDFs. Thanks to tractable dual formulations, we show that this framework enables a stable and efficient optimization of SDFs in the absence of ground truth supervision. Using a variety of synthetic and real data evaluations from different modalities, we show that of our DRO based learning framework can improve SDF learning with respect to baselines and the state-of-the-art",
    "checked": true,
    "id": "b5d06caad47614dbbd6b33d2bcf23436da5c32f8",
    "semantic_title": "toward robust neural reconstruction from sparse point sets",
    "citation_count": 2,
    "authors": [
      "Amine Ouasfi",
      "Shubhendu Jena",
      "Eric Marchand",
      "Adnane Boukhayma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_GPAvatar_High-fidelity_Head_Avatars_by_Learning_Efficient_Gaussian_Projections_CVPR_2025_paper.html": {
    "title": "GPAvatar: High-fidelity Head Avatars by Learning Efficient Gaussian Projections",
    "volume": "main",
    "abstract": "Existing radiance field-based head avatar methods have mostly relied on pre-computed explicit priors (e.g., mesh, point) or neural implicit representations, making it challenging to achieve high fidelity with both computational efficiency and low memory consumption. To overcome this, we present GPAvatar, a novel and efficient Gaussian splatting-based method for reconstructing high-fidelity dynamic 3D head avatars from monocular videos. We extend Gaussians in 3D space to a high-dimensional embedding space encompassing Gaussian's spatial position and avatar expression, enabling the representation of the head avatar with arbitrary pose and expression. To enable splatting-based rasterization, a linear transformation is learned to project each high-dimensional Gaussian back to the 3D space, which is sufficient to capture expression variations instead of using complex neural networks. Furthermore, we propose an adaptive densification strategy that dynamically allocates Gaussians to regions with high expression variance, improving the facial detail representation. Experimental results on three datasets show that our method outperforms existing state-of-the-art methods in rendering quality and speed while reducing memory usage in training and rendering",
    "checked": true,
    "id": "1a31d4c2d0bcefb55f106126d6d23f3571e197df",
    "semantic_title": "gpavatar: high-fidelity head avatars by learning efficient gaussian projections",
    "citation_count": 1,
    "authors": [
      "Wei-Qi Feng",
      "Dong Han",
      "Ze-Kang Zhou",
      "Shunkai Li",
      "Xiaoqiang Liu",
      "Pengfei Wan",
      "Di Zhang",
      "Miao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_PIAD_Pose_and_Illumination_agnostic_Anomaly_Detection_CVPR_2025_paper.html": {
    "title": "PIAD: Pose and Illumination agnostic Anomaly Detection",
    "volume": "main",
    "abstract": "We introduce the Pose and Illumination agnostic Anomaly Detection (PIAD) problem, a generalization of pose-agnostic anomaly detection (PAD). Being illumination agnostic is critical, as it relaxes the assumption that training data for an object has to be acquired in the same light configuration of the query images that we want to test. Moreover, even if the object is placed within the same capture environment, being illumination agnostic implies that we can relax the assumption that the relative pose between environment light and query object has to match the one in the training data. We introduce a new dataset to study this problem, containing both synthetic and real-world examples, propose a new baseline for PIAD, and demonstrate how our baseline provides state-of-the-art results in both PAD and PIAD, not only in the new proposed dataset, but also in existing datasets that were designed for the simpler PAD problem. Project page: https://kaichen-yang.github.io/piad/",
    "checked": true,
    "id": "50543c7cd80825b1e8240cffc856ea9c7be2f04f",
    "semantic_title": "piad: pose and illumination agnostic anomaly detection",
    "citation_count": 0,
    "authors": [
      "Kaichen Yang",
      "Junjie Cao",
      "Zeyu Bai",
      "Zhixun Su",
      "Andrea Tagliasacchi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Araujo_CAV-MAE_Sync_Improving_Contrastive_Audio-Visual_Mask_Autoencoders_via_Fine-Grained_Alignment_CVPR_2025_paper.html": {
    "title": "CAV-MAE Sync: Improving Contrastive Audio-Visual Mask Autoencoders via Fine-Grained Alignment",
    "volume": "main",
    "abstract": "Recent advances in audio-visual learning have shown promising results in learning representations across modalities. However, most approaches rely on global audio representations that fail to capture fine-grained temporal correspondences with visual frames.Additionally, existing methods often struggle with conflicting optimization objectives when trying to jointly learn reconstruction and cross-modal alignment. In this work, we propose CAV-MAE Sync as a simple yet effective extension of the original CAV-MAE framework for self-supervised audio-visual learning. We address three key challenges: First, we tackle the granularity mismatch between modalities by treating audio as a temporal sequence aligned with video frames, rather than using global representations. Second, we resolve conflicting optimization goals by separating contrastive and reconstruction objectives through dedicated global tokens. Third, we improve spatial localization by introducing learnable register tokens that reduce semantic load on patch tokens. We evaluate the proposed approach on AudioSet, VGG Sound, and the ADE20K Sound dataset on zero-shot retrieval, classification and localization tasks demonstrating state-of-the-art performance and outperforming more complex architectures. Code available at https://github.com/edsonroteia/cav-mae-sync",
    "checked": true,
    "id": "7a0a5f7567fcd3069344e5474ca9c8fe562be96b",
    "semantic_title": "cav-mae sync: improving contrastive audio-visual mask autoencoders via fine-grained alignment",
    "citation_count": 0,
    "authors": [
      "Edson Araujo",
      "Andrew Rouditchenko",
      "Yuan Gong",
      "Saurabhchand Bhati",
      "Samuel Thomas",
      "Brian Kingsbury",
      "Leonid Karlinsky",
      "Rogerio Feris",
      "James R. Glass",
      "Hilde Kuehne"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jung_Two_is_Better_than_One__Efficient_Ensemble_Defense_for_CVPR_2025_paper.html": {
    "title": "Two is Better than One: Efficient Ensemble Defense for Robust and Compact Models",
    "volume": "main",
    "abstract": "Deep learning-based computer vision systems adopt complex and large architectures to improve performance, yet they face challenges in deployment on resource-constrained mobile and edge devices. To address this issue, model compression techniques such as pruning, quantization, and matrix factorization have been proposed; however, these compressed models are often highly vulnerable to adversarial attacks. We introduce the Efficient Ensemble Defense (EED) technique, which diversifies the compression of a single base model based on different pruning importance scores and enhances ensemble diversity to achieve high adversarial robustness and resource efficiency. EED dynamically determines the number of necessary sub-models during the inference stage, minimizing unnecessary computations while maintaining high robustness. On the CIFAR-10 and SVHN datasets, EED demonstrated state-of-the-art robustness performance compared to existing adversarial pruning techniques, along with an inference speed improvement of up to 1.86 times. This proves that EED is a powerful defense solution in resource-constrained environments",
    "checked": true,
    "id": "f44c5ddfcfd382af083e1e6e194fd6b41cfa78ee",
    "semantic_title": "two is better than one: efficient ensemble defense for robust and compact models",
    "citation_count": 0,
    "authors": [
      "Yoojin Jung",
      "Byung Cheol Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Madar_Tiled_Diffusion_CVPR_2025_paper.html": {
    "title": "Tiled Diffusion",
    "volume": "main",
    "abstract": "Image tiling--the seamless connection of disparate images to create a coherent visual field--is crucial for applications such as texture creation, video game asset development, and digital art. Traditionally, tiles have been constructed manually, a method that poses significant limitations in scalability and flexibility. Recent research has attempted to automate this process using generative models. However, current approaches primarily focus on tiling textures and manipulating models for single-image generation, without inherently supporting the creation of multiple interconnected tiles across diverse domains.This paper presents Tiled Diffusion, a novel approach that extends the capabilities of diffusion models to accommodate the generation of cohesive tiling patterns across various domains of image synthesis that require tiling. Our method supports a wide range of tiling scenarios, from self-tiling to complex many-to-many connections, enabling seamless integration of multiple images.Tiled Diffusion automates the tiling process, eliminating the need for manual intervention and enhancing creative possibilities in various applications, such as seamlessly tiling of existing images, tiled texture creation, and 360deg synthesis",
    "checked": true,
    "id": "c4c009c0439c310994790b6b3720c9548756c972",
    "semantic_title": "tiled diffusion",
    "citation_count": 1,
    "authors": [
      "Or Madar",
      "Ohad Fried"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Using_Diffusion_Priors_for_Video_Amodal_Segmentation_CVPR_2025_paper.html": {
    "title": "Using Diffusion Priors for Video Amodal Segmentation",
    "volume": "main",
    "abstract": "Object permanence in humans is a fundamental cue that helps in understanding persistence of objects, even when they are fully occluded in the scene. Present day methods in object segmentation do not account for this amodal nature of the world, and only work for segmentation of visible or modal objects. Few amodal methods exist; single-image segmentation methods cannot handle high-levels of occlusions which are better inferred using temporal information, and multi-frame methods have focused solely on segmenting rigid objects. To this end, we propose to tackle video amodal segmentation by formulating it as a conditional generation task, thereby capitalizing on the foundational knowledge in video generative models. Our method is simple; we repurpose these models to condition on a sequence of modal mask frames of an object along with contextual depth maps, to learn which object boundary may be occluded and therefore, extended to hallucinate the complete extent of an object. This is followed by a content completion stage which is able to inpaint the occluded regions of an object. We benchmark our approach alongside a wide array of state-of-the-art methods on four datasets and show a dramatic improvement of upto 13% for amodal segmentation in an object's occluded region",
    "checked": true,
    "id": "d092320c1905974b0da6952728f2686e755cd97a",
    "semantic_title": "using diffusion priors for video amodal segmentation",
    "citation_count": 5,
    "authors": [
      "Kaihua Chen",
      "Deva Ramanan",
      "Tarasha Khurana"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Das_COBRA_COmBinatorial_Retrieval_Augmentation_for_Few-Shot_Adaptation_CVPR_2025_paper.html": {
    "title": "COBRA: COmBinatorial Retrieval Augmentation for Few-Shot Adaptation",
    "volume": "main",
    "abstract": "Retrieval augmentation, the practice of retrieving additional data from large auxiliary pools, has emerged as an effective technique for enhancing model performance in the low-data regime. Prior approaches have employed only nearest-neighbor based strategies for data selection, which retrieve auxiliary samples with high similarity to instances in the target task. However, these approaches are prone to selecting highly redundant samples, since they fail to incorporate any notion of diversity. In our work, we first demonstrate that data selection strategies used in prior retrieval-augmented few-shot adaptation settings can be generalized using a class of functions known as Combinatorial Mutual Information (CMI) measures. We then propose COBRA (COmBinatorial Retrieval Augmentation), which employs an alternative CMI measure that considers both diversity and similarity to a target dataset. COBRA consistently outperforms previous retrieval approaches across image classification tasks and few-shot learning techniques when used to retrieve samples from LAION-2B. COBRA introduces negligible computational overhead to the cost of retrieval while providing significant gains in downstream model performance",
    "checked": true,
    "id": "de6a3624a5e22cc72b6d65db6cbb706129282ec5",
    "semantic_title": "cobra: combinatorial retrieval augmentation for few-shot adaptation",
    "citation_count": 0,
    "authors": [
      "Arnav M. Das",
      "Gantavya Bhatt",
      "Lilly Kumari",
      "Sahil Verma",
      "Jeff Bilmes"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Dyn-HaMR_Recovering_4D_Interacting_Hand_Motion_from_a_Dynamic_Camera_CVPR_2025_paper.html": {
    "title": "Dyn-HaMR: Recovering 4D Interacting Hand Motion from a Dynamic Camera",
    "volume": "main",
    "abstract": "We propose Dyn-HaMR, to the best of our knowledge, the first approach to reconstruct 4D global hand motion from monocular videos recorded by dynamic cameras in the wild. Reconstructing accurate 3D hand meshes from monocular videos is a crucial task for understanding human behaviour, with significant applications in augmented and virtual reality (AR/VR). However, existing methods for monocular hand reconstruction typically rely on a weak perspective camera model, which simulates hand motion within a limited camera frustum. As a result, these approaches struggle to recover the full 3D global trajectory and often produce noisy or incorrect depth estimations, particularly when the video is captured by dynamic or moving cameras, which is common in egocentric scenarios. Our \\name consists of a multi-stage, multi-objective optimization pipeline, that factors in (i) simultaneous localization and mapping (SLAM) to robustly estimate relative camera motion, (ii) an interacting-hand prior for generative infilling and to refine the interaction dynamics, ensuring plausible recovery under (self-)occlusions, and (iii) hierarchical initialization through a combination of state-of-the-art hand tracking methods",
    "checked": true,
    "id": "9b105495188c96e9e89a2091465eaae3c27baa24",
    "semantic_title": "dyn-hamr: recovering 4d interacting hand motion from a dynamic camera",
    "citation_count": 4,
    "authors": [
      "Zhengdi Yu",
      "Stefanos Zafeiriou",
      "Tolga Birdal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_The_Scene_Language_Representing_Scenes_with_Programs_Words_and_Embeddings_CVPR_2025_paper.html": {
    "title": "The Scene Language: Representing Scenes with Programs, Words, and Embeddings",
    "volume": "main",
    "abstract": "We introduce the Scene Language, a visual scene representation that concisely and precisely describes the structure, semantics, and identity of visual scenes. It represents a scene with three key components: a program that specifies the hierarchical and relational structure of entities in the scene, words in natural language that summarize the semantic class of each entity, and embeddings that capture the visual identity of each entity. This representation can be inferred from pre-trained language models via a training-free inference technique, given text or image inputs. The resulting scene can be rendered into images using traditional, neural, or hybrid graphics renderers. Together, this forms an automated system for high-quality 3D and 4D scene generation. Compared with existing representations like scene graphs, our proposed Scene Language generates complex scenes with higher fidelity, while explicitly modeling the scene structures to enable precise control and editing",
    "checked": true,
    "id": "a2ea9af521c7a100f13270040703f205be6fec2a",
    "semantic_title": "the scene language: representing scenes with programs, words, and embeddings",
    "citation_count": 8,
    "authors": [
      "Yunzhi Zhang",
      "Zizhang Li",
      "Matt Zhou",
      "Shangzhe Wu",
      "Jiajun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Toussaint_ProbeSDF_Light_Field_Probes_For_Neural_Surface_Reconstruction_CVPR_2025_paper.html": {
    "title": "ProbeSDF: Light Field Probes For Neural Surface Reconstruction",
    "volume": "main",
    "abstract": "SDF-based differential rendering frameworks have achieved state-of-the-art multiview 3D shape reconstruction. In this work, we re-examine this family of approaches by minimally reformulating its core appearance model in a way that simultaneously yields faster computation and increased performance. To this goal, we exhibit a physically-inspired minimal radiance parametrization decoupling angular and spatial contributions, by encoding them with a small number of features stored in two respective volumetric grids of different resolutions. Requiring as little as four parameters per voxel, and a tiny MLP call inside a single fully fused kernel, our approach allows to enhance performance with both surface and image (PSNR) metrics, while providing a significant training speedup and real-time rendering. We show this performance to be consistently achieved on real data over two widely different and popular application fields, generic object and human subject shape reconstruction, using four representative and challenging datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Briac Toussaint",
      "Diego Thomas",
      "Jean-S√©bastien Franco"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bose_Descriptor-In-Pixel__Point-Feature_Tracking_For_Pixel_Processor_Arrays_CVPR_2025_paper.html": {
    "title": "Descriptor-In-Pixel : Point-Feature Tracking For Pixel Processor Arrays",
    "volume": "main",
    "abstract": "This paper presents a novel approach for joint point-feature detection and tracking, designed specifically for Pixel Processor Array (PPA) vision sensors. Instead of standard pixels, PPA sensors consist of thousands of \"pixel-processors\", enabling massive parallel computation of visual data at the point of light capture. Our approach performs all computation entirely in-pixel, meaning no raw image data need ever leave the sensor for external processing. We introduce a Descriptor-In-Pixel paradigm, in which a feature descriptor is held within the memory of each pixel-processor. The PPA's architecture enables the response of every processor's descriptor, upon the current image, to be computed in parallel. This produces a \"descriptor response map\", which, by generating the correct layout of descriptors across the pixel-processors, can be used for both point-feature detection and tracking. This reduces sensor output to just sparse feature locations and descriptors, read-out via an address-event interface, giving a greater than 1000X reduction in data transfer compared to raw image output. The sparse readout and complete utilization of all pixel-processors makes our approach very efficient. Our implementation upon the SCAMP-7 PPA prototype runs at over 3000 FPS (Frames Per Second), tracking point-features reliably under violent motion. This is the first work performing point-feature detection and tracking entirely in-pixel",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Laurie Bose",
      "Jianing Chen",
      "Piotr Dudek"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Hybrid_Concept_Bottleneck_Models_CVPR_2025_paper.html": {
    "title": "Hybrid Concept Bottleneck Models",
    "volume": "main",
    "abstract": "Concept Bottleneck Models (CBMs) provide an interpretable framework for neural networks by mapping visual features to predefined, human-understandable concepts. However, the application of CBMs is often constrained by insufficient concept annotations. Recently, multi-modal pre-trained models have shown promise in reducing annotation costs by aligning visual representations with textual concept embeddings. Nevertheless, the quality and completeness of the predefined concepts significantly affect the performance of CBMs.In this work, we propose Hybrid Concept Bottleneck Model (HybridCBM), a novel CBM framework to address the challenge of incomplete predefined concepts. Our method consists of two main components: a Static Concept Bank and a Dynamic Concept Bank. The Static Concept Bank directly leverages large language models (LLMs) for concept construction, while the Dynamic Concept Bank employs learnable vectors to capture complementary and valuable concepts continuously during training. After training, a pre-trained translator converts these vectors into human-understandable concepts, further enhancing model interpretability. Notably, HybridCBM is highly flexible and can be easily applied to any CBM to improve performance. Experimental results across multiple datasets demonstrate that HybridCBM outperforms current state-of-the-art methods and achieves comparable results to black-box models. Additionally, we propose novel metrics to evaluate the quality of the learned concepts, showing that they perform comparably to predefined concepts",
    "checked": true,
    "id": "920aedb337c063a402721af6fb0a2063b0fb0540",
    "semantic_title": "hybrid concept bottleneck models",
    "citation_count": 0,
    "authors": [
      "Yang Liu",
      "Tianwei Zhang",
      "Shi Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rai_UVGS_Reimagining_Unstructured_3D_Gaussian_Splatting_using_UV_Mapping_CVPR_2025_paper.html": {
    "title": "UVGS: Reimagining Unstructured 3D Gaussian Splatting using UV Mapping",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) has demonstrated superior quality in modeling 3D objects and scenes. However, generating 3DGS remains challenging due to their discrete, unstructured, and permutation-invariant nature. In this work, we present a simple yet effective method to overcome these challenges. We utilize spherical mapping to transform 3DGS into a structured 2D representation, termed UVGS. UVGS can be viewed as multi-channel images, with feature dimensions as a concatenation of Gaussian attributes such as position, scale, color, opacity, and rotation. We further find that these heterogeneous features can be compressed into a lower-dimensional (e.g., 3-channel) shared feature space using a carefully designed multi-branch network.The compressed UVGS can be treated as typical RGB images. Remarkably, we discover that typical VAEs trained with latent diffusion models can directly generalize to this new representation without additional training. Our novel representation makes it effortless to leverage foundational 2D models, such as diffusion models, to directly model 3DGS. Additionally, one can simply increase the 2D UV resolution to accommodate more Gaussians, making UVGS a scalable solution compared to typical 3D backbones. This approach immediately unlocks various novel generation applications of 3DGS by inherently utilizing the already developed superior 2D generation capabilities. In our experiments, we demonstrate various unconditional, conditional generation, and inpainting applications of 3DGS based on diffusion models, which were previously non-trivial",
    "checked": true,
    "id": "10f0d88160981f85fd6c270e65496ec5948a98d2",
    "semantic_title": "uvgs: reimagining unstructured 3d gaussian splatting using uv mapping",
    "citation_count": 3,
    "authors": [
      "Aashish Rai",
      "Dilin Wang",
      "Mihir Jain",
      "Nikolaos Sarafianos",
      "Kefan Chen",
      "Srinath Sridhar",
      "Aayush Prakash"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Dual_Consolidation_for_Pre-Trained_Model-Based_Domain-Incremental_Learning_CVPR_2025_paper.html": {
    "title": "Dual Consolidation for Pre-Trained Model-Based Domain-Incremental Learning",
    "volume": "main",
    "abstract": "Domain-Incremental Learning (DIL) involves the progressive adaptation of a model to new concepts across different domains. While recent advances in pre-trained models provide a solid foundation for DIL, learning new concepts often results in the catastrophic forgetting of pre-trained knowledge. Specifically, sequential model updates can overwrite both the representation and the classifier with knowledge from the latest domain. Thus, it is crucial to develop a representation and corresponding classifier that accommodate all seen domains throughout the learning process. To this end, we propose DUal ConsolidaTion (Duct) to unify and consolidate historical knowledge at both the representation and classifier levels. By merging the backbone of different stages, we create a representation space suitable for multiple domains incrementally. The merged representation serves as a balanced intermediary that captures task-specific features from all seen domains. Additionally, to address the mismatch between consolidated embeddings and the classifier, we introduce an extra classifier consolidation process. Leveraging class-wise semantic information, we estimate the classifier weights of old domains within the latest embedding space. By merging historical and estimated classifiers, we align them with the consolidated embedding space, facilitating incremental classification. Extensive experimental results on four benchmark datasets demonstrate Duct's state-of-the-art performance. Code is available at: https://github.com/Estrella-fugaz/CVPR25-Duct",
    "checked": true,
    "id": "1ce6fea8e70435e30826a49b85595e15bb969a03",
    "semantic_title": "dual consolidation for pre-trained model-based domain-incremental learning",
    "citation_count": 4,
    "authors": [
      "Da-Wei Zhou",
      "Zi-Wen Cai",
      "Han-Jia Ye",
      "Lijun Zhang",
      "De-Chuan Zhan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Learning_Physics-Based_Full-Body_Human_Reaching_and_Grasping_from_Brief_Walking_CVPR_2025_paper.html": {
    "title": "Learning Physics-Based Full-Body Human Reaching and Grasping from Brief Walking References",
    "volume": "main",
    "abstract": "Existing motion generation methods based on mocap data are often limited by data quality and coverage. In this work, we propose a framework that generates diverse, physically feasible full-body human reaching and grasping motions using only brief walking mocap data. Base on the observation that walking data captures valuable movement patterns transferable across tasks and, on the other hand, the advanced kinematic methods can generate diverse grasping poses, which can then be interpolated into motions to serve as task-specific guidance. Our approach incorporates an active data generation strategy to maximize the utility of the generated motions, along with a local feature alignment mechanism that transfers natural movement patterns from walking data to enhance both the success rate and naturalness of the synthesized motions. By combining the fidelity and stability of natural walking with the flexibility and generalizability of task-specific generated data, our method demonstrates strong performance and robust adaptability in diverse scenes and with unseen objects",
    "checked": true,
    "id": "05317fcf8bbafb25ee3eba5892c3fa7806e45ada",
    "semantic_title": "learning physics-based full-body human reaching and grasping from brief walking references",
    "citation_count": 3,
    "authors": [
      "Yitang Li",
      "Mingxian Lin",
      "Zhuo Lin",
      "Yipeng Deng",
      "Yue Cao",
      "Li Yi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_EmoEdit_Evoking_Emotions_through_Image_Manipulation_CVPR_2025_paper.html": {
    "title": "EmoEdit: Evoking Emotions through Image Manipulation",
    "volume": "main",
    "abstract": "Affective Image Manipulation (AIM) seeks to modify user-provided images to evoke specific emotions. This task is inherently complex due to its twofold objective: evoking the intended emotion while preserving image composition. Existing AIM methods primarily adjust color and style, often failing to elicit precise, profound emotional shifts. Drawing on psychological insights, we introduce EmoEdit, which extends AIM by incorporating content modifications to enhance emotional impact. Specifically, we construct EmoEditSet, a large-scale AIM dataset of 40,120 paired data through emotion attribution and data construction. To make generative models emotion-aware, we design an Emotion Adapter and train it using EmoEditSet. We further propose an instruction loss to capture semantic variations in each data pair. Our method is evaluated both qualitatively and quantitatively, demonstrating superior performance over state-of-the-art techniques. Additionally, we showcase the portability of our Emotion Adapter to other diffusion-based models, enhancing their emotion knowledge with diverse semantics. Code is available at: https://github.com/JingyuanYY/EmoEdit",
    "checked": true,
    "id": "13d2d5f5e9d8093ca485a2171f60eb67228ea849",
    "semantic_title": "emoedit: evoking emotions through image manipulation",
    "citation_count": 5,
    "authors": [
      "Jingyuan Yang",
      "Jiawei Feng",
      "Weibin Luo",
      "Dani Lischinski",
      "Daniel Cohen-Or",
      "Hui Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_RORem_Training_a_Robust_Object_Remover_with_Human-in-the-Loop_CVPR_2025_paper.html": {
    "title": "RORem: Training a Robust Object Remover with Human-in-the-Loop",
    "volume": "main",
    "abstract": "Despite the significant advancements, existing object removal methods struggle with incomplete removal, incorrect content synthesis and blurry synthesized regions, resulting in low success rates. Such issues are mainly caused by the lack of high-quality paired training data, as well as the self-supervised training paradigm adopted in these methods, which forces the model to in-paint the masked regions, leading to ambiguity between synthesizing the masked objects and restoring the background. To address these issues, we propose a semi-supervised learning strategy with human-in-the-loop to create high-quality paired training data, aiming to train a Robust Object Remover (RORem). We first collect 60K training pairs from open-source datasets to train an initial object removal model for generating removal samples, and then utilize human feedback to select a set of high-quality object removal pairs, with which we train a discriminator to automate the following training data generation process. By iterating this process for several rounds, we finally obtain a substantial object removal dataset with over 200K pairs. Fine-tuning the pre-trained stable diffusion model with this dataset, we obtain our RORem, which demonstrates state-of-the-art object removal performance in terms of both reliability and image quality. Particularly, RORem improves the object removal success rate over previous methods by more than 18%. The dataset, source code and trained model will be released",
    "checked": true,
    "id": "bc64caa1624587ca96d4d889abf6627908bae2ec",
    "semantic_title": "rorem: training a robust object remover with human-in-the-loop",
    "citation_count": 6,
    "authors": [
      "Ruibin Li",
      "Tao Yang",
      "Song Guo",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Vayani_All_Languages_Matter_Evaluating_LMMs_on_Culturally_Diverse_100_Languages_CVPR_2025_paper.html": {
    "title": "All Languages Matter: Evaluating LMMs on Culturally Diverse 100 Languages",
    "volume": "main",
    "abstract": "Existing Large Multimodal Models (LMMs) generally focus on only a few regions and languages. As LMMs continue to improve, it is increasingly important to ensure they understand cultural contexts, respect local sensitivities, and support low-resource languages, all while effectively integrating corresponding visual cues. In pursuit of culturally diverse global multimodal models, our proposed All Languages Matter Benchmark (ALM-bench) represents the largest and most comprehensive effort to date for evaluating LMMs across 100 languages. ALM-bench challenges existing models by testing their ability to understand and reason about culturally diverse images paired with text in various languages, including many low-resource languages traditionally underrepresented in multimodal research. The benchmark offers a robust and nuanced evaluation framework featuring various question formats, including True/False, multiple choice, and open-ended questions, which are further divided into short and long-answer categories. ALM-bench design ensures a comprehensive assessment of a model's ability to handle varied levels of difficulty in visual and linguistic reasoning. To capture the rich tapestry of global cultures, ALM-bench carefully curates content from 13 distinct cultural aspects, ranging from traditions and rituals to famous personalities and celebrations. Through this, ALM-bench not only provides a rigorous testing ground for state-of-the-art open and closed-source LMMs but also highlights the importance of cultural and linguistic inclusivity, encouraging the development of models that can serve diverse global populations effectively. Our benchmark will be publicly released",
    "checked": true,
    "id": "49bb41b653495a63d416636ae7545c44ec6b59cc",
    "semantic_title": "all languages matter: evaluating lmms on culturally diverse 100 languages",
    "citation_count": 25,
    "authors": [
      "Ashmal Vayani",
      "Dinura Dissanayake",
      "Hasindri Watawana",
      "Noor Ahsan",
      "Nevasini Sasikumar",
      "Omkar Thawakar",
      "Henok Biadglign Ademtew",
      "Yahya Hmaiti",
      "Amandeep Kumar",
      "Kartik Kukreja",
      "Mykola Maslych",
      "Wafa Al Ghallabi",
      "Mihail Minkov Mihaylov",
      "Chao Qin",
      "Abdelrahman M. Shaker",
      "Mike Zhang",
      "Mahardika Krisna Ihsani",
      "Amiel Gian Esplana",
      "Monil Gokani",
      "Shachar Mirkin",
      "Harsh Singh",
      "Ashay Srivastava",
      "Endre Hamerlik",
      "Fathinah Asma Izzati",
      "Fadillah Adamsyah Maani",
      "Sebastian Cavada",
      "Jenny Chim",
      "Rohit Gupta",
      "Sanjay Manjunath",
      "Kamila Zhumakhanova",
      "Feno Heriniaina Rabevohitra",
      "Azril Hafizi Amirudin",
      "Muhammad Ridzuan",
      "Daniya Najiha Abdul Kareem",
      "Ketan Pravin More",
      "Kunyang Li",
      "Pramesh Shakya",
      "Muhammad Saad",
      "Amirpouya Ghasemaghaei",
      "Amirbek Djanibekov",
      "Dilshod Azizov",
      "Branislava Jankovic",
      "Naman Bhatia",
      "Alvaro Cabrera",
      "Johan Obando-Ceron",
      "Olympiah Otieno",
      "Febian Farestam",
      "Muztoba Rabbani",
      "Sanoojan Ballah",
      "Santosh Sanjeev",
      "Abduragim Shtanchaev",
      "Maheen Fatima",
      "Thao Nguyen",
      "Amrin Kareem",
      "Toluwani Aremu",
      "Nathan Augusto Zacarias Xavier",
      "Amit Bhatkal",
      "Hawau Olamide Toyin",
      "Aman Chadha",
      "Hisham Cholakkal",
      "Rao Muhammad Anwer",
      "Michael Felsberg",
      "Jorma Laaksonen",
      "Thamar Solorio",
      "Monojit Choudhury",
      "Ivan Laptev",
      "Mubarak Shah",
      "Salman Khan",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yuan_SparseAlign_a_Fully_Sparse_Framework_for_Cooperative_Object_Detection_CVPR_2025_paper.html": {
    "title": "SparseAlign: a Fully Sparse Framework for Cooperative Object Detection",
    "volume": "main",
    "abstract": "Cooperative perception can increase the view field and decrease the occlusion of an ego vehicle, hence improving the perception performance and safety of autonomous driving. Despite the success of previous works on cooperative object detection, they mostly operate on dense Bird's Eye View (BEV) feature maps, which are computationally demanding and can hardly be extended to long-range detection problems. More efficient fully sparse frameworks are rarely explored. In this work, we design a fully sparse framework, SparseAlign, with three key features: an enhanced sparse 3D backbone, a query-based temporal context learning module, and a robust detection head specially tailored for sparse features. Extensive experimental results on both OPV2V and DairV2X datasets show that our framework, despite its sparsity, outperforms the state of the art with less communication bandwidth requirements. In addition, experiments on the OPV2Vt and DairV2Xt datasets for time-aligned cooperative object detection also show a significant performance gain compared to the baseline works",
    "checked": true,
    "id": "fcbf6be063a28d65ee61e68fb78f127cc59e5b78",
    "semantic_title": "sparsealign: a fully sparse framework for cooperative object detection",
    "citation_count": 0,
    "authors": [
      "Yunshuang Yuan",
      "Yan Xia",
      "Daniel Cremers",
      "Monika Sester"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_Video-Bench_Human-Aligned_Video_Generation_Benchmark_CVPR_2025_paper.html": {
    "title": "Video-Bench: Human-Aligned Video Generation Benchmark",
    "volume": "main",
    "abstract": "Video generation assessment is essential for ensuring that generative models produce visually realistic, high-quality videos while aligning with human expectations. Current video generation benchmarks fall into two main categories: traditional benchmarks, which use metrics and embeddings to evaluate generated video quality across multiple dimensions but often lack alignment with human judgments; and large language model (LLM)-based benchmarks, though capable of human-like reasoning, are constrained by a limited understanding of video quality metrics and cross-modal consistency.To address these challenges and establish a benchmark that better aligns with human preferences, this paper introduces Video-Bench, a comprehensive benchmark featuring a rich prompt suite and extensive evaluation dimensions. This benchmark represents the first attempt to systematically leverage MLLMs across all dimensions relevant to video generation assessment in generative models. By incorporating few-shot scoring and chain-of-query techniques, Video-Bench provides a structured, scalable approach to generated video evaluation. Experimental results demonstrate that MLLMs achieve superior alignment with human preferences across all dimensions. Moreover, in instances where our framework's assessments diverge from human evaluations, it consistently offers more objective and accurate insights, suggesting an even greater potential advantage over traditional human judgment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Han",
      "Siyuan Li",
      "Jiaqi Chen",
      "Yiwen Yuan",
      "Yuling Wu",
      "Yufan Deng",
      "Chak Tou Leong",
      "Hanwen Du",
      "Junchen Fu",
      "Youhua Li",
      "Jie Zhang",
      "Chi Zhang",
      "Li-jia Li",
      "Yongxin Ni"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/del_Rio_Data_Distributional_Properties_As_Inductive_Bias_for_Systematic_Generalization_CVPR_2025_paper.html": {
    "title": "Data Distributional Properties As Inductive Bias for Systematic Generalization",
    "volume": "main",
    "abstract": "Deep neural networks (DNNs) struggle at systematic generalization (SG). Several studies have evaluated the possibility of promoting SG through the proposal of novel architectures, loss functions, or training methodologies. Few studies, however, have focused on the role of training data properties in promoting SG. In this work, we investigate the impact of certain data distributional properties, as inductive biases for the SG ability of a multi-modal language model. To this end, we study three different properties. First, data diversity, instantiated as an increase in the possible values a latent property in the training distribution may take. Second, burstiness, where we probabilistically restrict the number of possible values of latent factors on particular inputs during training. Third, latent intervention, where a particular latent factor is altered randomly during training. We find that all three factors significantly enhance SG, with diversity contributing an 89% absolute increase in accuracy in the most affected property. Through a series of experiments, we test various hypotheses to understand why these properties promote SG. Finally, we find that Normalized Mutual Information (NMI) between latent attributes in the training distribution is strongly predictive of out-of-distribution generalization. We find that a mechanism by which lower NMI induces SG is in the geometry of representations. In particular, we find that NMI induces more parallelism in neural representations (i.e., input features coded in parallel neural vectors) of the model, a property related to the capacity of reasoning by analogy",
    "checked": true,
    "id": "1ea04dc0667be7637cac070599fb4a7f65712e77",
    "semantic_title": "data distributional properties as inductive bias for systematic generalization",
    "citation_count": 1,
    "authors": [
      "Felipe del Rio",
      "Alain Raymond-Saez",
      "Daniel Florea",
      "Rodrigo Toro Icarte",
      "Julio Hurtado",
      "Cristian B. Calderon",
      "Alvaro Soto"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_MergeVQ_A_Unified_Framework_for_Visual_Generation_and_Representation_with_CVPR_2025_paper.html": {
    "title": "MergeVQ: A Unified Framework for Visual Generation and Representation with Disentangled Token Merging and Quantization",
    "volume": "main",
    "abstract": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great success in both self-supervised pre-training and image generation. However, most existing methods struggle to address the trade-off in the shared latent space for generation quality vs. representation learning and efficiency. To push the limits of this paradigm, we propose MergeVQ, which incorporates token merging techniques into VQ-based generative models to bridge the gap between image generation and visual representation learning in a unified architecture. During pre-training, MergeVQ decouples top-k semantics from latent space with the token merge module after self-attention blocks in the encoder for subsequent Look-up Free Quantization (LFQ) and global alignment and recovers their fine-grained details through cross-attention in the decoder for reconstruction. As for second-stage generation, we introduce MergeAR, which performs KV Cache compression for efficient raster-order prediction. Extensive experiments on ImageNet verify that MergeVQ as an AR generative model achieves competitive performance in both visual representation learning and image generation tasks while maintaining favorable token efficiency and inference speed. The code and model will be available at https://apexgen-x.github.io/MergeVQ",
    "checked": true,
    "id": "c141b15c26a97ee131105b6152b47bac4ee8e1fa",
    "semantic_title": "mergevq: a unified framework for visual generation and representation with disentangled token merging and quantization",
    "citation_count": 4,
    "authors": [
      "Siyuan Li",
      "Luyuan Zhang",
      "Zedong Wang",
      "Juanxi Tian",
      "Cheng Tan",
      "Zicheng Liu",
      "Chang Yu",
      "Qingsong Xie",
      "Haonan Lu",
      "Haoqian Wang",
      "Zhen Lei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_InterAct_Advancing_Large-Scale_Versatile_3D_Human-Object_Interaction_Generation_CVPR_2025_paper.html": {
    "title": "InterAct: Advancing Large-Scale Versatile 3D Human-Object Interaction Generation",
    "volume": "main",
    "abstract": "While large-scale human motion capture datasets have advanced human motion generation, modeling and generating dynamic 3D human-object interactions (HOIs) remain challenging due to dataset limitations. Existing datasets often lack extensive, high-quality motion and annotation and exhibit artifacts such as contact penetration, floating, and incorrect hand motions. To address these issues, we introduce InterAct, a large-scale 3D HOI benchmark featuring dataset and methodological advancements. First, we consolidate and standardize 21.81 hours of HOI data from diverse sources, enriching it with detailed textual annotations. Second, we propose a unified optimization framework to enhance data quality by reducing artifacts and correcting hand motions. Leveraging the principle of contact invariance, we maintain human-object relationships while introducing motion variations, expanding the dataset to 30.70 hours. Third, we define six benchmarking tasks and develop a unified HOI generative modeling perspective, achieving state-of-the-art performance. Extensive experiments validate the utility of our dataset as a foundational resource for advancing 3D human-object interaction generation. The dataset will be publicly accessible to support further research in the field",
    "checked": true,
    "id": "6da250eb3006fc9a067a19e6a88caccab81f7a9e",
    "semantic_title": "interact: advancing large-scale versatile 3d human-object interaction generation",
    "citation_count": 3,
    "authors": [
      "Sirui Xu",
      "Dongting Li",
      "Yucheng Zhang",
      "Xiyan Xu",
      "Qi Long",
      "Ziyin Wang",
      "Yunzhi Lu",
      "Shuchang Dong",
      "Hezi Jiang",
      "Akshat Gupta",
      "Yu-Xiong Wang",
      "Liang-Yan Gui"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_TopoCellGen_Generating_Histopathology_Cell_Topology_with_a_Diffusion_Model_CVPR_2025_paper.html": {
    "title": "TopoCellGen: Generating Histopathology Cell Topology with a Diffusion Model",
    "volume": "main",
    "abstract": "Accurately modeling multi-class cell topology is crucial in digital pathology, as it provides critical insights into tissue structure and pathology. The synthetic generation of cell topology enables realistic simulations of complex tissue environments, enhances downstream tasks by augmenting training data, aligns more closely with pathologists' domain knowledge, and offers new opportunities for controlling and generalizing the tumor microenvironment. In this paper, we propose a novel approach that integrates topological constraints into a diffusion model to improve the generation of realistic, contextually accurate cell topologies. Our method refines the simulation of cell distributions and interactions, increasing the precision and interpretability of results in downstream tasks such as cell detection and classification. To assess the topological fidelity of generated layouts, we introduce a new metric, Topological Frechet Distance (TopoFD), which overcomes the limitations of traditional metrics like FID in evaluating topological structure. Experimental results demonstrate the effectiveness of our approach in generating multi-class cell layouts that capture intricate topological relationships. Code is available at https://github.com/Melon-Xu/TopoCellGen",
    "checked": true,
    "id": "bc91981f6d438f4b9a1ac8271c018297653849a3",
    "semantic_title": "topocellgen: generating histopathology cell topology with a diffusion model",
    "citation_count": 5,
    "authors": [
      "Meilong Xu",
      "Saumya Gupta",
      "Xiaoling Hu",
      "Chen Li",
      "Shahira Abousamra",
      "Dimitris Samaras",
      "Prateek Prasanna",
      "Chao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Anyattack_Towards_Large-scale_Self-supervised_Adversarial_Attacks_on_Vision-language_Models_CVPR_2025_paper.html": {
    "title": "Anyattack: Towards Large-scale Self-supervised Adversarial Attacks on Vision-language Models",
    "volume": "main",
    "abstract": "Due to their multimodal capabilities, Vision-Language Models (VLMs) have found numerous impactful applications in real-world scenarios. However, recent studies have revealed that VLMs are vulnerable to image-based adversarial attacks. Traditional targeted adversarial attacks require specific targets and labels, limiting their real-world impact. We present AnyAttack, a self-supervised framework that transcends the limitations of conventional attacks through a novel foundation model approach. By pre-training on the massive LAION-400M dataset without label supervision, AnyAttack achieves unprecedented flexibility - enabling any image to be transformed into an attack vector targeting any desired output across different VLMs. This approach fundamentally changes the threat landscape, making adversarial capabilities accessible at an unprecedented scale. Our extensive validation across five open-source VLMs (CLIP, BLIP, BLIP2, InstructBLIP, and MiniGPT-4) demonstrates AnyAttack's effectiveness across diverse multimodal tasks. Most concerning, AnyAttack seamlessly transfers to commercial systems including Google Gemini, Claude Sonnet, Microsoft Copilot and OpenAI GPT, revealing a systemic vulnerability requiring immediate attention",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Zhang",
      "Junhong Ye",
      "Xingjun Ma",
      "Yige Li",
      "Yunfan Yang",
      "Yunhao Chen",
      "Jitao Sang",
      "Dit-Yan Yeung"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nguyen_Joint_Optimization_of_Neural_Radiance_Fields_and_Continuous_Camera_Motion_CVPR_2025_paper.html": {
    "title": "Joint Optimization of Neural Radiance Fields and Continuous Camera Motion from a Monocular Video",
    "volume": "main",
    "abstract": "Neural Radiance Fields (NeRF) has demonstrated its superior capability to represent 3D geometry but require accurately precomputed camera poses during training. To mitigate this requirement, existing methods jointly optimize camera poses and NeRF often relying on good pose initialisation or depth priors. However, these approaches struggle in challenging scenarios, such as large rotations, as they map each camera to a world coordinate system. We propose a novel method that eliminates prior dependencies by modeling continuous camera motions as time-dependent angular velocity and velocity. Relative motions between cameras are learned first via velocity integration, while camera poses can be obtained by aggregating such relative motions up to a world coordinate system defined at a single time step within the video. Specifically, accurate continuous camera movements are learned through a time-dependent NeRF, which captures local scene geometry and motion by training from neighboring frames for each time step. The learned motions enable fine-tuning the NeRF to represent the full scene geometry. Experiments on Co3D and Scannet show our approach achieves superior camera pose and depth estimation and comparable novel-view synthesis performance compared to state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoang Chuong Nguyen",
      "Wei Mao",
      "Jose M. Alvarez",
      "Miaomiao Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gu_IRGS_Inter-Reflective_Gaussian_Splatting_with_2D_Gaussian_Ray_Tracing_CVPR_2025_paper.html": {
    "title": "IRGS: Inter-Reflective Gaussian Splatting with 2D Gaussian Ray Tracing",
    "volume": "main",
    "abstract": "In inverse rendering, accurately modeling visibility and indirect radiance for incident light is essential for capturing secondary effects. Due to the absence of a powerful Gaussian ray tracer, previous 3DGS-based methods have either adopted a simplified rendering equation or used learnable parameters to approximate incident light, resulting in inaccurate material and lighting estimations. To this end, we introduce the inter-reflective Gaussian splatting (IRGS) framework for inverse rendering. To capture inter-reflection, we apply the full rendering equation without simplification and compute incident radiance on the fly using the proposed differentiable 2D Gaussian ray tracing. Additionally, we present an efficient optimization scheme to handle the computational demands of Monte Carlo sampling for rendering equation evaluation. Furthermore, we introduce a novel strategy for querying the indirect radiance of incident light when relighting the optimized scenes. Extensive experiments on multiple standard benchmarks validate the effectiveness of IRGS, demonstrating its capability to accurately model complex inter-reflection effects",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun Gu",
      "Xiaofei Wei",
      "Zixuan Zeng",
      "Yuxuan Yao",
      "Li Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_InterMimic_Towards_Universal_Whole-Body_Control_for_Physics-Based_Human-Object_Interactions_CVPR_2025_paper.html": {
    "title": "InterMimic: Towards Universal Whole-Body Control for Physics-Based Human-Object Interactions",
    "volume": "main",
    "abstract": "Achieving realistic simulations of humans interacting with a wide range of objects has long been a fundamental goal. Extending physics-based motion imitation to complex human-object interactions (HOIs) is challenging due to intricate human-object coupling, variability in object geometries, and artifacts in motion capture data, such as inaccurate contacts and limited hand detail. We introduce InterMimic, a framework that enables a single policy to robustly learn from hours of imperfect MoCap data covering diverse full-body interactions with dynamic and varied objects. Our key insight is to employ a curriculum strategy -- perfect first, then scale up. We first train subject-specific teacher policies to mimic, retarget, and refine motion capture data. Next, we distill these teachers into a student policy, with the teachers acting as online experts providing direct supervision, as well as high-quality references. Notably, we incorporate RL fine-tuning on the student policy to surpass mere demonstration replication and achieve higher-quality solutions. Our experiments demonstrate that InterMimic produces realistic and diverse interactions across multiple HOI datasets. The learned policy generalizes in a zero-shot manner and seamlessly integrates with kinematic generators, elevating the framework from mere imitation to generative modeling of complex human-object interactions",
    "checked": true,
    "id": "e595cc43d012461f5bcf929a1a6e99440d826090",
    "semantic_title": "intermimic: towards universal whole-body control for physics-based human-object interactions",
    "citation_count": 22,
    "authors": [
      "Sirui Xu",
      "Hung Yu Ling",
      "Yu-Xiong Wang",
      "Liang-Yan Gui"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_Meta-Learning_Hyperparameters_for_Parameter_Efficient_Fine-Tuning_CVPR_2025_paper.html": {
    "title": "Meta-Learning Hyperparameters for Parameter Efficient Fine-Tuning",
    "volume": "main",
    "abstract": "Training large foundation models from scratch for domain-specific applications is almost impossible due to data limits and long-tailed distributions -- taking remote sensing (RS) as an example. Fine-tuning natural image pre-trained models on RS images is a straightforward solution. To reduce computational costs and improve performance on tail classes, existing methods apply parameter-efficient fine-tuning (PEFT) techniques, such as LoRA and AdaptFormer. However, we observe that fixed hyperparameters -- such as intra-layer positions, layer depth, and scaling factors, can considerably hinder PEFT performance, as fine-tuning on RS images proves highly sensitive to these settings. To address this, we propose MetaPEFT, a method incorporating adaptive scalers that dynamically adjust module influence during fine-tuning. MetaPEFT dynamically adjusts three key factors of PEFT on RS images: module insertion, layer selection, and module-wise learning rates, which collectively control the influence of PEFT modules across the network. We conduct extensive experiments on three transfer-learning scenarios and five datasets in both RS and natural image domains. The results show that MetaPEFT achieves state-of-the-art performance in cross-spectral adaptation, requiring only a small amount of trainable parameters and improving tail-class accuracy significantly",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichen Tian",
      "Yaoyao Liu",
      "Qianru Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cohen-Bar_TriTex_Learning_Texture_from_a_Single_Mesh_via_Triplane_Semantic_CVPR_2025_paper.html": {
    "title": "TriTex: Learning Texture from a Single Mesh via Triplane Semantic Features",
    "volume": "main",
    "abstract": "As 3D content creation continues to grow, transferring semantic textures between 3D meshes remains a significant challenge in computer graphics. While recent methods leverage text-to-image diffusion models for texturing, they often struggle to preserve the appearance of the source texture during texture transfer. We present TriTex, a novel approach that learns a volumetric texture field from a single textured mesh by mapping semantic features to surface colors. Using an efficient triplane-based architecture, our method enables semantic-aware texture transfer to a novel target mesh. Despite training on just one example, it generalizes effectively to diverse shapes within the same category. Extensive evaluation on our newly created benchmark dataset shows that TriTex achieves superior texture transfer quality and fast inference times compared to existing methods. Our approach advances single-example texture transfer, providing a practical solution for maintaining visual coherence across related 3D models in applications like game development and simulation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dana Cohen-Bar",
      "Daniel Cohen-Or",
      "Gal Chechik",
      "Yoni Kasten"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Efficient_Test-time_Adaptive_Object_Detection_via_Sensitivity-Guided_Pruning_CVPR_2025_paper.html": {
    "title": "Efficient Test-time Adaptive Object Detection via Sensitivity-Guided Pruning",
    "volume": "main",
    "abstract": "Continual test-time adaptive object detection (CTTA-OD) aims to online adapt a source pre-trained detector to ever-changing environments during inference under continuous domain shifts. Most existing CTTA-OD methods prioritize effectiveness while overlooking computational efficiency, which is crucial for resource-constrained scenarios. In this paper, we propose an efficient CTTA-OD method via pruning. Our motivation stems from the observation that not all learned source features are beneficial; certain domain-sensitive feature channels can adversely affect target domain performance. Inspired by this, we introduce a sensitivity-guided channel pruning strategy that quantifies each channel based on its sensitivity to domain discrepancies at both image and instance levels. We apply weighted sparsity regularization to selectively suppress and prune these sensitive channels, focusing adaptation efforts on invariant ones. Additionally, we introduce a stochastic channel reactivation mechanism to restore pruned channels, enabling recovery of potentially useful features and mitigating the risks of early pruning. Extensive experiments on three benchmarks show that our method achieves superior adaptation performance while reducing computational overhead by 12% in FLOPs compared to the recent SOTA method",
    "checked": true,
    "id": "bba3281f82a23faf90cf8d14401ddeb0529c4680",
    "semantic_title": "efficient test-time adaptive object detection via sensitivity-guided pruning",
    "citation_count": 0,
    "authors": [
      "Kunyu Wang",
      "Xueyang Fu",
      "Xin Lu",
      "Chengjie Ge",
      "Chengzhi Cao",
      "Wei Zhai",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wen_A_Data-Centric_Revisit_of_Pre-Trained_Vision_Models_for_Robot_Learning_CVPR_2025_paper.html": {
    "title": "A Data-Centric Revisit of Pre-Trained Vision Models for Robot Learning",
    "volume": "main",
    "abstract": "Pre-trained vision models (PVMs) are fundamental to modern robotics, yet their optimal configuration remains unclear. Through systematic evaluation, we find that while DINO and iBOT outperform MAE across visuomotor control and perception tasks, they struggle when trained on non-(single-)object-centric (NOC) data--a limitation strongly correlated with their diminished ability to learn object-centric representations. This investigation indicates that the ability to form object-centric representations from the non-object-centric robotics dataset is the key to success for PVMs. Motivated by this discovery, we designed SlotMIM, a method that induces object-centric representations by introducing a semantic bottleneck to reduce the number of prototypes to encourage the emergence of objectness as well as cross-view consistency regularization for encouraging multiview invariance. Our experiments encompass pre-training on object-centric, scene-centric, web-crawled, and ego-centric data. Across all settings, our approach learns transferrable representations and achieves significant improvements over prior work in image recognition, scene understanding, and robot learning evaluations. When scaled up with million-scale datasets, our method also demonstrates superior data efficiency and scalability. Our code and models are publicly available at https://github.com/CVMI-Lab/SlotMIM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Wen",
      "Bingchen Zhao",
      "Yilun Chen",
      "Jiangmiao Pang",
      "Xiaojuan Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Marsili_Visual_Agentic_AI_for_Spatial_Reasoning_with_a_Dynamic_API_CVPR_2025_paper.html": {
    "title": "Visual Agentic AI for Spatial Reasoning with a Dynamic API",
    "volume": "main",
    "abstract": "Visual reasoning -- the ability to interpret the visual world -- is crucial for embodied agents that operate within three-dimensional scenes. Progress in AI has led to vision and language models capable of answering questions from images. However, their performance declines when tasked with 3D spatial reasoning. To tackle the complexity of such reasoning problems, we introduce an agentic program synthesis approach where LLM agents collaboratively generate a Pythonic API with new functions to solve common subproblems. Our method overcomes limitations of prior approaches that rely on a static, human-defined API, allowing it to handle a wider range of queries. To assess AI capabilities for 3D understanding, we introduce a new benchmark of queries involving multiple steps of grounding and inference. We show that our method outperforms prior zero-shot models for visual reasoning in 3D and empirically validate the effectiveness of our agentic framework for 3D spatial reasoning tasks. Project website: https://glab-caltech.github.io/vadar/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Damiano Marsili",
      "Rohun Agrawal",
      "Yisong Yue",
      "Georgia Gkioxari"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_TAMT_Temporal-Aware_Model_Tuning_for_Cross-Domain_Few-Shot_Action_Recognition_CVPR_2025_paper.html": {
    "title": "TAMT: Temporal-Aware Model Tuning for Cross-Domain Few-Shot Action Recognition",
    "volume": "main",
    "abstract": "Going beyond few-shot action recognition (FSAR), cross-domain FSAR (CDFSAR) has attracted recent research interests by solving the domain gap lying in source-to-target transfer learning. Existing CDFSAR methods mainly focus on joint training of source and target data to mitigate the side effect of domain gap. However, such kind of methods suffer from two limitations: First, pair-wise joint training requires retraining deep models in case of one source data and multiple target ones, which incurs heavy computation cost, especially for large source and small target data. Second, pre-trained models after joint training are adopted to target domain in a straightforward manner, hardly taking full potential of pre-trained models and then limiting recognition performance. To overcome above limitations, this paper proposes a simple yet effective baseline, namely Temporal-Aware Model Tuning (TAMT) for CDFSAR. Specifically, our TAMT involves a decoupled paradigm by performing pre-training on source data and fine-tuning target data, which avoids retraining for multiple target data with single source. To effectively and efficiently explore the potential of pre-trained models in transferring to target domain, our TAMT proposes a Hierarchical Temporal Tuning Network (HTTN), whose core involves local temporal-aware adapters (TAA) and a global temporal-aware moment tuning (GTMT). Particularly, TAA learns few parameters to recalibrate the intermediate features of frozen pre-trained models, enabling efficient adaptation to target domains. Furthermore, GTMT helps to generate powerful video representations, improving match performance on the target domain. Experiments on several widely used video benchmarks show our TAMT outperforms the recently proposed counterparts by 13% 31%, achieving new state-of-the-art CDFSAR results",
    "checked": true,
    "id": "230caf86e1f491e3cccc88fb14001918232c9ea7",
    "semantic_title": "tamt: temporal-aware model tuning for cross-domain few-shot action recognition",
    "citation_count": 1,
    "authors": [
      "Yilong Wang",
      "Zilin Gao",
      "Qilong Wang",
      "Zhaofeng Chen",
      "Peihua Li",
      "Qinghua Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zang_Feature_Spectrum_Learning_for_Remote_Sensing_Change_Detection_CVPR_2025_paper.html": {
    "title": "Feature Spectrum Learning for Remote Sensing Change Detection",
    "volume": "main",
    "abstract": "Change detection (CD) holds significant implications for Earth observation, in which pseudo-changes between bitemporal images induced by imaging environmental factors are key challenges. Existing methods mainly regard pseudo-changes as a kind of style shift and alleviate it by transforming bitemporal images into the same style using generative adversarial networks (GANs). Nevertheless, their efforts are limited by the complexity of optimizing GANs and the absence of guidance from physical properties. This paper finds that the spectrum transformation (ST) has the potential to mitigate pseudo-changes by aligning in the frequency domain carrying the style. However, the benefit of ST is largely constrained by two drawbacks: 1) limited transformation space and 2) inefficient parameter search. To address these limitations, we propose a Feature Spectrum learning (FeaSpect) that adaptively eliminate pseudo-changes in the latent space. For the drawback 1), FeaSpect directs the transformation towards style-aligned discriminative features via feature spectrum transformation (FST). For the drawback 2), FeaSpect allows FST to be trainable, efficiently discovering optimal parameters via extraction box with adaptive attention and extraction box with learnable strides. Extensive experiments on challenging datasets demonstrate that our method remarkably outperforms existing methods and achieves a commendable trade-off between accuracy and efficiency. Importantly, our method can be easily injected into other frameworks, achieving consistent improvements",
    "checked": true,
    "id": "21b4fb546379ec71cb1245f89bf75e74f6f67896",
    "semantic_title": "feature spectrum learning for remote sensing change detection",
    "citation_count": 0,
    "authors": [
      "Qi Zang",
      "Dong Zhao",
      "Shuang Wang",
      "Dou Quan",
      "Zhun Zhong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gallagher-Syed_BioX-CPath_Biologically-driven_Explainable_Diagnostics_for_Multistain_IHC_Computational_Pathology_CVPR_2025_paper.html": {
    "title": "BioX-CPath: Biologically-driven Explainable Diagnostics for Multistain IHC Computational Pathology",
    "volume": "main",
    "abstract": "The development of biologically interpretable and explainable models remains a key challenge in computational pathology, particularly for multistain immunohistochemistry (IHC) analysis. We present BioX-CPath, an explainable graph neural network architecture for whole slide image (WSI) classification that leverages both spatial and semantic features across multiple stains. At its core, BioXCPath introduces a novel Stain-Aware Attention Pooling (SAAP) module that generates biologically meaningful, stain-aware patient embeddings. Our approach achieves state-of-the-art performance on both Rheumatoid Arthritis and Sjogren's Disease multistain datasets. Beyond performance metrics, BioX-CPath provides interpretable insights through stain attention scores, entropy measures, and stain interaction scores, that permit measuring model alignment with known pathological mechanisms. This biological grounding, combined with strong classification performance, makes BioX-CPath particularly suitable for clinical applications where interpretability is key. Source code and documentation can be found at: https://github.com/AmayaGS/BioX-CPath",
    "checked": true,
    "id": "304a128e9f00cbf0b4af580fe049b7739fe1694c",
    "semantic_title": "biox-cpath: biologically-driven explainable diagnostics for multistain ihc computational pathology",
    "citation_count": 0,
    "authors": [
      "Amaya Gallagher-Syed",
      "Henry Senior",
      "Omnia Alwazzan",
      "Elena Pontarini",
      "Michele Bombardieri",
      "Costantino Pitzalis",
      "Myles J. Lewis",
      "Michael R. Barnes",
      "Luca Rossi",
      "Gregory Slabaugh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_DriveDreamer4D_World_Models_Are_Effective_Data_Machines_for_4D_Driving_CVPR_2025_paper.html": {
    "title": "DriveDreamer4D: World Models Are Effective Data Machines for 4D Driving Scene Representation",
    "volume": "main",
    "abstract": "Closed-loop simulation is essential for advancing end-to-end autonomous driving systems. Contemporary sensor simulation methods, such as NeRF and 3DGS, rely predominantly on conditions closely aligned with training data distributions, which are largely confined to forward-driving scenarios. Consequently, these methods face limitations when rendering complex maneuvers (e.g., lane change, acceleration, deceleration). Recent advancements in autonomous-driving world models have demonstrated the potential to generate diverse driving videos. However, these approaches remain constrained to 2D video generation, inherently lacking the spatiotemporal coherence required to capture intricacies of dynamic driving environments.In this paper, we introduce DriveDreamer4D, which enhances 4D driving scene representation leveraging world model priors. Specifically, we utilize the world model as a data machine to synthesize novel trajectory videos, where structured conditions are explicitly leveraged to control the spatial-temporal consistency of traffic elements. Besides, the cousin data training strategy is proposed to facilitate merging real and synthetic data for optimizing 4DGS. To our knowledge, DriveDreamer4D is the first to utilize video generation models for improving 4D reconstruction in driving scenarios.Experimental results reveal that DriveDreamer4D significantly enhances generation quality under novel trajectory views, achieving a relative improvement in FID by 32.1%, 46.4%, and 16.3% compared to PVG, S3Gaussian, and Deformable-GS. Moreover, DriveDreamer4D markedly enhances the spatiotemporal coherence of driving agents, which is verified by a comprehensive user study and the relative increases of 22.6%, 43.5%, and 15.6% in the NTA-IoU metric",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guosheng Zhao",
      "Chaojun Ni",
      "Xiaofeng Wang",
      "Zheng Zhu",
      "Xueyang Zhang",
      "Yida Wang",
      "Guan Huang",
      "Xinze Chen",
      "Boyuan Wang",
      "Youyi Zhang",
      "Wenjun Mei",
      "Xingang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cai_LoKi_Low-dimensional_KAN_for_Efficient_Fine-tuning_Image_Models_CVPR_2025_paper.html": {
    "title": "LoKi: Low-dimensional KAN for Efficient Fine-tuning Image Models",
    "volume": "main",
    "abstract": "Pre-training + fine-tuning' has been widely used in various downstream tasks. Parameter-efficient fine-tuning (PEFT) has demonstrated higher efficiency and promising performance comapred to traditional full-tuning. The widely used adapter-based and prompt-based methods in PEFT can be uniformly represented as adding an MLP structure to the pre-trained model. These methods are prone to over-fitting in downstream tasks, due to the difference in data scale and distribution. To address this issue, we propose a new adapter-based PEFT module, i.e., LoKi, which consists of an encoder, a learnable activation layer, and a decoder. To maintain the simplicity of LoKi, we use single-layer linear networks for the encoder and decoder, and for the learnable activation layer, we use a Kolmogorov-Arnold Network (KAN) with the minimal number of layers (only 2 KAN linear layers). With a bottleneck rate much lower than that of Adapter, LoKi is equipped with fewer parameters (only half of Adapter) and eliminates slow training speed and high memory usage of KAN. We conduct extensive experiments on LoKi under image classification and video action recognition across 9 datasets. LoKi demonstrates highly competitive generalization performance compared to other PEFT methods with fewer tunable parameters, ensuring both effectiveness and efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Cai",
      "Renjie Pan",
      "Hua Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jun-Seong_Dr._Splat_Directly_Referring_3D_Gaussian_Splatting_via_Direct_Language_CVPR_2025_paper.html": {
    "title": "Dr. Splat: Directly Referring 3D Gaussian Splatting via Direct Language Embedding Registration",
    "volume": "main",
    "abstract": "We introduce Dr. Splat, a novel approach for open-vocabulary 3D scene understanding leveraging 3D Gaussian Splatting. Unlike existing language-embedded 3DGS methods, which rely on a rendering process, our method directly associates language-aligned CLIP embeddings with 3D Gaussians for holistic 3D scene understanding. The key of our method is a language feature registration technique where CLIP embeddings are assigned to the dominant Gaussians intersected by each pixel-ray. Moreover, we integrate Product Quantization (PQ) trained on general large scale image data to compactly represent embeddings without per-scene optimization. Experiments demonstrate that our approach significantly outperforms existing approaches in 3D perception benchmarks, such as open-vocabulary 3D semantic segmentation, 3D object localization, and 3D object selection tasks",
    "checked": true,
    "id": "c3a7c759bf8a8feebe82f80542301a1a7a8ff33c",
    "semantic_title": "dr. splat: directly referring 3d gaussian splatting via direct language embedding registration",
    "citation_count": 6,
    "authors": [
      "Kim Jun-Seong",
      "GeonU Kim",
      "Kim Yu-Ji",
      "Yu-Chiang Frank Wang",
      "Jaesung Choe",
      "Tae-Hyun Oh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Wavelet_and_Prototype_Augmented_Query-based_Transformer_for_Pixel-level_Surface_Defect_CVPR_2025_paper.html": {
    "title": "Wavelet and Prototype Augmented Query-based Transformer for Pixel-level Surface Defect Detection",
    "volume": "main",
    "abstract": "As an important part of intelligent manufacturing, pixel-level surface defect detection (SDD) aims to locate defect areas through mask prediction. Previous methods adopt the image-independent static convolution to indiscriminately classify per-pixel features for mask prediction, which leads to suboptimal results for some challenging scenes such as weak defects and cluttered backgrounds. In this paper, inspired by query-based methods, we propose a Wavelet and Prototype Augmented Query-based Transformer (WPFormer) for surface defect detection. Specifically, a set of dynamic queries for mask prediction is updated through the dual-domain transformer decoder. Firstly, a Wavelet-enhanced Cross-Attention (WCA) is proposed, which aggregates meaningful high- and low-frequency information of image features in the wavelet domain to refine queries. WCA enhances the representation of high-frequency components by capturing multi-scale relationships between different frequency components, enabling queries to focus more on defect details. Secondly, a Prototype-guided Cross-Attention (PCA) is proposed to refine queries through meta-prototypes in the spatial domain. The prototypes aggregate semantically meaningful tokens from image features, facilitating queries to aggregate crucial defect information under the cluttered backgrounds. Extensive experiments on three defect detection datasets (i.e., ESDIs-SOD, CrackSeg9k, and ZJU-Leaper) demonstrate that the proposed method achieves state-of-the-art performance in defect detection. The code will be available at https://github.com/yfhdm/WPFormer",
    "checked": true,
    "id": "a76ce17fc6a269adf7702c97a4fad485b00aafa5",
    "semantic_title": "wavelet and prototype augmented query-based transformer for pixel-level surface defect detection",
    "citation_count": 2,
    "authors": [
      "Feng Yan",
      "Xiaoheng Jiang",
      "Yang Lu",
      "Jiale Cao",
      "Dong Chen",
      "Mingliang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Marques_GauCho_Gaussian_Distributions_with_Cholesky_Decomposition_for_Oriented_Object_Detection_CVPR_2025_paper.html": {
    "title": "GauCho: Gaussian Distributions with Cholesky Decomposition for Oriented Object Detection",
    "volume": "main",
    "abstract": "Oriented Object Detection (OOD) has received increased attention in the past years, being a suitable solution for detecting elongated objects in remote sensing analysis. In particular, using regression loss functions based on Gaussian distributions has become attractive since they yield simple and differentiable terms. However, existing solutions are still based on regression heads that produce Oriented Bounding Boxes (OBBs), and the known problem of angular boundary discontinuity persists. In this work, we propose a regression head for OOD that directly produces Gaussian distributions based on the Cholesky matrix decomposition. The proposed head, named Gaucho, theoretically mitigates the boundary discontinuity problem and is fully compatible with recent Gaussian-based regression loss functions. Furthermore, we advocate using Oriented Ellipses (OEs) to represent oriented objects, which relates to GauCho through a bijective function and alleviates the encoding ambiguity problem for circular objects. Our experimental results show that GauCho can be a viable alternative to the traditional OBB head, achieving results comparable to or better than state-of-the-art detectors for the challenging dataset DOTA",
    "checked": true,
    "id": "6052618d2269a6fb73996ee6e6a7cec44c53ff91",
    "semantic_title": "gaucho: gaussian distributions with cholesky decomposition for oriented object detection",
    "citation_count": 1,
    "authors": [
      "Jos√© Henrique Lima Marques",
      "Jeffri Murrugarra-Llerena",
      "Claudio R. Jung"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zou_Alignment_Mining_and_Fusion_Representation_Alignment_with_Hard_Negative_Mining_CVPR_2025_paper.html": {
    "title": "Alignment, Mining and Fusion: Representation Alignment with Hard Negative Mining and Selective Knowledge Fusion for Medical Visual Question Answering",
    "volume": "main",
    "abstract": "Medical Visual Question Answering (Med-VQA) is a challenging task that requires a deep understanding of both medical images and textual questions. Although recent works leveraging Medical Vision-Language Pre-training (Med-VLP) have shown strong performance on the Med-VQA task, there is still no unified solution for modality alignment, and the issue of hard negatives remains under-explored. Additionally, commonly used knowledge fusion techniques for Med-VQA may introduce irrelevant information. In this work, we propose a framework to address these challenges through three key contributions: (1) a unified solution for heterogeneous modality alignments across multiple levels, modalities, views, and stages, leveraging methods such as contrastive learning and optimal transport theory; (2) a hard negative mining method that employs soft labels for multi-modality alignments and enforces the hard negative pair discrimination; and (3) a Gated Cross-Attention Module for Med-VQA that integrates the answer vocabulary as prior knowledge and select relevant information from it. Our framework outperforms the previous state-of-the-art on widely used Med-VQA datasets like RAD-VQA, SLAKE, PathVQA and VQA-2019. The code will be publicly available",
    "checked": true,
    "id": "29a552a6f08913ee3aa5b87032146d11c0140191",
    "semantic_title": "alignment, mining and fusion: representation alignment with hard negative mining and selective knowledge fusion for medical visual question answering",
    "citation_count": 1,
    "authors": [
      "Yuanhao Zou",
      "Zhaozheng Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_No_Thing_Nothing_Highlighting_Safety-Critical_Classes_for_Robust_LiDAR_Semantic_CVPR_2025_paper.html": {
    "title": "No Thing, Nothing: Highlighting Safety-Critical Classes for Robust LiDAR Semantic Segmentation in Adverse Weather",
    "volume": "main",
    "abstract": "Existing domain generalization methods for LiDAR semantic segmentation under adverse weather struggle to accurately predict \"things\" categories compared to \"stuff\" categories. In typical driving scenes, \"things\" categories can be dynamic and associated with higher collision risks, making them crucial for safe navigation and planning. Recognizing the importance of \"things\" categories, we identify their performance drop as a serious bottleneck in existing approaches. We observed that adverse weather induces degradation of semantic-level features and both corruption of local features, leading to a misprediction of \"things\" as \"stuff\". To mitigate these corruptions, we suggests our method, NTN - segmeNt Things for No-accident. To address semantic-level feature corruption, we bind each point feature to its superclass, preventing the misprediction of things classes into visually dissimilar categories. Additionally, to enhance robustness against local corruption caused by adverse weather, we define each LiDAR beam as a local region and propose a regularization term that aligns the clean data with its corrupted counterpart in feature space. NTN achieves state-of-the-art performance with a +2.6 mIoU gain on the SemanticKITTI-to-SemanticSTF benchmark and +7.9 mIoU on the SemanticPOSS-to-SemanticSTF benchmark. Notably, NTN achieves a +4.8 and +7.9 mIoU improvement on \"things\" classes, respectively, highlighting its effectiveness",
    "checked": true,
    "id": "266e06ac6f14fc3e7e8c9754d66c8aa8cc20d089",
    "semantic_title": "no thing, nothing: highlighting safety-critical classes for robust lidar semantic segmentation in adverse weather",
    "citation_count": 1,
    "authors": [
      "Junsung Park",
      "Hwijeong Lee",
      "Inha Kang",
      "Hyunjung Shim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_Mind_the_Gap_Detecting_Black-box_Adversarial_Attacks_in_the_Making_CVPR_2025_paper.html": {
    "title": "Mind the Gap: Detecting Black-box Adversarial Attacks in the Making through Query Update Analysis",
    "volume": "main",
    "abstract": "Adversarial attacks remain a significant threat that can jeopardize the integrity of Machine Learning (ML) models. In particular, query-based black-box attacks can generate malicious noise without having access to the victim model's architecture, making them practical in real-world contexts. The community has proposed several defenses against adversarial attacks, only to be broken by more advanced and adaptive attack strategies. In this paper, we propose a framework that detects if an adversarial noise instance is being generated. Unlike existing stateful defenses that detect adversarial noise generation by monitoring the input space, our approach learns adversarial patterns in the input update similarity space. In fact, we propose to observe a new metric called Delta Similarity (DS), which we show it captures more efficiently the adversarial behavior.We evaluate our approach against 8 state-of-the-art attacks, including adaptive attacks, where the adversary is aware of the defense and tries to evade detection. We find that our approach is significantly more robust than existing defenses both in terms of specificity and sensitivity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeonghwan Park",
      "Niall McLaughlin",
      "Ihsen Alouani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_Consistent_Normal_Orientation_for_3D_Point_Clouds_via_Least_Squares_CVPR_2025_paper.html": {
    "title": "Consistent Normal Orientation for 3D Point Clouds via Least Squares on Delaunay Graph",
    "volume": "main",
    "abstract": "The orientation of surface normals in 3D point cloud is a fundamental problem in computer vision and graphics. Determining a globally consistent orientation solely from the point cloud is however challenging due to the global scope of the problem and the discrete nature of point cloud, particularly in the presence of noise, outliers, holes, thin structures, and complex topologies. This paper presents an efficient, robust, and global algorithm for generating consistent normal orientation of a dense 3D point cloud. The basic idea is to transform the original binary normal orientation problem to finding a relaxed sign field on a Delaunay graph, which can be achieved by solving a sparse linear system. The Delaunay graph is constructed by triangulating a level set of an implicit function defined from the input point cloud. The shape diameter function is estimated to serve as a prior for determining an appropriate level value such that the level set implicitly defines the inner and outer shells enclosing the input point clouds. As such, our algorithm leverages the strengths of the shape diameter function, Delaunay triangulation, and the least-square techniques, making the underlying processes take both geometry and topology into consideration, and thus provides an efficient and robust solution for handling point clouds with complicated geometry and topology. Extensive experiments on various shapes with noise and outliers confirm the effectiveness and robustness of our algorithm",
    "checked": true,
    "id": "6029d993a463b6a4c605eacbd79e279bcac5a87c",
    "semantic_title": "consistent normal orientation for 3d point clouds via least squares on delaunay graph",
    "citation_count": 0,
    "authors": [
      "Rao Fu",
      "Jianmin Zheng",
      "Liang Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zuo_GaussianWorld_Gaussian_World_Model_for_Streaming_3D_Occupancy_Prediction_CVPR_2025_paper.html": {
    "title": "GaussianWorld: Gaussian World Model for Streaming 3D Occupancy Prediction",
    "volume": "main",
    "abstract": "3D occupancy prediction is important for autonomous driving due to its comprehensive perception of the surroundings. To incorporate sequential inputs, most existing methods fuse representations from previous frames to infer the current 3D occupancy. However, they fail to consider the continuity of driving scenarios and ignore the strong prior provided by the evolution of 3D scenes (e.g., only dynamic objects move). In this paper, we propose a world-modelbased framework to exploit the scene evolution for perception. We reformulate 3D occupancy prediction as a 4D occupancy forecasting problem conditioned on the current sensor input. We decompose the scene evolution into three factors: 1) ego motion alignment of static scenes; 2) local movements of dynamic objects; and 3) completion of newly-observed scenes. We then employ a Gaussian world model (GaussianWorld) to explicitly exploit these priors and infer the scene evolution in the 3D Gaussian space considering the current RGB observation. We evaluate the effectiveness of our framework on the widely used nuScenes dataset. Our GaussianWorld improves the performance of the single-frame counterpart by over 2% in mIoU without introducing additional computations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sicheng Zuo",
      "Wenzhao Zheng",
      "Yuanhui Huang",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_ICP_Immediate_Compensation_Pruning_for_Mid-to-high_Sparsity_CVPR_2025_paper.html": {
    "title": "ICP: Immediate Compensation Pruning for Mid-to-high Sparsity",
    "volume": "main",
    "abstract": "The increasing adoption of large-scale models under 7 billion parameters in both language and vision domains enables inference tasks on a single consumer-grade GPU but makes fine-tuning models of this scale, especially 7B models, challenging. This limits the applicability of pruning methods that require full fine-tuning. Meanwhile, pruning methods that do not require fine-tuning perform well at low sparsity levels (10%-50%) but struggle at mid-to-high sparsity levels (50%-70%), where the error behaves equivalently to that of semi-structured pruning. To address these issues, this paper introduces ICP, which finds a balance between full fine-tuning and zero fine-tuning. First, Sparsity Rearrange is used to reorganize the predefined sparsity levels, followed by Block-wise Compensate Pruning, which alternates pruning and compensation on the model's backbone, fully utilizing inference results while avoiding full model fine-tuning. Experiments show that ICP improves performance at mid-to-high sparsity levels compared to baselines, with only a slight increase in pruning time and no additional peak memory overhead",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Luo",
      "Xueming Fu",
      "Zihang Jiang",
      "S. Kevin Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_VinaBench_Benchmark_for_Faithful_and_Consistent_Visual_Narratives_CVPR_2025_paper.html": {
    "title": "VinaBench: Benchmark for Faithful and Consistent Visual Narratives",
    "volume": "main",
    "abstract": "Visual narrative generation transforms textual narratives into sequences of images illustrating the content of the text. However, generating visual narratives that are faithful to the input text and self-consistent across generated images remains an open challenge, due to the lack of knowledge constraints used for planning the stories. In this work, we propose a new benchmark, VinaBench, to address this challenge. Our benchmark annotates the underlying commonsense and discourse constraints in visual narrative samples, offering systematic scaffolds for learning the implicit strategies of visual storytelling. Based on the incorporated narrative constraints, we further propose novel metrics to closely evaluate the consistency of generated narrative images and the alignment of generations with the input textual narrative. Our results across three generative vision models demonstrate that learning with VinaBench's knowledge constraints effectively improves the faithfulness and cohesion of generated visual narratives",
    "checked": true,
    "id": "08958f691ed282fffc8f03dd6511241016a1a566",
    "semantic_title": "vinabench: benchmark for faithful and consistent visual narratives",
    "citation_count": 1,
    "authors": [
      "Silin Gao",
      "Sheryl Mathew",
      "Li Mi",
      "Sepideh Mamooler",
      "Mengjie Zhao",
      "Hiromi Wakaki",
      "Yuki Mitsufuji",
      "Syrielle Montariol",
      "Antoine Bosselut"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_ATA_Adaptive_Transformation_Agent_for_Text-Guided_Subject-Position_Variable_Background_Inpainting_CVPR_2025_paper.html": {
    "title": "ATA: Adaptive Transformation Agent for Text-Guided Subject-Position Variable Background Inpainting",
    "volume": "main",
    "abstract": "Image inpainting aims to fill the missing region of an image.Recently, there has been a surge of interest in foreground-conditioned background inpainting, a sub-task that fills the background of an image while the foreground subject and associated text prompt are provided.Existing background inpainting methods typically strictly preserve the subject's original position from the source image,resulting in inconsistencies between the subject and the generated background.To address this challenge, we propose a new task, the \"Text-Guided Subject-Position Variable Background Inpainting\", which aims to dynamically adjust the subject position to achieve a harmonious relationship between the subject andthe inpainted background, and propose the Adaptive Transformation Agent (A^\\text T A) for this task.Firstly, we design a PosAgent Block that adaptively predicts an appropriate displacement based on given features to achieve variable subject-position.Secondly, we design the Reverse Displacement Transform (RDT) module, which arranges multiple PosAgent blocks in a reverse structure, to transform hierarchical feature maps from deep to shallow based on semantic information.Thirdly, we equip A^\\text T A with a Position Switch Embedding to control whether the subject's position in the generated image is adaptively predicted or fixed.Extensive comparative experiments validate the effectiveness of our A^\\text T A approach, which not only demonstrates superior inpainting capabilities in subject-position variable inpainting, but also ensures good performance on subject-position fixed inpainting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizhe Tang",
      "Zhimin Sun",
      "Yuzhen Du",
      "Ran Yi",
      "Guangben Lu",
      "Teng Hu",
      "Luying Li",
      "Lizhuang Ma",
      "Fangyuan Zou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Optimizing_for_the_Shortest_Path_in_Denoising_Diffusion_Model_CVPR_2025_paper.html": {
    "title": "Optimizing for the Shortest Path in Denoising Diffusion Model",
    "volume": "main",
    "abstract": "In this research, we propose a novel denoising diffusion model based on shortest-path modeling that optimizes residual propagation to enhance both denoising efficiency and quality. Drawing on Denoising Diffusion Implicit Models (DDIM) and insights from graph theory, our model, termed the Shortest Path Diffusion Model (ShortDF), treats the denoising process as a shortest-path problem aimed at minimizing reconstruction error. By optimizing the initial residuals, we improve the efficiency of the reverse diffusion process and the quality of the generated samples. Extensive experiments on multiple standard benchmarks demonstrate that ShortDF significantly reduces diffusion time (or steps) while enhancing the visual fidelity of generated samples compared to prior methods. This work, we suppose, paves the way for interactive diffusion-based applications and establishes a foundation for rapid data generation. Code is available at https://github.com/UnicomAI/ShortDF",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ping Chen",
      "Xingpeng Zhang",
      "Zhaoxiang Liu",
      "Huan Hu",
      "Xiang Liu",
      "Kai Wang",
      "Min Wang",
      "Yanlin Qian",
      "Shiguo Lian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Antidote_A_Unified_Framework_for_Mitigating_LVLM_Hallucinations_in_Counterfactual_CVPR_2025_paper.html": {
    "title": "Antidote: A Unified Framework for Mitigating LVLM Hallucinations in Counterfactual Presupposition and Object Perception",
    "volume": "main",
    "abstract": "Large Vision-Language Models (LVLMs) have achieved impressive results across various cross-modal tasks. However, hallucinations, i.e., the models generating counterfactual responses, remain a challenge. Though recent studies have attempted to alleviate object perception hallucinations, they focus on the models' response generation, and overlook the task question itself. This paper discusses the vulnerability of LVLMs in solving counterfactual presupposition questions (CPQs), where the models are prone to accept the presuppositions of counterfactual objects and produce severe hallucinatory responses. To this end, we introduce \"Antidote\", a unified, synthetic data-driven post-training framework for mitigating both types of hallucination above. It leverages synthetic data to incorporate factual priors into questions to achieve self-correction, and decouples the mitigation process into a preference optimization problem. Furthermore, we construct \"CP-Bench\", a novel benchmark to evaluate LVLMs' ability to correctly handle CPQs and produce factual responses. Applied to the LLaVA series, Antidote can simultaneously enhance performance on CP-Bench by over 50%, POPE by 1.8-3.3%, and CHAIR & SHR by 30-50%, all without relying on external supervision from stronger LVLMs or human feedback and without introducing noticeable catastrophic forgetting issues",
    "checked": true,
    "id": "235d9db8d8cf178d522b13826354ecadc097d634",
    "semantic_title": "antidote: a unified framework for mitigating lvlm hallucinations in counterfactual presupposition and object perception",
    "citation_count": 1,
    "authors": [
      "Yuanchen Wu",
      "Lu Zhang",
      "Hang Yao",
      "Junlong Du",
      "Ke Yan",
      "Shouhong Ding",
      "Yunsheng Wu",
      "Xiaoqiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Language-Guided_Audio-Visual_Learning_for_Long-Term_Sports_Assessment_CVPR_2025_paper.html": {
    "title": "Language-Guided Audio-Visual Learning for Long-Term Sports Assessment",
    "volume": "main",
    "abstract": "Long-term sports assessment is a challenging task in video understanding since it requires judging complex movement variations and action-music coordination. However, there is no direct correlation between the diverse background music and movements in sporting events. Previous works require a large number of model parameters to learn potential associations between actions and music. To address this issue, we propose a language-guided audio-visual learning (MLAVL) framework that models \"audio-action-visual\" correlations guided by low-cost language modality. In our framework, multidimensional domain-based actions form action knowledge graphs, motivating audio-visual modalities to focus on task-relevant actions. We further design a shared-specific context encoder to integrate deep multimodal semantics, and an audio-visual cross-modal fusion module to evaluate action-music consistency. To match the sport's rules, we then propose a dual-branch prompt-guided grading module to weigh both visual and audio-visual performance. Extensive experiments demonstrate that our approach achieves state-of-the-art on four public long-term sports benchmarks while maintaining low parameters. Our code is available at https://github.com/XuHuangbiao/MLAVL",
    "checked": true,
    "id": "95cc35d0393856c4a6f4c7770409f77dc53c9ff7",
    "semantic_title": "language-guided audio-visual learning for long-term sports assessment",
    "citation_count": 1,
    "authors": [
      "Huangbiao Xu",
      "Xiao Ke",
      "Huanqi Wu",
      "Rui Xu",
      "Yuezhou Li",
      "Wenzhong Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_Dynamic_Pseudo_Labeling_via_Gradient_Cutting_for_High-Low_Entropy_Exploration_CVPR_2025_paper.html": {
    "title": "Dynamic Pseudo Labeling via Gradient Cutting for High-Low Entropy Exploration",
    "volume": "main",
    "abstract": "This study addresses the limitations of existing dynamic pseudo-labeling (DPL) techniques, which often utilize static or dynamic thresholds for confident sample selection. The existing methods fail to capture the non-linear relationship between task accuracy and model confidence, particularly in the context of overconfidence. This can limit the model's learning opportunities for high entropy samples that significantly influence a model's generalization ability. To solve this, we propose a novel gradient pass-based DPL technique that incorporates the high-entropy samples, which are typically overlooked. Our approach introduces two classifiers--low gradient pass (LGP) and high gradient pass (HGP)--to derive over- and under-confident dynamic thresholds that indicate the class-wise overconfidence acceleration, respectively. By combining the under- and over-confident states from the GP classifiers, we create a more adaptive and accurate PL method. Our main contributions highlight the importance of considering both low and high-confidence samples in enhancing the model's robustness and generalization for improved PL performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jae Hyeon Park",
      "Joo Hyeon Jeon",
      "Jae Yun Lee",
      "Sangyeon Ahn",
      "Min Hee Cha",
      "Min Geol Kim",
      "Hyeok Nam",
      "Sung In Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_VODiff_Controlling_Object_Visibility_Order_in_Text-to-Image_Generation_CVPR_2025_paper.html": {
    "title": "VODiff: Controlling Object Visibility Order in Text-to-Image Generation",
    "volume": "main",
    "abstract": "Recent advancements in diffusion models have significantly enhanced the performance of text-to-image models in image synthesis. To enable control over the the spatial locations of the generated objects,diffusion-based methods typically utilizeobject layout as an auxiliary input. However, we observe that this approach treats all objects as being on the same layer and neglect their visibility order, leading to the synthesis of overlapping objects with incorrect occlusions.To address this limitation, we introduce in this paper a new training-free framework that considers object visibility order explicitly and allows users to place overlapping objects in a stack of layers. Our framework consists of two visibility-based designs. First, we propose a novel Sequential Denoising Process (SDP) to divide the whole image generation into multiple stages for different objects, each stage primarily focuses on an object. Second, we propose a novel Visibility-Order-Aware (VOA) Loss to transform the layout and occlusion constraints into an attention map optimization process to improve the accuracy of synthesizing object occlusions in complex scenes. By merging these two novel components, our framework, dubbed VODiff, enables the generation of photorealistic images that satisfy user-specified spatial constraints and object occlusion relationships. In addition, we introduce VOBench, a diverse benchmark dataset containing 200 curated samples, each with a reference image, text prompts, object visibility orders and layout maps. We conduct extensive evaluations on this dataset to demonstrate the superiority of our approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Liang",
      "Jinyuan Jia",
      "Yuhao Liu",
      "Zhanghan Ke",
      "Hongbo Fu",
      "Rynson W. H. Lau"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Dual_Diffusion_for_Unified_Image_Generation_and_Understanding_CVPR_2025_paper.html": {
    "title": "Dual Diffusion for Unified Image Generation and Understanding",
    "volume": "main",
    "abstract": "Diffusion models have gained tremendous success in text-to-image generation, yet still struggle with visual understanding tasks, an area dominated by autoregressive vision-language models. We propose a large-scale and fully end-to-end diffusion model for multi-modal understanding and generation that significantly improves on existing diffusion-based multimodal models, and is the first of its kind to support the full suite of vision-language modeling capabilities. Inspired by the multimodal diffusion transformer (MM-DiT) and recent advances in discrete diffusion language modeling, we leverage a cross-modal maximum likelihood estimation framework that simultaneously trains the conditional likelihoods of both images and text jointly under a single loss function, which is back-propagated through both branches of the diffusion transformer. The resulting model is highly flexible and capable of a wide range of tasks including image generation, captioning, and visual question answering. Our model attained competitive performance compared to recent unified image understanding and generation models, demonstrating the potential of multimodal diffusion modeling as a promising alternative to autoregressive next-token prediction models",
    "checked": true,
    "id": "4eb8a27c29809fbdb5c769e394409ce8c48856e1",
    "semantic_title": "dual diffusion for unified image generation and understanding",
    "citation_count": 25,
    "authors": [
      "Zijie Li",
      "Henry Li",
      "Yichun Shi",
      "Amir Barati Farimani",
      "Yuval Kluger",
      "Linjie Yang",
      "Peng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_WeakMCN_Multi-task_Collaborative_Network_for_Weakly_Supervised_Referring_Expression_Comprehension_CVPR_2025_paper.html": {
    "title": "WeakMCN: Multi-task Collaborative Network for Weakly Supervised Referring Expression Comprehension and Segmentation",
    "volume": "main",
    "abstract": "Weakly supervised referring expression comprehension (WREC) and segmentation (WRES) aim to learn object grounding based on a given expression using weak supervision signals like image-text pairs. While these tasks have traditionally been modeled separately, we argue that they can benefit from joint learning in a multi-task framework. To this end, we propose WeakMCN, a novel multi-task collaborative network that effectively combines WREC and WRES with a dual-branch architecture. Specifically, the WREC branch is formulated as anchor-based contrastive learning, which also acts as a teacher to supervise the WRES branch. In WeakMCN, we propose two innovative designs to facilitate multi-task collaboration, namely Dynamic Visual Feature Enhancement (DVFE) and Collaborative Consistency Module (CCM). DVFE dynamically combines various pre-trained visual knowledge to meet different task requirements, while CCM promotes cross-task consistency from the perspective of optimization. Extensive experimental results on three popular REC and RES benchmarks, i.e., RefCOCO, RefCOCO+, and RefCOCOg, consistently demonstrate performance gains of WeakMCN over state-of-the-art single-task alternatives, e.g., up to 3.91% and 13.11% on RefCOCO for WREC and WRES tasks, respectively. Furthermore, experiments also validate the strong generalization ability of WeakMCN in both semi-supervised REC and RES settings against existing methods, e.g., +8.94% for semi-REC and +7.71% for semi-RES on 1% RefCOCO",
    "checked": true,
    "id": "a4c65265f1f05966fdbb11c7706c84e63da5ae4b",
    "semantic_title": "weakmcn: multi-task collaborative network for weakly supervised referring expression comprehension and segmentation",
    "citation_count": 1,
    "authors": [
      "Silin Cheng",
      "Yang Liu",
      "Xinwei He",
      "Sebastien Ourselin",
      "Lei Tan",
      "Gen Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_CAD-Llama_Leveraging_Large_Language_Models_for_Computer-Aided_Design_Parametric_3D_CVPR_2025_paper.html": {
    "title": "CAD-Llama: Leveraging Large Language Models for Computer-Aided Design Parametric 3D Model Generation",
    "volume": "main",
    "abstract": "Recently, Large Language Models (LLMs) have achieved significant success, prompting increased interest in expanding their generative capabilities beyond general text into domain-specific areas. This study investigates the generation of parametric sequences for computer-aided design (CAD) models using LLMs. This endeavor represents an initial step towards creating parametric 3D shapes with LLMs, as CAD model parameters directly correlate with shapes in three-dimensional space. Despite the formidable generative capacities of LLMs, this task remains challenging, as these models neither encounter parametric sequences during their pretraining phase nor possess direct awareness of 3D structures. To address this, we present CAD-Llama, a framework designed to enhance pretrained LLMs for generating parametric 3D CAD models. Specifically, we develop a hierarchical annotation pipeline and a code-like format to translate parametric 3D CAD command sequences into Structured Parametric CAD Code (SPCC), incorporating hierarchical semantic descriptions. Furthermore, we propose an adaptive pretraining approach utilizing SPCC, followed by an instruction tuning process aligned with CAD-specific guidelines. This methodology aims to equip LLMs with the spatial knowledge inherent in parametric sequences. Experimental results demonstrate that our framework significantly outperforms prior autoregressive methods and existing LLM baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Li",
      "Weijian Ma",
      "Xueyang Li",
      "Yunzhong Lou",
      "Guichun Zhou",
      "Xiangdong Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Leveraging_SD_Map_to_Augment_HD_Map-based_Trajectory_Prediction_CVPR_2025_paper.html": {
    "title": "Leveraging SD Map to Augment HD Map-based Trajectory Prediction",
    "volume": "main",
    "abstract": "Latest trajectory prediction models in real-world autonomous driving systems often rely on online High-Definition (HD) maps to understand the road environment.However, online HD maps suffer from perception errors and feature redundancy, which hinder the performance of HD map-based trajectory prediction models.To address these issues, we introduce a framework, termed SD map-Augmented Trajectory Prediction (SATP), which leverages Standard-Definition (SD) maps to enhance HD map-based trajectory prediction models.First, we propose an SD-HD fusion approach to leverage SD maps across the diverse range of HD map-based trajectory prediction models. Second, we design a novel AlignNet to align the SD map with the HD map, further improving the effectiveness of SD maps. Experiments on real-world autonomous driving benchmarks demonstrate that SATP not only improves the performance of HD map-based trajectory prediction up to 25% in real-world scenarios using online HD maps but also brings benefits in ideal scenarios with ground-truth HD maps",
    "checked": true,
    "id": "2a9962f1fb03ac2762f2a74aa31a0e7bc194c2c2",
    "semantic_title": "leveraging sd map to augment hd map-based trajectory prediction",
    "citation_count": 1,
    "authors": [
      "Zhiwei Dong",
      "Ran Ding",
      "Wei Li",
      "Peng Zhang",
      "Guobin Tang",
      "Jia Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_4DGC_Rate-Aware_4D_Gaussian_Compression_for_Efficient_Streamable_Free-Viewpoint_Video_CVPR_2025_paper.html": {
    "title": "4DGC: Rate-Aware 4D Gaussian Compression for Efficient Streamable Free-Viewpoint Video",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) has substantial potential for enabling photorealistic Free-Viewpoint Video (FVV) experiences. However, the vast number of Gaussians and their associated attributes poses significant challenges for storage and transmission. Existing methods typically handle dynamic 3DGS representation and compression separately, neglecting motion information and the rate-distortion (RD) trade-off during training, leading to performance degradation and increased model redundancy. To address this gap, we propose 4DGC, a novel rate-aware 4D Gaussian compression framework that significantly reduces storage size while maintaining superior RD performance for FVV. Specifically, 4DGC introduces a motion-aware dynamic Gaussian representation that utilizes a compact motion grid combined with sparse compensated Gaussians to exploit inter-frame similarities. This representation effectively handles large motions, preserving quality and reducing temporal redundancy. Furthermore, we present an end-to-end compression scheme that employs differentiable quantization and a tiny implicit entropy model to compress the motion grid and compensated Gaussians efficiently. The entire framework is jointly optimized using a rate-distortion trade-off. Extensive experiments demonstrate that 4DGC supports variable bitrates and consistently outperforms existing methods in RD performance across multiple datasets",
    "checked": true,
    "id": "20a12b9f9ce1528e9e59dd28eae11b6085be2035",
    "semantic_title": "4dgc: rate-aware 4d gaussian compression for efficient streamable free-viewpoint video",
    "citation_count": 4,
    "authors": [
      "Qiang Hu",
      "Zihan Zheng",
      "Houqiang Zhong",
      "Sihua Fu",
      "Li Song",
      "Xiaoyun Zhang",
      "Guangtao Zhai",
      "Yanfeng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tan_ONDA-Pose_Occlusion-Aware_Neural_Domain_Adaptation_for_Self-Supervised_6D_Object_Pose_CVPR_2025_paper.html": {
    "title": "ONDA-Pose: Occlusion-Aware Neural Domain Adaptation for Self-Supervised 6D Object Pose Estimation",
    "volume": "main",
    "abstract": "Self-supervised 6D object pose estimation has received increasing attention in computer vision recently. Some typical works in literature attempt to translate the synthetic images with object pose labels generated by object CAD models into the real domain, and then use the translated data for training. However, their performance is generally limited, since (i) there still exists a domain gap between the translated images and the real images and (ii) the translated images can not sufficiently reflect occlusions that exist in many real images. To address these problems, we propose an Occlusion-Aware Neural Domain Adaptation method for self-supervised 6D object Pose estimation, called ONDA-Pose. The proposed method comprises three main steps. Firstly, by utilizing both the training real images without pose labels and a CAD model, we explore a CAD-like radiance field for rendering corresponding synthetic images that have similar textures to those generated by the CAD model. Then, a backbone pose estimator trained on the synthetic data is employed to provide initial pose estimations for the synthetic images rendered from the CAD-like radiance field, and the initial object poses are refined by a global object pose refiner to generate pseudo object pose labels. Finally, the backbone pose estimator is further self-supervised as the final pose estimator by jointly utilizing the real images with pseudo object pose labels and the synthetic images rendered from the CAD-like radiance field. Experimental results on three public datasets demonstrate that ONDA-Pose significantly outperforms the comparative state-of-the-art methods in most cases",
    "checked": true,
    "id": "5e68481395262f5d452c9a58ea9324212341dcdd",
    "semantic_title": "onda-pose: occlusion-aware neural domain adaptation for self-supervised 6d object pose estimation",
    "citation_count": 1,
    "authors": [
      "Tao Tan",
      "Qiulei Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhen_Teller_Real-Time_Streaming_Audio-Driven_Portrait_Animation_with_Autoregressive_Motion_Generation_CVPR_2025_paper.html": {
    "title": "Teller: Real-Time Streaming Audio-Driven Portrait Animation with Autoregressive Motion Generation",
    "volume": "main",
    "abstract": "In this work, we introduce the first autoregressive framework for real-time, audio-driven portrait animation, a.k.a, talking head. Beyond the challenge of lengthy animation times, a critical challenge in realistic talking head generation lies in preserving the natural movement of diverse body parts. To this end, we propose Teller, the first streaming audio-driven protrait animation framework with autoregressive motion generation. Specifically, Teller first decomposes facial and body detail animation into two components: Facial Motion Latent Generation (FMLG) based on an autoregressive transfromer, and movement authenticity refinement using a Efficient Temporal Module (ETM).Concretely, FMLG employs a Residual VQ model to map the facial motion latent from the implicit keypoint-based model into discrete motion tokens, which are then temporally sliced with audio embeddings. This enables the AR tranformer to learn real-time, stream-based mappings from audio to motion.Furthermore, Teller incorporate ETM to capture finer motion details. This module ensures the physical consistency of body parts and accessories, such as neck muscles and earrings, improving the realism of these movements.Teller is designed to be efficient, surpassing the inference speed of diffusion-based models (Hallo 20.93s vs. Teller 0.92s for one second video generation), and achieves a real-time streaming performance of up to 25 FPS. Extensive experiments demonstrate that our method outperforms recent audio-driven portrait animation models, especially in small movements, as validated by human evaluations with a significant margin in quality and realism",
    "checked": true,
    "id": "5712848180898144f891b6dbf192ae744419b292",
    "semantic_title": "teller: real-time streaming audio-driven portrait animation with autoregressive motion generation",
    "citation_count": 5,
    "authors": [
      "Dingcheng Zhen",
      "Shunshun Yin",
      "Shiyang Qin",
      "Hou Yi",
      "Ziwei Zhang",
      "Siyuan Liu",
      "Gan Qi",
      "Ming Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Saunders_GASP_Gaussian_Avatars_with_Synthetic_Priors_CVPR_2025_paper.html": {
    "title": "GASP: Gaussian Avatars with Synthetic Priors",
    "volume": "main",
    "abstract": "Gaussian Splatting has changed the game for real-time photo-realistic rendering. One of the most popular applications of Gaussian Splatting is to create animatable avatars, known as Gaussian Avatars. Recent works have pushed the boundaries of quality and rendering efficiency but suffer from two main limitations. Either they require expensive multi-camera rigs to produce avatars with free-view rendering, or they can be trained with a single camera but only rendered at high quality from this fixed viewpoint. An ideal model would be trained using a short monocular video or image from available hardware, such as a webcam, and rendered from any view. To this end, we propose GASP: Gaussian Avatars with Synthetic Priors. To overcome the limitations of existing datasets, we exploit the pixel-perfect nature of synthetic data to train a Gaussian Avatar prior. By fitting this prior model to a single photo or videoand fine-tuning it, we get a high-quality Gaussian Avatar, which supports 360^\\circ rendering. Our prior is only required for fitting, not inference, enabling real-time application. Through our method, we obtain high-quality, animatable Avatars from limited data which can be animated and rendered at 70fps on commercial hardware",
    "checked": true,
    "id": "467105fc31ee613a0b0d5adaec7c19b55aeca2b2",
    "semantic_title": "gasp: gaussian avatars with synthetic priors",
    "citation_count": 2,
    "authors": [
      "Jack Saunders",
      "Charlie Hewitt",
      "Yanan Jian",
      "Marek Kowalski",
      "Tadas Baltrusaitis",
      "Yiye Chen",
      "Darren Cosker",
      "Virginia Estellers",
      "Nicholas Gyd√©",
      "Vinay P. Namboodiri",
      "Benjamin E. Lundell"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Q-PART_Quasi-Periodic_Adaptive_Regression_with_Test-time_Training_for_Pediatric_Left_CVPR_2025_paper.html": {
    "title": "Q-PART: Quasi-Periodic Adaptive Regression with Test-time Training for Pediatric Left Ventricular Ejection Fraction Regression",
    "volume": "main",
    "abstract": "In this work, we address the challenge of adaptive pediatric Left Ventricular Ejection Fraction (LVEF) assessment. While Test-time Training (TTT) approaches show promise for this task, they suffer from two significant limitations. Existing TTT works are primarily designed for classification tasks rather than continuous value regression, and they lack mechanisms to handle the quasi-periodic nature of cardiac signals. To tackle these issues, we propose a novel Quasi-Periodic Adaptive Regression with Test-time Training (Q-PART) framework. In the training stage, the proposed Quasi-Period Network decomposes the echocardiogram into periodic and aperiodic components within latent space by combining parameterized helix trajectories with Neural Controlled Differential Equations. During inference, our framework further employs a variance minimization strategy across image augmentations that simulate common quality issues in echocardiogram acquisition, along with differential adaptation rates for periodic and aperiodic components. Theoretical analysis is provided to demonstrate that our variance minimization objective effectively bounds the regression error under mild conditions. Furthermore, extensive experiments across three pediatric age groups demonstrate that Q-PART not only significantly outperforms existing approaches in pediatric LVEF prediction, but also exhibits strong clinical screening capability with high mAUROC scores (up to 0.9747) and maintains gender-fair performance across all metrics, validating its robustness and practical utility in pediatric echocardiography analysis",
    "checked": true,
    "id": "8a5c213b144c7d815e3b75e028fa90f649e773f3",
    "semantic_title": "q-part: quasi-periodic adaptive regression with test-time training for pediatric left ventricular ejection fraction regression",
    "citation_count": 0,
    "authors": [
      "Jie Liu",
      "Tiexin Qin",
      "Hui Liu",
      "Yilei Shi",
      "Lichao Mou",
      "Xiao Xiang Zhu",
      "Shiqi Wang",
      "Haoliang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rangwani_Composing_Parts_for_Expressive_Object_Generation_CVPR_2025_paper.html": {
    "title": "Composing Parts for Expressive Object Generation",
    "volume": "main",
    "abstract": "Image composition and generation are processes where the artists need control over various parts of the generated images. However, the current state-of-the-art generation models, like Stable Diffusion, cannot handle fine-grained part-level attributes in the text prompts. Specifically, when additional attribute details are added to the base text prompt, these text-to-image models either generate an image vastly different from the image generated from the base prompt or ignore the attribute details. To mitigate these issues, we introduce PartComposer, a training-free method that enables image generation based on fine-grained part-level attributes specified for objects in the base text prompt. This allows more control for artists and enables novel object compositions by combining distinctive object parts. PartComposer first localizes object parts by denoising the object region from a specific diffusion process. This enables each part token to be localized to the right region. After obtaining part masks, we run a localized diffusion process in each part region based on fine-grained part attributes and combine them to produce the final image. All stages of PartComposer are based on repurposing a pre-trained diffusion model, which enables it to generalize across domains. We demonstrate the effectiveness of part-level control provided by PartComposer through qualitative visual examples and quantitative comparisons with contemporary baselines",
    "checked": true,
    "id": "4a22ecba76e3125fae1fcdacf1c3b75966465b3f",
    "semantic_title": "composing parts for expressive object generation",
    "citation_count": 1,
    "authors": [
      "Harsh Rangwani",
      "Aishwarya Agarwal",
      "Kuldeep Kulkarni",
      "R. Venkatesh Babu",
      "Srikrishna Karanam"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_CarPlanner_Consistent_Auto-regressive_Trajectory_Planning_for_Large-Scale_Reinforcement_Learning_in_CVPR_2025_paper.html": {
    "title": "CarPlanner: Consistent Auto-regressive Trajectory Planning for Large-Scale Reinforcement Learning in Autonomous Driving",
    "volume": "main",
    "abstract": "Trajectory planning is vital for autonomous driving, ensuring safe and efficient navigation in complex environments. While recent learning-based methods, particularly reinforcement learning (RL), have shown promise in specific scenarios, RL planners struggle with training inefficiencies and managing large-scale, real-world driving scenarios.In this paper, we introduce CarPlanner, a Consistent auto-regressive Planner that uses RL to generate multi-modal trajectories. The auto-regressive structure enables efficient large-scale RL training, while the incorporation of consistency ensures stable policy learning by maintaining coherent temporal consistency across time steps. Moreover, CarPlanner employs a generation-selection framework with an expert-guided reward function and an invariant-view module, simplifying RL training and enhancing policy performance.Extensive analysis demonstrates that our proposed RL framework effectively addresses the challenges of training efficiency and performance enhancement, positioning CarPlanner as a promising solution for trajectory planning in autonomous driving.To the best of our knowledge, we are the first to demonstrate that the RL-based planner can surpass both IL- and rule-based state-of-the-arts (SOTAs) on the challenging large-scale real-world dataset nuPlan. Our proposed CarPlanner surpasses RL-, IL-, and rule-based SOTA approaches within this demanding dataset",
    "checked": true,
    "id": "10a5ecd2838e66b3a1f565bcf20a5e8033ef6e14",
    "semantic_title": "carplanner: consistent auto-regressive trajectory planning for large-scale reinforcement learning in autonomous driving",
    "citation_count": 7,
    "authors": [
      "Dongkun Zhang",
      "Jiaming Liang",
      "Ke Guo",
      "Sha Lu",
      "Qi Wang",
      "Rong Xiong",
      "Zhenwei Miao",
      "Yue Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qin_Apply_Hierarchical-Chain-of-Generation_to_Complex_Attributes_Text-to-3D_Generation_CVPR_2025_paper.html": {
    "title": "Apply Hierarchical-Chain-of-Generation to Complex Attributes Text-to-3D Generation",
    "volume": "main",
    "abstract": "Recent text-to-3D generation models have demonstrated remarkable abilities in producing high-quality 3D assets. Despite their great advancements, current models struggle to generate satisfying 3D objects with complex attributes. The difficulty for such complex attributes 3D generation arises from two aspects: (1) existing text-to-3D approaches typically lift text-to-image models to extract semantics via text encoders, while the text encoder exhibits limited comprehension ability for long descriptions, leading to deviated cross-attention focus, subsequently wrong attribute binding in generated results. (2) Objects with complex attributes often exhibit occlusion relationships between different parts, which demands a reasonable generation order as well as explicit disentanglement of different parts to enable structural coherent and attribute following results. Though some works introduce manual efforts to alleviate the above issues, their quality is unstable and highly reliant on manual information. To tackle above problems, we propose a automated method Hierarchical-Chain-of-Generation (HCoG). It leverages a large language model to analyze the long description, decomposes it into several blocks representing different object parts, and organizes an optimal generation order from in to out according to the occlusion relationship between parts, turning the whole generation process into a hierarchical chain. For optimization within each block, we first generate the necessary components coarsely, then bind their attributes precisely by target region localization and corresponding 3D Gaussian kernel optimization. For optimization between blocks, we introduce Gaussian Extension and Label Elimination to seamlessly generate new parts by extending new Gaussian kernels, re-assigning semantic labels, and eliminating unnecessary kernels, ensuring that only relevant parts are added without disrupting previously optimized parts. Experiments validate HCoG's effectiveness in handling complex attributes 3D assets and witnesses high-quality results. The code is available at https://github.com/Wakals/GASCOL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Qin",
      "Zhu Xu",
      "Yang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_PersonaHOI_Effortlessly_Improving_Face_Personalization_in_Human-Object_Interaction_Generation_CVPR_2025_paper.html": {
    "title": "PersonaHOI: Effortlessly Improving Face Personalization in Human-Object Interaction Generation",
    "volume": "main",
    "abstract": "We introduce PersonaHOI, a training- and tuning-free framework that fuses a general StableDiffusion model with a personalized face diffusion (PFD) model to generate identity-consistent human-object interaction (HOI) images. While existing PFD models have advanced significantly, they often overemphasize facial features at the expense of full-body coherence, PersonaHOI introduces an additional StableDiffusion (SD) branch guided by HOI-oriented text inputs. By incorporating cross-attention constraints in the PFD branch and spatial merging at both latent and residual levels, PersonaHOI preserves personalized facial details while ensuring interactive non-facial regions. Experiments, validated by a novel interaction alignment metric, demonstrate the superior realism and scalability of PersonaHOI, establishing a new standard for practical personalized face with HOI generation. Code is available at https://github.com/JoyHuYY1412/PersonaHOI",
    "checked": true,
    "id": "692526e8f1c02a0dbb11e5499cd9956e45ee1d1b",
    "semantic_title": "personahoi: effortlessly improving face personalization in human-object interaction generation",
    "citation_count": 0,
    "authors": [
      "Xinting Hu",
      "Haoran Wang",
      "Jan Eric Lenssen",
      "Bernt Schiele"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yi_Video-Panda_Parameter-efficient_Alignment_for_Encoder-free_Video-Language_Models_CVPR_2025_paper.html": {
    "title": "Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models",
    "volume": "main",
    "abstract": "We present an efficient encoder-free approach for video-language understanding that achieves competitive performance while significantly reducing computational overhead. Current video-language models typically rely on heavyweight image encoders (300M-1.1B parameters) or video encoders (1B-1.4B parameters), creating a substantial computational burden when processing multi-frame videos. Our method introduces a novel Spatio-Temporal Alignment Block (STAB) that directly processes video inputs without requiring pre-trained encoders while using only 45M parameters for visual processing - at least a 6.5x reduction compared to traditional approaches. The STAB architecture combines Local Spatio-Temporal Encoding for fine-grained feature extraction, efficient spatial downsampling through learned attention and separate mechanisms for modeling frame-level and video-level relationships. Our model achieves comparable or superior performance to encoder-based approaches for open-ended video question answering on standard benchmarks. The fine-grained video question-answering evaluation demonstrates our model's effectiveness, outperforming the encoder-based approaches Video-ChatGPT and Video-LLaVA in key aspects like correctness and temporal understanding. Extensive ablation studies validate our architectural choices and demonstrate the effectiveness of our spatio-temporal modeling approach while achieving 3-4x faster processing speeds than previous methods. Code is available at https://jh-yi.github.io/Video-Panda",
    "checked": true,
    "id": "c667af5b80352c4d3c217a9400fa7ffcbadcc94f",
    "semantic_title": "video-panda: parameter-efficient alignment for encoder-free video-language models",
    "citation_count": 0,
    "authors": [
      "Jinhui Yi",
      "Syed Talal Wasim",
      "Yanan Luo",
      "Muzammal Naseer",
      "Juergen Gall"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_COSMIC_Clique-Oriented_Semantic_Multi-space_Integration_for_Robust_CLIP_Test-Time_Adaptation_CVPR_2025_paper.html": {
    "title": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP Test-Time Adaptation",
    "volume": "main",
    "abstract": "Recent vision-language models (VLMs) face significant challenges in test-time adaptation to novel domains. While cache-based methods show promise by leveraging historical information, they struggle with both caching unreliable feature-label pairs and indiscriminately using single-class information during querying, significantly compromising adaptation accuracy. To address these limitations, we propose COSMIC (\\underline C lique-\\underline O riented \\underline S emantic \\underline M ulti-space \\underline I ntegration for \\underline C LIP), a robust test-time adaptation framework that enhances adaptability through multi-granular, cross-modal semantic caching and graph-based querying mechanisms. Our framework introduces two key innovations: Dual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual Semantics Graph constructs complementary semantic spaces by incorporating textual features, coarse-grained CLIP features, and fine-grained DINOv2 features to capture rich semantic relationships. Building upon these dual graphs, the Clique Guided Hyper-class component leverages structured class relationships to enhance prediction robustness through correlated class selection. Extensive experiments demonstrate COSMIC's superior performance across multiple benchmarks, achieving significant improvements over state-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on cross-domain generation with CLIP RN-50",
    "checked": true,
    "id": "887d7b9bbce38f2df1d5b583a51bc59ca04fba0c",
    "semantic_title": "cosmic: clique-oriented semantic multi-space integration for robust clip test-time adaptation",
    "citation_count": 1,
    "authors": [
      "Fanding Huang",
      "Jingyan Jiang",
      "Qinting Jiang",
      "Hebei Li",
      "Faisal Nadeem Khan",
      "Zhi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MonoSplat_Generalizable_3D_Gaussian_Splatting_from_Monocular_Depth_Foundation_Models_CVPR_2025_paper.html": {
    "title": "MonoSplat: Generalizable 3D Gaussian Splatting from Monocular Depth Foundation Models",
    "volume": "main",
    "abstract": "Recent advances in generalizable 3D Gaussian Splatting have demonstrated promising results in real-time high-fidelity rendering without per-scene optimization, yet existing approaches still struggle to handle unfamiliar visual content during inference on novel scenes due to limited generalizability. To address this challenge, we introduce MonoSplat, a novel framework that leverages rich visual priors from pre-trained monocular depth foundation models for robust Gaussian reconstruction. Our approach consists of two key components: a Mono-Multi Feature Adapter that transforms monocular features into multi-view representations, coupled with an Integrated Gaussian Prediction module that effectively fuses both feature types for precise Gaussian generation. Through the Adapter's lightweight attention mechanism, features are seamlessly aligned and aggregated across views while preserving valuable monocular priors, enabling the Prediction module to generate Gaussian primitives with accurate geometry and appearance. Through extensive experiments on diverse real-world datasets, we convincingly demonstrate that MonoSplat achieves superior reconstruction quality and generalization capability compared to existing methods while maintaining computational efficiency with minimal trainable parameters. Codes are available at \\href https://github.com/CUHK-AIM-Group/MonoSplat https://github.com/CUHK-AIM-Group/MonoSplat",
    "checked": true,
    "id": "5953c00375ab99937d8e0ecf276746036ebc33df",
    "semantic_title": "monosplat: generalizable 3d gaussian splatting from monocular depth foundation models",
    "citation_count": 1,
    "authors": [
      "Yifan Liu",
      "Keyu Fan",
      "Weihao Yu",
      "Chenxin Li",
      "Hao Lu",
      "Yixuan Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Hybrid_Global-Local_Representation_with_Augmented_Spatial_Guidance_for_Zero-Shot_Referring_CVPR_2025_paper.html": {
    "title": "Hybrid Global-Local Representation with Augmented Spatial Guidance for Zero-Shot Referring Image Segmentation",
    "volume": "main",
    "abstract": "Recent advances in zero-shot referring image segmentation (RIS), driven by models such as the Segment Anything Model (SAM) and CLIP, have made substantial progress in aligning visual and textual information. Despite these successes, the extraction of precise and high-quality mask region representations remains a critical challenge, limiting the full potential of RIS tasks. In this paper, we introduce a training-free, hybrid global-local feature extraction approach that integrates detailed mask-specific features with contextual information from the surrounding area, enhancing mask region representation. To further strengthen alignment between mask regions and referring expressions, we propose a spatial guidance augmentation strategy that improves spatial coherence, which is essential for accurately localizing described areas. By incorporating multiple spatial cues, this approach facilitates more robust and precise referring segmentation. Extensive experiments on standard RIS benchmarks demonstrate that our method significantly outperforms existing zero-shot referring segmentation models, achieving substantial performance gains. We believe our approach advances RIS tasks and establishes a versatile framework for region-text alignment, offering broader implications for cross-modal understanding and interaction. The code will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting Liu",
      "Siyuan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_SOLVE_Synergy_of_Language-Vision_and_End-to-End_Networks_for_Autonomous_Driving_CVPR_2025_paper.html": {
    "title": "SOLVE: Synergy of Language-Vision and End-to-End Networks for Autonomous Driving",
    "volume": "main",
    "abstract": "The integration of Vision-Language Models (VLMs) into autonomous driving systems has shown promise in addressing key challenges such as learning complexity, interpretability, and common-sense reasoning. However, existing approaches often struggle with efficient integration and real-time decision-making due to computational demands. In this paper, we introduce SOLVE, an innovative framework that synergizes VLMs with end-to-end (E2E) models to enhance autonomous vehicle planning. Our approach emphasizes knowledge sharing at the feature level through a shared visual encoder, enabling comprehensive interaction between VLM and E2E components. We propose a Trajectory Chain-of-Thought (T-CoT) paradigm, which progressively refines trajectory predictions, reducing uncertainty and improving accuracy. By employing a temporal decoupling strategy, SOLVE achieves efficient asynchronous cooperation, aligning high-quality VLM outputs with E2E real-time performance. Evaluated on the nuScenes dataset, our method demonstrates significant improvements in trajectory prediction accuracy, paving the way for more robust and reliable autonomous driving systems",
    "checked": true,
    "id": "1117e1f87be0eae172d1c0b53f620d9ec37517a6",
    "semantic_title": "solve: synergy of language-vision and end-to-end networks for autonomous driving",
    "citation_count": 4,
    "authors": [
      "Xuesong Chen",
      "Linjiang Huang",
      "Tao Ma",
      "Rongyao Fang",
      "Shaoshuai Shi",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Du_Shift_the_Lens_Environment-Aware_Unsupervised_Camouflaged_Object_Detection_CVPR_2025_paper.html": {
    "title": "Shift the Lens: Environment-Aware Unsupervised Camouflaged Object Detection",
    "volume": "main",
    "abstract": "Camouflaged Object Detection (COD) seeks to distinguish objects from their highly similar backgrounds. Existing work has essentially focused on isolating camouflaged objects from the environment, demonstrating ever-improving performance but at the cost of extensive annotations and complex optimizations. In this paper, we diverge from this paradigm and shift the lens to isolating the salient environment from the camouflaged object. We introduce EASE, an Environment-Aware unSupErvised COD framework that identifies the environment by referencing an environment prototype library and detects camouflaged objects by inverting the retrieved environmental features. Specifically, our approach (DiffPro) uses large multimodal models, diffusion models, and vision-foundation models to construct the environment prototype library. To retrieve environments from the library and refrain from confusing foreground and background, we incorporate three retrieval schemes: Kernel Density Estimation-based Adaptive Threshold (KDE-AT), Global-to-Local pixel-level retrieval (G2L), and Self-Retrieval (SR). Our experiments demonstrate significant improvements over current unsupervised methods, with EASE achieving an average gain of over 10% on the COD10K dataset. When integrated with SAM, EASE surpasses prompt-based segmentation approaches and performs competitively with state-of-the-art fully-supervised methods. Code is available at https://github.com/xiaohainku/EASE",
    "checked": true,
    "id": "9ffe2dac38933b2edc81c584ed5588574960e82c",
    "semantic_title": "shift the lens: environment-aware unsupervised camouflaged object detection",
    "citation_count": 2,
    "authors": [
      "Ji Du",
      "Fangwei Hao",
      "Mingyang Yu",
      "Desheng Kong",
      "Jiesheng Wu",
      "Bin Wang",
      "Jing Xu",
      "Ping Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Probability_Density_Geodesics_in_Image_Diffusion_Latent_Space_CVPR_2025_paper.html": {
    "title": "Probability Density Geodesics in Image Diffusion Latent Space",
    "volume": "main",
    "abstract": "Diffusion models indirectly estimate the probability density over a data space, which can be used to study its structure. In this work, we show that geodesics can be computed in diffusion latent space, where the norm induced by the spatially-varying inner product is inversely proportional to the probability density. In this formulation, a path that traverses a high density (that is, probable) region of image latent space is shorter than the equivalent path through a low density region. We present algorithms for solving the associated initial and boundary value problems and show how to compute the probability density along the path and the geodesic distance between two points. Using these techniques, we analyze how closely video clips approximate geodesics in a pre-trained image diffusion space. Finally, we demonstrate how these techniques can be applied to training-free image sequence interpolation and extrapolation, given a pre-trained image diffusion model",
    "checked": true,
    "id": "abee017622e57b96e0f65033c0ccfab460da1c3a",
    "semantic_title": "probability density geodesics in image diffusion latent space",
    "citation_count": 1,
    "authors": [
      "Qingtao Yu",
      "Jaskirat Singh",
      "Zhaoyuan Yang",
      "Peter Henry Tu",
      "Jing Zhang",
      "Hongdong Li",
      "Richard Hartley",
      "Dylan Campbell"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_High-quality_Point_Cloud_Oriented_Normal_Estimation_via_Hybrid_Angular_and_CVPR_2025_paper.html": {
    "title": "High-quality Point Cloud Oriented Normal Estimation via Hybrid Angular and Euclidean Distance Encoding",
    "volume": "main",
    "abstract": "The proliferation of Light Detection and Ranging (LiDAR) technology has facilitated the acquisition of three-dimensional point clouds, which are integral to applications in VR, AR, and Digital Twin. Oriented normals, critical for 3D reconstruction and scene analysis, cannot be directly extracted from scenes using LiDAR due to its operational principles. Previous traditional or learning-based methods are prone to inaccuracies due to uneven distribution and noise due to the dependence on local geometry features. This paper addresses the challenge of estimating oriented point normals by introducing a point cloud normal estimation framework via hybrid angular and Euclidean distance encoding (HAE). Our method overcomes the limitations of local geometric information by combining angular and Euclidean spaces to extract features from both point cloud coordinates and light rays, leading to more accurate normal estimation. The core of our network consists of an angular distance encoding module, which leverages both ray directions and point coordinates for unoriented normal refinement, and a ray feature fusion module for normal orientation, that is robust to noise. We also provide a point cloud dataset with ground truth normals, generated a virtual scanner, which reflects real scanning distributions and noise profiles",
    "checked": true,
    "id": "138d4441107ee2d62001323b07a98f04743844a7",
    "semantic_title": "high-quality point cloud oriented normal estimation via hybrid angular and euclidean distance encoding",
    "citation_count": 0,
    "authors": [
      "Yuanqi Li",
      "Jingcheng Huang",
      "Hongshen Wang",
      "Peiyuan Lv",
      "Yansong Liu",
      "Jiuming Zheng",
      "Jie Guo",
      "Yanwen Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_DriveScape_High-Resolution_Driving_Video_Generation_by_Multi-View_Feature_Fusion_CVPR_2025_paper.html": {
    "title": "DriveScape: High-Resolution Driving Video Generation by Multi-View Feature Fusion",
    "volume": "main",
    "abstract": "Recent advancements in generative models offer promising solutions for synthesizing realistic driving videos, aiding in training autonomous driving perception models. However, existing methods often struggle with high-resolution multi-view generation, mainly due to the significant memory and computational overhead caused by simultaneously inputting multi-view videos into denoising diffusion models.In this paper, we propose a driving video generation framework based on multi-view feature fusion named DriveScape for multi-view 3D condition-guided video generation. We introduce a Bi-Directional Modulated Transformer (BiMoT) module to encode, fuse and inject multi-view features along with various 3D road structures and objects, which enables high-resolution multi-view generation. Consequently, our approach allows precise control over video generation, greatly enhancing realism and providing a robust solution for creating high-quality, multi-view driving videos.Our framework achieves state-of-the-art results on the nuScenes dataset, demonstrating impressive generative quality metrics with an FID score of 8.34 and an FVD score of 76.39, as well as superior performance across various perception tasks. This lays the foundation for more accurate environment simulation in autonomous driving. We plan to make our code and pre-trained model publicly available.Please refer to index.html webpage in the supplementary materials for more visualization results",
    "checked": true,
    "id": "d2ecc05f3790e77e674fe3f3d6d45bc80beb583c",
    "semantic_title": "drivescape: high-resolution driving video generation by multi-view feature fusion",
    "citation_count": 1,
    "authors": [
      "Wei Wu",
      "Xi Guo",
      "Weixuan Tang",
      "Tingxuan Huang",
      "Chiyu Wang",
      "Chenjing Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tybl_Training-free_Neural_Architecture_Search_through_Variance_of_Knowledge_of_Deep_CVPR_2025_paper.html": {
    "title": "Training-free Neural Architecture Search through Variance of Knowledge of Deep Network Weights",
    "volume": "main",
    "abstract": "Deep learning has revolutionized computer vision, but it achieved its tremendous success using deep network architectures which are mostly hand-crafted and therefore likely suboptimal. Neural Architecture Search (NAS) aims to bridge this gap by following a well-defined optimization paradigm which systematically looks for the best architecture, given objective criterion such as maximal classification accuracy. The main limitation of NAS is however its astronomical computational cost, as it typically requires training each candidate network architecture from scratch.In this paper, we aim to alleviate this limitation by proposing a novel training-free proxy for image classification accuracy based on Fisher Information. The proposed proxy has a strong theoretical background in statistics and it allows estimating expected image classification accuracy of a given deep network without training the network, thus significantly reducing computational cost of standard NAS algorithms. Our training-free proxy achieves state-of-the-art results on three public datasets and in two search spaces, both when evaluated using previously proposed metrics, as well as using a new metric that we propose which we demonstrate is more informative for practical NAS applications. The source code is publicly available",
    "checked": true,
    "id": "9560d1a64b8ae67f6fb5f7b0c52d02a0e08e17e6",
    "semantic_title": "training-free neural architecture search through variance of knowledge of deep network weights",
    "citation_count": 0,
    "authors": [
      "Ondrej Tybl",
      "Lukas Neumann"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Every_SAM_Drop_Counts_Embracing_Semantic_Priors_for_Multi-Modality_Image_CVPR_2025_paper.html": {
    "title": "Every SAM Drop Counts: Embracing Semantic Priors for Multi-Modality Image Fusion and Beyond",
    "volume": "main",
    "abstract": "Multi-modality image fusion, particularly infrared and visible, plays a crucial role in integrating diverse modalities to enhance scene understanding. Although early research prioritized visual quality, preserving fine details and adapting to downstream tasks remains challenging. Recent approaches attempt task-specific design but rarely achieve \"The Best of Both Worlds\" due to inconsistent optimization goals. To address these issues, we propose a novel method that leverages the semantic knowledge from the Segment Anything Model (SAM) to grow the quality of fusion results and enable downstream task adaptability, namely SAGE. Specifically, we design a Semantic Persistent Attention (SPA) Module that efficiently maintains source information via the persistent repository while extracting high-level semantic priors from SAM. More importantly, to eliminate the impractical dependence on SAM during inference, we introduce a bi-level optimization-driven distillation mechanism with triplet losses, which allow the student network to effectively extract knowledge. Extensive experiments show that our method achieves a balance between high-quality visual results and downstream task adaptability while maintaining practical deployment efficiency. The code is available at https://github.com/RollingPlain/SAGE_IVIF",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanyao Wu",
      "Haoyu Liu",
      "Hongming Fu",
      "Yichuan Peng",
      "Jinyuan Liu",
      "Xin Fan",
      "Risheng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_EgoLife_Towards_Egocentric_Life_Assistant_CVPR_2025_paper.html": {
    "title": "EgoLife: Towards Egocentric Life Assistant",
    "volume": "main",
    "abstract": "We introduce EgoLife, a project to develop an egocentric life assistant that accompanies and enhances personal efficiency through AI-powered wearable glasses. To lay the foundation for this assistant, we conducted a comprehensive data collection study where six participants lived together for one week, continuously recording their daily activities - including discussions, shopping, cooking, socializing, and entertainment - using AI glasses for multimodal egocentric video capture, along with synchronized third-person-view video references. This effort resulted in the EgoLife Dataset, a comprehensive 300-hour egocentric, interpersonal, multiview, and multimodal daily life dataset with intensive annotation. Leveraging this dataset, we introduce EgoLifeQA, a suite of long-context, life-oriented question-answering tasks designed to provide meaningful assistance in daily life by addressing practical questions such as recalling past relevant events, monitoring health habits, and offering personalized recommendations. To address the key technical challenges of (1) developing robust visual-audio models for egocentric data, (2) enabling identity recognition, and (3) facilitating long-context question answering over extensive temporal information, we introduce EgoButler, an integrated system comprising EgoGPT and EgoRAG. EgoGPT is an omni-modal model trained on egocentric datasets, achieving state-of-the-art performance on egocentric video understanding. EgoRAG is a retrieval-based component that supports answering ultra-long-context questions. Our experimental studies verify their working mechanisms and reveal critical factors and bottlenecks, guiding future improvements. By releasing our datasets, models, and benchmarks, we aim to stimulate further research in egocentric AI assistants",
    "checked": true,
    "id": "42fcf9ee97c13235f5b8003725395a65beaa6cbb",
    "semantic_title": "egolife: towards egocentric life assistant",
    "citation_count": 12,
    "authors": [
      "Jingkang Yang",
      "Shuai Liu",
      "Hongming Guo",
      "Yuhao Dong",
      "Xiamengwei Zhang",
      "Sicheng Zhang",
      "Pengyun Wang",
      "Zitang Zhou",
      "Binzhu Xie",
      "Ziyue Wang",
      "Bei Ouyang",
      "Zhengyu Lin",
      "Marco Cominelli",
      "Zhongang Cai",
      "Bo Li",
      "Yuanhan Zhang",
      "Peiyuan Zhang",
      "Fangzhou Hong",
      "Joerg Widmer",
      "Francesco Gringoli",
      "Lei Yang",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xing_RAEncoder_A_Label-Free_Reversible_Adversarial_Examples_Encoder_for_Dataset_Intellectual_CVPR_2025_paper.html": {
    "title": "RAEncoder: A Label-Free Reversible Adversarial Examples Encoder for Dataset Intellectual Property Protection",
    "volume": "main",
    "abstract": "Reversible Adversarial Examples (RAE) are designed to protect the intellectual property of datasets. Such examples can function as imperceptible adversarial examples to erode the model performance of unauthorized users while allowing authorized users to remove the adversarial perturbations and recover the original samples for normal model training. With the rise of Self-Supervised Learning (SSL), an increasing number of unlabeled datasets and pre-trained encoders are available in the community. However, existing RAE methods not only rely on well-labeled datasets for training Supervised Learning (SL) models but also exhibit poor adversarial transferability when attacking SSL pre-trained encoders. To address these challenges, we propose RAEncoder, the first framework for RAEs without the need for labeled samples. RAEncoder aims to generate universal adversarial perturbations by targeting SSL pre-trained encoders. Unlike traditional RAE approaches, the pre-trained encoder outputs the feature distribution of the protected dataset rather than classification labels, enhancing both the attack success rate and transferability of RAEs. Extensive experiments are conducted on six pre-trained encoders and four SL models, covering aspects such as imperceptibility and transferability. Our results demonstrate that RAEncoder effectively protects unlabeled datasets from malicious infringements. Additional robustness experiments further confirm the security of RAEncoder in practical application scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Xing",
      "Zhuo Tian",
      "Xuefeng Fan",
      "Xiaoyi Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_BrepGiff_Lightweight_Generation_of_Complex_B-rep_with_3D_GAT_Diffusion_CVPR_2025_paper.html": {
    "title": "BrepGiff: Lightweight Generation of Complex B-rep with 3D GAT Diffusion",
    "volume": "main",
    "abstract": "Despite advancements in Computer-Aided-Design (CAD) generation, direct generation of complex Boundary Representation (B-rep) CAD models remains challenging. This difficulty arises from the parametric nature of B-rep data, complicating the encoding and generation of its geometric and topological information. To address this, we introduce BrepGiff, a lightweight generation approach for high-quality and complex B-rep model based on 3D Graph Diffusion. First, we transfer B-rep models into 3D graphs representation. Specifically, BrepGiff extracts and integrates topological and geometric features to construct a 3D graph where nodes correspond to face centroids in 3D space, preserving adjacency and degree information. Geometric features are derived by sampling points in the UV domain and extracting face and edge features. Then, BrepGiff applies a Graph Attention Network (GAT) to enforce topological constraints from local to global during the degree-guided diffusion process. With the 3D graph representation and efficient diffusion process, our method significantly reduces the computational cost and improves the quality, thus achieving lightweight generation of complex models. Experiments show that BrepGiff can generate complex B-rep models (>100 faces) using only 2 RTX4090 GPUs, achieving state-of-the-art performance in B-rep generation",
    "checked": true,
    "id": "efb93c0a49f2eec36381ff7d88043a0e732ba627",
    "semantic_title": "brepgiff: lightweight generation of complex b-rep with 3d gat diffusion",
    "citation_count": 0,
    "authors": [
      "Hao Guo",
      "Xiaoshui Huang",
      "Hao jiacheng",
      "Yunpeng Bai",
      "Hongping Gan",
      "Yilei Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Towards_Fine-Grained_Interpretability_Counterfactual_Explanations_for_Misclassification_with_Saliency_Partition_CVPR_2025_paper.html": {
    "title": "Towards Fine-Grained Interpretability: Counterfactual Explanations for Misclassification with Saliency Partition",
    "volume": "main",
    "abstract": "Attribution-based explanation techniques capture key patterns to enhance visual interpretability. However, these patterns often lack the granularity needed for insight in fine-grained tasks, particularly in cases of model misclassification, where explanations may be insufficiently detailed. To address this limitation, we propose a fine-grained counterfactual explanation framework that generates both object-level and part-level interpretability, addressing two fundamental questions: (1) which fine-grained features contribute to model misclassification, and (2) where dominant local features influence counterfactual adjustments. Our approach yields explainable counterfactuals in a non-generative manner by quantifying similarity and weighting component contributions within regions of interest between correctly classified and misclassified samples. Furthermore, we introduce an importance-isolation module grounded in Shapley value contributions, isolating features with region-specific relevance. Extensive experiments demonstrate the superiority of our approach in capturing more granular, intuitively meaningful regions, surpassing fine-grained methods",
    "checked": true,
    "id": "ec3fb8889884ee52645d5e41e7e519e3dabb11da",
    "semantic_title": "towards fine-grained interpretability: counterfactual explanations for misclassification with saliency partition",
    "citation_count": 0,
    "authors": [
      "Lintong Zhang",
      "Kang Yin",
      "Seong-Whan Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Song_Prior-free_3D_Object_Tracking_CVPR_2025_paper.html": {
    "title": "Prior-free 3D Object Tracking",
    "volume": "main",
    "abstract": "In this paper, we introduce a novel, truly prior-free 3D object tracking method that operates without given any model or training priors. Unlike existing methods that typically require pre-defined 3D models or specific training datasets as priors, which limit their applicability, our method is free from these constraints. Our method consists of a geometry generation module and a pose optimization module. Its core idea is to enable these two modules to automatically and iteratively enhance each other, thereby gradually building all the necessary information for the tracking task. We thus call the method as Bidirectional Iterative Tracking(BIT). The geometry generation module starts without priors and gradually generates high-precision mesh models for tracking, while the pose optimization module generates additional data during object tracking to further refine the generated models. Moreover, the generated 3D models can be stored and easily reused, allowing for seamless integration into various other tracking systems, not just our methods. Experimental results demonstrate that BIT outperforms many existing methods, even those that extensively utilize prior knowledge, while BIT does not rely on such information. Additionally, the generated 3D models deliver results comparable to actual 3D models, highlighting their superior and innovative qualities. The code is available at https://github.com/songxiuqiang/BIT.git",
    "checked": true,
    "id": "68de7ab954a39b185e07dda4f4eebce512e4527b",
    "semantic_title": "prior-free 3d object tracking",
    "citation_count": 0,
    "authors": [
      "Xiuqiang Song",
      "Li Jin",
      "Zhengxian Zhang",
      "Jiachen Li",
      "Fan Zhong",
      "Guofeng Zhang",
      "Xueying Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_LatentHOI_On_the_Generalizable_Hand_Object_Motion_Generation_with_Latent_CVPR_2025_paper.html": {
    "title": "LatentHOI: On the Generalizable Hand Object Motion Generation with Latent Hand Diffusion",
    "volume": "main",
    "abstract": "Current research on generating 3D hand-object interaction motion primarily focuses on in-domain objects. Generalization to unseen objects is essential for practical applications, yet it remains both challenging and largely unexplored.In this paper, we propose LatentHOI, a novel approach designed to tackle the challenges of generalizing hand-object interaction to unseen objects.Our main insight lies in decoupling high-level temporal motion from fine-grained spatial hand-object interactions with a latent diffusion model coupled with a Grasping Variational Autoencoder (GraspVAE). This configuration not only enhances the conditional dependency between spatial grasp and temporal motion but also improves data utilization and reduces overfitting through regularization in the latent space. We conducted extensive experiments in an unseen-object setting on both single-hand grasping and bi-manual motion datasets, including GRAB, DexYCB, and OakInk.Quantitative and qualitative evaluations demonstrate that our method significantly enhances the realism and physical plausibility of generated motions for unseen objects, both in single and bimanual manipulations, compared to the state-of-the-art",
    "checked": true,
    "id": "eb8143981f7270ce5027df25dc69af0cca586071",
    "semantic_title": "latenthoi: on the generalizable hand object motion generation with latent hand diffusion",
    "citation_count": 1,
    "authors": [
      "Muchen Li",
      "Sammy Christen",
      "Chengde Wan",
      "Yujun Cai",
      "Renjie Liao",
      "Leonid Sigal",
      "Shugao Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Progressive_Correspondence_Regenerator_for_Robust_3D_Registration_CVPR_2025_paper.html": {
    "title": "Progressive Correspondence Regenerator for Robust 3D Registration",
    "volume": "main",
    "abstract": "Obtaining enough high-quality correspondences is crucial for robust registration. Existing correspondence refinement methods mostly follow the paradigm of outlier removal, which either fails to correctly identify the accurate correspondences under extreme outlier ratios, or select too few correct correspondences to support robust registration. To address this challenge, we propose a novel approach named Regor, which is a progressive correspondence regenerator that generates higher-quality matches whist sufficiently robust for numerous outliers. In each iteration, we first apply prior-guided local grouping and generalized mutual matching to generate the local region correspondences. A powerful center-aware three-point consistency is then presented to achieve local correspondence correction, instead of removal. Further, we employ global correspondence refinement to obtain accurate correspondences from a global perspective. Through progressive iterations, this process yields a large number of high-quality correspondences. Extensive experiments on both indoor and outdoor datasets demonstrate that the proposed Regor significantly outperforms existing outlier removal techniques. More critically, our approach obtain 10 times more correct correspondences than outlier removal methods. As a result, our method is able to achieve robust registration even with weak features. The code is available at https://github.com/GuiyuZhao/Regor",
    "checked": true,
    "id": "2609a75f8f187c53ec3246c860115818193cb014",
    "semantic_title": "progressive correspondence regenerator for robust 3d registration",
    "citation_count": 0,
    "authors": [
      "Guiyu Zhao",
      "Sheng Ao",
      "Ye Zhang",
      "Kai Xu",
      "Yulan Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Cross-Modal_3D_Representation_with_Multi-View_Images_and_Point_Clouds_CVPR_2025_paper.html": {
    "title": "Cross-Modal 3D Representation with Multi-View Images and Point Clouds",
    "volume": "main",
    "abstract": "The advancement of 3D understanding and representation is a crucial step for the next phase of autonomous driving, robotics, augmented and virtual reality, 3D gaming and 3D e-commerce products. However, existing 3D semantic representation research has primarily focused on point clouds to perceive 3D objects and scenes, overlooking the rich visual details offered by multi-view images, thereby limiting the potential of 3D semantic representation. This paper introduces OpenView, a novel representation method that integrates both point clouds and multi-view images to form a unified 3D representation. OpenView comprises a unique fusion framework, sequence-independent modeling, a cross-modal fusion encoder, and a progressive hard learning strategy. Our experiments demonstrate that OpenView outperforms the state-of-the-art by 11.5% and 5.5% on the R@1 metric for cross-modal retrieval and the Top-1 metric for zero-shot classification tasks, respectively. Furthermore, we showcase some applications of OpenView: 3D retrieval, 3D captioning and hierarchical data clustering, highlighting its generality in the field of 3D representation learning",
    "checked": true,
    "id": "7f2f822d0271a1be4dd9f1adecbf0b9fb6a3d982",
    "semantic_title": "cross-modal 3d representation with multi-view images and point clouds",
    "citation_count": 0,
    "authors": [
      "Ziyang Zhou",
      "Pinghui Wang",
      "Zi Liang",
      "Haitao Bai",
      "Ruofei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ventura_Chapter-Llama_Efficient_Chaptering_in_Hour-Long_Videos_with_LLMs_CVPR_2025_paper.html": {
    "title": "Chapter-Llama: Efficient Chaptering in Hour-Long Videos with LLMs",
    "volume": "main",
    "abstract": "We address the task of video chaptering, i.e., partitioning a long video timeline into semantic units and generating corresponding chapter titles. While relatively underexplored, automatic chaptering has the potential to enable efficient navigation and content retrieval in long-form videos. In this paper, we achieve strong chaptering performance on hour-long videos by efficiently addressing the problem in the text domain with our \"Chapter-Llama\" framework. Specifically, we leverage a pre-trained large language model (LLM) with large context window, and feed as input (i) speech transcripts and (ii) captions describing video frames, along with their respective timestamps. Given the inefficiency of exhaustively captioning all frames, we propose a lightweight speech-guided frame selection strategy based on speech transcript content, and experimentally demonstrate remarkable advantages. We train the LLM to output timestamps for the chapter boundaries, as well as free-form chapter titles. This simple yet powerful approach scales to processing one-hour long videos in a single forward pass. Our results demonstrate substantial improvements (e.g., 45.3 vs 26.7 F1 score) over the state of the art on the recent VidChapters-7M benchmark. To promote further research, we release our code and models at our project page",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Ventura",
      "Antoine Yang",
      "Cordelia Schmid",
      "G√ºl Varol"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ni_Decompositional_Neural_Scene_Reconstruction_with_Generative_Diffusion_Prior_CVPR_2025_paper.html": {
    "title": "Decompositional Neural Scene Reconstruction with Generative Diffusion Prior",
    "volume": "main",
    "abstract": "Decompositional reconstruction of 3D scenes, with complete shapes and detailed texture of all objects within, is intriguing for downstream applications but remains challenging, particularly with sparse views as input. Recent approaches incorporate semantic or geometric regularization to address this issue, but they suffer significant degradation in underconstrained areas and fail to recover occluded regions. We argue that the key to solving this problem lies in supplementing missing information for these areas. To this end, we propose DP-Recon, which employs diffusion priors in the form of Score Distillation Sampling (SDS) to optimize the neural representation of each individual object under novel views. This provides additional information for the underconstrained areas, but directly incorporating diffusion prior raises potential conflicts between the reconstruction and generative guidance. Therefore, we further introduce a visibility-guided approach to dynamically adjust the per-pixel SDS loss weights. Together these components enhance both geometry and appearance recovery while remaining faithful to input images. Extensive experiments across Replica and ScanNet++ demonstrate that our method significantly outperforms state-of-the-art methods. Notably, it achieves better object reconstruction under 10 views than the baselines under 100 views. Our method enables seamless text-based editing for geometry and appearance through SDS optimization and produces decomposed object meshes with detailed UV maps that support photorealistic Visual effects (VFX) editing. The project page is available at https://dp-recon.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junfeng Ni",
      "Yu Liu",
      "Ruijie Lu",
      "Zirui Zhou",
      "Song-Chun Zhu",
      "Yixin Chen",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Distribution_Prototype_Diffusion_Learning_for_Open-set_Supervised_Anomaly_Detection_CVPR_2025_paper.html": {
    "title": "Distribution Prototype Diffusion Learning for Open-set Supervised Anomaly Detection",
    "volume": "main",
    "abstract": "In Open-set Supervised Anomaly Detection (OSAD), the existing methods typically generate pseudo anomalies to compensate for the scarcity of observed anomaly samples, while overlooking critical priors of normal samples, leading to less effective discriminative boundaries. To address this issue, we propose a Distribution Prototype Diffusion Learning (DPDL) method aimed at enclosing normal samples within a compact and discriminative distribution space. Specifically, we construct multiple learnable Gaussian prototypes to create a latent representation space for abundant and diverse normal samples and learn a Schr\"odinger bridge to facilitate a diffusive transition toward these prototypes for normal samples while steering anomaly samples away. Moreover, to enhance inter-sample separation, we design a dispersion feature learning way in hyperspherical space, which benefits the identification of out-ofdistribution anomalies. Experimental results demonstrate the effectiveness and superiority of our proposed DPDL, achieving state-of-the-art performance on 9 public datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fuyun Wang",
      "Tong Zhang",
      "Yuanzhi Wang",
      "Yide Qiu",
      "Xin Liu",
      "Xu Guo",
      "Zhen Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Learning_Visual_Generative_Priors_without_Text_CVPR_2025_paper.html": {
    "title": "Learning Visual Generative Priors without Text",
    "volume": "main",
    "abstract": "Although text-to-image (T2I) models have recently thrived as visual generative priors, their reliance on high-quality text-image pairs makes scaling up expensive. We argue that grasping the cross-modality alignment is not a necessity for a sound visual generative prior, whose focus should be on texture modeling. Such a philosophy inspires us to study image-to-image (I2I) generation, where models can learn from in-the-wild images in a self-supervised manner. We first develop a pure vision-based training framework, Lumos, and confirm the feasibility and the scalability of learning I2I models. We then find that, as an upstream task of T2I, our I2I model serves as a more foundational visual prior and achieves on-par or better performance than existing T2I models using only 1/10 text-image pairs for fine-tuning. We further demonstrate the superiority of I2I priors over T2I priors on some text-irrelevant vision tasks, like image-to-3D and image-to-video",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuailei Ma",
      "Kecheng Zheng",
      "Ying Wei",
      "Wei Wu",
      "Fan Lu",
      "Yifei Zhang",
      "Chen-Wei Xie",
      "Biao Gong",
      "Jiapeng Zhu",
      "Yujun Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Joint_Scheduling_of_Causal_Prompts_and_Tasks_for_Multi-Task_Learning_CVPR_2025_paper.html": {
    "title": "Joint Scheduling of Causal Prompts and Tasks for Multi-Task Learning",
    "volume": "main",
    "abstract": "Multi-task prompt learning has emerged as a promising technique for fine-tuning pre-trained Vision-Language Models (VLMs) to various downstream tasks. However, existing methods ignore challenges caused by spurious correlations and dynamic task relationships, which may reduce the model performance. To tackle these challenges, we propose JSCPT, a novel approach for Joint Scheduling of Causal Prompts and Tasks to enhance multi-task prompt learning. Specifically, we first design a Multi-Task Vison-Language Prompt (MTVLP) model, which learns task-shared and task-specific vison-language prompts and selects useful prompt features via causal intervention, alleviating spurious correlations. Then, we propose the task-prompt scheduler that models inter-task affinities and assesses the causal effect of prompt features to optimize the multi-task prompt learning process. Finally, we formulate the scheduler and the multi-task prompt learning process as a bi-level optimization problem to optimize prompts and tasks adaptively. In the lower optimization, MTVLP is updated with the scheduled gradient, while in the upper optimization, the scheduler is updated with the implicit gradient. Extensive experiments show the superiority of our proposed JSCPT approach over several baselines in terms of multi-task prompt learning for pre-trained VLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoyang Li",
      "Jianyang Qin",
      "Jinhao Cui",
      "Zeyu Liu",
      "Ning Hu",
      "Qing Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Full-DoF_Egomotion_Estimation_for_Event_Cameras_Using_Geometric_Solvers_CVPR_2025_paper.html": {
    "title": "Full-DoF Egomotion Estimation for Event Cameras Using Geometric Solvers",
    "volume": "main",
    "abstract": "For event cameras, current sparse geometric solvers for egomotion estimation assume that the rotational displacements are known, such as those provided by an IMU. Thus, they can only recover the translational motion parameters. Recovering full-DoF motion parameters using a sparse geometric solver is a more challenging task, and has not yet been investigated. In this paper, we propose several solvers to estimate both rotational and translational velocities within a unified framework. Our method leverages event manifolds induced by line segments. The problem formulations are based on either an incidence relation for lines or a novel coplanarity relation for normal vectors. We demonstrate the possibility of recovering full-DoF egomotion parameters for both angular and linear velocities without requiring extra sensor measurements or motion priors. To achieve efficient optimization, we exploit the Adam framework with a first-order approximation of rotations for quick initialization. Experiments on both synthetic and real-world data demonstrate the effectiveness of our method. The code is available at https://github.com/jizhaox/relpose-event",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ji Zhao",
      "Banglei Guan",
      "Zibin Liu",
      "Laurent Kneip"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_3DGUT_Enabling_Distorted_Cameras_and_Secondary_Rays_in_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "3DGUT: Enabling Distorted Cameras and Secondary Rays in Gaussian Splatting",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) enables efficient reconstruction and high-fidelity real-time rendering of complex scenes on consumer hardware. However, due to its rasterization-based formulation, 3DGS is constrained to ideal pinhole cameras and lacks support for secondary lighting effects. Recent methods address these limitations by tracing the particles instead, but, this comes at the cost of significantly slower rendering. In this work, we propose 3D Gaussian Unscented Transform (3DGUT), replacing the EWA splatting formulation with the Unscented Transform that approximates the particles through sigma points, which can be projected exactly under any nonlinear projection function. This modification enables trivial support of distorted cameras with time dependent effects such as rolling shutter, while retaining the efficiency of rasterization. Additionally, we align our rendering formulation with that of tracing-based methods, enabling secondary ray tracing required to represent phenomena such as reflections and refraction within the same 3D representation. The source code is available at: https://github.com/nv-tlabs/3dgrut",
    "checked": true,
    "id": "a311a6d01d3cb49fc82919f92e0e8f472096df15",
    "semantic_title": "3dgut: enabling distorted cameras and secondary rays in gaussian splatting",
    "citation_count": 22,
    "authors": [
      "Qi Wu",
      "Janick Martinez Esturo",
      "Ashkan Mirzaei",
      "Nicolas Mo√´nne-Loccoz",
      "Zan Gojcic"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/You_Teaching_Large_Language_Models_to_Regress_Accurate_Image_Quality_Scores_CVPR_2025_paper.html": {
    "title": "Teaching Large Language Models to Regress Accurate Image Quality Scores Using Score Distribution",
    "volume": "main",
    "abstract": "With the rapid advancement of Multi-modal Large Language Models (MLLMs), MLLM-based Image Quality Assessment (IQA) methods have shown promising performance in linguistic quality description. However, current methods still fall short in accurately scoring image quality. In this work, we aim to leverage MLLMs to regress accurate quality scores. A key challenge is that the quality score is inherently continuous, typically modeled as a Gaussian distribution, whereas MLLMs generate discrete token outputs. This mismatch necessitates score discretization. Previous approaches discretize the mean score into a one-hot label, resulting in information loss and failing to capture inter-image relationships. We propose a distribution-based approach that discretizes the score distribution into a soft label. This method preserves the characteristics of the score distribution, achieving high accuracy and maintaining inter-image relationships. Moreover, to address dataset variation, where different IQA datasets exhibit various distributions, we introduce a fidelity loss based on Thurstone's model. This loss captures intra-dataset relationships, facilitating co-training across multiple IQA datasets. With these designs, we develop the distribution-based Depicted image Quality Assessment model for Score regression (DeQA-Score). Experiments across multiple benchmarks show that DeQA-Score stably outperforms baselines in score regression. Also, DeQA-Score can predict the score distribution that closely aligns with human annotations. Codes and model weights have been released in https://depictqa.github.io/deqa-score/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan You",
      "Xin Cai",
      "Jinjin Gu",
      "Tianfan Xue",
      "Chao Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_Lifting_the_Veil_on_Visual_Information_Flow_in_MLLMs_Unlocking_CVPR_2025_paper.html": {
    "title": "Lifting the Veil on Visual Information Flow in MLLMs: Unlocking Pathways to Faster Inference",
    "volume": "main",
    "abstract": "Multimodal large language models (MLLMs) improve performance on vision-language tasks by integrating visual features from pre-trained vision encoders into large language models (LLMs). However, how MLLMs process and utilize visual information remains unclear. In this paper, a shift in the dominant flow of visual information is uncovered: (1) in shallow layers, strong interactions are observed between image tokens and instruction tokens, where most visual information is injected into instruction tokens to form cross-modal semantic representations; (2) in deeper layers, image tokens primarily interact with each other, aggregating the remaining visual information to optimize semantic representations within the visual modality. Based on these insights, we propose Hierarchical Modality-Aware Pruning (HiMAP), a plug-and-play inference acceleration method that dynamically prunes image tokens at specific layers, reducing computational costs by approximately 65% without sacrificing performance. Our findings offer a new understanding of visual information processing in MLLMs and provide a state-of-the-art solution for efficient inference",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Yin",
      "Guangzong Si",
      "Zilei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Your_Scale_Factors_are_My_Weapon_Targeted_Bit-Flip_Attacks_on_CVPR_2025_paper.html": {
    "title": "Your Scale Factors are My Weapon: Targeted Bit-Flip Attacks on Vision Transformers via Scale Factor Manipulation",
    "volume": "main",
    "abstract": "Vision Transformers (ViTs) have experienced significant progress and are quantized for deployment in resource-constrained applications. Quantized models are vulnerable to targeted bit-flip attacks (BFAs). A targeted BFA prepares a trigger and a corresponding Trojan/backdoor, inserting the latter (with RowHammer bit flipping) into a victim model, to mislead its classification on samples containing the trigger. Existing targeted BFAs on quantized ViTs are limited in that: (1) they require numerous bit-flips, and (2) the separation between flipped bits is below 4 KB, making attacks infeasible with RowHammer in real-world scenarios. We propose a new and practical targeted attack Flip-S against quantized ViTs. The core insight is that in quantized models, a scale factor change ripples through a batch of model weights. Consequently, flipping bits in scale factors, rather than solely in model weights, enables more cost-effective attacks. We design a Scale-Factor-Search (SFS) algorithm to identify critical bits in scale factors for flipping, and adopt a mutual exclusion strategy to guarantee a 4 KB separation between flips. We evaluate Flip-S on CIFAR-10 and ImageNet datasets across five ViT architectures and two quantization levels. Results show that Flip-S achieves attack success rate (ASR) exceeding 90.0% on all models with 50 bits flipped, outperforming baselines with ASR typically below 80.0%. Furthermore, compared to the SOTA, Flip-S reduces the number of required bit-flips by 8x-20xwhile reaching equal or higher ASR. Our source code is publicly available",
    "checked": true,
    "id": "6b234baee1a7d213aeb43c7aa2bf33b3a8010497",
    "semantic_title": "your scale factors are my weapon: targeted bit-flip attacks on vision transformers via scale factor manipulation",
    "citation_count": 0,
    "authors": [
      "Jialai Wang",
      "Yuxiao Wu",
      "Weiye Xu",
      "Yating Huang",
      "Chao Zhang",
      "Zongpeng Li",
      "Mingwei Xu",
      "Zhenkai Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Marten_Visual_Question_Answering_with_Mask_Generation_for_Multi-modal_Document_CVPR_2025_paper.html": {
    "title": "Marten: Visual Question Answering with Mask Generation for Multi-modal Document Understanding",
    "volume": "main",
    "abstract": "Multi-modal Large Language Models (MLLMs) have introduced a novel dimension to document understanding, i.e., they endow large language models with visual comprehension capabilities; however, how to design a suitable image-text pre-training task for bridging the visual and language modality in document-level MLLMs remains underexplored. In this study, we introduce a novel visual-language alignment method that casts the key issue as a Visual Question Answering with Mask generation (VQAMask) task, optimizing two tasks simultaneously: VQA-based text parsing and mask generation. The former allows the model to implicitly align images and text at the semantic level. The latter introduces an additional mask generator (discarded during inference) to explicitly ensure alignment between visual texts within images and their corresponding image regions at a spatially-aware level. Together, they can prevent model hallucinations when parsing visual text and effectively promote spatially-aware feature representation learning. To support the proposed VQAMask task, we construct a comprehensive image-mask generation pipeline and provide a large-scale dataset with 6M data (MTMask6M). Subsequently, we demonstrate that introducing the proposed mask generation task yields competitive document-level understanding performance. Leveraging the proposed VQAMask, we introduce Marten, a training-efficient MLLM tailored for document-level understanding. Extensive experiments show that our Marten consistently achieves significant improvements among 8B-MLLMs in document-centric tasks. Code and datasets are available at https://github.com/PriNing/Marten",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zining Wang",
      "Tongkun Guan",
      "Pei Fu",
      "Chen Duan",
      "Qianyi Jiang",
      "Zhentao Guo",
      "Shan Guo",
      "Junfeng Luo",
      "Wei Shen",
      "Xiaokang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Mamba-Reg_Vision_Mamba_Also_Needs_Registers_CVPR_2025_paper.html": {
    "title": "Mamba-Reg: Vision Mamba Also Needs Registers",
    "volume": "main",
    "abstract": "Similar to Vision Transformers, this paper identifies artifacts also present within the feature maps of Vision Mamba. These artifacts, corresponding to high-norm tokens emerging in low-information background areas of images, appear much more severe in Vision Mamba---they exist prevalently even with the tiny-sized model and activate extensively across background regions. To mitigate this issue, we follow the prior solution of introducing register tokens into Vision Mamba. To better cope with Mamba blocks' uni-directional inference paradigm, two key modifications are introduced: 1) evenly inserting registers throughout the input token sequence, and 2) recycling registers for final decision predictions. We term this new architecture MambaReg. Qualitative observations suggest, compared to vanilla Vision Mamba, MambaReg's feature maps appear cleaner and more focused on semantically meaningful regions. Quantitatively, MambaReg attains stronger performance and scales better. For example, on the ImageNet benchmark, our MambaReg-B attains 83.0% accuracy, significantly outperforming Vim-B's 81.8%; furthermore, we provide the first successful scaling to the large model size (i.e., with 340M parameters), attaining a competitive accuracy of 83.6% (84.5% if finetuned with 384x384 inputs). Additional validation on the downstream semantic segmentation task also supports MambaReg's efficacy",
    "checked": true,
    "id": "5c2009c75774f6c38bb16fdba66e730b8b1d0d6e",
    "semantic_title": "mamba-reg: vision mamba also needs registers",
    "citation_count": 27,
    "authors": [
      "Feng Wang",
      "Jiahao Wang",
      "Sucheng Ren",
      "Guoyizhe Wei",
      "Jieru Mei",
      "Wei Shao",
      "Yuyin Zhou",
      "Alan Yuille",
      "Cihang Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Schnaus_Its_a_Blind_Match_Towards_Vision-Language_Correspondence_without_Parallel_Data_CVPR_2025_paper.html": {
    "title": "It's a (Blind) Match! Towards Vision-Language Correspondence without Parallel Data",
    "volume": "main",
    "abstract": "The platonic representation hypothesis suggests that vision and language embeddings become more homogeneous as model and dataset sizes increase. In particular, pairwise distances within each modality become more similar. This suggests that as foundation models mature, it may become possible to match vision and language embeddings in a fully unsupervised fashion, i.e. without parallel data. We present the first feasibility study, and investigate conformity of existing vision and language foundation models in the context of unsupervised, or \"blind\", matching. First, we formulate unsupervised matching as a quadratic assignment problem and introduce a novel heuristic that outperforms previous solvers. We also develop a technique to find optimal matching problems, for which a non-trivial match is very likely. Second, we conduct an extensive study deploying a range of vision and language models on four datasets. Our analysis reveals that for many problem instances, vision and language representations can be indeed matched without supervision. This finding opens up the exciting possibility of embedding semantic knowledge into other modalities virtually annotation-free. As a proof of concept, we showcase an unsupervised classifier, which achieves non-trivial classification accuracy without any image-text annotation",
    "checked": true,
    "id": "afb181e3d3e091aa57fb1b8eafd2103a17678372",
    "semantic_title": "it's a (blind) match! towards vision-language correspondence without parallel data",
    "citation_count": 2,
    "authors": [
      "Dominik Schnaus",
      "Nikita Araslanov",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_Open_Set_Label_Shift_with_Test_Time_Out-of-Distribution_Reference_CVPR_2025_paper.html": {
    "title": "Open Set Label Shift with Test Time Out-of-Distribution Reference",
    "volume": "main",
    "abstract": "Open set label shift (OSLS) occurs when label distributions change from a source to a target distribution, and the target distribution has an additional out-of-distribution (OOD) class.In this work, we build estimators for both source and target open set label distributions using a source domain in-distribution (ID) classifier and an ID/OOD classifier. With reasonable assumptions on the ID/OOD classifier, the estimators are assembled into a sequence of three stages: 1) an estimate of the source label distribution of the OOD class, 2) an EM algorithm for Maximum Likelihood estimates (MLE) of the target label distribution, and 3) an estimate of the target label distribution of OOD class under relaxed assumptions on the OOD classifier.The sampling errors of estimates in 1) and 3) are quantified with a concentration inequality.The estimation result allows us to correct the ID classifier trained on the source distribution to the target distribution without retraining.Experiments on a variety of open set label shift settings demonstrate the effectiveness of our model.Our code is available at https://github.com/ChangkunYe/OpenSetLabelShift",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changkun Ye",
      "Russell Tsuchida",
      "Lars Petersson",
      "Nick Barnes"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nam_Visual_Persona_Foundation_Model_for_Full-Body_Human_Customization_CVPR_2025_paper.html": {
    "title": "Visual Persona: Foundation Model for Full-Body Human Customization",
    "volume": "main",
    "abstract": "We introduce Visual Persona, a foundation model for text-to-image full-body human customization that, given a single in-the-wild human image, generates diverse images of the individual guided by text descriptions. Unlike prior methods that focus solely on preserving facial identity, our approach captures detailed full-body appearance, aligning with text descriptions for body structure and scene variations. Training this model requires large-scale paired human data, consisting of multiple images per individual with consistent full-body identities, which is notoriously difficult to obtain. To address this, we propose a data curation pipeline leveraging vision-language models to evaluate full-body appearance consistency, resulting in Visual Persona-500K--a dataset of 580k paired human images across 100k unique identities. For precise appearance transfer, we introduce a transformer encoder-decoder architecture adapted to a pre-trained text-to-image diffusion model, which augments the input image into distinct body regions, encodes these regions as local appearance features, and projects them into dense identity embeddings independently to condition the diffusion model for synthesizing customized images. Visual Persona consistently surpasses existing approaches, generating high-quality, customized images from in-the-wild inputs. Extensive ablation studies validate design choices, and we demonstrate the versatility of Visual Persona across various downstream tasks",
    "checked": true,
    "id": "0dbfde86ee14b4d5a501f64f649e0e775625cf7b",
    "semantic_title": "visual persona: foundation model for full-body human customization",
    "citation_count": 3,
    "authors": [
      "Jisu Nam",
      "Soowon Son",
      "Zhan Xu",
      "Jing Shi",
      "Difan Liu",
      "Feng Liu",
      "Seungryong Kim",
      "Yang Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_SOGS_Second-Order_Anchor_for_Advanced_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "SOGS: Second-Order Anchor for Advanced 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "Anchor-based 3D Gaussian splatting (3D-GS) exploits anchor features in 3D Gaussian prediction, which has achieved impressive 3D rendering quality with reduced Gaussian redundancy. On the other hand, it often encounters the dilemma among anchor features, model size, and rendering quality - large anchor features lead to large 3D models and high-quality rendering whereas reducing anchor features degrades Gaussian attribute prediction which leads to clear artifacts in the rendered textures and geometries. We design SOGS, an anchor-based 3D-GS technique that introduces second-order anchors to achieve superior rendering quality and reduced anchor features and model size simultaneously. Specifically, SOGS incorporates covariance-based second-order statistics and correlation across feature dimensions to augment features within each anchor, compensating for the reduced feature size and improving rendering quality effectively. In addition, it introduces a selective gradient loss to enhance the optimization of scene textures and scene geometries, leading to high-quality rendering with small anchor features. Extensive experiments over multiple widely adopted benchmarks show that SOGS achieves superior rendering quality in novel view synthesis with clearly reduced model size",
    "checked": true,
    "id": "a18df6f9ad6bf78c5b73afd77f8a6627e130abd8",
    "semantic_title": "sogs: second-order anchor for advanced 3d gaussian splatting",
    "citation_count": 0,
    "authors": [
      "Jiahui Zhang",
      "Fangneng Zhan",
      "Ling Shao",
      "Shijian Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_GaussianFormer-2_Probabilistic_Gaussian_Superposition_for_Efficient_3D_Occupancy_Prediction_CVPR_2025_paper.html": {
    "title": "GaussianFormer-2: Probabilistic Gaussian Superposition for Efficient 3D Occupancy Prediction",
    "volume": "main",
    "abstract": "3D semantic occupancy prediction has garnered attention as an important task for the robustness of vision-centric autonomous driving, which predicts fine-grained geometry and semantics of the surrounding scene. Most existing methods leverage dense grid-based scene representations, overlooking the spatial sparsity of the driving scenes, which leads to computational redundancy. Although 3D semantic Gaussian serves as an object-centric sparse alternative, most of the Gaussians still describe the empty region with low efficiency. To address this, we propose a probabilistic Gaussian superposition model which interprets each Gaussian as a probability distribution of its neighborhood being occupied and conforms to probabilistic multiplication to derive the overall geometry. Furthermore, we adopt the exact Gaussian mixture model for semantics calculation to avoid unnecessary overlapping of Gaussians. To effectively initialize Gaussians in non-empty region, we design a distribution-based initialization module which learns the pixel-aligned occupancy distribution instead of the depth of surfaces. We conduct extensive experiments on nuScenes and KITTI-360 datasets and our GaussianFormer-2 achieves state-of-the-art performance with high efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanhui Huang",
      "Amonnut Thammatadatrakoon",
      "Wenzhao Zheng",
      "Yunpeng Zhang",
      "Dalong Du",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_MExD_An_Expert-Infused_Diffusion_Model_for_Whole-Slide_Image_Classification_CVPR_2025_paper.html": {
    "title": "MExD: An Expert-Infused Diffusion Model for Whole-Slide Image Classification",
    "volume": "main",
    "abstract": "Whole Slide Image (WSI) classification poses unique challenges due to the vast image size and numerous non-informative regions, which introduce noise and cause data imbalance during feature aggregation. To address these issues, we propose MExD, an Expert-Infused Diffusion Model that combines the strengths of a Mixture-of-Experts (MoE) mechanism with a diffusion model for enhanced classification. MExD balances patch feature distribution through a novel MoE-based aggregator that selectively emphasizes relevant information, effectively filtering noise, addressing data imbalance, and extracting essential features. These features are then integrated via a diffusion-based generative process to directly yield the class distribution for the WSI. Moving beyond conventional discriminative approaches, MExD represents the first generative strategy in WSI classification, capturing fine-grained details for robust and precise results. Our MExD is validated on three widely-used benchmarks--Camelyon16, TCGA-NSCLC, and BRACS--consistently achieving state-of-the-art performance in both binary and multi-class tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianwei Zhao",
      "Xin Li",
      "Fan Yang",
      "Qiang Zhai",
      "Ao Luo",
      "Yang Zhao",
      "Hong Cheng",
      "Huazhu Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Buch_Flexible_Frame_Selection_for_Efficient_Video_Reasoning_CVPR_2025_paper.html": {
    "title": "Flexible Frame Selection for Efficient Video Reasoning",
    "volume": "main",
    "abstract": "Video-language models have shown promise for addressing a range of multimodal tasks for video understanding, such as video question-answering. However, the inherent computational challenges of processing long video data and increasing model sizes have led to standard approaches that are limited by the number of frames they can process. In this work, we propose the Flexible Frame Selector (FFS), a learnable policy model with a new flexible selection operation, that helps alleviate input context restrictions by enabling video-language models to focus on the most informative frames for the downstream multimodal task, without adding undue processing cost. Our method differentiates from prior work due to its learnability, efficiency, and flexibility. We verify the efficacy of our method on standard video-question answering and reasoning benchmarks, and observe that our model can improve base video-language model accuracy while reducing the number of downstream processed frames",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shyamal Buch",
      "Arsha Nagrani",
      "Anurag Arnab",
      "Cordelia Schmid"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_EventGPT_Event_Stream_Understanding_with_Multimodal_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "EventGPT: Event Stream Understanding with Multimodal Large Language Models",
    "volume": "main",
    "abstract": "Event cameras capture visual information as asynchronous pixel change streams, excelling in challenging lighting and high-dynamic scenarios. Existing multimodal large language models (MLLMs) concentrate on natural RGB images, failing in scenarios where event data fits better. In this paper, we introduce EventGPT, the first MLLM for event stream understanding, pioneering the integration of large language models (LLMs) with event-based vision. To bridge the huge domain gap, we propose a three-stage optimization paradigm to progressively equip a pre-trained LLM with event understanding. Our EventGPT consists of an event encoder, a spatio-temporal aggregator, a linear projector, an event-language adapter, and an LLM. Firstly, GPT-generated RGB image-text pairs warm up the linear projector, following LLaVA, as the gap between natural images and language is smaller. Secondly, we construct N-ImageNet-Chat, a large synthetic dataset of event data and corresponding texts to enable the use of the spatio-temporal aggregator and to train the event-language adapter, thereby aligning event features more closely with the language space. Finally, we gather an instruction dataset, Event-Chat, which contains extensive real-world data to fine-tune the entire model, further enhancing its generalization ability. We construct a comprehensive benchmark, and experiments show that EventGPT surpasses previous state-of-the-art MLLMs in generation quality, descriptive accuracy, and reasoning capability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaoyu Liu",
      "Jianing Li",
      "Guanghui Zhao",
      "Yunjian Zhang",
      "Xin Meng",
      "Fei Richard Yu",
      "Xiangyang Ji",
      "Ming Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Pixel-level_and_Semantic-level_Adjustable_Super-resolution_A_Dual-LoRA_Approach_CVPR_2025_paper.html": {
    "title": "Pixel-level and Semantic-level Adjustable Super-resolution: A Dual-LoRA Approach",
    "volume": "main",
    "abstract": "Diffusion prior-based methods have shown impressive results in real-world image super-resolution (SR). However, most existing methods entangle pixel-level and semantic-level SR objectives in the training process, struggling to balance pixel-wise fidelity and perceptual quality. Meanwhile, users have varying preferences on SR results, thus it is demanded to develop an adjustable SR model that can be tailored to different fidelity-perception preferences during inference without re-training. We present Pixel-level and Semantic-level Adjustable SR (PiSA-SR), which learns two LoRA modules upon the pre-trained stable-diffusion (SD) model to achieve improved and adjustable SR results. We first formulate the SD-based SR problem as learning the residual between the low-quality input and the high-quality output, then show that the learning objective can be decoupled into two distinct LoRA weight spaces: one is characterized by the l2-loss for pixel-level regression, and another is characterized by the LPIPS and classifier score distillation losses to extract semantic information from pre-trained classification and SD models. In its default setting, PiSA-SR can be performed in a single diffusion step, achieving leading real-world SR results in both quality and efficiency. By introducing two adjustable guidance scales on the two LoRA modules to control the strengths of pixel-wise fidelity and semantic-level details during inference, PiSA-SR can offer flexible SR results according to user preference without re-training. The source code of our method can be found at https://github.com/csslc/PiSA-SR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingchen Sun",
      "Rongyuan Wu",
      "Zhiyuan Ma",
      "Shuaizheng Liu",
      "Qiaosi Yi",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Let_Samples_Speak_Mitigating_Spurious_Correlation_by_Exploiting_the_Clusterness_CVPR_2025_paper.html": {
    "title": "Let Samples Speak: Mitigating Spurious Correlation by Exploiting the Clusterness of Samples",
    "volume": "main",
    "abstract": "Deep learning models are known to often learn features that spuriously correlate with the class label during training but are irrelevant to the prediction task. Existing methods typically address this issue by annotating potential spurious attributes, or filtering spurious features based on some empirical assumptions (e.g., simplicity of bias). However, these methods may yield unsatisfying performance due to the intricate and elusive nature of spurious correlations in real-world data. In this paper, we propose a data-oriented approach to mitigate the spurious correlation in deep learning models. We observe that samples that are influenced by spurious features tend to exhibit a dispersed distribution in the learned feature space. This allows us to identify the presence of spurious features. Subsequently, we obtain a bias-invariant representation by neutralizing the spuriousfeatures based on a simple grouping strategy. Then, we learn a feature transformation to eliminate the spuriousfeatures by aligning with this bias-invariant representation. Finally, we update the classifier by incorporating the learned feature transformation and obtain an unbiased model. By integrating the aforementioned identifying, neutralizing, eliminating and updating procedures, we build an effective pipeline for mitigating spurious correlation. Experiments on image and NLP debiasing benchmarks show an improvement in worst group accuracy of more than 20% compared to standard empirical risk minimization (ERM). Codes and checkpoints are available at https://github.com/davelee-uestc/nsf_debiasing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiwei Li",
      "Junzhuo Liu",
      "Yuanyuan Ren",
      "Yuchen Zheng",
      "Yahao Liu",
      "Wen Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Mr._DETR_Instructive_Multi-Route_Training_for_Detection_Transformers_CVPR_2025_paper.html": {
    "title": "Mr. DETR: Instructive Multi-Route Training for Detection Transformers",
    "volume": "main",
    "abstract": "Existing methods enhance the training of detection transformers by incorporating an auxiliary one-to-many assignment. In this work, we treat the model as a multi-task framework, simultaneously performing one-to-one and one-to-many predictions. We investigate the roles of each component in the transformer decoder across these two training targets, including self-attention, cross-attention, and feed-forward network. Our empirical results demonstrate that any independent component in the decoder can effectively learn both targets simultaneously, even when other components are shared. This finding leads us to propose a multi-route training mechanism, featuring a primary route for one-to-one prediction and two auxiliary training routes for one-to-many prediction. We enhance the training mechanism with a novel instructive self-attention that dynamically and flexibly guides object queries for one-to-many prediction. The auxiliary routes are removed during inference, ensuring no impact on model architecture or inference cost. We conduct extensive experiments on various baselines, achieving consistent improvements as shown in Fig. 1. Project page: https://visual-ai.github.io/mrdetr",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chang-Bin Zhang",
      "Yujie Zhong",
      "Kai Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_MITracker_Multi-View_Integration_for_Visual_Object_Tracking_CVPR_2025_paper.html": {
    "title": "MITracker: Multi-View Integration for Visual Object Tracking",
    "volume": "main",
    "abstract": "Multi-view object tracking (MVOT) offers promising solutions to challenges such as occlusion and target loss, which are common in traditional single-view tracking. However, progress has been limited by the lack of comprehensive multi-view datasets and effective cross-view integration methods. To overcome these limitations, we compiled a Multi-View object Tracking (MVTrack) dataset of 234K high-quality annotated frames featuring 27 distinct objects across various scenes. In conjunction with this dataset, we introduce a novel MVOT method, Multi-View Integration Tracker (MITracker), to efficiently integrate multi-view object features and provide stable tracking outcomes. MITracker can track any object in video frames of arbitrary length from arbitrary viewpoints. The key advancements of our method over traditional single-view approaches come from two aspects: (1) MITracker transforms 2D image features into a 3D feature volume and compresses it into a bird's eye view (BEV) plane, facilitating inter-view information fusion; (2) we propose an attention mechanism that leverages geometric information from fused 3D feature volume to refine the tracking results at each view. MITracker outperforms existing methods on the MVTrack and GMTD datasets, achieving state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengjie Xu",
      "Yitao Zhu",
      "Haotian Jiang",
      "Jiaming Li",
      "Zhenrong Shen",
      "Sheng Wang",
      "Haolin Huang",
      "Xinyu Wang",
      "Han Zhang",
      "Qing Yang",
      "Qian Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dou_Hearing_Hands_Generating_Sounds_from_Physical_Interactions_in_3D_Scenes_CVPR_2025_paper.html": {
    "title": "Hearing Hands: Generating Sounds from Physical Interactions in 3D Scenes",
    "volume": "main",
    "abstract": "We study the problem of making 3D scene reconstructions interactive by asking the following question: can we predict the sounds of human hands physically interacting with a scene? First, we record a video of a human manipulating objects within a 3D scene using their hands. We then use these action-sound pairs to train a rectified flow model to map 3D hand trajectories to their corresponding audio. At test time, a user can query the model for other actions, parameterized as sequences of hand poses, to estimate their corresponding sounds. In our experiments, we find that our generated sounds accurately convey material properties and actions, and that they are often indistinguishable to human observers from real sounds. Project page: https://www.yimingdou.com/hearing_hands/",
    "checked": true,
    "id": "ca2ad9f1eeb3c020f6b02dc2926fd20b5e75c029",
    "semantic_title": "hearing hands: generating sounds from physical interactions in 3d scenes",
    "citation_count": 0,
    "authors": [
      "Yiming Dou",
      "Wonseok Oh",
      "Yuqing Luo",
      "Antonio Loquercio",
      "Andrew Owens"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yao_AirRoom_Objects_Matter_in_Room_Reidentification_CVPR_2025_paper.html": {
    "title": "AirRoom: Objects Matter in Room Reidentification",
    "volume": "main",
    "abstract": "Room reidentification (ReID) is a challenging yet essential task with numerous applications in fields such as augmented reality (AR) and homecare robotics. Existing visual place recognition (VPR) methods, which typically rely on global descriptors or aggregate local features, often struggle in cluttered indoor environments densely populated with man-made objects. These methods tend to overlook the crucial role of object-oriented information. To address this, we propose AirRoom, an object-aware pipeline that integrates multi-level object-oriented information--from global context to object patches, object segmentation, and keypoints--utilizing a coarse-to-fine retrieval approach. Extensive experiments on four newly constructed datasets--MPReID, HMReID, GibsonReID, and ReplicaReID--demonstrate that AirRoom outperforms state-of-the-art (SOTA) models across nearly all evaluation metrics, with improvements ranging from 6% to 80%. Moreover, AirRoom exhibits significant flexibility, allowing various modules within the pipeline to be substituted with different alternatives without compromising overall performance. It also shows robust and consistent performance under diverse viewpoint variations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runmao Yao",
      "Yi Du",
      "Zhuoqun Chen",
      "Haoze Zheng",
      "Chen Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Berasi_Not_Only_Text_Exploring_Compositionality_of_Visual_Representations_in_Vision-Language_CVPR_2025_paper.html": {
    "title": "Not Only Text: Exploring Compositionality of Visual Representations in Vision-Language Models",
    "volume": "main",
    "abstract": "Vision-Language Models (VLMs) learn a shared feature space for text and images, enabling the comparison of inputs of different modalities. While prior works demonstrated that VLMs organize natural language representations into regular structures encoding composite meanings, it remains unclear if compositional patterns also emerge in the visual embedding space. In this work, we investigate compositionality in the image domain, where the analysis of compositional properties is challenged by noise and sparsity of visual data. We address these problems and propose a framework, called Geodesically Decomposable Embeddings (GDE), that approximates image representations with geometry-aware compositional structures in the latent space. We demonstrate that visual embeddings of pre-trained VLMs exhibit a compositional arrangement, and evaluate the effectiveness of this property in the tasks of compositional classification and group robustness. GDE achieves stronger performance in compositional classification compared to its counterpart method that assumes linear geometry of the latent space. Notably, it is particularly effective for group robustness, where we achieve higher results than task-specific solutions. Our results indicate that VLMs can automatically develop a human-like form of compositional reasoning in the visual domain, making their underlying processes more interpretable. Code is available at https://github.com/BerasiDavide/vlm_image_compositionality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davide Berasi",
      "Matteo Farina",
      "Massimiliano Mancini",
      "Elisa Ricci",
      "Nicola Strisciuglio"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_DefMamba_Deformable_Visual_State_Space_Model_CVPR_2025_paper.html": {
    "title": "DefMamba: Deformable Visual State Space Model",
    "volume": "main",
    "abstract": "Recently, state space models (SSM), particularly Mamba, have attracted significant attention from scholars due to their ability to effectively balance computational efficiency and performance. However, most existing visual Mamba methods flatten images into 1D sequences using predefined scan orders, which results the model being less capable of utilizing the spatial structural information of the image during the feature extraction process. To address this issue, we proposed a novel visual foundation model called DefMamba. This model includes a multi-scale backbone structure and deformable mamba (DM) blocks, which dynamically adjust the scanning path to prioritize important information, thus enhancing the capture and processing of relevant input features. By combining a deformable scanning (DS) strategy, this model significantly improves its ability to learn image structures and detects changes in object details. Numerous experiments have shown that DefMamba achieves state-of-the-art performance in various visual tasks, including image classification, object detection, instance segmentation, and semantic segmentation. The code is open source on DefMamba",
    "checked": true,
    "id": "96e6332547ce759f4ad39986ea40088223411f32",
    "semantic_title": "defmamba: deformable visual state space model",
    "citation_count": 2,
    "authors": [
      "Leiye Liu",
      "Miao Zhang",
      "Jihao Yin",
      "Tingwei Liu",
      "Wei Ji",
      "Yongri Piao",
      "Huchuan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_HOP_Heterogeneous_Topology-based_Multimodal_Entanglement_for_Co-Speech_Gesture_Generation_CVPR_2025_paper.html": {
    "title": "HOP: Heterogeneous Topology-based Multimodal Entanglement for Co-Speech Gesture Generation",
    "volume": "main",
    "abstract": "Co-speech gestures are crucial non-verbal cues that enhance speech clarity and expressiveness in human communication, which have attracted increasing attention in multimodal research. While the existing methods have made strides in gesture accuracy, challenges remain in generating diverse and coherent gestures, as most approaches assume independence among multimodal inputs and lack explicit modeling of their interactions. In this work, we propose a novel multimodal learning method named HOP for co-speech gesture generation that captures the heterogeneous entanglement between gesture motion, audio rhythm, and text semantics, enabling the generation of coordinated gestures. By leveraging spatiotemporal graph modeling, we achieve the alignment of audio and action. Moreover, to enhance modality coherence, we build the audio-text semantic representation based on a reprogramming module, which is beneficial for cross-modality adaptation. Our approach enables the trimodal system to learn each other's features and represent them in the form of topological entanglement. Extensive experiments demonstrate that HOP achieves state-of-the-art performance, offering more natural and expressive co-speech gesture generation. More information, codes, and demos are available here: https://star-uu-wang.github.io/HOP/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongye Cheng",
      "Tianyu Wang",
      "Guangsi Shi",
      "Zexing Zhao",
      "Yanwei Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_VoxelSplat_Dynamic_Gaussian_Splatting_as_an_Effective_Loss_for_Occupancy_CVPR_2025_paper.html": {
    "title": "VoxelSplat: Dynamic Gaussian Splatting as an Effective Loss for Occupancy and Flow Prediction",
    "volume": "main",
    "abstract": "Recent advancements in camera-based occupancy prediction have focused on the simultaneous prediction of 3D semantics and scene flow, a task that presents significant challenges due to specific difficulties, e.g., occlusions and unbalanced dynamic environments. In this paper, we analyze these challenges and their underlying causes. To address them, we propose a novel regularization framework called VoxelSplat. This framework leverages recent developments in 3D Gaussian Splatting to enhance model performance in two key ways: (i)Enhanced Semantics Supervision through 2D Projection: During training, our method decodes sparse semantic 3D Gaussians from 3D representations and projects them onto the 2D camera view. This provides additional supervision signals in the camera-visible space, allowing 2D labels to improve the learning of 3D semantics. (ii) Scene Flow Learning: Our framework uses the predicted scene flow to model the motion of Gaussians, and is thus able to learn the scene flow of moving objects in a self-supervised manner using the labels of adjacent frames. Our method can be seamlessly integrated into various existing occupancy models, enhancingperformance without increasing inference time. Extensive experiments on benchmark datasets demonstrate the effectiveness of VoxelSplat in improving the accuracy of both semantic occupancy and scene flow estimation. The project page and codes are available at https://zzy816.github.io/VoxelSplat-Demo/",
    "checked": true,
    "id": "a57368c1f96fb02b4df1c4466911cefd36169d94",
    "semantic_title": "voxelsplat: dynamic gaussian splatting as an effective loss for occupancy and flow prediction",
    "citation_count": 1,
    "authors": [
      "Ziyue Zhu",
      "Shenlong Wang",
      "Jin Xie",
      "Jiang-jiang Liu",
      "Jingdong Wang",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Exploring_Scene_Affinity_for_Semi-Supervised_LiDAR_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Exploring Scene Affinity for Semi-Supervised LiDAR Semantic Segmentation",
    "volume": "main",
    "abstract": "This paper explores scene affinity (AIScene), namely intra-scene consistency and inter-scene correlation, for semi-supervised LiDAR semantic segmentation in driving scenes. Adopting teacher-student training, AIScene employs a teacher network to generate pseudo-labeled scenes from unlabeled data, which then supervise the student network's learning. Unlike most methods that include all points in pseudo-labeled scenes for forward propagation but only pseudo-labeled points for backpropagation, AIScene removes points without pseudo-labels, ensuring consistency in both forward and backward propagation within the scene. This simple point erasure strategy effectively prevents unsupervised, semantically ambiguous points (excluded in backpropagation) from affecting the learning of pseudo-labeled points. Moreover, AIScene incorporates patch-based data augmentation, mixing multiple scenes at both scene and instance levels. Compared to existing augmentation techniques that typically perform scene-level mixing between two scenes, our method enhances the semantic diversity of labeled (or pseudo-labeled) scenes, thereby improving the semi-supervised performance of segmentation models. Experiments show that AIScene outperforms previous methods on two popular benchmarks across four settings, achieving notable improvements of 1.9% and 2.1% in the most challenging 1% labeled data. The code will be released at https://github.com/azhuantou/AIScene",
    "checked": true,
    "id": "22793b512ddbd2f9294a4f20169bee81c5f64595",
    "semantic_title": "exploring scene affinity for semi-supervised lidar semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Chuandong Liu",
      "Xingxing Weng",
      "Shuguo Jiang",
      "Pengcheng Li",
      "Lei Yu",
      "Gui-Song Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jang_ControlFace_Harnessing_Facial_Parametric_Control_for_Face_Rigging_CVPR_2025_paper.html": {
    "title": "ControlFace: Harnessing Facial Parametric Control for Face Rigging",
    "volume": "main",
    "abstract": "Manipulation of facial images to meet specific controls such as pose, expression, and lighting, also referred to as face rigging is a complex task in computer vision. Existing methods are limited by their reliance on image datasets, which necessitates individual-specific fine-tuning and limits their ability to retain fine-grained identity and semantic details, reducing practical usability. To overcome these limitations, we introduce ControlFace, a novel face rigging method conditioned on 3DMM renderings that enables flexible, high-fidelity control. ControlFace employs a dual-branch U-Nets: one, referred to as FaceNet, captures identity and fine details, while the other focuses on generation. To enhance control precision, control mixer module encodes the correlated features between the target-aligned control and reference-aligned control, and a novel guidance method, reference control guidance, steers the generation process for better control adherence. By training on a facial video dataset, we fully utilize FaceNet's rich representations while ensuring control adherence. Extensive experiments demonstrate ControlFace's superior performance in identity preservation, and control precision, highlighting its practicality. Code and pre-trained weights will be publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wooseok Jang",
      "Youngjun Hong",
      "Geonho Cha",
      "Seungryong Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Um_Minority-Focused_Text-to-Image_Generation_via_Prompt_Optimization_CVPR_2025_paper.html": {
    "title": "Minority-Focused Text-to-Image Generation via Prompt Optimization",
    "volume": "main",
    "abstract": "We investigate the generation of minority samples using pretrained text-to-image (T2I) latent diffusion models. Minority instances, in the context of T2I generation, can be defined as ones living on low-density regions of text-conditional data distributions. They are valuable for various applications of modern T2I generators, such as data augmentation and creative AI. Unfortunately, existing pretrained T2I diffusion models primarily focus on high-density regions, largely due to the influence of guided samplers (like CFG) that are essential for high-quality generation. To address this, we present a novel framework to counter the high-density-focus of T2I diffusion models. Specifically, we first develop an online prompt optimization framework that encourages emergence of desired properties during inference while preserving semantic contents of user-provided prompts. We subsequently tailor this generic prompt optimizer into a specialized solver that promotes generation of minority features by incorporating a carefully-crafted likelihood objective. Extensive experiments conducted across various types of T2I models demonstrate that our approach significantly enhances the capability to produce high-quality minority instances compared to existing samplers. Code is available at https://github.com/soobin-um/MinorityPrompt",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soobin Um",
      "Jong Chul Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dai_Imputation-free_and_Alignment-free_Incomplete_Multi-view_Clustering_Driven_by_Consensus_Semantic_CVPR_2025_paper.html": {
    "title": "Imputation-free and Alignment-free: Incomplete Multi-view Clustering Driven by Consensus Semantic Learning",
    "volume": "main",
    "abstract": "In incomplete multi-view clustering (IMVC), missing data induce prototype shifts within views and semantic inconsistencies across views. A feasible solution is to explore cross-view consistency in paired complete observations, further imputing and aligning the similarity relationships inherently shared across views. Nevertheless, existing methods are constrained by two-tiered limitations: (1) Neither instance- nor cluster-level consistency learning construct a semantic space shared across views to learn consensus semantics. The former enforces cross-view instances alignment, and wrongly regards unpaired observations with semantic consistency as negative pairs; the latter focuses on cross-view cluster counterparts while coarsely handling fine-grained intra-cluster relationships within views. (2) Excessive reliance on consistency results in unreliable imputation and alignment without incorporating view-specific cluster information. Thus, we propose an IMVC framework, imputation- and alignment-free for consensus semantics learning (FreeCSL). To bridge semantic gaps across all observations, we learn consensus prototypes from available data to discover a shared space, where semantically similar observations are pulled closer for consensus semantics learning. To capture semantic relationships within specific views, we design a heuristic graph clustering based on modularity to recover cluster structure with intra-cluster compactness and inter-cluster separation for cluster semantics enhancement. Extensive experiments demonstrate, compared to state-of-the-art competitors, FreeCSL achieves more confident and robust assignments on IMVC task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhuo Dai",
      "Jiaqi Jin",
      "Zhibin Dong",
      "Siwei Wang",
      "Xinwang Liu",
      "En Zhu",
      "Xihong Yang",
      "Xinbiao Gan",
      "Yu Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Sensitivity-Aware_Efficient_Fine-Tuning_via_Compact_Dynamic-Rank_Adaptation_CVPR_2025_paper.html": {
    "title": "Sensitivity-Aware Efficient Fine-Tuning via Compact Dynamic-Rank Adaptation",
    "volume": "main",
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) is a fundamental research problem in computer vision, which aims to tune a few of parameters for efficient storage and adaptation of pre-trained vision models. Recently, sensitivity-aware parameter efficient fine-tuning method (SPT) addresses this problem by identifying sensitive parameters and then leveraging its sparse characteristic to combine unstructured and structured tuning for PEFT. However, existing methods only focus on sparse characteristic of sensitive parameters but overlook its distribution characteristic, which results in additional storage burden and limited performance improvement. In this paper, we find that the distribution of sensitive parameters is not chaotic, but concentrates in a small number of rows or columns in each parameter matrix. Inspired by this fact, we propose a Compact Dynamic-Rank Adaptation-based tuning method for Sensitivity-aware Parameter efficient fine-Tuning, called CDRA-SPT. Specifically, we first identify the sensitive parameters that require tuning for each downstream task. Then, we reorganize the sensitive parameters by following its row and column into a compact sub-parameter matrix. Finally, a dynamic-rank adaptation is designed and applied at sub-parameter matrix level for PEFT. Its advantage is that the dynamic-rank characteristic of sub-parameter matrix can be fully exploited for PEFT. Extensive experiments show that our method achieves superior performance over previous state-of-the-art methods",
    "checked": true,
    "id": "9d05f4808e2371135aafd5a740ffa5600017b142",
    "semantic_title": "sensitivity-aware efficient fine-tuning via compact dynamic-rank adaptation",
    "citation_count": 1,
    "authors": [
      "Tianran Chen",
      "Jiarui Chen",
      "Baoquan Zhang",
      "Zhehao Yu",
      "Shidong Chen",
      "Rui Ye",
      "Xutao Li",
      "Yunming Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_MANTA_A_Large-Scale_Multi-View_and_Visual-Text_Anomaly_Detection_Dataset_for_CVPR_2025_paper.html": {
    "title": "MANTA: A Large-Scale Multi-View and Visual-Text Anomaly Detection Dataset for Tiny Objects",
    "volume": "main",
    "abstract": "We present MANTA, a visual-text anomaly detection dataset for tiny objects. The visual component comprises over 137.3K images across 38 object categories spanning five typical domains, of which 8.6K images are labeled as anomalous with pixel-level annotations. Each image is captured from five distinct viewpoints to ensure comprehensive object coverage. The text component consists of two subsets: Declarative Knowledge, including 875 words that describe common anomalies across various domains and specific categories, with detailed explanations for \\left< what, why, how \\right>, including causes and visual characteristics; and Constructivist Learning, providing 2K multiple-choice questions with varying levels of difficulty, each paired with images and corresponded answer explanations. We also propose a baseline for visual-text tasks and conduct extensive benchmarking experiments to evaluate advanced methods across different settings, highlighting the challenges and efficacy of our dataset",
    "checked": true,
    "id": "44cc67e36ba7ac838228bb9d975f1de4862a03c3",
    "semantic_title": "manta: a large-scale multi-view and visual-text anomaly detection dataset for tiny objects",
    "citation_count": 7,
    "authors": [
      "Lei Fan",
      "Dongdong Fan",
      "Zhiguang Hu",
      "Yiwen Ding",
      "Donglin Di",
      "Kai Yi",
      "Maurice Pagnucco",
      "Yang Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kwak_MoDec-GS_Global-to-Local_Motion_Decomposition_and_Temporal_Interval_Adjustment_for_Compact_CVPR_2025_paper.html": {
    "title": "MoDec-GS: Global-to-Local Motion Decomposition and Temporal Interval Adjustment for Compact Dynamic 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) has made significant strides in scene representation and neural rendering, with intense efforts focused on adapting it for dynamic scenes. Despite delivering remarkable rendering quality and speed, existing methods struggle with storage demands and the representation of complex real-world motions. To address these challenges, we propose MoDec-GS, a memory-efficient Gaussian splatting framework designed to reconstruct novel views in challenging scenarios with complex motions. We introduce Global-to-Local Motion Decomposition (GLMD) to effectively capture dynamic motions in a coarse-to-fine manner. This approach leverages Global Canonical Scaffolds (Global CS) and Local Canonical Scaffolds (Local CS), which extend static Scaffold representation to dynamic video reconstruction. For Global CS, we propose Global Anchor Deformation (GAD) to efficiently represent global dynamics along complex motions by directly deforming the implicit Scaffold attributes, including anchor position, offset, and local context features. Next, we finely adjust local motions via the Local Gaussian Deformation (LGD) of Local CS explicitly. Additionally, we introduce Temporal Interval Adjustment (TIA) to automatically control the temporal coverage of each Local CS during training, enabling MoDec-GS to find optimal interval assignments based on the specified number of temporal segments. Extensive evaluations demonstrate that MoDec-GS achieves an average 70% reduction in model size over state-of-the-art methods for dynamic 3D Gaussians from real-world dynamic videos while maintaining or even improving rendering quality",
    "checked": true,
    "id": "41cfff98f3d89f454f2d4b2b0fbbd1997714b162",
    "semantic_title": "modec-gs: global-to-local motion decomposition and temporal interval adjustment for compact dynamic 3d gaussian splatting",
    "citation_count": 5,
    "authors": [
      "Sangwoon Kwak",
      "Joonsoo Kim",
      "Jun Young Jeong",
      "Won-Sik Cheong",
      "Jihyong Oh",
      "Munchurl Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_DaCapo_Score_Distillation_as_Stacked_Bridge_for_Fast_and_High-quality_CVPR_2025_paper.html": {
    "title": "DaCapo: Score Distillation as Stacked Bridge for Fast and High-quality 3D Editing",
    "volume": "main",
    "abstract": "Score Distillation Sampling (SDS) has been successfully extended to text-driven 3D scene editing with 2D pretrained diffusion models. However, SDS-based editing methods suffer from lengthy optimization processes with slow inference and low quality. We attribute the issue of lengthy optimization to the stochastic optimization scheme used in SDS-based editing, where many steps may conflict with each other (e.g., the inherent trade-off between editing and preservation). To reduce this internal conflict and speed up the editing process, we propose to separate editing and preservation in time with a diffusion time schedule and frame the 3D editing optimization process as a diffusion bridge sampling process. Motivated by the analysis above, we introduce DaCapo, a fast diffusion sampling-like 3D editing method that incorporates a novel stacked bridge framework, which estimates a direct diffusion bridge between source and target distribution with only a pretrained 2D diffusion model. Specifically, It models the editing process as a combination of inversion and generation, where both processes happen simultaneously as a stack of Diffusion Bridges. DaCapo shows a 15x speed-up with comparable results to the state-of-the-art SDS-based method. It completes the process in just 2,500 steps on a single GPU and accommodates a variety of 3D representation methods",
    "checked": true,
    "id": "12073255bead0da900cd1df012e5ceecf4912441",
    "semantic_title": "dacapo: score distillation as stacked bridge for fast and high-quality 3d editing",
    "citation_count": 2,
    "authors": [
      "Yufei Huang",
      "Bangyan Liao",
      "Yuqi Hu",
      "Haitao Lin",
      "Lirong Wu",
      "Siyuan Li",
      "Cheng Tan",
      "Zicheng Liu",
      "Yunfan Liu",
      "Zelin Zang",
      "Chang Yu",
      "Zhen Lei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_A_Selective_Re-learning_Mechanism_for_Hyperspectral_Fusion_Imaging_CVPR_2025_paper.html": {
    "title": "A Selective Re-learning Mechanism for Hyperspectral Fusion Imaging",
    "volume": "main",
    "abstract": "Hyperspectral fusion imaging is challenged by high computational cost due to the abundant spectral information. We find that pixels in regions with smooth spatial-spectral structure can be reconstructed well using a shallow network, while only those in regions with complex spatial-spectral structure require a deeper network. However, existing methods process all pixels uniformly, which ignores this property. To leverage this property, we propose a Selective Re-learning Fusion Network (SRLF) that initially extracts features from all pixels uniformly and then selectively refines distorted feature points. Specifically, SRLF first employs a Preliminary Fusion Module with robust global modeling capability to generate a preliminary fusion feature. Afterward, it applies a Selective Re-learning Module to focus on improving distorted feature points in the preliminary fusion feature. To achieve targeted learning, we present a novel Spatial-Spectral Structure-Guided Selective Re-learning Mechanism (SSG-SRL) that integrates the observation model to identify the feature points with spatial or spectral distortions. Only these distorted points are sent to the corresponding re-learning blocks, reducing both computational cost and the risk of overfitting. Finally, we develop an SRLF-Net, composed of multiple cascaded SRLFs, which surpasses multiple state-of-the-art methods on several datasets with minimal computational cost",
    "checked": true,
    "id": "5557d258758ede3e338ebe5df099f353700979d6",
    "semantic_title": "a selective re-learning mechanism for hyperspectral fusion imaging",
    "citation_count": 0,
    "authors": [
      "Yuanye Liu",
      "Jinyang Liu",
      "Renwei Dian",
      "Shutao Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_SCSegamba_Lightweight_Structure-Aware_Vision_Mamba_for_Crack_Segmentation_in_Structures_CVPR_2025_paper.html": {
    "title": "SCSegamba: Lightweight Structure-Aware Vision Mamba for Crack Segmentation in Structures",
    "volume": "main",
    "abstract": "Pixel-level segmentation of structural cracks across various scenarios remains a considerable challenge. Current methods encounter challenges in effectively modeling crack morphology and texture, facing challenges in balancing segmentation quality with low computational resource usage. To overcome these limitations, we propose a lightweight Structure-Aware Vision Mamba Network (SCSegamba), capable of generating high-quality pixel-level segmentation maps by leveraging both the morphological information and texture cues of crack pixels with minimal computational cost. Specifically, we developed a Structure-Aware Visual State Space module (SAVSS), which incorporates a lightweight Gated Bottleneck Convolution (GBC) and a Structure-Aware Scanning Strategy (SASS). The key insight of GBC lies in its effectiveness in modeling the morphological information of cracks, while the SASS enhances the perception of crack topology and texture by strengthening the continuity of semantic information between crack pixels. Experiments on crack benchmark datasets demonstrate that our method outperforms other state-of-the-art (SOTA) methods, achieving the highest performance with only 2.8M parameters. On the multi-scenario dataset, our method reached 0.8390 in F1 score and 0.8479 in mIoU",
    "checked": true,
    "id": "1ebc529c300fadac3eeff5778af123a9ddfb798f",
    "semantic_title": "scsegamba: lightweight structure-aware vision mamba for crack segmentation in structures",
    "citation_count": 4,
    "authors": [
      "Hui Liu",
      "Chen Jia",
      "Fan Shi",
      "Xu Cheng",
      "Shengyong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Autoregressive_Sequential_Pretraining_for_Visual_Tracking_CVPR_2025_paper.html": {
    "title": "Autoregressive Sequential Pretraining for Visual Tracking",
    "volume": "main",
    "abstract": "Recent advancements in visual object tracking have shifted towards a sequential generation paradigm, where object deformation and motion exhibit strong temporal dependencies. Despite the importance of these dependencies, widely adopted image-level pretrained backbones barely capture the dynamics in the consecutive video, which is the essence of tracking. Thus, we propose AutoRegressive Sequential Pretraining (ARP), an unsupervised spatio-temporal learner, via generating the evolution of object appearance and motion in video sequences.Our method leverages a diffusion model to autoregressively generate the future frame appearance, conditioned on historical embeddings extracted by a general encoder. Furthermore, to ensure trajectory coherence, the same encoder is employed to learn trajectory consistency by generating coordinate sequences in a reverse autoregressive fashion, a process we term back-tracking. Further, we integrate the pretrained ARP into ARTrackV2, creating ARPTrack, which is further fine-tuned for tracking tasks. ARPTrack achieves state-of-the-art performance across multiple benchmarks, becoming the first tracker to surpass 80% AO on GOT-10k, while maintaining high efficiency. These results demonstrate the effectiveness of our approach in capturing temporal dependencies for continuous video tracking",
    "checked": true,
    "id": "2a7fd569e1c203f3ad25fcd1d8412b2efb3bbe6e",
    "semantic_title": "autoregressive sequential pretraining for visual tracking",
    "citation_count": 3,
    "authors": [
      "Shiyi Liang",
      "Yifan Bai",
      "Yihong Gong",
      "Xing Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Number_it_Temporal_Grounding_Videos_like_Flipping_Manga_CVPR_2025_paper.html": {
    "title": "Number it: Temporal Grounding Videos like Flipping Manga",
    "volume": "main",
    "abstract": "Video Large Language Models (Vid-LLMs) have made remarkable advancements in comprehending video content for QA dialogue. However, they struggle to extend this visual understanding to tasks requiring precise temporal localization, known as Video Temporal Grounding (VTG). To address this, we introduce Number-Prompt (NumPro), a novel method that empowers Vid-LLMs to bridge visual comprehension with temporal grounding by adding unique numerical identifiers to each video frame. Treating a video as a sequence of numbered frame images, NumPro transforms VTG into an intuitive process: flipping through manga panels in sequence. This allows Vid-LLMs to \"read\" event timelines, accurately linking visual content with corresponding temporal information. Our experiments demonstrate that NumPro significantly boosts VTG performance of top-tier Vid-LLMs without additional computational cost. Furthermore, fine-tuning on a NumPro-enhanced dataset defines a new state-of-the-art for VTG, surpassing previous top-performing methods by up to 6.9% in mIoU for moment retrieval and 8.5% in mAP for highlight detection. The code is available at https://github.com/yongliang-wu/NumPro",
    "checked": true,
    "id": "1b794e2d6d228c6d53df68b516b155cbb922573f",
    "semantic_title": "number it: temporal grounding videos like flipping manga",
    "citation_count": 18,
    "authors": [
      "Yongliang Wu",
      "Xinting Hu",
      "Yuyang Sun",
      "Yizhou Zhou",
      "Wenbo Zhu",
      "Fengyun Rao",
      "Bernt Schiele",
      "Xu Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zu_Collaborative_Tree_Search_for_Enhancing_Embodied_Multi-Agent_Collaboration_CVPR_2025_paper.html": {
    "title": "Collaborative Tree Search for Enhancing Embodied Multi-Agent Collaboration",
    "volume": "main",
    "abstract": "Embodied agents based on large language models (LLMs) face significant challenges in collaborative tasks, requiring effective communication and reasonable division of labor to ensure efficient and correct task completion. Previous approaches with simple communication patterns carry erroneous or incoherent agent actions, which can lead to additional risks. To address these problems, we propose Cooperative Tree Search (CoTS), a framework designed to significantly improve collaborative planning and task execution efficiency among embodied agents. CoTS guides multi-agents to discuss long-term strategic plans within a modified Monte Carlo tree, searching along LLM-driven reward functions to provide a more thoughtful and promising approach to cooperation. Another key feature of our method is the introduction of a plan evaluation module, which not only prevents agent action confusion caused by frequent plan updates but also ensures plan updates when the current plan becomes unsuitable. Experimental results show that the proposed method performs excellently in planning, communication, and collaboration on embodied environments (CWAH and TDW-MAT), efficiently completing long-term, complex tasks and significantly outperforming existing methods",
    "checked": true,
    "id": "0458899327d1fb8b8b3fc5a141cca832aa66288b",
    "semantic_title": "collaborative tree search for enhancing embodied multi-agent collaboration",
    "citation_count": 1,
    "authors": [
      "Lizheng Zu",
      "Lin Lin",
      "Song Fu",
      "Na Zhao",
      "Pan Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_PromptHMR_Promptable_Human_Mesh_Recovery_CVPR_2025_paper.html": {
    "title": "PromptHMR: Promptable Human Mesh Recovery",
    "volume": "main",
    "abstract": "Human pose and shape (HPS) estimation presents challenges in diverse scenarios such as crowded scenes, person-person interactions, and single-view reconstruction. Existing approaches lack mechanisms to incorporate auxiliary \"side information\" that could enhance reconstruction accuracy in such challenging scenarios. Furthermore, the most accurate methods rely on cropped person detections and cannot exploit scene context while methods that process the whole image often fail to detect people and are less accurate than methods that use crops. While recent language-based methods explore HPS reasoning through large language or vision-language models, their metric accuracy is well below the state of the art. In contrast, we present PromptHMR, a transformer-based promptable method that reformulates HPS estimation through spatial and semantic prompts. Our method processes full images to maintain scene context and accepts multiple input modalities: spatial prompts like bounding boxes and masks, and semantic prompts like language descriptions or interaction labels. PromptHMR demonstrates robust performance across challenging scenarios: estimating people from bounding boxes as small as faces in crowded scenes, improving body shape estimation through language descriptions, modeling person-person interactions, and producing temporally coherent motions in videos. Experiments on benchmarks show that PromptHMR achieves state-of-the-art performance while offering flexible prompt-based control over the HPS estimation process",
    "checked": true,
    "id": "a6b084b4046e93d72d39ef4a818dbf82c27d20e6",
    "semantic_title": "prompthmr: promptable human mesh recovery",
    "citation_count": 4,
    "authors": [
      "Yufu Wang",
      "Yu Sun",
      "Priyanka Patel",
      "Kostas Daniilidis",
      "Michael J. Black",
      "Muhammed Kocabas"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pallotta_SyncVP_Joint_Diffusion_for_Synchronous_Multi-Modal_Video_Prediction_CVPR_2025_paper.html": {
    "title": "SyncVP: Joint Diffusion for Synchronous Multi-Modal Video Prediction",
    "volume": "main",
    "abstract": "Predicting future video frames is essential for decision-making systems, yet RGB frames alone often lack the information needed to fully capture the underlying complexities of the real world. To address this limitation, we propose a multi-modal framework for Synchronous Video Prediction (SyncVP) that incorporates complementary data modalities, enhancing the richness and accuracy of future predictions. SyncVP builds on pre-trained modality-specific diffusion models and introduces an efficient spatio-temporal cross-attention module to enable effective information sharing across modalities. We evaluate SyncVP on standard benchmark datasets, such as Cityscapes and BAIR, using depth as an additional modality. We furthermore demonstrate its generalization to other modalities on SYNTHIA with semantic information and ERA5-Land with climate data. Notably, SyncVP achieves state-of-the-art performance, even in scenarios where only one modality is present, demonstrating its robustness and potential for a wide range of applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enrico Pallotta",
      "Sina Mokhtarzadeh Azar",
      "Shuai Li",
      "Olga Zatsarynna",
      "Juergen Gall"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_HUSH_Holistic_Panoramic_3D_Scene_Understanding_using_Spherical_Harmonics_CVPR_2025_paper.html": {
    "title": "HUSH: Holistic Panoramic 3D Scene Understanding using Spherical Harmonics",
    "volume": "main",
    "abstract": "Motivated by the efficiency of spherical harmonics (SH) in representing various physical phenomena, we propose a Holistic panoramic 3D scene Understanding framework using Spherical Harmonics, dubbed as HUSH. Our approach focuses on a unified framework adaptable to various 3D scene understanding tasks via SH bases. To achieve this, we first estimate SH coefficients, allowing for the adaptive configuration of the SH bases specific to each scene. HUSH then employs a hierarchical attention module that uses SH bases as queries to generate comprehensive scene features by integrating these scene-adaptive SH bases with image features. Additionally, we introduce an SH basis index module that adaptively emphasizes relevant SH bases to produce task-relevant features, enhancing the versatility of HUSH across different scene understanding tasks. Finally, by combining the scene features with task-relevant features in the task-specific heads, we perform various scene understanding tasks, including depth, surface normal and room layout estimation. Experiments demonstrate that HUSH achieves state-of-the-art performance on depth estimation benchmarks, highlighting the robustness and scalability of using SH in panoramic 3D scene understanding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongsung Lee",
      "Harin Park",
      "Byeong-Uk Lee",
      "Kyungdon Joo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SkillMimic_Learning_Basketball_Interaction_Skills_from_Demonstrations_CVPR_2025_paper.html": {
    "title": "SkillMimic: Learning Basketball Interaction Skills from Demonstrations",
    "volume": "main",
    "abstract": "Traditional reinforcement learning methods for human-object interaction (HOI) rely on labor-intensive, manually designed skill rewards that do not generalize well across different interactions. We introduce SkillMimic, a unified data-driven framework that fundamentally changes how agents learn interaction skills by eliminating the need for skill-specific rewards. Our key insight is that a unified HOI imitation reward can effectively capture the essence of diverse interaction patterns from HOI datasets. This enables SkillMimic to learn a single policy that not only masters multiple interaction skills but also facilitates skill transitions, with both diversity and generalization improving as the HOI dataset grows. For evaluation, we collect and introduce two basketball datasets containing approximately 35 minutes of diverse basketball skills. Extensive experiments show that SkillMimic successfully masters a wide range of basketball skills including stylistic variations in dribbling, layup, and shooting. Moreover, these learned skills can be effectively composed by a high-level controller to accomplish complex and long-horizon tasks such as consecutive scoring, opening new possibilities for scalable and generalizable interaction skill learning. Project page: https://ingrid789.github.io/SkillMimic/",
    "checked": true,
    "id": "3b47700f8c75739392bfb4a3a1ea3fa0458c7fb8",
    "semantic_title": "skillmimic: learning basketball interaction skills from demonstrations",
    "citation_count": 4,
    "authors": [
      "Yinhuai Wang",
      "Qihan Zhao",
      "Runyi Yu",
      "Hok Wai Tsui",
      "Ailing Zeng",
      "Jing Lin",
      "Zhengyi Luo",
      "Jiwen Yu",
      "Xiu Li",
      "Qifeng Chen",
      "Jian Zhang",
      "Lei Zhang",
      "Ping Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/You_VISTREAM_Improving_Computation_Efficiency_of_Visual_Streaming_Perception_via_Law-of-Charge-Conservation_CVPR_2025_paper.html": {
    "title": "VISTREAM: Improving Computation Efficiency of Visual Streaming Perception via Law-of-Charge-Conservation Inspired Spiking Neural Network",
    "volume": "main",
    "abstract": "Visual streaming perception (VSP) involves online intelligent processing of sequential frames captured by vision sensors, enabling real-time decision-making in applications such as autonomous driving, UAVs, and AR/VR. However, the computational efficiency of VSP on edge devices remains a challenge due to power constraints and the underutilization of temporal dependencies between frames. While spiking neural networks (SNNs) offer biologically inspired event-driven processing with potential energy benefits, their practical advantage over artificial neural networks (ANNs) for VSP tasks remains unproven.In this work, we introduce a novel framework, VISTREAM, which leverages the Law of Charge Conservation (LoCC) property in ST-BIF neurons and a differential encoding (DiffEncode) scheme to optimize SNN inference for VSP. By encoding temporal differences between neighboring frames and eliminating frequent membrane resets, VISTREAM achieves significant computational efficiency while maintaining accuracy equivalent to its ANN counterpart. We provide theoretical proofs of equivalence and validate VISTREAM across diverse VSP tasks, including object detection, tracking, and segmentation, demonstrating substantial energy savings without compromising performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kang You",
      "Ziling Wei",
      "Jing Yan",
      "Boning Zhang",
      "Qinghai Guo",
      "Yaoyu Zhang",
      "Zhezhi He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Garg_STPro_Spatial_and_Temporal_Progressive_Learning_for_Weakly_Supervised_Spatio-Temporal_CVPR_2025_paper.html": {
    "title": "STPro: Spatial and Temporal Progressive Learning for Weakly Supervised Spatio-Temporal Grounding",
    "volume": "main",
    "abstract": "In this work, we study Weakly Supervised Spatio-Temporal Video Grounding (WSTVG), a challenging task of localizing subjects spatio-temporally in videos using only textual queries and no bounding box supervision. Inspired by recent advances in vision-language foundation models, we investigate their utility for WSTVG, leveraging their zero-shot grounding capabilities. However, we find that a simple adaptation lacks essential spatio-temporal grounding abilities. To bridge this gap, we introduce Tubelet Referral Grounding (TRG), which connects textual queries to tubelets to enable spatio-temporal predictions. Despite its promise, TRG struggles with compositional action understanding and dense scene scenarios. To address these limitations, we propose STPro, a progressive learning framework with two key modules: Sub-Action Temporal Curriculum Learning (SA-TCL), which incrementally builds compositional action understanding, and Congestion-Guided Spatial Curriculum Learning (CG-SCL), which adapts the model to complex scenes by spatially increasing task difficulty. STPro achieves state-of-the-art results on three benchmark datasets, with improvements of 1.0% on VidSTG-Declarative and 3.0% on HCSTVG-v1",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aaryan Garg",
      "Akash Kumar",
      "Yogesh S Rawat"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_RGBAvatar_Reduced_Gaussian_Blendshapes_for_Online_Modeling_of_Head_Avatars_CVPR_2025_paper.html": {
    "title": "RGBAvatar: Reduced Gaussian Blendshapes for Online Modeling of Head Avatars",
    "volume": "main",
    "abstract": "We present Reduced Gaussian Blendshapes Avatar (RGBAvatar), a method for reconstructing photorealistic, animatable head avatars at speeds sufficient for on-the-fly reconstruction. Unlike prior approaches that utilize linear bases from 3D morphable models (3DMM) to model Gaussian blendshapes, our method maps tracked 3DMM parameters into reduced blendshape weights with an MLP, leading to a compact set of blendshape bases. The learned compact base composition effectively captures essential facial details for specific individuals, and does not rely on the fixed base composition weights of 3DMM, leading to enhanced reconstruction quality and higher efficiency. To further expedite the reconstruction process, we develop a novel color initialization estimation method and a batch-parallel Gaussian rasterization process, achieving state-of-the-art quality with training throughput of about 630 images per second. Moreover, we propose a local-global sampling strategy that enables direct on-the-fly reconstruction, immediately reconstructing the model as video streams in real time while achieving quality comparable to offline settings. Our source code is available at https://github.com/gapszju/RGBAvatar",
    "checked": true,
    "id": "86d7b8785bca05b9e5690539a366da6fb020b32f",
    "semantic_title": "rgbavatar: reduced gaussian blendshapes for online modeling of head avatars",
    "citation_count": 2,
    "authors": [
      "Linzhou Li",
      "Yumeng Li",
      "Yanlin Weng",
      "Youyi Zheng",
      "Kun Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Donnelly_Rashomon_Sets_for_Prototypical-Part_Networks_Editing_Interpretable_Models_in_Real-Time_CVPR_2025_paper.html": {
    "title": "Rashomon Sets for Prototypical-Part Networks: Editing Interpretable Models in Real-Time",
    "volume": "main",
    "abstract": "Interpretability is critical for machine learning models in high-stakes settings because it allows users to verify the model's reasoning. In computer vision, prototypical part models (ProtoPNets) have become the dominant model type to meet this need. Users can easily identify flaws in ProtoPNets, but fixing problems in a ProtoPNet requires slow, difficult retraining that is not guaranteed to resolve the issue. This problem is called the \"interaction bottleneck.\" We solve the interaction bottleneck for ProtoPNets by simultaneously finding many equally good ProtoPNets (i.e., a draw from a \"Rashomon set\"). We show that our framework - called Proto-RSet - quickly produces many accurate, diverse ProtoPNets, allowing users to correct problems in real time while maintaining performance guarantees with respect to the training set. We demonstrate the utility of this method in two settings: 1) removing synthetic bias introduced to a bird-identification model and 2) debugging a skin cancer identification model. This tool empowers non-machine-learning experts, such as clinicians or domain experts, to quickly refine and correct machine learning models without repeated retraining by machine learning experts",
    "checked": true,
    "id": "8a1203de104759035aa4a9ffcb627a73e0faef2a",
    "semantic_title": "rashomon sets for prototypical-part networks: editing interpretable models in real-time",
    "citation_count": 2,
    "authors": [
      "Jon Donnelly",
      "Zhicheng Guo",
      "Alina Jade Barnett",
      "Hayden McTavish",
      "Chaofan Chen",
      "Cynthia Rudin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_EEE-Bench_A_Comprehensive_Multimodal_Electrical_And_Electronics_Engineering_Benchmark_CVPR_2025_paper.html": {
    "title": "EEE-Bench: A Comprehensive Multimodal Electrical And Electronics Engineering Benchmark",
    "volume": "main",
    "abstract": "Recent studies on large language models (LLMs) and large multimodal models (LMMs) have demonstrated promising skills in various domains including science and mathematics. However, their capability in more challenging and real-world related scenarios like engineering has not been systematically studied. To bridge this gap, we propose EEE-Bench, a multimodal benchmark aimed at assessing LMMs' capabilities in solving practical engineering tasks, using electrical and electronics engineering (EEE) as the testbed. Our benchmark consists of 2860 hand-picked and carefully curated problems spanning 10 essential subdomains such as analog circuits, control systems, etc. Compared to other domains, engineering problems are intrinsically 1) more visually complex and versatile and 2) less deterministic in solutions. Successful solutions to these problems often demand more-than-usual rigorous integration of visual and textual information as models need to understand intricate images like abstract circuits and system diagrams while taking professional instructions. Alongside EEE-Bench, we provide extensive quantitative evaluations, fine-grained analysis, and improvement methods using 17 widely-used open- and closed-sourced LLMs and LMMs and 7 popular prompting techniques. Our results reveal notable deficiencies in current foundation models for EEE, including an average performance ranging from 19.48% to 46.78% and a tendency toward \"laziness\" in overlooking essential visual context. In summary, we believe EEE-Bench not only reveals some noteworthy limitations of LMMs but also provides a valuable resource for advancing research on their application in practical engineering tasks, driving future improvements in their capability to handle complex, real-world scenarios",
    "checked": true,
    "id": "257e659de68385696f6c3b412ef82d6958897989",
    "semantic_title": "eee-bench: a comprehensive multimodal electrical and electronics engineering benchmark",
    "citation_count": 4,
    "authors": [
      "Ming Li",
      "Jike Zhong",
      "Tianle Chen",
      "Yuxiang Lai",
      "Konstantinos Psounis"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Text-Driven_Fashion_Image_Editing_with_Compositional_Concept_Learning_and_Counterfactual_CVPR_2025_paper.html": {
    "title": "Text-Driven Fashion Image Editing with Compositional Concept Learning and Counterfactual Abduction",
    "volume": "main",
    "abstract": "Fashion image editing is a valuable tool for designers to convey their creative ideas by visualizing design concepts. With the recent advances in text editing methods, significant progress has been made in fashion image editing. However, they face two key challenges: spurious correlations in training data often induce changes in other areas when editing an area representing the intended editing concept, and these models typically lack the ability to edit multiple concepts simultaneously. To address the above challenges, we propose a novel \\underline T ext-driven \\underline F ashion \\underline I mage edi\\underline T ing framework called T-FIT to mitigate the impact of spurious correlation by integrating counterfactual reasoning with compositional concept learning to precisely ensure compositional multi-concept fashion image editing relying solely on text descriptions. Specifically, T-FIT includes three key components. (i) Counterfactual abduction module, which learns an exogenous variable of the source image by a denoising U-Net model. (ii) Concept learning module, which identifies concepts in fashion image editing--such as clothing types and colors and projects a target concept into the space spanned from a series of textual prompts. (iii) Concept composition module, which enables simultaneous adjustments of multiple concepts by aggregating each concept's direction vector obtained from the concept learning module. Extensive experiments show that our method can achieve state-of-the-art performance on various fashion image editing tasks, including single-concept editing (e.g., sleeve length, clothing type) and multi-concept editing (e.g., color & sleeve length)",
    "checked": true,
    "id": "a256d32e9a2adc3fcd56a6ddf1133538ba90a238",
    "semantic_title": "text-driven fashion image editing with compositional concept learning and counterfactual abduction",
    "citation_count": 1,
    "authors": [
      "Shanshan Huang",
      "Haoxuan Li",
      "Chunyuan Zheng",
      "Mingyuan Ge",
      "Wei Gao",
      "Lei Wang",
      "Li Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_EnvPoser_Environment-aware_Realistic_Human_Motion_Estimation_from_Sparse_Observations_with_CVPR_2025_paper.html": {
    "title": "EnvPoser: Environment-aware Realistic Human Motion Estimation from Sparse Observations with Uncertainty Modeling",
    "volume": "main",
    "abstract": "Estimating full-body motion using the tracking signals of head and hands from VR devices holds great potential for various applications. However, the sparsity and unique distribution of observations present a significant challenge, resulting in an ill-posed problem with multiple feasible solutions (i.e., hypotheses). This amplifies uncertainty and ambiguity in full-body motion estimation, especially for the lower-body joints. Therefore, we propose a new method, EnvPoser, that employs a two-stage framework to perform full-body motion estimation using sparse tracking signals and pre-scanned environment from VR devices. EnvPoser models the multi-hypothesis nature of human motion through an uncertainty-aware estimation module in the first stage. In the second stage, we refine these multi-hypothesis estimates by integrating semantic and geometric environmental constraints, ensuring that the final motion estimation aligns realistically with both the environmental context and physical interactions.Qualitative and quantitative experiments on two public datasets demonstrate that our method achieves state-of-the-art performance, highlighting significant improvements in human motion estimation within motion-environment interaction scenarios. Project page: https://xspc.github.io/EnvPoser/",
    "checked": true,
    "id": "4c075231bbf0ef70ac8a8e88c94e303287b1e7ca",
    "semantic_title": "envposer: environment-aware realistic human motion estimation from sparse observations with uncertainty modeling",
    "citation_count": 3,
    "authors": [
      "Songpengcheng Xia",
      "Yu Zhang",
      "Zhuo Su",
      "Xiaozheng Zheng",
      "Zheng Lv",
      "Guidong Wang",
      "Yongjie Zhang",
      "Qi Wu",
      "Lei Chu",
      "Ling Pei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Heidari_A_Unified_Framework_for_Heterogeneous_Semi-supervised_Learning_CVPR_2025_paper.html": {
    "title": "A Unified Framework for Heterogeneous Semi-supervised Learning",
    "volume": "main",
    "abstract": "In this work, we introduce a novel problem setup termed as Heterogeneous Semi-Supervised Learning (HSSL), which presents unique challenges by bridging the semi-supervised learning (SSL) task and the unsupervised domain adaptation (UDA) task, and expanding standard semi-supervised learning to cope with heterogeneous training data. At its core, HSSL aims to learn a prediction model using a combination of labeled and unlabeled training data drawn separately from heterogeneous domains that share a common set of semantic categories; this model is intended to differentiate the semantic categories of test instances sampled from both the labeled and unlabeled domains. In particular, the labeled and unlabeled domains have dissimilar label distributions and class feature distributions. This heterogeneity, coupled with the assorted sources of the test data, introduces significant challenges to standard SSL and UDA methods. Therefore, we propose a novel method, Unified Framework for Heterogeneous Semi-supervised Learning (Uni-HSSL), to address HSSL by directly learning a fine-grained classifier from the heterogeneous data, which adaptively handles the inter-domain heterogeneity while leveraging both the unlabeled data and the inter-domain semantic class relationships for cross-domain knowledge transfer and adaptation. We conduct comprehensive experiments and the experimental results validate the efficacy and superior performance of the proposed Uni-HSSL over state-of-the-art semi-supervised learning and unsupervised domain adaptation methods",
    "checked": true,
    "id": "b40072447dfc195888becc43d0fd9cd0ac71785f",
    "semantic_title": "a unified framework for heterogeneous semi-supervised learning",
    "citation_count": 0,
    "authors": [
      "Marzi Heidari",
      "Abdullah Alchihabi",
      "Hao Yan",
      "Yuhong Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Adapting_Text-to-Image_Generation_with_Feature_Difference_Instruction_for_Generic_Image_CVPR_2025_paper.html": {
    "title": "Adapting Text-to-Image Generation with Feature Difference Instruction for Generic Image Restoration",
    "volume": "main",
    "abstract": "Diffusion-based Text-to-Image (T2I) models have demonstrated significant potential in image restoration. However, existing models continue to grapple with challenges such as complex training and prompt design. We introduce a new perspective for improving image restoration by injecting knowledge from pretrained vision-language models into current T2I models. We empirically show that the degradation and content representations in BLIP-2 can be linearly separated, providing promising degradation guidance for image restoration. Specifically, the Feature Difference Instruction (FDI) is first extracted by Q-Formers through a simple subtraction operation based on reference image pairs. Then, we propose a multi-scale FDI adapter to decouple the degradation style and corrupted artifacts, and inject the styleflow exclusively into specific blocks through adapter-tuning, thereby preventing noise interference and eschewing the need for cumbersome weight retraining. In this way, we can train various task-specific adapters according to different degradations, achieving rich detail enhancement in the restoration results. Furthermore, the proposed FDI adapters have attractive properties of practical value, such as composability and generalization ability for all-in-one and mixed-degradation restoration. Extensive experiments under various settings demonstrate that our method has promising repairing quality over 10 image restoration tasks and a wide range of other applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Wang",
      "Hehe Fan",
      "Huichen Yang",
      "Sarvnaz Karimi",
      "Lina Yao",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zha_ReCon_Enhancing_True_Correspondence_Discrimination_through_Relation_Consistency_for_Robust_CVPR_2025_paper.html": {
    "title": "ReCon: Enhancing True Correspondence Discrimination through Relation Consistency for Robust Noisy Correspondence Learning",
    "volume": "main",
    "abstract": "Can we accurately identify the true correspondences from multimodal datasets containing mismatched data pairs? Existing methods primarily emphasize the similarity matching between the representations of objects across modalities, potentially neglecting the crucial relation consistency within modalities that are particularly important for distinguishing the true and false correspondences. Such an omission often runs the risk of misidentifying negatives as positives, thus leading to unanticipated performance degradation. To address this problem, we propose a general Relation Consistency learning framework, namely ReCon, to accurately discriminate the true correspondences among the multimodal data and thus effectively mitigate the adverse impact caused by mismatches. Specifically, ReCon leverages a novel relation consistency learning to ensure the dual-alignment, respectively of, the cross-modal relation consistency between different modalities and the intra-modal relation consistency within modalities. Thanks to such dual constrains on relations, ReCon significantly enhances its effectiveness for true correspondence discrimination and therefore reliably filters out the mismatched pairs to mitigate the risks of wrong supervisions. Extensive experiments on three widely-used benchmark datasets, including Flickr30K, MS-COCO, and Conceptual Captions, are conducted to demonstrate the effectiveness and superiority of ReCon compared with other SOTAs. The code is available at: https://anonymous.4open.science/r/ReCon-NCL",
    "checked": true,
    "id": "eb9d8334080429602e216ceb80642d23bdb51ab8",
    "semantic_title": "recon: enhancing true correspondence discrimination through relation consistency for robust noisy correspondence learning",
    "citation_count": 0,
    "authors": [
      "Quanxing Zha",
      "Xin Liu",
      "Shu-Juan Peng",
      "Yiu-ming Cheung",
      "Xing Xu",
      "Nannan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bao_Free360_Layered_Gaussian_Splatting_for_Unbounded_360-Degree_View_Synthesis_from_CVPR_2025_paper.html": {
    "title": "Free360: Layered Gaussian Splatting for Unbounded 360-Degree View Synthesis from Extremely Sparse and Unposed Views",
    "volume": "main",
    "abstract": "Neural rendering has demonstrated remarkable success in high-quality 3D neural reconstruction and novel view synthesis with dense input views and accurate poses. However, applying it to sparse, unposed views in unbounded 360* scenes remains a challenging problem. In this paper, we propose a novel neural rendering framework to accomplish the unposed and extremely sparse-view 3D reconstruction in unbounded 360* scenes. To resolve the spatial ambiguity inherent in unbounded scenes with sparse input views, we propose a layered Gaussian-based representation to effectively model the scene with distinct spatial layers. By employing a dense stereo reconstruction model to recover coarse geometry, we introduce a layer-specific bootstrap optimization to refine the noise and fill occluded regions in the reconstruction. Furthermore, we propose an iterative fusion of reconstruction and generation alongside an uncertainty-aware training approach to facilitate mutual conditioning and enhancement between these two processes. Comprehensive experiments show that our approach outperforms existing state-of-the-art methods in terms of rendering quality and surface reconstruction accuracy. Project page: https://zju3dv.github.io/free360/",
    "checked": true,
    "id": "f8386b5be704ae181e34c74fc3491315f179f93c",
    "semantic_title": "free360: layered gaussian splatting for unbounded 360-degree view synthesis from extremely sparse and unposed views",
    "citation_count": 3,
    "authors": [
      "Chong Bao",
      "Xiyu Zhang",
      "Zehao Yu",
      "Jiale Shi",
      "Guofeng Zhang",
      "Songyou Peng",
      "Zhaopeng Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Novello_Tuning_the_Frequencies_Robust_Training_for_Sinusoidal_Neural_Networks_CVPR_2025_paper.html": {
    "title": "Tuning the Frequencies: Robust Training for Sinusoidal Neural Networks",
    "volume": "main",
    "abstract": "Sinusoidal neural networks have been shown effective as implicit neural representations (INRs) of low-dimensional signals, due to their smoothness and high representation capacity. However, initializing and training them remain empirical tasks which lack on deeper understanding to guide the learning process. To fill this gap, our work introduces a theoretical framework that explains the capacity property of sinusoidal networks and offers robust control mechanisms for initialization and training. Our analysis is based on a novel amplitude-phase expansion of the sinusoidal multilayer perceptron, showing how its layer compositions produce a large number of new frequencies expressed as integer combinations of the input frequencies. This relationship can be directly used to initialize the input neurons, as a form of spectral sampling, and to bound the network's spectrum while training. Our method, referred to as TUNER (TUNing sinusoidal nEtwoRks), greatly improves the stability and convergence of sinusoidal INR training, leading to detailed reconstructions, while preventing overfitting",
    "checked": true,
    "id": "57fa8b20cb5e5d2e84a2f561eff0fb421744b96a",
    "semantic_title": "tuning the frequencies: robust training for sinusoidal neural networks",
    "citation_count": 3,
    "authors": [
      "Tiago Novello",
      "Diana Aldana",
      "Andre Araujo",
      "Luiz Velho"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chng_Preconditioners_for_the_Stochastic_Training_of_Neural_Fields_CVPR_2025_paper.html": {
    "title": "Preconditioners for the Stochastic Training of Neural Fields",
    "volume": "main",
    "abstract": "Neural fields encode continuous multidimensional signals as neural networks, enabling diverse applications in computer vision, robotics, and geometry. While Adam is effective for stochastic optimization, it often requires long training times. To address this, we explore alternative optimization techniques to accelerate training without sacrificing accuracy. Traditional second-order methods like L-BFGS are unsuitable for stochastic settings. We propose a theoretical framework for training neural fields with curvature-aware diagonal preconditioners, demonstrating their effectiveness across tasks such as image reconstruction, shape modeling, and Neural Radiance Fields (NeRF)",
    "checked": true,
    "id": "a2d3413dc1fb9d1f6f92e65217aa02dc315f6d3f",
    "semantic_title": "preconditioners for the stochastic training of neural fields",
    "citation_count": 1,
    "authors": [
      "Shin-Fang Chng",
      "Hemanth Saratchandran",
      "Simon Lucey"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Open_Ad-hoc_Categorization_with_Contextualized_Feature_Learning_CVPR_2025_paper.html": {
    "title": "Open Ad-hoc Categorization with Contextualized Feature Learning",
    "volume": "main",
    "abstract": "Adaptive categorization of visual scenes is essential for AI agents to handle changing tasks. Unlike fixed common categories for plants or animals, ad-hoc categories, such as things to sell at a garage sale, are created dynamically to achieve specific tasks. We study open ad-hoc categorization, where the goal is to infer novel concepts and categorize images based on a given context, a small set of labeled exemplars, and some unlabeled data. We have two key insights: 1) recognizing ad-hoc categories relies on the same perceptual processes as common categories; 2) novel concepts can be discovered semantically by expanding contextual cues or visually by clustering similar patterns. We propose OAK, a simple model that introduces a single learnable context token into CLIP, trained with CLIP's objective of aligning visual and textual features and GCD's objective of clustering similar images. On Stanford and Clevr-4 datasets, OAK consistently achieves the state-of-art in accuracy and concept discovery across multiple categorizations, including 87.4% novel accuracy on Stanford Mood, surpassing CLIP and GCD by over 50%. Moreover, OAK generates interpretable saliency maps, focusing on hands for Action, faces for Mood, and backgrounds for Location, promoting transparency and trust while enabling accurate and flexible categorization",
    "checked": true,
    "id": "0d1961806376afacc3063673bc204cf78e990420",
    "semantic_title": "open ad-hoc categorization with contextualized feature learning",
    "citation_count": 0,
    "authors": [
      "Zilin Wang",
      "Sangwoo Mo",
      "Stella X. Yu",
      "Sima Behpour",
      "Liu Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Real-time_Free-view_Human_Rendering_from_Sparse-view_RGB_Videos_using_Double_CVPR_2025_paper.html": {
    "title": "Real-time Free-view Human Rendering from Sparse-view RGB Videos using Double Unprojected Textures",
    "volume": "main",
    "abstract": "Real-time free-view human rendering from sparse-view RGB inputs is a challenging task due to the sensor scarcity and the tight time budget. To ensure efficiency, recent methods leverage 2D CNNs operating in texture space to learn rendering primitives. However, they either jointly learn geometry and appearance, or completely ignore sparse image information for geometry estimation, significantly harming visual quality and robustness to unseen body poses. To address these issues, we present Double Unprojected Textures, which at the core disentangles coarse geometric deformation estimation from appearance synthesis, enabling robust and photorealistic 4K rendering in real-time. Specifically, we first introduce a novel image-conditioned template deformation network, which estimates the coarse deformation of the human template from a first unprojected texture. This updated geometry is then used to apply a second and more accurate texture unprojection. The resulting texture map has fewer artifacts and better alignment with input views, which benefits our learning of finer-level geometry and appearance represented by Gaussian splats. We validate the effectiveness and efficiency of the proposed method in quantitative and qualitative experiments, which significantly surpasses other state-of-the-art methods",
    "checked": true,
    "id": "75f5a30f249d3268abebab81cf85089370141936",
    "semantic_title": "real-time free-view human rendering from sparse-view rgb videos using double unprojected textures",
    "citation_count": 4,
    "authors": [
      "Guoxing Sun",
      "Rishabh Dabral",
      "Heming Zhu",
      "Pascal Fua",
      "Christian Theobalt",
      "Marc Habermann"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dang_ECBench_Can_Multi-modal_Foundation_Models_Understand_the_Egocentric_World_A_CVPR_2025_paper.html": {
    "title": "ECBench: Can Multi-modal Foundation Models Understand the Egocentric World? A Holistic Embodied Cognition Benchmark",
    "volume": "main",
    "abstract": "The enhancement of generalization in robots by large vision-language models (LVLMs) is increasingly evident. Therefore, the embodied cognitive abilities of LVLMs based on egocentric videos are of great interest. However, current datasets for embodied video question answering lack comprehensive and systematic evaluation frameworks. Critical embodied cognitive issues, such as robotic self-cognition, dynamic scene perception, and hallucination, are rarely addressed. To tackle these challenges, we propose ECBench, a high-quality benchmark designed to systematically evaluate the embodied cognitive abilities of LVLMs. ECBench features a diverse range of scene video sources, open and varied question formats, and 30 dimensions of embodied cognition. To ensure quality, balance, and high visual dependence, ECBench uses class-independent meticulous human annotation and multi-round question screening strategies. Additionally, we introduce ECEval, a comprehensive evaluation system that ensures the fairness and rationality of the indicators. Utilizing ECBench, we conduct extensive evaluations of proprietary, open-source, and task-specific LVLMs. ECBench is pivotal in advancing the embodied cognitive capabilities of LVLMs, laying a solid foundation for developing reliable core models for embodied agents. All data and code is available at https://github.com/Rh-Dang/ECBench",
    "checked": true,
    "id": "b40de8665ccbfa12614958ecd25e7cc523e655bd",
    "semantic_title": "ecbench: can multi-modal foundation models understand the egocentric world? a holistic embodied cognition benchmark",
    "citation_count": 7,
    "authors": [
      "Ronghao Dang",
      "Yuqian Yuan",
      "Wenqi Zhang",
      "Yifei Xin",
      "Boqiang Zhang",
      "Long Li",
      "Liuyi Wang",
      "Qinyang Zeng",
      "Xin Li",
      "Lidong Bing"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ji_SfM-Free_3D_Gaussian_Splatting_via_Hierarchical_Training_CVPR_2025_paper.html": {
    "title": "SfM-Free 3D Gaussian Splatting via Hierarchical Training",
    "volume": "main",
    "abstract": "Standard 3D Gaussian Splatting (3DGS) relies on known or pre-computed camera poses and a sparse point cloud, obtained from structure-from-motion (SfM) preprocessing, to initialize and grow 3D Gaussians. We propose a novel SfM-Free 3DGS (SFGS) method for video input, eliminating the need for known camera poses and SfM preprocessing. Our approach introduces a hierarchical training strategy that trains and merges multiple 3D Gaussian representations -- each optimized for specific scene regions -- into a single, unified 3DGS model representing the entire scene. To compensate for large camera motions, we leverage video frame interpolation models. Additionally, we incorporate multi-source supervision to reduce overfitting and enhance representation. Experimental results reveal that our approach significantly surpasses state-of-the-art SfM-free novel view synthesis methods. On the Tanks and Temples dataset, we improve PSNR by an average of 2.25dB, with a maximum gain of 3.72dB in the best scene. On the CO3D-V2 dataset, we achieve an average PSNR boost of 1.74dB, with a top gain of 3.90dB. The code is available at \\href https://github.com/jibo27/3DGS_Hierarchical_Training/ https://github.com/jibo27/3DGS_Hierarchical_Training",
    "checked": true,
    "id": "1c900a3b5d8270581297719699c5ab7c738045da",
    "semantic_title": "sfm-free 3d gaussian splatting via hierarchical training",
    "citation_count": 3,
    "authors": [
      "Bo Ji",
      "Angela Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lavoie_Large_Self-Supervised_Models_Bridge_the_Gap_in_Domain_Adaptive_Object_CVPR_2025_paper.html": {
    "title": "Large Self-Supervised Models Bridge the Gap in Domain Adaptive Object Detection",
    "volume": "main",
    "abstract": "The current state-of-the-art methods in domain adaptive object detection (DAOD) use Mean Teacher self-labelling, where a teacher model, directly derived as an exponential moving average of the student model, is used to generate labels on the target domain which are then used to improve both models in a positive loop. This couples learning and generating labels on the target domain, and other recent works also leverage the generated labels to add additional domain alignment losses. We believe this coupling is brittle and excessively constrained: there is no guarantee that a student trained only on source data can generate accurate target domain labels and initiate the positive feedback loop, and much better target domain labels can likely be generated by using a large pretrained network that has been exposed to much more data. Vision foundational models are exactly such models, and they have shown impressive task generalization capabilities even when frozen. We want to leverage these models for DAOD and introduce DINO Teacher, which consists of two components. First, we train a new labeller on source data only using a large frozen DINOv2 backbone and show it generates more accurate labels than Mean Teacher. Next, we align the student's source and target image patch features with those from a DINO encoder, driving source and target representations closer to the generalizable DINO representation. We obtain state-of-the-art performance on multiple DAOD datasets",
    "checked": true,
    "id": "35898bf63248b3cecccc18719479c01b18f7025b",
    "semantic_title": "large self-supervised models bridge the gap in domain adaptive object detection",
    "citation_count": 3,
    "authors": [
      "Marc-Antoine Lavoie",
      "Anas Mahmoud",
      "Steven L. Waslander"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Dynamic_Updates_for_Language_Adaptation_in_Visual-Language_Tracking_CVPR_2025_paper.html": {
    "title": "Dynamic Updates for Language Adaptation in Visual-Language Tracking",
    "volume": "main",
    "abstract": "The consistency between the semantic information provided by the multi-modal reference and the tracked object is crucial for visual-language (VL) tracking. However, existing VL tracking frameworks rely on static multi-modal references to locate dynamic objects, which can lead to semantic discrepancies and reduce the robustness of the tracker. To address this issue, we propose a novel vision-language tracking framework, named DUTrack, which captures the latest state of the target by dynamically updating multi-modal references to maintain consistency.Specifically, we introduce a Dynamic Language Update Module, which leverages a large language model to generate dynamic language descriptions for the object based on visual features and object category information. Then, we design a Dynamic Template Capture Module, which captures the regions in the image that highly match the dynamic language descriptions. Furthermore, to ensure the efficiency of description generation, we design an update strategy that assesses changes in target displacement, scale, and other factors to decide on updates. Finally, the dynamic template and language descriptions that record the latest state of the target are used to update the multi-modal references, providing more accurate reference information for subsequent inference and enhancing the robustness of the tracker.DUTrack achieves new state-of-the-art performance on four mainstream vision-language and two vision-only tracking benchmarks, including LaSOT, LaSOT_ext, TNL2K, OTB99-Lang, GOT-10K, and UAV123",
    "checked": true,
    "id": "9d277913f8da30722d0654852678acd39a261c89",
    "semantic_title": "dynamic updates for language adaptation in visual-language tracking",
    "citation_count": 3,
    "authors": [
      "Xiaohai Li",
      "Bineng Zhong",
      "Qihua Liang",
      "Zhiyi Mo",
      "Jian Nong",
      "Shuxiang Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Multi-focal_Conditioned_Latent_Diffusion_for_Person_Image_Synthesis_CVPR_2025_paper.html": {
    "title": "Multi-focal Conditioned Latent Diffusion for Person Image Synthesis",
    "volume": "main",
    "abstract": "The Latent Diffusion Model (LDM) has demonstrated strong capabilities in high-resolution image generation and has been widely employed for Pose-Guided Person Image Synthesis (PGPIS), yielding promising results. However, the compression process of LDM often results in the deterioration of details, particularly in sensitive areas such as facial features and clothing textures. In this paper, we propose a Multi-focal Conditioned Latent Diffusion (MCLD) method to address these limitations by conditioning the model on disentangled, pose-invariant features from these sensitive regions. Our approach utilizes a multi-focal condition aggregation module, which effectively integrates facial identity and texture-specific information, enhancing the model's ability to produce appearance realistic and identity-consistent images. Our method demonstrates consistent identity and appearance generation on the DeepFashion dataset and enables flexible person image editing due to its generation consistency. The code is available at https://github.com/jqliu09/mcld",
    "checked": true,
    "id": "37f17f46b1d01ed688bc0b630a9098c505cec705",
    "semantic_title": "multi-focal conditioned latent diffusion for person image synthesis",
    "citation_count": 0,
    "authors": [
      "Jiaqi Liu",
      "Jichao Zhang",
      "Paolo Rota",
      "Nicu Sebe"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Uncertainty_Meets_Diversity_A_Comprehensive_Active_Learning_Framework_for_Indoor_CVPR_2025_paper.html": {
    "title": "Uncertainty Meets Diversity: A Comprehensive Active Learning Framework for Indoor 3D Object Detection",
    "volume": "main",
    "abstract": "Active learning has emerged as a promising approach to reduce the substantial annotation burden in 3D object detection tasks, spurring several initiatives in outdoor environments. However, its application in indoor environments remains unexplored. Compared to outdoor 3D datasets, indoor datasets face significant challenges, including fewer training samples per class, a greater number of classes, more severe class imbalance, and more diverse scene types and intra-class variances.This paper presents the first study on active learning for indoor 3D object detection, where we propose a novel framework tailored for this task. Our method incorporates two key criteria - uncertainty and diversity - to actively select the most ambiguous and informative unlabeled samples for annotation. The uncertainty criterion accounts for both inaccurate detections and undetected objects, ensuring that the most ambiguous samples are prioritized. Meanwhile, the diversity criterion is formulated as a joint optimization problem that maximizes the diversity of both object class distributions and scene types, using a new Class-aware Adaptive Prototype (CAP) bank. The CAP bank dynamically allocates representative prototypes to each class, helping to capture varying intra-class diversity across different categories.We evaluate our method on SUN RGB-D and ScanNetV2, where it outperforms baselines by a significant margin, achieving over 85% of fully-supervised performance with just 10% of the annotation budget",
    "checked": true,
    "id": "6662b19e5ff2ec3a3f3038b72960c172e709005a",
    "semantic_title": "uncertainty meets diversity: a comprehensive active learning framework for indoor 3d object detection",
    "citation_count": 0,
    "authors": [
      "Jiangyi Wang",
      "Na Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_CASAGPT_Cuboid_Arrangement_and_Scene_Assembly_for_Interior_Design_CVPR_2025_paper.html": {
    "title": "CASAGPT: Cuboid Arrangement and Scene Assembly for Interior Design",
    "volume": "main",
    "abstract": "We present a novel approach for indoor scene synthesis, which learns to arrange decomposed cuboid primitives to represent 3D objects within a scene. Unlike conventional methods that use bounding boxes to determine the placement and scale of 3D objects, our approach leverages cuboids as a straightforward yet highly effective alternative for modeling objects. This allows for compact scene generation while minimizing object intersections. Our approach, coined CASAGPT for Cuboid Arrangement and Scene Assembly, employs an autoregressive model to sequentially arrange cuboids, producing physically plausible scenes. By applying rejection sampling during the fine-tuning stage to filter out scenes with object collisions, our model further reduces intersections and enhances scene quality. Additionally, we introduce a refined dataset, 3DFRONT-NC, which eliminates significant noise presented in the original dataset, 3D-FRONT. Extensive experiments on the 3D-FRONT dataset as well as our dataset demonstrate that our approach consistently outperforms the state-of-the-art methods, enhancing the realism of generated scenes, and providing a promising direction for 3D scene synthesis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weitao Feng",
      "Hang Zhou",
      "Jing Liao",
      "Li Cheng",
      "Wenbo Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pang_Identity-Clothing_Similarity_Modeling_for_Unsupervised_Clothing_Change_Person_Re-Identification_CVPR_2025_paper.html": {
    "title": "Identity-Clothing Similarity Modeling for Unsupervised Clothing Change Person Re-Identification",
    "volume": "main",
    "abstract": "Clothing change person re-identification (CC-ReID) aims to match different images of the same person, even when the clothing varies across images. To reduce manual labeling costs, existing unsupervised CC-ReID methods employ clustering algorithms to generate pseudo-labels. However, they often fail to assign the same pseudo-label to two images with the same identity but different clothing--referred to as a clothing change positive pair--thus hindering clothing-invariant feature learning. To address this issue, we propose the identity-clothing similarity modeling (ICSM) framework. To effectively connect clothing change positive pairs, ICSM first performs clothing-aware learning to leverage all discriminative information, including clothing, to obtain compact clusters. It then extracts cluster-level identity and clothing features and performs inter-cluster similarity estimation to identify clothing change positive clusters, reliable negative clusters, and hard negative clusters for each compact cluster. During optimization, we design an adaptive version of existing optimization methods to enhance similarities of clothing change positive pairs, while also introducing text semantics as a supervisory signal to further promote clothing invariance. Extensive experimental results across multiple datasets validate the effectiveness of the proposed framework, demonstrating its superiority over existing unsupervised methods and its competitiveness with some supervised approaches",
    "checked": true,
    "id": "a88fe34736d378ccedb38cbeef929dd414469c7c",
    "semantic_title": "identity-clothing similarity modeling for unsupervised clothing change person re-identification",
    "citation_count": 0,
    "authors": [
      "Zhiqi Pang",
      "Junjie Wang",
      "Lingling Zhao",
      "Chunyu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mao_Evaluating_Model_Perception_of_Color_Illusions_in_Photorealistic_Scenes_CVPR_2025_paper.html": {
    "title": "Evaluating Model Perception of Color Illusions in Photorealistic Scenes",
    "volume": "main",
    "abstract": "We study the perception of color illusions by vision-language models. Color illusion, where a person's visual system perceives color differently from actual color, is well-studied in human vision. However, it remains underexplored whether vision-language models (VLMs), trained on large-scale human data, exhibit similar perceptual biases when confronted with such color illusions. We propose an automated framework for generating color illusion images, resulting in RCID (Realistic Color Illusion Dataset), a dataset of 19,000 realistic illusion images. Our experiments show that all studied VLMs exhibit perceptual biases similar human vision. Finally, we train a model to distinguish both human perception and actual pixel differences",
    "checked": true,
    "id": "84af7f9c6f8ef69237076788e2bf56221cb90c60",
    "semantic_title": "evaluating model perception of color illusions in photorealistic scenes",
    "citation_count": 1,
    "authors": [
      "Lingjun Mao",
      "Zineng Tang",
      "Alane Suhr"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_MINIMA_Modality_Invariant_Image_Matching_CVPR_2025_paper.html": {
    "title": "MINIMA: Modality Invariant Image Matching",
    "volume": "main",
    "abstract": "Image matching for both cross-view and cross-modality plays a critical role in multimodal perception. In practice, the modality gap caused by different imaging systems/styles poses great challenges to the matching task. Existing works try to extract invariant features for specific modalities and train on limited datasets, showing poor generalization. In this paper, we present MINIMA, a unified image matching framework for multiple cross-modal cases. Without pursuing fancy modules, our MINIMA aims to enhance universal performance from the perspective of data scaling up. For such purpose, we propose a simple yet effective data engine that can freely produce a large dataset containing multiple modalities, rich scenarios, and accurate matching labels. Specifically, we scale up the modalities from cheap but rich RGB-only matching data, by means of generative models. Under this setting, the matching labels and rich diversity of the RGB dataset are well inherited by the generated multimodal data. Benefiting from this, we construct MD-syn, a new comprehensive dataset that fills the data gap for general multimodal image matching. With MD-syn, we can directly train any advanced matching pipeline on randomly selected modality pairs to obtain cross-modal ability. Extensive experiments on in-domain and zero-shot matching tasks, including 19 cross-modal cases, demonstrate that our MINIMA can significantly outperform the baselines and even surpass modality-specific methods. The dataset and code are available at https://github.com/LSXI7/MINIMA",
    "checked": true,
    "id": "d9857eeb03a1f1eaa2ee98f95e5f090cd5350cbb",
    "semantic_title": "minima: modality invariant image matching",
    "citation_count": 0,
    "authors": [
      "Jiangwei Ren",
      "Xingyu Jiang",
      "Zizhuo Li",
      "Dingkang Liang",
      "Xin Zhou",
      "Xiang Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_OccMamba_Semantic_Occupancy_Prediction_with_State_Space_Models_CVPR_2025_paper.html": {
    "title": "OccMamba: Semantic Occupancy Prediction with State Space Models",
    "volume": "main",
    "abstract": "Training deep learning models for semantic occupancy prediction is challenging due to factors such as a large number of occupancy cells, severe occlusion, limited visual cues, complicated driving scenarios, etc. Recent methods often adopt transformer-based architectures given their strong capability in learning input-conditioned weights and long-range relationships. However, transformer-based networks are notorious for their quadratic computation complexity, seriously undermining their efficacy and deployment in semantic occupancy prediction. Inspired by the global modeling and linear computation complexity of the Mamba architecture, we present the first Mamba-based network for semantic occupancy prediction, termed OccMamba. Specifically, we first design the hierarchical Mamba module and local context processor to better aggregate global and local contextual information, respectively. Besides, to relieve the inherent domain gap between the linguistic and 3D domains, we present a simple yet effective 3D-to-1D reordering scheme, i.e., height-prioritized 2D Hilbert expansion. It can maximally retain the spatial structure of 3D voxels as well as facilitate the processing of Mamba blocks. Endowed with the aforementioned designs, our OccMamba is capable of directly and efficiently processing large volumes of dense scene grids, achieving state-of-the-art performance across three prevalent occupancy prediction benchmarks, including OpenOccupancy, SemanticKITTI, and SemanticPOSS. Notably, on OpenOccupancy, our OccMamba outperforms the previous state-of-the-art Co-Occ by 5.1% IoU and 4.3% mIoU, respectively. Our implementation is open-sourced and available at: https://github.com/USTCLH/OccMamba",
    "checked": true,
    "id": "6d4cdaa8c9a4bf45febb8943cfb78f8199a827a6",
    "semantic_title": "occmamba: semantic occupancy prediction with state space models",
    "citation_count": 12,
    "authors": [
      "Heng Li",
      "Yuenan Hou",
      "Xiaohan Xing",
      "Yuexin Ma",
      "Xiao Sun",
      "Yanyong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Held_3D_Convex_Splatting_Radiance_Field_Rendering_with_3D_Smooth_Convexes_CVPR_2025_paper.html": {
    "title": "3D Convex Splatting: Radiance Field Rendering with 3D Smooth Convexes",
    "volume": "main",
    "abstract": "Recent advances in radiance field reconstruction, such as 3D Gaussian Splatting (3DGS), have achieved high-quality novel view synthesis and fast rendering by representing scenes with compositions of Gaussian primitives. However, 3D Gaussians present several limitations for scene reconstruction. Accurately capturing hard edges is challenging without significantly increasing the number of Gaussians, creating a large memory footprint. Moreover, they struggle to represent flat surfaces, as they are diffused in space. Without hand-crafted regularizers, they tend to disperse irregularly around the actual surface. To circumvent these issues, we introduce a novel method, named 3D Convex Splatting (3DCS), which leverages 3D smooth convexes as primitives for modeling geometrically-meaningful radiance fields from multi-view images. Smooth convex shapes offer greater flexibility than Gaussians, allowing for a better representation of 3D scenes with hard edges and dense volumes using fewer primitives. Powered by our efficient CUDA-based rasterizer, 3DCS achieves superior performance over 3DGS on benchmarks such as Mip-NeRF360, Tanks and Temples, and Deep Blending. Specifically, our method attains an improvement of up to 0.81 in PSNR and 0.026 in LPIPS compared to 3DGS while maintaining high rendering speeds and reducing the number of required primitives. Our results highlight the potential of 3D Convex Splatting to become the new standard for high-quality scene reconstruction and novel view synthesis. The project page is https://convexsplatting.github.io/",
    "checked": true,
    "id": "d1166b0fc9595eed0189a16f39403870c3639da7",
    "semantic_title": "3d convex splatting: radiance field rendering with 3d smooth convexes",
    "citation_count": 17,
    "authors": [
      "Jan Held",
      "Renaud Vandeghen",
      "Abdullah Hamdi",
      "Adrien Deliege",
      "Anthony Cioppa",
      "Silvio Giancola",
      "Andrea Vedaldi",
      "Bernard Ghanem",
      "Marc Van Droogenbroeck"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_3D_Prior_Is_All_You_Need_Cross-Task_Few-shot_2D_Gaze_CVPR_2025_paper.html": {
    "title": "3D Prior Is All You Need: Cross-Task Few-shot 2D Gaze Estimation",
    "volume": "main",
    "abstract": "3D and 2D gaze estimation share the fundamental objective of capturing eye movements but are traditionally treated as two distinct research domains. In this paper, we introduce a novel cross-task few-shot 2D gaze estimation approach, aiming to adapt a pre-trained 3D gaze estimation network for 2D gaze prediction on unseen devices using only a few training images. This task is highly challenging due to the domain gap between 3D and 2D gaze, unknown screen poses, and limited training data. To address these challenges, we propose a novel framework that bridges the gap between 3D and 2D gaze. Our framework contains a physics-based differentiable projection module with learnable parameters to model screen poses and project 3D gaze into 2D gaze. The framework is fully differentiable and can integrate into existing 3D gaze networks without modifying their original architecture. Additionally, we introduce a dynamic pseudo-labelling strategy for flipped images, which is particularly challenging for 2D labels due to unknown screen poses. To overcome this, we reverse the projection process by converting 2D labels to 3D space, where flipping is performed. Notably, this 3D space is not aligned with the camera coordinate system, so we learn a dynamic transformation matrix to compensate for this misalignment. We evaluate our method on MPIIGaze, EVE, and GazeCapture datasets, collected respectively on laptops, desktop computers, and mobile devices. The superior performance highlights the effectiveness of our approach, and demonstrates its strong potential for real-world applications",
    "checked": true,
    "id": "451d070aacaa137305e5a987f4e632e7ffe1069d",
    "semantic_title": "3d prior is all you need: cross-task few-shot 2d gaze estimation",
    "citation_count": 1,
    "authors": [
      "Yihua Cheng",
      "Hengfei Wang",
      "Zhongqun Zhang",
      "Yang Yue",
      "Boeun Kim",
      "Feng Lu",
      "Hyung Jin Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Cheb-GR_Rethinking_K-nearest_Neighbor_Search_in_Re-ranking_for_Person_Re-identification_CVPR_2025_paper.html": {
    "title": "Cheb-GR: Rethinking K-nearest Neighbor Search in Re-ranking for Person Re-identification",
    "volume": "main",
    "abstract": "Person re-identification (ReID) is the task of matching individuals across different camera views. Existing approaches typically employ neural networks to extract discriminative features, ranking gallery images based on their similarities to probe images. While effective, these methods are often enhanced through re-ranking, a post-processing step that refines initial retrieval results without requiring additional model training. However, current re-ranking methods mostly rely on k-nearest neighbor search to extract similar images that might have the same identity as the query, which is time-consuming with a high computation burden, limiting their applications in reality. We rethink the effect of the k-nearest neighbor search and introduce the Chebyshev's Theorem-guided Graph Re-ranking (Cheb-GR) method, which adopts the adaptive neighbor search guided by Chebyshev's Theorem over the k-nearest neighbor search for efficient neighbor selection. Our method leverages graph convolution operations to refine image features and achieve robust re-ranking, leading to enhanced retrieval performance. Furthermore, we provide a theoretical analysis based on Chebyshev's Inequality to elucidate the factors contributing to the strong performance of the proposed method. Our method significantly reduces the computation costs while maintaining relatively strong performance. Through extensive experiments in both general and cross-domain settings, we demonstrate the effectiveness of Cheb-GR and its potential for real-world applications",
    "checked": true,
    "id": "f4005153b2840afb1c97e481e0b6c174e8b8f382",
    "semantic_title": "cheb-gr: rethinking k-nearest neighbor search in re-ranking for person re-identification",
    "citation_count": 0,
    "authors": [
      "Jinxi Yang",
      "He Li",
      "Bo Du",
      "Mang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nekrasov_Spotting_the_Unexpected_STU_A_3D_LiDAR_Dataset_for_Anomaly_CVPR_2025_paper.html": {
    "title": "Spotting the Unexpected (STU): A 3D LiDAR Dataset for Anomaly Segmentation in Autonomous Driving",
    "volume": "main",
    "abstract": "To operate safely, autonomous vehicles (AVs) need to detect and handle unexpected objects or anomalies on the road. While significant research exists for anomaly detection and segmentation in 2D, research progress in 3D is underexplored. Existing datasets lack high-quality multimodal data that are typically found in AVs. This paper presents a novel dataset for anomaly segmentation in driving scenarios. To the best of our knowledge, it is the first publicly available dataset focused on road anomaly segmentation with dense 3D semantic labeling, incorporating both LiDAR and camera data, as well as sequential information to enable anomaly detection across various ranges. This capability is critical for the safe navigation of autonomous vehicles. We adapted and evaluated several baseline models for 3D segmentation, highlighting the challenges of 3D anomaly detection in driving environments. Our dataset and evaluation code will be openly available, facilitating the testing and performance comparison of different approaches",
    "checked": true,
    "id": "3494759d7807c2227c5014a10e0e47950df5b97d",
    "semantic_title": "spotting the unexpected (stu): a 3d lidar dataset for anomaly segmentation in autonomous driving",
    "citation_count": 2,
    "authors": [
      "Alexey Nekrasov",
      "Malcolm Burdorf",
      "Stewart Worrall",
      "Bastian Leibe",
      "Julie Stephany Berrio Perez"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jung_Is_Right_Right_Enhancing_Object_Orientation_Understanding_in_Multimodal_Large_CVPR_2025_paper.html": {
    "title": "Is `Right' Right? Enhancing Object Orientation Understanding in Multimodal Large Language Models through Egocentric Instruction Tuning",
    "volume": "main",
    "abstract": "Multimodal large language models (MLLMs) act as essential interfaces, connecting humans with AI technologies in multimodal applications. However, current MLLMs face challenges in accurately interpreting object orientation in images due to inconsistent orientation annotations in training data, hindering the development of a coherent orientation understanding. To overcome this, we propose egocentric instruction tuning, which aligns MLLMs' orientation understanding with the user's perspective, based on a consistent annotation standard derived from the user's egocentric viewpoint. We first generate egocentric instruction data that leverages MLLMs' ability to recognize object details and applies prior knowledge for orientation understanding. Using this data, we perform instruction tuning to enhance the model's capability for accurate orientation interpretation. In addition, we introduce EgoOrientBench, a benchmark that evaluates MLLMs' orientation understanding across three tasks using images collected from diverse domains. Experimental results on this benchmark show that egocentric instruction tuning significantly improves orientation understanding without compromising overall MLLM performance. The instruction data and benchmark dataset are available on our project page at https://github.com/jhCOR/EgoOrientBench",
    "checked": false,
    "id": "b3017d44ad81e879a66c2551d0a7e30a53212e35",
    "semantic_title": "is ‚Äòright' right? enhancing object orientation understanding in multimodal large language models through egocentric instruction tuning",
    "citation_count": 2,
    "authors": [
      "Ji Hyeok Jung",
      "Eun Tae Kim",
      "Seoyeon Kim",
      "Joo Ho Lee",
      "Bumsoo Kim",
      "Buru Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chang_GCC_Generative_Color_Constancy_via_Diffusing_a_Color_Checker_CVPR_2025_paper.html": {
    "title": "GCC: Generative Color Constancy via Diffusing a Color Checker",
    "volume": "main",
    "abstract": "Color constancy methods often struggle to generalize across different camera sensors due to varying spectral sensitivities. We present GCC, which leverages diffusion models to inpaint color checkers into images for illumination estimation. Our key innovations include (1) a single-step deterministic inference approach that inpaints color checkers reflecting scene illumination, (2) a Laplacian decomposition technique that preserves checker structure while allowing illumination-dependent color adaptation, and (3) a mask-based data augmentation strategy for handling imprecise color checker annotations. By harnessing rich priors from pre-trained diffusion models, GCC demonstrates strong robustness in challenging cross-camera scenarios. These results highlight our method's effective generalization capability across different camera characteristics without requiring sensor-specific training, making it a versatile and practical solution for real-world applications",
    "checked": true,
    "id": "2ec2f137f7362f2b24abed9d90e17e9591573c39",
    "semantic_title": "gcc: generative color constancy via diffusing a color checker",
    "citation_count": 1,
    "authors": [
      "Chen-Wei Chang",
      "Cheng-De Fan",
      "Chia-Che Chang",
      "Yi-Chen Lo",
      "Yu-Chee Tseng",
      "Jiun-Long Huang",
      "Yu-Lun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Perincherry_Do_Visual_Imaginations_Improve_Vision-and-Language_Navigation_Agents_CVPR_2025_paper.html": {
    "title": "Do Visual Imaginations Improve Vision-and-Language Navigation Agents?",
    "volume": "main",
    "abstract": "Vision-and-Language Navigation (VLN) agents are tasked with navigating an unseen environment using natural language instructions. In this work, we study if visual representations of sub-goals implied by the instructions can serve as navigational cues and lead to increased navigation performance. To synthesize these visual representations or \"imaginations\", we leverage a text-to-image diffusion model on landmark references contained in segmented instructions. These imaginations are provided to VLN agents as an added modality to act as landmark cues and an auxiliary loss is added to explicitly encourage relating these with their corresponding referring expressions. Our findings reveal an increase in success rate (SR) of ~1 point and up to ~0.5 points in success scaled by inverse path length (SPL) across agents. These results suggest that the proposed approach reinforces visual understanding compared to relying on language instructions alone",
    "checked": true,
    "id": "efcf6e845929e9d2d880c211242d70c8d0f4bd9c",
    "semantic_title": "do visual imaginations improve vision-and-language navigation agents?",
    "citation_count": 4,
    "authors": [
      "Akhil Perincherry",
      "Jacob Krantz",
      "Stefan Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jin_On_Denoising_Walking_Videos_for_Gait_Recognition_CVPR_2025_paper.html": {
    "title": "On Denoising Walking Videos for Gait Recognition",
    "volume": "main",
    "abstract": "To capture individual gait patterns, excluding identity-irrelevant cues in walking videos, such as clothing texture and color, remains a persistent challenge for vision-based gait recognition. Traditional silhouette and pose-based methods, though theoretically effective at removing such distractions, often fall short of high accuracy due to their sparse and less informative inputs. To address this, emerging end-to-end methods focus on directly denoising RGB videos using global optimization and human-defined priors. Building on this trend, we propose a novel gait denoising method, DenosingGait. Inspired by the philosophy that \"what I cannot create, I do not understand\", we turn to generative diffusion models, uncovering how these models can partially filter out irrelevant factors for improved gait understanding. Based on this generation-driven denoising, we introduce feature matching, a kind of popular geometrical constraint in optical flow and depth estimation, to compact multi-channel float-encoded RGB information into two-channel direction vectors that represent local structural features, where within-frame matching captures spatial details and cross-frame matching conveys temporal dynamics. Experiments on the CCPG, CAISA-B*, and SUSTech1K datasets demonstrate that DenoisingGait achieves a new SoTA performance in most cases for both within-domain and cross-domain evaluations. Code is available at https://github.com/ShiqiYu/OpenGait",
    "checked": true,
    "id": "729531f318e9c6ceded5934de0b73f8973662ee6",
    "semantic_title": "on denoising walking videos for gait recognition",
    "citation_count": 1,
    "authors": [
      "Dongyang Jin",
      "Chao Fan",
      "Jingzhe Ma",
      "Jingkai Zhou",
      "Weihua Chen",
      "Shiqi Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Silva-Rodriguez_Conformal_Prediction_for_Zero-Shot_Models_CVPR_2025_paper.html": {
    "title": "Conformal Prediction for Zero-Shot Models",
    "volume": "main",
    "abstract": "Vision-language models pre-trained at large scale have shown unprecedented adaptability and generalization to downstream tasks. Although its discriminative potential has been widely explored, its reliability and uncertainty are still overlooked. In this work, we investigate the capabilities of CLIP models under the split conformal prediction paradigm, which provides theoretical guarantees to black-box models based on a small, labeled calibration set. In contrast to the main body of literature on conformal predictors in vision classifiers, foundation models exhibit a particular characteristic: they are pre-trained on a one-time basis on an inaccessible source domain, different from the transferred task. This domain drift negatively affects the efficiency of the conformal sets and poses additional challenges. To alleviate this issue, we propose Conf-OT, a transfer learning setting that operates transductive over the combined calibration and query sets. Solving an optimal transport problem, the proposed method bridges the domain gap between pre-training and adaptation without requiring additional data splits but still maintaining coverage guarantees. We comprehensively explore this conformal prediction strategy on a broad span of 15 datasets and three non-conformity scores. Conf-OT provides consistent relative improvements of up to 20% on set efficiency while being 15 times faster than popular transductive approaches",
    "checked": true,
    "id": "d7f137d86d828df52baa8bd92e87ffbd77712bb3",
    "semantic_title": "conformal prediction for zero-shot models",
    "citation_count": 2,
    "authors": [
      "Julio Silva-Rodr√≠guez",
      "Ismail Ben Ayed",
      "Jose Dolz"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_PhysAnimator_Physics-Guided_Generative_Cartoon_Animation_CVPR_2025_paper.html": {
    "title": "PhysAnimator: Physics-Guided Generative Cartoon Animation",
    "volume": "main",
    "abstract": "Creating hand-drawn animation sequences is labor-intensive and demands professional expertise. We introduce PhysAnimator, a novel approach for generating physically plausible meanwhile anime-stylized animation from static anime illustrations. Our method seamlessly integrates physics-based simulations with data-driven generative models to produce dynamic and visually compelling animations. To capture the fluidity and exaggeration characteristic of anime, we perform image-space deformable body simulations on extracted mesh geometries. We enhance artistic control by introducing customizable energy strokes and incorporating rigging point support, enabling the creation of tailored animation effects such as wind interactions. Finally, we extract and warp sketches from the simulation sequence, generating a texture-agnostic representation, and employ a sketch-guided video diffusion model to synthesize high-quality animation frames. The resulting animations exhibit temporal consistency and visual plausibility, demonstrating the effectiveness of our method in creating dynamic anime-style animations",
    "checked": true,
    "id": "2f70d229556e1193b2bab0861edc975621a33f14",
    "semantic_title": "physanimator: physics-guided generative cartoon animation",
    "citation_count": 7,
    "authors": [
      "Tianyi Xie",
      "Yiwei Zhao",
      "Ying Jiang",
      "Chenfanfu Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_SeriesBench_A_Benchmark_for_Narrative-Driven_Drama_Series_Understanding_CVPR_2025_paper.html": {
    "title": "SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding",
    "volume": "main",
    "abstract": "With the rapid development of Multi-modal Large Language Models (MLLMs), an increasing number of benchmarks have been established to evaluate the video understanding capabilities of these models. However, these benchmarks focus on standalone videos and only assess \"visual elements\" like human actions and object states. In reality, contemporary videos often encompass complex and continuous narratives, typically presented as a series. To address this challenge, we propose SeriesBench, a benchmark consisting of 105 carefully curated narrative-driven series, covering 28 specialized tasks that require deep narrative understanding to solve. Specifically, we first select a diverse set of drama series spanning various genres. Then, we introduce a novel long-span narrative annotation method, combined with a full-information transformation approach to convert manual annotations into diverse task formats. To further enhance the model's capacity for detailed analysis of plot structures and character relationships within series, we propose a novel narrative reasoning framework, PC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs still face significant challenges in understanding narrative-driven series, while PC-DCoT enables these MLLMs to achieve performance improvements. Overall, our SeriesBench and PC-DCoT highlight the critical necessity of advancing model capabilities for understanding narrative-driven series, guiding future MLLMs development. SeriesBench is publicly available at https://github.com/zackhxn/SeriesBench-CVPR2025",
    "checked": true,
    "id": "8b2b72fbf5502c725f759786ed4041f653f05324",
    "semantic_title": "seriesbench: a benchmark for narrative-driven drama series understanding",
    "citation_count": 0,
    "authors": [
      "Chenkai Zhang",
      "Yiming Lei",
      "Zeming Liu",
      "Haitao Leng",
      "ShaoGuo Liu",
      "Tingting Gao",
      "Qingjie Liu",
      "Yunhong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Weakly_Supervised_Temporal_Action_Localization_via_Dual-Prior_Collaborative_Learning_Guided_CVPR_2025_paper.html": {
    "title": "Weakly Supervised Temporal Action Localization via Dual-Prior Collaborative Learning Guided by Multimodal Large Language Models",
    "volume": "main",
    "abstract": "Recent breakthroughs in Multimodal Large Language Models (MLLMs) have gained significant recognition within the deep learning community, where the fusion of the Video Foundation Models (VFMs) and Large Language Models(LLMs) has proven instrumental in constructing robust video understanding systems, effectively surmounting constraints associated with predefined visual tasks. These sophisticated MLLMs exhibit remarkable proficiency in comprehending videos, swiftly attaining unprecedented performance levels across diverse benchmarks. However, their operation demands substantial memory and computational resources, underscoring the continued importance of traditional models in video comprehension tasks. In this paper, we introduce a novel learning paradigm termed MLLM4WTAL. This paradigm harnesses the potential of MLLM to offer temporal action key semantics and complete semantic textual cues for conventional Weakly-supervised Temporal Action Localization (WTAL) methods. MLLM4WTAL facilitates the enhancement of WTAL by leveraging MLLM guidance. It achieves this by integrating two distinct modules: Key Semantic Matching (KSM) and Complete Semantic Reconstruction (CSR). These modules work in tandem to effectively address prevalent issues like incomplete and over-complete outcomes common in WTAL methods. Rigorous experiments are conducted to validate the efficacy of our proposed approach in augmenting the performance of various heterogeneous WTAL models",
    "checked": true,
    "id": "1bc54139d2d44065b17333a3a09c31ddb8efe784",
    "semantic_title": "weakly supervised temporal action localization via dual-prior collaborative learning guided by multimodal large language models",
    "citation_count": 3,
    "authors": [
      "Quan Zhang",
      "Jinwei Fang",
      "Rui Yuan",
      "Xi Tang",
      "Yuxin Qi",
      "Ke Zhang",
      "Chun Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_FIMA-Q_Post-Training_Quantization_for_Vision_Transformers_by_Fisher_Information_Matrix_CVPR_2025_paper.html": {
    "title": "FIMA-Q: Post-Training Quantization for Vision Transformers by Fisher Information Matrix Approximation",
    "volume": "main",
    "abstract": "Post-training quantization (PTQ) has stood out as a cost-effective and promising model compression approach over recent years, as it eliminates the need for retraining on the entire dataset. Unfortunately, most existing PTQ methods for Vision Transformers (ViTs) exhibit a notable drop in accuracy, especially in low-bit cases. To tackle these challenges, we analyze the extensively utilized Hessian-guided quantization loss, and uncover certain limitations within the approximated pre-activation Hessian. Following the block-by-block reconstruction paradigm of PTQ, we first derive a quantization loss based on the Fisher Information Matrix (FIM). Due to the large scale of the complete FIM, we establish the relationship between KL divergence and FIM in the PTQ scenario to enable fast computation of the quantization loss during reconstruction. Subsequently, we develop a Diagonal Plus Low-Rank (DPLR) estimation on FIM to achieve a more nuanced quantization loss. Our extensive experiments, conducted across various vision tasks with distinct representative ViT-based architectures on public benchmark datasets, demonstrate that our method outperforms the state-of-the-art approaches, especially in the case of low-bit quantization. The source code is available at https://github.com/ShiheWang/FIMA-Q",
    "checked": true,
    "id": "f98c7380a0ee129da91328f60f21686f3c8c0c35",
    "semantic_title": "fima-q: post-training quantization for vision transformers by fisher information matrix approximation",
    "citation_count": 1,
    "authors": [
      "Zhuguanyu Wu",
      "Shihe Wang",
      "Jiayi Zhang",
      "Jiaxin Chen",
      "Yunhong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_HotSpot_Signed_Distance_Function_Optimization_with_an_Asymptotically_Sufficient_Condition_CVPR_2025_paper.html": {
    "title": "HotSpot: Signed Distance Function Optimization with an Asymptotically Sufficient Condition",
    "volume": "main",
    "abstract": "We propose a method, HotSpot, for optimizing neural signed distance functions. Existing losses, such as the eikonal loss, act as necessary but insufficient constraints and cannot guarantee that the recovered implicit function represents a true distance function, even if the output minimizes these losses almost everywhere. Furthermore, the eikonal loss suffers from stability issues in optimization. Finally, in conventional methods, regularization losses that penalize surface area distort the reconstructed signed distance function. We address these challenges by designing a loss function using the solution of a screened Poisson equation. Our loss, when minimized, provides an asymptotically sufficient condition to ensure the output converges to a true distance function. Our loss also leads to stable optimization and naturally penalizes large surface areas. We present theoretical analysis and experiments on both challenging 2D and 3D datasets and show that our method provides better surface reconstruction and a more accurate distance approximation",
    "checked": true,
    "id": "a7bf811d18860311628d471fccbe55deb3ddbda3",
    "semantic_title": "hotspot: signed distance function optimization with an asymptotically sufficient condition",
    "citation_count": 2,
    "authors": [
      "Zimo Wang",
      "Cheng Wang",
      "Taiki Yoshino",
      "Sirui Tao",
      "Ziyang Fu",
      "Tzu-Mao Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_GliaNet_Adaptive_Neural_Network_Structure_Learning_with_Glia-Driven_CVPR_2025_paper.html": {
    "title": "GliaNet: Adaptive Neural Network Structure Learning with Glia-Driven",
    "volume": "main",
    "abstract": "Neural networks derived from the M-P model have excelled in various visual tasks. However, as a simplified simulation version of the brain neural pathway, their structures are locked during training, causing over-fitting and over-parameterization. Although recent models have begun using the biomimetic concept and empirical pruning, they still result in irrational pruning, potentially affecting the accuracy of the model. In this paper, we introduce the Glia unit, composed of oligodendrocytes (Oli) and astrocytes (Ast), to emulate the exact workflow of the mammalian brain, thereby enhancing the biological plausibility of neural functions. Oli selects neurons involved in signal transmission during neural communication and, together with Ast, adaptively optimizes the neural structure. Specifically, we first construct the artificial Glia-Neuron (G-N) model, which is formulated at the instance, group, and interaction levels with adaptive and collaborative mechanisms. Then, we construct GliaNet based on our G-N model, whose structure and connections can be continuously optimized during training. Experiments show that our GliaNet advances state-of-the-art on multiple tasks while significantly reducing its parameters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengqiao Han",
      "Liyuan Pan",
      "Xiabi Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_BACON_Improving_Clarity_of_Image_Captions_via_Bag-of-Concept_Graphs_CVPR_2025_paper.html": {
    "title": "BACON: Improving Clarity of Image Captions via Bag-of-Concept Graphs",
    "volume": "main",
    "abstract": "Advancements in large Vision-Language Models have brought precise, accurate image captioning, vital for advancing multi-modal image understanding and processing. Yet these captions often carry lengthy, intertwined contexts that are difficult to parse and frequently overlook essential cues, posing a great barrier for models like GroundingDINO and SDXL, which lack the strong text encoding and syntax analysis needed to fully leverage dense captions.To address this, we propose BACON, a prompting method that breaks down VLM-generated captions into disentangled, structured elements such as objects, relationships, styles, and themes. This approach not only minimizes confusion from handling complex contexts but also allows for efficient transfer into a JSON dictionary, enabling models without linguistic processing capabilities to easily access key information.We annotated 100,000 image-caption pairs using BACON with GPT-4V and trained an LLaVA captioner on this dataset, enabling it to produce BACON-style captions without relying on costly GPT-4V resources. Evaluations of overall quality, precision, and recall--as well as user studies--demonstrate that the resulting caption model consistently outperforms other state-of-the-art VLM models in generating high-quality captions.Additionally, we show that BACON-style captions exhibit better clarity when applied to various models, enabling them to accomplish previously unattainable tasks or surpass existing SOTA solutions without training. For example, BACON-style captions help groundingDINO achieve 1.51 times higher recall scores on open-vocabulary object detection tasks compared to leading methods",
    "checked": true,
    "id": "8cc04354e5b308e30206103ba3ea5f8c2b59c0b0",
    "semantic_title": "bacon: improving clarity of image captions via bag-of-concept graphs",
    "citation_count": 1,
    "authors": [
      "Zhantao Yang",
      "Ruili Feng",
      "Keyu Yan",
      "Huangji Wang",
      "Zhicai Wang",
      "Shangwen Zhu",
      "Han Zhang",
      "Jie Xiao",
      "Pingyu Wu",
      "Kai Zhu",
      "Jixuan Chen",
      "Chen-Wei Xie",
      "Yue Yang",
      "Hongyang Zhang",
      "Yu Liu",
      "Fan Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_EntitySAM_Segment_Everything_in_Video_CVPR_2025_paper.html": {
    "title": "EntitySAM: Segment Everything in Video",
    "volume": "main",
    "abstract": "Automatically tracking and segmenting every video entity remains a significant challenge. Despite rapid advancements in video segmentation, even state-of-the-art models like SAM 2 struggle to consistently track all entities across a video--a task we refer to as Video Entity Segmentation.We propose EntitySAM, a framework for zero-shot video entity segmentation. EntitySAM extends SAM 2 by removing the need for explicit prompts, allowing automatic discovery and tracking of all entities, including those appearing in later frames. We incorporate query-based entity discovery and association into SAM 2, inspired by transformer-based object detectors. Specifically, we introduce an entity decoder to facilitate inter-object communication and an automatic prompt generator using learnable object queries. Additionally, we add a semantic encoder to enhance SAM 2's semantic awareness, improving segmentation quality. Trained on image-level mask annotations without category information from the COCO dataset, EntitySAM demonstrates strong generalization on four zero-shot video segmentation tasks: Video Entity, Panoptic, Instance, and Semantic Segmentation. Results on six popular benchmarks show that EntitySAM outperforms previous unified video segmentation methods and strong baselines, setting new standards for zero-shot video segmentation",
    "checked": true,
    "id": "f56311b354fa32955ebaa1d12e0f346e0d16705d",
    "semantic_title": "entitysam: segment everything in video",
    "citation_count": 1,
    "authors": [
      "Mingqiao Ye",
      "Seoung Wug Oh",
      "Lei Ke",
      "Joon-Young Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tong_GS-2DGS_Geometrically_Supervised_2DGS_for_Reflective_Object_Reconstruction_CVPR_2025_paper.html": {
    "title": "GS-2DGS: Geometrically Supervised 2DGS for Reflective Object Reconstruction",
    "volume": "main",
    "abstract": "3D modeling of highly reflective objects remains challenging due to strong view-dependent appearances. While previous SDF-based methods can recover high-quality meshes, they are often time-consuming and tend to produce over-smoothed surfaces. In contrast, 3D Gaussian Splatting (3DGS) offers the advantage of high speed and detailed real-time rendering, but extracting surfaces from the Gaussians can be noisy due to the lack of geometric constraints. To bridge the gap between these approaches, we propose a novel reconstruction method called GS-2DGS for reflective objects based on 2D Gaussian Splatting (2DGS). Our approach combines the rapid rendering capabilities of Gaussian Splatting with additional geometric information from a foundation model. Experimental results on synthetic and real datasets demonstrate that our method significantly outperforms Gaussian-based techniques in terms of reconstruction and relighting and achieves performance comparable to SDF-based methods while being an order of magnitude faster",
    "checked": true,
    "id": "9ef5f905c3a2d6d829397884b09161b566a4ac09",
    "semantic_title": "gs-2dgs: geometrically supervised 2dgs for reflective object reconstruction",
    "citation_count": 4,
    "authors": [
      "Jinguang Tong",
      "Xuesong Li",
      "Fahira Afzal Maken",
      "Sundaram Muthu",
      "Lars Petersson",
      "Chuong Nguyen",
      "Hongdong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Libra-Merging_Importance-redundancy_and_Pruning-merging_Trade-off_for_Acceleration_Plug-in_in_Large_CVPR_2025_paper.html": {
    "title": "Libra-Merging: Importance-redundancy and Pruning-merging Trade-off for Acceleration Plug-in in Large Vision-Language Model",
    "volume": "main",
    "abstract": "Large Vision-Language Models (LVLMs) have achieved significant progress in recent years. However, the expensive inference cost limits the realistic deployment of LVLMs. Some works find that visual tokens are redundant and compress tokens to reduce the inference cost. These works identify important non-redundant tokens as target tokens, then prune the remaining tokens (non-target tokens) or merge them into target tokens. However, target token identification faces the token importance-redundancy dilemma. Besides, token merging and pruning face a dilemma between disrupting target token information and losing non-target token information. To solve these problems, we propose a novel visual token compression scheme, named Libra-Merging. In target token identification, Libra-Merging selects the most important tokens from spatially discrete intervals, achieving a more robust token importance-redundancy trade-off than relying on a hyper-parameter. In token compression, when non-target tokens are dissimilar to target tokens, Libra-Merging does not merge them into the target tokens, thus avoiding disrupting target token information. Meanwhile, Libra-Merging condenses these non-target tokens into an information compensation token to prevent losing important non-target token information. Our method can serve as a plug-in for diverse LVLMs, and extensive experimental results demonstrate its effectiveness. The code will be publicly available at https://github.com/longrongyang/Libra-Merging",
    "checked": true,
    "id": "7813f950adfb0e59ebdfea22d9a53cbf3ca4c172",
    "semantic_title": "libra-merging: importance-redundancy and pruning-merging trade-off for acceleration plug-in in large vision-language model",
    "citation_count": 0,
    "authors": [
      "Longrong Yang",
      "Dong Shen",
      "Chaoxiang Cai",
      "Kaibing Chen",
      "Fan Yang",
      "Tingting Gao",
      "Di Zhang",
      "Xi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VasTSD_Learning_3D_Vascular_Tree-state_Space_Diffusion_Model_for_Angiography_CVPR_2025_paper.html": {
    "title": "VasTSD: Learning 3D Vascular Tree-state Space Diffusion Model for Angiography Synthesis",
    "volume": "main",
    "abstract": "Angiography imaging is a medical imaging technique that enhances the visibility of blood vessels within the body by using contrast agents. Angiographic images can effectively assist in the diagnosis of vascular diseases. However, contrast agents may bring extra radiation exposure which is harmful to patients with health risks. To mitigate these concerns, in this paper, we aim to automatically generate angiography from non-angiographic inputs, by leveraging and enhancing the inherent physical properties of vascular structures. Previous methods relying on 2D slice-based angiography synthesis struggle with maintaining continuity in 3D vascular structures and exhibit limited effectiveness across different imaging modalities. We propose VasTSD, a 3D vascular tree-state space diffusion model to synthesize angiography from 3D non-angiographic volumes, with a novel state space serialization approach that dynamically constructs vascular tree topologies, integrating these with a diffusion-based generative model to ensure the generation of anatomically continuous vasculature in 3D volumes. A pre-trained vision embedder is employed to construct vascular state space representations, enabling consistent modeling of vascular structures across multiple modalities. Extensive experiments on various angiographic datasets demonstrate the superiority of VasTSD over prior works, achieving enhanced continuity of blood vessels in synthesized angiographic synthesis in multiple modalities and anatomical regions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhifeng Wang",
      "Renjiao Yi",
      "Xin Wen",
      "Chenyang Zhu",
      "Kai Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_PanSplat_4K_Panorama_Synthesis_with_Feed-Forward_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting",
    "volume": "main",
    "abstract": "With the advent of portable 360deg cameras, panorama has gained significant attention in applications like virtual reality (VR), virtual tours, robotics, and autonomous driving. As a result, wide-baseline panorama view synthesis has emerged as a vital task, where high resolution, fast inference, and memory efficiency are essential. Nevertheless, existing methods typically focus on lower resolutions (512 x1024) due to demanding memory and computational requirements. In this paper, we present PanSplat, a generalizable, feed-forward approach that efficiently supports resolution up to 4K (2048 x4096). Our approach features a tailored spherical 3D Gaussian pyramid with a Fibonacci lattice arrangement, enhancing image quality while reducing information redundancy. To accommodate the demands of high resolution, we propose a pipeline that integrates a hierarchical spherical cost volume and localized Gaussian heads, enabling two-step deferred backpropagation for memory-efficient training on a single A100 GPU. Experiments demonstrate that PanSplat achieves state-of-the-art results with superior efficiency and image quality across both synthetic and real-world datasets",
    "checked": true,
    "id": "afc9767531539c3929df396e1339772a9f6b6aac",
    "semantic_title": "pansplat: 4k panorama synthesis with feed-forward gaussian splatting",
    "citation_count": 5,
    "authors": [
      "Cheng Zhang",
      "Haofei Xu",
      "Qianyi Wu",
      "Camilo Cruz Gambardella",
      "Dinh Phung",
      "Jianfei Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_WISNet_Pseudo_Label_Generation_on_Unbalanced_and_Patch_Annotated_Waste_CVPR_2025_paper.html": {
    "title": "WISNet: Pseudo Label Generation on Unbalanced and Patch Annotated Waste Images",
    "volume": "main",
    "abstract": "Computer-vision-based assessment on waste sorting is desired to replace manpower supervision in Shanghai city. Due to the hardness of labeling a multitude of waste images, it is infeasible to train a semantic segmentation model for this purpose directly. In this work, we construct a new dataset consisting of 12,208 waste images, upon which seed regions (i.e., patches) are annotated and classified into 21 categories in a crowdsourcing fashion. To obtain pixel-level labels to train an effective segmentation model, we propose a weakly-supervised waste image pseudo label generation scheme, called WISNet. Specifically, we train a cohesive feature extractor with contrastive prototype learning, incorporating an unsupervised classification pretext task to help the extractor focus on more discriminative regions even with the same category. Furthermore, we propose an effective iterative patch expansion method to generate accurate pixel-level pseudo labels. Given these generated pseudo labels, a few-shot segmentation model can be trained to segment waste images. We implement and deploy WISNet in two real-world scenarios and conduct intensive experiments. Results show that WISNet can achieve a state-of-the-art 40.2% final segmentation mIoU on our waste benchmark, outperforming all other baselines and demonstrating the efficacy of WISNet",
    "checked": true,
    "id": "bb24e7169a187a29cda29efb9714ebc63df417b9",
    "semantic_title": "wisnet: pseudo label generation on unbalanced and patch annotated waste images",
    "citation_count": 0,
    "authors": [
      "Shifan Zhang",
      "Hongzi Zhu",
      "Yinan He",
      "Minyi Guo",
      "Ziyang Lou",
      "Shan Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ruiz-Ponce_MixerMDM_Learnable_Composition_of_Human_Motion_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "MixerMDM: Learnable Composition of Human Motion Diffusion Models",
    "volume": "main",
    "abstract": "Generating human motion guided by conditions such as textual descriptions is challenging due to the need for datasets with pairs of high-quality motion and their corresponding conditions. The difficulty increases when aiming for finer control in the generation. To that end, prior works have proposed to combine several motion diffusion models pre-trained on datasets with different types of conditions, thus allowing control with multiple conditions. However, the proposed merging strategies overlook that the optimal way to combine the generation processes might depend on the particularities of each pre-trained generative model and also the specific textual descriptions. In this context, we introduce MixerMDM, the first learnable model composition technique for combining pre-trained text-conditioned human motion diffusion models. Unlike previous approaches, MixerMDM provides a dynamic mixing strategy that is trained in an adversarial fashion to learn to combine the denoising process of each model depending on the set of conditions driving the generation. By using MixerMDM to combine single- and multi-person motion diffusion models, we achieve fine-grained control on the dynamics of every person individually, and also on the overall interaction. Furthermore, we propose a new evaluation technique that, for the first time in this task, measures the interaction and individual quality by computing the alignment between the mixed generated motions and their conditions as well as the capabilities of MixerMDM to adapt the mixing throughout the denoising process depending on the motions to mix",
    "checked": true,
    "id": "3b8dc2583d66701b62b77fc7081c910551e03a7a",
    "semantic_title": "mixermdm: learnable composition of human motion diffusion models",
    "citation_count": 1,
    "authors": [
      "Pablo Ruiz-Ponce",
      "German Barquero",
      "Cristina Palmero",
      "Sergio Escalera",
      "Jos√© Garc√≠a-Rodr√≠guez"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Hand-held_Object_Reconstruction_from_RGB_Video_with_Dynamic_Interaction_CVPR_2025_paper.html": {
    "title": "Hand-held Object Reconstruction from RGB Video with Dynamic Interaction",
    "volume": "main",
    "abstract": "This work aims to reconstruct the 3D geometry of a rigid object manipulated by one or both hands using monocular RGB video. Previous methods rely on Structure-from-Motion or hand priors to estimate relative motion between the object and camera, which typically assume textured objects or single-hand interactions. To accurately recover object geometry in dynamic interactions, we incorporate priors from 3D generation model into object pose estimation and propose semantic consistency constraints to solve the challenge of shape and texture discrepancy between the generated priors and observations. The poses are initialized, followed by joint optimization of the object poses and implicit neural representation. During optimization, a novel pose outlier voting strategy with inter-view consistency is proposed to correct large pose errors. Experiments on three datasets demonstrate that our method significantly outperforms the state-of-the-art in reconstruction quality for both single- and two-hand scenarios. Our project page: https://east-j.github.io/dynhor/",
    "checked": true,
    "id": "ef69a4bc0ce2f24c500975e3e86f5f3db03ad38e",
    "semantic_title": "hand-held object reconstruction from rgb video with dynamic interaction",
    "citation_count": 1,
    "authors": [
      "Shijian Jiang",
      "Qi Ye",
      "Rengan Xie",
      "Yuchi Huo",
      "Jiming Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_LEDiff_Latent_Exposure_Diffusion_for_HDR_Generation_CVPR_2025_paper.html": {
    "title": "LEDiff: Latent Exposure Diffusion for HDR Generation",
    "volume": "main",
    "abstract": "While consumer displays increasingly support more than 10 stops of dynamic range, most image assets -- such as internet photographs and generative AI content -- remain limited to 8-bit low dynamic range (LDR), constraining their utility across high dynamic range (HDR) applications. Currently, no generative model can produce high-bit, high-dynamic range content in a generalizable way. Existing LDR-to-HDR conversion methods often struggle to produce photorealistic details and physically-plausible dynamic range in the clipped areas. We introduce LEDiff, a method that enables a generative model with HDR content generation through latent space fusion inspired by image-space exposure fusion techniques. It also functions as an LDR-to-HDR converter, expanding the dynamic range of existing low-dynamic range images. Our approach uses a small HDR dataset to enable a pretrained diffusion model to recover detail and dynamic range in clipped highlights and shadows. LEDiff brings HDR capabilities to existing generative models and converts any LDR image to HDR, creating photorealistic HDR outputs for image generation, image-based lighting (HDR environment map generation), and photographic effects such as depth of field simulation, where linear HDR data is essential for realistic quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Wang",
      "Zhihao Xia",
      "Thomas Leimkuhler",
      "Karol Myszkowski",
      "Xuaner Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Video_Depth_Anything_Consistent_Depth_Estimation_for_Super-Long_Videos_CVPR_2025_paper.html": {
    "title": "Video Depth Anything: Consistent Depth Estimation for Super-Long Videos",
    "volume": "main",
    "abstract": "Depth Anything has achieved remarkable success in monocular depth estimation with strong generalization ability. However, it suffers from temporal inconsistency in videos, hindering its practical applications. Various methods have been proposed to alleviate this issue by leveraging video generation models or introducing priors from optical flow and camera poses. Nonetheless, these methods are only applicable to short videos (10 seconds) and require a trade-off between quality and computational efficiency. We propose Video Depth Anything for high-quality, consistent depth estimation in super-long videos (over several minutes) without sacrificing efficiency. We base our model on Depth Anything V2 and replace its head with an efficient spatial-temporal head. We design a straightforward yet effective temporal consistency loss by constraining the temporal depth gradient, eliminating the need for additional geometric priors. The model is trained on a joint dataset of video depth and unlabeled images, similar to Depth Anything V2. Moreover, a novel key-frame-based strategy is developed for long video inference. Experiments show that our model can be applied to arbitrarily long videos without compromising quality, consistency, or generalization ability. Comprehensive evaluations on multiple video benchmarks demonstrate that our approach sets a new state-of-the-art in zero-shot video depth estimation. We offer models of different scales to support a range of scenarios, with our smallest model capable of real-time performance at 30 FPS",
    "checked": true,
    "id": "72838afbbc15d3da945fbb245fa082f330f9d0f8",
    "semantic_title": "video depth anything: consistent depth estimation for super-long videos",
    "citation_count": 40,
    "authors": [
      "Sili Chen",
      "Hengkai Guo",
      "Shengnan Zhu",
      "Feihu Zhang",
      "Zilong Huang",
      "Jiashi Feng",
      "Bingyi Kang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_VideoAutoArena_An_Automated_Arena_for_Evaluating_Large_Multimodal_Models_in_CVPR_2025_paper.html": {
    "title": "VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation",
    "volume": "main",
    "abstract": "Large multimodal models (LMMs) with advanced video analysis capabilities have recently garnered significant attention. However, most evaluations rely on traditional methods like multiple-choice question answering in benchmarks such as VideoMME and LongVideoBench, which are prone to lack the depth needed to capture the complex demands of real-world users. To address this limitation--and due to the prohibitive cost and slow pace of human annotation for video tasks--we introduce VideoAutoArena, an arena-style benchmark inspired by LMSYS Chatbot Arena's framework, designed to automatically assess LMMs' video analysis abilities. VideoAutoArena utilizes user simulation to generate open-ended, adaptive questions that rigorously assess model performance in video understanding. The benchmark features an automated, scalable evaluation framework, incorporating a modified ELO Rating System for fair and continuous comparisons across multiple LMMs. To validate our automated judging system, we construct a \"gold standard\" using a carefully curated subset of human annotations, demonstrating that our arena strongly aligns with human judgment while maintaining scalability. Additionally, we introduce a fault-driven evolution strategy, progressively increasing question complexity to push models toward handling more challenging video analysis scenarios. Experimental results demonstrate that VideoAutoArena effectively differentiates among state-of-the-art LMMs, providing insights into model strengths and areas for improvement. To further streamline our evaluation, we introduce VideoAutoBench as an auxiliary benchmark, where human annotators label winners in a subset of VideoAutoArena battles. We use GPT-4o as a judge to compare responses against these human-validated answers. Together, VideoAutoArena and VideoAutoBench offer a cost-effective, and scalable framework for evaluating LMMs in user-centric video analysis",
    "checked": true,
    "id": "7c1fc0a5cd0d655f8a77139f3dc053998dbf7165",
    "semantic_title": "videoautoarena: an automated arena for evaluating large multimodal models in video analysis through user simulation",
    "citation_count": 6,
    "authors": [
      "Ziyang Luo",
      "Haoning Wu",
      "Dongxu Li",
      "Jing Ma",
      "Mohan Kankanhalli",
      "Junnan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_InstanceCap_Improving_Text-to-Video_Generation_via_Instance-aware_Structured_Caption_CVPR_2025_paper.html": {
    "title": "InstanceCap: Improving Text-to-Video Generation via Instance-aware Structured Caption",
    "volume": "main",
    "abstract": "Text-to-video generation has evolved rapidly in recent years, delivering remarkable results. Training typically relies on video-caption paired data, which plays a crucial role in enhancing generation performance. However, current video captions often suffer from insufficient details, hallucinations and imprecise motion depiction, affecting the fidelity and consistency of generated videos. In this work, we propose a novel instance-aware structured caption framework, termed \\mathtt InstanceCap , to achieve instance-level and fine-grained video caption for the first time. Based on this scheme, we design an auxiliary models cluster to convert original video into instances to enhance instance fidelity. Video instances are further used to refine dense prompts into structured phrases, achieving concise yet precise descriptions. Furthermore, a 22K \\mathtt InstanceVid dataset is curated for training, and an enhancement pipeline that tailored to \\mathtt InstanceCap structure is proposed for inference. Experimental results demonstrate that our proposed \\mathtt InstanceCap significantly outperform previous models, ensuring high fidelity between captions and videos while reducing hallucinations",
    "checked": true,
    "id": "5cb9df5d8e2b50c62135067cca4dd219d4f8874f",
    "semantic_title": "instancecap: improving text-to-video generation via instance-aware structured caption",
    "citation_count": 4,
    "authors": [
      "Tiehan Fan",
      "Kepan Nan",
      "Rui Xie",
      "Penghao Zhou",
      "Zhenheng Yang",
      "Chaoyou Fu",
      "Xiang Li",
      "Jian Yang",
      "Ying Tai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guan_AudCast_Audio-Driven_Human_Video_Generation_by_Cascaded_Diffusion_Transformers_CVPR_2025_paper.html": {
    "title": "AudCast: Audio-Driven Human Video Generation by Cascaded Diffusion Transformers",
    "volume": "main",
    "abstract": "Despite the recent progress of audio-driven video generation, existing methods mostly focus on driving facial movements, leading to non-coherent head and body dynamics. Moving forward, it is desirable yet challenging to generate holistic human videos with both accurate lip-sync and delicate co-speech gestures w.r.t. given audio. In this work, we propose AudCast, a generalized audio-driven human video generation framework adopting a cascade Diffusion-Transformers (DiTs) paradigm, which synthesizes holistic human videos based on a reference image and a given audio. 1) Firstly, an audio-conditioned Holistic Human DiT architecture is proposed to directly drive the movements of any human body with vivid gesture dynamics. 2) Then to enhance hand and face details that are well-knownly difficult to handle, a Regional Refinement DiT leverages regional 3D fitting as the bridge to reform the signals, producing the final results. Extensive experiments demonstrate that our framework generates high-fidelity audio-driven holistic human videos with temporal coherence and fine facial and hand details",
    "checked": true,
    "id": "cd4698de98ab0edc14fea7eb3128a33352a87ed9",
    "semantic_title": "audcast: audio-driven human video generation by cascaded diffusion transformers",
    "citation_count": 1,
    "authors": [
      "Jiazhi Guan",
      "Kaisiyuan Wang",
      "Zhiliang Xu",
      "Quanwei Yang",
      "Yasheng Sun",
      "Shengyi He",
      "Borong Liang",
      "Yukang Cao",
      "Yingying Li",
      "Haocheng Feng",
      "Errui Ding",
      "Jingdong Wang",
      "Youjian Zhao",
      "Hang Zhou",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cui_Luminance-GS_Adapting_3D_Gaussian_Splatting_to_Challenging_Lighting_Conditions_with_CVPR_2025_paper.html": {
    "title": "Luminance-GS: Adapting 3D Gaussian Splatting to Challenging Lighting Conditions with View-Adaptive Curve Adjustment",
    "volume": "main",
    "abstract": "Capturing high-quality photographs under diverse real-world lighting conditions is challenging, as both natural lighting (e.g., low-light) and camera exposure settings (e.g., exposure time) significantly impact image quality. This challenge becomes more pronounced in multi-view scenarios, where variations in lighting and image signal processor (ISP) settings across viewpoints introduce photometric inconsistencies. Such lighting degradations and view-dependent variations pose substantial challenges to novel view synthesis (NVS) frameworks based on Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS). To address this, we introduce Luminance-GS, a novel approach to achieving high-quality novel view synthesis results under diverse challenging lighting conditions using 3DGS. By adopting per-view color matrix mapping and view adaptive curve adjustments, Luminance-GS achieves state-of-the-art (SOTA) results across various lighting conditions--including low-light, overexposure, and varying exposure--while not altering the original 3DGS explicit representation. Compared to previous NeRF- and 3DGS-based baselines, Luminance-GS provides real-time rendering speed with improved reconstruction quality. The source code is available at https://github.com/cuiziteng/Luminance-GS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziteng Cui",
      "Xuangeng Chu",
      "Tatsuya Harada"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yura_EventSplat_3D_Gaussian_Splatting_from_Moving_Event_Cameras_for_Real-time_CVPR_2025_paper.html": {
    "title": "EventSplat: 3D Gaussian Splatting from Moving Event Cameras for Real-time Rendering",
    "volume": "main",
    "abstract": "We introduce a method for using event camera data in novel view synthesis via Gaussian Splatting. Event cameras offer exceptional temporal resolution and a high dynamic range. Leveraging these capabilities allows us to effectively address the novel view synthesis challenge in the presence of fast camera motion. For initialization of the optimization process, our approach uses prior knowledge encoded in an event-to-video model. We also use spline interpolation for obtaining high quality poses along the event camera trajectory. This enhances the reconstruction quality from fast-moving cameras while overcoming the computational limitations traditionally associated with event-based Neural Radiance Field (NeRF) methods. Our experimental evaluation demonstrates that our results achieve higher visual fidelity and better performance than existing event-based NeRF approaches while being an order of magnitude faster to render",
    "checked": true,
    "id": "4ee1986b6da19e04b68d566341d6c9298a688411",
    "semantic_title": "eventsplat: 3d gaussian splatting from moving event cameras for real-time rendering",
    "citation_count": 4,
    "authors": [
      "Toshiya Yura",
      "Ashkan Mirzaei",
      "Igor Gilitschenski"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Thinking_in_Space_How_Multimodal_Large_Language_Models_See_Remember_CVPR_2025_paper.html": {
    "title": "Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces",
    "volume": "main",
    "abstract": "Humans possess the visual-spatial intelligence to remember spaces from sequential visual observations. However, can Multimodal Large Language Models (MLLMs) trained on million-scale video datasets also \"think in space\" from videos? We present a novel video-based visual-spatial intelligence benchmark (VSI-Bench) of over 5,000 question-answer pairs, and find that MLLMs exhibit competitive--though subhuman--visual-spatial intelligence. We probe models to express how they think in space both linguistically and visually and find that while spatial reasoning capabilities remain the primary bottleneck for MLLMs to reach higher benchmark performance, local world models and spatial awareness do emerge within these models. Notably, prevailing linguistic reasoning techniques (e.g., chain-of-thought, self-consistency, tree-of-thoughts) fail to improve performance, whereas explicitly generating cognitive maps during question-answering enhances MLLMs' spatial distance awareness",
    "checked": true,
    "id": "376461a2c049f6fa51a4303853fdc672e4d07a0d",
    "semantic_title": "thinking in space: how multimodal large language models see, remember, and recall spaces",
    "citation_count": 178,
    "authors": [
      "Jihan Yang",
      "Shusheng Yang",
      "Anjali W. Gupta",
      "Rilyn Han",
      "Li Fei-Fei",
      "Saining Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_3D_Student_Splatting_and_Scooping_CVPR_2025_paper.html": {
    "title": "3D Student Splatting and Scooping",
    "volume": "main",
    "abstract": "Recently, 3D Gaussian Splatting (3DGS) provides a new framework for novel view synthesis, and has spiked a new wave of research in neural rendering and related applications. As 3DGS is becoming a foundational component of many models, any improvement on 3DGS itself can bring huge benefits. To this end, we aim to improve the fundamental paradigm and formulation of 3DGS. We argue that as an unnormalized mixture model, it needs to be neither Gaussians nor splatting. We subsequently propose a new mixture model consisting of flexible Student's t distributions, with both positive (splatting) and negative (scooping) densities. We name our model Student Splatting and Scooping, or SSS. When providing better expressivity, SSS also poses new challenges in learning. Therefore, we also propose a new principled sampling approach for optimization. Through exhaustive evaluation and comparison, across multiple datasets, settings, and metrics, we demonstrate that SSS outperforms existing methods in terms of quality and parameter efficiency, e.g. achieving matching or better quality with similar numbers of components, and obtaining comparable results while reducing the component number by as much as 82%",
    "checked": true,
    "id": "1c0985f27952302c6f44ce4083afdfd19d00ee4d",
    "semantic_title": "3d student splatting and scooping",
    "citation_count": 2,
    "authors": [
      "Jialin Zhu",
      "Jiangbei Yue",
      "Feixiang He",
      "He Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_World-consistent_Video_Diffusion_with_Explicit_3D_Modeling_CVPR_2025_paper.html": {
    "title": "World-consistent Video Diffusion with Explicit 3D Modeling",
    "volume": "main",
    "abstract": "Recent advancements in diffusion models have set new benchmarks in image and video generation, enabling realistic visual synthesis across single- and multi-frame contexts. However, these models still struggle with efficiently and explicitly generating 3D-consistent content. To address this, we propose World-consistent Video Diffusion (WVD), a novel framework that incorporates explicit 3D supervision using XYZ images, which encode global 3D coordinates for each image pixel. More specifically, we train a diffusion transformer to learn the joint distribution of RGB and XYZ frames. This approach supports multi-task adaptability via a flexible inpainting strategy. For example, WVD can estimate XYZ frames from ground-truth RGB or generate novel RGB frames using XYZ projections along a specified camera trajectory. In doing so, WVD unifies tasks like single-image-to-3D generation, multi-view stereo, and camera-controlled video generation. Our approach demonstrates competitive performance across multiple benchmarks, providing a scalable solution for 3D-consistent video and image generation with a single pretrained model",
    "checked": true,
    "id": "b1092759de1ba84e621d9bc65b6cd61a3c4a1633",
    "semantic_title": "world-consistent video diffusion with explicit 3d modeling",
    "citation_count": 16,
    "authors": [
      "Qihang Zhang",
      "Shuangfei Zhai",
      "Miguel √Ångel Bautista Martin",
      "Kevin Miao",
      "Alexander Toshev",
      "Joshua Susskind",
      "Jiatao Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_A_Stitch_in_Time_Saves_Nine_Small_VLM_is_a_CVPR_2025_paper.html": {
    "title": "A Stitch in Time Saves Nine: Small VLM is a Precise Guidance for Accelerating Large VLMs",
    "volume": "main",
    "abstract": "Vision-language models (VLMs) have shown remarkable success across various multi-modal tasks, yet large VLMs encounter significant efficiency challenges due to processing numerous visual tokens. A promising approach to accelerating large VLM inference is using partial information, such as attention maps from specific layers, to assess token importance and prune less essential tokens. However, our study reveals three key insights: (i) Partial attention information is insufficient for accurately identifying critical visual tokens, resulting in suboptimal performance, especially at low token retention ratios; (ii) Global attention information, such as the attention map aggregated across all layers, more effectively preserves essential tokens and maintains performance under aggressive pruning. However, it requires a full inference pass, which increases computational load and is therefore impractical in existing methods; and (iii) The global attention map aggregated from a small VLM closely resembles that of a large VLM, suggesting an efficient alternative. Based on these findings, we introduce \\underline S mall VLM \\underline G uidance for \\underline L arge VLMs (SGL). Specifically, we employ the aggregated attention map from a small VLM guide the pruning of visual tokens in a large VLM. Additionally, we develop a small VLM early exiting mechanism to make full use of the small VLM's predictions, dynamically invoking the larger VLM only when necessary, yielding a superior trade-off between accuracy and computational cost. Extensive evaluations across 11 benchmarks demonstrate the effectiveness and generalizability of our method, achieving up to 91% pruning ratio for visual tokens while retaining competitive performance",
    "checked": true,
    "id": "32ab8c59adcf1b5b5fdee2f5ddd8addfcba3bf67",
    "semantic_title": "a stitch in time saves nine: small vlm is a precise guidance for accelerating large vlms",
    "citation_count": 16,
    "authors": [
      "Wangbo Zhao",
      "Yizeng Han",
      "Jiasheng Tang",
      "Zhikai Li",
      "Yibing Song",
      "Kai Wang",
      "Zhangyang Wang",
      "Yang You"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guizilini_Zero-Shot_Novel_View_and_Depth_Synthesis_with_Multi-View_Geometric_Diffusion_CVPR_2025_paper.html": {
    "title": "Zero-Shot Novel View and Depth Synthesis with Multi-View Geometric Diffusion",
    "volume": "main",
    "abstract": "Current methods for 3D scene reconstruction from sparse posed images employ intermediate 3D representations such as neural fields, voxel grids, or 3D Gaussians, to achieve multi-view consistent scene appearance and geometry. In this paper we introduce MVGD, a diffusion-based architecture capable of direct pixel-level generation of images and depth maps from novel viewpoints, given an arbitrary number of input views. Our method uses raymap conditioning to both augment visual features with spatial information from different viewpoints, as well as to guide the generation of images and depth maps from novel views. A key aspect of our approach is the multi-task generation of images and depth maps, using learnable task embeddings to guide the diffusion process towards specific modalities. We train this model on a collection of more than 60 million multi-view samples from publicly available datasets, and propose techniques to enable efficient and consistent learning in such diverse conditions. We also propose a novel strategy that enables the efficient training of larger models by incrementally fine-tuning smaller ones, with promising scaling behavior. Through extensive experiments, we report state-of-the-art results in multiple novel view synthesis benchmarks, as well as multi-view stereo and video depth estimation",
    "checked": true,
    "id": "e72b1cb2eeccb3bde0863aa37ba2d1970a21fb87",
    "semantic_title": "zero-shot novel view and depth synthesis with multi-view geometric diffusion",
    "citation_count": 3,
    "authors": [
      "Vitor Guizilini",
      "Muhammad Zubair Irshad",
      "Dian Chen",
      "Greg Shakhnarovich",
      "Rares Ambrus"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bi_Unveiling_Visual_Perception_in_Language_Models_An_Attention_Head_Analysis_CVPR_2025_paper.html": {
    "title": "Unveiling Visual Perception in Language Models: An Attention Head Analysis Approach",
    "volume": "main",
    "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated remarkable progress in visual understanding. This impressive leap raises a compelling question: how can language models, initially trained solely on linguistic data, effectively interpret and process visual content? This paper aims to address this question with systematic investigation across 4 model families and 4 model scales, uncovering a unique class of attention heads that focus specifically on visual content. Our analysis reveals a strong correlation between the behavior of these attention heads, the distribution of attention weights, and their concentration on visual tokens within the input. These findings enhance our understanding of how LLMs adapt to multimodal tasks, demonstrating their potential to bridge the gap between textual and visual understanding. This work paves the way for the development of AI systems capable of engaging with diverse modalities",
    "checked": true,
    "id": "6e05f2edd1cc348032f9b65bb24205c6bfdb0b6c",
    "semantic_title": "unveiling visual perception in language models: an attention head analysis approach",
    "citation_count": 9,
    "authors": [
      "Jing Bi",
      "Junjia Guo",
      "Yunlong Tang",
      "Lianggong Bruce Wen",
      "Zhang Liu",
      "Bingjie Wang",
      "Chenliang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_SemanticDraw_Towards_Real-Time_Interactive_Content_Creation_from_Image_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "SemanticDraw: Towards Real-Time Interactive Content Creation from Image Diffusion Models",
    "volume": "main",
    "abstract": "We introduce SemanticDraw, a new paradigm of interactive content creation where high-quality images are generated in near real-time from given multiple hand-drawn regions, each encoding prescribed semantic meaning. In order to maximize the productivity of content creators and to fully realize their artistic imagination, it requires both quick interactive interfaces and fine-grained regional controls in their tools. Despite astonishing generation quality from recent diffusion models, we find that existing approaches for regional controllability are very slow (52 seconds for 512 x 512 image) while not compatible with acceleration methods such as LCM, blocking their huge potential in interactive content creation. From this observation, we build our solution for interactive content creation in two steps: (1) we establish compatibility between region-based controls and acceleration techniques for diffusion models, maintaining high fidelity of multi-prompt image generation with x 10 reduced number of inference steps, (2) we increase the generation throughput with our new multi-prompt stream batch pipeline, enabling low-latency generation from multiple, region-based text prompts on a single RTX 2080 Ti GPU. Our proposed framework is generalizable to any existing diffusion models and acceleration schedulers, allowing sub-second (0.64 seconds) image content creation application upon well-established image diffusion models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaerin Lee",
      "Daniel Sungho Jung",
      "Kanggeon Lee",
      "Kyoung Mu Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Basak_SemiDAViL_Semi-supervised_Domain_Adaptation_with_Vision-Language_Guidance_for_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "SemiDAViL: Semi-supervised Domain Adaptation with Vision-Language Guidance for Semantic Segmentation",
    "volume": "main",
    "abstract": "Domain Adaptation (DA) and Semi-supervised Learning (SSL) converge in Semi-supervised Domain Adaptation (SSDA), where the objective is to transfer knowledge from a source domain to a target domain using a combination of limited labeled target samples and abundant unlabeled target data. Although intuitive, a simple amalgamation of DA and SSL is suboptimal in semantic segmentation due to two major reasons: (1) previous methods, while able to learn good segmentation boundaries, are prone to confuse classes with similar visual appearance due to limited supervision; and (2) skewed and imbalanced training data distribution preferring source representation learning whereas impeding from exploring limited information about tailed classes. Language guidance can serve as a pivotal semantic bridge, facilitating robust class discrimination and mitigating visual ambiguities by leveraging the rich semantic relationships encoded in pre-trained language models to enhance feature representations across domains. Therefore, we propose the first language-guided SSDA setting for semantic segmentation in this work. Specifically, we harness the semantic generalization capabilities inherent in vision-language models (VLMs) to establish a synergistic framework within the SSDA paradigm. To address the inherent class-imbalance challenges in long-tailed distributions, we introduce class-balanced segmentation loss formulations that effectively regularize the learning process. Through extensive experimentation across diverse domain adaptation scenarios, our approach demonstrates substantial performance improvements over contemporary state-of-the-art (SoTA) methodologies",
    "checked": true,
    "id": "ae02f9b8ee48a42210323a238af1cdb8bfee8342",
    "semantic_title": "semidavil: semi-supervised domain adaptation with vision-language guidance for semantic segmentation",
    "citation_count": 0,
    "authors": [
      "Hritam Basak",
      "Zhaozheng Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ruan_Learning_Partonomic_3D_Reconstruction_from_Image_Collections_CVPR_2025_paper.html": {
    "title": "Learning Partonomic 3D Reconstruction from Image Collections",
    "volume": "main",
    "abstract": "Reconstructing the 3D shape of an object from a single-view image is a fundamental task in computer vision. Recent advances in differentiable rendering have enabled 3D reconstruction from image collections using only 2D annotations. However, these methods mainly focus on whole-object reconstruction and overlook object partonomy, which is essential for intelligent agents interacting with physical environments. This paper aims at learning partonomic 3D reconstruction from collections of images with only 2D annotations. Our goal is not only to reconstruct the shape of an object from a single-view image but also to decompose the shape into meaningful semantic parts. To handle the expanded solution space and frequent part occlusions in single-view images, we introduce a novel approach that represents, parses, and learns the structural compositionality of 3D objects. This approach comprises: (1) a compact and expressive compositional representation of object geometry, achieved through disentangled modeling of large shape variations, constituent parts, and detailed part deformations as multi-granularity neural fields; (2) a part transformer that recovers precise partonomic geometry and handles occlusions, through effective part-to-pixel grounding and part-to-part relational modeling; and (3) a 2D-supervised learning method that jointly learns the compositional representation and part transformer, by bridging object shape and parts, image synthesis, and differentiable rendering. Extensive experiments on ShapeNetPart, PartNet, and CUB-200-2011 demonstrate the effectiveness of our approach on both overall and partonomic reconstruction. Code, models, and data are avaliable at https://github.com/XiaoqianRuan1/Partonomic_Reconstruction",
    "checked": true,
    "id": "cc5199ac4485f0023a5592bc2b73911e9aaeb138",
    "semantic_title": "learning partonomic 3d reconstruction from image collections",
    "citation_count": 0,
    "authors": [
      "Xiaoqian Ruan",
      "Pei Yu",
      "Dian Jia",
      "Hyeonjeong Park",
      "Peixi Xiong",
      "Wei Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gerogiannis_Arc2Avatar_Generating_Expressive_3D_Avatars_from_a_Single_Image_via_CVPR_2025_paper.html": {
    "title": "Arc2Avatar: Generating Expressive 3D Avatars from a Single Image via ID Guidance",
    "volume": "main",
    "abstract": "Inspired by the effectiveness of 3D Gaussian Splatting (3DGS) in reconstructing detailed 3D scenes within multi-view setups and the emergence of large 2D human foundation models, we introduce Arc2Avatar, the first SDS-based method utilizing a human face foundation model as guidance with just a single image as input. To achieve that, we extend such a model for diverse-view human head generation by fine-tuning on synthetic data and modifying its conditioning. Our avatars maintain a dense correspondence with a human face mesh template, allowing blendshape-based expression generation. This is achieved through a modified 3DGS approach, connectivity regularizers, and a strategic initialization tailored for our task. Additionally, we propose an optional efficient SDS-based correction step to refine the blendshape expressions, enhancing realism and diversity. Experiments demonstrate that Arc2Avatar achieves state-of-the-art realism and identity preservation, effectively addressing color issues by allowing the use of very low guidance, enabled by our strong identity prior and initialization strategy, without compromising detail",
    "checked": true,
    "id": "92c1d9bbdf0de323454c8eea1a773f1be5d26470",
    "semantic_title": "arc2avatar: generating expressive 3d avatars from a single image via id guidance",
    "citation_count": 4,
    "authors": [
      "Dimitrios Gerogiannis",
      "Foivos Paraperas Papantoniou",
      "Rolandos Alexandros Potamias",
      "Alexandros Lattas",
      "Stefanos Zafeiriou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ryu_Seeing_Speech_and_Sound_Distinguishing_and_Locating_Audio_Sources_in_CVPR_2025_paper.html": {
    "title": "Seeing Speech and Sound: Distinguishing and Locating Audio Sources in Visual Scenes",
    "volume": "main",
    "abstract": "We present a unified model capable of simultaneously grounding both spoken language and non-speech sounds within a visual scene, addressing key limitations in current audio-visual grounding models. Existing approaches are typically limited to handling either speech or non-speech sounds independently, or at best, together but sequentially without mixing. This limitation prevents them from capturing the complexity of real-world audio sources that are often mixed. Our approach introduces a \"mix-and-separate\" framework with audio-visual alignment objectives that jointly learn correspondence and disentanglement using mixed audio. Through these objectives, our model learns to produce distinct embeddings for each audio type, enabling effective disentanglement and grounding across mixed audio sources.Additionally, we created a new dataset to evaluate simultaneous grounding of mixed audio sources, demonstrating that our model outperforms prior methods. Our approach also achieves state-of-the-art performance in standard segmentation and cross-modal retrieval tasks, highlighting the benefits of our mix-and-separate approach",
    "checked": true,
    "id": "6ed68196c8fd81f81574fb747c46ad355efa5f71",
    "semantic_title": "seeing speech and sound: distinguishing and locating audio sources in visual scenes",
    "citation_count": 0,
    "authors": [
      "Hyeonggon Ryu",
      "Seongyu Kim",
      "Joon Son Chung",
      "Arda Senocak"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kaneko_Structure_from_Collision_CVPR_2025_paper.html": {
    "title": "Structure from Collision",
    "volume": "main",
    "abstract": "Recent advancements in neural 3D representations, such as neural radiance fields (NeRF) and 3D Gaussian splatting (3DGS), have made accurate estimation of the 3D structure from multiview images possible. However, this capability is limited to estimating the visible external structure, and it is still difficult to identify the invisible internal structure hidden behind the surface. To overcome this limitation, we address a new task called structure from collision (SfC), which aims to estimate the structure (including the invisible internal one) of an object from the appearance changes at collision. To solve this task, we propose a novel model called SfC-NeRF, which optimizes the invisible internal structure of the object through a video sequence under physical, appearance (i.e., visible external structure)-preserving, and keyframe constraints. In particular, to avoid falling into undesirable local optima owing to its ill-posed nature, we propose volume annealing, i.e., searching for the global optima by repeatedly reducing and expanding the volume. Extensive experiments on 115 objects involving diverse structures (i.e., various cavity shapes, locations, and sizes) and various material properties reveal the properties of SfC and demonstrate the effectiveness of the proposed SfC-NeRF",
    "checked": true,
    "id": "e3963c9eabc7465121f416a6e9c590c7933ae0a0",
    "semantic_title": "structure from collision",
    "citation_count": 0,
    "authors": [
      "Takuhiro Kaneko"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_ODA-GAN_Orthogonal_Decoupling_Alignment_GAN_Assisted_by_Weakly-supervised_Learning_for_CVPR_2025_paper.html": {
    "title": "ODA-GAN: Orthogonal Decoupling Alignment GAN Assisted by Weakly-supervised Learning for Virtual Immunohistochemistry Staining",
    "volume": "main",
    "abstract": "Recently, virtual staining has emerged as a promising alternative to revolutionize histological staining by digitally generating stains. However, most existing methods suffer from the curse of staining unreality and unreliability. In this paper, we propose the Orthogonal Decoupling Alignment Generative Adversarial Network (ODA-GAN) for unpaired virtual immunohistochemistry (IHC) staining. Our approach is based on the assumption that an image consists of IHC staining-related features, which influence staining distribution and intensity, and staining-unrelated features, such as tissue morphology. Leveraging a pathology foundation model, we first develop a weakly-supervised segmentation pipeline as an alternative to expert annotations. We introduce an Orthogonal MLP (O-MLP) module to project image features into an orthogonal space, decoupling them into staining-related and unrelated components. Additionally, we propose a Dual-stream PatchNCE (DPNCE) loss to resolve contrastive learning contradictions in the staining-related space, thereby enhancing staining accuracy. To further improve realism, we introduce a Multi-layer Domain Alignment (MDA) module to bridge the domain gap between generated and real IHC images. Evaluations on three benchmark datasets show that our ODA-GAN reaches state-of-the-art (SOTA) performance. Our source code is available at https://github.com/ittong/ODA-GAN",
    "checked": true,
    "id": "c43963d4fa92fbdfe542be5fe5d9138b057f151d",
    "semantic_title": "oda-gan: orthogonal decoupling alignment gan assisted by weakly-supervised learning for virtual immunohistochemistry staining",
    "citation_count": 1,
    "authors": [
      "Tong Wang",
      "Mingkang Wang",
      "Zhongze Wang",
      "Hongkai Wang",
      "Qi Xu",
      "Fengyu Cong",
      "Hongming Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_EVOS_Efficient_Implicit_Neural_Training_via_EVOlutionary_Selector_CVPR_2025_paper.html": {
    "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
    "volume": "main",
    "abstract": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for accelerating Implicit Neural Representation (INR). Unlike conventional INR training that feeds all samples through the neural network in each iteration, our approach restricts training to strategically selected points, reducing computational overhead by eliminating redundant forward passes.Specifically, we treat each sample as an individual in an evolutionary process, where only those fittest ones survive and merit inclusion in training, adaptively evolving with the neural network dynamics. While this is conceptually similar to Evolutionary Algorithms, their distinct objectives (selection for acceleration vs. iterative solution optimization) require a fundamental redefinition of evolutionary mechanisms for our context.In response, we design sparse fitness evaluation, frequency-guided crossover, and augmented unbiased mutation to comprise EVOS. These components respectively guide sample selection with reduced computational cost, enhance performance through frequency-domain balance, and mitigate selection bias from cached evaluation. Extensive experiments demonstrate that our method achieves approximately 48%-66% reduction in training time while ensuring superior convergence without additional cost, establishing state-of-the-art acceleration among recent sampling-based strategies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weixiang Zhang",
      "Shuzhao Xie",
      "Chengwei Ren",
      "Siyi Xie",
      "Chen Tang",
      "Shijia Ge",
      "Mingzi Wang",
      "Zhi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Du_Crab_A_Unified_Audio-Visual_Scene_Understanding_Model_with_Explicit_Cooperation_CVPR_2025_paper.html": {
    "title": "Crab: A Unified Audio-Visual Scene Understanding Model with Explicit Cooperation",
    "volume": "main",
    "abstract": "In recent years, numerous tasks have been proposed to encourage model to develop specified capability in understanding audio-visual scene, primarily categorized into temporal localization, spatial localization, spatio-temporal reasoning, and pixel-level understanding. Instead, human possesses a unified understanding ability for diversified tasks. Therefore, designing an audio-visual model with general capability to unify these tasks is of great value. However, simply joint training for all tasks can lead to interference due to the heterogeneity of audiovisual data and complex relationship among tasks. We argue that this problem can be solved through explicit cooperation among tasks. To achieve this goal, we propose a unified learning method which achieves explicit inter-task cooperation from both the perspectives of data and model thoroughly. Specifically, considering the labels of existing datasets are simple words, we carefully refine these datasets and construct an Audio-Visual Unified Instruction-tuning dataset with Explicit reasoning process (AV-UIE), which clarifies the cooperative relationship among tasks. Subsequently, to facilitate concrete cooperation in learning stage, an interaction-aware LoRA structure with multiple LoRA heads is designed to learn different aspects of audiovisual data interaction. By unifying the explicit cooperation across the data and model aspect, our method not only surpasses existing unified audio-visual model on multiple tasks, but also outperforms most specialized models for certain tasks. Furthermore, we also visualize the process of explicit cooperation and surprisingly find that each LoRA head has certain audio-visual understanding ability. Code and dataset: https://github.com/GeWu-Lab/Crab",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Henghui Du",
      "Guangyao Li",
      "Chang Zhou",
      "Chunjie Zhang",
      "Alan Zhao",
      "Di Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Nullu_Mitigating_Object_Hallucinations_in_Large_Vision-Language_Models_via_HalluSpace_CVPR_2025_paper.html": {
    "title": "Nullu: Mitigating Object Hallucinations in Large Vision-Language Models via HalluSpace Projection",
    "volume": "main",
    "abstract": "Recent studies have shown that large vision-language models (LVLMs) often suffer from the issue of object hallucinations (OH). To mitigate this issue, we introduce an efficient method that edits the model weights based on an unsafe subspace, which we call HalluSpace in this paper. With truthful and hallucinated text prompts accompanying the visual content as inputs, the HalluSpace can be identified by extracting the hallucinated embedding features and removing the truthful representations in LVLMs. By orthogonalizing the model weights, input features will be projected into the Null space of the HalluSpace to reduce OH, based on which we name our method Nullu. We reveal that HalluSpaces generally contain prior information in the large language models (LLMs) applied to build LVLMs, which have been shown as essential causes of OH in previous studies. Therefore, null space projection suppresses the LLMs' priors to filter out the hallucinated features, resulting in contextually accurate outputs. Experiments show that our method can effectively mitigate OH across different LVLM families without extra inference costs and also show strong performance in general LVLM benchmarks. Code is released at https://github.com/Ziwei-Zheng/Nullu",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Le Yang",
      "Ziwei Zheng",
      "Boxu Chen",
      "Zhengyu Zhao",
      "Chenhao Lin",
      "Chao Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_OralXrays-9_Towards_Hospital-Scale_Panoramic_X-ray_Anomaly_Detection_via_Personalized_Multi-Object_CVPR_2025_paper.html": {
    "title": "OralXrays-9: Towards Hospital-Scale Panoramic X-ray Anomaly Detection via Personalized Multi-Object Query-Aware Mining",
    "volume": "main",
    "abstract": "In clinical practice, panoramic dental radiography is a widely employed imaging technique that can provide a detailed and comprehensive view of dental structures and surrounding tissues for identifying various oral anomalies. However, due to the complexity of oral anomalies and the scarcity of available data, existing research still suffers from substantial challenges in automated oral anomaly detection. To this end, this paper presents a new hospital-scale panoramic X-ray benchmark, namely \"OralXrays-9\", which consists of 12,688 panoramic X-ray images with 84,113 meticulously annotated instances across nine common oral anomalies. Correspondingly, we propose a personalized Multi-Object Query-Aware Mining (MOQAM) paradigm, which jointly incorporates the Distribution-IoU Region Proposal Network (DI-RPN) and Class-Balanced Spherical Contrastive Regularization (CB-SCR) mechanisms to address the challenges posed by multi-scale variations and class-imbalanced distributions. To the best of our knowledge, this is the first attempt to develop AI-driven diagnostic systems specifically designed for multi-object oral anomaly detection, utilizing publicly available data resources. Extensive experiments on the newly-published OralXrays-9 dataset and real-world nature scenarios consistently demonstrate the superiority of our MOQAM in revolutionizing oral healthcare practices",
    "checked": true,
    "id": "fbda82b30954d60fe81f62906b542695ac7f8673",
    "semantic_title": "oralxrays-9: towards hospital-scale panoramic x-ray anomaly detection via personalized multi-object query-aware mining",
    "citation_count": 1,
    "authors": [
      "Bingzhi Chen",
      "Sisi Fu",
      "Xiaocheng Fang",
      "Jieyi Cai",
      "Boya Zhang",
      "Minhua Lu",
      "Yishu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_MEET_Towards_Memory-Efficient_Temporal_Sparse_Deep_Neural_Networks_CVPR_2025_paper.html": {
    "title": "MEET: Towards Memory-Efficient Temporal Sparse Deep Neural Networks",
    "volume": "main",
    "abstract": "Deep Neural Networks (DNNs) are accurate but compute-intensive, leading to substantial energy consumption during inference. Exploiting temporal redundancy through \\Delta-\\Sigma convolution in video processing has proven to greatly enhance computation efficiency. However, temporal \\Delta-\\Sigma DNNs typically require substantial memory for storing neuron states to compute inter-frame differences, hindering their on-chip deployment. To mitigate this memory cost, directly compressing the states can disrupt the linearity of temporal \\Delta-\\Sigma convolution, causing accumulated errors in long-term \\Delta-\\Sigma processing. Thus, we propose MEET, an optimization framework for MEmory-Efficient Temporal \\Delta-\\Sigma DNNs. MEET transfers the state compression challenge to a well-established weight compression problem by trading fewer activations for more weights and introduces a co-design of network architecture and suppression method to optimize for mixed spatial-temporal execution. Evaluations on three vision applications demonstrate a reduction of 5.1~13.3 xin total memory compared to the most computation-efficient temporal DNNs, while preserving the computation efficiency and model accuracy in long-term \\Delta-\\Sigma processing. MEET facilitates the deployment of temporal \\Delta-\\Sigma DNNs within on-chip memory of embedded event-driven platforms, empowering low-power edge processing",
    "checked": true,
    "id": "15da18611c9980e53f4ef3eb27cafb0e8d4db779",
    "semantic_title": "meet: towards memory-efficient temporal sparse deep neural networks",
    "citation_count": 0,
    "authors": [
      "Zeqi Zhu",
      "Ibrahim Batuhan Akkaya",
      "Luc Waeijen",
      "Egor Bondarev",
      "Arash Pourtaherian",
      "Orlando Moreira"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hess_SplatAD_Real-Time_Lidar_and_Camera_Rendering_with_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "SplatAD: Real-Time Lidar and Camera Rendering with 3D Gaussian Splatting for Autonomous Driving",
    "volume": "main",
    "abstract": "Ensuring the safety of autonomous robots, such as self-driving vehicles, requires extensive testing across diverse driving scenarios. Simulation is a key ingredient for conducting such testing in a cost-effective and scalable way. Neural rendering methods have gained popularity, as they can build simulation environments from collected logs in a data-driven manner. However, existing neural radiance field (NeRF) methods for sensor-realistic rendering of camera and lidar data suffer from low rendering speeds, limiting their applicability for large-scale testing. While 3D Gaussian Splatting (3DGS) enables real-time rendering, current methods are limited to camera data and are unable to render lidar data essential for autonomous driving. To address these limitations, we propose SplatAD, the first 3DGS-based method for realistic, real-time rendering of dynamic scenes for both camera and lidar data. SplatAD accurately models key sensor-specific phenomena such as rolling shutter effects, lidar intensity, and lidar ray dropouts, using purpose-built algorithms to optimize rendering efficiency. Evaluation across three autonomous driving datasets demonstrates that SplatAD achieves state-of-the-art rendering quality with up to +2 PSNR for NVS and +3 PSNR for reconstruction while increasing rendering speed over NeRF-based methods by an order of magnitude. Code to be released upon publication",
    "checked": true,
    "id": "5a6e213db2f41d54cfcc145bf5c9a9d520045eda",
    "semantic_title": "splatad: real-time lidar and camera rendering with 3d gaussian splatting for autonomous driving",
    "citation_count": 23,
    "authors": [
      "Georg Hess",
      "Carl Lindstr√∂m",
      "Maryam Fatemi",
      "Christoffer Petersson",
      "Lennart Svensson"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Audio-Visual_Instance_Segmentation_CVPR_2025_paper.html": {
    "title": "Audio-Visual Instance Segmentation",
    "volume": "main",
    "abstract": "In this paper, we propose a new multi-modal task, termed audio-visual instance segmentation (AVIS), which aims to simultaneously identify, segment and track individual sounding object instances in audible videos. To facilitate this research, we introduce a high-quality benchmark named AVISeg, containing over 90K instance masks from 26 semantic categories in 926 long videos. Additionally, we propose a strong baseline model for this task. Our model first localizes sound source within each frame, and condenses object-specific contexts into concise tokens. Then it builds long-range audio-visual dependencies between these tokens using window-based attention, and tracks sounding objects among the entire video sequences. Extensive experiments reveal that our method performs best on AVISeg, surpassing the existing methods from related tasks. We further conduct the evaluation on several multi-modal large models. Unfortunately, they exhibits subpar performance on instance-level sound source localization and temporal perception. We expect that AVIS will inspire the community towards a more comprehensive multi-modal understanding. Dataset and code is available at https://github.com/ruohaoguo/avis",
    "checked": true,
    "id": "88b0416955d0ff5bb06366f42577d1de0f84995a",
    "semantic_title": "audio-visual instance segmentation",
    "citation_count": 7,
    "authors": [
      "Ruohao Guo",
      "Xianghua Ying",
      "Yaru Chen",
      "Dantong Niu",
      "Guangyao Li",
      "Liao Qu",
      "Yanyu Qi",
      "Jinxing Zhou",
      "Bowei Xing",
      "Wenzhen Yue",
      "Ji Shi",
      "Qixun Wang",
      "Peiliang Zhang",
      "Buwen Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rao_Probabilistic_Prompt_Distribution_Learning_for_Animal_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "Probabilistic Prompt Distribution Learning for Animal Pose Estimation",
    "volume": "main",
    "abstract": "Multi-species animal pose estimation has emerged as a challenging yet critical task, hindered by substantial visual diversity and uncertainty. This paper challenges the problem by efficient prompt learning for Vision-Language Pretrained (VLP) models, e.g. CLIP, aiming to resolve the cross-species generalization problem. At the core of the solution lies in the prompt designing, probabilistic prompt modeling and cross-modal adaptation, thereby enabling prompts to compensate for cross-modal information and effectively overcome large data variances under unbalanced data distribution. To this end, we propose a novel probabilistic prompting approach to fully explore textual descriptions, which could alleviate the diversity issues caused by long-tail property and increase the adaptability of prompts on unseen category instance. Specifically, we first introduce a set of learnable prompts and propose a diversity loss to maintain distinctiveness among prompts, thus representing diverse image attributes. Diverse textual probabilistic representations are sampled and used as the guidance for the pose estimation. Subsequently, we explore three different cross-modal fusion strategies at spatial level to alleviate the adverse impacts of visual uncertainty. Extensive experiments on multi-species animal pose benchmarks show that our method achieves the state-of-the-art performance under both supervised and zero-shot settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiyong Rao",
      "Brian Nlong Zhao",
      "Yu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_dFLMoE_Decentralized_Federated_Learning_via_Mixture_of_Experts_for_Medical_CVPR_2025_paper.html": {
    "title": "dFLMoE: Decentralized Federated Learning via Mixture of Experts for Medical Data Analysis",
    "volume": "main",
    "abstract": "Federated learning has wide applications in the medical field. It enables knowledge sharing among different healthcare institutes while protecting patients' privacy. However, existing federated learning systems are typically centralized, requiring clients to upload client-specific knowledge to a central server for aggregation. This centralized approach would integrate the knowledge from each client into a centralized server, and the knowledge would be already undermined during the centralized integration before it reaches back to each client. Besides, the centralized approach also creates a dependency on the central server, which may affect training stability if the server malfunctions or connections are unstable. To address these issues, we propose a decentralized federated learning framework named dFLMoE. In our framework, clients directly exchange lightweight head models with each other. After exchanging, each client treats both local and received head models as individual experts, and utilizes a client-specific Mixture of Experts (MoE) approach to make collective decisions. This design not only reduces the knowledge damage with client-specific aggregations but also removes the dependency on the central server to enhance the robustness of the framework. We validate our framework on multiple medical tasks, demonstrating that our method evidently outperforms state-of-the-art approaches under both model homogeneity and heterogeneity settings",
    "checked": true,
    "id": "3ecb8447b2c2bd1c0a890ca2425cae26c2b1df51",
    "semantic_title": "dflmoe: decentralized federated learning via mixture of experts for medical data analysis",
    "citation_count": 0,
    "authors": [
      "Luyuan Xie",
      "Tianyu Luan",
      "Wenyuan Cai",
      "Guochen Yan",
      "Zhaoyu Chen",
      "Nan Xi",
      "Yuejian Fang",
      "Qingni Shen",
      "Zhonghai Wu",
      "Junsong Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_Reconstructing_Humans_with_a_Biomechanically_Accurate_Skeleton_CVPR_2025_paper.html": {
    "title": "Reconstructing Humans with a Biomechanically Accurate Skeleton",
    "volume": "main",
    "abstract": "In this paper, we introduce a method for reconstructing 3D humans from a single image using a biomechanically accurate skeleton model. To achieve this, we train a transformer that takes an image as input and estimates the parameters of the model. Due to the lack of training data for this task, we build a pipeline to produce pseudo ground truth model parameters for single images and implement a training procedure that iteratively refines these pseudo labels. Compared to state-of-the-art methods for 3D human mesh recovery, our model achieves competitive performance on standard benchmarks, while it significantly outperforms them in settings with extreme 3D poses and viewpoints. Additionally, we show that previous reconstruction methods frequently violate joint angle limits, leading to unnatural rotations. In contrast, our approach leverages the biomechanically plausible degrees of freedom making more realistic joint rotation estimates. We validate our approach across multiple human pose estimation benchmarks. We make the code, models and data available at: https://isshikihugh.github.io/HSMR/",
    "checked": true,
    "id": "685636c9ed3b03c059287385fdc8ba528a8b9174",
    "semantic_title": "reconstructing humans with a biomechanically accurate skeleton",
    "citation_count": 3,
    "authors": [
      "Yan Xia",
      "Xiaowei Zhou",
      "Etienne Vouga",
      "Qixing Huang",
      "Georgios Pavlakos"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Man_AdaCM2_On_Understanding_Extremely_Long-Term_Video_with_Adaptive_Cross-Modality_Memory_CVPR_2025_paper.html": {
    "title": "AdaCM^2: On Understanding Extremely Long-Term Video with Adaptive Cross-Modality Memory Reduction",
    "volume": "main",
    "abstract": "The advancements in large language models (LLMs) have propelled the improvement of video understanding tasks by incorporating LLMs with visual models. However, most existing LLM-based models (e.g., VideoLLaMA, VideoChat) are constrained to processing short-duration videos. Recent attempts to understand long-term videos by extracting and compressing visual features into a fixed memory size. Nevertheless, those methods leverage only visual modality to merge video tokens and overlook the correlation between visual and textual queries, leading to difficulties in effectively handling complex question-answering tasks. To address the challenges of long videos and complex prompts, we propose AdaCM^2, which, for the first time, introduces an adaptive cross-modality memory reduction approach to video-text alignment in an auto-regressive manner on video streams. Our extensive experiments on various video understanding tasks, such as video captioning, video question answering, and video classification, demonstrate that AdaCM^2 achieves state-of-the-art performance across multiple datasets while significantly reducing memory usage. Notably, it achieves a 4.5% improvement across multiple tasks in the LVU dataset with a GPU memory consumption reduction of up to 65%",
    "checked": false,
    "id": "725be923f3c87846ddb11af319806153b57a28c5",
    "semantic_title": "adacm2: on understanding extremely long-term video with adaptive cross-modality memory reduction",
    "citation_count": 1,
    "authors": [
      "Yuanbin Man",
      "Ying Huang",
      "Chengming Zhang",
      "Bingzhe Li",
      "Wei Niu",
      "Miao Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/An_Mitigating_Object_Hallucinations_in_Large_Vision-Language_Models_with_Assembly_of_CVPR_2025_paper.html": {
    "title": "Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention",
    "volume": "main",
    "abstract": "Despite great success across various multimodal tasks, Large Vision-Language Models (LVLMs) often encounter object hallucinations with generated textual responses being inconsistent with the actual objects in images. We examine different LVLMs and pinpoint that one root cause of object hallucinations lies with deficient attention on discriminative image features. Specifically, LVLMs often predominantly attend to prompt-irrelevant global features instead of prompt-relevant local features, undermining their visual grounding capacity and leading to object hallucinations. We propose Assembly of Global and Local Attention (AGLA) , a training-free and plug-and-play approach that mitigates hallucinations by assembling global features for response generation and local features for visual discrimination simultaneously. Specifically, we introduce an image-prompt matching scheme that captures prompt-relevant local features from images, leading to an augmented view of the input image where prompt-relevant content is highlighted while irrelevant distractions are suppressed. Hallucinations can thus be mitigated with a calibrated logit distribution that is from generative global features of the original image and discriminative local features of the augmented image. Extensive experiments show the superiority of AGLA in LVLM hallucination mitigation, demonstrating its wide applicability across both discriminative and generative tasks. Our code is available at https://github.com/Lackel/AGLA",
    "checked": true,
    "id": "896d85d6310ffc6e4e3d328b54f127e898841bd4",
    "semantic_title": "mitigating object hallucinations in large vision-language models with assembly of global and local attention",
    "citation_count": 20,
    "authors": [
      "Wenbin An",
      "Feng Tian",
      "Sicong Leng",
      "Jiahao Nie",
      "Haonan Lin",
      "Qianying Wang",
      "Ping Chen",
      "Xiaoqin Zhang",
      "Shijian Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VGGT_Visual_Geometry_Grounded_Transformer_CVPR_2025_paper.html": {
    "title": "VGGT: Visual Geometry Grounded Transformer",
    "volume": "main",
    "abstract": "We present VGGT, a feed-forward neural network that directly infers all key 3D attributes of a scene, including camera parameters, point maps, depth maps, and 3D point tracks, from one, a few, or hundreds of its views. This approach is a step forward in 3D computer vision, where models have typically been constrained to and specialized for single tasks. It is also simple and efficient, reconstructing images in under one second, and still outperforming alternatives that require post-processing with visual geometry optimization techniques. The network achieves state-of-the-art results in multiple 3D tasks, including camera parameter estimation, multi-view depth estimation, dense point cloud reconstruction, and 3D point tracking. We also show that using pretrained VGGT as a feature backbone significantly enhances downstream tasks, such as non-rigid point tracking and feed-forward novel view synthesis. Code and models are publicly available at https://github.com/facebookresearch/vggt",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianyuan Wang",
      "Minghao Chen",
      "Nikita Karaev",
      "Andrea Vedaldi",
      "Christian Rupprecht",
      "David Novotny"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jang_Silent_Branding_Attack_Trigger-free_Data_Poisoning_Attack_on_Text-to-Image_Diffusion_CVPR_2025_paper.html": {
    "title": "Silent Branding Attack: Trigger-free Data Poisoning Attack on Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "Text-to-image diffusion models have achieved remarkable success in generating high-quality contents from text prompts. However, their reliance on publicly available data and the growing trend of data sharing for fine-tuning make these models particularly vulnerable to data poisoning attacks. In this work, we introduce the Silent Branding Attack, a novel data poisoning method that manipulates text-to-image diffusion models to generate images containing specific brand logos or symbols without any text triggers. We find that when certain visual patterns are repeatedly in the training data, the model learns to reproduce them naturally in its outputs, even without prompt mentions. Leveraging this, we develop an automated data poisoning algorithm that unobtrusively injects logos into original images, ensuring they blend naturally and remain undetected. Models trained on this poisoned dataset generate images containing logos without degrading image quality or text alignment. We experimentally validate our silent branding attack across two realistic settings on large-scale high-quality image datasets and style personalization datasets, achieving high success rates even without a specific text trigger. Human evaluation and quantitative metrics including logo detection show that our method can stealthily embed logos",
    "checked": true,
    "id": "68f5a42d1b762c1f08056992ab39169ea7e28507",
    "semantic_title": "silent branding attack: trigger-free data poisoning attack on text-to-image diffusion models",
    "citation_count": 2,
    "authors": [
      "Sangwon Jang",
      "June Suk Choi",
      "Jaehyeong Jo",
      "Kimin Lee",
      "Sung Ju Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_UniSTD_Towards_Unified_Spatio-Temporal_Learning_across_Diverse_Disciplines_CVPR_2025_paper.html": {
    "title": "UniSTD: Towards Unified Spatio-Temporal Learning across Diverse Disciplines",
    "volume": "main",
    "abstract": "Traditional spatiotemporal models generally rely on task-specific architectures, which limit their generalizability and scalability across diverse tasks due to domain-specific design requirements. In this paper, we introduce UniSTD, a unified Transformer-based framework for spatiotemporal modeling, which is inspired by advances in recent foundation models with the two-stage pretraining-then-adaption paradigm. Specifically, our work demonstrates that task-agnostic pretraining on 2D vision and vision-text datasets can build a generalizable model foundation for spatiotemporal learning, followed by specialized joint training on spatiotemporal datasets to enhance task-specific adaptability. To improve the learning capabilities across domains, our framework employs a rank-adaptive mixture-of-expert adaptation by using fractional interpolation to relax the discrete variables so that can be optimized in the continuous space. Additionally, we introduce a temporal module to incorporate temporal dynamics explicitly. We evaluate our approach on a large-scale dataset covering 10 tasks across 4 disciplines, demonstrating that a unified spatiotemporal model can achieve scalable, cross-task learning and support up to 10 tasks simultaneously within one model while reducing training costs in multi-domain applications. Code will be available at https://github.com/1hunters/UniSTD",
    "checked": true,
    "id": "b4ec2d8d8c7878b20863f5afda7567746450ffa5",
    "semantic_title": "unistd: towards unified spatio-temporal learning across diverse disciplines",
    "citation_count": 0,
    "authors": [
      "Chen Tang",
      "Xinzhu Ma",
      "Encheng Su",
      "Xiufeng Song",
      "Xiaohong Liu",
      "Wei-Hong Li",
      "Lei Bai",
      "Wanli Ouyang",
      "Xiangyu Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Visual_Consensus_Prompting_for_Co-Salient_Object_Detection_CVPR_2025_paper.html": {
    "title": "Visual Consensus Prompting for Co-Salient Object Detection",
    "volume": "main",
    "abstract": "Existing co-salient object detection (CoSOD) methods generally employ a three-stage architecture (i.e., encoding, consensus extraction & dispersion, and prediction) along with a typical full fine-tuning paradigm. Although they yield certain benefits, they exhibit two notable limitations: 1) This architecture relies on encoded features to facilitate consensus extraction, but the meticulously extracted consensus does not provide timely guidance to the encoding stage. 2) This paradigm involves globally updating all parameters of the model, which is parameter-inefficient and hinders the effective representation of knowledge within the foundation model for this task. Therefore, in this paper, we propose an interaction-effective and parameter-efficient concise architecture for the CoSOD task, addressing two key limitations. It introduces, for the first time, a parameter-efficient prompt tuning paradigm and seamlessly embeds consensus into the prompts to formulate task-specific Visual Consensus Prompts (VCP). Our VCP aims to induce the frozen foundation model to perform better on CoSOD tasks by formulating task-specific visual consensus prompts with minimized tunable parameters. Concretely, the primary insight of the purposeful Consensus Prompt Generator (CPG) is to enforce limited tunable parameters to focus on co-salient representations and generate consensus prompts. The formulated Consensus Prompt Disperser (CPD) leverages consensus prompts to form task-specific visual consensus prompts, thereby arousing the powerful potential of pre-trained models in addressing CoSOD tasks. Extensive experiments demonstrate that our concise VCP outperforms 13 cutting-edge full fine-tuning models, achieving the new state of the art (with 6.8% improvement in F_m metrics on the most challenging CoCA dataset). Source code has been available at https://github.com/WJ-CV/VCP",
    "checked": true,
    "id": "d0b7a352f43b1c7290f36ac057974e02e961eb87",
    "semantic_title": "visual consensus prompting for co-salient object detection",
    "citation_count": 0,
    "authors": [
      "Jie Wang",
      "Nana Yu",
      "Zihao Zhang",
      "Yahong Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_Mani-GS_Gaussian_Splatting_Manipulation_with_Triangular_Mesh_CVPR_2025_paper.html": {
    "title": "Mani-GS: Gaussian Splatting Manipulation with Triangular Mesh",
    "volume": "main",
    "abstract": "Neural 3D representations, such as Neural Radiation Fields (NeRF), excel at producing photorealistic rendering results but lack the flexibility for manipulation and editing which is crucial for content creation. However, manipulating NeRF is not highly controllable and requires a long training and inference time. With the emergence of 3D Gaussian Splatting (3DGS), extremely high-fidelity novel view synthesis can be achieved using an explicit point-based 3D representation with much faster training and rendering speed. However, there is still a lack of effective means to manipulate 3DGS freely while maintaining rendering quality. In this work, we aim to tackle the challenge of achieving manipulable photo-realistic rendering. We propose to utilize a triangular mesh to manipulate 3DGS directly with self-adaptation. This approach reduces the need to design various algorithms for different types of 3DGS manipulation. By utilizing a triangle shape-aware Gaussian binding and adapting method, we can achieve 3DGS manipulation and preserve high-fidelity rendering. In addition, our method is also effective with inaccurate meshes extracted from 3DGS. Experiments demonstrate our method's effectiveness and superiority over baseline approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangjun Gao",
      "Xiaoyu Li",
      "Yiyu Zhuang",
      "Qi Zhang",
      "Wenbo Hu",
      "Chaopeng Zhang",
      "Yao Yao",
      "Ying Shan",
      "Long Quan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_UniHOPE_A_Unified_Approach_for_Hand-Only_and_Hand-Object_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "UniHOPE: A Unified Approach for Hand-Only and Hand-Object Pose Estimation",
    "volume": "main",
    "abstract": "Estimating the 3D pose of hand and potential hand-held object from monocular images is a longstanding challenge. Yet, existing methods are specialized, focusing on either bare-hand or hand interacting with object. No method can flexibly handle both scenarios and their performance degrades when applied to the other scenario. In this paper, we propose UniHOPE, a unified approach for general 3D hand-object pose estimation, flexibly adapting both scenarios. Technically, we design a grasp-aware feature fusion module to integrate hand-object features with an object switcher to dynamically control the hand-object pose estimation according to grasping status. Further, to uplift the robustness of hand pose estimation regardless of object presence, we generate realistic de-occluded image pairs to train the model to learn object-induced hand occlusions, and formulate multi-level feature enhancement techniques for learning occlusion-invariant features. Extensive experiments on three commonly-used benchmarks demonstrate UniHOPE's SOTA performance in addressing hand-only and hand-object scenarios. Code will be released on https://github.com/JoyboyWang/UniHOPE_Pytorch",
    "checked": true,
    "id": "22cfd0593a5a265f70d09a473de9b0fd1d7f6b4d",
    "semantic_title": "unihope: a unified approach for hand-only and hand-object pose estimation",
    "citation_count": 1,
    "authors": [
      "Yinqiao Wang",
      "Hao Xu",
      "Pheng-Ann Heng",
      "Chi-Wing Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gadot_RL-RC-DoT_A_Block-level_RL_agent_for_Task-Aware_Video_Compression_CVPR_2025_paper.html": {
    "title": "RL-RC-DoT: A Block-level RL agent for Task-Aware Video Compression",
    "volume": "main",
    "abstract": "Video encoders optimize compression for human perception by minimizing reconstruction error under bit-rate constraints. In many modern applications such as autonomous driving, an overwhelming majority of videos serve as input for AI systems performing tasks like object recognition or segmentation, rather than being watched by humans. It is therefore useful to optimize the encoder for a downstream task instead of for perceptual image quality. However, a major challenge is how to combine such downstream optimization with existing standard video encoders, which are highly efficient and popular. Here, we address this challenge by controlling the Quantization Parameters (QPs) at the macro-block level to optimize the downstream task. This granular control allows us to prioritize encoding for task-relevant regions within each frame. We formulate this optimization problem as a Reinforcement Learning (RL) task, where the agent learns to balance long-term implications of choosing QPs on both task performance and bit-rate constraints. Notably, our policy does not require the downstream task as an input during inference, making it suitable for streaming applications and edge devices such as vehicles. We demonstrate significant improvements in two tasks, car detection, and ROI (saliency) encoding. Our approach improves task performance for a given bit rate compared to traditional task agnostic encoding methods, paving the way for more efficient task-aware video compression",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uri Gadot",
      "Assaf Shocher",
      "Shie Mannor",
      "Gal Chechik",
      "Assaf Hallak"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_Quantization_without_Tears_CVPR_2025_paper.html": {
    "title": "Quantization without Tears",
    "volume": "main",
    "abstract": "Deep neural networks, while achieving remarkable success across diverse tasks, demand significant resources, including computation, GPU memory, bandwidth, storage, and energy. Network quantization, as a standard compression and acceleration technique, reduces storage costs and enables potential inference acceleration by discretizing network weights and activations into a finite set of integer values. However, current quantization methods are often complex and sensitive, requiring extensive task-specific hyperparameters, where even a single misconfiguration can impair model performance, limiting generality across different models and tasks. In this paper, we propose Quantization without Tears (QwT), a method that simultaneously achieves quantization speed, accuracy, simplicity, and generality. The key insight of QwT is to incorporate a lightweight additional structure into the quantized network to mitigate information loss during quantization. This structure consists solely of a small set of linear layers, keeping the method simple and efficient. More importantly, it provides a closed-form solution, allowing us to improve accuracy effortlessly under 2 minutes. Extensive experiments across various vision, language, and multimodal tasks demonstrate that QwT is both highly effective and versatile. In fact, our approach offers a robust solution for network quantization that combines simplicity, accuracy, and adaptability, which provides new insights for the design of novel quantization paradigms",
    "checked": true,
    "id": "44cf5908f8a1f5d0e17d79c9a4857b2354e9eb5e",
    "semantic_title": "quantization without tears",
    "citation_count": 9,
    "authors": [
      "Minghao Fu",
      "Hao Yu",
      "Jie Shao",
      "Junjie Zhou",
      "Ke Zhu",
      "Jianxin Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_PHGC_Procedural_Heterogeneous_Graph_Completion_for_Natural_Language_Task_Verification_CVPR_2025_paper.html": {
    "title": "PHGC: Procedural Heterogeneous Graph Completion for Natural Language Task Verification in Egocentric Videos",
    "volume": "main",
    "abstract": "Natural Language-based Egocentric Task Verification (NLETV) aims to equip agents to determine if operation flows of procedural tasks in egocentric videos align with natural language instructions. Describing rules with natural language provides generalizable applications, but also raises cross-modal heterogeneity and hierarchical misalignment challenges. In this paper, we proposed a novel approach termed Procedural Heterogeneous Graph Completion (PHGC), which addresses these challenges with heterogeneous graphs representing the logic in rules and operation flows. Specifically, our PHGC method mainly consists of three key components: (1) Heterogeneous Graph Construction module that defines objective states and operation flows as vertices, with temporal and sequential relations as edges. (2) Cross-Modal Path Finding module that aligns semantic relations between hierarchical video and text elements. (3) Discriminative Entity Representation module excavates hidden entities that integrate general logical relations and discriminative cues to reveal final verification results. Additionally, we further constructed a new dataset called CSV-NL comprised of realistic videos. Extensive experiments on the two benchmark datasets covering both digital and physical scenarios, i.e., EgoTV and CSV-NL, demonstrate that our proposed PHGC establishes state-of-the-art performance across different settings. Our code and dataset are available at https://github.com/XunCHN/PHGC",
    "checked": true,
    "id": "1d54b6323c7d20d61e37eda3c35164737cef82e1",
    "semantic_title": "phgc: procedural heterogeneous graph completion for natural language task verification in egocentric videos",
    "citation_count": 0,
    "authors": [
      "Xun Jiang",
      "Zhiyi Huang",
      "Xing Xu",
      "Jingkuan Song",
      "Fumin Shen",
      "Heng Tao Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_Recognition-Synergistic_Scene_Text_Editing_CVPR_2025_paper.html": {
    "title": "Recognition-Synergistic Scene Text Editing",
    "volume": "main",
    "abstract": "Scene text editing aims to modify text content within scene images while maintaining style consistency. Traditional methods achieve this by explicitly disentangling style and content from the source image and then fusing the style with the target content, while ensuring content consistency using a pre-trained recognition model. Despite notable progress, these methods suffer from complex pipelines, leading to suboptimal performance in complex scenarios. In this work, we introduce Recognition-Synergistic Scene Text Editing (RS-STE), a novel approach that fully exploits the intrinsic synergy of text recognition for editing. Our model seamlessly integrates text recognition with text editing within a unified framework, and leverages the recognition model's ability to implicitly disentangle style and content while ensuring content consistency. Specifically, our approach employs a multi-modal parallel decoder based on transformer architecture, which predicts both text content and stylized images in parallel. Additionally, our cyclic self-supervised fine-tuning strategy enables effective training on unpaired real-world data without ground truth, enhancing style and content consistency through a twice-cyclic generation process. Built on a relatively simple architecture, RS-STE achieves state-of-the-art performance on both synthetic and real-world benchmarks, and further demonstrates the effectiveness of leveraging the generated hard cases to boost the performance of downstream recognition tasks. Code will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyao Fang",
      "Pengyuan Lyu",
      "Jingjing Wu",
      "Chengquan Zhang",
      "Jun Yu",
      "Guangming Lu",
      "Wenjie Pei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qin_Towards_Consistent_Multi-Task_Learning_Unlocking_the_Potential_of_Task-Specific_Parameters_CVPR_2025_paper.html": {
    "title": "Towards Consistent Multi-Task Learning: Unlocking the Potential of Task-Specific Parameters",
    "volume": "main",
    "abstract": "Multi-task learning (MTL) has gained widespread application for its ability to transfer knowledge across tasks, improving resource efficiency and generalization. However, gradient conflicts from different tasks remain a major challenge in MTL. Previous gradient-based and loss-based methods primarily focus on gradient optimization in shared parameters, often overlooking the potential of task-specific parameters. This work points out that task-specific parameters not only capture task-specific information but also influence the gradients propagated to shared parameters, which in turn affects gradient conflicts. Motivated by this insight, we propose ConsMTL, which models MTL as a bi-level optimization problem: in the upper-level optimization, we perform gradient aggregation on shared parameters to find a joint update vector that minimizes gradient conflicts; in the lower-level optimization, we introduce an additional loss for task-specific parameters guiding the k gradients of shared parameters to gradually converge towards the joint update vector. Our design enables the optimization of both shared and task-specific parameters to consistently alleviate gradient conflicts. Extensive experiments show that ConsMTL achieves state-of-the-art performance across various benchmarks with task numbers ranging from 2 to 40, demonstrating its superior performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohan Qin",
      "Xiaoxing Wang",
      "Junchi Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_WildAvatar_Learning_In-the-wild_3D_Avatars_from_the_Web_CVPR_2025_paper.html": {
    "title": "WildAvatar: Learning In-the-wild 3D Avatars from the Web",
    "volume": "main",
    "abstract": "Existing research on avatar creation is typically limited to laboratory datasets, which require high costs against scalability and exhibit insufficient representation of the real world. On the other hand, the web abounds with off-the-shelf real-world human videos, but these videos vary in quality and require accurate annotations for avatar creation. To this end, we propose an automatic annotating pipeline with filtering protocols to curate these humans from the web. Our pipeline surpasses state-of-the-art methods on the EMDB benchmark, and the filtering protocols boost verification metrics on web videos. We then curate WildAvatar, a web-scale in-the-wild human avatar creation dataset extracted from YouTube, with 10,000+ different human subjects and scenes. WildAvatar is at least 10xricher than previous datasets for 3D human avatar creation and closer to the real world. To explore its potential, we demonstrate the quality and generalizability of avatar creation methods on WildAvatar. We will publicly release our code, data source links and annotations to push forward 3D human avatar creation and other related fields for real-world applications",
    "checked": true,
    "id": "39a9a6281335da7228192b8c1d9c1dcaf64da951",
    "semantic_title": "wildavatar: learning in-the-wild 3d avatars from the web",
    "citation_count": 0,
    "authors": [
      "Zihao Huang",
      "Shoukang Hu",
      "Guangcong Wang",
      "Tianqi Liu",
      "Yuhang Zang",
      "Zhiguo Cao",
      "Wei Li",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_BooW-VTON_Boosting_In-the-Wild_Virtual_Try-On_via_Mask-Free_Pseudo_Data_Training_CVPR_2025_paper.html": {
    "title": "BooW-VTON: Boosting In-the-Wild Virtual Try-On via Mask-Free Pseudo Data Training",
    "volume": "main",
    "abstract": "Image-based virtual try-on is an increasingly popular and important task to generate realistic try-on images of the specific person.Recent methods model virtual try-on as image mask-inpaint task, which requires masking the person image and results in significant loss of spatial information. Especially, for in-the-wild try-on scenarios with complex poses and occlusions, mask-based methods often introduce noticeable artifacts. Our research found that a mask-free approach can fully leverage spatial and lighting information from the original person image, enabling high-quality virtual try-on. Consequently, we propose a novel training paradigm for a mask-free try-on diffusion model. We ensure the model's mask-free try-on capability by creating high-quality pseudo-data and further enhance its handling of complex spatial information through effective in-the-wild data augmentation. Besides, a try-on localization loss is designed to concentrate on try-on area while suppressing garment features in non-try-on areas, ensuring precise rendering of garments and preservation of fore/back-ground. In the end, we introduce BooW-VTON, the mask-free virtual try-on diffusion model, which delivers SOTA try-on quality without parsing cost. Extensive qualitative and quantitative experiments have demonstrated superior performance in wild scenarios with such a low-demand input",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanpu Zhang",
      "Dan Song",
      "Pengxin Zhan",
      "Tianyu Chang",
      "Jianhao Zeng",
      "Qingguo Chen",
      "Weihua Luo",
      "An-An Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_Rectified_Diffusion_Guidance_for_Conditional_Generation_CVPR_2025_paper.html": {
    "title": "Rectified Diffusion Guidance for Conditional Generation",
    "volume": "main",
    "abstract": "Classifier-Free Guidance (CFG), which combines the conditional and unconditional score functions with two coefficients summing to one, serves as a practical technique for diffusion model sampling. Theoretically, however, denoising with CFG cannot be expressed as a reciprocal diffusion process, which may consequently leave some hidden risks during use. In this work, we revisit the theory behind CFG and rigorously confirm that the improper configuration of the combination coefficients (*i.e.*, the widely used summing-to-one version) brings about expectation shift of the generative distribution. To rectify this issue, we propose ReCFG with a relaxation on the guidance coefficients such that denoising with ReCFG strictly aligns with the diffusion theory. We further show that our approach enjoys a **closed-form** solution given the guidance strength. That way, the rectified coefficients can be readily pre-computed via traversing the observed data, leaving the sampling speed barely affected. Empirical evidence on real-world data demonstrate the compatibility of our post-hoc design with existing state-of-the-art diffusion models, including both class-conditioned ones (*e.g.*, EDM2 on ImageNet) and text-conditioned ones (*e.g.*, SD3 on CC12M), without any retraining. Code is available at https://github.com/thuxmf/recfg",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengfei Xia",
      "Nan Xue",
      "Yujun Shen",
      "Ran Yi",
      "Tieliang Gong",
      "Yong-Jin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bokhovkin_SceneFactor_Factored_Latent_3D_Diffusion_for_Controllable_3D_Scene_Generation_CVPR_2025_paper.html": {
    "title": "SceneFactor: Factored Latent 3D Diffusion for Controllable 3D Scene Generation",
    "volume": "main",
    "abstract": "We present SceneFactor, a diffusion-based approach for large-scale 3D scene generation that enables controllable generation and effortless editing. SceneFactor enables text-guided 3D scene synthesis through our factored diffusion formulation, leveraging latent semantic and geometric manifolds for generation of arbitrary-sized 3D scenes. While text input enables easy, controllable generation, text guidance remains imprecise for intuitive, localized editing and manipulation of the generated 3D scenes. Our factored semantic diffusion generates a proxy semantic space composed of semantic 3D boxes that enables controllable editing of generated scenes by adding, removing, changing the size of the semantic 3D proxy boxes that guides high-fidelity, consistent 3D geometric editing. Extensive experiments demonstrate that our approach enables high-fidelity 3D scene synthesis with effective controllable editing through our factored diffusion approach",
    "checked": true,
    "id": "e5960d5d7689ee7a00c88b27f5ab97ff7f737b3d",
    "semantic_title": "scenefactor: factored latent 3d diffusion for controllable 3d scene generation",
    "citation_count": 8,
    "authors": [
      "Aleksey Bokhovkin",
      "Quan Meng",
      "Shubham Tulsiani",
      "Angela Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_HiFi-Portrait_Zero-shot_Identity-preserved_Portrait_Generation_with_High-fidelity_Multi-face_Fusion_CVPR_2025_paper.html": {
    "title": "HiFi-Portrait: Zero-shot Identity-preserved Portrait Generation with High-fidelity Multi-face Fusion",
    "volume": "main",
    "abstract": "Recent advancements in diffusion-based technologies have made significant strides, particularly in identity-preserved portrait generation (IPG). However, when using multiple reference images from the same ID, existing methods typically produce lower-fidelity portraits and struggle to customize face attributes precisely. To address these issues, this paper presents HiFi-Portrait, a high-fidelity method for zero-shot portrait generation. Specifically, we first introduce the face refiner and landmark generator to obtain fine-grained multi-face features and 3D-aware face landmarks. The landmarks include the reference ID and the target attributes. Then, we design HiFi-Net to fuse multi-face features and align them with landmarks, which improves ID fidelity and face control. In addition, we devise an automated pipeline to construct an ID-based dataset for training HiFi-Portrait. Extensive experimental results demonstrate that our method surpasses the SOTA approaches in face similarity and controllability. Furthermore, our method is also compatible with previous SDXL-based works",
    "checked": true,
    "id": "770d6b46ab03cd187a10ca90c189fb102b420cba",
    "semantic_title": "hifi-portrait: zero-shot identity-preserved portrait generation with high-fidelity multi-face fusion",
    "citation_count": 0,
    "authors": [
      "Yifang Xu",
      "Benxiang Zhai",
      "Yunzhuo Sun",
      "Ming Li",
      "Yang Li",
      "Sidan Du"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_IAAO_Interactive_Affordance_Learning_for_Articulated_Objects_in_3D_Environments_CVPR_2025_paper.html": {
    "title": "IAAO: Interactive Affordance Learning for Articulated Objects in 3D Environments",
    "volume": "main",
    "abstract": "This work presents IAAO, a novel framework that builds an explicit 3D model for intelligent agents to gain understanding of articulated objects in their environment through interaction. Unlike prior methods that rely on task-specific networks and assumptions about movable parts, our IAAO leverages large foundation models to estimate interactive affordances and part articulations in three stages. We first build hierarchical features and label fields for each object state using 3D Gaussian Splatting (3DGS) by distilling mask features and view-consistent labels from multi-view images. We then perform object- and part-level queries on the 3D Gaussian primitives to identify static and articulated elements, estimating global transformations and local articulation parameters along with affordances. Finally, scenes from different states are merged and refined based on the estimated transformations, enabling robust affordance-based interaction and manipulation of objects. Experimental results demonstrate the effectiveness of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Can Zhang",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jin_FloVD_Optical_Flow_Meets_Video_Diffusion_Model_for_Enhanced_Camera-Controlled_CVPR_2025_paper.html": {
    "title": "FloVD: Optical Flow Meets Video Diffusion Model for Enhanced Camera-Controlled Video Synthesis",
    "volume": "main",
    "abstract": "We present FloVD, a novel video diffusion model for camera-controllable video generation. FloVD leverages optical flow to represent the motions of the camera and moving objects. This approach offers two key benefits. Since optical flow can be directly estimated from videos, our approach allows for the use of arbitrary training videos without ground-truth camera parameters. Moreover, as background optical flow encodes 3D correlation across different viewpoints, our method enables detailed camera control by leveraging the background motion. To synthesize natural object motion while supporting detailed camera control, our framework adopts a two-stage video synthesis pipeline consisting of optical flow generation and flow-conditioned video synthesis. Extensive experiments demonstrate the superiority of our method over previous approaches in terms of accurate camera control and natural object motion synthesis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonjoon Jin",
      "Qi Dai",
      "Chong Luo",
      "Seung-Hwan Baek",
      "Sunghyun Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_RAD_Region-Aware_Diffusion_Models_for_Image_Inpainting_CVPR_2025_paper.html": {
    "title": "RAD: Region-Aware Diffusion Models for Image Inpainting",
    "volume": "main",
    "abstract": "Diffusion models have achieved remarkable success in image generation, with applications broadening across various domains.Inpainting is one such application that can benefit significantly from diffusion models. Existing methods either hijack the reverse process of a pretrained diffusion model or cast the problem into a larger framework, i.e., conditioned generation. However, these approaches often require nested loops in the generation process or additional components for conditioning. In this paper, we present region-aware diffusion models (RAD) for inpainting with a simple yet effective reformulation of the vanilla diffusion models. RAD utilizes a different noise schedule for each pixel, which allows local regions to be generated asynchronously while considering the global image context. A plain reverse process requires no additional components, enabling RAD to achieve inference time up to 100 times faster than the state-of-the-art approaches. Moreover, we employ low-rank adaptation (LoRA) to fine-tune RAD based on other pretrained diffusion models, reducing computational burdens in training as well. Experiments demonstrated that RAD provides state-of-the-art results both qualitatively and quantitatively, on the FFHQ, LSUN Bedroom, and ImageNet datasets",
    "checked": true,
    "id": "f53fbaa2c17ca8077a354f682e4c0039a01f77bf",
    "semantic_title": "rad: region-aware diffusion models for image inpainting",
    "citation_count": 5,
    "authors": [
      "Sora Kim",
      "Sungho Suh",
      "Minsik Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ding_RaSS_Improving_Denoising_Diffusion_Samplers_with_Reinforced_Active_Sampling_Scheduler_CVPR_2025_paper.html": {
    "title": "RaSS: Improving Denoising Diffusion Samplers with Reinforced Active Sampling Scheduler",
    "volume": "main",
    "abstract": "Recent years have witnessed the great success of denoising diffusion samplers in improving the generative capability and sampling efficiency given a pre-trained diffusion model. However, most sampling schedulers in diffusion models lack the sampling dynamics and planning capability for future generation results, leading to suboptimal solutions. To overcome this, we propose the Reinforced Active Sampling Scheduler, termed RaSS, intending to find the optimal sampling trajectory by actively planning and adjusting the sampling steps for each sampling process in time. Concretely, RaSS divides the whole sampling process into five stages and introduces a reinforcement learning (RL) agent to continuously monitor the generated instance and perceive the potential generation results, thereby achieving optimal instance- and state-adaptive sampling steps decision. Meanwhile, a sampling reward is designed to assist the planning capability of the RL agent by balancing the sampling efficiency and generation quality. The RaSS is a plug-and-play module, which is applicable to multiple denoising diffusion samplers of diffusion models. Extensive experiments on different benchmarks have shown that our RaSS can consistently improve the generation quality and efficiency across various tasks, without introducing significant computational overhead",
    "checked": true,
    "id": "a5387845561c3b2636fe87b7682126129c113391",
    "semantic_title": "rass: improving denoising diffusion samplers with reinforced active sampling scheduler",
    "citation_count": 0,
    "authors": [
      "Xin Ding",
      "Lei Yu",
      "Xin Li",
      "Zhijun Tu",
      "Hanting Chen",
      "Jie Hu",
      "Zhibo Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Min_Supervising_Sound_Localization_by_In-the-wild_Egomotion_CVPR_2025_paper.html": {
    "title": "Supervising Sound Localization by In-the-wild Egomotion",
    "volume": "main",
    "abstract": "We present a method for learning binaural sound localization from ego-motion in videos. When the camera moves in a video, the direction of sound sources will change along with it. We train an audio model to predict sound directions that are consistent with visual estimates of camera motion, which we obtain using methods from multi-view geometry. This provides a weak but plentiful form of supervision that we combine with traditional binaural cues. To evaluate this idea, we propose a dataset of real-world audio-visual videos with ego-motion. We show that our model can successfully learn from this real-world data, and that it obtains strong performance on sound localization tasks",
    "checked": true,
    "id": "57bc6c73ed76b5a455f456299b2644b863c1be57",
    "semantic_title": "supervising sound localization by in-the-wild egomotion",
    "citation_count": 0,
    "authors": [
      "Anna Min",
      "Ziyang Chen",
      "Hang Zhao",
      "Andrew Owens"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_AutoLUT_LUT-Based_Image_Super-Resolution_with_Automatic_Sampling_and_Adaptive_Residual_CVPR_2025_paper.html": {
    "title": "AutoLUT: LUT-Based Image Super-Resolution with Automatic Sampling and Adaptive Residual Learning",
    "volume": "main",
    "abstract": "In recent years, the increasing popularity of Hi-DPI screens has driven a rising demand for high-resolution images. However, the limited computational power of edge devices poses a challenge in deploying complex super-resolution neural networks, highlighting the need for efficient methods. While prior works have made significant progress, they have not fully exploited pixel-level information. Moreover, their reliance on fixed sampling patterns limits both accuracy and the ability to capture fine details in low-resolution images. To address these challenges, we introduce two plug-and-play modules designed to capture and leverage pixel information effectively in Look-Up Table (LUT) based super-resolution networks. Our method introduces Automatic Sampling (AutoSample), a flexible LUT sampling approach where sampling weights are dynamically learned during training to adapt to pixel variations and expand the receptive field without added inference cost. We also incorporate Adaptive Residual Learning (AdaRL) to enhance inter-layer connections, enabling detailed information flow and improving the network's ability to reconstruct fine details. Our method achieves significant performance improvements on both MuLUT and SPF-LUT while maintaining similar storage sizes. Specifically, for MuLUT, we achieve a PSNR improvement of approximately +0.20 dB improvement on average across five datasets . For SPF-LUT, with more than a 50% reduction in storage space and about a 2/3 reduction in inference time, our method still maintains performance comparable to the original",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuheng Xu",
      "Shijie Yang",
      "Xin Liu",
      "Jie Liu",
      "Jie Tang",
      "Gangshan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peng_Understanding_Fine-tuning_CLIP_for_Open-vocabulary_Semantic_Segmentation_in_Hyperbolic_Space_CVPR_2025_paper.html": {
    "title": "Understanding Fine-tuning CLIP for Open-vocabulary Semantic Segmentation in Hyperbolic Space",
    "volume": "main",
    "abstract": "CLIP, a foundational vision-language model, has emerged as a powerful tool for open-vocabulary semantic segmentation. While freezing the text encoder preserves its powerful embeddings, recent studies show that fine-tuning both the text and image encoders jointly significantly enhances segmentation performance, especially for classes from open sets. In this work, we explain this phenomenon from the perspective of hierarchical alignment, since during fine-tuning, the hierarchy level of image embeddings shifts from image-level to pixel-level. We achieve this by leveraging hyperbolic space, which naturally encoders hierarchical structures. Our key observation is that, during fine-tuning, the hyperbolic radius of CLIP's text embeddings decreases, facilitating better alignment with the pixel-level hierarchical structure of visual data. Building on this insight, we propose HyperCLIP, a novel fine-tuning strategy that adjusts the hyperbolic radius of the text embeddings through scaling transformations. By doing so, HyperCLIP equips CLIP with segmentation capability while introducing only a small number of learnable parameters. Our experiments demonstrate that HyperCLIP achieves state-of-the-art performance on open-vocabulary semantic segmentation tasks across three benchmarks, while fine-tuning only approximately 4% of the total parameters of CLIP. More importantly, we observe that after adjustment, CLIP's text embeddings exhibit a relatively fixed hyperbolic radius across datasets, suggesting that the segmentation task has a characteristic level in hyperbolic space",
    "checked": true,
    "id": "c348ad6d5e50c2fe5abcc546b9147167abe4dff0",
    "semantic_title": "understanding fine-tuning clip for open-vocabulary semantic segmentation in hyperbolic space",
    "citation_count": 1,
    "authors": [
      "Zelin Peng",
      "Zhengqin Xu",
      "Zhilin Zeng",
      "Changsong Wen",
      "Yu Huang",
      "Menglin Yang",
      "Feilong Tang",
      "Wei Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiong_TexGaussian_Generating_High-quality_PBR_Material_via_Octree-based_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "TexGaussian: Generating High-quality PBR Material via Octree-based 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "Physically Based Rendering (PBR) materials play a crucial role in modern graphics, enabling photorealistic rendering across diverse environment maps. Developing an effective and efficient algorithm that is capable of automatically generating high-quality PBR materials rather than RGB texture for 3D meshes can significantly streamline the 3D content creation. Most existing methods leverage pre-trained 2D diffusion models for multi-view image synthesis, which often leads to severe inconsistency between the generated textures and input 3D meshes. This paper presents TexGaussian, a novel method that uses octant-aligned 3D Gaussian Splatting for rapid PBR material generation. Specifically, we place each 3D Gaussian on the finest leaf node of the octree built from the input 3D mesh to render the multi-view images not only for the albedo map but also for roughness and metallic. Moreover, our model is trained in a regression manner instead of diffusion denoising, capable of generating the PBR material for a 3D mesh in a single feed-forward process. Extensive experiments on publicly available benchmarks demonstrate that our method synthesizes more visually pleasing PBR materials and runs faster than previous methods in both unconditional and text-conditional scenarios, exhibiting better consistency with the given geometry. Our code and trained models are available at https://3d-aigc.github.io/TexGaussian",
    "checked": true,
    "id": "225f14fdb8c85c8ff9d202ab864580df913a0fc5",
    "semantic_title": "texgaussian: generating high-quality pbr material via octree-based 3d gaussian splatting",
    "citation_count": 8,
    "authors": [
      "Bojun Xiong",
      "Jialun Liu",
      "Jiakui Hu",
      "Chenming Wu",
      "Jinbo Wu",
      "Xing Liu",
      "Chen Zhao",
      "Errui Ding",
      "Zhouhui Lian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mao_OSV_One_Step_is_Enough_for_High-Quality_Image_to_Video_CVPR_2025_paper.html": {
    "title": "OSV: One Step is Enough for High-Quality Image to Video Generation",
    "volume": "main",
    "abstract": "Video diffusion models have shown great potential in generating high-quality videos, making them an increasingly popular focus. However, their inherent iterative nature leads to substantial computational and time costs. Although techniques such as consistency distillation and adversarial training have been employed to accelerate video diffusion by reducing inference steps, these methods often simply transfer the generation approaches from Image diffusion models to video diffusion models. As a result, these methods frequently fall short in terms of both performance and training stability. In this work, we introduce a two-stage training framework that effectively combines consistency distillation with adversarial training to address these challenges. Additionally, we propose a novel video discriminator design, which eliminates the need for decoding the video latents and improves the final performance. Our model is capable of producing high-quality videos in merely one-step, with the flexibility to perform multi-step refinement for further performance enhancement. Our quantitative evaluation on the OpenVid-1M benchmark shows that our model significantly outperforms existing methods. Notably, our 1-step performance (FVD 171.15) exceeds the 8-step performance of the consistency distillation based method, AnimateLCM (FVD 184.79), and approaches the 25-step performance of advanced Stable Video Diffusion (FVD 156.94)",
    "checked": true,
    "id": "c0ff16baf7bbf197f2ecfdf57a496f9a2ef1c5c4",
    "semantic_title": "osv: one step is enough for high-quality image to video generation",
    "citation_count": 16,
    "authors": [
      "Xiaofeng Mao",
      "Zhengkai Jiang",
      "Fu-yun Wang",
      "Jiangning Zhang",
      "Hao Chen",
      "Mingmin Chi",
      "Yabiao Wang",
      "Wenhan Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Duan_Fuzzy_Multimodal_Learning_for_Trusted_Cross-modal_Retrieval_CVPR_2025_paper.html": {
    "title": "Fuzzy Multimodal Learning for Trusted Cross-modal Retrieval",
    "volume": "main",
    "abstract": "Cross-modal retrieval aims to match related samples across distinct modalities, facilitating the retrieval and discovery of heterogeneous information. Although existing methods show promising performance, most are deterministic models and are unable to capture the uncertainty inherent in the retrieval outputs, leading to potentially unreliable results. To address this issue, we propose a novel framework called FUzzy Multimodal lEarning (FUME), which is able to self-estimate epistemic uncertainty, thereby embracing trusted cross-modal retrieval. Specifically, our FUME leverages the Fuzzy Set Theory to view the outputs of the classification network as a set of membership degrees and quantify category credibility by incorporating both possibility and necessity measures. However, directly optimizing the category credibility could mislead the model by over-optimizing the necessity for unmatched categories. To overcome this challenge, we present a novel fuzzy multimodal learning strategy, which utilizes label information to guide necessity optimization in the right direction, thereby indirectly optimizing category credibility and achieving accurate decision uncertainty quantification. Furthermore, we design an uncertainty merging scheme that accounts for decision uncertainties, thus further refining uncertainty estimates and boosting the trustworthiness of retrieval results. Extensive experiments on five benchmark datasets demonstrate that FUME remarkably improves both retrieval performance and reliability, offering a prospective solution for cross-modal retrieval in high-stakes applications. Code is available at https://github.com/siyuancncd/FUME",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Duan",
      "Yuan Sun",
      "Dezhong Peng",
      "Zheng Liu",
      "Xiaomin Song",
      "Peng Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qiu_AniGS_Animatable_Gaussian_Avatar_from_a_Single_Image_with_Inconsistent_CVPR_2025_paper.html": {
    "title": "AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent Gaussian Reconstruction",
    "volume": "main",
    "abstract": "Generating animatable human avatars from a single image is essential for various digital human modeling applications. Existing 3D reconstruction methods often struggle to capture fine details in animatable models, while generative approaches for controllable animation, though avoiding explicit 3D modeling, suffer from viewpoint inconsistencies in extreme poses and computational inefficiencies. In this paper, we address these challenges by leveraging the power of generative models to produce detailed multi-view canonical pose images, which help resolve ambiguities in animatable human reconstruction. We then propose a robust method for 3D reconstruction of inconsistent images, enabling real-time rendering during inference. Specifically, we adapt a transformer-based Text-to-Video model to generate multi-view canonical pose images and normal maps, pretraining on a large-scale monocular video dataset to improve generalization. To handle view inconsistencies, we recast the reconstruction problem as a 4D task and introduce an efficient 3D modeling approach using 4D Gaussian Splatting. Experiments demonstrate that our method achieves photorealistic, real-time animation of 3D human avatars from in-the-wild images, showcasing its effectiveness and generalization capability",
    "checked": true,
    "id": "4043e1ff3d33a890f363afe45df17a7cd02cab8c",
    "semantic_title": "anigs: animatable gaussian avatar from a single image with inconsistent gaussian reconstruction",
    "citation_count": 11,
    "authors": [
      "Lingteng Qiu",
      "Shenhao Zhu",
      "Qi Zuo",
      "Xiaodong Gu",
      "Yuan Dong",
      "Junfei Zhang",
      "Chao Xu",
      "Zhe Li",
      "Weihao Yuan",
      "Liefeng Bo",
      "Guanying Chen",
      "Zilong Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_GUI-Xplore_Empowering_Generalizable_GUI_Agents_with_One_Exploration_CVPR_2025_paper.html": {
    "title": "GUI-Xplore: Empowering Generalizable GUI Agents with One Exploration",
    "volume": "main",
    "abstract": "GUI agents hold significant potential to enhance the experience and efficiency of human-device interaction. However, current methods face challenges in generalizing across applications (apps) and tasks, primarily due to two fundamental limitations in existing datasets. First, these datasets overlook developer-induced structural variations among apps, limiting the transferability of knowledge across diverse software environments. Second, many of them focus solely on navigation tasks, which restricts their capacity to represent comprehensive software architectures and complex user interactions. To address these challenges, we introduce GUI-Xplore, a dataset meticulously designed to enhance cross-application and cross-task generalization via an exploration-and-reasoning framework. GUI-Xplore integrates pre-recorded exploration videos providing contextual insights, alongside five hierarchically structured downstream tasks designed to comprehensively evaluate GUI agent capabilities. To fully exploit GUI-Xplore's unique features, we propose Xplore-Agent, a GUI agent framework that combines Action-aware GUI Modeling with Graph-Guided Environment Reasoning. Further experiments indicate that Xplore-Agent achieves a 10% improvement over existing methods in unfamiliar environments, yet there remains significant potential for further enhancement towards truly generalizable GUI agents",
    "checked": true,
    "id": "bf5362c1b03221c91f715da97d8346c6e47619fc",
    "semantic_title": "gui-xplore: empowering generalizable gui agents with one exploration",
    "citation_count": 6,
    "authors": [
      "Yuchen Sun",
      "Shanhui Zhao",
      "Tao Yu",
      "Hao Wen",
      "Samith Va",
      "Mengwei Xu",
      "Yuanchun Li",
      "Chongyang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Few-Shot_Recognition_via_Stage-Wise_Retrieval-Augmented_Finetuning_CVPR_2025_paper.html": {
    "title": "Few-Shot Recognition via Stage-Wise Retrieval-Augmented Finetuning",
    "volume": "main",
    "abstract": "Few-shot recognition (FSR) aims to train a classification model with only a few labeled examples of each concept concerned by a downstream task, where data annotation cost can be prohibitively high. We develop methods to solve FSR by leveraging a pretrained Vision-Language Model (VLM). We particularly explore retrieval-augmented learning (RAL), which retrieves open data, e.g., the VLM's pretraining dataset, to learn models for better serving downstream tasks. RAL has been studied in zero-shot recognition but remains under-explored in FSR. Although applying RAL to FSR may seem straightforward, we observe interesting and novel challenges and opportunities. First, somewhat surprisingly, finetuning a VLM on a large amount of retrieved data underperforms state-of-the-art zero-shot methods. This is due to the imbalanced distribution of retrieved data and its domain gaps with the few-shot examples in the downstream task. Second, more surprisingly, we find that simply finetuning a VLM solely on few-shot examples significantly outperforms previous FSR methods, and finetuning on the mix of retrieved and few-shot data yields even better results. Third, to mitigate the imbalanced distribution and domain gap issues, we propose Stage-Wise retrieval-Augmented fineTuning (SWAT), which involves end-to-end finetuning on mixed data in the first stage and retraining the classifier on the few-shot data in the second stage. Extensive experiments on nine popular benchmarks demonstrate that SWAT significantly outperforms previous methods by >6% accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Liu",
      "Huixin Zhang",
      "Shubham Parashar",
      "Shu Kong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Concept_Replacer_Replacing_Sensitive_Concepts_in_Diffusion_Models_via_Precision_CVPR_2025_paper.html": {
    "title": "Concept Replacer: Replacing Sensitive Concepts in Diffusion Models via Precision Localization",
    "volume": "main",
    "abstract": "As large-scale diffusion models continue to advance, they excel at producing high-quality images but often generate unwanted content, such as sexually explicit or violent content. Existing methods for concept removal generally guide the image generation process but can unintentionally modify unrelated regions, leading to inconsistencies with the original model. We propose a novel approach for targeted concept replacing in diffusion models, enabling specific concepts to be removed without affecting non-target areas. Our method introduces a dedicated concept localizer for precisely identifying the target concept during the denoising process, trained with few-shot learning to require minimal labeled data. Within the identified region, we introduce a training-free Dual Prompts Cross-Attention (DPCA) module to substitute the target concept, ensuring minimal disruption to surrounding content. We evaluate our method on concept localization precision and replacement efficiency. Experimental results demonstrate that our method achieves superior precision in localizing target concepts and performs coherent concept replacement with minimal impact on non-target areas, outperforming existing approaches",
    "checked": true,
    "id": "1f21281245362ca32d4a4d27ca0f04bfa7ed155c",
    "semantic_title": "concept replacer: replacing sensitive concepts in diffusion models via precision localization",
    "citation_count": 4,
    "authors": [
      "Lingyun Zhang",
      "Yu Xie",
      "Yanwei Fu",
      "Ping Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bai_A_Regularization-Guided_Equivariant_Approach_for_Image_Restoration_CVPR_2025_paper.html": {
    "title": "A Regularization-Guided Equivariant Approach for Image Restoration",
    "volume": "main",
    "abstract": "Equivariant and invariant deep learning models have been developed to exploit intrinsic symmetries in data, demonstrating significant effectiveness in certain scenarios. However, these methods often suffer from limited representation accuracy and rely on strict symmetry assumptions that may not hold in practice. These limitations pose a significant drawback for image restoration tasks, which demands high accuracy and precise symmetry representation. To address these challenges, we propose a rotation-equivariant regularization strategy that adaptively enforces the appropriate symmetry constraints on the data while preserving the network's representational accuracy. Specifically, we introduce EQ-Reg, a regularizer designed to enhance rotation equivariance, which innovatively extends the insights of data-augmentation-based and equivariant-based methodologies. This is achieved through self-supervised learning and the spatial rotation and cyclic channel shift of feature maps deduce in the equivariant framework. Our approach firstly enables a non-strictly equivariant network suitable for image restoration, providing a simple and adaptive mechanism for adjusting equivariance based on task. Extensive experiments across three low-level tasks demonstrate the superior accuracy and generalization capability of our method, outperforming state-of-the-art approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulu Bai",
      "Jiahong Fu",
      "Qi Xie",
      "Deyu Meng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qiao_RestorGS_Depth-aware_Gaussian_Splatting_for_Efficient_3D_Scene_Restoration_CVPR_2025_paper.html": {
    "title": "RestorGS: Depth-aware Gaussian Splatting for Efficient 3D Scene Restoration",
    "volume": "main",
    "abstract": "3D Gaussian Splatting (3DGS) has recently achieved remarkable progress in novel view synthesis. However, existing methods rely heavily on high-quality data for rendering and struggle to handle degraded scenes with multi-view inconsistency, leading to inferior rendering quality. To address this challenge, we propose a novel Depth-aware Gaussian Splatting for efficient 3D scene Restoration, called RestorGS, which flexibly restores multiple degraded scenes using a unified framework. Specifically, RestorGS consists of two core designs: Appearance Decoupling and Depth-Guided Modeling. The former exploits appearance learning over spherical harmonics to decouple clear and degraded Gaussian, thus separating the clear views from the degraded ones. Collaboratively, the latter leverages the depth information to guide the degradation modeling, thereby facilitating the decoupling process. Benefiting from the above optimization strategy, our method achieves high-quality restoration while enabling real-time rendering speed. Extensive experiments show that our RestorGS outperforms existing methods significantly in underwater, nighttime, and hazy scenes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanjian Qiao",
      "Mingwen Shao",
      "Lingzhuang Meng",
      "Kai Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_IM-Portrait_Learning_3D-aware_Video_Diffusion_for_Photorealistic_Talking_Heads_from_CVPR_2025_paper.html": {
    "title": "IM-Portrait: Learning 3D-aware Video Diffusion for Photorealistic Talking Heads from Monocular VideosC",
    "volume": "main",
    "abstract": "We propose a novel 3D-aware diffusion-based method for generating photorealistic talking head videos directly from a single identity image and explicit control signals (e.g., expressions). Our method generates Multiplane Images (MPIs) that ensure geometric consistency, making them ideal for immersive viewing experiences like binocular videos for VR headsets.Unlike existing methods that often require a separate stage or joint optimization to reconstruct a 3D representation (such as NeRF or 3D Gaussians), our approach directly generates the final output through a single denoising process, eliminating the need for post-processing steps to render novel views efficiently.To effectively learn from monocular videos, we introduce a training mechanism that reconstructs the output MPI randomly in either the target or the reference camera space. This approach enables the model to simultaneously learn sharp image details and underlying 3D information.Extensive experiments demonstrate the effectiveness of our method, which achieves competitive avatar quality and novel-view rendering capabilities, even without explicit 3D reconstruction or high-quality multi-view training data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Li",
      "Ziqian Bai",
      "Feitong Tan",
      "Zhaopeng Cui",
      "Sean Fanello",
      "Yinda Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Deep_Fair_Multi-View_Clustering_with_Attention_KAN_CVPR_2025_paper.html": {
    "title": "Deep Fair Multi-View Clustering with Attention KAN",
    "volume": "main",
    "abstract": "Multi-view clustering is effective in unsupervised multi-view data analysis and has received considerable attention. However, most existing methods excessively emphasize certain attributes, resulting in unfair clustering outcomes, i.e., certain sensitive attributes dominate the clustering results. Moreover, existing methods struggle to effectively capture complex nonlinear relationships and interactions across views, limiting their ability to achieve optimal clustering performance. Therefore, in this work, we propose a novel method, Deep Fair Multi-View Clustering with Attention Kolmogorov-Arnold Network (DFMVC-AKAN), to generate fair clustering results while maintaining robust performance. DFMVC-AKAN integrates attention mechanisms into Kolmogorov-Arnold Networks (KAN) to exploit the complex nonlinear inter-view relationships. Specifically, KAN provides a nonlinear feature representation capable of efficiently approximating arbitrary multivariate continuous functions, augmented by a hybrid attention mechanism which enables the model to dynamically focus on the most relevant features. Finally, we refine the clustering assignments with a distribution alignment module to ensure fair outcomes across diverse groups while maintaining discriminative ability. Experimental results on four datasets containing sensitive attributes demonstrate that DFMVC-AKAN significantly improves fairness and clustering performance compared to state-of-the-art methods",
    "checked": true,
    "id": "f0b1e7177ec10eca839f0ac3630960ed0edfc489",
    "semantic_title": "deep fair multi-view clustering with attention kan",
    "citation_count": 0,
    "authors": [
      "HaiMing Xu",
      "Qianqian Wang",
      "Boyue Wang",
      "Quanxue Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_LineArt_A_Knowledge-guided_Training-free_High-quality_Appearance_Transfer_for_Design_Drawing_CVPR_2025_paper.html": {
    "title": "LineArt: A Knowledge-guided Training-free High-quality Appearance Transfer for Design Drawing with Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Wang",
      "Hongzhen Li",
      "Heng Fang",
      "Yichen Peng",
      "Haoran Xie",
      "Xi Yang",
      "Chuntao Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_4Real-Video_Learning_Generalizable_Photo-Realistic_4D_Video_Diffusion_CVPR_2025_paper.html": {
    "title": "4Real-Video: Learning Generalizable Photo-Realistic 4D Video Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoyang Wang",
      "Peiye Zhuang",
      "Tuan Duc Ngo",
      "Willi Menapace",
      "Aliaksandr Siarohin",
      "Michael Vasilkovsky",
      "Ivan Skorokhodov",
      "Sergey Tulyakov",
      "Peter Wonka",
      "Hsin-Ying Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kumar_DynaMoDe-NeRF_Motion-aware_Deblurring_Neural_Radiance_Field_for_Dynamic_Scenes_CVPR_2025_paper.html": {
    "title": "DynaMoDe-NeRF: Motion-aware Deblurring Neural Radiance Field for Dynamic Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashish Kumar",
      "Rajagopalan A. N."
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_VideoICL_Confidence-based_Iterative_In-context_Learning_for_Out-of-Distribution_Video_Understanding_CVPR_2025_paper.html": {
    "title": "VideoICL: Confidence-based Iterative In-context Learning for Out-of-Distribution Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangsan Kim",
      "Geon Park",
      "Youngwan Lee",
      "Woongyeong Yeo",
      "Sung Ju Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Garber_Zero-Shot_Image_Restoration_Using_Few-Step_Guidance_of_Consistency_Models_and_CVPR_2025_paper.html": {
    "title": "Zero-Shot Image Restoration Using Few-Step Guidance of Consistency Models (and Beyond)",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomer Garber",
      "Tom Tirer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xue_Similarity-Guided_Layer-Adaptive_Vision_Transformer_for_UAV_Tracking_CVPR_2025_paper.html": {
    "title": "Similarity-Guided Layer-Adaptive Vision Transformer for UAV Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaocan Xue",
      "Bineng Zhong",
      "Qihua Liang",
      "Yaozong Zheng",
      "Ning Li",
      "Yuanliang Xue",
      "Shuxiang Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_LidarGait_Learning_Local_Features_and_Size_Awareness_from_LiDAR_Point_CVPR_2025_paper.html": {
    "title": "LidarGait++: Learning Local Features and Size Awareness from LiDAR Point Clouds for 3D Gait Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanfu Shen",
      "Rui Wang",
      "Lixin Duan",
      "Shiqi Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_UrbanCAD_Towards_Highly_Controllable_and_Photorealistic_3D_Vehicles_for_Urban_CVPR_2025_paper.html": {
    "title": "UrbanCAD: Towards Highly Controllable and Photorealistic 3D Vehicles for Urban Scene Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichong Lu",
      "Yichi Cai",
      "Shangzhan Zhang",
      "Hongyu Zhou",
      "Haoji Hu",
      "Huimin Yu",
      "Andreas Geiger",
      "Yiyi Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Coarse_Correspondences_Boost_Spatial-Temporal_Reasoning_in_Multimodal_Language_Model_CVPR_2025_paper.html": {
    "title": "Coarse Correspondences Boost Spatial-Temporal Reasoning in Multimodal Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benlin Liu",
      "Yuhao Dong",
      "Yiqin Wang",
      "Zixian Ma",
      "Yansong Tang",
      "Luming Tang",
      "Yongming Rao",
      "Wei-Chiu Ma",
      "Ranjay Krishna"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jin_Diff-Palm_Realistic_Palmprint_Generation_with_Polynomial_Creases_and_Intra-Class_Variation_CVPR_2025_paper.html": {
    "title": "Diff-Palm: Realistic Palmprint Generation with Polynomial Creases and Intra-Class Variation Controllable Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianlong Jin",
      "Chenglong Zhao",
      "Ruixin Zhang",
      "Sheng Shang",
      "Jianqing Xu",
      "Jingyun Zhang",
      "ShaoMing Wang",
      "Yang Zhao",
      "Shouhong Ding",
      "Wei Jia",
      "Yunsheng Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wen_FoundationStereo_Zero-Shot_Stereo_Matching_CVPR_2025_paper.html": {
    "title": "FoundationStereo: Zero-Shot Stereo Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Wen",
      "Matthew Trepte",
      "Joseph Aribido",
      "Jan Kautz",
      "Orazio Gallo",
      "Stan Birchfield"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_Z-Magic_Zero-shot_Multiple_Attributes_Guided_Image_Creator_CVPR_2025_paper.html": {
    "title": "Z-Magic: Zero-shot Multiple Attributes Guided Image Creator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingying Deng",
      "Xiangyu He",
      "Fan Tang",
      "Weiming Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wei_UniNet_A_Contrastive_Learning-guided_Unified_Framework_with_Feature_Selection_for_CVPR_2025_paper.html": {
    "title": "UniNet: A Contrastive Learning-guided Unified Framework with Feature Selection for Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shun Wei",
      "Jielin Jiang",
      "Xiaolong Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_Chain_of_Semantics_Programming_in_3D_Gaussian_Splatting_Representation_for_CVPR_2025_paper.html": {
    "title": "Chain of Semantics Programming in 3D Gaussian Splatting Representation for 3D Vision Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Shi",
      "Mingyue Xiang",
      "Hao Sun",
      "Yixuan Huang",
      "Zhi Weng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tong_On_the_Zero-shot_Adversarial_Robustness_of_Vision-Language_Models_A_Truly_CVPR_2025_paper.html": {
    "title": "On the Zero-shot Adversarial Robustness of Vision-Language Models: A Truly Zero-shot and Training-free Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baoshun Tong",
      "Hanjiang Lai",
      "Yan Pan",
      "Jian Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Towards_General_Visual-Linguistic_Face_Forgery_Detection_CVPR_2025_paper.html": {
    "title": "Towards General Visual-Linguistic Face Forgery Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Sun",
      "Shen Chen",
      "Taiping Yao",
      "Ziyin Zhou",
      "Jiayi Ji",
      "Xiaoshuai Sun",
      "Chia-Wen Lin",
      "Rongrong Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Movie_Weaver_Tuning-Free_Multi-Concept_Video_Personalization_with_Anchored_Prompts_CVPR_2025_paper.html": {
    "title": "Movie Weaver: Tuning-Free Multi-Concept Video Personalization with Anchored Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Liang",
      "Haoyu Ma",
      "Zecheng He",
      "Tingbo Hou",
      "Ji Hou",
      "Kunpeng Li",
      "Xiaoliang Dai",
      "Felix Juefei-Xu",
      "Samaneh Azadi",
      "Animesh Sinha",
      "Peizhao Zhang",
      "Peter Vajda",
      "Diana Marculescu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Geng_LongVALE_Vision-Audio-Language-Event_Benchmark_Towards_Time-Aware_Omni-Modal_Perception_of_Long_Videos_CVPR_2025_paper.html": {
    "title": "LongVALE: Vision-Audio-Language-Event Benchmark Towards Time-Aware Omni-Modal Perception of Long Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tiantian Geng",
      "Jinrui Zhang",
      "Qingni Wang",
      "Teng Wang",
      "Jinming Duan",
      "Feng Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_MVPortrait_Text-Guided_Motion_and_Emotion_Control_for_Multi-view_Vivid_Portrait_CVPR_2025_paper.html": {
    "title": "MVPortrait: Text-Guided Motion and Emotion Control for Multi-view Vivid Portrait Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yukang Lin",
      "Hokit Fung",
      "Jianjin Xu",
      "Zeping Ren",
      "Adela S.M. Lau",
      "Guosheng Yin",
      "Xiu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_MoEdit_On_Learning_Quantity_Perception_for_Multi-object_Image_Editing_CVPR_2025_paper.html": {
    "title": "MoEdit: On Learning Quantity Perception for Multi-object Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanfeng Li",
      "Kahou Chan",
      "Yue Sun",
      "Chantong Lam",
      "Tong Tong",
      "Zitong Yu",
      "Keren Fu",
      "Xiaohong Liu",
      "Tao Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gizdov_Seeing_More_with_Less_Human-like_Representations_in_Vision_Models_CVPR_2025_paper.html": {
    "title": "Seeing More with Less: Human-like Representations in Vision Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrey Gizdov",
      "Shimon Ullman",
      "Daniel Harari"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Modeling_Thousands_of_Human_Annotators_for_Generalizable_Text-to-Image_Person_Re-identification_CVPR_2025_paper.html": {
    "title": "Modeling Thousands of Human Annotators for Generalizable Text-to-Image Person Re-identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayu Jiang",
      "Changxing Ding",
      "Wentao Tan",
      "Junhong Wang",
      "Jin Tao",
      "Xiangmin Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Accelerating_Multimodal_Large_Language_Models_by_Searching_Optimal_Vision_Token_CVPR_2025_paper.html": {
    "title": "Accelerating Multimodal Large Language Models by Searching Optimal Vision Token Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyu Zhao",
      "Zhenting Wang",
      "Felix Juefei-Xu",
      "Xide Xia",
      "Miao Liu",
      "Xiaofang Wang",
      "Mingfu Liang",
      "Ning Zhang",
      "Dimitris N. Metaxas",
      "Licheng Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Safari_Matrix-Free_Shared_Intrinsics_Bundle_Adjustment_CVPR_2025_paper.html": {
    "title": "Matrix-Free Shared Intrinsics Bundle Adjustment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Safari"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_AeroGen_Enhancing_Remote_Sensing_Object_Detection_with_Diffusion-Driven_Data_Generation_CVPR_2025_paper.html": {
    "title": "AeroGen: Enhancing Remote Sensing Object Detection with Diffusion-Driven Data Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Datao Tang",
      "Xiangyong Cao",
      "Xuan Wu",
      "Jialin Li",
      "Jing Yao",
      "Xueru Bai",
      "Dongsheng Jiang",
      "Yin Li",
      "Deyu Meng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Tra-MoE_Learning_Trajectory_Prediction_Model_from_Multiple_Domains_for_Adaptive_CVPR_2025_paper.html": {
    "title": "Tra-MoE: Learning Trajectory Prediction Model from Multiple Domains for Adaptive Policy Conditioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiange Yang",
      "Haoyi Zhu",
      "Yating Wang",
      "Gangshan Wu",
      "Tong He",
      "Limin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Mitigating_Hallucinations_in_Large_Vision-Language_Models_via_DPO_On-Policy_Data_CVPR_2025_paper.html": {
    "title": "Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihe Yang",
      "Xufang Luo",
      "Dongqi Han",
      "Yunjian Xu",
      "Dongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Style_Quantization_for_Data-Efficient_GAN_Training_CVPR_2025_paper.html": {
    "title": "Style Quantization for Data-Efficient GAN Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Wang",
      "Xin Lan",
      "Jizhe Zhou",
      "Yuxin Tian",
      "Jiancheng Lv"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Localizing_Events_in_Videos_with_Multimodal_Queries_CVPR_2025_paper.html": {
    "title": "Localizing Events in Videos with Multimodal Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gengyuan Zhang",
      "Mang Ling Ada Fok",
      "Jialu Ma",
      "Yan Xia",
      "Daniel Cremers",
      "Philip Torr",
      "Volker Tresp",
      "Jindong Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_PhysVLM_Enabling_Visual_Language_Models_to_Understand_Robotic_Physical_Reachability_CVPR_2025_paper.html": {
    "title": "PhysVLM: Enabling Visual Language Models to Understand Robotic Physical Reachability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijie Zhou",
      "Manli Tao",
      "Chaoyang Zhao",
      "Haiyun Guo",
      "Honghui Dong",
      "Ming Tang",
      "Jinqiao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Stracke_CleanDIFT_Diffusion_Features_without_Noise_CVPR_2025_paper.html": {
    "title": "CleanDIFT: Diffusion Features without Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nick Stracke",
      "Stefan Andreas Baumann",
      "Kolja Bauer",
      "Frank Fundel",
      "Bj√∂rn Ommer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hoogeboom_Simpler_Diffusion_1.5_FID_on_ImageNet512_with_Pixel-space_Diffusion_CVPR_2025_paper.html": {
    "title": "Simpler Diffusion: 1.5 FID on ImageNet512 with Pixel-space Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Emiel Hoogeboom",
      "Thomas Mensink",
      "Jonathan Heek",
      "Kay Lamerigts",
      "Ruiqi Gao",
      "Tim Salimans"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Uncertainty-Instructed_Structure_Injection_for_Generalizable_HD_Map_Construction_CVPR_2025_paper.html": {
    "title": "Uncertainty-Instructed Structure Injection for Generalizable HD Map Construction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaolu Liu",
      "Ruizi Yang",
      "Song Wang",
      "Wentong Li",
      "Junbo Chen",
      "Jianke Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Velayudhan_STING-BEE_Towards_Vision-Language_Model_for_Real-World_X-ray_Baggage_Security_Inspection_CVPR_2025_paper.html": {
    "title": "STING-BEE: Towards Vision-Language Model for Real-World X-ray Baggage Security Inspection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Divya Velayudhan",
      "Abdelfatah Ahmed",
      "Mohamad Alansari",
      "Neha Gour",
      "Abderaouf Behouch",
      "Taimur Hassan",
      "Syed Talal Wasim",
      "Nabil Maalej",
      "Muzammal Naseer",
      "Juergen Gall",
      "Mohammed Bennamoun",
      "Ernesto Damiani",
      "Naoufel Werghi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Not_All_Parameters_Matter_Masking_Diffusion_Models_for_Enhancing_Generation_CVPR_2025_paper.html": {
    "title": "Not All Parameters Matter: Masking Diffusion Models for Enhancing Generation Ability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Wang",
      "Senmao Li",
      "Fei Yang",
      "Jianye Wang",
      "Ziheng Zhang",
      "Yuhan Liu",
      "Yaxing Wang",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Agro_MAD_Memory-Augmented_Detection_of_3D_Objects_CVPR_2025_paper.html": {
    "title": "MAD: Memory-Augmented Detection of 3D Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Agro",
      "Sergio Casas",
      "Patrick Wang",
      "Thomas Gilles",
      "Raquel Urtasun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kamberov_Doppelgangers_and_Adversarial_Vulnerability_CVPR_2025_paper.html": {
    "title": "Doppelgangers and Adversarial Vulnerability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "George Kamberov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zamfir_Complexity_Experts_are_Task-Discriminative_Learners_for_Any_Image_Restoration_CVPR_2025_paper.html": {
    "title": "Complexity Experts are Task-Discriminative Learners for Any Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eduard Zamfir",
      "Zongwei Wu",
      "Nancy Mehta",
      "Yuedong Tan",
      "Danda Pani Paudel",
      "Yulun Zhang",
      "Radu Timofte"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Generative_Omnimatte_Learning_to_Decompose_Video_into_Layers_CVPR_2025_paper.html": {
    "title": "Generative Omnimatte: Learning to Decompose Video into Layers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao-Chih Lee",
      "Erika Lu",
      "Sarah Rumbley",
      "Michal Geyer",
      "Jia-Bin Huang",
      "Tali Dekel",
      "Forrester Cole"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_5100_Breaking_Performance_Shackles_of_Full_Fine-Tuning_on_Visual_Recognition_CVPR_2025_paper.html": {
    "title": "5%>100%: Breaking Performance Shackles of Full Fine-Tuning on Visual Recognition Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongshuo Yin",
      "Leiyi Hu",
      "Bin Li",
      "Youqun Zhang",
      "Xue Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Santra_Precise_Event_Spotting_in_Sports_Videos_Solving_Long-Range_Dependency_and_CVPR_2025_paper.html": {
    "title": "Precise Event Spotting in Sports Videos: Solving Long-Range Dependency and Class Imbalance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanchayan Santra",
      "Vishal Chudasama",
      "Pankaj Wasnik",
      "Vineeth N Balasubramanian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Real-IAD_D3_A_Real-World_2DPseudo-3D3D_Dataset_for_Industrial_Anomaly_Detection_CVPR_2025_paper.html": {
    "title": "Real-IAD D3: A Real-World 2D/Pseudo-3D/3D Dataset for Industrial Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbing Zhu",
      "Lidong Wang",
      "Ziqing Zhou",
      "Chengjie Wang",
      "Yurui Pan",
      "Ruoyi Zhang",
      "Zhuhao Chen",
      "Linjie Cheng",
      "Bin-Bin Gao",
      "Jiangning Zhang",
      "Zhenye Gan",
      "Yuxie Wang",
      "Yulong Chen",
      "Shuguang Qian",
      "Mingmin Chi",
      "Bo Peng",
      "Lizhuang Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Steady_Progress_Beats_Stagnation_Mutual_Aid_of_Foundation_and_Conventional_CVPR_2025_paper.html": {
    "title": "Steady Progress Beats Stagnation: Mutual Aid of Foundation and Conventional Models in Mixed Domain Semi-Supervised Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qinghe Ma",
      "Jian Zhang",
      "Zekun Li",
      "Lei Qi",
      "Qian Yu",
      "Yinghuan Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Afane_ATP_Adaptive_Threshold_Pruning_for_Efficient_Data_Encoding_in_Quantum_CVPR_2025_paper.html": {
    "title": "ATP: Adaptive Threshold Pruning for Efficient Data Encoding in Quantum Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Afane",
      "Gabrielle Ebbrecht",
      "Ying Wang",
      "Juntao Chen",
      "Junaid Farooq"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shum_Color_Alignment_in_Diffusion_CVPR_2025_paper.html": {
    "title": "Color Alignment in Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ka Chun Shum",
      "Binh-Son Hua",
      "Duc Thanh Nguyen",
      "Sai-Kit Yeung"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Reilly_LLAVIDAL_A_Large_LAnguage_VIsion_Model_for_Daily_Activities_of_CVPR_2025_paper.html": {
    "title": "LLAVIDAL: A Large LAnguage VIsion Model for Daily Activities of Living",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dominick Reilly",
      "Rajatsubhra Chakraborty",
      "Arkaprava Sinha",
      "Manish Kumar Govind",
      "Pu Wang",
      "Francois Bremond",
      "Le Xue",
      "Srijan Das"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Language-Guided_Salient_Object_Ranking_CVPR_2025_paper.html": {
    "title": "Language-Guided Salient Object Ranking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fang Liu",
      "Yuhao Liu",
      "Ke Xu",
      "Shuquan Ye",
      "Gerhard Petrus Hancke",
      "Rynson W. H. Lau"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_Decoupled_Motion_Expression_Video_Segmentation_CVPR_2025_paper.html": {
    "title": "Decoupled Motion Expression Video Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Fang",
      "Runmin Cong",
      "Xiankai Lu",
      "Xiaofei Zhou",
      "Sam Kwong",
      "Wei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ouyang_K-LoRA_Unlocking_Training-Free_Fusion_of_Any_Subject_and_Style_LoRAs_CVPR_2025_paper.html": {
    "title": "K-LoRA: Unlocking Training-Free Fusion of Any Subject and Style LoRAs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziheng Ouyang",
      "Zhen Li",
      "Qibin Hou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_Towards_More_General_Video-based_Deepfake_Detection_through_Facial_Component_Guided_CVPR_2025_paper.html": {
    "title": "Towards More General Video-based Deepfake Detection through Facial Component Guided Adaptation for Foundation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue-Hua Han",
      "Tai-Ming Huang",
      "Kai-Lung Hua",
      "Jun-Cheng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_WF-VAE_Enhancing_Video_VAE_by_Wavelet-Driven_Energy_Flow_for_Latent_CVPR_2025_paper.html": {
    "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongjian Li",
      "Bin Lin",
      "Yang Ye",
      "Liuhan Chen",
      "Xinhua Cheng",
      "Shenghai Yuan",
      "Li Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_SP3D_Boosting_Sparsely-Supervised_3D_Object_Detection_via_Accurate_Cross-Modal_Semantic_CVPR_2025_paper.html": {
    "title": "SP3D: Boosting Sparsely-Supervised 3D Object Detection via Accurate Cross-Modal Semantic Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijia Zhao",
      "Qiming Xia",
      "Xusheng Guo",
      "Pufan Zou",
      "Maoji Zheng",
      "Hai Wu",
      "Chenglu Wen",
      "Cheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ji_ARKit_LabelMaker_A_New_Scale_for_Indoor_3D_Scene_Understanding_CVPR_2025_paper.html": {
    "title": "ARKit LabelMaker: A New Scale for Indoor 3D Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangda Ji",
      "Silvan Weder",
      "Francis Engelmann",
      "Marc Pollefeys",
      "Hermann Blum"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_VoCo-LLaMA_Towards_Vision_Compression_with_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "VoCo-LLaMA: Towards Vision Compression with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xubing Ye",
      "Yukang Gan",
      "Xiaoke Huang",
      "Yixiao Ge",
      "Yansong Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Henschel_StreamingT2V_Consistent_Dynamic_and_Extendable_Long_Video_Generation_from_Text_CVPR_2025_paper.html": {
    "title": "StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Roberto Henschel",
      "Levon Khachatryan",
      "Hayk Poghosyan",
      "Daniil Hayrapetyan",
      "Vahram Tadevosyan",
      "Zhangyang Wang",
      "Shant Navasardyan",
      "Humphrey Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_Focal_Split_Untethered_Snapshot_Depth_from_Differential_Defocus_CVPR_2025_paper.html": {
    "title": "Focal Split: Untethered Snapshot Depth from Differential Defocus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Luo",
      "John Mamish",
      "Alan Fu",
      "Thomas Concannon",
      "Josiah Hester",
      "Emma Alexander",
      "Qi Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_AFL_A_Single-Round_Analytic_Approach_for_Federated_Learning_with_Pre-trained_CVPR_2025_paper.html": {
    "title": "AFL: A Single-Round Analytic Approach for Federated Learning with Pre-trained Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Run He",
      "Kai Tong",
      "Di Fang",
      "Han Sun",
      "Ziqian Zeng",
      "Haoran Li",
      "Tianyi Chen",
      "Huiping Zhuang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_XLRS-Bench_Could_Your_Multimodal_LLMs_Understand_Extremely_Large_Ultra-High-Resolution_Remote_CVPR_2025_paper.html": {
    "title": "XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large Ultra-High-Resolution Remote Sensing Imagery?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengxiang Wang",
      "Hongzhen Wang",
      "Zonghao Guo",
      "Di Wang",
      "Yulin Wang",
      "Mingshuo Chen",
      "Qiang Ma",
      "Long Lan",
      "Wenjing Yang",
      "Jing Zhang",
      "Zhiyuan Liu",
      "Maosong Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_BOLT_Boost_Large_Vision-Language_Model_Without_Training_for_Long-form_Video_CVPR_2025_paper.html": {
    "title": "BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuming Liu",
      "Chen Zhao",
      "Tianqi Xu",
      "Bernard Ghanem"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Berisha_Efficient_Data_Driven_Mixture-of-Expert_Extraction_from_Trained_Networks_CVPR_2025_paper.html": {
    "title": "Efficient Data Driven Mixture-of-Expert Extraction from Trained Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uranik Berisha",
      "Jens Mehnert",
      "Alexandru Paul Condurache"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bilecen_Reference-Based_3D-Aware_Image_Editing_with_Triplanes_CVPR_2025_paper.html": {
    "title": "Reference-Based 3D-Aware Image Editing with Triplanes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bahri Batuhan Bilecen",
      "Yigit Yalin",
      "Ning Yu",
      "Aysegul Dundar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_StyleSSP_Sampling_StartPoint_Enhancement_for_Training-free_Diffusion-based_Method_for_Style_CVPR_2025_paper.html": {
    "title": "StyleSSP: Sampling StartPoint Enhancement for Training-free Diffusion-based Method for Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruojun Xu",
      "Weijie Xi",
      "XiaoDi Wang",
      "Yongbo Mao",
      "Zach Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shao_PURA_Parameter_Update-Recovery_Test-Time_Adaption_for_RGB-T_Tracking_CVPR_2025_paper.html": {
    "title": "PURA: Parameter Update-Recovery Test-Time Adaption for RGB-T Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zekai Shao",
      "Yufan Hu",
      "Bin Fan",
      "Hongmin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_One_is_Plenty_A_Polymorphic_Feature_Interpreter_for_Immutable_Heterogeneous_CVPR_2025_paper.html": {
    "title": "One is Plenty: A Polymorphic Feature Interpreter for Immutable Heterogeneous Collaborative Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Xia",
      "Quan Yuan",
      "Guiyang Luo",
      "Xiaoyuan Fu",
      "Yang Li",
      "Xuanhan Zhu",
      "Tianyou Luo",
      "Siheng Chen",
      "Jinglin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_Towards_All-in-One_Medical_Image_Re-Identification_CVPR_2025_paper.html": {
    "title": "Towards All-in-One Medical Image Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Tian",
      "Kaiyuan Ji",
      "Rongzhao Zhang",
      "Yankai Jiang",
      "Chunyi Li",
      "Xiaosong Wang",
      "Guangtao Zhai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_SegAgent_Exploring_Pixel_Understanding_Capabilities_in_MLLMs_by_Imitating_Human_CVPR_2025_paper.html": {
    "title": "SegAgent: Exploring Pixel Understanding Capabilities in MLLMs by Imitating Human Annotator Trajectories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muzhi Zhu",
      "Yuzhuo Tian",
      "Hao Chen",
      "Chunluan Zhou",
      "Qingpei Guo",
      "Yang Liu",
      "Ming Yang",
      "Chunhua Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Motions_as_Queries_One-Stage_Multi-Person_Holistic_Human_Motion_Capture_CVPR_2025_paper.html": {
    "title": "Motions as Queries: One-Stage Multi-Person Holistic Human Motion Capture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kenkun Liu",
      "Yurong Fu",
      "Weihao Yuan",
      "Jing Lin",
      "Peihao Li",
      "Xiaodong Gu",
      "Lingteng Qiu",
      "Haoqian Wang",
      "Zilong Dong",
      "Xiaoguang Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_SceneCrafter_Controllable_Multi-View_Driving_Scene_Editing_CVPR_2025_paper.html": {
    "title": "SceneCrafter: Controllable Multi-View Driving Scene Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehao Zhu",
      "Yuliang Zou",
      "Chiyu Max Jiang",
      "Bo Sun",
      "Vincent Casser",
      "Xiukun Huang",
      "Jiahao Wang",
      "Zhenpei Yang",
      "Ruiqi Gao",
      "Leonidas Guibas",
      "Mingxing Tan",
      "Dragomir Anguelov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_AMO_Sampler_Enhancing_Text_Rendering_with_Overshooting_CVPR_2025_paper.html": {
    "title": "AMO Sampler: Enhancing Text Rendering with Overshooting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xixi Hu",
      "Keyang Xu",
      "Bo Liu",
      "Qiang Liu",
      "Hongliang Fei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_ImViD_Immersive_Volumetric_Videos_for_Enhanced_VR_Engagement_CVPR_2025_paper.html": {
    "title": "ImViD: Immersive Volumetric Videos for Enhanced VR Engagement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengxian Yang",
      "Shi Pan",
      "Shengqi Wang",
      "Haoxiang Wang",
      "Li Lin",
      "Guanjun Li",
      "Zhengqi Wen",
      "Borong Lin",
      "Jianhua Tao",
      "Tao Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wei_Integral_Fast_Fourier_Color_Constancy_CVPR_2025_paper.html": {
    "title": "Integral Fast Fourier Color Constancy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenjun Wei",
      "Yanlin Qian",
      "Huaian Chen",
      "Junkang Dai",
      "Yi Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gui_I2VGuard_Safeguarding_Images_against_Misuse_in_Diffusion-based_Image-to-Video_Models_CVPR_2025_paper.html": {
    "title": "I2VGuard: Safeguarding Images against Misuse in Diffusion-based Image-to-Video Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongnan Gui",
      "Xun Guo",
      "Wengang Zhou",
      "Yan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Victorica_Saliuitl_Ensemble_Salience_Guided_Recovery_of_Adversarial_Patches_against_CNNs_CVPR_2025_paper.html": {
    "title": "Saliuitl: Ensemble Salience Guided Recovery of Adversarial Patches against CNNs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mauricio Byrd Victorica",
      "Gy√∂rgy D√°n",
      "Henrik Sandberg"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Matsubara_HeatFormer_A_Neural_Optimizer_for_Multiview_Human_Mesh_Recovery_CVPR_2025_paper.html": {
    "title": "HeatFormer: A Neural Optimizer for Multiview Human Mesh Recovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuto Matsubara",
      "Ko Nishino"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_ResCLIP_Residual_Attention_for_Training-free_Dense_Vision-language_Inference_CVPR_2025_paper.html": {
    "title": "ResCLIP: Residual Attention for Training-free Dense Vision-language Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Yang",
      "Jinhong Deng",
      "Wen Li",
      "Lixin Duan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_GPS_as_a_Control_Signal_for_Image_Generation_CVPR_2025_paper.html": {
    "title": "GPS as a Control Signal for Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Feng",
      "Ziyang Chen",
      "Aleksander Holynski",
      "Alexei A. Efros",
      "Andrew Owens"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_CPath-Omni_A_Unified_Multimodal_Foundation_Model_for_Patch_and_Whole_CVPR_2025_paper.html": {
    "title": "CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole Slide Image Analysis in Computational Pathology",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Sun",
      "Yixuan Si",
      "Chenglu Zhu",
      "Xuan Gong",
      "Kai Zhang",
      "Pingyi Chen",
      "Ye Zhang",
      "Zhongyi Shui",
      "Tao Lin",
      "Lin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cui_OPTICAL_Leveraging_Optimal_Transport_for_Contribution_Allocation_in_Dataset_Distillation_CVPR_2025_paper.html": {
    "title": "OPTICAL: Leveraging Optimal Transport for Contribution Allocation in Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Cui",
      "Yulei Qin",
      "Wengang Zhou",
      "Hongsheng Li",
      "Houqiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yugay_MAGiC-SLAM_Multi-Agent_Gaussian_Globally_Consistent__SLAM_CVPR_2025_paper.html": {
    "title": "MAGiC-SLAM: Multi-Agent Gaussian Globally Consistent SLAM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vladimir Yugay",
      "Theo Gevers",
      "Martin R. Oswald"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qian_Dispider_Enabling_Video_LLMs_with_Active_Real-Time_Interaction_via_Disentangled_CVPR_2025_paper.html": {
    "title": "Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Qian",
      "Shuangrui Ding",
      "Xiaoyi Dong",
      "Pan Zhang",
      "Yuhang Zang",
      "Yuhang Cao",
      "Dahua Lin",
      "Jiaqi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_NTClick_Achieving_Precise_Interactive_Segmentation_With_Noise-tolerant_Clicks_CVPR_2025_paper.html": {
    "title": "NTClick: Achieving Precise Interactive Segmentation With Noise-tolerant Clicks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyi Zhang",
      "Ting Liu",
      "Xiaochao Qu",
      "Luoqi Liu",
      "Yao Zhao",
      "Yunchao Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_Show_and_Segment_Universal_Medical_Image_Segmentation_via_In-Context_Learning_CVPR_2025_paper.html": {
    "title": "Show and Segment: Universal Medical Image Segmentation via In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhe Gao",
      "Di Liu",
      "Zhuowei Li",
      "Yunsheng Li",
      "Dongdong Chen",
      "Mu Zhou",
      "Dimitris N. Metaxas"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cao_MVGenMaster_Scaling_Multi-View_Generation_from_Any_Image_via_3D_Priors_CVPR_2025_paper.html": {
    "title": "MVGenMaster: Scaling Multi-View Generation from Any Image via 3D Priors Enhanced Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenjie Cao",
      "Chaohui Yu",
      "Shang Liu",
      "Fan Wang",
      "Xiangyang Xue",
      "Yanwei Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_CADCrafter_Generating_Computer-Aided_Design_Models_from_Unconstrained_Images_CVPR_2025_paper.html": {
    "title": "CADCrafter: Generating Computer-Aided Design Models from Unconstrained Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Chen",
      "Jiacheng Wei",
      "Tianrun Chen",
      "Chi Zhang",
      "Xiaofeng Yang",
      "Shangzhan Zhang",
      "Bingchen Yang",
      "Chuan-Sheng Foo",
      "Guosheng Lin",
      "Qixing Huang",
      "Fayao Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Bayesian_Test-Time_Adaptation_for_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "Bayesian Test-Time Adaptation for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lihua Zhou",
      "Mao Ye",
      "Shuaifeng Li",
      "Nianxin Li",
      "Xiatian Zhu",
      "Lei Deng",
      "Hongbin Liu",
      "Zhen Lei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Alzayer_Generative_Multiview_Relighting_for_3D_Reconstruction_under_Extreme_Illumination_Variation_CVPR_2025_paper.html": {
    "title": "Generative Multiview Relighting for 3D Reconstruction under Extreme Illumination Variation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hadi Alzayer",
      "Philipp Henzler",
      "Jonathan T. Barron",
      "Jia-Bin Huang",
      "Pratul P. Srinivasan",
      "Dor Verbin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Causal_Composition_Diffusion_Model_for_Closed-loop_Traffic_Generation_CVPR_2025_paper.html": {
    "title": "Causal Composition Diffusion Model for Closed-loop Traffic Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haohong Lin",
      "Xin Huang",
      "Tung Phan",
      "David Hayden",
      "Huan Zhang",
      "Ding Zhao",
      "Siddhartha Srinivasa",
      "Eric Wolff",
      "Hongge Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Change3D_Revisiting_Change_Detection_and_Captioning_from_A_Video_Modeling_CVPR_2025_paper.html": {
    "title": "Change3D: Revisiting Change Detection and Captioning from A Video Modeling Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duowang Zhu",
      "Xiaohu Huang",
      "Haiyan Huang",
      "Hao Zhou",
      "Zhenfeng Shao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_DyMO_Training-Free_Diffusion_Model_Alignment_with_Dynamic_Multi-Objective_Scheduling_CVPR_2025_paper.html": {
    "title": "DyMO: Training-Free Diffusion Model Alignment with Dynamic Multi-Objective Scheduling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Xie",
      "Dong Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_HiMoR_Monocular_Deformable_Gaussian_Reconstruction_with_Hierarchical_Motion_Representation_CVPR_2025_paper.html": {
    "title": "HiMoR: Monocular Deformable Gaussian Reconstruction with Hierarchical Motion Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Liang",
      "Tianhan Xu",
      "Yuta Kikuchi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_GENIUS_A_Generative_Framework_for_Universal_Multimodal_Search_CVPR_2025_paper.html": {
    "title": "GENIUS: A Generative Framework for Universal Multimodal Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungyeon Kim",
      "Xinliang Zhu",
      "Xiaofan Lin",
      "Muhammet Bastan",
      "Douglas Gray",
      "Suha Kwak"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Enhanced_Visual-Semantic_Interaction_with_Tailored_Prompts_for_Pedestrian_Attribute_Recognition_CVPR_2025_paper.html": {
    "title": "Enhanced Visual-Semantic Interaction with Tailored Prompts for Pedestrian Attribute Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyi Wu",
      "Yan Huang",
      "Min Gao",
      "Yuzhen Niu",
      "Yuzhong Chen",
      "Qiang Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Boss_SF3D_Stable_Fast_3D_Mesh_Reconstruction_with_UV-unwrapping_and_Illumination_CVPR_2025_paper.html": {
    "title": "SF3D: Stable Fast 3D Mesh Reconstruction with UV-unwrapping and Illumination Disentanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mark Boss",
      "Zixuan Huang",
      "Aaryaman Vasishta",
      "Varun Jampani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_HSI-GPT_A_General-Purpose_Large_Scene-Motion-Language_Model_for_Human_Scene_Interaction_CVPR_2025_paper.html": {
    "title": "HSI-GPT: A General-Purpose Large Scene-Motion-Language Model for Human Scene Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Wang",
      "Yali Li",
      "Xiang Li",
      "Shengjin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Towards_Precise_Embodied_Dialogue_Localization_via_Causality_Guided_Diffusion_CVPR_2025_paper.html": {
    "title": "Towards Precise Embodied Dialogue Localization via Causality Guided Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Wang",
      "Le Wang",
      "Sanping Zhou",
      "Jingyi Tian",
      "Zheng Qin",
      "Yabing Wang",
      "Gang Hua",
      "Wei Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Vid2Avatar-Pro_Authentic_Avatar_from_Videos_in_the_Wild_via_Universal_CVPR_2025_paper.html": {
    "title": "Vid2Avatar-Pro: Authentic Avatar from Videos in the Wild via Universal Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Guo",
      "Junxuan Li",
      "Yash Kant",
      "Yaser Sheikh",
      "Shunsuke Saito",
      "Chen Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_RoomPainter_View-Integrated_Diffusion_for_Consistent_Indoor_Scene_Texturing_CVPR_2025_paper.html": {
    "title": "RoomPainter: View-Integrated Diffusion for Consistent Indoor Scene Texturing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhipeng Huang",
      "Wangbo Yu",
      "Xinhua Cheng",
      "Chengshu Zhao",
      "Yunyang Ge",
      "Mingyi Guo",
      "Li Yuan",
      "Yonghong Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Attribute-formed_Class-specific_Concept_Space_Endowing_Language_Bottleneck_Model_with_Better_CVPR_2025_paper.html": {
    "title": "Attribute-formed Class-specific Concept Space: Endowing Language Bottleneck Model with Better Interpretability and Scalability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianyang Zhang",
      "Qianli Luo",
      "Guowu Yang",
      "Wenjing Yang",
      "Weide Liu",
      "Guosheng Lin",
      "Fengmao Lv"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qi_Customized_Condition_Controllable_Generation_for_Video_Soundtrack_CVPR_2025_paper.html": {
    "title": "Customized Condition Controllable Generation for Video Soundtrack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Qi",
      "Kunsheng Ma",
      "Changsheng Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_ProjAttacker_A_Configurable_Physical_Adversarial_Attack_for_Face_Recognition_via_CVPR_2025_paper.html": {
    "title": "ProjAttacker: A Configurable Physical Adversarial Attack for Face Recognition via Projector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanwei Liu",
      "Hui Wei",
      "Chengyu Jia",
      "Ruqi Xiao",
      "Weijian Ruan",
      "Xingxing Wei",
      "Joey Tianyi Zhou",
      "Zheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_EfficientViM_Efficient_Vision_Mamba_with_Hidden_State_Mixer_based_State_CVPR_2025_paper.html": {
    "title": "EfficientViM: Efficient Vision Mamba with Hidden State Mixer based State Space Duality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanghyeok Lee",
      "Joonmyung Choi",
      "Hyunwoo J. Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tu_A4A_Adapter_for_Adapter_Transfer_via_All-for-All_Mapping_for_Cross-Architecture_CVPR_2025_paper.html": {
    "title": "A4A: Adapter for Adapter Transfer via All-for-All Mapping for Cross-Architecture Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keyu Tu",
      "Mengqi Huang",
      "Zhuowei Chen",
      "Zhendong Mao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Athar_ViCaS_A_Dataset_for_Combining_Holistic_and_Pixel-level_Video_Understanding_CVPR_2025_paper.html": {
    "title": "ViCaS: A Dataset for Combining Holistic and Pixel-level Video Understanding using Captions with Grounded Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Athar",
      "Xueqing Deng",
      "Liang-Chieh Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_A_Universal_Scale-Adaptive_Deformable_Transformer_for_Image_Restoration_across_Diverse_CVPR_2025_paper.html": {
    "title": "A Universal Scale-Adaptive Deformable Transformer for Image Restoration across Diverse Artifacts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuyi He",
      "Yuhui Quan",
      "Ruotao Xu",
      "Hui Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mao_WISE_A_Framework_for_Gigapixel_Whole-Slide-Image_Lossless_Compression_CVPR_2025_paper.html": {
    "title": "WISE: A Framework for Gigapixel Whole-Slide-Image Lossless Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Mao",
      "Jun Wang",
      "Nan Guan",
      "Chun Jason Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Takeda_Gromov-Wasserstein_Problem_with_Cyclic_Symmetry_CVPR_2025_paper.html": {
    "title": "Gromov-Wasserstein Problem with Cyclic Symmetry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shoichiro Takeda",
      "Yasunori Akagi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_IRIS_Inverse_Rendering_of_Indoor_Scenes_from_Low_Dynamic_Range_CVPR_2025_paper.html": {
    "title": "IRIS: Inverse Rendering of Indoor Scenes from Low Dynamic Range Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chih-Hao Lin",
      "Jia-Bin Huang",
      "Zhengqin Li",
      "Zhao Dong",
      "Christian Richardt",
      "Tuotuo Li",
      "Michael Zollh√∂fer",
      "Johannes Kopf",
      "Shenlong Wang",
      "Changil Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_SimAvatar_Simulation-Ready_Avatars_with_Layered_Hair_and_Clothing_CVPR_2025_paper.html": {
    "title": "SimAvatar: Simulation-Ready Avatars with Layered Hair and Clothing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueting Li",
      "Ye Yuan",
      "Shalini De Mello",
      "Gilles Daviet",
      "Jonathan Leaf",
      "Miles Macklin",
      "Jan Kautz",
      "Umar Iqbal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Test-Time_Backdoor_Detection_for_Object_Detection_Models_CVPR_2025_paper.html": {
    "title": "Test-Time Backdoor Detection for Object Detection Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hangtao Zhang",
      "Yichen Wang",
      "Shihui Yan",
      "Chenyu Zhu",
      "Ziqi Zhou",
      "Linshan Hou",
      "Shengshan Hu",
      "Minghui Li",
      "Yanjun Zhang",
      "Leo Yu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_Towards_Precise_Scaling_Laws_for_Video_Diffusion_Transformers_CVPR_2025_paper.html": {
    "title": "Towards Precise Scaling Laws for Video Diffusion Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanyang Yin",
      "Yaqi Zhao",
      "Mingwu Zheng",
      "Ke Lin",
      "Jiarong Ou",
      "Rui Chen",
      "Victor Shea-Jay Huang",
      "Jiahao Wang",
      "Xin Tao",
      "Pengfei Wan",
      "Di Zhang",
      "Baoqun Yin",
      "Wentao Zhang",
      "Kun Gai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_RoGSplat_Learning_Robust_Generalizable_Human_Gaussian_Splatting_from_Sparse_Multi-View_CVPR_2025_paper.html": {
    "title": "RoGSplat: Learning Robust Generalizable Human Gaussian Splatting from Sparse Multi-View Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjin Xiao",
      "Qing Zhang",
      "Yonewei Nie",
      "Lei Zhu",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bai_SDBF_Steep-Decision-Boundary_Fingerprinting_for_Hard-Label_Tampering_Detection_of_DNN_Models_CVPR_2025_paper.html": {
    "title": "SDBF: Steep-Decision-Boundary Fingerprinting for Hard-Label Tampering Detection of DNN Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofan Bai",
      "Shixin Li",
      "Xiaojing Ma",
      "Bin Benjamin Zhu",
      "Dongmei Zhang",
      "Linchen Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_EnliveningGS_Active_Locomotion_of_3DGS_CVPR_2025_paper.html": {
    "title": "EnliveningGS: Active Locomotion of 3DGS",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Shen",
      "Tianjia Shao",
      "Kun Zhou",
      "Chenfanfu Jiang",
      "Yin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cai_SPMTrack_Spatio-Temporal_Parameter-Efficient_Fine-Tuning_with_Mixture_of_Experts_for_Scalable_CVPR_2025_paper.html": {
    "title": "SPMTrack: Spatio-Temporal Parameter-Efficient Fine-Tuning with Mixture of Experts for Scalable Visual Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenrui Cai",
      "Qingjie Liu",
      "Yunhong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wimbauer_AnyCam_Learning_to_Recover_Camera_Poses_and_Intrinsics_from_Casual_CVPR_2025_paper.html": {
    "title": "AnyCam: Learning to Recover Camera Poses and Intrinsics from Casual Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Wimbauer",
      "Weirong Chen",
      "Dominik Muhle",
      "Christian Rupprecht",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_Knowledge-Aligned_Counterfactual-Enhancement_Diffusion_Perception_for_Unsupervised_Cross-Domain_Visual_Emotion_Recognition_CVPR_2025_paper.html": {
    "title": "Knowledge-Aligned Counterfactual-Enhancement Diffusion Perception for Unsupervised Cross-Domain Visual Emotion Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen Yin",
      "Yong Wang",
      "Guiduo Duan",
      "Dongyang Zhang",
      "Xin Hu",
      "Yuan-Fang Li",
      "Tao He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hegde_Distilling_Multi-modal_Large_Language_Models_for_Autonomous_Driving_CVPR_2025_paper.html": {
    "title": "Distilling Multi-modal Large Language Models for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Deepti Hegde",
      "Rajeev Yasarla",
      "Hong Cai",
      "Shizhong Han",
      "Apratim Bhattacharyya",
      "Shweta Mahajan",
      "Litian Liu",
      "Risheek Garrepalli",
      "Vishal M. Patel",
      "Fatih Porikli"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Pixel-aligned_RGB-NIR_Stereo_Imaging_and_Dataset_for_Robot_Vision_CVPR_2025_paper.html": {
    "title": "Pixel-aligned RGB-NIR Stereo Imaging and Dataset for Robot Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinnyeong Kim",
      "Seung-Hwan Baek"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Can_Machines_Understand_Composition_Dataset_and_Benchmark_for_Photographic_Image_CVPR_2025_paper.html": {
    "title": "Can Machines Understand Composition? Dataset and Benchmark for Photographic Image Composition Embedding and Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoran Zhao",
      "Peng Lu",
      "Anran Zhang",
      "Peipei Li",
      "Xia Li",
      "Xuannan Liu",
      "Yang Hu",
      "Shiyi Chen",
      "Liwei Wang",
      "Wenhao Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Stojnic_LPOSS_Label_Propagation_Over_Patches_and_Pixels_for_Open-vocabulary_Semantic_CVPR_2025_paper.html": {
    "title": "LPOSS: Label Propagation Over Patches and Pixels for Open-vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vladan Stojniƒá",
      "Yannis Kalantidis",
      "Ji≈ô√≠ Matas",
      "Giorgos Tolias"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Towards_Efficient_Foundation_Model_for_Zero-shot_Amodal_Segmentation_CVPR_2025_paper.html": {
    "title": "Towards Efficient Foundation Model for Zero-shot Amodal Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaochen Liu",
      "Limeng Qiao",
      "Xiangxiang Chu",
      "Lin Ma",
      "Tingting Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_PhysGen3D_Crafting_a_Miniature_Interactive_World_from_a_Single_Image_CVPR_2025_paper.html": {
    "title": "PhysGen3D: Crafting a Miniature Interactive World from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyuan Chen",
      "Hanxiao Jiang",
      "Shaowei Liu",
      "Saurabh Gupta",
      "Yunzhu Li",
      "Hao Zhao",
      "Shenlong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Duan_Docopilot_Improving_Multimodal_Models_for_Document-Level_Understanding_CVPR_2025_paper.html": {
    "title": "Docopilot: Improving Multimodal Models for Document-Level Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Duan",
      "Zhe Chen",
      "Yusong Hu",
      "Weiyun Wang",
      "Shenglong Ye",
      "Botian Shi",
      "Lewei Lu",
      "Qibin Hou",
      "Tong Lu",
      "Hongsheng Li",
      "Jifeng Dai",
      "Wenhai Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ravishankar_Scaling_Properties_of_Diffusion_Models_For_Perceptual_Tasks_CVPR_2025_paper.html": {
    "title": "Scaling Properties of Diffusion Models For Perceptual Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rahul Ravishankar",
      "Zeeshan Patel",
      "Jathushan Rajasegaran",
      "Jitendra Malik"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Perrett_HD-EPIC_A_Highly-Detailed_Egocentric_Video_Dataset_CVPR_2025_paper.html": {
    "title": "HD-EPIC: A Highly-Detailed Egocentric Video Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Toby Perrett",
      "Ahmad Darkhalil",
      "Saptarshi Sinha",
      "Omar Emara",
      "Sam Pollard",
      "Kranti Kumar Parida",
      "Kaiting Liu",
      "Prajwal Gatti",
      "Siddhant Bansal",
      "Kevin Flanagan",
      "Jacob Chalk",
      "Zhifan Zhu",
      "Rhodri Guerrier",
      "Fahd Abdelazim",
      "Bin Zhu",
      "Davide Moltisanti",
      "Michael Wray",
      "Hazel Doughty",
      "Dima Damen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Exact_Exploring_Space-Time_Perceptive_Clues_for_Weakly_Supervised_Satellite_Image_CVPR_2025_paper.html": {
    "title": "Exact: Exploring Space-Time Perceptive Clues for Weakly Supervised Satellite Image Time Series Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zhu",
      "Yan Zhu",
      "Jiayu Xiao",
      "Tianxiang Xiao",
      "Yike Ma",
      "Yucheng Zhang",
      "Feng Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Advancing_Myopia_To_Holism_Fully_Contrastive_Language-Image_Pre-training_CVPR_2025_paper.html": {
    "title": "Advancing Myopia To Holism: Fully Contrastive Language-Image Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haicheng Wang",
      "Chen Ju",
      "Weixiong Lin",
      "Shuai Xiao",
      "Mengting Chen",
      "Yixuan Huang",
      "Chang Liu",
      "Mingshuai Yao",
      "Jinsong Lan",
      "Ying Chen",
      "Qingwen Liu",
      "Yanfeng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yao_PolarFree_Polarization-based_Reflection-Free_Imaging_CVPR_2025_paper.html": {
    "title": "PolarFree: Polarization-based Reflection-Free Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingde Yao",
      "Menglu Wang",
      "King-Man Tam",
      "Lingen Li",
      "Tianfan Xue",
      "Jinwei Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_H-MoRe_Learning_Human-centric_Motion_Representation_for_Action_Analysis_CVPR_2025_paper.html": {
    "title": "H-MoRe: Learning Human-centric Motion Representation for Action Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanbo Huang",
      "Xiaoming Liu",
      "Yu Kong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kucuksozen_Hierarchical_Compact_Clustering_Attention_COCA_for_Unsupervised_Object-Centric_Learning_CVPR_2025_paper.html": {
    "title": "Hierarchical Compact Clustering Attention (COCA) for Unsupervised Object-Centric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Can Kucuksozen",
      "Yucel Yemez"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Effortless_Active_Labeling_for_Long-Term_Test-Time_Adaptation_CVPR_2025_paper.html": {
    "title": "Effortless Active Labeling for Long-Term Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guowei Wang",
      "Changxing Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_Leveraging_Temporal_Cues_for_Semi-Supervised_Multi-View_3D_Object_Detection_CVPR_2025_paper.html": {
    "title": "Leveraging Temporal Cues for Semi-Supervised Multi-View 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhyung Park",
      "Navyata Sanghvi",
      "Hiroki Adachi",
      "Yoshihisa Shibata",
      "Shawn Hunt",
      "Shinya Tanaka",
      "Hironobu Fujiyoshi",
      "Kris Kitani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_Self-supervised_ControlNet_with_Spatio-Temporal_Mamba_for_Real-world_Video_Super-resolution_CVPR_2025_paper.html": {
    "title": "Self-supervised ControlNet with Spatio-Temporal Mamba for Real-world Video Super-resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijun Shi",
      "Jing Xu",
      "Lijing Lu",
      "Zhihang Li",
      "Kai Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Etaat_LATTE-MV_Learning_to_Anticipate_Table_Tennis_Hits_from_Monocular_Videos_CVPR_2025_paper.html": {
    "title": "LATTE-MV: Learning to Anticipate Table Tennis Hits from Monocular Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Etaat",
      "Dvij Kalaria",
      "Nima Rahmanian",
      "S. Shankar Sastry"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Logits_DeConfusion_with_CLIP_for_Few-Shot_Learning_CVPR_2025_paper.html": {
    "title": "Logits DeConfusion with CLIP for Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Li",
      "Fang Liu",
      "Zehua Hao",
      "Xinyi Wang",
      "Lingling Li",
      "Xu Liu",
      "Puhua Chen",
      "Wenping Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Distilling_Spatially-Heterogeneous_Distortion_Perception_for_Blind_Image_Quality_Assessment_CVPR_2025_paper.html": {
    "title": "Distilling Spatially-Heterogeneous Distortion Perception for Blind Image Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xudong Li",
      "Wenjie Nie",
      "Yan Zhang",
      "Runze Hu",
      "Ke Li",
      "Xiawu Zheng",
      "Liujuan Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_Pay_Attention_to_the_Foreground_in_Object-Centric_Learning_CVPR_2025_paper.html": {
    "title": "Pay Attention to the Foreground in Object-Centric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pinzhuo Tian",
      "Shengjie Yang",
      "Hang Yu",
      "Alex Kot"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_2DMamba_Efficient_State_Space_Model_for_Image_Representation_with_Applications_CVPR_2025_paper.html": {
    "title": "2DMamba: Efficient State Space Model for Image Representation with Applications on Giga-Pixel Whole Slide Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingwei Zhang",
      "Anh Tien Nguyen",
      "Xi Han",
      "Vincent Quoc-Huy Trinh",
      "Hong Qin",
      "Dimitris Samaras",
      "Mahdi S. Hosseini"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Unboxed_Geometrically_and_Temporally_Consistent_Video_Outpainting_CVPR_2025_paper.html": {
    "title": "Unboxed: Geometrically and Temporally Consistent Video Outpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongrui Yu",
      "Martina Megaro-Boldini",
      "Robert W. Sumner",
      "Abdelaziz Djelouah"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_K-Sort_Arena_Efficient_and_Reliable_Benchmarking_for_Generative_Models_via_CVPR_2025_paper.html": {
    "title": "K-Sort Arena: Efficient and Reliable Benchmarking for Generative Models via K-wise Human Preferences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhikai Li",
      "Xuewen Liu",
      "Dongrong Joe Fu",
      "Jianquan Li",
      "Qingyi Gu",
      "Kurt Keutzer",
      "Zhen Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Seeking_Consistent_Flat_Minima_for_Better_Domain_Generalization_via_Refining_CVPR_2025_paper.html": {
    "title": "Seeking Consistent Flat Minima for Better Domain Generalization via Refining Loss Landscapes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aodi Li",
      "Liansheng Zhuang",
      "Xiao Long",
      "Minghong Yao",
      "Shafei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lincetto_MultimodalStudio_A_Heterogeneous_Sensor_Dataset_and_Framework_for_Neural_Rendering_CVPR_2025_paper.html": {
    "title": "MultimodalStudio: A Heterogeneous Sensor Dataset and Framework for Neural Rendering across Multiple Imaging Modalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Federico Lincetto",
      "Gianluca Agresti",
      "Mattia Rossi",
      "Pietro Zanuttigh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Dense-SfM_Structure_from_Motion_with_Dense_Consistent_Matching_CVPR_2025_paper.html": {
    "title": "Dense-SfM: Structure from Motion with Dense Consistent Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JongMin Lee",
      "Sungjoo Yoo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_FluidNexus_3D_Fluid_Reconstruction_and_Prediction_from_a_Single_Video_CVPR_2025_paper.html": {
    "title": "FluidNexus: 3D Fluid Reconstruction and Prediction from a Single Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Gao",
      "Hong-Xing Yu",
      "Bo Zhu",
      "Jiajun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_MuTri_Multi-view_Tri-alignment_for_OCT_to_OCTA_3D_Image_Translation_CVPR_2025_paper.html": {
    "title": "MuTri: Multi-view Tri-alignment for OCT to OCTA 3D Image Translation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuangzhuang Chen",
      "Hualiang Wang",
      "Chubin Ou",
      "Xiaomeng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_Sketchy_Bounding-box_Supervision_for_3D_Instance_Segmentation_CVPR_2025_paper.html": {
    "title": "Sketchy Bounding-box Supervision for 3D Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Deng",
      "Le Hui",
      "Jin Xie",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_Image_Quality_Assessment_Investigating_Causal_Perceptual_Effects_with_Abductive_Counterfactual_CVPR_2025_paper.html": {
    "title": "Image Quality Assessment: Investigating Causal Perceptual Effects with Abductive Counterfactual Inference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Shen",
      "Mingliang Zhou",
      "Yu Chen",
      "Xuekai Wei",
      "Yong Feng",
      "Huayan Pu",
      "Weijia Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_Pos3R_6D_Pose_Estimation_for_Unseen_Objects_Made_Easy_CVPR_2025_paper.html": {
    "title": "Pos3R: 6D Pose Estimation for Unseen Objects Made Easy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijian Deng",
      "Dylan Campbell",
      "Chunyi Sun",
      "Jiahao Zhang",
      "Shubham Kanitkar",
      "Matt E. Shaffer",
      "Stephen Gould"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_DeformCL_Learning_Deformable_Centerline_Representation_for_Vessel_Extraction_in_3D_CVPR_2025_paper.html": {
    "title": "DeformCL: Learning Deformable Centerline Representation for Vessel Extraction in 3D Medical Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziwei Zhao",
      "Zhixing Zhang",
      "Yuhang Liu",
      "Zhao Zhang",
      "Haojun Yu",
      "Dong Wang",
      "Liwei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_StreetCrafter_Street_View_Synthesis_with_Controllable_Video_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "StreetCrafter: Street View Synthesis with Controllable Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunzhi Yan",
      "Zhen Xu",
      "Haotong Lin",
      "Haian Jin",
      "Haoyu Guo",
      "Yida Wang",
      "Kun Zhan",
      "Xianpeng Lang",
      "Hujun Bao",
      "Xiaowei Zhou",
      "Sida Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_OCRT_Boosting_Foundation_Models_in_the_Open_World_with_Object-Concept-Relation_CVPR_2025_paper.html": {
    "title": "OCRT: Boosting Foundation Models in the Open World with Object-Concept-Relation Triad",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luyao Tang",
      "Yuxuan Yuan",
      "Chaoqi Chen",
      "Zeyu Zhang",
      "Yue Huang",
      "Kun Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_SPARS3R_Semantic_Prior_Alignment_and_Regularization_for_Sparse_3D_Reconstruction_CVPR_2025_paper.html": {
    "title": "SPARS3R: Semantic Prior Alignment and Regularization for Sparse 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutao Tang",
      "Yuxiang Guo",
      "Deming Li",
      "Cheng Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_VidBot_Learning_Generalizable_3D_Actions_from_In-the-Wild_2D_Human_Videos_CVPR_2025_paper.html": {
    "title": "VidBot: Learning Generalizable 3D Actions from In-the-Wild 2D Human Videos for Zero-Shot Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanzhi Chen",
      "Boyang Sun",
      "Anran Zhang",
      "Marc Pollefeys",
      "Stefan Leutenegger"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mao_Learning_Person-Specific_Animatable_Face_Models_from_In-the-Wild_Images_via_a_CVPR_2025_paper.html": {
    "title": "Learning Person-Specific Animatable Face Models from In-the-Wild Images via a Shared Base Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiang Mao",
      "Zhenfeng Fan",
      "ZhiJie Zhang",
      "Zhiheng Zhang",
      "Shihong Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_TIMotion_Temporal_and_Interactive_Framework_for_Efficient_Human-Human_Motion_Generation_CVPR_2025_paper.html": {
    "title": "TIMotion: Temporal and Interactive Framework for Efficient Human-Human Motion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yabiao Wang",
      "Shuo Wang",
      "Jiangning Zhang",
      "Ke Fan",
      "Jiafu Wu",
      "Zhucun Xue",
      "Yong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Majumder_Which_Viewpoint_Shows_it_Best_Language_for_Weakly_Supervising_View_CVPR_2025_paper.html": {
    "title": "Which Viewpoint Shows it Best? Language for Weakly Supervising View Selection in Multi-view Instructional Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sagnik Majumder",
      "Tushar Nagarajan",
      "Ziad Al-Halah",
      "Reina Pradhan",
      "Kristen Grauman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chu_RaCFormer_Towards_High-Quality_3D_Object_Detection_via_Query-based_Radar-Camera_Fusion_CVPR_2025_paper.html": {
    "title": "RaCFormer: Towards High-Quality 3D Object Detection via Query-based Radar-Camera Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaomeng Chu",
      "Jiajun Deng",
      "Guoliang You",
      "Yifan Duan",
      "Houqiang Li",
      "Yanyong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_Hybrid_Reciprocal_Transformer_with_Triplet_Feature_Alignment_for_Scene_Graph_CVPR_2025_paper.html": {
    "title": "Hybrid Reciprocal Transformer with Triplet Feature Alignment for Scene Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Fu",
      "Tiantian Zhang",
      "Kai Chen",
      "Qi Dou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_Understanding_Multi-Task_Activities_from_Single-Task_Videos_CVPR_2025_paper.html": {
    "title": "Understanding Multi-Task Activities from Single-Task Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Shen",
      "Ehsan Elhamifar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Co-Speech_Gesture_Video_Generation_with_Implicit_Motion-Audio_Entanglement_CVPR_2025_paper.html": {
    "title": "Co-Speech Gesture Video Generation with Implicit Motion-Audio Entanglement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinjie Li",
      "Ziyi Chen",
      "Xinlu Yu",
      "Iek-Heng Chu",
      "Peng Chang",
      "Jing Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_TransPixeler_Advancing_Text-to-Video_Generation_with_Transparency_CVPR_2025_paper.html": {
    "title": "TransPixeler: Advancing Text-to-Video Generation with Transparency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luozhou Wang",
      "Yijun Li",
      "Zhifei Chen",
      "Jui-Hsien Wang",
      "Zhifei Zhang",
      "He Zhang",
      "Zhe Lin",
      "Ying-Cong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Adaptive_Keyframe_Sampling_for_Long_Video_Understanding_CVPR_2025_paper.html": {
    "title": "Adaptive Keyframe Sampling for Long Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Tang",
      "Jihao Qiu",
      "Lingxi Xie",
      "Yunjie Tian",
      "Jianbin Jiao",
      "Qixiang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kaduri_Whats_in_the_Image_A_Deep-Dive_into_the_Vision_of_CVPR_2025_paper.html": {
    "title": "What's in the Image? A Deep-Dive into the Vision of Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omri Kaduri",
      "Shai Bagon",
      "Tali Dekel"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peng_Person_De-reidentification_A_Variation-guided_Identity_Shift_Modeling_CVPR_2025_paper.html": {
    "title": "Person De-reidentification: A Variation-guided Identity Shift Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Xing Peng",
      "Yu-Ming Tang",
      "Kun-Yu Lin",
      "Qize Yang",
      "Jingke Meng",
      "Xihan Wei",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_FreeSim_Toward_Free-viewpoint_Camera_Simulation_in_Driving_Scenes_CVPR_2025_paper.html": {
    "title": "FreeSim: Toward Free-viewpoint Camera Simulation in Driving Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lue Fan",
      "Hao Zhang",
      "Qitai Wang",
      "Hongsheng Li",
      "Zhaoxiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sami_Gradient_Inversion_Attacks_on_Parameter-Efficient_Fine-Tuning_CVPR_2025_paper.html": {
    "title": "Gradient Inversion Attacks on Parameter-Efficient Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hasin Us Sami",
      "Swapneel Sen",
      "Amit K. Roy-Chowdhury",
      "Srikanth V. Krishnamurthy",
      "Basak Guler"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_UPME_An_Unsupervised_Peer_Review_Framework_for_Multimodal_Large_Language_CVPR_2025_paper.html": {
    "title": "UPME: An Unsupervised Peer Review Framework for Multimodal Large Language Model Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihui Zhang",
      "Munan Ning",
      "Zheyuan Liu",
      "Yue Huang",
      "Shuo Yang",
      "Yanbo Wang",
      "Jiayi Ye",
      "Xiao Chen",
      "Yibing Song",
      "Li Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_DiGIT_Multi-Dilated_Gated_Encoder_and_Central-Adjacent_Region_Integrated_Decoder_for_CVPR_2025_paper.html": {
    "title": "DiGIT: Multi-Dilated Gated Encoder and Central-Adjacent Region Integrated Decoder for Temporal Action Detection Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ho-Joong Kim",
      "Yearang Lee",
      "Jung-Ho Hong",
      "Seong-Whan Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_MBQ_Modality-Balanced_Quantization_for_Large_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "MBQ: Modality-Balanced Quantization for Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shiyao Li",
      "Yingchun Hu",
      "Xuefei Ning",
      "Xihui Liu",
      "Ke Hong",
      "Xiaotao Jia",
      "Xiuhong Li",
      "Yaqi Yan",
      "Pei Ran",
      "Guohao Dai",
      "Shengen Yan",
      "Huazhong Yang",
      "Yu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Florence-VL_Enhancing_Vision-Language_Models_with_Generative_Vision_Encoder_and_Depth-Breadth_CVPR_2025_paper.html": {
    "title": "Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiuhai Chen",
      "Jianwei Yang",
      "Haiping Wu",
      "Dianqi Li",
      "Jianfeng Gao",
      "Tianyi Zhou",
      "Bin Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_VideoDPO_Omni-Preference_Alignment_for_Video_Diffusion_Generation_CVPR_2025_paper.html": {
    "title": "VideoDPO: Omni-Preference Alignment for Video Diffusion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runtao Liu",
      "Haoyu Wu",
      "Ziqiang Zheng",
      "Chen Wei",
      "Yingqing He",
      "Renjie Pi",
      "Qifeng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_Seq2Time_Sequential_Knowledge_Transfer_for_Video_LLM_Temporal_Grounding_CVPR_2025_paper.html": {
    "title": "Seq2Time: Sequential Knowledge Transfer for Video LLM Temporal Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andong Deng",
      "Zhongpai Gao",
      "Anwesa Choudhuri",
      "Benjamin Planche",
      "Meng Zheng",
      "Bin Wang",
      "Terrence Chen",
      "Chen Chen",
      "Ziyan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_GPVK-VL_Geometry-Preserving_Virtual_Keyframes_for_Visual_Localization_under_Large_Viewpoint_CVPR_2025_paper.html": {
    "title": "GPVK-VL: Geometry-Preserving Virtual Keyframes for Visual Localization under Large Viewpoint Changes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunxuan Li",
      "Lei Fan",
      "Xiaoying Xing",
      "Jianxiong Zhou",
      "Ying Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zanella_Realistic_Test-Time_Adaptation_of_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "Realistic Test-Time Adaptation of Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxime Zanella",
      "Cl√©ment Fuchs",
      "Christophe De Vleeschouwer",
      "Ismail Ben Ayed"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kang_SelfSplat_Pose-Free_and_3D_Prior-Free_Generalizable_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "SelfSplat: Pose-Free and 3D Prior-Free Generalizable 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gyeongjin Kang",
      "Jisang Yoo",
      "Jihyeon Park",
      "Seungtae Nam",
      "Hyeonsoo Im",
      "Sangheon Shin",
      "Sangpil Kim",
      "Eunbyung Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Enhancing_Virtual_Try-On_with_Synthetic_Pairs_and_Error-Aware_Noise_Scheduling_CVPR_2025_paper.html": {
    "title": "Enhancing Virtual Try-On with Synthetic Pairs and Error-Aware Noise Scheduling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nannan Li",
      "Kevin J. Shih",
      "Bryan A. Plummer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lai_Exploring_Simple_Open-Vocabulary_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Exploring Simple Open-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihang Lai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MP-GUI_Modality_Perception_with_MLLMs_for_GUI_Understanding_CVPR_2025_paper.html": {
    "title": "MP-GUI: Modality Perception with MLLMs for GUI Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziwei Wang",
      "Weizhi Chen",
      "Leyang Yang",
      "Sheng Zhou",
      "Shengchu Zhao",
      "Hanbei Zhan",
      "Jiongchao Jin",
      "Liangcheng Li",
      "Zirui Shao",
      "Jiajun Bu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Associative_Transformer_CVPR_2025_paper.html": {
    "title": "Associative Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwei Sun",
      "Hideya Ochiai",
      "Zhirong Wu",
      "Stephen Lin",
      "Ryota Kanai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_Improving_Adversarial_Transferability_on_Vision_Transformers_via_Forward_Propagation_Refinement_CVPR_2025_paper.html": {
    "title": "Improving Adversarial Transferability on Vision Transformers via Forward Propagation Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Ren",
      "Zhengyu Zhao",
      "Chenhao Lin",
      "Bo Yang",
      "Lu Zhou",
      "Zhe Liu",
      "Chao Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pei_Seeing_What_Matters_Empowering_CLIP_with_Patch_Generation-to-Selection_CVPR_2025_paper.html": {
    "title": "Seeing What Matters: Empowering CLIP with Patch Generation-to-Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gensheng Pei",
      "Tao Chen",
      "Yujia Wang",
      "Xinhao Cai",
      "Xiangbo Shu",
      "Tianfei Zhou",
      "Yazhou Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bian_ChatGarment_Garment_Estimation_Generation_and_Editing_via_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "ChatGarment: Garment Estimation, Generation and Editing via Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Bian",
      "Chenghao Xu",
      "Yuliang Xiu",
      "Artur Grigorev",
      "Zhen Liu",
      "Cewu Lu",
      "Michael J. Black",
      "Yao Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Enhancing_Online_Continual_Learning_with_Plug-and-Play_State_Space_Model_and_CVPR_2025_paper.html": {
    "title": "Enhancing Online Continual Learning with Plug-and-Play State Space Model and Class-Conditional Mixture of Discretization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sihao Liu",
      "Yibo Yang",
      "Xiaojie Li",
      "David A. Clifton",
      "Bernard Ghanem"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_RDD_Robust_Feature_Detector_and_Descriptor_using_Deformable_Transformer_CVPR_2025_paper.html": {
    "title": "RDD: Robust Feature Detector and Descriptor using Deformable Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gonglin Chen",
      "Tianwen Fu",
      "Haiwei Chen",
      "Wenbin Teng",
      "Hanyuan Xiao",
      "Yajie Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Building_Vision_Models_upon_Heat_Conduction_CVPR_2025_paper.html": {
    "title": "Building Vision Models upon Heat Conduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaozhi Wang",
      "Yue Liu",
      "Yunjie Tian",
      "Yunfan Liu",
      "Yaowei Wang",
      "Qixiang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_GRAPHGPT-O_Synergistic_Multimodal_Comprehension_and_Generation_on_Graphs_CVPR_2025_paper.html": {
    "title": "GRAPHGPT-O: Synergistic Multimodal Comprehension and Generation on Graphs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Fang",
      "Bowen Jin",
      "Jiacheng Shen",
      "Sirui Ding",
      "Qiaoyu Tan",
      "Jiawei Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Model_Poisoning_Attacks_to_Federated_Learning_via_Multi-Round_Consistency_CVPR_2025_paper.html": {
    "title": "Model Poisoning Attacks to Federated Learning via Multi-Round Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yueqi Xie",
      "Minghong Fang",
      "Neil Zhenqiang Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_TaoAvatar_Real-Time_Lifelike_Full-Body_Talking_Avatars_for_Augmented_Reality_via_CVPR_2025_paper.html": {
    "title": "TaoAvatar: Real-Time Lifelike Full-Body Talking Avatars for Augmented Reality via 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianchuan Chen",
      "Jingchuan Hu",
      "Gaige Wang",
      "Zhonghua Jiang",
      "Tiansong Zhou",
      "Zhiwen Chen",
      "Chengfei Lv"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Erasing_Undesirable_Influence_in_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Erasing Undesirable Influence in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Wu",
      "Trung Le",
      "Munawar Hayat",
      "Mehrtash Harandi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Meng_LT3SD_Latent_Trees_for_3D_Scene_Diffusion_CVPR_2025_paper.html": {
    "title": "LT3SD: Latent Trees for 3D Scene Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quan Meng",
      "Lei Li",
      "Matthias Nie√üner",
      "Angela Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_Stacking_Brick_by_Brick_Aligned_Feature_Isolation_for_Incremental_Face_CVPR_2025_paper.html": {
    "title": "Stacking Brick by Brick: Aligned Feature Isolation for Incremental Face Forgery Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jikang Cheng",
      "Zhiyuan Yan",
      "Ying Zhang",
      "Li Hao",
      "Jiaxin Ai",
      "Qin Zou",
      "Chen Li",
      "Zhongyuan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_CO-SPY_Combining_Semantic_and_Pixel_Features_to_Detect_Synthetic_Images_CVPR_2025_paper.html": {
    "title": "CO-SPY: Combining Semantic and Pixel Features to Detect Synthetic Images by AI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyuan Cheng",
      "Lingjuan Lyu",
      "Zhenting Wang",
      "Xiangyu Zhang",
      "Vikash Sehwag"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Meng_Closest_Neighbors_are_Harmful_for_Lightweight_Masked_Auto-encoders_CVPR_2025_paper.html": {
    "title": "Closest Neighbors are Harmful for Lightweight Masked Auto-encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Meng",
      "Ahmed Hasssan",
      "Li Yang",
      "Deliang Fan",
      "Jinwoo Shin",
      "Jae-sun Seo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_CraftsMan3D_High-fidelity_Mesh_Generation_with_3D_Native_Diffusion_and_Interactive_CVPR_2025_paper.html": {
    "title": "CraftsMan3D: High-fidelity Mesh Generation with 3D Native Diffusion and Interactive Geometry Refiner",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiyu Li",
      "Jiarui Liu",
      "Hongyu Yan",
      "Rui Chen",
      "Yixun Liang",
      "Xuelin Chen",
      "Ping Tan",
      "Xiaoxiao Long"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Decouple-Then-Merge_Finetune_Diffusion_Models_as_Multi-Task_Learning_CVPR_2025_paper.html": {
    "title": "Decouple-Then-Merge: Finetune Diffusion Models as Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianli Ma",
      "Xuefei Ning",
      "Dongrui Liu",
      "Li Niu",
      "Linfeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ebrahimi_GIF_Generative_Inspiration_for_Face_Recognition_at_Scale_CVPR_2025_paper.html": {
    "title": "GIF: Generative Inspiration for Face Recognition at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saeed Ebrahimi",
      "Sahar Rahimi",
      "Ali Dabouei",
      "Srinjoy Das",
      "Jeremy M. Dawson",
      "Nasser M. Nasrabadi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zayene_HELVIPAD_A_Real-World_Dataset_for_Omnidirectional_Stereo_Depth_Estimation_CVPR_2025_paper.html": {
    "title": "HELVIPAD: A Real-World Dataset for Omnidirectional Stereo Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mehdi Zayene",
      "Jannik Endres",
      "Albias Havolli",
      "Charles Corbi√®re",
      "Salim Cherkaoui",
      "Alexandre Kontouli",
      "Alexandre Alahi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_GENMANIP_LLM-driven_Simulation_for_Generalizable_Instruction-Following_Manipulation_CVPR_2025_paper.html": {
    "title": "GENMANIP: LLM-driven Simulation for Generalizable Instruction-Following Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ning Gao",
      "Yilun Chen",
      "Shuai Yang",
      "Xinyi Chen",
      "Yang Tian",
      "Hao Li",
      "Haifeng Huang",
      "Hanqing Wang",
      "Tai Wang",
      "Jiangmiao Pang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_SKDream_Controllable_Multi-view_and_3D_Generation_with_Arbitrary_Skeletons_CVPR_2025_paper.html": {
    "title": "SKDream: Controllable Multi-view and 3D Generation with Arbitrary Skeletons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanyou Xu",
      "Zongxin Yang",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Towards_Enhanced_Image_Inpainting_Mitigating_Unwanted_Object_Insertion_and_Preserving_CVPR_2025_paper.html": {
    "title": "Towards Enhanced Image Inpainting: Mitigating Unwanted Object Insertion and Preserving Color Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yikai Wang",
      "Chenjie Cao",
      "Junqiu Yu",
      "Ke Fan",
      "Xiangyang Xue",
      "Yanwei Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Optimus-2_Multimodal_Minecraft_Agent_with_Goal-Observation-Action_Conditioned_Policy_CVPR_2025_paper.html": {
    "title": "Optimus-2: Multimodal Minecraft Agent with Goal-Observation-Action Conditioned Policy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zaijing Li",
      "Yuquan Xie",
      "Rui Shao",
      "Gongwei Chen",
      "Dongmei Jiang",
      "Liqiang Nie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jin_Classic_Video_Denoising_in_a_Machine_Learning_World_Robust_Fast_CVPR_2025_paper.html": {
    "title": "Classic Video Denoising in a Machine Learning World: Robust, Fast, and Controllable",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Jin",
      "Simon Niklaus",
      "Zhoutong Zhang",
      "Zhihao Xia",
      "Chunle Guo",
      "Yuting Yang",
      "Jiawen Chen",
      "Chongyi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tzamos_Practical_Solutions_to_the_Relative_Pose_of_Three_Calibrated_Cameras_CVPR_2025_paper.html": {
    "title": "Practical Solutions to the Relative Pose of Three Calibrated Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Charalambos Tzamos",
      "Viktor Kocur",
      "Yaqing Ding",
      "Daniel Barath",
      "Zuzana Berger Haladova",
      "Torsten Sattler",
      "Zuzana Kukelova"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Localized_Concept_Erasure_for_Text-to-Image_Diffusion_Models_Using_Training-Free_Gated_CVPR_2025_paper.html": {
    "title": "Localized Concept Erasure for Text-to-Image Diffusion Models Using Training-Free Gated Low-Rank Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byung Hyun Lee",
      "Sungjin Lim",
      "Se Young Chun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Schmalfuss_PARC_A_Quantitative_Framework_Uncovering_the_Symmetries_within_Vision_Language_CVPR_2025_paper.html": {
    "title": "PARC: A Quantitative Framework Uncovering the Symmetries within Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jenny Schmalfuss",
      "Nadine Chang",
      "Vibashan VS",
      "Maying Shen",
      "Andres Bruhn",
      "Jose M. Alvarez"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mu_RoboTwin_Dual-Arm_Robot_Benchmark_with_Generative_Digital_Twins_CVPR_2025_paper.html": {
    "title": "RoboTwin: Dual-Arm Robot Benchmark with Generative Digital Twins",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yao Mu",
      "Tianxing Chen",
      "Zanxin Chen",
      "Shijia Peng",
      "Zhiqian Lan",
      "Zeyu Gao",
      "Zhixuan Liang",
      "Qiaojun Yu",
      "Yude Zou",
      "Mingkun Xu",
      "Lunkai Lin",
      "Zhiqiang Xie",
      "Mingyu Ding",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Population_Normalization_for_Federated_Learning_CVPR_2025_paper.html": {
    "title": "Population Normalization for Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoyao Wang",
      "Fan Yi",
      "Peizhu Gong",
      "Caitou He",
      "Cheng Jin",
      "Weizhong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lei_AnimateAnything_Consistent_and_Controllable_Animation_for_Video_Generation_CVPR_2025_paper.html": {
    "title": "AnimateAnything: Consistent and Controllable Animation for Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guojun Lei",
      "Chi Wang",
      "Rong Zhang",
      "Yikai Wang",
      "Hong Li",
      "Weiwei Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sinitsyn_PRaDA_Projective_Radial_Distortion_Averaging_CVPR_2025_paper.html": {
    "title": "PRaDA: Projective Radial Distortion Averaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniil Sinitsyn",
      "Linus H√§renstam-Nielsen",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_GenAssets_Generating_in-the-wild_3D_Assets_in_Latent_Space_CVPR_2025_paper.html": {
    "title": "GenAssets: Generating in-the-wild 3D Assets in Latent Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ze Yang",
      "Jingkang Wang",
      "Haowei Zhang",
      "Sivabalan Manivasagam",
      "Yun Chen",
      "Raquel Urtasun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dumitriu_RipVIS_Rip_Currents_Video_Instance_Segmentation_Benchmark_for_Beach_Monitoring_CVPR_2025_paper.html": {
    "title": "RipVIS: Rip Currents Video Instance Segmentation Benchmark for Beach Monitoring and Safety",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrei Dumitriu",
      "Florin Tatui",
      "Florin Miron",
      "Aakash Ralhan",
      "Radu Tudor Ionescu",
      "Radu Timofte"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ta_Low-Rank_Adaptation_in_Multilinear_Operator_Networks_for_Security-Preserving_Incremental_Learning_CVPR_2025_paper.html": {
    "title": "Low-Rank Adaptation in Multilinear Operator Networks for Security-Preserving Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huu Binh Ta",
      "Duc Nguyen",
      "Quyen Tran",
      "Toan Tran",
      "Tung Pham"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dibene_Camera_Resection_from_Known_Line_Pencils_and_a_Radially_Distorted_CVPR_2025_paper.html": {
    "title": "Camera Resection from Known Line Pencils and a Radially Distorted Scanline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juan C. Dibene",
      "Enrique Dunn"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bekci_ESCAPE_Equivariant_Shape_Completion_via_Anchor_Point_Encoding_CVPR_2025_paper.html": {
    "title": "ESCAPE: Equivariant Shape Completion via Anchor Point Encoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Burak Bekci",
      "Nassir Navab",
      "Federico Tombari",
      "Mahdi Saleh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liao_SPC-GS_Gaussian_Splatting_with_Semantic-Prompt_Consistency_for_Indoor_Open-World_Free-view_CVPR_2025_paper.html": {
    "title": "SPC-GS: Gaussian Splatting with Semantic-Prompt Consistency for Indoor Open-World Free-view Synthesis from Sparse Inputs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guibiao Liao",
      "Qing Li",
      "Zhenyu Bao",
      "Guoping Qiu",
      "Kanglin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_M3amba_Memory_Mamba_is_All_You_Need_for_Whole_Slide_CVPR_2025_paper.html": {
    "title": "M3amba: Memory Mamba is All You Need for Whole Slide Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tingting Zheng",
      "Kui Jiang",
      "Yi Xiao",
      "Sicheng Zhao",
      "Hongxun Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Satellite_to_GroundScape_-_Large-scale_Consistent_Ground_View_Generation_from_CVPR_2025_paper.html": {
    "title": "Satellite to GroundScape - Large-scale Consistent Ground View Generation from Satellite Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ningli Xu",
      "Rongjun Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Samira_Variance-Based_Membership_Inference_Attacks_Against_Large-Scale_Image_Captioning_Models_CVPR_2025_paper.html": {
    "title": "Variance-Based Membership Inference Attacks Against Large-Scale Image Captioning Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Samira",
      "Edan Habler",
      "Yuval Elovici",
      "Asaf Shabtai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_Redefining_Creative_in_Dictionary_Towards_an_Enhanced_Semantic_Understanding_of_CVPR_2025_paper.html": {
    "title": "Redefining <Creative> in Dictionary: Towards an Enhanced Semantic Understanding of Creative Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fu Feng",
      "Yucheng Xie",
      "Xu Yang",
      "Jing Wang",
      "Xin Geng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Terris_FiRe_Fixed-points_of_Restoration_Priors_for_Solving_Inverse_Problems_CVPR_2025_paper.html": {
    "title": "FiRe: Fixed-points of Restoration Priors for Solving Inverse Problems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthieu Terris",
      "Ulugbek S. Kamilov",
      "Thomas Moreau"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Learning_Dynamic_Collaborative_Network_for_Semi-supervised_3D_Vessel_Segmentation_CVPR_2025_paper.html": {
    "title": "Learning Dynamic Collaborative Network for Semi-supervised 3D Vessel Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiao Xu",
      "Xin Chen",
      "Lihe Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Temporal_Alignment-Free_Video_Matching_for_Few-shot_Action_Recognition_CVPR_2025_paper.html": {
    "title": "Temporal Alignment-Free Video Matching for Few-shot Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SuBeen Lee",
      "WonJun Moon",
      "Hyun Seok Seong",
      "Jae-Pil Heo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/C_OSLoPrompt_Bridging_Low-Supervision_Challenges_and_Open-Set_Domain_Generalization_in_CLIP_CVPR_2025_paper.html": {
    "title": "OSLoPrompt: Bridging Low-Supervision Challenges and Open-Set Domain Generalization in CLIP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamad Hassan N C",
      "Divyam Gupta",
      "Mainak Singha",
      "Sai Bhargav Rongali",
      "Ankit Jha",
      "Muhammad Haris Khan",
      "Biplab Banerjee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Dinomaly_The_Less_Is_More_Philosophy_in_Multi-Class_Unsupervised_Anomaly_CVPR_2025_paper.html": {
    "title": "Dinomaly: The Less Is More Philosophy in Multi-Class Unsupervised Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jia Guo",
      "Shuai Lu",
      "Weihang Zhang",
      "Fang Chen",
      "Huiqi Li",
      "Hongen Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_VLog_Video-Language_Models_by_Generative_Retrieval_of_Narration_Vocabulary_CVPR_2025_paper.html": {
    "title": "VLog: Video-Language Models by Generative Retrieval of Narration Vocabulary",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Qinghong Lin",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_CoMBO_Conflict_Mitigation_via_Branched_Optimization_for_Class_Incremental_Segmentation_CVPR_2025_paper.html": {
    "title": "CoMBO: Conflict Mitigation via Branched Optimization for Class Incremental Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Fang",
      "Anqi Zhang",
      "Guangyu Gao",
      "Jianbo Jiao",
      "Chi Harold Liu",
      "Yunchao Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Forensics-Bench_A_Comprehensive_Forgery_Detection_Benchmark_Suite_for_Large_Vision_CVPR_2025_paper.html": {
    "title": "Forensics-Bench: A Comprehensive Forgery Detection Benchmark Suite for Large Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Wang",
      "Chenghui Lv",
      "Xian Li",
      "Shichao Dong",
      "Huadong Li",
      "Kelu Yao",
      "Chao Li",
      "Wenqi Shao",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Detect-and-Guide_Self-regulation_of_Diffusion_Models_for_Safe_Text-to-Image_Generation_via_CVPR_2025_paper.html": {
    "title": "Detect-and-Guide: Self-regulation of Diffusion Models for Safe Text-to-Image Generation via Guideline Token Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feifei Li",
      "Mi Zhang",
      "Yiming Sun",
      "Min Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dhiman_MirrorVerse_Pushing_Diffusion_Models_to_Realistically_Reflect_the_World_CVPR_2025_paper.html": {
    "title": "MirrorVerse: Pushing Diffusion Models to Realistically Reflect the World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ankit Dhiman",
      "Manan Shah",
      "R Venkatesh Babu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dai_EAP-GS_Efficient_Augmentation_of_Pointcloud_for_3D_Gaussian_Splatting_in_CVPR_2025_paper.html": {
    "title": "EAP-GS: Efficient Augmentation of Pointcloud for 3D Gaussian Splatting in Few-shot Scene Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongrui Dai",
      "Yuxiang Xing"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yuan_Empowering_Large_Language_Models_with_3D_Situation_Awareness_CVPR_2025_paper.html": {
    "title": "Empowering Large Language Models with 3D Situation Awareness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Yuan",
      "Yibo Peng",
      "Jinke Ren",
      "Yinghong Liao",
      "Yatong Han",
      "Chun-Mei Feng",
      "Hengshuang Zhao",
      "Guanbin Li",
      "Shuguang Cui",
      "Zhen Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nguyen_Forensic_Self-Descriptions_Are_All_You_Need_for_Zero-Shot_Detection_Open-Set_CVPR_2025_paper.html": {
    "title": "Forensic Self-Descriptions Are All You Need for Zero-Shot Detection, Open-Set Source Attribution, and Clustering of AI-generated Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tai D. Nguyen",
      "Aref Azizpour",
      "Matthew C. Stamm"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xing_EchoTraffic_Enhancing_Traffic_Anomaly_Understanding_with_Audio-Visual_Insights_CVPR_2025_paper.html": {
    "title": "EchoTraffic: Enhancing Traffic Anomaly Understanding with Audio-Visual Insights",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenghao Xing",
      "Hao Chen",
      "Binzhu Xie",
      "Jiaqi Xu",
      "Ziyu Guo",
      "Xuemiao Xu",
      "Jianye Hao",
      "Chi-Wing Fu",
      "Xiaowei Hu",
      "Pheng-Ann Heng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_FlexDrive_Toward_Trajectory_Flexibility_in_Driving_Scene_Gaussian_Splatting_Reconstruction_CVPR_2025_paper.html": {
    "title": "FlexDrive: Toward Trajectory Flexibility in Driving Scene Gaussian Splatting Reconstruction and Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingqiu Zhou",
      "Lue Fan",
      "Linjiang Huang",
      "Xiaoyu Shi",
      "Si Liu",
      "Zhaoxiang Zhang",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhong_Taming_Video_Diffusion_Prior_with_Scene-Grounding_Guidance_for_3D_Gaussian_CVPR_2025_paper.html": {
    "title": "Taming Video Diffusion Prior with Scene-Grounding Guidance for 3D Gaussian Splatting from Sparse Inputs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingji Zhong",
      "Zhihao Li",
      "Dave Zhenyu Chen",
      "Lanqing Hong",
      "Dan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_Interactive_Medical_Image_Segmentation_A_Benchmark_Dataset_and_Baseline_CVPR_2025_paper.html": {
    "title": "Interactive Medical Image Segmentation: A Benchmark Dataset and Baseline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junlong Cheng",
      "Bin Fu",
      "Jin Ye",
      "Guoan Wang",
      "Tianbin Li",
      "Haoyu Wang",
      "Ruoyu Li",
      "He Yao",
      "Junren Cheng",
      "Jingwen Li",
      "Yanzhou Su",
      "Min Zhu",
      "Junjun He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_GigaHands_A_Massive_Annotated_Dataset_of_Bimanual_Hand_Activities_CVPR_2025_paper.html": {
    "title": "GigaHands: A Massive Annotated Dataset of Bimanual Hand Activities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rao Fu",
      "Dingxi Zhang",
      "Alex Jiang",
      "Wanjia Fu",
      "Austin Funk",
      "Daniel Ritchie",
      "Srinath Sridhar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lian_AutoSSVH_Exploring_Automated_Frame_Sampling_for_Efficient_Self-Supervised_Video_Hashing_CVPR_2025_paper.html": {
    "title": "AutoSSVH: Exploring Automated Frame Sampling for Efficient Self-Supervised Video Hashing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Niu Lian",
      "Jun Li",
      "Jinpeng Wang",
      "Ruisheng Luo",
      "Yaowei Wang",
      "Shu-Tao Xia",
      "Bin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cocchi_Augmenting_Multimodal_LLMs_with_Self-Reflective_Tokens_for_Knowledge-based_Visual_Question_CVPR_2025_paper.html": {
    "title": "Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Federico Cocchi",
      "Nicholas Moratelli",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_DifIISR_A_Diffusion_Model_with_Gradient_Guidance_for_Infrared_Image_CVPR_2025_paper.html": {
    "title": "DifIISR: A Diffusion Model with Gradient Guidance for Infrared Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyuan Li",
      "Zirui Wang",
      "Yang Zou",
      "Zhixin Chen",
      "Jun Ma",
      "Zhiying Jiang",
      "Long Ma",
      "Jinyuan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Recurrent_Feature_Mining_and_Keypoint_Mixup_Padding_for_Category-Agnostic_Pose_CVPR_2025_paper.html": {
    "title": "Recurrent Feature Mining and Keypoint Mixup Padding for Category-Agnostic Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Chen",
      "Weilong Chen",
      "Yifan Zuo",
      "Yuming Fang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_FrugalNeRF_Fast_Convergence_for_Extreme_Few-shot_Novel_View_Synthesis_without_CVPR_2025_paper.html": {
    "title": "FrugalNeRF: Fast Convergence for Extreme Few-shot Novel View Synthesis without Learned Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chin-Yang Lin",
      "Chung-Ho Wu",
      "Chang-Han Yeh",
      "Shih-Han Yen",
      "Cheng Sun",
      "Yu-Lun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jang_3D-GSW_3D_Gaussian_Splatting_for_Robust_Watermarking_CVPR_2025_paper.html": {
    "title": "3D-GSW: 3D Gaussian Splatting for Robust Watermarking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youngdong Jang",
      "Hyunje Park",
      "Feng Yang",
      "Heeju Ko",
      "Euijin Choo",
      "Sangpil Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Pioneering_4-Bit_FP_Quantization_for_Diffusion_Models_Mixup-Sign_Quantization_and_CVPR_2025_paper.html": {
    "title": "Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maosen Zhao",
      "Pengtao Chen",
      "Chong Yu",
      "Yan Wen",
      "Xudong Tan",
      "Tao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_OpenING_A_Comprehensive_Benchmark_for_Judging_Open-ended_Interleaved_Image-Text_Generation_CVPR_2025_paper.html": {
    "title": "OpenING: A Comprehensive Benchmark for Judging Open-ended Interleaved Image-Text Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengfei Zhou",
      "Xiaopeng Peng",
      "Jiajun Song",
      "Chuanhao Li",
      "Zhaopan Xu",
      "Yue Yang",
      "Ziyao Guo",
      "Hao Zhang",
      "Yuqi Lin",
      "Yefei He",
      "Lirui Zhao",
      "Shuo Liu",
      "Tianhua Li",
      "Yuxuan Xie",
      "Xiaojun Chang",
      "Yu Qiao",
      "Wenqi Shao",
      "Kaipeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Choi_Dual_Exposure_Stereo_for_Extended_Dynamic_Range_3D_Imaging_CVPR_2025_paper.html": {
    "title": "Dual Exposure Stereo for Extended Dynamic Range 3D Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juhyung Choi",
      "Jinnyeong Kim",
      "Seokjun Choi",
      "Jinwoo Lee",
      "Samuel Brucker",
      "Mario Bijelic",
      "Felix Heide",
      "Seung-Hwan Baek"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shvetsova_Unbiasing_through_Textual_Descriptions_Mitigating_Representation_Bias_in_Video_Benchmarks_CVPR_2025_paper.html": {
    "title": "Unbiasing through Textual Descriptions: Mitigating Representation Bias in Video Benchmarks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nina Shvetsova",
      "Arsha Nagrani",
      "Bernt Schiele",
      "Hilde Kuehne",
      "Christian Rupprecht"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Embodied_Scene_Understanding_for_Vision_Language_Models_via_MetaVQA_CVPR_2025_paper.html": {
    "title": "Embodied Scene Understanding for Vision Language Models via MetaVQA",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weizhen Wang",
      "Chenda Duan",
      "Zhenghao Peng",
      "Yuxin Liu",
      "Bolei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ge_CompGS_Unleashing_2D_Compositionality_for_Compositional_Text-to-3D_via_Dynamically_Optimizing_CVPR_2025_paper.html": {
    "title": "CompGS: Unleashing 2D Compositionality for Compositional Text-to-3D via Dynamically Optimizing 3D Gaussians",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chongjian Ge",
      "Chenfeng Xu",
      "Yuanfeng Ji",
      "Chensheng Peng",
      "Masayoshi Tomizuka",
      "Ping Luo",
      "Mingyu Ding",
      "Varun Jampani",
      "Wei Zhan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shao_Learning_Temporally_Consistent_Video_Depth_from_Video_Diffusion_Priors_CVPR_2025_paper.html": {
    "title": "Learning Temporally Consistent Video Depth from Video Diffusion Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Shao",
      "Yuanbo Yang",
      "Hongyu Zhou",
      "Youmin Zhang",
      "Yujun Shen",
      "Vitor Guizilini",
      "Yue Wang",
      "Matteo Poggi",
      "Yiyi Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chu_FIRE_Robust_Detection_of_Diffusion-Generated_Images_via_Frequency-Guided_Reconstruction_Error_CVPR_2025_paper.html": {
    "title": "FIRE: Robust Detection of Diffusion-Generated Images via Frequency-Guided Reconstruction Error",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beilin Chu",
      "Xuan Xu",
      "Xin Wang",
      "Yufei Zhang",
      "Weike You",
      "Linna Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Assessing_and_Learning_Alignment_of_Unimodal_Vision_and_Language_Models_CVPR_2025_paper.html": {
    "title": "Assessing and Learning Alignment of Unimodal Vision and Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Le Zhang",
      "Qian Yang",
      "Aishwarya Agrawal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_Samba_A_Unified_Mamba-based_Framework_for_General_Salient_Object_Detection_CVPR_2025_paper.html": {
    "title": "Samba: A Unified Mamba-based Framework for General Salient Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao He",
      "Keren Fu",
      "Xiaohong Liu",
      "Qijun Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Action_Detail_Matters_Refining_Video_Recognition_with_Local_Action_Queries_CVPR_2025_paper.html": {
    "title": "Action Detail Matters: Refining Video Recognition with Local Action Queries",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengmeng Wang",
      "Zeyi Huang",
      "Xiangjie Kong",
      "Guojiang Shen",
      "Guang Dai",
      "Jingdong Wang",
      "Yong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_PAVE_Patching_and_Adapting_Video_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "PAVE: Patching and Adapting Video Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoming Liu",
      "Yiquan Li",
      "Khoi Duc Nguyen",
      "Yiwu Zhong",
      "Yin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rokuss_LesionLocator_Zero-Shot_Universal_Tumor_Segmentation_and_Tracking_in_3D_Whole-Body_CVPR_2025_paper.html": {
    "title": "LesionLocator: Zero-Shot Universal Tumor Segmentation and Tracking in 3D Whole-Body Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maximilian Rokuss",
      "Yannick Kirchhoff",
      "Seval Akbal",
      "Balint Kovacs",
      "Saikat Roy",
      "Constantin Ulrich",
      "Tassilo Wald",
      "Lukas T. Rotkopf",
      "Heinz-Peter Schlemmer",
      "Klaus Maier-Hein"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_Generative_Map_Priors_for_Collaborative_BEV_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Generative Map Priors for Collaborative BEV Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahui Fu",
      "Yue Gong",
      "Luting Wang",
      "Shifeng Zhang",
      "Xu Zhou",
      "Si Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Coherent_3D_Portrait_Video_Reconstruction_via_Triplane_Fusion_CVPR_2025_paper.html": {
    "title": "Coherent 3D Portrait Video Reconstruction via Triplane Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengze Wang",
      "Xueting Li",
      "Chao Liu",
      "Matthew Chan",
      "Michael Stengel",
      "Henry Fuchs",
      "Shalini De Mello",
      "Koki Nagano"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Generative_Image_Layer_Decomposition_with_Visual_Effects_CVPR_2025_paper.html": {
    "title": "Generative Image Layer Decomposition with Visual Effects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinrui Yang",
      "Qing Liu",
      "Yijun Li",
      "Soo Ye Kim",
      "Daniil Pakhomov",
      "Mengwei Ren",
      "Jianming Zhang",
      "Zhe Lin",
      "Cihang Xie",
      "Yuyin Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_AR-Diffusion_Asynchronous_Video_Generation_with_Auto-Regressive_Diffusion_CVPR_2025_paper.html": {
    "title": "AR-Diffusion: Asynchronous Video Generation with Auto-Regressive Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingzhen Sun",
      "Weining Wang",
      "Gen Li",
      "Jiawei Liu",
      "Jiahui Sun",
      "Wanquan Feng",
      "Shanshan Lao",
      "Siyu Zhou",
      "Qian He",
      "Jing Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pang_ManiVideo_Generating_Hand-Object_Manipulation_Video_with_Dexterous_and_Generalizable_Grasping_CVPR_2025_paper.html": {
    "title": "ManiVideo: Generating Hand-Object Manipulation Video with Dexterous and Generalizable Grasping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youxin Pang",
      "Ruizhi Shao",
      "Jiajun Zhang",
      "Hanzhang Tu",
      "Yun Liu",
      "Boyao Zhou",
      "Hongwen Zhang",
      "Yebin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DOF-GS_Adjustable_Depth-of-Field_3D_Gaussian_Splatting_for_Post-Capture_Refocusing_Defocus_CVPR_2025_paper.html": {
    "title": "DOF-GS: Adjustable Depth-of-Field 3D Gaussian Splatting for Post-Capture Refocusing, Defocus Rendering and Blur Removal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Wang",
      "Praneeth Chakravarthula",
      "Baoquan Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qi_The_Photographers_Eye_Teaching_Multimodal_Large_Language_Models_to_See_CVPR_2025_paper.html": {
    "title": "The Photographer's Eye: Teaching Multimodal Large Language Models to See, and Critique Like Photographers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daiqing Qi",
      "Handong Zhao",
      "Jing Shi",
      "Simon Jenni",
      "Yifei Fan",
      "Franck Dernoncourt",
      "Scott Cohen",
      "Sheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Revisiting_Audio-Visual_Segmentation_with_Vision-Centric_Transformer_CVPR_2025_paper.html": {
    "title": "Revisiting Audio-Visual Segmentation with Vision-Centric Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaofei Huang",
      "Rui Ling",
      "Tianrui Hui",
      "Hongyu Li",
      "Xu Zhou",
      "Shifeng Zhang",
      "Si Liu",
      "Richang Hong",
      "Meng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Synergizing_Motion_and_Appearance_Multi-Scale_Compensatory_Codebooks_for_Talking_Head_CVPR_2025_paper.html": {
    "title": "Synergizing Motion and Appearance: Multi-Scale Compensatory Codebooks for Talking Head Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuling Zhao",
      "Fa-Ting Hong",
      "Xiaoshui Huang",
      "Dan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_HOIGPT_Learning_Long-Sequence_Hand-Object_Interaction_with_Language_Models_CVPR_2025_paper.html": {
    "title": "HOIGPT: Learning Long-Sequence Hand-Object Interaction with Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingzhen Huang",
      "Fu-Jen Chu",
      "Bugra Tekin",
      "Kevin J. Liang",
      "Haoyu Ma",
      "Weiyao Wang",
      "Xingyu Chen",
      "Pierre Gleize",
      "Hongfei Xue",
      "Siwei Lyu",
      "Kris Kitani",
      "Matt Feiszli",
      "Hao Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bie_GraphI2P_Image-to-Point_Cloud_Registration_with_Exploring_Pattern_of_Correspondence_via_CVPR_2025_paper.html": {
    "title": "GraphI2P: Image-to-Point Cloud Registration with Exploring Pattern of Correspondence via Graph Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lin Bie",
      "Shouan Pan",
      "Siqi Li",
      "Yining Zhao",
      "Yue Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_SoftVQ-VAE_Efficient_1-Dimensional_Continuous_Tokenizer_CVPR_2025_paper.html": {
    "title": "SoftVQ-VAE: Efficient 1-Dimensional Continuous Tokenizer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Chen",
      "Ze Wang",
      "Xiang Li",
      "Ximeng Sun",
      "Fangyi Chen",
      "Jiang Liu",
      "Jindong Wang",
      "Bhiksha Raj",
      "Zicheng Liu",
      "Emad Barsoum"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hao_FedCS_Coreset_Selection_for_Federated_Learning_CVPR_2025_paper.html": {
    "title": "FedCS: Coreset Selection for Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhe Hao",
      "Weiying Xie",
      "Daixun Li",
      "Haonan Qin",
      "Hangyu Ye",
      "Leyuan Fang",
      "Yunsong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_DPC_Dual-Prompt_Collaboration_for_Tuning_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "DPC: Dual-Prompt Collaboration for Tuning Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyang Li",
      "Liang Wang",
      "Chao Wang",
      "Jing Jiang",
      "Yan Peng",
      "Guodong Long"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xing_Dual-Granularity_Semantic_Guided_Sparse_Routing_Diffusion_Model_for_General_Pansharpening_CVPR_2025_paper.html": {
    "title": "Dual-Granularity Semantic Guided Sparse Routing Diffusion Model for General Pansharpening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinghui Xing",
      "Litao Qu",
      "Shizhou Zhang",
      "Di Xu",
      "Yingkun Yang",
      "Yanning Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_AIM-Fair_Advancing_Algorithmic_Fairness_via_Selectively_Fine-Tuning_Biased_Models_with_CVPR_2025_paper.html": {
    "title": "AIM-Fair: Advancing Algorithmic Fairness via Selectively Fine-Tuning Biased Models with Contextual Synthetic Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zengqun Zhao",
      "Ziquan Liu",
      "Yu Cao",
      "Shaogang Gong",
      "Ioannis Patras"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chu_Robust_Multi-Object_4D_Generation_for_In-the-wild_Videos_CVPR_2025_paper.html": {
    "title": "Robust Multi-Object 4D Generation for In-the-wild Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen-Hsuan Chu",
      "Lei Ke",
      "Jianmeng Liu",
      "Mingxiao Huo",
      "Pavel Tokmakov",
      "Katerina Fragkiadaki"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_OmniMMI_A_Comprehensive_Multi-modal_Interaction_Benchmark_in_Streaming_Video_Contexts_CVPR_2025_paper.html": {
    "title": "OmniMMI: A Comprehensive Multi-modal Interaction Benchmark in Streaming Video Contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Wang",
      "Yueqian Wang",
      "Bo Chen",
      "Tong Wu",
      "Dongyan Zhao",
      "Zilong Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_SOAP_Vision-Centric_3D_Semantic_Scene_Completion_with_Scene-Adaptive_Decoder_and_CVPR_2025_paper.html": {
    "title": "SOAP: Vision-Centric 3D Semantic Scene Completion with Scene-Adaptive Decoder and Occluded Region-Aware View Projection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyo-Jun Lee",
      "Yeong Jun Koh",
      "Hanul Kim",
      "Hyunseop Kim",
      "Yonguk Lee",
      "Jinu Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Snaebjarnarson_Taxonomy-Aware_Evaluation_of_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "Taxonomy-Aware Evaluation of Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "V√©steinn Sn√¶bjarnarson",
      "Kevin Du",
      "Niklas Stoehr",
      "Serge Belongie",
      "Ryan Cotterell",
      "Nico Lang",
      "Stella Frank"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Active_Event-based_Stereo_Vision_CVPR_2025_paper.html": {
    "title": "Active Event-based Stereo Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianing Li",
      "Yunjian Zhang",
      "Haiqian Han",
      "Xiangyang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_Mono-InternVL_Pushing_the_Boundaries_of_Monolithic_Multimodal_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gen Luo",
      "Xue Yang",
      "Wenhan Dou",
      "Zhaokai Wang",
      "Jiawen Liu",
      "Jifeng Dai",
      "Yu Qiao",
      "Xizhou Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Trevithick_SimVS_Simulating_World_Inconsistencies_for_Robust_View_Synthesis_CVPR_2025_paper.html": {
    "title": "SimVS: Simulating World Inconsistencies for Robust View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Trevithick",
      "Roni Paiss",
      "Philipp Henzler",
      "Dor Verbin",
      "Rundi Wu",
      "Hadi Alzayer",
      "Ruiqi Gao",
      "Ben Poole",
      "Jonathan T. Barron",
      "Aleksander Holynski",
      "Ravi Ramamoorthi",
      "Pratul P. Srinivasan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_FLAVC_Learned_Video_Compression_with_Feature_Level_Attention_CVPR_2025_paper.html": {
    "title": "FLAVC: Learned Video Compression with Feature Level Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun Zhang",
      "Heming Sun",
      "Jiro Katto"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qu_An_End-to-End_Robust_Point_Cloud_Semantic_Segmentation_Network_with_Single-Step_CVPR_2025_paper.html": {
    "title": "An End-to-End Robust Point Cloud Semantic Segmentation Network with Single-Step Conditional Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wentao Qu",
      "Jing Wang",
      "YongShun Gong",
      "Xiaoshui Huang",
      "Liang Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_From_Zero_to_Detail_Deconstructing_Ultra-High-Definition_Image_Restoration_from_Progressive_CVPR_2025_paper.html": {
    "title": "From Zero to Detail: Deconstructing Ultra-High-Definition Image Restoration from Progressive Spectral Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Zhao",
      "Zhizhou Chen",
      "Yunzhe Xu",
      "Enxuan Gu",
      "Jian Li",
      "Zili Yi",
      "Qian Wang",
      "Jian Yang",
      "Ying Tai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Thoker_SMILE_Infusing_Spatial_and_Motion_Semantics_in_Masked_Video_Learning_CVPR_2025_paper.html": {
    "title": "SMILE: Infusing Spatial and Motion Semantics in Masked Video Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fida Mohammad Thoker",
      "Letian Jiang",
      "Chen Zhao",
      "Bernard Ghanem"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Video_Language_Model_Pretraining_with_Spatio-temporal_Masking_CVPR_2025_paper.html": {
    "title": "Video Language Model Pretraining with Spatio-temporal Masking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Wu",
      "Zhaobo Qi",
      "Junshu Sun",
      "Yaowei Wang",
      "Qingming Huang",
      "Shuhui Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_COSMOS_Cross-Modality_Self-Distillation_for_Vision_Language_Pre-training_CVPR_2025_paper.html": {
    "title": "COSMOS: Cross-Modality Self-Distillation for Vision Language Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanghwan Kim",
      "Rui Xiao",
      "Mariana-Iuliana Georgescu",
      "Stephan Alaniz",
      "Zeynep Akata"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Lifting_Motion_to_the_3D_World_via_2D_Diffusion_CVPR_2025_paper.html": {
    "title": "Lifting Motion to the 3D World via 2D Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaman Li",
      "C. Karen Liu",
      "Jiajun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_TAPT_Test-Time_Adversarial_Prompt_Tuning_for_Robust_Inference_in_Vision-Language_CVPR_2025_paper.html": {
    "title": "TAPT: Test-Time Adversarial Prompt Tuning for Robust Inference in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Wang",
      "Kai Chen",
      "Jiaming Zhang",
      "Jingjing Chen",
      "Xingjun Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Udandarao_Active_Data_Curation_Effectively_Distills_Large-Scale_Multimodal_Models_CVPR_2025_paper.html": {
    "title": "Active Data Curation Effectively Distills Large-Scale Multimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vishaal Udandarao",
      "Nikhil Parthasarathy",
      "Muhammad Ferjad Naeem",
      "Talfan Evans",
      "Samuel Albanie",
      "Federico Tombari",
      "Yongqin Xian",
      "Alessio Tonioni",
      "Olivier J. Henaff"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wei_PCDreamer_Point_Cloud_Completion_Through_Multi-view_Diffusion_Priors_CVPR_2025_paper.html": {
    "title": "PCDreamer: Point Cloud Completion Through Multi-view Diffusion Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangshun Wei",
      "Yuan Feng",
      "Long Ma",
      "Chen Wang",
      "Yuanfeng Zhou",
      "Changjian Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kerssies_Your_ViT_is_Secretly_an_Image_Segmentation_Model_CVPR_2025_paper.html": {
    "title": "Your ViT is Secretly an Image Segmentation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tommie Kerssies",
      "Niccol√≤ Cavagnero",
      "Alexander Hermans",
      "Narges Norouzi",
      "Giuseppe Averta",
      "Bastian Leibe",
      "Gijs Dubbelman",
      "Daan de Geus"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mao_Cross-Rejective_Open-Set_SAR_Image_Registration_CVPR_2025_paper.html": {
    "title": "Cross-Rejective Open-Set SAR Image Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shasha Mao",
      "Shiming Lu",
      "Zhaolong Du",
      "Licheng Jiao",
      "Shuiping Gou",
      "Luntian Mou",
      "Xuequan Lu",
      "Lin Xiong",
      "Yimeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Synthetic_Data_is_an_Elegant_GIFT_for_Continual_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "Synthetic Data is an Elegant GIFT for Continual Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Wu",
      "Wuxuan Shi",
      "Jinqiao Wang",
      "Mang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_SplineGS_Robust_Motion-Adaptive_Spline_for_Real-Time_Dynamic_3D_Gaussians_from_CVPR_2025_paper.html": {
    "title": "SplineGS: Robust Motion-Adaptive Spline for Real-Time Dynamic 3D Gaussians from Monocular Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jongmin Park",
      "Minh-Quan Viet Bui",
      "Juan Luis Gonzalez Bello",
      "Jaeho Moon",
      "Jihyong Oh",
      "Munchurl Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shang_SCSA_A_Plug-and-Play_Semantic_Continuous-Sparse_Attention_for_Arbitrary_Semantic_Style_CVPR_2025_paper.html": {
    "title": "SCSA: A Plug-and-Play Semantic Continuous-Sparse Attention for Arbitrary Semantic Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunnan Shang",
      "Zhizhong Wang",
      "Hongwei Wang",
      "Xiangming Meng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Timestep_Embedding_Tells_Its_Time_to_Cache_for_Video_Diffusion_CVPR_2025_paper.html": {
    "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Liu",
      "Shiwei Zhang",
      "Xiaofeng Wang",
      "Yujie Wei",
      "Haonan Qiu",
      "Yuzhong Zhao",
      "Yingya Zhang",
      "Qixiang Ye",
      "Fang Wan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Cant_Slow_Me_Down_Learning_Robust_and_Hardware-Adaptive_Object_Detectors_CVPR_2025_paper.html": {
    "title": "Can't Slow Me Down: Learning Robust and Hardware-Adaptive Object Detectors against Latency Attacks for Edge Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Wang",
      "Zichen Wang",
      "Cong Wang",
      "Yuanchao Shu",
      "Ruilong Deng",
      "Peng Cheng",
      "Jiming Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jeong_Multi-modal_Knowledge_Distillation-based_Human_Trajectory_Forecasting_CVPR_2025_paper.html": {
    "title": "Multi-modal Knowledge Distillation-based Human Trajectory Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaewoo Jeong",
      "Seohee Lee",
      "Daehee Park",
      "Giwon Lee",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_SAM2Object_Consolidating_View_Consistency_via_SAM2_for_Zero-Shot_3D_Instance_CVPR_2025_paper.html": {
    "title": "SAM2Object: Consolidating View Consistency via SAM2 for Zero-Shot 3D Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihuai Zhao",
      "Junbao Zhuo",
      "Jiansheng Chen",
      "Huimin Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dubinski_CDI_Copyrighted_Data_Identification_in_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "CDI: Copyrighted Data Identification in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jan Dubi≈Ñski",
      "Antoni Kowalczuk",
      "Franziska Boenisch",
      "Adam Dziedzic"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fixelle_Hypergraph_Vision_Transformers_Images_are_More_than_Nodes_More_than_CVPR_2025_paper.html": {
    "title": "Hypergraph Vision Transformers: Images are More than Nodes, More than Edges",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joshua Fixelle"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hou_Binarized_Neural_Network_for_Multi-spectral_Image_Fusion_CVPR_2025_paper.html": {
    "title": "Binarized Neural Network for Multi-spectral Image Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junming Hou",
      "Xiaoyu Chen",
      "Ran Ran",
      "Xiaofeng Cong",
      "Xinyang Liu",
      "Jian Wei You",
      "Liang-Jian Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_CRISP_Object_Pose_and_Shape_Estimation_with_Test-Time_Adaptation_CVPR_2025_paper.html": {
    "title": "CRISP: Object Pose and Shape Estimation with Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingnan Shi",
      "Rajat Talak",
      "Harry Zhang",
      "David Jin",
      "Luca Carlone"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_ShiftwiseConv_Small_Convolutional_Kernel_with_Large_Kernel_Effect_CVPR_2025_paper.html": {
    "title": "ShiftwiseConv: Small Convolutional Kernel with Large Kernel Effect",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dachong Li",
      "Li Li",
      "Zhuangzhuang Chen",
      "Jianqiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_GaussianIP_Identity-Preserving_Realistic_3D_Human_Generation_via_Human-Centric_Diffusion_Prior_CVPR_2025_paper.html": {
    "title": "GaussianIP: Identity-Preserving Realistic 3D Human Generation via Human-Centric Diffusion Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichen Tang",
      "Yuan Yao",
      "Miaomiao Cui",
      "Liefeng Bo",
      "Hongyu Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Creating_Your_Editable_3D_Photorealistic_Avatar_with_Tetrahedron-constrained_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanxi Liu",
      "Yifang Men",
      "Zhouhui Lian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Duan_FineVQ_Fine-Grained_User_Generated_Content_Video_Quality_Assessment_CVPR_2025_paper.html": {
    "title": "FineVQ: Fine-Grained User Generated Content Video Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiyu Duan",
      "Qiang Hu",
      "Jiarui Wang",
      "Liu Yang",
      "Zitong Xu",
      "Lu Liu",
      "Xiongkuo Min",
      "Chunlei Cai",
      "Tianxiao Ye",
      "Xiaoyun Zhang",
      "Guangtao Zhai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Unveiling_the_Ignorance_of_MLLMs_Seeing_Clearly_Answering_Incorrectly_CVPR_2025_paper.html": {
    "title": "Unveiling the Ignorance of MLLMs: Seeing Clearly, Answering Incorrectly",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yexin Liu",
      "Zhengyang Liang",
      "Yueze Wang",
      "Xianfeng Wu",
      "Feilong Tang",
      "Muyang He",
      "Jian Li",
      "Zheng Liu",
      "Harry Yang",
      "Sernam Lim",
      "Bo Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_Object-Shot_Enhanced_Grounding_Network_for_Egocentric_Video_CVPR_2025_paper.html": {
    "title": "Object-Shot Enhanced Grounding Network for Egocentric Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yisen Feng",
      "Haoyu Zhang",
      "Meng Liu",
      "Weili Guan",
      "Liqiang Nie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zatsarynna_MANTA_Diffusion_Mamba_for_Efficient_and_Effective_Stochastic_Long-Term_Dense_CVPR_2025_paper.html": {
    "title": "MANTA: Diffusion Mamba for Efficient and Effective Stochastic Long-Term Dense Action Anticipation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Olga Zatsarynna",
      "Emad Bahrami",
      "Yazan Abu Farha",
      "Gianpiero Francesca",
      "Juergen Gall"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_METASCENES_Towards_Automated_Replica_Creation_for_Real-world_3D_Scans_CVPR_2025_paper.html": {
    "title": "METASCENES: Towards Automated Replica Creation for Real-world 3D Scans",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huangyue Yu",
      "Baoxiong Jia",
      "Yixin Chen",
      "Yandan Yang",
      "Puhao Li",
      "Rongpeng Su",
      "Jiaxin Li",
      "Qing Li",
      "Wei Liang",
      "Song-Chun Zhu",
      "Tengyu Liu",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Robust_Multimodal_Survival_Prediction_with_Conditional_Latent_Differentiation_Variational_AutoEncoder_CVPR_2025_paper.html": {
    "title": "Robust Multimodal Survival Prediction with Conditional Latent Differentiation Variational AutoEncoder",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Zhou",
      "Jiao Tang",
      "Yingli Zuo",
      "Peng Wan",
      "Daoqiang Zhang",
      "Wei Shao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rahimi_Sim-to-Real_Causal_Transfer_A_Metric_Learning_Approach_to_Causally-Aware_Interaction_CVPR_2025_paper.html": {
    "title": "Sim-to-Real Causal Transfer: A Metric Learning Approach to Causally-Aware Interaction Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahmad Rahimi",
      "Po-Chien Luan",
      "Yuejiang Liu",
      "Frano Rajiƒç",
      "Alexandre Alahi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cho_Ev-3DOD_Pushing_the_Temporal_Boundaries_of_3D_Object_Detection_with_CVPR_2025_paper.html": {
    "title": "Ev-3DOD: Pushing the Temporal Boundaries of 3D Object Detection with Event Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoonhee Cho",
      "Jae-Young Kang",
      "Youngho Kim",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Quan_Zero-Shot_Blind-spot_Image_Denoising_via_Implicit_Neural_Sampling_CVPR_2025_paper.html": {
    "title": "Zero-Shot Blind-spot Image Denoising via Implicit Neural Sampling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhui Quan",
      "Tianxiang Zheng",
      "Zhiyuan Ma",
      "Hui Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ahn_Nearly_Zero-Cost_Protection_Against_Mimicry_by_Personalized_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Nearly Zero-Cost Protection Against Mimicry by Personalized Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Namhyuk Ahn",
      "KiYoon Yoo",
      "Wonhyuk Ahn",
      "Daesik Kim",
      "Seung-Hun Nam"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Tripartite_Weight-Space_Ensemble_for_Few-Shot_Class-Incremental_Learning_CVPR_2025_paper.html": {
    "title": "Tripartite Weight-Space Ensemble for Few-Shot Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juntae Lee",
      "Munawar Hayat",
      "Sungrack Yun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gong_The_Devil_is_in_Temporal_Token_High_Quality_Video_Reasoning_CVPR_2025_paper.html": {
    "title": "The Devil is in Temporal Token: High Quality Video Reasoning Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sitong Gong",
      "Yunzhi Zhuge",
      "Lu Zhang",
      "Zongxin Yang",
      "Pingping Zhang",
      "Huchuan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mei_PerLA_Perceptive_3D_Language_Assistant_CVPR_2025_paper.html": {
    "title": "PerLA: Perceptive 3D Language Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guofeng Mei",
      "Wei Lin",
      "Luigi Riz",
      "Yujiao Wu",
      "Fabio Poiesi",
      "Yiming Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_LITA-GS_Illumination-Agnostic_Novel_View_Synthesis_via_Reference-Free_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "LITA-GS: Illumination-Agnostic Novel View Synthesis via Reference-Free 3D Gaussian Splatting and Physical Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Zhou",
      "Wei Dong",
      "Jun Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xue_PhyT2V_LLM-Guided_Iterative_Self-Refinement_for_Physics-Grounded_Text-to-Video_Generation_CVPR_2025_paper.html": {
    "title": "PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyao Xue",
      "Xiangyu Yin",
      "Boyuan Yang",
      "Wei Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jeong_Track4Gen_Teaching_Video_Diffusion_Models_to_Track_Points_Improves_Video_CVPR_2025_paper.html": {
    "title": "Track4Gen: Teaching Video Diffusion Models to Track Points Improves Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeonho Jeong",
      "Chun-Hao P. Huang",
      "Jong Chul Ye",
      "Niloy J. Mitra",
      "Duygu Ceylan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qi_Mask2DiT_Dual_Mask-based_Diffusion_Transformer_for_Multi-Scene_Long_Video_Generation_CVPR_2025_paper.html": {
    "title": "Mask^2DiT: Dual Mask-based Diffusion Transformer for Multi-Scene Long Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhao Qi",
      "Jianlong Yuan",
      "Wanquan Feng",
      "Shancheng Fang",
      "Jiawei Liu",
      "SiYu Zhou",
      "Qian He",
      "Hongtao Xie",
      "Yongdong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_JamMa_Ultra-lightweight_Local_Feature_Matching_with_Joint_Mamba_CVPR_2025_paper.html": {
    "title": "JamMa: Ultra-lightweight Local Feature Matching with Joint Mamba",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyong Lu",
      "Songlin Du"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tao_DyCoke_Dynamic_Compression_of_Tokens_for_Fast_Video_Large_Language_CVPR_2025_paper.html": {
    "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keda Tao",
      "Can Qin",
      "Haoxuan You",
      "Yang Sui",
      "Huan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Flotho_T-FAKE_Synthesizing_Thermal_Images_for_Facial_Landmarking_CVPR_2025_paper.html": {
    "title": "T-FAKE: Synthesizing Thermal Images for Facial Landmarking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philipp Flotho",
      "Moritz Piening",
      "Anna Kukleva",
      "Gabriele Steidl"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Albastaki_Multi-Resolution_Pathology-Language_Pre-training_Model_with_Text-Guided_Visual_Representation_CVPR_2025_paper.html": {
    "title": "Multi-Resolution Pathology-Language Pre-training Model with Text-Guided Visual Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shahad Albastaki",
      "Anabia Sohail",
      "Iyyakutti Iyappan Ganapathi",
      "Basit Alawode",
      "Asim Khan",
      "Sajid Javed",
      "Naoufel Werghi",
      "Mohammed Bennamoun",
      "Arif Mahmood"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_InteractAnything_Zero-shot_Human_Object_Interaction_Synthesis_via_LLM_Feedback_and_CVPR_2025_paper.html": {
    "title": "InteractAnything: Zero-shot Human Object Interaction Synthesis via LLM Feedback and Object Affordance Parsing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinlu Zhang",
      "Yixin Chen",
      "Zan Wang",
      "Jie Yang",
      "Yizhou Wang",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gabeff_MammAlps_A_Multi-view_Video_Behavior_Monitoring_Dataset_of_Wild_Mammals_CVPR_2025_paper.html": {
    "title": "MammAlps: A Multi-view Video Behavior Monitoring Dataset of Wild Mammals in the Swiss Alps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Valentin Gabeff",
      "Haozhe Qi",
      "Brendan Flaherty",
      "Gencer Sumbul",
      "Alexander Mathis",
      "Devis Tuia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Diffusion-based_Realistic_Listening_Head_Generation_via_Hybrid_Motion_Modeling_CVPR_2025_paper.html": {
    "title": "Diffusion-based Realistic Listening Head Generation via Hybrid Motion Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinuo Wang",
      "Yanbo Fan",
      "Xuan Wang",
      "Guo Yu",
      "Fei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Su_SAT-HMR_Real-Time_Multi-Person_3D_Mesh_Estimation_via_Scale-Adaptive_Tokens_CVPR_2025_paper.html": {
    "title": "SAT-HMR: Real-Time Multi-Person 3D Mesh Estimation via Scale-Adaptive Tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi Su",
      "Xiaoxuan Ma",
      "Jiajun Su",
      "Yizhou Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_PICD_Versatile_Perceptual_Image_Compression_with_Diffusion_Rendering_CVPR_2025_paper.html": {
    "title": "PICD: Versatile Perceptual Image Compression with Diffusion Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongda Xu",
      "Jiahao Li",
      "Bin Li",
      "Yan Wang",
      "Ya-Qin Zhang",
      "Yan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_UniScene_Unified_Occupancy-centric_Driving_Scene_Generation_CVPR_2025_paper.html": {
    "title": "UniScene: Unified Occupancy-centric Driving Scene Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bohan Li",
      "Jiazhe Guo",
      "Hongsi Liu",
      "Yingshuang Zou",
      "Yikang Ding",
      "Xiwu Chen",
      "Hu Zhu",
      "Feiyang Tan",
      "Chi Zhang",
      "Tiancai Wang",
      "Shuchang Zhou",
      "Li Zhang",
      "Xiaojuan Qi",
      "Hao Zhao",
      "Mu Yang",
      "Wenjun Zeng",
      "Xin Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Wonderland_Navigating_3D_Scenes_from_a_Single_Image_CVPR_2025_paper.html": {
    "title": "Wonderland: Navigating 3D Scenes from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanwen Liang",
      "Junli Cao",
      "Vidit Goel",
      "Guocheng Qian",
      "Sergei Korolev",
      "Demetri Terzopoulos",
      "Konstantinos N. Plataniotis",
      "Sergey Tulyakov",
      "Jian Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_Learning_from_Streaming_Video_with_Orthogonal_Gradients_CVPR_2025_paper.html": {
    "title": "Learning from Streaming Video with Orthogonal Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tengda Han",
      "Dilara Gokay",
      "Joseph Heyward",
      "Chuhan Zhang",
      "Daniel Zoran",
      "Viorica Patraucean",
      "Joao Carreira",
      "Dima Damen",
      "Andrew Zisserman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_Towards_Satellite_Image_Road_Graph_Extraction_A_Global-Scale_Dataset_and_CVPR_2025_paper.html": {
    "title": "Towards Satellite Image Road Graph Extraction: A Global-Scale Dataset and A Novel Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pan Yin",
      "Kaiyu Li",
      "Xiangyong Cao",
      "Jing Yao",
      "Lei Liu",
      "Xueru Bai",
      "Feng Zhou",
      "Deyu Meng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_SuperLightNet_Lightweight_Parameter_Aggregation_Network_for_Multimodal_Brain_Tumor_Segmentation_CVPR_2025_paper.html": {
    "title": "SuperLightNet: Lightweight Parameter Aggregation Network for Multimodal Brain Tumor Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Yu",
      "Jiacheng Cao",
      "Li Liu",
      "Minghua Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gonzalez_VideoSPatS_Video_SPatiotemporal_Splines_for_Disentangled_Occlusion_Appearance_and_Motion_CVPR_2025_paper.html": {
    "title": "VideoSPatS: Video SPatiotemporal Splines for Disentangled Occlusion, Appearance and Motion Modeling and Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juan Luis Gonzalez",
      "Xu Yao",
      "Alex Whelan",
      "Kyle Olszewski",
      "Hyeongwoo Kim",
      "Pablo Garrido"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guimard_Classifier-to-Bias_Toward_Unsupervised_Automatic_Bias_Detection_for_Visual_Classifiers_CVPR_2025_paper.html": {
    "title": "Classifier-to-Bias: Toward Unsupervised Automatic Bias Detection for Visual Classifiers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quentin Guimard",
      "Moreno D'Inc√†",
      "Massimiliano Mancini",
      "Elisa Ricci"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shrivastava_Self-Supervised_Spatial_Correspondence_Across_Modalities_CVPR_2025_paper.html": {
    "title": "Self-Supervised Spatial Correspondence Across Modalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ayush Shrivastava",
      "Andrew Owens"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_MOS-Attack_A_Scalable_Multi-objective_Adversarial_Attack_Framework_CVPR_2025_paper.html": {
    "title": "MOS-Attack: A Scalable Multi-objective Adversarial Attack Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ping Guo",
      "Cheng Gong",
      "Xi Lin",
      "Fei Liu",
      "Zhichao Lu",
      "Qingfu Zhang",
      "Zhenkun Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_Six-CD_Benchmarking_Concept_Removals_for_Text-to-image_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Six-CD: Benchmarking Concept Removals for Text-to-image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Ren",
      "Kangrui Chen",
      "Yingqian Cui",
      "Shenglai Zeng",
      "Hui Liu",
      "Yue Xing",
      "Jiliang Tang",
      "Lingjuan Lyu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pandey_Motion_Modes_What_Could_Happen_Next_CVPR_2025_paper.html": {
    "title": "Motion Modes: What Could Happen Next?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karran Pandey",
      "Yannick Hold-Geoffroy",
      "Matheus Gadelha",
      "Niloy J. Mitra",
      "Karan Singh",
      "Paul Guerrero"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Finer-CAM_Spotting_the_Difference_Reveals_Finer_Details_for_Visual_Explanation_CVPR_2025_paper.html": {
    "title": "Finer-CAM: Spotting the Difference Reveals Finer Details for Visual Explanation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziheng Zhang",
      "Jianyang Gu",
      "Arpita Chowdhury",
      "Zheda Mai",
      "David Carlyn",
      "Tanya Berger-Wolf",
      "Yu Su",
      "Wei-Lun Chao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Benidir_The_Change_You_Want_To_Detect_Semantic_Change_Detection_In_CVPR_2025_paper.html": {
    "title": "The Change You Want To Detect: Semantic Change Detection In Earth Observation With Hybrid Data Generationf",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanis Benidir",
      "Nicolas Gonthier",
      "Clement Mallet"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Muller_Black-Box_Forgery_Attacks_on_Semantic_Watermarks_for_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Black-Box Forgery Attacks on Semantic Watermarks for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andreas M√ºller",
      "Denis Lukovnikov",
      "Jonas Thietke",
      "Asja Fischer",
      "Erwin Quiring"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hui_An_Image-like_Diffusion_Method_for_Human-Object_Interaction_Detection_CVPR_2025_paper.html": {
    "title": "An Image-like Diffusion Method for Human-Object Interaction Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofei Hui",
      "Haoxuan Qu",
      "Hossein Rahmani",
      "Jun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VidSeg_Training-free_Video_Semantic_Segmentation_based_on_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "VidSeg: Training-free Video Semantic Segmentation based on Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Wang",
      "Abdelrahman Eldesokey",
      "Mohit Mendiratta",
      "Fangneng Zhan",
      "Adam Kortylewski",
      "Christian Theobalt",
      "Peter Wonka"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_COB-GS_Clear_Object_Boundaries_in_3DGS_Segmentation_Based_on_Boundary-Adaptive_CVPR_2025_paper.html": {
    "title": "COB-GS: Clear Object Boundaries in 3DGS Segmentation Based on Boundary-Adaptive Gaussian Splitting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Zhang",
      "Junjun Jiang",
      "Youyu Chen",
      "Kui Jiang",
      "Xianming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Weakly_Supervised_Semantic_Segmentation_via_Progressive_Confidence_Region_Expansion_CVPR_2025_paper.html": {
    "title": "Weakly Supervised Semantic Segmentation via Progressive Confidence Region Expansion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangfeng Xu",
      "Pinyi Zhang",
      "Wenxuan Huang",
      "Yunhang Shen",
      "Haosheng Chen",
      "Jingzhong Lin",
      "Wei Li",
      "Gaoqi He",
      "Jiao Xie",
      "Shaohui Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Khosla_RELOCATE_A_Simple_Training-Free_Baseline_for_Visual_Query_Localization_Using_CVPR_2025_paper.html": {
    "title": "RELOCATE: A Simple Training-Free Baseline for Visual Query Localization Using Region-Based Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Savya Khosla",
      "Sethuraman T V",
      "Alexander Schwing",
      "Derek Hoiem"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cho_PEER_Pressure_Model-to-Model_Regularization_for_Single_Source_Domain_Generalization_CVPR_2025_paper.html": {
    "title": "PEER Pressure: Model-to-Model Regularization for Single Source Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Kyu Cho",
      "Inwoo Hwang",
      "Sanghack Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Griffiths_HOTFormerLoc_Hierarchical_Octree_Transformer_for_Versatile_Lidar_Place_Recognition_Across_CVPR_2025_paper.html": {
    "title": "HOTFormerLoc: Hierarchical Octree Transformer for Versatile Lidar Place Recognition Across Ground and Aerial Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ethan Griffiths",
      "Maryam Haghighat",
      "Simon Denman",
      "Clinton Fookes",
      "Milad Ramezani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qin_Revisiting_Fairness_in_Multitask_Learning_A_Performance-Driven_Approach_for_Variance_CVPR_2025_paper.html": {
    "title": "Revisiting Fairness in Multitask Learning: A Performance-Driven Approach for Variance Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohan Qin",
      "Xiaoxing Wang",
      "Junchi Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Piccinelli_UniK3D_Universal_Camera_Monocular_3D_Estimation_CVPR_2025_paper.html": {
    "title": "UniK3D: Universal Camera Monocular 3D Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luigi Piccinelli",
      "Christos Sakaridis",
      "Mattia Segu",
      "Yung-Hsu Yang",
      "Siyuan Li",
      "Wim Abbeloos",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_ConMo_Controllable_Motion_Disentanglement_and_Recomposition_for_Zero-Shot_Motion_Transfer_CVPR_2025_paper.html": {
    "title": "ConMo: Controllable Motion Disentanglement and Recomposition for Zero-Shot Motion Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Gao",
      "Zijin Yin",
      "Changcheng Hua",
      "Yuxin Peng",
      "Kongming Liang",
      "Zhanyu Ma",
      "Jun Guo",
      "Yang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_VideoMage_Multi-Subject_and_Motion_Customization_of_Text-to-Video_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "VideoMage: Multi-Subject and Motion Customization of Text-to-Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chi-Pin Huang",
      "Yen-Siang Wu",
      "Hung-Kai Chung",
      "Kai-Po Chang",
      "Fu-En Yang",
      "Yu-Chiang Frank Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nguyen_AG-VPReID_A_Challenging_Large-Scale_Benchmark_for_Aerial-Ground_Video-based_Person_Re-Identification_CVPR_2025_paper.html": {
    "title": "AG-VPReID: A Challenging Large-Scale Benchmark for Aerial-Ground Video-based Person Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huy Nguyen",
      "Kien Nguyen",
      "Akila Pemasiri",
      "Feng Liu",
      "Sridha Sridharan",
      "Clinton Fookes"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Reed_EBS-EKF_Accurate_and_High_Frequency_Event-based_Star_Tracking_CVPR_2025_paper.html": {
    "title": "EBS-EKF: Accurate and High Frequency Event-based Star Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Albert W. Reed",
      "Connor Hashemi",
      "Dennis Melamed",
      "Nitesh Menon",
      "Keigo Hirakawa",
      "Scott McCloskey"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_PersonaBooth_Personalized_Text-to-Motion_Generation_CVPR_2025_paper.html": {
    "title": "PersonaBooth: Personalized Text-to-Motion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boeun Kim",
      "Hea In Jeong",
      "JungHoon Sung",
      "Yihua Cheng",
      "Jeongmin Lee",
      "Ju Yong Chang",
      "Sang-Il Choi",
      "Younggeun Choi",
      "Saim Shin",
      "Jungho Kim",
      "Hyung Jin Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Al-Emadi_Benchmarking_Object_Detectors_under_Real-World_Distribution_Shifts_in_Satellite_Imagery_CVPR_2025_paper.html": {
    "title": "Benchmarking Object Detectors under Real-World Distribution Shifts in Satellite Imagery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sara A. Al-Emadi",
      "Yin Yang",
      "Ferda Ofli"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_SAIST_Segment_Any_Infrared_Small_Target_Model_Guided_by_Contrastive_CVPR_2025_paper.html": {
    "title": "SAIST: Segment Any Infrared Small Target Model Guided by Contrastive Language-Image Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingjin Zhang",
      "Xiaolong Li",
      "Fei Gao",
      "Jie Guo",
      "Xinbo Gao",
      "Jing Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peng_Star_with_Bilinear_Mapping_CVPR_2025_paper.html": {
    "title": "Star with Bilinear Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zelin Peng",
      "Yu Huang",
      "Zhengqin Xu",
      "Feilong Tang",
      "Ming Hu",
      "Xiaokang Yang",
      "Wei Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Closed-Loop_Supervised_Fine-Tuning_of_Tokenized_Traffic_Models_CVPR_2025_paper.html": {
    "title": "Closed-Loop Supervised Fine-Tuning of Tokenized Traffic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhejun Zhang",
      "Peter Karkus",
      "Maximilian Igl",
      "Wenhao Ding",
      "Yuxiao Chen",
      "Boris Ivanovic",
      "Marco Pavone"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_DIFIX3D_Improving_3D_Reconstructions_with_Single-Step_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "DIFIX3D+: Improving 3D Reconstructions with Single-Step Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jay Zhangjie Wu",
      "Yuxuan Zhang",
      "Haithem Turki",
      "Xuanchi Ren",
      "Jun Gao",
      "Mike Zheng Shou",
      "Sanja Fidler",
      "Zan Gojcic",
      "Huan Ling"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Time_of_the_Flight_of_the_Gaussians_Optimizing_Depth_Indirectly_CVPR_2025_paper.html": {
    "title": "Time of the Flight of the Gaussians: Optimizing Depth Indirectly in Dynamic Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runfeng Li",
      "Mikhail Okunev",
      "Zixuan Guo",
      "Anh Ha Duong",
      "Christian Richardt",
      "Matthew O'Toole",
      "James Tompkin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Align3R_Aligned_Monocular_Depth_Estimation_for_Dynamic_Videos_CVPR_2025_paper.html": {
    "title": "Align3R: Aligned Monocular Depth Estimation for Dynamic Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Lu",
      "Tianyu Huang",
      "Peng Li",
      "Zhiyang Dou",
      "Cheng Lin",
      "Zhiming Cui",
      "Zhen Dong",
      "Sai-Kit Yeung",
      "Wenping Wang",
      "Yuan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Garosi_Compositional_Caching_for_Training-free_Open-vocabulary_Attribute_Detection_CVPR_2025_paper.html": {
    "title": "Compositional Caching for Training-free Open-vocabulary Attribute Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marco Garosi",
      "Alessandro Conti",
      "Gaowen Liu",
      "Elisa Ricci",
      "Massimiliano Mancini"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_Seek_Common_Ground_While_Reserving_Differences_Semi-Supervised_Image-Text_Sentiment_Recognition_CVPR_2025_paper.html": {
    "title": "Seek Common Ground While Reserving Differences: Semi-Supervised Image-Text Sentiment Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wuyou Xia",
      "Guoli Jia",
      "Sicheng Zhao",
      "Jufeng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huynh_CoLLM_A_Large_Language_Model_for_Composed_Image_Retrieval_CVPR_2025_paper.html": {
    "title": "CoLLM: A Large Language Model for Composed Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuong Huynh",
      "Jinyu Yang",
      "Ashish Tawari",
      "Mubarak Shah",
      "Son Tran",
      "Raffay Hamid",
      "Trishul Chilimbi",
      "Abhinav Shrivastava"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Anomize_Better_Open_Vocabulary_Video_Anomaly_Detection_CVPR_2025_paper.html": {
    "title": "Anomize: Better Open Vocabulary Video Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Li",
      "Wenxuan Liu",
      "Jingjing Chen",
      "Ruixu Zhang",
      "Yuran Wang",
      "Xian Zhong",
      "Zheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lan_Efficient_Diffusion_as_Low_Light_Enhancer_CVPR_2025_paper.html": {
    "title": "Efficient Diffusion as Low Light Enhancer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanzhou Lan",
      "Qianli Ma",
      "Yuqi Yang",
      "Zhigang Wang",
      "Dong Wang",
      "Xuelong Li",
      "Bin Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_GraphMimic_Graph-to-Graphs_Generative_Modeling_from_Videos_for_Policy_Learning_CVPR_2025_paper.html": {
    "title": "GraphMimic: Graph-to-Graphs Generative Modeling from Videos for Policy Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangyan Chen",
      "Te Cui",
      "Meiling Wang",
      "Chengcai Yang",
      "Mengxiao Hu",
      "Haoyang Lu",
      "Yao Mu",
      "Zicai Peng",
      "Tianxing Zhou",
      "Xinran Jiang",
      "Yi Yang",
      "Yufeng Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Koneputugodage_VI3NR_Variance_Informed_Initialization_for_Implicit_Neural_Representations_CVPR_2025_paper.html": {
    "title": "VI^3NR: Variance Informed Initialization for Implicit Neural Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chamin Hewa Koneputugodage",
      "Yizhak Ben-Shabat",
      "Sameera Ramasinghe",
      "Stephen Gould"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_MMVU_Measuring_Expert-Level_Multi-Discipline_Video_Understanding_CVPR_2025_paper.html": {
    "title": "MMVU: Measuring Expert-Level Multi-Discipline Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yilun Zhao",
      "Haowei Zhang",
      "Lujing Xie",
      "Tongyan Hu",
      "Guo Gan",
      "Yitao Long",
      "Zhiyuan Hu",
      "Weiyuan Chen",
      "Chuhan Li",
      "Zhijian Xu",
      "Chengye Wang",
      "Ziyao Shangguan",
      "Zhenwen Liang",
      "Yixin Liu",
      "Chen Zhao",
      "Arman Cohan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_M-LLM_Based_Video_Frame_Selection_for_Efficient_Video_Understanding_CVPR_2025_paper.html": {
    "title": "M-LLM Based Video Frame Selection for Efficient Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Hu",
      "Feng Gao",
      "Xiaohan Nie",
      "Peng Zhou",
      "Son Tran",
      "Tal Neiman",
      "Lingyun Wang",
      "Mubarak Shah",
      "Raffay Hamid",
      "Bing Yin",
      "Trishul Chilimbi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sidhu_Search_and_Detect_Training-Free_Long_Tail_Object_Detection_via_Web-Image_CVPR_2025_paper.html": {
    "title": "Search and Detect: Training-Free Long Tail Object Detection via Web-Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mankeerat Sidhu",
      "Hetarth Chopra",
      "Ansel Blume",
      "Jeonghwan Kim",
      "Revanth Gangi Reddy",
      "Heng Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hong_EgoLM_Multi-Modal_Language_Model_of_Egocentric_Motions_CVPR_2025_paper.html": {
    "title": "EgoLM: Multi-Modal Language Model of Egocentric Motions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangzhou Hong",
      "Vladimir Guzov",
      "Hyo Jin Kim",
      "Yuting Ye",
      "Richard Newcombe",
      "Ziwei Liu",
      "Lingni Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Unleashing_the_Potential_of_Multi-modal_Foundation_Models_and_Video_Diffusion_CVPR_2025_paper.html": {
    "title": "Unleashing the Potential of Multi-modal Foundation Models and Video Diffusion for 4D Dynamic Physical Scene Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoman Liu",
      "Weicai Ye",
      "Yan Luximon",
      "Pengfei Wan",
      "Di Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Diffusion_Model_is_Effectively_Its_Own_Teacher_CVPR_2025_paper.html": {
    "title": "Diffusion Model is Effectively Its Own Teacher",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyin Ma",
      "Runpeng Yu",
      "Songhua Liu",
      "Gongfan Fang",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Long_Video_Diffusion_Generation_with_Segmented_Cross-Attention_and_Content-Rich_Video_CVPR_2025_paper.html": {
    "title": "Long Video Diffusion Generation with Segmented Cross-Attention and Content-Rich Video Data Curation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Yan",
      "Yuxuan Cai",
      "Qiuyue Wang",
      "Yuan Zhou",
      "Wenhao Huang",
      "Huan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Learning_Heterogeneous_Tissues_with_Mixture_of_Experts_for_Gigapixel_Whole_CVPR_2025_paper.html": {
    "title": "Learning Heterogeneous Tissues with Mixture of Experts for Gigapixel Whole Slide Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junxian Wu",
      "Minheng Chen",
      "Xinyi Ke",
      "Tianwang Xun",
      "Xiaoming Jiang",
      "Hongyu Zhou",
      "Lizhi Shao",
      "Youyong Kong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pilligua_HyperNVD_Accelerating_Neural_Video_Decomposition_via_Hypernetworks_CVPR_2025_paper.html": {
    "title": "HyperNVD: Accelerating Neural Video Decomposition via Hypernetworks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maria Pilligua",
      "Danna Xue",
      "Javier Vazquez-Corral"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_UnCommon_Objects_in_3D_CVPR_2025_paper.html": {
    "title": "UnCommon Objects in 3D",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingchen Liu",
      "Piyush Tayal",
      "Jianyuan Wang",
      "Jesus Zarzar",
      "Tom Monnier",
      "Konstantinos Tertikas",
      "Jiali Duan",
      "Antoine Toisoul",
      "Jason Y. Zhang",
      "Natalia Neverova",
      "Andrea Vedaldi",
      "Roman Shapovalov",
      "David Novotny"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_Disentangled_Pose_and_Appearance_Guidance_for_Multi-Pose_Generation_CVPR_2025_paper.html": {
    "title": "Disentangled Pose and Appearance Guidance for Multi-Pose Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tengfei Xiao",
      "Yue Wu",
      "Yuelong Li",
      "Can Qin",
      "Maoguo Gong",
      "Qiguang Miao",
      "Wenping Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Mind_the_Gap_Confidence_Discrepancy_Can_Guide_Federated_Semi-Supervised_Learning_CVPR_2025_paper.html": {
    "title": "Mind the Gap: Confidence Discrepancy Can Guide Federated Semi-Supervised Learning Across Pseudo-Mismatch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijie Liu",
      "Xinyi Shang",
      "Yiqun Zhang",
      "Yang Lu",
      "Chen Gong",
      "Jing-Hao Xue",
      "Hanzi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lei_Instant_Adversarial_Purification_with_Adversarial_Consistency_Distillation_CVPR_2025_paper.html": {
    "title": "Instant Adversarial Purification with Adversarial Consistency Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun Tong Lei",
      "Hon Ming Yam",
      "Zhongliang Guo",
      "Yifei Qian",
      "Chun Pong Lau"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_Learning_Textual_Prompts_for_Open-World_Semi-Supervised_Learning_CVPR_2025_paper.html": {
    "title": "Learning Textual Prompts for Open-World Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxin Fan",
      "Junbiao Cui",
      "Jiye Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Buchner_Electromyography-Informed_Facial_Expression_Reconstruction_for_Physiological-Based_Synthesis_and_Analysis_CVPR_2025_paper.html": {
    "title": "Electromyography-Informed Facial Expression Reconstruction for Physiological-Based Synthesis and Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim B√ºchner",
      "Christoph Anders",
      "Orlando Guntinas-Lichius",
      "Joachim Denzler"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_LongDiff_Training-Free_Long_Video_Generation_in_One_Go_CVPR_2025_paper.html": {
    "title": "LongDiff: Training-Free Long Video Generation in One Go",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoling Li",
      "Hossein Rahmani",
      "Qiuhong Ke",
      "Jun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kansabanik_Feature_Selection_for_Latent_Factor_Models_CVPR_2025_paper.html": {
    "title": "Feature Selection for Latent Factor Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rittwika Kansabanik",
      "Adrian Barbu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Preserve_or_Modify_Context-Aware_Evaluation_for_Balancing_Preservation_and_Modification_CVPR_2025_paper.html": {
    "title": "Preserve or Modify? Context-Aware Evaluation for Balancing Preservation and Modification in Text-Guided Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoonjeon Kim",
      "Soohyun Ryu",
      "Yeonsung Jung",
      "Hyunkoo Lee",
      "Joowon Kim",
      "June Yong Yang",
      "Jaeryong Hwang",
      "Eunho Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Mask-Adapter_The_Devil_is_in_the_Masks_for_Open-Vocabulary_Segmentation_CVPR_2025_paper.html": {
    "title": "Mask-Adapter: The Devil is in the Masks for Open-Vocabulary Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongkang Li",
      "Tianheng Cheng",
      "Bin Feng",
      "Wenyu Liu",
      "Xinggang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_MPDrive_Improving_Spatial_Understanding_with_Marker-Based_Prompt_Learning_for_Autonomous_CVPR_2025_paper.html": {
    "title": "MPDrive: Improving Spatial Understanding with Marker-Based Prompt Learning for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Zhang",
      "Xiaofan Li",
      "Zhihao Xu",
      "Wenjie Peng",
      "Zijian Zhou",
      "Miaojing Shi",
      "Shuangping Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Improving_the_Transferability_of_Adversarial_Attacks_on_Face_Recognition_with_CVPR_2025_paper.html": {
    "title": "Improving the Transferability of Adversarial Attacks on Face Recognition with Diverse Parameters Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengfan Zhou",
      "Bangjie Yin",
      "Hefei Ling",
      "Qianyu Zhou",
      "Wenxuan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qiu_Adapting_to_Observation_Length_of_Trajectory_Prediction_via_Contrastive_Learning_CVPR_2025_paper.html": {
    "title": "Adapting to Observation Length of Trajectory Prediction via Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiqi Qiu",
      "Jun Gong",
      "Xinyu Zhang",
      "Siqi Luo",
      "Bowen Zhang",
      "Yi Cen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Choi_Fine-Grained_Image-Text_Correspondence_with_Cost_Aggregation_for_Open-Vocabulary_Part_Segmentation_CVPR_2025_paper.html": {
    "title": "Fine-Grained Image-Text Correspondence with Cost Aggregation for Open-Vocabulary Part Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiho Choi",
      "Seonho Lee",
      "Minhyun Lee",
      "Seungho Lee",
      "Hyunjung Shim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_NitroFusion_High-Fidelity_Single-Step_Diffusion_through_Dynamic_Adversarial_Training_CVPR_2025_paper.html": {
    "title": "NitroFusion: High-Fidelity Single-Step Diffusion through Dynamic Adversarial Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dar-Yen Chen",
      "Hmrishav Bandyopadhyay",
      "Kai Zou",
      "Yi-Zhe Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bu_ByTheWay_Boost_Your_Text-to-Video_Generation_Model_to_Higher_Quality_in_CVPR_2025_paper.html": {
    "title": "ByTheWay: Boost Your Text-to-Video Generation Model to Higher Quality in a Training-free Way",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiazi Bu",
      "Pengyang Ling",
      "Pan Zhang",
      "Tong Wu",
      "Xiaoyi Dong",
      "Yuhang Zang",
      "Yuhang Cao",
      "Dahua Lin",
      "Jiaqi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_CMMLoc_Advancing_Text-to-PointCloud_Localization_with_Cauchy-Mixture-Model_Based_Framework_CVPR_2025_paper.html": {
    "title": "CMMLoc: Advancing Text-to-PointCloud Localization with Cauchy-Mixture-Model Based Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanlong Xu",
      "Haoxuan Qu",
      "Jun Liu",
      "Wenxiao Zhang",
      "Xun Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Masked_Point-Entity_Contrast_for_Open-Vocabulary_3D_Scene_Understanding_CVPR_2025_paper.html": {
    "title": "Masked Point-Entity Contrast for Open-Vocabulary 3D Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Wang",
      "Baoxiong Jia",
      "Ziyu Zhu",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Decoupling_Training-Free_Guided_Diffusion_by_ADMM_CVPR_2025_paper.html": {
    "title": "Decoupling Training-Free Guided Diffusion by ADMM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youyuan Zhang",
      "Zehua Liu",
      "Zenan Li",
      "Zhaoyu Li",
      "James J. Clark",
      "Xujie Si"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Garrido-Munoz_On_the_Generalization_of_Handwritten_Text_Recognition_Models_CVPR_2025_paper.html": {
    "title": "On the Generalization of Handwritten Text Recognition Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Carlos Garrido-Munoz",
      "Jorge Calvo-Zaragoza"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nguyen_SwiftEdit_Lightning_Fast_Text-Guided_Image_Editing_via_One-Step_Diffusion_CVPR_2025_paper.html": {
    "title": "SwiftEdit: Lightning Fast Text-Guided Image Editing via One-Step Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Trong-Tung Nguyen",
      "Quang Nguyen",
      "Khoi Nguyen",
      "Anh Tran",
      "Cuong Pham"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Learning_from_Synchronization_Self-Supervised_Uncalibrated_Multi-View_Person_Association_in_Challenging_CVPR_2025_paper.html": {
    "title": "Learning from Synchronization: Self-Supervised Uncalibrated Multi-View Person Association in Challenging Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keqi Chen",
      "Vinkle Srivastav",
      "Didier Mutter",
      "Nicolas Padoy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luu_RC-AutoCalib_An_End-to-End_Radar-Camera_Automatic_Calibration_Network_CVPR_2025_paper.html": {
    "title": "RC-AutoCalib: An End-to-End Radar-Camera Automatic Calibration Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Van-Tin Luu",
      "Yon-Lin Cai",
      "Vu-Hoang Tran",
      "Wei-Chen Chiu",
      "Yi-Ting Chen",
      "Ching-Chun Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhuang_Argus_A_Compact_and_Versatile_Foundation_Model_for_Vision_CVPR_2025_paper.html": {
    "title": "Argus: A Compact and Versatile Foundation Model for Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiming Zhuang",
      "Chen Chen",
      "Zhizhong Li",
      "Sina Sajadmanesh",
      "Jingtao Li",
      "Jiabo Huang",
      "Vikash Sehwag",
      "Vivek Sharma",
      "Hirotaka Shinozaki",
      "Felan Carlo Garcia",
      "Yihao Zhan",
      "Naohiro Adachi",
      "Ryoji Eki",
      "Michael Spranger",
      "Peter Stone",
      "Lingjuan Lyu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_CLIP-driven_Coarse-to-fine_Semantic_Guidance_for_Fine-grained_Open-set_Semi-supervised_Learning_CVPR_2025_paper.html": {
    "title": "CLIP-driven Coarse-to-fine Semantic Guidance for Fine-grained Open-set Semi-supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaokun Li",
      "Yaping Huang",
      "Qingji Guan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_InsTaG_Learning_Personalized_3D_Talking_Head_from_Few-Second_Video_CVPR_2025_paper.html": {
    "title": "InsTaG: Learning Personalized 3D Talking Head from Few-Second Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahe Li",
      "Jiawei Zhang",
      "Xiao Bai",
      "Jin Zheng",
      "Jun Zhou",
      "Lin Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_Sampling_Innovation-Based_Adaptive_Compressive_Sensing_CVPR_2025_paper.html": {
    "title": "Sampling Innovation-Based Adaptive Compressive Sensing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhifu Tian",
      "Tao Hu",
      "Chaoyang Niu",
      "Di Wu",
      "Shu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_A_Simple_Data_Augmentation_for_Feature_Distribution_Skewed_Federated_Learning_CVPR_2025_paper.html": {
    "title": "A Simple Data Augmentation for Feature Distribution Skewed Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunlu Yan",
      "Huazhu Fu",
      "Yuexiang Li",
      "Jinheng Xie",
      "Jun Ma",
      "Guang Yang",
      "Lei Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hong_MotionBench_Benchmarking_and_Improving_Fine-grained_Video_Motion_Understanding_for_Vision_CVPR_2025_paper.html": {
    "title": "MotionBench: Benchmarking and Improving Fine-grained Video Motion Understanding for Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyi Hong",
      "Yean Cheng",
      "Zhuoyi Yang",
      "Weihan Wang",
      "Lefan Wang",
      "Xiaotao Gu",
      "Shiyu Huang",
      "Yuxiao Dong",
      "Jie Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Benchmarking_Large_Vision-Language_Models_via_Directed_Scene_Graph_for_Comprehensive_CVPR_2025_paper.html": {
    "title": "Benchmarking Large Vision-Language Models via Directed Scene Graph for Comprehensive Image Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Lu",
      "Wei Wu",
      "Kecheng Zheng",
      "Shuailei Ma",
      "Biao Gong",
      "Jiawei Liu",
      "Wei Zhai",
      "Yang Cao",
      "Yujun Shen",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pu_ART_Anonymous_Region_Transformer_for_Variable_Multi-Layer_Transparent_Image_Generation_CVPR_2025_paper.html": {
    "title": "ART: Anonymous Region Transformer for Variable Multi-Layer Transparent Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Pu",
      "Yiming Zhao",
      "Zhicong Tang",
      "Ruihong Yin",
      "Haoxing Ye",
      "Yuhui Yuan",
      "Dong Chen",
      "Jianmin Bao",
      "Sirui Zhang",
      "Yanbin Wang",
      "Lin Liang",
      "Lijuan Wang",
      "Ji Li",
      "Xiu Li",
      "Zhouhui Lian",
      "Gao Huang",
      "Baining Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Rotation-Equivariant_Self-Supervised_Method_in_Image_Denoising_CVPR_2025_paper.html": {
    "title": "Rotation-Equivariant Self-Supervised Method in Image Denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanze Liu",
      "Jiahong Fu",
      "Qi Xie",
      "Deyu Meng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_ArcPro_Architectural_Programs_for_Structured_3D_Abstraction_of_Sparse_Points_CVPR_2025_paper.html": {
    "title": "ArcPro: Architectural Programs for Structured 3D Abstraction of Sparse Points",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qirui Huang",
      "Runze Zhang",
      "Kangjun Liu",
      "Minglun Gong",
      "Hao Zhang",
      "Hui Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ozturk_GLane3D_Detecting_Lanes_with_Graph_of_3D_Keypoints_CVPR_2025_paper.html": {
    "title": "GLane3D: Detecting Lanes with Graph of 3D Keypoints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Halil ƒ∞brahim √ñzt√ºrk",
      "Muhammet Esat Kalfaoƒülu",
      "Ozsel Kilinc"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Minimal_Interaction_Seperated_Tuning_A_New_Paradigm_for_Visual_Adaptation_CVPR_2025_paper.html": {
    "title": "Minimal Interaction Seperated Tuning: A New Paradigm for Visual Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ningyuan Tang",
      "Minghao Fu",
      "Jianxin Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bulo_Hardware-Rasterized_Ray-Based_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "Hardware-Rasterized Ray-Based Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Samuel Rota Bul√≤",
      "Nemanja Bartolovic",
      "Lorenzo Porzi",
      "Peter Kontschieder"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tong_FlashSloth__Lightning_Multimodal_Large_Language_Models_via_Embedded_Visual_CVPR_2025_paper.html": {
    "title": "FlashSloth : Lightning Multimodal Large Language Models via Embedded Visual Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Tong",
      "Bokai Lai",
      "Yiyi Zhou",
      "Gen Luo",
      "Yunhang Shen",
      "Ke Li",
      "Xiaoshuai Sun",
      "Rongrong Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kashiani_FreqDebias_Towards_Generalizable_Deepfake_Detection_via_Consistency-Driven_Frequency_Debiasing_CVPR_2025_paper.html": {
    "title": "FreqDebias: Towards Generalizable Deepfake Detection via Consistency-Driven Frequency Debiasing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hossein Kashiani",
      "Niloufar Alipour Talemi",
      "Fatemeh Afghah"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Multi-subject_Open-set_Personalization_in_Video_Generation_CVPR_2025_paper.html": {
    "title": "Multi-subject Open-set Personalization in Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tsai-Shien Chen",
      "Aliaksandr Siarohin",
      "Willi Menapace",
      "Yuwei Fang",
      "Kwot Sin Lee",
      "Ivan Skorokhodov",
      "Kfir Aberman",
      "Jun-Yan Zhu",
      "Ming-Hsuan Yang",
      "Sergey Tulyakov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Wav2Sem_Plug-and-Play_Audio_Semantic_Decoupling_for_3D_Speech-Driven_Facial_Animation_CVPR_2025_paper.html": {
    "title": "Wav2Sem: Plug-and-Play Audio Semantic Decoupling for 3D Speech-Driven Facial Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Li",
      "Ju Dai",
      "Xin Zhao",
      "Feng Zhou",
      "Junjun Pan",
      "Lei Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Attraction_Diminishing_and_Distributing_for_Few-Shot_Class-Incremental_Learning_CVPR_2025_paper.html": {
    "title": "Attraction Diminishing and Distributing for Few-Shot Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li-Jun Zhao",
      "Zhen-Duo Chen",
      "Yongxin Wang",
      "Xin Luo",
      "Xin-Shun Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Matsuki_4DTAM_Non-Rigid_Tracking_and_Mapping_via_Dynamic_Surface_Gaussians_CVPR_2025_paper.html": {
    "title": "4DTAM: Non-Rigid Tracking and Mapping via Dynamic Surface Gaussians",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hidenobu Matsuki",
      "Gwangbin Bae",
      "Andrew J. Davison"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lv_T2SG_Traffic_Topology_Scene_Graph_for_Topology_Reasoning_in_Autonomous_CVPR_2025_paper.html": {
    "title": "T2SG: Traffic Topology Scene Graph for Topology Reasoning in Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changsheng Lv",
      "Mengshi Qi",
      "Liang Liu",
      "Huadong Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Unseen_Visual_Anomaly_Generation_CVPR_2025_paper.html": {
    "title": "Unseen Visual Anomaly Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Sun",
      "Yunkang Cao",
      "Hao Dong",
      "Olga Fink"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qian_T2ICount_Enhancing_Cross-modal_Understanding_for_Zero-Shot_Counting_CVPR_2025_paper.html": {
    "title": "T2ICount: Enhancing Cross-modal Understanding for Zero-Shot Counting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Qian",
      "Zhongliang Guo",
      "Bowen Deng",
      "Chun Tong Lei",
      "Shuai Zhao",
      "Chun Pong Lau",
      "Xiaopeng Hong",
      "Michael P. Pound"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sushko_RealEdit_Reddit_Edits_As_a_Large-scale_Empirical_Dataset_for_Image_CVPR_2025_paper.html": {
    "title": "RealEdit: Reddit Edits As a Large-scale Empirical Dataset for Image Transformations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Sushko",
      "Ayana Bharadwaj",
      "Zhi Yang Lim",
      "Vasily Ilin",
      "Ben Caffee",
      "Dongping Chen",
      "Mohammadreza Salehi",
      "Cheng-Yu Hsieh",
      "Ranjay Krishna"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VideoScene_Distilling_Video_Diffusion_Model_to_Generate_3D_Scenes_in_CVPR_2025_paper.html": {
    "title": "VideoScene: Distilling Video Diffusion Model to Generate 3D Scenes in One Step",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyang Wang",
      "Fangfu Liu",
      "Jiawei Chi",
      "Yueqi Duan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_3D-HGS_3D_Half-Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "3D-HGS: 3D Half-Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haolin Li",
      "Jinyang Liu",
      "Mario Sznaier",
      "Octavia Camps"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_FG2_Fine-Grained_Cross-View_Localization_by_Fine-Grained_Feature_Matching_CVPR_2025_paper.html": {
    "title": "FG^2: Fine-Grained Cross-View Localization by Fine-Grained Feature Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zimin Xia",
      "Alexandre Alahi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_ReNeg_Learning_Negative_Embedding_with_Reward_Guidance_CVPR_2025_paper.html": {
    "title": "ReNeg: Learning Negative Embedding with Reward Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaomin Li",
      "Yixuan Liu",
      "Takashi Isobe",
      "Xu Jia",
      "Qinpeng Cui",
      "Dong Zhou",
      "Dong Li",
      "You He",
      "Huchuan Lu",
      "Zhongdao Wang",
      "Emad Barsoum"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Scale_Efficient_Training_for_Large_Datasets_CVPR_2025_paper.html": {
    "title": "Scale Efficient Training for Large Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qing Zhou",
      "Junyu Gao",
      "Qi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Distilled_Prompt_Learning_for_Incomplete_Multimodal_Survival_Prediction_CVPR_2025_paper.html": {
    "title": "Distilled Prompt Learning for Incomplete Multimodal Survival Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingxue Xu",
      "Fengtao Zhou",
      "Chenyu Zhao",
      "Yihui Wang",
      "Can Yang",
      "Hao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/An_Decoder_Gradient_Shield_Provable_and_High-Fidelity_Prevention_of_Gradient-Based_Box-Free_CVPR_2025_paper.html": {
    "title": "Decoder Gradient Shield: Provable and High-Fidelity Prevention of Gradient-Based Box-Free Watermark Removal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan An",
      "Guang Hua",
      "Zhengru Fang",
      "Guowen Xu",
      "Susanto Rahardja",
      "Yuguang Fang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_MotionPro_A_Precise_Motion_Controller_for_Image-to-Video_Generation_CVPR_2025_paper.html": {
    "title": "MotionPro: A Precise Motion Controller for Image-to-Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongwei Zhang",
      "Fuchen Long",
      "Zhaofan Qiu",
      "Yingwei Pan",
      "Wu Liu",
      "Ting Yao",
      "Tao Mei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Goku_Flow_Based_Video_Generative_Foundation_Models_CVPR_2025_paper.html": {
    "title": "Goku: Flow Based Video Generative Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shoufa Chen",
      "Chongjian Ge",
      "Yuqi Zhang",
      "Yida Zhang",
      "Fengda Zhu",
      "Hao Yang",
      "Hongxiang Hao",
      "Hui Wu",
      "Zhichao Lai",
      "Yifei Hu",
      "Ting-Che Lin",
      "Shilong Zhang",
      "Fu Li",
      "Chuan Li",
      "Xing Wang",
      "Yanghua Peng",
      "Peize Sun",
      "Ping Luo",
      "Yi Jiang",
      "Zehuan Yuan",
      "Bingyue Peng",
      "Xiaobing Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zou_Learning_Conditional_Space-Time_Prompt_Distributions_for_Video_Class-Incremental_Learning_CVPR_2025_paper.html": {
    "title": "Learning Conditional Space-Time Prompt Distributions for Video Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohan Zou",
      "Wenchao Ma",
      "Shu Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Convex_Combination_Star_Shape_Prior_for_Data-driven_Image_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Convex Combination Star Shape Prior for Data-driven Image Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Zhao",
      "Jun Xie",
      "Shengzhe Chen",
      "Jun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Poppi_Hyperbolic_Safety-Aware_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "Hyperbolic Safety-Aware Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tobia Poppi",
      "Tejaswi Kasarla",
      "Pascal Mettes",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kweon_WISH_Weakly_Supervised_Instance_Segmentation_using_Heterogeneous_Labels_CVPR_2025_paper.html": {
    "title": "WISH: Weakly Supervised Instance Segmentation using Heterogeneous Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeokjun Kweon",
      "Kuk-Jin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_SinGS_Animatable_Single-Image_Human_Gaussian_Splats_with_Kinematic_Priors_CVPR_2025_paper.html": {
    "title": "SinGS: Animatable Single-Image Human Gaussian Splats with Kinematic Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufan Wu",
      "Xuanhong Chen",
      "Wen Li",
      "Shunran Jia",
      "Hualiang Wei",
      "Kairui Feng",
      "Jialiang Chen",
      "Yuhan Li",
      "Ang He",
      "Weimin Zhang",
      "Bingbing Ni",
      "Wenjun Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peng_Parameter-efficient_Fine-tuning_in_Hyperspherical_Space_for_Open-vocabulary_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Parameter-efficient Fine-tuning in Hyperspherical Space for Open-vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zelin Peng",
      "Zhengqin Xu",
      "Zhilin Zeng",
      "Yu Huang",
      "Yaoming Wang",
      "Wei Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Relative_Pose_Estimation_through_Affine_Corrections_of_Monocular_Depth_Priors_CVPR_2025_paper.html": {
    "title": "Relative Pose Estimation through Affine Corrections of Monocular Depth Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Yu",
      "Shaohui Liu",
      "R√©mi Pautrat",
      "Marc Pollefeys",
      "Viktor Larsson"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Zero-1-to-A_Zero-Shot_One_Image_to_Animatable_Head_Avatars_Using_Video_CVPR_2025_paper.html": {
    "title": "Zero-1-to-A: Zero-Shot One Image to Animatable Head Avatars Using Video Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenglin Zhou",
      "Fan Ma",
      "Hehe Fan",
      "Tat-Seng Chua"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nguyen_Occlusion-aware_Text-Image-Point_Cloud_Pretraining_for_Open-World_3D_Object_Recognition_CVPR_2025_paper.html": {
    "title": "Occlusion-aware Text-Image-Point Cloud Pretraining for Open-World 3D Object Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khanh Nguyen",
      "Ghulam Mubashar Hassan",
      "Ajmal Mian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xing_Conical_Visual_Concentration_for_Efficient_Large_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "Conical Visual Concentration for Efficient Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long Xing",
      "Qidong Huang",
      "Xiaoyi Dong",
      "Jiajie Lu",
      "Pan Zhang",
      "Yuhang Zang",
      "Yuhang Cao",
      "Conghui He",
      "Jiaqi Wang",
      "Feng Wu",
      "Dahua Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Balle_Good_Cheap_and_Fast_Overfitted_Image_Compression_with_Wasserstein_Distortion_CVPR_2025_paper.html": {
    "title": "Good, Cheap, and Fast: Overfitted Image Compression with Wasserstein Distortion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jona Ball√©",
      "Luca Versari",
      "Emilien Dupont",
      "Hyunjik Kim",
      "Matthias Bauer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Period-LLM_Extending_the_Periodic_Capability_of_Multimodal_Large_Language_Model_CVPR_2025_paper.html": {
    "title": "Period-LLM: Extending the Periodic Capability of Multimodal Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuting Zhang",
      "Hao Lu",
      "Qingyong Hu",
      "Yin Wang",
      "Kaishen Yuan",
      "Xin Liu",
      "Kaishun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_V2X-R_Cooperative_LiDAR-4D_Radar_Fusion_with_Denoising_Diffusion_for_3D_CVPR_2025_paper.html": {
    "title": "V2X-R: Cooperative LiDAR-4D Radar Fusion with Denoising Diffusion for 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xun Huang",
      "Jinlong Wang",
      "Qiming Xia",
      "Siheng Chen",
      "Bisheng Yang",
      "Xin Li",
      "Cheng Wang",
      "Chenglu Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dai_Multi-Modal_Synergistic_Implicit_Image_Enhancement_for_Efficient_Optical_Flow_Estimation_CVPR_2025_paper.html": {
    "title": "Multi-Modal Synergistic Implicit Image Enhancement for Efficient Optical Flow Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weichen Dai",
      "Hexing Wu",
      "Xiaoyang Weng",
      "Yuxin Zheng",
      "Yuhang Ming",
      "Wanzeng Kong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_TAROT_Towards_Essentially_Domain-Invariant_Robustness_with_Theoretical_Justification_CVPR_2025_paper.html": {
    "title": "TAROT: Towards Essentially Domain-Invariant Robustness with Theoretical Justification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyoon Yang",
      "Jihu Lee",
      "Yongdai Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pierard_Foundations_of_the_Theory_of_Performance-Based_Ranking_CVPR_2025_paper.html": {
    "title": "Foundations of the Theory of Performance-Based Ranking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "S√©bastien Pi√©rard",
      "Ana√Øs Halin",
      "Anthony Cioppa",
      "Adrien Deliege",
      "Marc Van Droogenbroeck"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Unveiling_the_Mist_over_3D_Vision-Language_Understanding_Object-centric_Evaluation_with_CVPR_2025_paper.html": {
    "title": "Unveiling the Mist over 3D Vision-Language Understanding: Object-centric Evaluation with Chain-of-Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangyong Huang",
      "Baoxiong Jia",
      "Yan Wang",
      "Ziyu Zhu",
      "Xiongkun Linghu",
      "Qing Li",
      "Song-Chun Zhu",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Generating_Multimodal_Driving_Scenes_via_Next-Scene_Prediction_CVPR_2025_paper.html": {
    "title": "Generating Multimodal Driving Scenes via Next-Scene Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanhao Wu",
      "Haoyang Zhang",
      "Tianwei Lin",
      "Lichao Huang",
      "Shujie Luo",
      "Rui Wu",
      "Congpei Qiu",
      "Wei Ke",
      "Tong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/On_BIGS_Bimanual_Category-agnostic_Interaction_Reconstruction_from_Monocular_Videos_via_3D_CVPR_2025_paper.html": {
    "title": "BIGS: Bimanual Category-agnostic Interaction Reconstruction from Monocular Videos via 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongwan On",
      "Kyeonghwan Gwak",
      "Gunyoung Kang",
      "Junuk Cha",
      "Soohyun Hwang",
      "Hyein Hwang",
      "Seungryul Baek"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chae_APT_Adaptive_Personalized_Training_for_Diffusion_Models_with_Limited_Data_CVPR_2025_paper.html": {
    "title": "APT: Adaptive Personalized Training for Diffusion Models with Limited Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JungWoo Chae",
      "Jiyoon Kim",
      "JaeWoong Choi",
      "Kyungyul Kim",
      "Sangheum Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Symmetry_Strikes_Back_From_Single-Image_Symmetry_Detection_to_3D_Generation_CVPR_2025_paper.html": {
    "title": "Symmetry Strikes Back: From Single-Image Symmetry Detection to 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Li",
      "Zixuan Huang",
      "Anh Thai",
      "James M. Rehg"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Frequency-Biased_Synergistic_Design_for_Image_Compression_and_Compensation_CVPR_2025_paper.html": {
    "title": "Frequency-Biased Synergistic Design for Image Compression and Compensation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Liu",
      "Qi Zheng",
      "Zihao Liu",
      "Yilian Zhong",
      "Peiye Liu",
      "Tao Liu",
      "Shusong Xu",
      "Yanheng Lu",
      "Sicheng Li",
      "Dimin Niu",
      "Yibo Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_PosterMaker_Towards_High-Quality_Product_Poster_Generation_with_Accurate_Text_Rendering_CVPR_2025_paper.html": {
    "title": "PosterMaker: Towards High-Quality Product Poster Generation with Accurate Text Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Gao",
      "Zihang Lin",
      "Chuanbin Liu",
      "Min Zhou",
      "Tiezheng Ge",
      "Bo Zheng",
      "Hongtao Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Sparse_Voxels_Rasterization_Real-time_High-fidelity_Radiance_Field_Rendering_CVPR_2025_paper.html": {
    "title": "Sparse Voxels Rasterization: Real-time High-fidelity Radiance Field Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Sun",
      "Jaesung Choe",
      "Charles Loop",
      "Wei-Chiu Ma",
      "Yu-Chiang Frank Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhong_Rethinking_Personalized_Aesthetics_Assessment_Employing_Physique_Aesthetics_Assessment_as_An_CVPR_2025_paper.html": {
    "title": "Rethinking Personalized Aesthetics Assessment: Employing Physique Aesthetics Assessment as An Exemplification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haobin Zhong",
      "Shuai He",
      "Anlong Ming",
      "Huadong Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_You_See_it_You_Got_it_Learning_3D_Creation_on_CVPR_2025_paper.html": {
    "title": "You See it, You Got it: Learning 3D Creation on Pose-Free Videos at Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baorui Ma",
      "Huachen Gao",
      "Haoge Deng",
      "Zhengxiong Luo",
      "Tiejun Huang",
      "Lulu Tang",
      "Xinlong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zeng_MambaIC_State_Space_Models_for_High-Performance_Learned_Image_Compression_CVPR_2025_paper.html": {
    "title": "MambaIC: State Space Models for High-Performance Learned Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanhu Zeng",
      "Hao Tang",
      "Yihua Shao",
      "Siyu Chen",
      "Ling Shao",
      "Yan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_SCAP_Transductive_Test-Time_Adaptation_via_Supportive_Clique-based_Attribute_Prompting_CVPR_2025_paper.html": {
    "title": "SCAP: Transductive Test-Time Adaptation via Supportive Clique-based Attribute Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyu Zhang",
      "Kunlun Xu",
      "Zichen Liu",
      "Yuxin Peng",
      "Jiahuan Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Instant_Gaussian_Stream_Fast_and_Generalizable_Streaming_of_Dynamic_Scene_CVPR_2025_paper.html": {
    "title": "Instant Gaussian Stream: Fast and Generalizable Streaming of Dynamic Scene Reconstruction via Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinbo Yan",
      "Rui Peng",
      "Zhiyan Wang",
      "Luyang Tang",
      "Jiayu Yang",
      "Jie Liang",
      "Jiahao Wu",
      "Ronggang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Locality-Aware_Zero-Shot_Human-Object_Interaction_Detection_CVPR_2025_paper.html": {
    "title": "Locality-Aware Zero-Shot Human-Object Interaction Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanghyun Kim",
      "Deunsol Jung",
      "Minsu Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_PEACE_Empowering_Geologic_Map_Holistic_Understanding_with_MLLMs_CVPR_2025_paper.html": {
    "title": "PEACE: Empowering Geologic Map Holistic Understanding with MLLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangyu Huang",
      "Tianyi Gao",
      "Haoran Xu",
      "Qihao Zhao",
      "Yang Song",
      "Zhipeng Gui",
      "Tengchao Lv",
      "Hao Chen",
      "Lei Cui",
      "Scarlett Li",
      "Furu Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lai_Tracktention_Leveraging_Point_Tracking_to_Attend_Videos_Faster_and_Better_CVPR_2025_paper.html": {
    "title": "Tracktention: Leveraging Point Tracking to Attend Videos Faster and Better",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihang Lai",
      "Andrea Vedaldi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_ConceptGuard_Continual_Personalized_Text-to-Image_Generation_with_Forgetting_and_Confusion_Mitigation_CVPR_2025_paper.html": {
    "title": "ConceptGuard: Continual Personalized Text-to-Image Generation with Forgetting and Confusion Mitigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zirun Guo",
      "Tao Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qi_Two_by_Two_Learning_Multi-Task_Pairwise_Objects_Assembly_for_Generalizable_CVPR_2025_paper.html": {
    "title": "Two by Two: Learning Multi-Task Pairwise Objects Assembly for Generalizable Robot Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Qi",
      "Yuanchen Ju",
      "Tianming Wei",
      "Chi Chu",
      "Lawson L.S. Wong",
      "Huazhe Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_SGFormer_Satellite-Ground_Fusion_for_3D_Semantic_Scene_Completion_CVPR_2025_paper.html": {
    "title": "SGFormer: Satellite-Ground Fusion for 3D Semantic Scene Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiyue Guo",
      "Jiarui Hu",
      "Junjie Hu",
      "Hujun Bao",
      "Guofeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sinha_MARVEL-40M_Multi-Level_Visual_Elaboration_for_High-Fidelity_Text-to-3D_Content_Creation_CVPR_2025_paper.html": {
    "title": "MARVEL-40M+: Multi-Level Visual Elaboration for High-Fidelity Text-to-3D Content Creation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sankalp Sinha",
      "Mohammad Sadil Khan",
      "Muhammad Usama",
      "Shino Sam",
      "Didier Stricker",
      "Sk Aziz Ali",
      "Muhammad Zeshan Afzal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Random_Conditioning_for_Diffusion_Model_Compression_with_Distillation_CVPR_2025_paper.html": {
    "title": "Random Conditioning for Diffusion Model Compression with Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dohyun Kim",
      "Sehwan Park",
      "Geonhee Han",
      "Seung Wook Kim",
      "Paul Hongsuck Seo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Hierarchical_Gaussian_Mixture_Model_Splatting_for_Efficient_and_Part_Controllable_CVPR_2025_paper.html": {
    "title": "Hierarchical Gaussian Mixture Model Splatting for Efficient and Part Controllable 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qitong Yang",
      "Mingtao Feng",
      "Zijie Wu",
      "Weisheng Dong",
      "Fangfang Wu",
      "Yaonan Wang",
      "Ajmal Mian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shugaev_ERUPT_Efficient_Rendering_with_Unposed_Patch_Transformer_CVPR_2025_paper.html": {
    "title": "ERUPT: Efficient Rendering with Unposed Patch Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxim V. Shugaev",
      "Vincent Chen",
      "Maxim Karrenbach",
      "Kyle Ashley",
      "Bridget Kennedy",
      "Naresh P. Cuntoor"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Rethinking_End-to-End_2D_to_3D_Scene_Segmentation_in_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "Rethinking End-to-End 2D to 3D Scene Segmentation in Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runsong Zhu",
      "Shi Qiu",
      "Zhengzhe Liu",
      "Ka-Hei Hui",
      "Qianyi Wu",
      "Pheng-Ann Heng",
      "Chi-Wing Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Quad-Pixel_Image_Defocus_Deblurring_A_New_Benchmark_and_Model_CVPR_2025_paper.html": {
    "title": "Quad-Pixel Image Defocus Deblurring: A New Benchmark and Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Chen",
      "Yin Xie",
      "Xiaoxiu Peng",
      "Lihu Sun",
      "Wenkai Su",
      "Xiaodong Yang",
      "Chengming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nacson_DocVLM_Make_Your_VLM_an_Efficient_Reader_CVPR_2025_paper.html": {
    "title": "DocVLM: Make Your VLM an Efficient Reader",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mor Shpigel Nacson",
      "Aviad Aberdam",
      "Roy Ganz",
      "Elad Ben Avraham",
      "Alona Golts",
      "Yair Kittenplon",
      "Shai Mazor",
      "Ron Litman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Revisiting_Source-Free_Domain_Adaptation_Insights_into_Representativeness_Generalization_and_Variety_CVPR_2025_paper.html": {
    "title": "Revisiting Source-Free Domain Adaptation: Insights into Representativeness, Generalization, and Variety",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ronghang Zhu",
      "Mengxuan Hu",
      "Weiming Zhuang",
      "Lingjuan Lyu",
      "Xiang Yu",
      "Sheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Adaptive_Unimodal_Regulation_for_Balanced_Multimodal_Information_Acquisition_CVPR_2025_paper.html": {
    "title": "Adaptive Unimodal Regulation for Balanced Multimodal Information Acquisition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengxiang Huang",
      "Yake Wei",
      "Zequn Yang",
      "Di Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Heterogeneous_Skeleton-Based_Action_Representation_Learning_CVPR_2025_paper.html": {
    "title": "Heterogeneous Skeleton-Based Action Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongsong Wang",
      "Xiaoyan Ma",
      "Jidong Kuang",
      "Jie Gui"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_FLARE_Feed-forward_Geometry_Appearance_and_Camera_Estimation_from_Uncalibrated_Sparse_CVPR_2025_paper.html": {
    "title": "FLARE: Feed-forward Geometry, Appearance and Camera Estimation from Uncalibrated Sparse Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangzhan Zhang",
      "Jianyuan Wang",
      "Yinghao Xu",
      "Nan Xue",
      "Christian Rupprecht",
      "Xiaowei Zhou",
      "Yujun Shen",
      "Gordon Wetzstein"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Improving_Gaussian_Splatting_with_Localized_Points_Management_CVPR_2025_paper.html": {
    "title": "Improving Gaussian Splatting with Localized Points Management",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haosen Yang",
      "Chenhao Zhang",
      "Wenqing Wang",
      "Marco Volino",
      "Adrian Hilton",
      "Li Zhang",
      "Xiatian Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_GEAL_Generalizable_3D_Affordance_Learning_with_Cross-Modal_Consistency_CVPR_2025_paper.html": {
    "title": "GEAL: Generalizable 3D Affordance Learning with Cross-Modal Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongyue Lu",
      "Lingdong Kong",
      "Tianxin Huang",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Dynamic_Derivation_and_Elimination_Audio_Visual_Segmentation_with_Enhanced_Audio_CVPR_2025_paper.html": {
    "title": "Dynamic Derivation and Elimination: Audio Visual Segmentation with Enhanced Audio Semantics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Liu",
      "Liying Yang",
      "Peike Li",
      "Dadong Wang",
      "Lincheng Li",
      "Xin Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dal_Cin_AnyMap_Learning_a_General_Camera_Model_for_Structure-from-Motion_with_Unknown_CVPR_2025_paper.html": {
    "title": "AnyMap: Learning a General Camera Model for Structure-from-Motion with Unknown Distortion in Dynamic Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Porfiri Dal Cin",
      "Georgi Dikov",
      "Jihong Ju",
      "Mohsen Ghafoorian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_ESC_Erasing_Space_Concept_for_Knowledge_Deletion_CVPR_2025_paper.html": {
    "title": "ESC: Erasing Space Concept for Knowledge Deletion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tae-Young Lee",
      "Sundong Park",
      "Minwoo Jeon",
      "Hyoseok Hwang",
      "Gyeong-Moon Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Language_Guided_Concept_Bottleneck_Models_for_Interpretable_Continual_Learning_CVPR_2025_paper.html": {
    "title": "Language Guided Concept Bottleneck Models for Interpretable Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lu Yu",
      "Haoyu Han",
      "Zhe Tao",
      "Hantao Yao",
      "Changsheng Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_One-Way_Ticket_Time-Independent_Unified_Encoder_for_Distilling_Text-to-Image_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "One-Way Ticket: Time-Independent Unified Encoder for Distilling Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Senmao Li",
      "Lei Wang",
      "Kai Wang",
      "Tao Liu",
      "Jiehang Xie",
      "Joost van de Weijer",
      "Fahad Shahbaz Khan",
      "Shiqi Yang",
      "Yaxing Wang",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Su_Domain_Adaptive_Diabetic_Retinopathy_Grading_with_Model_Absence_and_Flowing_CVPR_2025_paper.html": {
    "title": "Domain Adaptive Diabetic Retinopathy Grading with Model Absence and Flowing Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxin Su",
      "Song Tang",
      "Xiaofeng Liu",
      "Xiaojing Yi",
      "Mao Ye",
      "Chunxiao Zu",
      "Jiahao Li",
      "Xiatian Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Temporal_Separation_with_Entropy_Regularization_for_Knowledge_Distillation_in_Spiking_CVPR_2025_paper.html": {
    "title": "Temporal Separation with Entropy Regularization for Knowledge Distillation in Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kairong Yu",
      "Chengting Yu",
      "Tianqing Zhang",
      "Xiaochen Zhao",
      "Shu Yang",
      "Hongwei Wang",
      "Qiang Zhang",
      "Qi Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_LoRASculpt_Sculpting_LoRA_for_Harmonizing_General_and_Specialized_Knowledge_in_CVPR_2025_paper.html": {
    "title": "LoRASculpt: Sculpting LoRA for Harmonizing General and Specialized Knowledge in Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Liang",
      "Wenke Huang",
      "Guancheng Wan",
      "Qu Yang",
      "Mang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SEAL_Semantic_Attention_Learning_for_Long_Video_Representation_CVPR_2025_paper.html": {
    "title": "SEAL: Semantic Attention Learning for Long Video Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lan Wang",
      "Yujia Chen",
      "Du Tran",
      "Vishnu Naresh Boddeti",
      "Wen-Sheng Chu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_Re-HOLD_Video_Hand_Object_Interaction_Reenactment_via_adaptive_Layout-instructed_Diffusion_CVPR_2025_paper.html": {
    "title": "Re-HOLD: Video Hand Object Interaction Reenactment via adaptive Layout-instructed Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingying Fan",
      "Quanwei Yang",
      "Kaisiyuan Wang",
      "Hang Zhou",
      "Yingying Li",
      "Haocheng Feng",
      "Errui Ding",
      "Yu Wu",
      "Jingdong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_Theoretical_Insights_in_Model_Inversion_Robustness_and_Conditional_Entropy_Maximization_CVPR_2025_paper.html": {
    "title": "Theoretical Insights in Model Inversion Robustness and Conditional Entropy Maximization for Collaborative Inference Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Song Xia",
      "Yi Yu",
      "Wenhan Yang",
      "Meiwen Ding",
      "Zhuo Chen",
      "Ling-Yu Duan",
      "Alex C. Kot",
      "Xudong Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bhunia_Odd-One-Out_Anomaly_Detection_by_Comparing_with_Neighbors_CVPR_2025_paper.html": {
    "title": "Odd-One-Out: Anomaly Detection by Comparing with Neighbors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ankan Bhunia",
      "Changjian Li",
      "Hakan Bilen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SCFlow2_Plug-and-Play_Object_Pose_Refiner_with_Shape-Constraint_Scene_Flow_CVPR_2025_paper.html": {
    "title": "SCFlow2: Plug-and-Play Object Pose Refiner with Shape-Constraint Scene Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyuan Wang",
      "Rui Song",
      "Jiaojiao Li",
      "Kerui Cheng",
      "David Ferstl",
      "Yinlin Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_D3CTTA_Domain-Dependent_Decorrelation_for_Continual_Test-Time_Adaption_of_3D_LiDAR_CVPR_2025_paper.html": {
    "title": "D^3CTTA: Domain-Dependent Decorrelation for Continual Test-Time Adaption of 3D LiDAR Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jichun Zhao",
      "Haiyong Jiang",
      "Haoxuan Song",
      "Jun Xiao",
      "Dong Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Flowing_from_Words_to_Pixels_A_Noise-Free_Framework_for_Cross-Modality_CVPR_2025_paper.html": {
    "title": "Flowing from Words to Pixels: A Noise-Free Framework for Cross-Modality Evolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihao Liu",
      "Xi Yin",
      "Alan Yuille",
      "Andrew Brown",
      "Mannat Singh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bandyopadhyay_FlipSketch_Flipping_Static_Drawings_to_Text-Guided_Sketch_Animations_CVPR_2025_paper.html": {
    "title": "FlipSketch: Flipping Static Drawings to Text-Guided Sketch Animations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hmrishav Bandyopadhyay",
      "Yi-Zhe Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kulkarni_Interpretable_Generative_Models_through_Post-hoc_Concept_Bottlenecks_CVPR_2025_paper.html": {
    "title": "Interpretable Generative Models through Post-hoc Concept Bottlenecks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Akshay Kulkarni",
      "Ge Yan",
      "Chung-En Sun",
      "Tuomas Oikarinen",
      "Tsui-Wei Weng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Vinker_SketchAgent_Language-Driven_Sequential_Sketch_Generation_CVPR_2025_paper.html": {
    "title": "SketchAgent: Language-Driven Sequential Sketch Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yael Vinker",
      "Tamar Rott Shaham",
      "Kristine Zheng",
      "Alex Zhao",
      "Judith E Fan",
      "Antonio Torralba"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_DRAWER_Digital_Reconstruction_and_Articulation_With_Environment_Realism_CVPR_2025_paper.html": {
    "title": "DRAWER: Digital Reconstruction and Articulation With Environment Realism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongchi Xia",
      "Entong Su",
      "Marius Memmel",
      "Arhan Jain",
      "Raymond Yu",
      "Numfor Mbiziwo-Tiapo",
      "Ali Farhadi",
      "Abhishek Gupta",
      "Shenlong Wang",
      "Wei-Chiu Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_GoLF-NRT_Integrating_Global_Context_and_Local_Geometry_for_Few-Shot_View_CVPR_2025_paper.html": {
    "title": "GoLF-NRT: Integrating Global Context and Local Geometry for Few-Shot View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "You Wang",
      "Li Fang",
      "Hao Zhu",
      "Fei Hu",
      "Long Ye",
      "Zhan Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Deep_Change_Monitoring_A_Hyperbolic_Representative_Learning_Framework_and_a_CVPR_2025_paper.html": {
    "title": "Deep Change Monitoring: A Hyperbolic Representative Learning Framework and a Dataset for Long-term Fine-grained Tree Change Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yante Li",
      "Hanwen Qi",
      "Haoyu Chen",
      "Xinlian Liang",
      "Guoying Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_A_Closer_Look_at_Time_Steps_is_Worthy_of_Triple_CVPR_2025_paper.html": {
    "title": "A Closer Look at Time Steps is Worthy of Triple Speed-Up for Diffusion Model Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Wang",
      "Mingjia Shi",
      "Yukun Zhou",
      "Zekai Li",
      "Zhihang Yuan",
      "Yuzhang Shang",
      "Xiaojiang Peng",
      "Hanwang Zhang",
      "Yang You"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xing_Empowering_LLMs_to_Understand_and_Generate_Complex_Vector_Graphics_CVPR_2025_paper.html": {
    "title": "Empowering LLMs to Understand and Generate Complex Vector Graphics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ximing Xing",
      "Juncheng Hu",
      "Guotao Liang",
      "Jing Zhang",
      "Dong Xu",
      "Qian Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhai_PanoGS_Gaussian-based_Panoptic_Segmentation_for_3D_Open_Vocabulary_Scene_Understanding_CVPR_2025_paper.html": {
    "title": "PanoGS: Gaussian-based Panoptic Segmentation for 3D Open Vocabulary Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongjia Zhai",
      "Hai Li",
      "Zhenzhe Li",
      "Xiaokun Pan",
      "Yijia He",
      "Guofeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Watermarking_One_for_All_A_Robust_Watermarking_Scheme_Against_Partial_CVPR_2025_paper.html": {
    "title": "Watermarking One for All: A Robust Watermarking Scheme Against Partial Image Theft",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaozhi Liu",
      "Silu Cao",
      "Zhenxing Qian",
      "Xinpeng Zhang",
      "Sheng Li",
      "Wanli Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hong_ITA-MDT_Image-Timestep-Adaptive_Masked_Diffusion_Transformer_Framework_for_Image-Based_Virtual_Try-On_CVPR_2025_paper.html": {
    "title": "ITA-MDT: Image-Timestep-Adaptive Masked Diffusion Transformer Framework for Image-Based Virtual Try-On",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ji Woo Hong",
      "Tri Ton",
      "Trung X. Pham",
      "Gwanhyeong Koo",
      "Sunjae Yoon",
      "Chang D. Yoo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kriz_MultiVENT_2.0_A_Massive_Multilingual_Benchmark_for_Event-Centric_Video_Retrieval_CVPR_2025_paper.html": {
    "title": "MultiVENT 2.0: A Massive Multilingual Benchmark for Event-Centric Video Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reno Kriz",
      "Kate Sanders",
      "David Etter",
      "Kenton Murray",
      "Cameron Carpenter",
      "Hannah Recknor",
      "Jimena Guallar-Blasco",
      "Alexander Martin",
      "Eugene Yang",
      "Benjamin Van Durme"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_VolFormer_Explore_More_Comprehensive_Cube_Interaction_for_Hyperspectral_Image_Restoration_CVPR_2025_paper.html": {
    "title": "VolFormer: Explore More Comprehensive Cube Interaction for Hyperspectral Image Restoration and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dabing Yu",
      "Zheng Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_Minding_Fuzzy_Regions_A_Data-driven_Alternating_Learning_Paradigm_for_Stable_CVPR_2025_paper.html": {
    "title": "Minding Fuzzy Regions: A Data-driven Alternating Learning Paradigm for Stable Lesion Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lexin Fang",
      "Yunyang Xu",
      "Xiang Ma",
      "Xuemei Li",
      "Caiming Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peng_BizGen_Advancing_Article-level_Visual_Text_Rendering_for_Infographics_Generation_CVPR_2025_paper.html": {
    "title": "BizGen: Advancing Article-level Visual Text Rendering for Infographics Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyang Peng",
      "Shishi Xiao",
      "Keming Wu",
      "Qisheng Liao",
      "Bohan Chen",
      "Kevin Lin",
      "Danqing Huang",
      "Ji Li",
      "Yuhui Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_MLVU_Benchmarking_Multi-task_Long_Video_Understanding_CVPR_2025_paper.html": {
    "title": "MLVU: Benchmarking Multi-task Long Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Zhou",
      "Yan Shu",
      "Bo Zhao",
      "Boya Wu",
      "Zhengyang Liang",
      "Shitao Xiao",
      "Minghao Qin",
      "Xi Yang",
      "Yongping Xiong",
      "Bo Zhang",
      "Tiejun Huang",
      "Zheng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Recovering_Dynamic_3D_Sketches_from_Videos_CVPR_2025_paper.html": {
    "title": "Recovering Dynamic 3D Sketches from Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaeah Lee",
      "Changwoon Choi",
      "Young Min Kim",
      "Jaesik Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_IM-Zero_Instance-level_Motion_Controllable_Video_Generation_in_a_Zero-shot_Manner_CVPR_2025_paper.html": {
    "title": "IM-Zero: Instance-level Motion Controllable Video Generation in a Zero-shot Manner",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuyang Huang",
      "Yabo Chen",
      "Li Ding",
      "Xiaopeng Zhang",
      "Wenrui Dai",
      "Junni Zou",
      "Hongkai Xiong",
      "Qi Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tai_EigenGS_Representation_From_Eigenspace_to_Gaussian_Image_Space_CVPR_2025_paper.html": {
    "title": "EigenGS Representation: From Eigenspace to Gaussian Image Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lo-Wei Tai",
      "Ching-En Li",
      "Cheng-Lin Chen",
      "Chih-Jung Tsai",
      "Hwann-Tzong Chen",
      "Tyng-Luh Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Link-based_Contrastive_Learning_for_One-Shot_Unsupervised_Domain_Adaptation_CVPR_2025_paper.html": {
    "title": "Link-based Contrastive Learning for One-Shot Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Zhang",
      "Mingyue Bin",
      "Yuyang Zhang",
      "Zhongyuan Wang",
      "Zhen Han",
      "Chao Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_SmartCLIP_Modular_Vision-language_Alignment_with_Identification_Guarantees_CVPR_2025_paper.html": {
    "title": "SmartCLIP: Modular Vision-language Alignment with Identification Guarantees",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaoan Xie",
      "Lingjing Lingjing",
      "Yujia Zheng",
      "Yu Yao",
      "Zeyu Tang",
      "Eric P. Xing",
      "Guangyi Chen",
      "Kun Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jin_UniMamba_Unified_Spatial-Channel_Representation_Learning_with_Group-Efficient_Mamba_for_LiDAR-based_CVPR_2025_paper.html": {
    "title": "UniMamba: Unified Spatial-Channel Representation Learning with Group-Efficient Mamba for LiDAR-based 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Jin",
      "Haisheng Su",
      "Kai Liu",
      "Cong Ma",
      "Wei Wu",
      "Fei HUI",
      "Junchi Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_MaSS13K_A_Matting-level_Semantic_Segmentation_Benchmark_CVPR_2025_paper.html": {
    "title": "MaSS13K: A Matting-level Semantic Segmentation Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxi Xie",
      "Minghan Li",
      "Hui Zeng",
      "Jun Luo",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Rethinking_the_Adversarial_Robustness_of_Multi-Exit_Neural_Networks_in_an_CVPR_2025_paper.html": {
    "title": "Rethinking the Adversarial Robustness of Multi-Exit Neural Networks in an Attack-Defense Game",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keyizhi Xu",
      "Chi Zhang",
      "Zhan Chen",
      "Zhongyuan Wang",
      "Chunxia Xiao",
      "Chao Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Enhancing_Testing-Time_Robustness_for_Trusted_Multi-View_Classification_in_the_Wild_CVPR_2025_paper.html": {
    "title": "Enhancing Testing-Time Robustness for Trusted Multi-View Classification in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Liu",
      "Yufei Chen",
      "Xiaodong Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Q-DiT_Accurate_Post-Training_Quantization_for_Diffusion_Transformers_CVPR_2025_paper.html": {
    "title": "Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Chen",
      "Yuan Meng",
      "Chen Tang",
      "Xinzhu Ma",
      "Jingyan Jiang",
      "Xin Wang",
      "Zhi Wang",
      "Wenwu Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_ROD-MLLM_Towards_More_Reliable_Object_Detection_in_Multimodal_Large_Language_CVPR_2025_paper.html": {
    "title": "ROD-MLLM: Towards More Reliable Object Detection in Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heng Yin",
      "Yuqiang Ren",
      "Ke Yan",
      "Shouhong Ding",
      "Yongtao Hao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_RoboGround_Robotic_Manipulation_with_Grounded_Vision-Language_Priors_CVPR_2025_paper.html": {
    "title": "RoboGround: Robotic Manipulation with Grounded Vision-Language Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haifeng Huang",
      "Xinyi Chen",
      "Yilun Chen",
      "Hao Li",
      "Xiaoshen Han",
      "Zehan Wang",
      "Tai Wang",
      "Jiangmiao Pang",
      "Zhou Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_VideoGuide_Improving_Video_Diffusion_Models_without_Training_Through_a_Teachers_CVPR_2025_paper.html": {
    "title": "VideoGuide: Improving Video Diffusion Models without Training Through a Teacher's Guide",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dohun Lee",
      "Bryan Sangwoo Kim",
      "Geon Yeong Park",
      "Jong Chul Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Improving_Transferable_Targeted_Attacks_with_Feature_Tuning_Mixup_CVPR_2025_paper.html": {
    "title": "Improving Transferable Targeted Attacks with Feature Tuning Mixup",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaisheng Liang",
      "Xuelong Dai",
      "Yanjie Li",
      "Dong Wang",
      "Bin Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_OmniStereo_Real-time_Omnidireactional_Depth_Estimation_with_Multiview_Fisheye_Cameras_CVPR_2025_paper.html": {
    "title": "OmniStereo: Real-time Omnidireactional Depth Estimation with Multiview Fisheye Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxi Deng",
      "Yushen Wang",
      "Haitao Meng",
      "Zuoxun Hou",
      "Yi Chang",
      "Gang Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_DroneSplat_3D_Gaussian_Splatting_for_Robust_3D_Reconstruction_from_In-the-Wild_CVPR_2025_paper.html": {
    "title": "DroneSplat: 3D Gaussian Splatting for Robust 3D Reconstruction from In-the-Wild Drone Imagery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiadong Tang",
      "Yu Gao",
      "Dianyi Yang",
      "Liqi Yan",
      "Yufeng Yue",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Duan_SDGOCC_Semantic_and_Depth-Guided_Birds-Eye_View_Transformation_for_3D_Multimodal_CVPR_2025_paper.html": {
    "title": "SDGOCC: Semantic and Depth-Guided Bird's-Eye View Transformation for 3D Multimodal Occupancy Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "ZaiPeng Duan",
      "ChenXu Dang",
      "Xuzhong Hu",
      "Pei An",
      "Junfeng Ding",
      "Jie Zhan",
      "YunBiao Xu",
      "Jie Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_DrivingSphere_Building_a_High-fidelity_4D_World_for_Closed-loop_Simulation_CVPR_2025_paper.html": {
    "title": "DrivingSphere: Building a High-fidelity 4D World for Closed-loop Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Yan",
      "Dongming Wu",
      "Wencheng Han",
      "Junpeng Jiang",
      "Xia Zhou",
      "Kun Zhan",
      "Cheng-zhong Xu",
      "Jianbing Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_nnWNet_Rethinking_the_Use_of_Transformers_in_Biomedical_Image_Segmentation_CVPR_2025_paper.html": {
    "title": "nnWNet: Rethinking the Use of Transformers in Biomedical Image Segmentation and Calling for a Unified Evaluation Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanfeng Zhou",
      "Lingrui Li",
      "Le Lu",
      "Minfeng Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Efficient_Video_Face_Enhancement_with_Enhanced_Spatial-Temporal_Consistency_CVPR_2025_paper.html": {
    "title": "Efficient Video Face Enhancement with Enhanced Spatial-Temporal Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutong Wang",
      "Jiajie Teng",
      "Jiajiong Cao",
      "Yuming Li",
      "Chenguang Ma",
      "Hongteng Xu",
      "Dixin Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Saravanan_VELOCITI_Benchmarking_Video-Language_Compositional_Reasoning_with_Strict_Entailment_CVPR_2025_paper.html": {
    "title": "VELOCITI: Benchmarking Video-Language Compositional Reasoning with Strict Entailment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Darshana Saravanan",
      "Varun Gupta",
      "Darshan Singh",
      "Zeeshan Khan",
      "Vineet Gandhi",
      "Makarand Tapaswi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Seeing_is_Not_Believing_Adversarial_Natural_Object_Optimization_for_Hard-Label_CVPR_2025_paper.html": {
    "title": "Seeing is Not Believing: Adversarial Natural Object Optimization for Hard-Label 3D Scene Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daizong Liu",
      "Wei Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Song_IDProtector_An_Adversarial_Noise_Encoder_to_Protect_Against_ID-Preserving_Image_CVPR_2025_paper.html": {
    "title": "IDProtector: An Adversarial Noise Encoder to Protect Against ID-Preserving Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiren Song",
      "Pei Yang",
      "Hai Ci",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_HuPerFlow_A_Comprehensive_Benchmark_for_Human_vs._Machine_Motion_Estimation_CVPR_2025_paper.html": {
    "title": "HuPerFlow: A Comprehensive Benchmark for Human vs. Machine Motion Estimation Comparison",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yung-Hao Yang",
      "Zitang Sun",
      "Taiki Fukiage",
      "Shin'ya Nishida"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Spartalis_LoTUS_Large-Scale_Machine_Unlearning_with_a_Taste_of_Uncertainty_CVPR_2025_paper.html": {
    "title": "LoTUS: Large-Scale Machine Unlearning with a Taste of Uncertainty",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christoforos N. Spartalis",
      "Theodoros Semertzidis",
      "Efstratios Gavves",
      "Petros Daras"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SleeperMark_Towards_Robust_Watermark_against_Fine-Tuning_Text-to-image_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "SleeperMark: Towards Robust Watermark against Fine-Tuning Text-to-image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zilan Wang",
      "Junfeng Guo",
      "Jiacheng Zhu",
      "Yiming Li",
      "Heng Huang",
      "Muhao Chen",
      "Zhengzhong Tu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mai_Lessons_and_Insights_from_a_Unifying_Study_of_Parameter-Efficient_Fine-Tuning_CVPR_2025_paper.html": {
    "title": "Lessons and Insights from a Unifying Study of Parameter-Efficient Fine-Tuning (PEFT) in Visual Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheda Mai",
      "Ping Zhang",
      "Cheng-Hao Tu",
      "Hong-You Chen",
      "Quang-Huy Nguyen",
      "Li Zhang",
      "Wei-Lun Chao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kant_Pippo_High-Resolution_Multi-View_Humans_from_a_Single_Image_CVPR_2025_paper.html": {
    "title": "Pippo: High-Resolution Multi-View Humans from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yash Kant",
      "Ethan Weber",
      "Jin Kyu Kim",
      "Rawal Khirodkar",
      "Su Zhaoen",
      "Julieta Martinez",
      "Igor Gilitschenski",
      "Shunsuke Saito",
      "Timur Bagautdinov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_H2ST_Hierarchical_Two-Sample_Tests_for_Continual_Out-of-Distribution_Detection_CVPR_2025_paper.html": {
    "title": "H2ST: Hierarchical Two-Sample Tests for Continual Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhang Liu",
      "Wenjie Zhao",
      "Yunhui Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gu_MetaWriter_Personalized_Handwritten_Text_Recognition_Using_Meta-Learned_Prompt_Tuning_CVPR_2025_paper.html": {
    "title": "MetaWriter: Personalized Handwritten Text Recognition Using Meta-Learned Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhao Gu",
      "Li Gu",
      "Chingyee Yee Suen",
      "Yang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jeon_Subnet-Aware_Dynamic_Supernet_Training_for_Neural_Architecture_Search_CVPR_2025_paper.html": {
    "title": "Subnet-Aware Dynamic Supernet Training for Neural Architecture Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeimin Jeon",
      "Youngmin Oh",
      "Junghyup Lee",
      "Donghyeon Baek",
      "Dohyung Kim",
      "Chanho Eom",
      "Bumsub Ham"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cao_MoVE-KD_Knowledge_Distillation_for_VLMs_with_Mixture_of_Visual_Encoders_CVPR_2025_paper.html": {
    "title": "MoVE-KD: Knowledge Distillation for VLMs with Mixture of Visual Encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiajun Cao",
      "Yuan Zhang",
      "Tao Huang",
      "Ming Lu",
      "Qizhe Zhang",
      "Ruichuan An",
      "Ningning Ma",
      "Shanghang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yuan_CamFreeDiff_Camera-free_Image_to_Panorama_Generation_with_Diffusion_Model_CVPR_2025_paper.html": {
    "title": "CamFreeDiff: Camera-free Image to Panorama Generation with Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoding Yuan",
      "Shitao Tang",
      "Kejie Li",
      "Peng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gu_Improving_Visual_and_Downstream_Performance_of_Low-Light_Enhancer_with_Vision_CVPR_2025_paper.html": {
    "title": "Improving Visual and Downstream Performance of Low-Light Enhancer with Vision Foundation Models Collaboration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Gu",
      "Haoxuan Wang",
      "Pengyang Ling",
      "Zhixiang Wei",
      "Huaian Chen",
      "Yi Jin",
      "Enhong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yue_EchoWorld_Learning_Motion-Aware_World_Models_for_Echocardiography_Probe_Guidance_CVPR_2025_paper.html": {
    "title": "EchoWorld: Learning Motion-Aware World Models for Echocardiography Probe Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Yue",
      "Yulin Wang",
      "Haojun Jiang",
      "Pan Liu",
      "Shiji Song",
      "Gao Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Choi_Controllable_Human_Image_Generation_with_Personalized_Multi-Garments_CVPR_2025_paper.html": {
    "title": "Controllable Human Image Generation with Personalized Multi-Garments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yisol Choi",
      "Sangkyung Kwak",
      "Sihyun Yu",
      "Hyungwon Choi",
      "Jinwoo Shin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Asokan_FineLIP_Extending_CLIPs_Reach_via_Fine-Grained_Alignment_with_Longer_Text_CVPR_2025_paper.html": {
    "title": "FineLIP: Extending CLIP's Reach via Fine-Grained Alignment with Longer Text Inputs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mothilal Asokan",
      "Kebin Wu",
      "Fatima Albreiki"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Oh_Illumination_Spectrum_Estimation_for_Multispectral_Images_via_Surface_Reflectance_Modeling_CVPR_2025_paper.html": {
    "title": "Illumination Spectrum Estimation for Multispectral Images via Surface Reflectance Modeling and Spatial-Spectral Feature Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyejin Oh",
      "Woo-Shik Kim",
      "Sangyoon Lee",
      "YungKyung Park",
      "Je-Won Kang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_UHD-processer_Unified_UHD_Image_Restoration_with_Progressive_Frequency_Learning_and_CVPR_2025_paper.html": {
    "title": "UHD-processer: Unified UHD Image Restoration with Progressive Frequency Learning and Degradation-aware Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yidi Liu",
      "Dong Li",
      "Xueyang Fu",
      "Xin Lu",
      "Jie Huang",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ge_Divot_Diffusion_Powers_Video_Tokenizer_for_Comprehension_and_Generation_CVPR_2025_paper.html": {
    "title": "Divot: Diffusion Powers Video Tokenizer for Comprehension and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuying Ge",
      "Yizhuo Li",
      "Yixiao Ge",
      "Ying Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Towards_Zero-Shot_Anomaly_Detection_and_Reasoning_with_Multimodal_Large_Language_CVPR_2025_paper.html": {
    "title": "Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacong Xu",
      "Shao-Yuan Lo",
      "Bardia Safaei",
      "Vishal M. Patel",
      "Isht Dwivedi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Towards_Explainable_and_Unprecedented_Accuracy_in_Matching_Challenging_Finger_Crease_CVPR_2025_paper.html": {
    "title": "Towards Explainable and Unprecedented Accuracy in Matching Challenging Finger Crease Patterns",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Zhou",
      "Chengdong Dong",
      "Ajay Kumar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Neural_Hierarchical_Decomposition_for_Single_Image_Plant_Modeling_CVPR_2025_paper.html": {
    "title": "Neural Hierarchical Decomposition for Single Image Plant Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Liu",
      "Zhanglin Cheng",
      "Naoto Yokoya"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tu_GBC-Splat_Generalizable_Gaussian-Based_Clothed_Human_Digitalization_under_Sparse_RGB_Cameras_CVPR_2025_paper.html": {
    "title": "GBC-Splat: Generalizable Gaussian-Based Clothed Human Digitalization under Sparse RGB Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanzhang Tu",
      "Zhanfeng Liao",
      "Boyao Zhou",
      "Shunyuan Zheng",
      "Xilong Zhou",
      "Liuxin Zhang",
      "QianYing Wang",
      "Yebin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bahmani_AC3D_Analyzing_and_Improving_3D_Camera_Control_in_Video_Diffusion_CVPR_2025_paper.html": {
    "title": "AC3D: Analyzing and Improving 3D Camera Control in Video Diffusion Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sherwin Bahmani",
      "Ivan Skorokhodov",
      "Guocheng Qian",
      "Aliaksandr Siarohin",
      "Willi Menapace",
      "Andrea Tagliasacchi",
      "David B. Lindell",
      "Sergey Tulyakov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jatyani_A_Unified_Model_for_Compressed_Sensing_MRI_Across_Undersampling_Patterns_CVPR_2025_paper.html": {
    "title": "A Unified Model for Compressed Sensing MRI Across Undersampling Patterns",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armeet Singh Jatyani",
      "Jiayun Wang",
      "Aditi Chandrashekar",
      "Zihui Wu",
      "Miguel Liu-Schiaffini",
      "Bahareh Tolooshams",
      "Anima Anandkumar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Video-Guided_Foley_Sound_Generation_with_Multimodal_Controls_CVPR_2025_paper.html": {
    "title": "Video-Guided Foley Sound Generation with Multimodal Controls",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Chen",
      "Prem Seetharaman",
      "Bryan Russell",
      "Oriol Nieto",
      "David Bourgin",
      "Andrew Owens",
      "Justin Salamon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Dual-Agent_Optimization_framework_for_Cross-Domain_Few-Shot_Segmentation_CVPR_2025_paper.html": {
    "title": "Dual-Agent Optimization framework for Cross-Domain Few-Shot Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyang Li",
      "Yuan Wang",
      "Wangkai Li",
      "Tianzhu Zhang",
      "Xiang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_SACB-Net_Spatial-awareness_Convolutions_for_Medical_Image_Registration_CVPR_2025_paper.html": {
    "title": "SACB-Net: Spatial-awareness Convolutions for Medical Image Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinxing Cheng",
      "Tianyang Zhang",
      "Wenqi Lu",
      "Qingjie Meng",
      "Alejandro F. Frangi",
      "Jinming Duan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Text_Embedding_is_Not_All_You_Need_Attention_Control_for_CVPR_2025_paper.html": {
    "title": "Text Embedding is Not All You Need: Attention Control for Text-to-Image Semantic Alignment with Text Self-Attention Maps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeeyung Kim",
      "Erfan Esmaeili",
      "Qiang Qiu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_DCEvo_Discriminative_Cross-Dimensional_Evolutionary_Learning_for_Infrared_and_Visible_Image_CVPR_2025_paper.html": {
    "title": "DCEvo: Discriminative Cross-Dimensional Evolutionary Learning for Infrared and Visible Image Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinyuan Liu",
      "Bowei Zhang",
      "Qingyun Mei",
      "Xingyuan Li",
      "Yang Zou",
      "Zhiying Jiang",
      "Long Ma",
      "Risheng Liu",
      "Xin Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dong_TSD-SR_One-Step_Diffusion_with_Target_Score_Distillation_for_Real-World_Image_CVPR_2025_paper.html": {
    "title": "TSD-SR: One-Step Diffusion with Target Score Distillation for Real-World Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linwei Dong",
      "Qingnan Fan",
      "Yihong Guo",
      "Zhonghao Wang",
      "Qi Zhang",
      "Jinwei Chen",
      "Yawei Luo",
      "Changqing Zou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nakayama_AIpparel_A_Multimodal_Foundation_Model_for_Digital_Garments_CVPR_2025_paper.html": {
    "title": "AIpparel: A Multimodal Foundation Model for Digital Garments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kiyohiro Nakayama",
      "Jan Ackermann",
      "Timur Levent Kesdogan",
      "Yang Zheng",
      "Maria Korosteleva",
      "Olga Sorkine-Hornung",
      "Leonidas J. Guibas",
      "Guandao Yang",
      "Gordon Wetzstein"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Fast3R_Towards_3D_Reconstruction_of_1000_Images_in_One_Forward_CVPR_2025_paper.html": {
    "title": "Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianing Yang",
      "Alexander Sax",
      "Kevin J. Liang",
      "Mikael Henaff",
      "Hao Tang",
      "Ang Cao",
      "Joyce Chai",
      "Franziska Meier",
      "Matt Feiszli"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lei_StyleStudio_Text-Driven_Style_Transfer_with_Selective_Control_of_Style_Elements_CVPR_2025_paper.html": {
    "title": "StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingkun Lei",
      "Xue Song",
      "Beier Zhu",
      "Hao Wang",
      "Chi Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Didolkar_CTRL-O_Language-Controllable_Object-Centric_Visual_Representation_Learning_CVPR_2025_paper.html": {
    "title": "CTRL-O: Language-Controllable Object-Centric Visual Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aniket Didolkar",
      "Andrii Zadaianchuk",
      "Rabiul Awal",
      "Maximilian Seitzer",
      "Efstratios Gavves",
      "Aishwarya Agrawal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_PO3AD_Predicting_Point_Offsets_toward_Better_3D_Point_Cloud_Anomaly_CVPR_2025_paper.html": {
    "title": "PO3AD: Predicting Point Offsets toward Better 3D Point Cloud Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianan Ye",
      "Weiguang Zhao",
      "Xi Yang",
      "Guangliang Cheng",
      "Kaizhu Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nandam_Text_Augmented_Correlation_Transformer_For_Few-shot_Classification__Segmentation_CVPR_2025_paper.html": {
    "title": "Text Augmented Correlation Transformer For Few-shot Classification & Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Srinivasa Rao Nandam",
      "Sara Atito",
      "Zhenhua Feng",
      "Josef Kittler",
      "Muhammad Awais"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal_CVPR_2025_paper.html": {
    "title": "F^3OCUS - Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Updating Strategy via Multi-objective Meta-Heuristics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pramit Saha",
      "Felix Wagner",
      "Divyanshu Mishra",
      "Can Peng",
      "Anshul Thakur",
      "David A. Clifton",
      "Konstantinos Kamnitsas",
      "J. Alison Noble"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_ICT_Image-Object_Cross-Level_Trusted_Intervention_for_Mitigating_Object_Hallucination_in_CVPR_2025_paper.html": {
    "title": "ICT: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junzhe Chen",
      "Tianshu Zhang",
      "Shiyu Huang",
      "Yuwei Niu",
      "Linfeng Zhang",
      "Lijie Wen",
      "Xuming Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bernal-Berdun_PreciseCam_Precise_Camera_Control_for_Text-to-Image_Generation_CVPR_2025_paper.html": {
    "title": "PreciseCam: Precise Camera Control for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Edurne Bernal-Berdun",
      "Ana Serrano",
      "Belen Masia",
      "Matheus Gadelha",
      "Yannick Hold-Geoffroy",
      "Xin Sun",
      "Diego Gutierrez"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Oh_3D_Occupancy_Prediction_with_Low-Resolution_Queries_via_Prototype-aware_View_Transformation_CVPR_2025_paper.html": {
    "title": "3D Occupancy Prediction with Low-Resolution Queries via Prototype-aware View Transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gyeongrok Oh",
      "Sungjune Kim",
      "Heeju Ko",
      "Hyung-gun Chi",
      "Jinkyu Kim",
      "Dongwook Lee",
      "Daehyun Ji",
      "Sungjoon Choi",
      "Sujin Jang",
      "Sangpil Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Unified_Dense_Prediction_of_Video_Diffusion_CVPR_2025_paper.html": {
    "title": "Unified Dense Prediction of Video Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lehan Yang",
      "Lu Qi",
      "Xiangtai Li",
      "Sheng Li",
      "Varun Jampani",
      "Ming-Hsuan Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liao_Can_Large_Vision-Language_Models_Correct_Semantic_Grounding_Errors_By_Themselves_CVPR_2025_paper.html": {
    "title": "Can Large Vision-Language Models Correct Semantic Grounding Errors By Themselves?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan-Hong Liao",
      "Rafid Mahmood",
      "Sanja Fidler",
      "David Acuna"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_SET_Spectral_Enhancement_for_Tiny_Object_Detection_CVPR_2025_paper.html": {
    "title": "SET: Spectral Enhancement for Tiny Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huixin Sun",
      "Runqi Wang",
      "Yanjing Li",
      "Linlin Yang",
      "Shaohui Lin",
      "Xianbin Cao",
      "Baochang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_g3D-LF_Generalizable_3D-Language_Feature_Fields_for_Embodied_Tasks_CVPR_2025_paper.html": {
    "title": "g3D-LF: Generalizable 3D-Language Feature Fields for Embodied Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Wang",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Towards_Million-Scale_Adversarial_Robustness_Evaluation_With_Stronger_Individual_Attacks_CVPR_2025_paper.html": {
    "title": "Towards Million-Scale Adversarial Robustness Evaluation With Stronger Individual Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yong Xie",
      "Weijie Zheng",
      "Hanxun Huang",
      "Guangnan Ye",
      "Xingjun Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Temporal_Action_Detection_Model_Compression_by_Progressive_Block_Drop_CVPR_2025_paper.html": {
    "title": "Temporal Action Detection Model Compression by Progressive Block Drop",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyong Chen",
      "Yong Guo",
      "Jiaming Liang",
      "Sitong Zhuang",
      "Runhao Zeng",
      "Xiping Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chung_Differentiable_Inverse_Rendering_with_Interpretable_Basis_BRDFs_CVPR_2025_paper.html": {
    "title": "Differentiable Inverse Rendering with Interpretable Basis BRDFs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoon-Gyu Chung",
      "Seokjun Choi",
      "Seung-Hwan Baek"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_EquiPose_Exploiting_Permutation_Equivariance_for_Relative_Camera_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "EquiPose: Exploiting Permutation Equivariance for Relative Camera Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhen Liu",
      "Qiulei Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Face_Forgery_Video_Detection_via_Temporal_Forgery_Cue_Unraveling_CVPR_2025_paper.html": {
    "title": "Face Forgery Video Detection via Temporal Forgery Cue Unraveling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zonghui Guo",
      "Yingjie Liu",
      "Jie Zhang",
      "Haiyong Zheng",
      "Shiguang Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Manasyan_Temporally_Consistent_Object-Centric_Learning_by_Contrasting_Slots_CVPR_2025_paper.html": {
    "title": "Temporally Consistent Object-Centric Learning by Contrasting Slots",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anna Manasyan",
      "Maximilian Seitzer",
      "Filip Radovic",
      "Georg Martius",
      "Andrii Zadaianchuk"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_MC2_Multi-concept_Guidance_for_Customized__Multi-concept_Generation_CVPR_2025_paper.html": {
    "title": "MC^2: Multi-concept Guidance for Customized Multi-concept Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxiu Jiang",
      "Yabo Zhang",
      "Kailai Feng",
      "Xiaohe Wu",
      "Wenbo Li",
      "Renjing Pei",
      "Fan Li",
      "Wangmeng Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_UniReal_Universal_Image_Generation_and_Editing_via_Learning_Real-world_Dynamics_CVPR_2025_paper.html": {
    "title": "UniReal: Universal Image Generation and Editing via Learning Real-world Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xi Chen",
      "Zhifei Zhang",
      "He Zhang",
      "Yuqian Zhou",
      "Soo Ye Kim",
      "Qing Liu",
      "Yijun Li",
      "Jianming Zhang",
      "Nanxuan Zhao",
      "Yilin Wang",
      "Hui Ding",
      "Zhe Lin",
      "Hengshuang Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Pursuing_Temporal-Consistent_Video_Virtual_Try-On_via_Dynamic_Pose_Interaction_CVPR_2025_paper.html": {
    "title": "Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Li",
      "Wenqi Zhong",
      "Wei Yu",
      "Yingwei Pan",
      "Dingwen Zhang",
      "Ting Yao",
      "Junwei Han",
      "Tao Mei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Exploring_Contextual_Attribute_Density_in_Referring_Expression_Counting_CVPR_2025_paper.html": {
    "title": "Exploring Contextual Attribute Density in Referring Expression Counting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhicheng Wang",
      "Zhiyu Pan",
      "Zhan Peng",
      "Jian Cheng",
      "Liwen Xiao",
      "Wei Jiang",
      "Zhiguo Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jose_DINOv2_Meets_Text_A_Unified_Framework_for_Image-_and_Pixel-Level_CVPR_2025_paper.html": {
    "title": "DINOv2 Meets Text: A Unified Framework for Image- and Pixel-Level Vision-Language Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cijo Jose",
      "Th√©o Moutakanni",
      "Dahyun Kang",
      "Federico Baldassarre",
      "Timoth√©e Darcet",
      "Hu Xu",
      "Daniel Li",
      "Marc Szafraniec",
      "Micha√´l Ramamonjisoa",
      "Maxime Oquab",
      "Oriane Sim√©oni",
      "Huy V. Vo",
      "Patrick Labatut",
      "Piotr Bojanowski"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Learning_Affine_Correspondences_by_Integrating_Geometric_Constraints_CVPR_2025_paper.html": {
    "title": "Learning Affine Correspondences by Integrating Geometric Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengju Sun",
      "Banglei Guan",
      "Zhenbao Yu",
      "Yang Shang",
      "Qifeng Yu",
      "Daniel Barath"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_UCOD-DPL_Unsupervised_Camouflaged_Object_Detection_via_Dynamic_Pseudo-label_Learning_CVPR_2025_paper.html": {
    "title": "UCOD-DPL: Unsupervised Camouflaged Object Detection via Dynamic Pseudo-label Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiqi Yan",
      "Lvhai Chen",
      "Huaijia Kou",
      "Shengchuan Zhang",
      "Yan Zhang",
      "Liujuan Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dinh_Geometry_in_Style_3D_Stylization_via_Surface_Normal_Deformation_CVPR_2025_paper.html": {
    "title": "Geometry in Style: 3D Stylization via Surface Normal Deformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nam Anh Dinh",
      "Itai Lang",
      "Hyunwoo Kim",
      "Oded Stein",
      "Rana Hanocka"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rui_Multi-modal_Vision_Pre-training_for_Medical_Image_Analysis_CVPR_2025_paper.html": {
    "title": "Multi-modal Vision Pre-training for Medical Image Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaohao Rui",
      "Lingzhi Chen",
      "Zhenyu Tang",
      "Lilong Wang",
      "Mianxin Liu",
      "Shaoting Zhang",
      "Xiaosong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_SegMAN_Omni-scale_Context_Modeling_with_State_Space_Models_and_Local_CVPR_2025_paper.html": {
    "title": "SegMAN: Omni-scale Context Modeling with State Space Models and Local Attention for Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunxiang Fu",
      "Meng Lou",
      "Yizhou Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qiu_STEP_Enhancing_Video-LLMs_Compositional_Reasoning_by_Spatio-Temporal_Graph-guided_Self-Training_CVPR_2025_paper.html": {
    "title": "STEP: Enhancing Video-LLMs' Compositional Reasoning by Spatio-Temporal Graph-guided Self-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyi Qiu",
      "Minghe Gao",
      "Long Qian",
      "Kaihang Pan",
      "Qifan Yu",
      "Juncheng Li",
      "Wenjie Wang",
      "Siliang Tang",
      "Yueting Zhuang",
      "Tat-Seng Chua"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_OmniFlow_Any-to-Any_Generation_with_Multi-Modal_Rectified_Flows_CVPR_2025_paper.html": {
    "title": "OmniFlow: Any-to-Any Generation with Multi-Modal Rectified Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shufan Li",
      "Konstantinos Kallidromitis",
      "Akash Gokul",
      "Zichun Liao",
      "Yusuke Kato",
      "Kazuki Kozuka",
      "Aditya Grover"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_PVC_Progressive_Visual_Token_Compression_for_Unified_Image_and_Video_CVPR_2025_paper.html": {
    "title": "PVC: Progressive Visual Token Compression for Unified Image and Video Processing in Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyu Yang",
      "Xuan Dong",
      "Xizhou Zhu",
      "Weijie Su",
      "Jiahao Wang",
      "Hao Tian",
      "Zhe Chen",
      "Wenhai Wang",
      "Lewei Lu",
      "Jifeng Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sabathier_LIM_Large_Interpolator_Model_for_Dynamic_Reconstruction_CVPR_2025_paper.html": {
    "title": "LIM: Large Interpolator Model for Dynamic Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Remy Sabathier",
      "Niloy J. Mitra",
      "David Novotny"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_Multiple_Object_Tracking_as_ID_Prediction_CVPR_2025_paper.html": {
    "title": "Multiple Object Tracking as ID Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruopeng Gao",
      "Ji Qi",
      "Limin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ge_AutoPresent_Designing_Structured_Visuals_from_Scratch_CVPR_2025_paper.html": {
    "title": "AutoPresent: Designing Structured Visuals from Scratch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaxin Ge",
      "Zora Zhiruo Wang",
      "Xuhui Zhou",
      "Yi-Hao Peng",
      "Sanjay Subramanian",
      "Qinyue Tan",
      "Maarten Sap",
      "Alane Suhr",
      "Daniel Fried",
      "Graham Neubig",
      "Trevor Darrell"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_SLAM3R_Real-Time_Dense_Scene_Reconstruction_from_Monocular_RGB_Videos_CVPR_2025_paper.html": {
    "title": "SLAM3R: Real-Time Dense Scene Reconstruction from Monocular RGB Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzheng Liu",
      "Siyan Dong",
      "Shuzhe Wang",
      "Yingda Yin",
      "Yanchao Yang",
      "Qingnan Fan",
      "Baoquan Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_PIDLoc_Cross-View_Pose_Optimization_Network_Inspired_by_PID_Controllers_CVPR_2025_paper.html": {
    "title": "PIDLoc: Cross-View Pose Optimization Network Inspired by PID Controllers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wooju Lee",
      "Juhye Park",
      "Dasol Hong",
      "Changki Sung",
      "Youngwoo Seo",
      "DongWan Kang",
      "Hyun Myung"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chou_VisionArena_230k_Real_World_User-VLM_Conversations_with_Preference_Labels_CVPR_2025_paper.html": {
    "title": "VisionArena: 230k Real World User-VLM Conversations with Preference Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Chou",
      "Lisa Dunlap",
      "Koki Mashita",
      "Krishna Mandal",
      "Trevor Darrell",
      "Ion Stoica",
      "Joseph E. Gonzalez",
      "Wei-Lin Chiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_FAM_Diffusion_Frequency_and_Attention_Modulation_for_High-Resolution_Image_Generation_CVPR_2025_paper.html": {
    "title": "FAM Diffusion: Frequency and Attention Modulation for High-Resolution Image Generation with Stable Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haosen Yang",
      "Adrian Bulat",
      "Isma Hadji",
      "Hai X. Pham",
      "Xiatian Zhu",
      "Georgios Tzimiropoulos",
      "Brais Martinez"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_DreamOmni_Unified_Image_Generation_and_Editing_CVPR_2025_paper.html": {
    "title": "DreamOmni: Unified Image Generation and Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Xia",
      "Yuechen Zhang",
      "Jingyao Li",
      "Chengyao Wang",
      "Yitong Wang",
      "Xinglong Wu",
      "Bei Yu",
      "Jiaya Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Hash3D_Training-free_Acceleration_for_3D_Generation_CVPR_2025_paper.html": {
    "title": "Hash3D: Training-free Acceleration for 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyi Yang",
      "Songhua Liu",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cong_SemGeoMo_Dynamic_Contextual_Human_Motion_Generation_with_Semantic_and_Geometric_CVPR_2025_paper.html": {
    "title": "SemGeoMo: Dynamic Contextual Human Motion Generation with Semantic and Geometric Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peishan Cong",
      "Ziyi Wang",
      "Yuexin Ma",
      "Xiangyu Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_MultiGO_Towards_Multi-level_Geometry_Learning_for_Monocular_3D_Textured_Human_CVPR_2025_paper.html": {
    "title": "MultiGO: Towards Multi-level Geometry Learning for Monocular 3D Textured Human Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gangjian Zhang",
      "Nanjie Yao",
      "Shunsi Zhang",
      "Hanfeng Zhao",
      "Guoliang Pang",
      "Jian Shu",
      "Hao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Generative_Photomontage_CVPR_2025_paper.html": {
    "title": "Generative Photomontage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sean J. Liu",
      "Nupur Kumari",
      "Ariel Shamir",
      "Jun-Yan Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Multi-view_Reconstruction_via_SfM-guided_Monocular_Depth_Estimation_CVPR_2025_paper.html": {
    "title": "Multi-view Reconstruction via SfM-guided Monocular Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Guo",
      "He Zhu",
      "Sida Peng",
      "Haotong Lin",
      "Yunzhi Yan",
      "Tao Xie",
      "Wenguan Wang",
      "Xiaowei Zhou",
      "Hujun Bao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Learning_Hazing_to_Dehazing_Towards_Realistic_Haze_Generation_for_Real-World_CVPR_2025_paper.html": {
    "title": "Learning Hazing to Dehazing: Towards Realistic Haze Generation for Real-World Image Dehazing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiyi Wang",
      "Yushuo Zheng",
      "Zicheng Zhang",
      "Chunyi Li",
      "Shuaicheng Liu",
      "Guangtao Zhai",
      "Xiaohong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_HuMoCon_Concept_Discovery_for_Human_Motion_Understanding_CVPR_2025_paper.html": {
    "title": "HuMoCon: Concept Discovery for Human Motion Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihang Fang",
      "Chengcheng Tang",
      "Bugra Tekin",
      "Shugao Ma",
      "Yanchao Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Loiseau_RUBIK_A_Structured_Benchmark_for_Image_Matching_across_Geometric_Challenges_CVPR_2025_paper.html": {
    "title": "RUBIK: A Structured Benchmark for Image Matching across Geometric Challenges",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thibaut Loiseau",
      "Guillaume Bourmaud"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Fast_and_Accurate_Gigapixel_Pathological_Image_Classification_with_Hierarchical_Distillation_CVPR_2025_paper.html": {
    "title": "Fast and Accurate Gigapixel Pathological Image Classification with Hierarchical Distillation Multi-Instance Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiuyang Dong",
      "Junjun Jiang",
      "Kui Jiang",
      "Jiahan Li",
      "Yongbing Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bai_FreeScene_Mixed_Graph_Diffusion_for_3D_Scene_Synthesis_from_Free_CVPR_2025_paper.html": {
    "title": "FreeScene: Mixed Graph Diffusion for 3D Scene Synthesis from Free Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongyuan Bai",
      "Wangyuanfan Bai",
      "Dong Chen",
      "Tieru Wu",
      "Manyi Li",
      "Rui Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_Rethinking_Correspondence-based_Category-Level_Object_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "Rethinking Correspondence-based Category-Level Object Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huan Ren",
      "Wenfei Yang",
      "Shifeng Zhang",
      "Tianzhu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Croitoru_Curriculum_Direct_Preference_Optimization_for_Diffusion_and_Consistency_Models_CVPR_2025_paper.html": {
    "title": "Curriculum Direct Preference Optimization for Diffusion and Consistency Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Florinel-Alin Croitoru",
      "Vlad Hondru",
      "Radu Tudor Ionescu",
      "Nicu Sebe",
      "Mubarak Shah"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_IncEventGS_Pose-Free_Gaussian_Splatting_from_a_Single_Event_Camera_CVPR_2025_paper.html": {
    "title": "IncEventGS: Pose-Free Gaussian Splatting from a Single Event Camera",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Huang",
      "Chengrui Dong",
      "Xuanhua Chen",
      "Peidong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gutbrod_OpenMIBOOD_Open_Medical_Imaging_Benchmarks_for_Out-Of-Distribution_Detection_CVPR_2025_paper.html": {
    "title": "OpenMIBOOD: Open Medical Imaging Benchmarks for Out-Of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max Gutbrod",
      "David Rauber",
      "Danilo Weber Nunes",
      "Christoph Palm"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Detecting_Open_World_Objects_via_Partial_Attribute_Assignment_CVPR_2025_paper.html": {
    "title": "Detecting Open World Objects via Partial Attribute Assignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muli Yang",
      "Gabriel James Goenawan",
      "Huaiyuan Qin",
      "Kai Han",
      "Xi Peng",
      "Yanhua Yang",
      "Hongyuan Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Heiman_FactCheXcker_Mitigating_Measurement_Hallucinations_in_Chest_X-ray_Report_Generation_Models_CVPR_2025_paper.html": {
    "title": "FactCheXcker: Mitigating Measurement Hallucinations in Chest X-ray Report Generation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alice Heiman",
      "Xiaoman Zhang",
      "Emma Chen",
      "Sung Eun Kim",
      "Pranav Rajpurkar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Malik_Neural_Inverse_Rendering_from_Propagating_Light_CVPR_2025_paper.html": {
    "title": "Neural Inverse Rendering from Propagating Light",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anagh Malik",
      "Benjamin Attal",
      "Andrew Xie",
      "Matthew O'Toole",
      "David B. Lindell"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_When_the_Future_Becomes_the_Past_Taming_Temporal_Correspondence_for_CVPR_2025_paper.html": {
    "title": "When the Future Becomes the Past: Taming Temporal Correspondence for Self-supervised Video Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Liu",
      "Qianqian Xu",
      "Peisong Wen",
      "Siran Dai",
      "Qingming Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dang_Personalized_Preference_Fine-tuning_of_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Personalized Preference Fine-tuning of Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meihua Dang",
      "Anikait Singh",
      "Linqi Zhou",
      "Stefano Ermon",
      "Jiaming Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DecoupledGaussian_Object-Scene_Decoupling_for_Physics-Based_Interaction_CVPR_2025_paper.html": {
    "title": "DecoupledGaussian: Object-Scene Decoupling for Physics-Based Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miaowei Wang",
      "Yibo Zhang",
      "Weiwei Xu",
      "Rui Ma",
      "Changqing Zou",
      "Daniel Morris"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_UniPose_A_Unified_Multimodal_Framework_for_Human_Pose_Comprehension_Generation_CVPR_2025_paper.html": {
    "title": "UniPose: A Unified Multimodal Framework for Human Pose Comprehension, Generation and Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiheng Li",
      "Ruibing Hou",
      "Hong Chang",
      "Shiguang Shan",
      "Xilin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ji_POMP_Physics-consistent_Motion_Generative_Model_through_Phase_Manifolds_CVPR_2025_paper.html": {
    "title": "POMP: Physics-consistent Motion Generative Model through Phase Manifolds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Ji",
      "Ye Pan",
      "Zhimeng Liu",
      "Shuai Tan",
      "Xiaogang Jin",
      "Xiaokang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_NN-Former_Rethinking_Graph_Structure_in_Neural_Architecture_Representation_CVPR_2025_paper.html": {
    "title": "NN-Former: Rethinking Graph Structure in Neural Architecture Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihan Xu",
      "Haokui Zhang",
      "Yaowei Wang",
      "Wei Zeng",
      "Shiliang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_DashGaussian_Optimizing_3D_Gaussian_Splatting_in_200_Seconds_CVPR_2025_paper.html": {
    "title": "DashGaussian: Optimizing 3D Gaussian Splatting in 200 Seconds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youyu Chen",
      "Junjun Jiang",
      "Kui Jiang",
      "Xiao Tang",
      "Zhihao Li",
      "Xianming Liu",
      "Yinyu Nie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qian_Reasoning_to_Attend_Try_to_Understand_How_SEG_Token_Works_CVPR_2025_paper.html": {
    "title": "Reasoning to Attend: Try to Understand How <SEG> Token Works",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Qian",
      "Xin Yin",
      "Dejing Dou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_ReSpec_Relevance_and_Specificity_Grounded_Online_Filtering_for_Learning_on_CVPR_2025_paper.html": {
    "title": "ReSpec: Relevance and Specificity Grounded Online Filtering for Learning on Video-Text Data Streams",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chris Dongjoo Kim",
      "Jihwan Moon",
      "Sangwoo Moon",
      "Heeseung Yun",
      "Sihaeng Lee",
      "Aniruddha Kembhavi",
      "Soonyoung Lee",
      "Gunhee Kim",
      "Sangho Lee",
      "Christopher Clark"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_A_Unified_Image-Dense_Annotation_Generation_Model_for_Underwater_Scenes_CVPR_2025_paper.html": {
    "title": "A Unified Image-Dense Annotation Generation Model for Underwater Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongkai Lin",
      "Dingkang Liang",
      "Zhenghao Qi",
      "Xiang Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dhouib_PACT_Pruning_and_Clustering-Based_Token_Reduction_for_Faster_Visual_Language_CVPR_2025_paper.html": {
    "title": "PACT: Pruning and Clustering-Based Token Reduction for Faster Visual Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Dhouib",
      "Davide Buscaldi",
      "Sonia Vanier",
      "Aymen Shabou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_R-SCoRe_Revisiting_Scene_Coordinate_Regression_for_Robust_Large-Scale_Visual_Localization_CVPR_2025_paper.html": {
    "title": "R-SCoRe: Revisiting Scene Coordinate Regression for Robust Large-Scale Visual Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xudong Jiang",
      "Fangjinhua Wang",
      "Silvano Galliani",
      "Christoph Vogel",
      "Marc Pollefeys"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_DynRefer_Delving_into_Region-level_Multimodal_Tasks_via_Dynamic_Resolution_CVPR_2025_paper.html": {
    "title": "DynRefer: Delving into Region-level Multimodal Tasks via Dynamic Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhong Zhao",
      "Feng Liu",
      "Yue Liu",
      "Mingxiang Liao",
      "Chen Gong",
      "Qixiang Ye",
      "Fang Wan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jeong_Playing_the_Fool_Jailbreaking_LLMs_and_Multimodal_LLMs_with_Out-of-Distribution_CVPR_2025_paper.html": {
    "title": "Playing the Fool: Jailbreaking LLMs and Multimodal LLMs with Out-of-Distribution Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joonhyun Jeong",
      "Seyun Bae",
      "Yeonsung Jung",
      "Jaeryong Hwang",
      "Eunho Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Style_Evolving_along_Chain-of-Thought_for_Unknown-Domain_Object_Detection_CVPR_2025_paper.html": {
    "title": "Style Evolving along Chain-of-Thought for Unknown-Domain Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Zhang",
      "Aming Wu",
      "Yahong Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_NTR-Gaussian_Nighttime_Dynamic_Thermal_Reconstruction_with_4D_Gaussian_Splatting_Based_CVPR_2025_paper.html": {
    "title": "NTR-Gaussian: Nighttime Dynamic Thermal Reconstruction with 4D Gaussian Splatting Based on Thermodynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Yang",
      "Yuxiang Liu",
      "Zeyu Cui",
      "Yu Liu",
      "Maojun Zhang",
      "Shen Yan",
      "Qing Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_OmniSplat_Taming_Feed-Forward_3D_Gaussian_Splatting_for_Omnidirectional_Images_with_CVPR_2025_paper.html": {
    "title": "OmniSplat: Taming Feed-Forward 3D Gaussian Splatting for Omnidirectional Images with Editable Capabilities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suyoung Lee",
      "Jaeyoung Chung",
      "Kihoon Kim",
      "Jaeyoo Huh",
      "Gunhee Lee",
      "Minsoo Lee",
      "Kyoung Mu Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_VideoWorld_Exploring_Knowledge_Learning_from_Unlabeled_Videos_CVPR_2025_paper.html": {
    "title": "VideoWorld: Exploring Knowledge Learning from Unlabeled Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongwei Ren",
      "Yunchao Wei",
      "Xun Guo",
      "Yao Zhao",
      "Bingyi Kang",
      "Jiashi Feng",
      "Xiaojie Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_FSHNet_Fully_Sparse_Hybrid_Network_for_3D_Object_Detection_CVPR_2025_paper.html": {
    "title": "FSHNet: Fully Sparse Hybrid Network for 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Liu",
      "Mingyue Cui",
      "Boyang Li",
      "Quanmin Liang",
      "Tinghe Hong",
      "Kai Huang",
      "Yunxiao Shan",
      "Kai Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_3D-SLNR_A_Super_Lightweight_Neural_Representation_for_Large-scale_3D_Mapping_CVPR_2025_paper.html": {
    "title": "3D-SLNR: A Super Lightweight Neural Representation for Large-scale 3D Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhui Shi",
      "Fulin Tang",
      "Ning An",
      "Yihong Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gu_UniVAD_A_Training-free_Unified_Model_for_Few-shot_Visual_Anomaly_Detection_CVPR_2025_paper.html": {
    "title": "UniVAD: A Training-free Unified Model for Few-shot Visual Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaopeng Gu",
      "Bingke Zhu",
      "Guibo Zhu",
      "Yingying Chen",
      "Ming Tang",
      "Jinqiao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_STINR_Deciphering_Spatial_Transcriptomics_via_Implicit_Neural_Representation_CVPR_2025_paper.html": {
    "title": "STINR: Deciphering Spatial Transcriptomics via Implicit Neural Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yisi Luo",
      "Xile Zhao",
      "Kai Ye",
      "Deyu Meng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shao_Remote_Photoplethysmography_in_Real-World_and_Extreme_Lighting_Scenarios_CVPR_2025_paper.html": {
    "title": "Remote Photoplethysmography in Real-World and Extreme Lighting Scenarios",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Shao",
      "Lei Luo",
      "Jianjun Qian",
      "Mengkai Yan",
      "Shuo Chen",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jamal_Multi-Modal_Contrastive_Masked_Autoencoders_A_Two-Stage_Progressive_Pre-training_Approach_for_CVPR_2025_paper.html": {
    "title": "Multi-Modal Contrastive Masked Autoencoders: A Two-Stage Progressive Pre-training Approach for RGBD Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Abdullah Jamal",
      "Omid Mohareri"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_JTD-UAV_MLLM-Enhanced_Joint_Tracking_and_Description_Framework_for_Anti-UAV_Systems_CVPR_2025_paper.html": {
    "title": "JTD-UAV: MLLM-Enhanced Joint Tracking and Description Framework for Anti-UAV Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Wang",
      "Jian Zhao",
      "Zhaoxin Fan",
      "Xin Zhang",
      "Xuecheng Wu",
      "Yudian Zhang",
      "Lei Jin",
      "Xinyue Li",
      "Gang Wang",
      "Mengxi Jia",
      "Ping Hu",
      "Zheng Zhu",
      "Xuelong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_HaWoR_World-Space_Hand_Motion_Reconstruction_from_Egocentric_Videos_CVPR_2025_paper.html": {
    "title": "HaWoR: World-Space Hand Motion Reconstruction from Egocentric Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinglei Zhang",
      "Jiankang Deng",
      "Chao Ma",
      "Rolandos Alexandros Potamias"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lai_Font-Agent_Enhancing_Font_Understanding_with_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "Font-Agent: Enhancing Font Understanding with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingxin Lai",
      "Cuijie Xu",
      "Haitian Shi",
      "Guoqing Yang",
      "Xiaoning Li",
      "Zhiming Luo",
      "Shaozi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jia_Secret_Lies_in_Color_Enhancing_AI-Generated_Images_Detection_with_Color_CVPR_2025_paper.html": {
    "title": "Secret Lies in Color: Enhancing AI-Generated Images Detection with Color Distribution Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zexi Jia",
      "Chuanwei Huang",
      "Yeshuang Zhu",
      "Hongyan Fei",
      "Xiaoyue Duan",
      "Zhiqiang Yuan",
      "Ying Deng",
      "Jiapei Zhang",
      "Jinchao Zhang",
      "Jie Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Heinrich_RADIOv2.5_Improved_Baselines_for_Agglomerative_Vision_Foundation_Models_CVPR_2025_paper.html": {
    "title": "RADIOv2.5: Improved Baselines for Agglomerative Vision Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Greg Heinrich",
      "Mike Ranzinger",
      "Hongxu Yin",
      "Yao Lu",
      "Jan Kautz",
      "Andrew Tao",
      "Bryan Catanzaro",
      "Pavlo Molchanov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Vincent_High_Temporal_Consistency_through_Semantic_Similarity_Propagation_in_Semi-Supervised_Video_CVPR_2025_paper.html": {
    "title": "High Temporal Consistency through Semantic Similarity Propagation in Semi-Supervised Video Semantic Segmentation for Autonomous Flight",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "C√©dric Vincent",
      "Taehyoung Kim",
      "Henri Mee√ü"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Cross-Modal_and_Uncertainty-Aware_Agglomeration_for_Open-Vocabulary_3D_Scene_Understanding_CVPR_2025_paper.html": {
    "title": "Cross-Modal and Uncertainty-Aware Agglomeration for Open-Vocabulary 3D Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinlong Li",
      "Cristiano Saltori",
      "Fabio Poiesi",
      "Nicu Sebe"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Generative_Gaussian_Splatting_for_Unbounded_3D_City_Generation_CVPR_2025_paper.html": {
    "title": "Generative Gaussian Splatting for Unbounded 3D City Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haozhe Xie",
      "Zhaoxi Chen",
      "Fangzhou Hong",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Du_SVLTA_Benchmarking_Vision-Language_Temporal_Alignment_via_Synthetic_Video_Situation_CVPR_2025_paper.html": {
    "title": "SVLTA: Benchmarking Vision-Language Temporal Alignment via Synthetic Video Situation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Du",
      "Bo Wu",
      "Yan Lu",
      "Zhendong Mao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Mixture_of_Submodules_for_Domain_Adaptive_Person_Search_CVPR_2025_paper.html": {
    "title": "Mixture of Submodules for Domain Adaptive Person Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minsu Kim",
      "Seungryong Kim",
      "Kwanghoon Sohn"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tourani_Unsupervised_Discovery_of_Facial_Landmarks_and_Head_Pose_CVPR_2025_paper.html": {
    "title": "Unsupervised Discovery of Facial Landmarks and Head Pose",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Satyajit Tourani",
      "Siddharth Tourani",
      "Arif Mahmood",
      "Muhammad Haris Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Instruct-CLIP_Improving_Instruction-Guided_Image_Editing_with_Automated_Data_Refinement_Using_CVPR_2025_paper.html": {
    "title": "Instruct-CLIP: Improving Instruction-Guided Image Editing with Automated Data Refinement Using Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sherry X. Chen",
      "Misha Sra",
      "Pradeep Sen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Stabilizing_and_Accelerating_Autofocus_with_Expert_Trajectory_Regularized_Deep_Reinforcement_CVPR_2025_paper.html": {
    "title": "Stabilizing and Accelerating Autofocus with Expert Trajectory Regularized Deep Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shouhang Zhu",
      "Chenglin Li",
      "Yuankun Jiang",
      "Li Wei",
      "Nuowen Kan",
      "Ziyang Zheng",
      "Wenrui Dai",
      "Junni Zou",
      "Hongkai Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pham_SharpDepth_Sharpening_Metric_Depth_Predictions_Using_Diffusion_Distillation_CVPR_2025_paper.html": {
    "title": "SharpDepth: Sharpening Metric Depth Predictions Using Diffusion Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duc-Hai Pham",
      "Tung Do",
      "Phong Nguyen",
      "Binh-Son Hua",
      "Khoi Nguyen",
      "Rang Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Karmann_Repurposing_Stable_Diffusion_Attention_for_Training-Free_Unsupervised_Interactive_Segmentation_CVPR_2025_paper.html": {
    "title": "Repurposing Stable Diffusion Attention for Training-Free Unsupervised Interactive Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Markus Karmann",
      "Onay Urfalioglu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_GO-N3RDet_Geometry_Optimized_NeRF-enhanced_3D_Object_Detector_CVPR_2025_paper.html": {
    "title": "GO-N3RDet: Geometry Optimized NeRF-enhanced 3D Object Detector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zechuan Li",
      "Hongshan Yu",
      "Yihao Ding",
      "Jinhao Qiao",
      "Basim Azam",
      "Naveed Akhtar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_DPSeg_Dual-Prompt_Cost_Volume_Learning_for_Open-Vocabulary_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "DPSeg: Dual-Prompt Cost Volume Learning for Open-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyu Zhao",
      "Xiaoguang Li",
      "Lingjia Shi",
      "Nasrin Imanpour",
      "Song Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wei_EvEnhancer_Empowering_Effectiveness_Efficiency_and_Generalizability_for_Continuous_Space-Time_Video_CVPR_2025_paper.html": {
    "title": "EvEnhancer: Empowering Effectiveness, Efficiency and Generalizability for Continuous Space-Time Video Super-Resolution with Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuoyan Wei",
      "Feng Li",
      "Shengeng Tang",
      "Yao Zhao",
      "Huihui Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Seeing_A_3D_World_in_A_Grain_of_Sand_CVPR_2025_paper.html": {
    "title": "Seeing A 3D World in A Grain of Sand",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufan Zhang",
      "Yu Ji",
      "Yu Guo",
      "Jinwei Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Simulator_HC_Regression-based_Online_Simulation_of_Starting_Problem-Solution_Pairs_for_CVPR_2025_paper.html": {
    "title": "Simulator HC: Regression-based Online Simulation of Starting Problem-Solution Pairs for Homotopy Continuation in Geometric Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyue Zhang",
      "Zijia Dai",
      "Wanting Xu",
      "Laurent Kneip"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Dynamic_Integration_of_Task-Specific_Adapters_for_Class_Incremental_Learning_CVPR_2025_paper.html": {
    "title": "Dynamic Integration of Task-Specific Adapters for Class Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiashuo Li",
      "Shaokun Wang",
      "Bo Qian",
      "Yuhang He",
      "Xing Wei",
      "Qiang Wang",
      "Yihong Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_MoFlow_One-Step_Flow_Matching_for_Human_Trajectory_Forecasting_via_Implicit_CVPR_2025_paper.html": {
    "title": "MoFlow: One-Step Flow Matching for Human Trajectory Forecasting via Implicit Maximum Likelihood Estimation based Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxiang Fu",
      "Qi Yan",
      "Lele Wang",
      "Ke Li",
      "Renjie Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_EgoPressure_A_Dataset_for_Hand_Pressure_and_Pose_Estimation_in_CVPR_2025_paper.html": {
    "title": "EgoPressure: A Dataset for Hand Pressure and Pose Estimation in Egocentric Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiming Zhao",
      "Taein Kwon",
      "Paul Streli",
      "Marc Pollefeys",
      "Christian Holz"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Morshed_DiverseFlow_Sample-Efficient_Diverse_Mode_Coverage_in_Flows_CVPR_2025_paper.html": {
    "title": "DiverseFlow: Sample-Efficient Diverse Mode Coverage in Flows",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mashrur M. Morshed",
      "Vishnu Boddeti"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Reason-before-Retrieve_One-Stage_Reflective_Chain-of-Thoughts_for_Training-Free_Zero-Shot_Composed_Image_Retrieval_CVPR_2025_paper.html": {
    "title": "Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot Composed Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanmin Tang",
      "Jue Zhang",
      "Xiaoting Qin",
      "Jing Yu",
      "Gaopeng Gou",
      "Gang Xiong",
      "Qingwei Lin",
      "Saravan Rajmohan",
      "Dongmei Zhang",
      "Qi Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_UniGraspTransformer_Simplified_Policy_Distillation_for_Scalable_Dexterous_Robotic_Grasping_CVPR_2025_paper.html": {
    "title": "UniGraspTransformer: Simplified Policy Distillation for Scalable Dexterous Robotic Grasping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbo Wang",
      "Fangyun Wei",
      "Lei Zhou",
      "Xi Chen",
      "Lin Luo",
      "Xiaohan Yi",
      "Yizhong Zhang",
      "Yaobo Liang",
      "Chang Xu",
      "Yan Lu",
      "Jiaolong Yang",
      "Baining Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mei_GeoMM_On_Geodesic_Perspective_for_Multi-modal_Learning_CVPR_2025_paper.html": {
    "title": "GeoMM: On Geodesic Perspective for Multi-modal Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shibin Mei",
      "Hang Wang",
      "Bingbing Ni"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_VISCO_Benchmarking_Fine-Grained_Critique_and_Correction_Towards_Self-Improvement_in_Visual_CVPR_2025_paper.html": {
    "title": "VISCO: Benchmarking Fine-Grained Critique and Correction Towards Self-Improvement in Visual Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueqing Wu",
      "Yuheng Ding",
      "Bingxuan Li",
      "Pan Lu",
      "Da Yin",
      "Kai-Wei Chang",
      "Nanyun Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ni_MaskGWM_A_Generalizable_Driving_World_Model_with_Video_Mask_Reconstruction_CVPR_2025_paper.html": {
    "title": "MaskGWM: A Generalizable Driving World Model with Video Mask Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingcheng Ni",
      "Yuxin Guo",
      "Yichen Liu",
      "Rui Chen",
      "Lewei Lu",
      "Zehuan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qian_3D-MVP_3D_Multiview_Pretraining_for_Manipulation_CVPR_2025_paper.html": {
    "title": "3D-MVP: 3D Multiview Pretraining for Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengyi Qian",
      "Kaichun Mo",
      "Valts Blukis",
      "David F. Fouhey",
      "Dieter Fox",
      "Ankit Goyal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Enhanced_OoD_Detection_through_Cross-Modal_Alignment_of_Multi-Modal_Representations_CVPR_2025_paper.html": {
    "title": "Enhanced OoD Detection through Cross-Modal Alignment of Multi-Modal Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeonghyeon Kim",
      "Sangheum Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Adaptive_Dropout_Unleashing_Dropout_across_Layers_for_Generalizable_Image_Super-Resolution_CVPR_2025_paper.html": {
    "title": "Adaptive Dropout: Unleashing Dropout across Layers for Generalizable Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Xu",
      "Jie Huang",
      "Wei Yu",
      "Jiangtong Tan",
      "Zhen Zou",
      "Feng Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_Breaking_the_Memory_Barrier_of_Contrastive_Loss_via_Tile-Based_Strategy_CVPR_2025_paper.html": {
    "title": "Breaking the Memory Barrier of Contrastive Loss via Tile-Based Strategy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zesen Cheng",
      "Hang Zhang",
      "Kehan Li",
      "Sicong Leng",
      "Zhiqiang Hu",
      "Fei Wu",
      "Deli Zhao",
      "Xin Li",
      "Lidong Bing"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tan_Mimir_Improving_Video_Diffusion_Models_for_Precise_Text_Understanding_CVPR_2025_paper.html": {
    "title": "Mimir: Improving Video Diffusion Models for Precise Text Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuai Tan",
      "Biao Gong",
      "Yutong Feng",
      "Kecheng Zheng",
      "Dandan Zheng",
      "Shuwei Shi",
      "Yujun Shen",
      "Jingdong Chen",
      "Ming Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_UCM-VeID_V2_A_Richer_Dataset_and_A_Pre-training_Method_for_CVPR_2025_paper.html": {
    "title": "UCM-VeID V2: A Richer Dataset and A Pre-training Method for UAV Cross-Modality Vehicle Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyue Liu",
      "Jiahao Qi",
      "Chen Chen",
      "KangCheng Bin",
      "Ping Zhong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Learning_Phase_Distortion_with_Selective_State_Space_Models_for_Video_CVPR_2025_paper.html": {
    "title": "Learning Phase Distortion with Selective State Space Models for Video Turbulence Mitigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingguang Zhang",
      "Nicholas Chimitt",
      "Xijun Wang",
      "Yu Yuan",
      "Stanley H. Chan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Goswami_RoboPEPP_Vision-Based_Robot_Pose_and_Joint_Angle_Estimation_through_Embedding_CVPR_2025_paper.html": {
    "title": "RoboPEPP: Vision-Based Robot Pose and Joint Angle Estimation through Embedding Predictive Pre-Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Raktim Gautam Goswami",
      "Prashanth Krishnamurthy",
      "Yann LeCun",
      "Farshad Khorrami"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Distraction_is_All_You_Need_for_Multimodal_Large_Language_Model_CVPR_2025_paper.html": {
    "title": "Distraction is All You Need for Multimodal Large Language Model Jailbreaking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zuopeng Yang",
      "Jiluan Fan",
      "Anli Yan",
      "Erdun Gao",
      "Xin Lin",
      "Tao Li",
      "Kanghua Mo",
      "Changyu Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zohar_Apollo__An_Exploration_of_Video_Understanding_in_Large_Multimodal_CVPR_2025_paper.html": {
    "title": "Apollo: An Exploration of Video Understanding in Large Multimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Orr Zohar",
      "Xiaohan Wang",
      "Yann Dubois",
      "Nikhil Mehta",
      "Tong Xiao",
      "Philippe Hansen-Estruch",
      "Licheng Yu",
      "Xiaofang Wang",
      "Felix Juefei-Xu",
      "Ning Zhang",
      "Serena Yeung-Levy",
      "Xide Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Skip_Tuning_Pre-trained_Vision-Language_Models_are_Effective_and_Efficient_Adapters_CVPR_2025_paper.html": {
    "title": "Skip Tuning: Pre-trained Vision-Language Models are Effective and Efficient Adapters Themselves",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shihan Wu",
      "Ji Zhang",
      "Pengpeng Zeng",
      "Lianli Gao",
      "Jingkuan Song",
      "Heng Tao Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_PatchDPO_Patch-level_DPO_for_Finetuning-free_Personalized_Image_Generation_CVPR_2025_paper.html": {
    "title": "PatchDPO: Patch-level DPO for Finetuning-free Personalized Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihan Huang",
      "Long Chan",
      "Jinlong Liu",
      "Wanggui He",
      "Hao Jiang",
      "Mingli Song",
      "Jie Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Learning_to_Normalize_on_the_SPD_Manifold_under_Bures-Wasserstein_Geometry_CVPR_2025_paper.html": {
    "title": "Learning to Normalize on the SPD Manifold under Bures-Wasserstein Geometry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Wang",
      "Shaocheng Jin",
      "Ziheng Chen",
      "Xiaoqing Luo",
      "Xiao-Jun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cuttano_SAMWISE_Infusing_Wisdom_in_SAM2_for_Text-Driven_Video_Segmentation_CVPR_2025_paper.html": {
    "title": "SAMWISE: Infusing Wisdom in SAM2 for Text-Driven Video Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Claudia Cuttano",
      "Gabriele Trivigno",
      "Gabriele Rosi",
      "Carlo Masone",
      "Giuseppe Averta"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_MegaSaM_Accurate_Fast_and_Robust_Structure_and_Motion_from_Casual_CVPR_2025_paper.html": {
    "title": "MegaSaM: Accurate, Fast and Robust Structure and Motion from Casual Dynamic Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengqi Li",
      "Richard Tucker",
      "Forrester Cole",
      "Qianqian Wang",
      "Linyi Jin",
      "Vickie Ye",
      "Angjoo Kanazawa",
      "Aleksander Holynski",
      "Noah Snavely"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_BEVDiffuser_Plug-and-Play_Diffusion_Model_for_BEV_Denoising_with_Ground-Truth_Guidance_CVPR_2025_paper.html": {
    "title": "BEVDiffuser: Plug-and-Play Diffusion Model for BEV Denoising with Ground-Truth Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Ye",
      "Burhaneddin Yaman",
      "Sheng Cheng",
      "Feng Tao",
      "Abhirup Mallik",
      "Liu Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_GeoAvatar_Geometrically-Consistent_Multi-Person_Avatar_Reconstruction_from_Sparse_Multi-View_Videos_CVPR_2025_paper.html": {
    "title": "GeoAvatar: Geometrically-Consistent Multi-Person Avatar Reconstruction from Sparse Multi-View Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soohyun Lee",
      "Seoyeon Kim",
      "HeeKyung Lee",
      "Won-Sik Jeong",
      "Joo Ho Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shao_FinePhys_Fine-grained_Human_Action_Generation_by_Explicitly_Incorporating_Physical_Laws_CVPR_2025_paper.html": {
    "title": "FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dian Shao",
      "Mingfei Shi",
      "Shengda Xu",
      "Haodong Chen",
      "Yongle Huang",
      "Binglu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_DiET-GS_Diffusion_Prior_and_Event_Stream-Assisted_Motion_Deblurring_3D_Gaussian_CVPR_2025_paper.html": {
    "title": "DiET-GS: Diffusion Prior and Event Stream-Assisted Motion Deblurring 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungjun Lee",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hanson_Speedy-Splat_Fast_3D_Gaussian_Splatting_with_Sparse_Pixels_and_Sparse_CVPR_2025_paper.html": {
    "title": "Speedy-Splat: Fast 3D Gaussian Splatting with Sparse Pixels and Sparse Primitives",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Hanson",
      "Allen Tu",
      "Geng Lin",
      "Vasu Singla",
      "Matthias Zwicker",
      "Tom Goldstein"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SeedVR_Seeding_Infinity_in_Diffusion_Transformer_Towards_Generic_Video_Restoration_CVPR_2025_paper.html": {
    "title": "SeedVR: Seeding Infinity in Diffusion Transformer Towards Generic Video Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianyi Wang",
      "Zhijie Lin",
      "Meng Wei",
      "Yang Zhao",
      "Ceyuan Yang",
      "Chen Change Loy",
      "Lu Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhong_Beyond_Generation_A_Diffusion-based_Low-level_Feature_Extractor_for_Detecting_AI-generated_CVPR_2025_paper.html": {
    "title": "Beyond Generation: A Diffusion-based Low-level Feature Extractor for Detecting AI-generated Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan Zhong",
      "Haoyu Chen",
      "Yiran Xu",
      "Zhenxing Qian",
      "Xinpeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Robust-MVTON_Learning_Cross-Pose_Feature_Alignment_and_Fusion_for_Robust_Multi-View_CVPR_2025_paper.html": {
    "title": "Robust-MVTON: Learning Cross-Pose Feature Alignment and Fusion for Robust Multi-View Virtual Try-On",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nannan Zhang",
      "Yijiang Li",
      "Dong Du",
      "Zheng Chong",
      "Zhengwentai Sun",
      "Jianhao Zeng",
      "Yusheng Dai",
      "Zhengyu Xie",
      "Hairui Zhu",
      "Xiaoguang Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Plizzari_Omnia_de_EgoTempo_Benchmarking_Temporal_Understanding_of_Multi-Modal_LLMs_in_CVPR_2025_paper.html": {
    "title": "Omnia de EgoTempo: Benchmarking Temporal Understanding of Multi-Modal LLMs in Egocentric Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chiara Plizzari",
      "Alessio Tonioni",
      "Yongqin Xian",
      "Achin Kulshrestha",
      "Federico Tombari"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_ODHSR_Online_Dense_3D_Reconstruction_of_Humans_and_Scenes_from_CVPR_2025_paper.html": {
    "title": "ODHSR: Online Dense 3D Reconstruction of Humans and Scenes from Monocular Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zetong Zhang",
      "Manuel Kaufmann",
      "Lixin Xue",
      "Jie Song",
      "Martin R. Oswald"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yuan_Identity-Preserving_Text-to-Video_Generation_by_Frequency_Decomposition_CVPR_2025_paper.html": {
    "title": "Identity-Preserving Text-to-Video Generation by Frequency Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenghai Yuan",
      "Jinfa Huang",
      "Xianyi He",
      "Yunyang Ge",
      "Yujun Shi",
      "Liuhan Chen",
      "Jiebo Luo",
      "Li Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_FreeGave_3D_Physics_Learning_from_Dynamic_Videos_by_Gaussian_Velocity_CVPR_2025_paper.html": {
    "title": "FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinxi Li",
      "Ziyang Song",
      "Siyuan Zhou",
      "Bo Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_SpiritSight_Agent_Advanced_GUI_Agent_with_One_Look_CVPR_2025_paper.html": {
    "title": "SpiritSight Agent: Advanced GUI Agent with One Look",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Huang",
      "Ziming Cheng",
      "Junting Pan",
      "Zhaohui Hou",
      "Mingjie Zhan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Zero-Shot_Monocular_Scene_Flow_Estimation_in_the_Wild_CVPR_2025_paper.html": {
    "title": "Zero-Shot Monocular Scene Flow Estimation in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiqing Liang",
      "Abhishek Badki",
      "Hang Su",
      "James Tompkin",
      "Orazio Gallo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_MG-MotionLLM_A_Unified_Framework_for_Motion_Comprehension_and_Generation_across_CVPR_2025_paper.html": {
    "title": "MG-MotionLLM: A Unified Framework for Motion Comprehension and Generation across Multiple Granularities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bizhu Wu",
      "Jinheng Xie",
      "Keming Shen",
      "Zhe Kong",
      "Jianfeng Ren",
      "Ruibin Bai",
      "Rong Qu",
      "Linlin Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_Retaining_Knowledge_and_Enhancing_Long-Text_Representations_in_CLIP_through_Dual-Teacher_CVPR_2025_paper.html": {
    "title": "Retaining Knowledge and Enhancing Long-Text Representations in CLIP through Dual-Teacher Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuheng Feng",
      "Changsong Wen",
      "Zelin Peng",
      "Li jiaye",
      "Siyu Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_MMRL_Multi-Modal_Representation_Learning_for_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "MMRL: Multi-Modal Representation Learning for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuncheng Guo",
      "Xiaodong Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nam_Optical-Flow_Guided_Prompt_Optimization_for_Coherent_Video_Generation_CVPR_2025_paper.html": {
    "title": "Optical-Flow Guided Prompt Optimization for Coherent Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyelin Nam",
      "Jaemin Kim",
      "Dohun Lee",
      "Jong Chul Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peng_MOS_Modeling_Object-Scene_Associations_in_Generalized_Category_Discovery_CVPR_2025_paper.html": {
    "title": "MOS: Modeling Object-Scene Associations in Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyuan Peng",
      "Jinpeng Ma",
      "Zhimin Sun",
      "Ran Yi",
      "Haichuan Song",
      "Xin Tan",
      "Lizhuang Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tan_Anchor-Aware_Similarity_Cohesion_in_Target_Frames_Enables_Predicting_Temporal_Moment_CVPR_2025_paper.html": {
    "title": "Anchor-Aware Similarity Cohesion in Target Frames Enables Predicting Temporal Moment Boundaries in 2D",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawei Tan",
      "Hongxing Wang",
      "Junwu Weng",
      "Jiaxin Li",
      "Zhilong Ou",
      "Kang Dang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shanmugam_Test-time_Augmentation_Improves_Efficiency_in_Conformal_Prediction_CVPR_2025_paper.html": {
    "title": "Test-time Augmentation Improves Efficiency in Conformal Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Divya Shanmugam",
      "Helen Lu",
      "Swami Sankaranarayanan",
      "John Guttag"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_Breaking_the_Low-Rank_Dilemma_of_Linear_Attention_CVPR_2025_paper.html": {
    "title": "Breaking the Low-Rank Dilemma of Linear Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihang Fan",
      "Huaibo Huang",
      "Ran He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_StoryGPT-V_Large_Language_Models_as_Consistent_Story_Visualizers_CVPR_2025_paper.html": {
    "title": "StoryGPT-V: Large Language Models as Consistent Story Visualizers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoqian Shen",
      "Mohamed Elhoseiny"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Code-as-Monitor_Constraint-aware_Visual_Programming_for_Reactive_and_Proactive_Robotic_Failure_CVPR_2025_paper.html": {
    "title": "Code-as-Monitor: Constraint-aware Visual Programming for Reactive and Proactive Robotic Failure Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enshen Zhou",
      "Qi Su",
      "Cheng Chi",
      "Zhizheng Zhang",
      "Zhongyuan Wang",
      "Tiejun Huang",
      "Lu Sheng",
      "He Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Embracing_Collaboration_Over_Competition_Condensing_Multiple_Prompts_for_Visual_In-Context_CVPR_2025_paper.html": {
    "title": "Embracing Collaboration Over Competition: Condensing Multiple Prompts for Visual In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinpeng Wang",
      "Tianci Luo",
      "Yaohua Zha",
      "Yan Feng",
      "Ruisheng Luo",
      "Bin Chen",
      "Tao Dai",
      "Long Chen",
      "Yaowei Wang",
      "Shu-Tao Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Rethinking_Reconstruction_and_Denoising_in_the_Dark_New_Perspective_General_CVPR_2025_paper.html": {
    "title": "Rethinking Reconstruction and Denoising in the Dark: New Perspective, General Architecture and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tengyu Ma",
      "Long Ma",
      "Ziye Li",
      "Yuetong Wang",
      "Jinyuan Liu",
      "Chengpei Xu",
      "Risheng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hadji_Edge-SD-SR_Low_Latency_and_Parameter_Efficient_On-device_Super-Resolution_with_Stable_CVPR_2025_paper.html": {
    "title": "Edge-SD-SR: Low Latency and Parameter Efficient On-device Super-Resolution with Stable Diffusion via Bidirectional Conditioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Isma Hadji",
      "Mehdi Noroozi",
      "Victor Escorcia",
      "Anestis Zaganidis",
      "Brais Martinez",
      "Georgios Tzimiropoulos"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_Unity_in_Diversity_Video_Editing_via_Gradient-Latent_Purification_CVPR_2025_paper.html": {
    "title": "Unity in Diversity: Video Editing via Gradient-Latent Purification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyu Gao",
      "Kunlin Yang",
      "Xuan Yao",
      "Yufan Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Revealing_Key_Details_to_See_Differences_A_Novel_Prototypical_Perspective_CVPR_2025_paper.html": {
    "title": "Revealing Key Details to See Differences: A Novel Prototypical Perspective for Skeleton-based Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongda Liu",
      "Yunfan Liu",
      "Min Ren",
      "Hao Wang",
      "Yunlong Wang",
      "Zhenan Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dages_Finsler_Multi-Dimensional_Scaling_Manifold_Learning_for_Asymmetric_Dimensionality_Reduction_and_CVPR_2025_paper.html": {
    "title": "Finsler Multi-Dimensional Scaling: Manifold Learning for Asymmetric Dimensionality Reduction and Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Dag√®s",
      "Simon Weber",
      "Ya-Wei Eileen Lin",
      "Ronen Talmon",
      "Daniel Cremers",
      "Michael Lindenbaum",
      "Alfred M. Bruckstein",
      "Ron Kimmel"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_VideoEspresso_A_Large-Scale_Chain-of-Thought_Dataset_for_Fine-Grained_Video_Reasoning_via_CVPR_2025_paper.html": {
    "title": "VideoEspresso: A Large-Scale Chain-of-Thought Dataset for Fine-Grained Video Reasoning via Core Frame Selection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songhao Han",
      "Wei Huang",
      "Hairong Shi",
      "Le Zhuo",
      "Xiu Su",
      "Shifeng Zhang",
      "Xu Zhou",
      "Xiaojuan Qi",
      "Yue Liao",
      "Si Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_INFP_Audio-Driven_Interactive_Head_Generation_in_Dyadic_Conversations_CVPR_2025_paper.html": {
    "title": "INFP: Audio-Driven Interactive Head Generation in Dyadic Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongming Zhu",
      "Longhao Zhang",
      "Zhengkun Rong",
      "Tianshu Hu",
      "Shuang Liang",
      "Zhipeng Ge"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Federated_Learning_with_Domain_Shift_Eraser_CVPR_2025_paper.html": {
    "title": "Federated Learning with Domain Shift Eraser",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Wang",
      "Zihui Wang",
      "Zheng Wang",
      "Xiaoliang Fan",
      "Cheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lahlali_Cross-Modal_Distillation_for_2D3D_Multi-Object_Discovery_from_2D_Motion_CVPR_2025_paper.html": {
    "title": "Cross-Modal Distillation for 2D/3D Multi-Object Discovery from 2D Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saad Lahlali",
      "Sandra Kara",
      "Hejer Ammar",
      "Florian Chabot",
      "Nicolas Granger",
      "Herv√© Le Borgne",
      "Quoc-Cuong Pham"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cai_DiTCtrl_Exploring_Attention_Control_in_Multi-Modal_Diffusion_Transformer_for_Tuning-Free_CVPR_2025_paper.html": {
    "title": "DiTCtrl: Exploring Attention Control in Multi-Modal Diffusion Transformer for Tuning-Free Multi-Prompt Longer Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minghong Cai",
      "Xiaodong Cun",
      "Xiaoyu Li",
      "Wenze Liu",
      "Zhaoyang Zhang",
      "Yong Zhang",
      "Ying Shan",
      "Xiangyu Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Bridge_Frame_and_Event_Common_Spatiotemporal_Fusion_for_High-Dynamic_Scene_CVPR_2025_paper.html": {
    "title": "Bridge Frame and Event: Common Spatiotemporal Fusion for High-Dynamic Scene Optical Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanyu Zhou",
      "Haonan Wang",
      "Haoyue Liu",
      "Yuxing Duan",
      "Yi Chang",
      "Luxin Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_EVPGS_Enhanced_View_Prior_Guidance_for_Splatting-based_Extrapolated_View_Synthesis_CVPR_2025_paper.html": {
    "title": "EVPGS: Enhanced View Prior Guidance for Splatting-based Extrapolated View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahe Li",
      "Feiyu Wang",
      "Xiaochao Qu",
      "Chengjing Wu",
      "Luoqi Liu",
      "Ting Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shao_GREAT_Geometry-Intention_Collaborative_Inference_for_Open-Vocabulary_3D_Object_Affordance_Grounding_CVPR_2025_paper.html": {
    "title": "GREAT: Geometry-Intention Collaborative Inference for Open-Vocabulary 3D Object Affordance Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yawen Shao",
      "Wei Zhai",
      "Yuhang Yang",
      "Hongchen Luo",
      "Yang Cao",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Marchellus_Link_to_the_Past_Temporal_Propagation_for_Fast_3D_Human_CVPR_2025_paper.html": {
    "title": "Link to the Past: Temporal Propagation for Fast 3D Human Reconstruction from Monocular Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matthew Marchellus",
      "Nadhira Noor",
      "In Kyu Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Inversion_Circle_Interpolation_Diffusion-based_Image_Augmentation_for_Data-scarce_Classification_CVPR_2025_paper.html": {
    "title": "Inversion Circle Interpolation: Diffusion-based Image Augmentation for Data-scarce Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanghao Wang",
      "Long Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Deterministic_Certification_of_Graph_Neural_Networks_against_Graph_Poisoning_Attacks_CVPR_2025_paper.html": {
    "title": "Deterministic Certification of Graph Neural Networks against Graph Poisoning Attacks with Arbitrary Perturbations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiate Li",
      "Meng Pang",
      "Yun Dong",
      "Binghui Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_A3_Few-shot_Prompt_Learning_of_Unlearnable_Examples_with_Cross-Modal_Adversarial_CVPR_2025_paper.html": {
    "title": "A3: Few-shot Prompt Learning of Unlearnable Examples with Cross-Modal Adversarial Feature Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Wang",
      "Xitong Gao",
      "Dongping Liao",
      "Tianrui Qin",
      "Yu-liang Lu",
      "Cheng-zhong Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lv_Adapting_Pre-trained_3D_Models_for_Point_Cloud_Video_Understanding_via_CVPR_2025_paper.html": {
    "title": "Adapting Pre-trained 3D Models for Point Cloud Video Understanding via Cross-frame Spatio-temporal Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baixuan Lv",
      "Yaohua Zha",
      "Tao Dai",
      "Xue Yuerong",
      "Ke Chen",
      "Shu-Tao Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bae_MASH-VLM_Mitigating_Action-Scene_Hallucination_in_Video-LLMs_through_Disentangled_Spatial-Temporal_Representations_CVPR_2025_paper.html": {
    "title": "MASH-VLM: Mitigating Action-Scene Hallucination in Video-LLMs through Disentangled Spatial-Temporal Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyungho Bae",
      "Jinhyung Kim",
      "Sihaeng Lee",
      "Soonyoung Lee",
      "Gunhee Lee",
      "Jinwoo Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lai_UWAV_Uncertainty-weighted_Weakly-supervised_Audio-Visual_Video_Parsing_CVPR_2025_paper.html": {
    "title": "UWAV: Uncertainty-weighted Weakly-supervised Audio-Visual Video Parsing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yung-Hsuan Lai",
      "Janek Ebbers",
      "Yu-Chiang Frank Wang",
      "Fran√ßois Germain",
      "Michael Jeffrey Jones",
      "Moitreya Chatterjee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Mosaic_of_Modalities_A_Comprehensive_Benchmark_for_Multimodal_Graph_Learning_CVPR_2025_paper.html": {
    "title": "Mosaic of Modalities: A Comprehensive Benchmark for Multimodal Graph Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Zhu",
      "Yuhang Zhou",
      "Shengyi Qian",
      "Zhongmou He",
      "Tong Zhao",
      "Neil Shah",
      "Danai Koutra"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_TSP-Mamba_The_Travelling_Salesman_Problem_Meets_Mamba_for_Image_Super-resolution_CVPR_2025_paper.html": {
    "title": "TSP-Mamba: The Travelling Salesman Problem Meets Mamba for Image Super-resolution and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Zhou",
      "Xinyu Lin",
      "Jiangbo Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_MVPaint_Synchronized_Multi-View_Diffusion_for_Painting_Anything_3D_CVPR_2025_paper.html": {
    "title": "MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Cheng",
      "Juncheng Mu",
      "Xianfang Zeng",
      "Xin Chen",
      "Anqi Pang",
      "Chi Zhang",
      "Zhibin Wang",
      "Bin Fu",
      "Gang Yu",
      "Ziwei Liu",
      "Liang Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_FirePlace_Geometric_Refinements_of_LLM_Common_Sense_Reasoning_for_3D_CVPR_2025_paper.html": {
    "title": "FirePlace: Geometric Refinements of LLM Common Sense Reasoning for 3D Object Placement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ian Huang",
      "Yanan Bao",
      "Karen Truong",
      "Howard Zhou",
      "Cordelia Schmid",
      "Leonidas Guibas",
      "Alireza Fathi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/You_RENO_Real-Time_Neural_Compression_for_3D_LiDAR_Point_Clouds_CVPR_2025_paper.html": {
    "title": "RENO: Real-Time Neural Compression for 3D LiDAR Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kang You",
      "Tong Chen",
      "Dandan Ding",
      "M. Salman Asif",
      "Zhan Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gielisse_End-to-End_Implicit_Neural_Representations_for_Classification_CVPR_2025_paper.html": {
    "title": "End-to-End Implicit Neural Representations for Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Gielisse",
      "Jan van Gemert"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_ASAP_Advancing_Semantic_Alignment_Promotes_Multi-Modal_Manipulation_Detecting_and_Grounding_CVPR_2025_paper.html": {
    "title": "ASAP: Advancing Semantic Alignment Promotes Multi-Modal Manipulation Detecting and Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenxing Zhang",
      "Yaxiong Wang",
      "Lechao Cheng",
      "Zhun Zhong",
      "Dan Guo",
      "Meng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sheng_UNICL-SAM_Uncertainty-Driven_In-Context_Segmentation_with_Part_Prototype_Discovery_CVPR_2025_paper.html": {
    "title": "UNICL-SAM: Uncertainty-Driven In-Context Segmentation with Part Prototype Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dianmo Sheng",
      "Dongdong Chen",
      "Zhentao Tan",
      "Qiankun Liu",
      "Qi Chu",
      "Tao Gong",
      "Bin Liu",
      "Jing Han",
      "Wenbin Tu",
      "Shengwei Xu",
      "Nenghai Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tschernezki_Layered_Motion_Fusion_Lifting_Motion_Segmentation_to_3D_in_Egocentric_CVPR_2025_paper.html": {
    "title": "Layered Motion Fusion: Lifting Motion Segmentation to 3D in Egocentric Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vadim Tschernezki",
      "Diane Larlus",
      "Iro Laina",
      "Andrea Vedaldi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_FADE_Frequency-Aware_Diffusion_Model_Factorization_for_Video_Editing_CVPR_2025_paper.html": {
    "title": "FADE: Frequency-Aware Diffusion Model Factorization for Video Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixuan Zhu",
      "Haolin Wang",
      "Shilin Ma",
      "Wenliang Zhao",
      "Yansong Tang",
      "Lei Chen",
      "Jie Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MotiF_Making_Text_Count_in_Image_Animation_with_Motion_Focal_CVPR_2025_paper.html": {
    "title": "MotiF: Making Text Count in Image Animation with Motion Focal Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijie Wang",
      "Samaneh Azadi",
      "Rohit Girdhar",
      "Saketh Rambhatla",
      "Chen Sun",
      "Xi Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Towards_Explicit_Geometry-Reflectance_Collaboration_for_Generalized_LiDAR_Segmentation_in_Adverse_CVPR_2025_paper.html": {
    "title": "Towards Explicit Geometry-Reflectance Collaboration for Generalized LiDAR Segmentation in Adverse Weather",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longyu Yang",
      "Ping Hu",
      "Shangbo Yuan",
      "Lu Zhang",
      "Jun Liu",
      "Hengtao Shen",
      "Xiaofeng Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mi_Data_Synthesis_with_Diverse_Styles_for_Face_Recognition_via_3DMM-Guided_CVPR_2025_paper.html": {
    "title": "Data Synthesis with Diverse Styles for Face Recognition via 3DMM-Guided Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxi Mi",
      "Zhizhou Zhong",
      "Yuge Huang",
      "Qiuyang Yuan",
      "Xuan Zhao",
      "Jianqing Xu",
      "Shouhong Ding",
      "Shaoming Wang",
      "Rizen Guo",
      "Shuigeng Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cai_Diffusion_Self-Distillation_for_Zero-Shot_Customized_Image_Generation_CVPR_2025_paper.html": {
    "title": "Diffusion Self-Distillation for Zero-Shot Customized Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengqu Cai",
      "Eric Ryan Chan",
      "Yunzhi Zhang",
      "Leonidas Guibas",
      "Jiajun Wu",
      "Gordon Wetzstein"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Uncertainty-guided_Perturbation_for_Image_Super-Resolution_Diffusion_Model_CVPR_2025_paper.html": {
    "title": "Uncertainty-guided Perturbation for Image Super-Resolution Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leheng Zhang",
      "Weiyi You",
      "Kexuan Shi",
      "Shuhang Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Geometric_Knowledge-Guided_Localized_Global_Distribution_Alignment_for_Federated_Learning_CVPR_2025_paper.html": {
    "title": "Geometric Knowledge-Guided Localized Global Distribution Alignment for Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanbiao Ma",
      "Wei Dai",
      "Wenke Huang",
      "Jiayi Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Grobrugge_Towards_Human-Understandable_Multi-Dimensional_Concept_Discovery_CVPR_2025_paper.html": {
    "title": "Towards Human-Understandable Multi-Dimensional Concept Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arne Grobr√ºgge",
      "Niklas K√ºhl",
      "Gerhard Satzger",
      "Philipp Spitzer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_GlyphMastero_A_Glyph_Encoder_for_High-Fidelity_Scene_Text_Editing_CVPR_2025_paper.html": {
    "title": "GlyphMastero: A Glyph Encoder for High-Fidelity Scene Text Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Wang",
      "Ting Liu",
      "Xiaochao Qu",
      "Chengjing Wu",
      "Luoqi Liu",
      "Xiaolin Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xing_ConText-CIR_Learning_from_Concepts_in_Text_for_Composed_Image_Retrieval_CVPR_2025_paper.html": {
    "title": "ConText-CIR: Learning from Concepts in Text for Composed Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Xing",
      "Pranavi Kolouju",
      "Robert Pless",
      "Abby Stylianou",
      "Nathan Jacobs"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MaskGaussian_Adaptive_3D_Gaussian_Representation_from_Probabilistic_Masks_CVPR_2025_paper.html": {
    "title": "MaskGaussian: Adaptive 3D Gaussian Representation from Probabilistic Masks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Liu",
      "Zhihang Zhong",
      "Yifan Zhan",
      "Sheng Xu",
      "Xiao Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hong_Perturb-and-Revise_Flexible_3D_Editing_with_Generative_Trajectories_CVPR_2025_paper.html": {
    "title": "Perturb-and-Revise: Flexible 3D Editing with Generative Trajectories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Susung Hong",
      "Johanna Karras",
      "Ricardo Martin-Brualla",
      "Ira Kemelmacher-Shlizerman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Geng_Birth_and_Death_of_a_Rose_CVPR_2025_paper.html": {
    "title": "Birth and Death of a Rose",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Geng",
      "Yunzhi Zhang",
      "Shangzhe Wu",
      "Jiajun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Learning_Compatible_Multi-Prize_Subnetworks_for_Asymmetric_Retrieval_CVPR_2025_paper.html": {
    "title": "Learning Compatible Multi-Prize Subnetworks for Asymmetric Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushuai Sun",
      "Zikun Zhou",
      "Dongmei Jiang",
      "Yaowei Wang",
      "Jun Yu",
      "Guangming Lu",
      "Wenjie Pei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_SoundVista_Novel-View_Ambient_Sound_Synthesis_via_Visual-Acoustic_Binding_CVPR_2025_paper.html": {
    "title": "SoundVista: Novel-View Ambient Sound Synthesis via Visual-Acoustic Binding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingfei Chen",
      "Israel D. Gebru",
      "Ishwarya Ananthabhotla",
      "Christian Richardt",
      "Dejan Markovic",
      "Jake Sandakly",
      "Steven Krenn",
      "Todd Keebler",
      "Eli Shlizerman",
      "Alexander Richard"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Abbasi_CLIP_Under_the_Microscope_A_Fine-Grained_Analysis_of_Multi-Object_Representation_CVPR_2025_paper.html": {
    "title": "CLIP Under the Microscope: A Fine-Grained Analysis of Multi-Object Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reza Abbasi",
      "Ali Nazari",
      "Aminreza Sefid",
      "Mohammadali Banayeeanzade",
      "Mohammad Hossein Rohban",
      "Mahdieh Soleymani Baghshah"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MetricGrids_Arbitrary_Nonlinear_Approximation_with_Elementary_Metric_Grids_based_Implicit_CVPR_2025_paper.html": {
    "title": "MetricGrids: Arbitrary Nonlinear Approximation with Elementary Metric Grids based Implicit Neural Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shu Wang",
      "Yanbo Gao",
      "Shuai Li",
      "Chong Lv",
      "Xun Cai",
      "Chuankun Li",
      "Hui Yuan",
      "Jinglin Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Navigating_Image_Restoration_with_VARs_Distribution_Alignment_Prior_CVPR_2025_paper.html": {
    "title": "Navigating Image Restoration with VAR's Distribution Alignment Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyang Wang",
      "Naishan Zheng",
      "Jie Huang",
      "Feng Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_MovieBench_A_Hierarchical_Movie_Level_Dataset_for_Long_Video_Generation_CVPR_2025_paper.html": {
    "title": "MovieBench: A Hierarchical Movie Level Dataset for Long Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weijia Wu",
      "Mingyu Liu",
      "Zeyu Zhu",
      "Xi Xia",
      "Haoen Feng",
      "Wen Wang",
      "Kevin Qinghong Lin",
      "Chunhua Shen",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_Dissecting_and_Mitigating_Diffusion_Bias_via_Mechanistic_Interpretability_CVPR_2025_paper.html": {
    "title": "Dissecting and Mitigating Diffusion Bias via Mechanistic Interpretability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingdong Shi",
      "Changming Li",
      "Yifan Wang",
      "Yongxiang Zhao",
      "Anqi Pang",
      "Sibei Yang",
      "Jingyi Yu",
      "Kan Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dampfhoffer_Graph_Neural_Network_Combining_Event_Stream_and_Periodic_Aggregation_for_CVPR_2025_paper.html": {
    "title": "Graph Neural Network Combining Event Stream and Periodic Aggregation for Low-Latency Event-based Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manon Dampfhoffer",
      "Thomas Mesquida",
      "Damien Joubert",
      "Thomas Dalgaty",
      "Pascal Vivet",
      "Christoph Posch"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Be_More_Specific_Evaluating_Object-centric_Realism_in_Synthetic_Images_CVPR_2025_paper.html": {
    "title": "Be More Specific: Evaluating Object-centric Realism in Synthetic Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anqi Liang",
      "Ciprian Corneanu",
      "Qianli Feng",
      "Giorgio Giannone",
      "Aleix Martinez"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Correlative_and_Discriminative_Label_Grouping_for_Multi-Label_Visual_Prompt_Tuning_CVPR_2025_paper.html": {
    "title": "Correlative and Discriminative Label Grouping for Multi-Label Visual Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei-Lei Ma",
      "Shuo Xu",
      "Ming-Kun Xie",
      "Lei Wang",
      "Dengdi Sun",
      "Haifeng Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Simsar_LoRACLR_Contrastive_Adaptation_for_Customization_of_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "LoRACLR: Contrastive Adaptation for Customization of Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enis Simsar",
      "Thomas Hofmann",
      "Federico Tombari",
      "Pinar Yanardag"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Su_ArtFormer_Controllable_Generation_of_Diverse_3D_Articulated_Objects_CVPR_2025_paper.html": {
    "title": "ArtFormer: Controllable Generation of Diverse 3D Articulated Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayi Su",
      "Youhe Feng",
      "Zheng Li",
      "Jinhua Song",
      "Yangfan He",
      "Botao Ren",
      "Botian Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nousias_Opportunistic_Single-Photon_Time_of_Flight_CVPR_2025_paper.html": {
    "title": "Opportunistic Single-Photon Time of Flight",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sotiris Nousias",
      "Mian Wei",
      "Howard Xiao",
      "Maxx Wu",
      "Shahmeer Athar",
      "Kevin J. Wang",
      "Anagh Malik",
      "David A. Barmherzig",
      "David B. Lindell",
      "Kyros N. Kutulakos"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Bridging_Gait_Recognition_and_Large_Language_Models_Sequence_Modeling_CVPR_2025_paper.html": {
    "title": "Bridging Gait Recognition and Large Language Models Sequence Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaopeng Yang",
      "Jilong Wang",
      "Saihui Hou",
      "Xu Liu",
      "Chunshui Cao",
      "Liang Wang",
      "Yongzhen Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Man_Argus_Vision-Centric_Reasoning_with_Grounded_Chain-of-Thought_CVPR_2025_paper.html": {
    "title": "Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunze Man",
      "De-An Huang",
      "Guilin Liu",
      "Shiwei Sheng",
      "Shilong Liu",
      "Liang-Yan Gui",
      "Jan Kautz",
      "Yu-Xiong Wang",
      "Zhiding Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_Bootstrap_Your_Own_Views_Masked_Ego-Exo_Modeling_for_Fine-grained_View-invariant_CVPR_2025_paper.html": {
    "title": "Bootstrap Your Own Views: Masked Ego-Exo Modeling for Fine-grained View-invariant Video Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungin Park",
      "Jiyoung Lee",
      "Kwanghoon Sohn"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jin_SFDM_Robust_Decomposition_of_Geometry_and_Reflectance_for_Realistic_Face_CVPR_2025_paper.html": {
    "title": "SFDM: Robust Decomposition of Geometry and Reflectance for Realistic Face Rendering from Sparse-view Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daisheng Jin",
      "Jiangbei Hu",
      "Baixin Xu",
      "Yuxin Dai",
      "Chen Qian",
      "Ying He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_DiSRT-In-Bed_Diffusion-Based_Sim-to-Real_Transfer_Framework_for_In-Bed_Human_Mesh_Recovery_CVPR_2025_paper.html": {
    "title": "DiSRT-In-Bed: Diffusion-Based Sim-to-Real Transfer Framework for In-Bed Human Mesh Recovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Gao",
      "Ce Zheng",
      "Laszlo A. Jeni",
      "Zackory Erickson"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wen_Ouroboros3D_Image-to-3D_Generation_via_3D-aware_Recursive_Diffusion_CVPR_2025_paper.html": {
    "title": "Ouroboros3D: Image-to-3D Generation via 3D-aware Recursive Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Wen",
      "Zehuan Huang",
      "Yaohui Wang",
      "Xinyuan Chen",
      "Lu Sheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Di_QMambaBSR_Burst_Image_Super-Resolution_with_Query_State_Space_Model_CVPR_2025_paper.html": {
    "title": "QMambaBSR: Burst Image Super-Resolution with Query State Space Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Di",
      "Long Peng",
      "Peizhe Xia",
      "Wenbo Li",
      "Renjing Pei",
      "Yang Cao",
      "Yang Wang",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Su_Encapsulated_Composition_of_Text-to-Image_and_Text-to-Video_Models_for_High-Quality_Video_CVPR_2025_paper.html": {
    "title": "Encapsulated Composition of Text-to-Image and Text-to-Video Models for High-Quality Video Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tongtong Su",
      "Chengyu Wang",
      "Bingyan Liu",
      "Jun Huang",
      "Dongming Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jung_Multi-Group_Proportional_Representations_for_Text-to-Image_Models_CVPR_2025_paper.html": {
    "title": "Multi-Group Proportional Representations for Text-to-Image Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sangwon Jung",
      "Alex Oesterling",
      "Claudio Mayrink Verdun",
      "Sajani Vithana",
      "Taesup Moon",
      "Flavio P. Calmon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Messaoud_Towards_Generalizable_Trajectory_Prediction_using_Dual-Level_Representation_Learning_and_Adaptive_CVPR_2025_paper.html": {
    "title": "Towards Generalizable Trajectory Prediction using Dual-Level Representation Learning and Adaptive Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaouther Messaoud",
      "Matthieu Cord",
      "Alexandre Alahi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_CoMatcher_Multi-View_Collaborative_Feature_Matching_CVPR_2025_paper.html": {
    "title": "CoMatcher: Multi-View Collaborative Feature Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jintao Zhang",
      "Zimin Xia",
      "Mingyue Dong",
      "Shuhan Shen",
      "Linwei Yue",
      "Xianwei Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_COUNTS_Benchmarking_Object_Detectors_and_Multimodal_Large_Language_Models_under_CVPR_2025_paper.html": {
    "title": "COUNTS: Benchmarking Object Detectors and Multimodal Large Language Models under Distribution Shifts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiansheng Li",
      "Xingxuan Zhang",
      "Hao Zou",
      "Yige Guo",
      "Renzhe Xu",
      "Yilong Liu",
      "Chuzhao Zhu",
      "Yue He",
      "Peng Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mughal_Retrieving_Semantics_from_the_Deep_an_RAG_Solution_for_Gesture_CVPR_2025_paper.html": {
    "title": "Retrieving Semantics from the Deep: an RAG Solution for Gesture Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "M. Hamza Mughal",
      "Rishabh Dabral",
      "Merel C.J. Scholman",
      "Vera Demberg",
      "Christian Theobalt"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_HOT_Hadamard-based_Optimized_Training_CVPR_2025_paper.html": {
    "title": "HOT: Hadamard-based Optimized Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seonggon Kim",
      "Juncheol Shin",
      "Seung-taek Woo",
      "Eunhyeok Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kundu_Towards_a_Universal_Synthetic_Video_Detector_From_Face_or_Background_CVPR_2025_paper.html": {
    "title": "Towards a Universal Synthetic Video Detector: From Face or Background Manipulations to Fully AI-Generated Content",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohit Kundu",
      "Hao Xiong",
      "Vishal Mohanty",
      "Athula Balachandran",
      "Amit K. Roy-Chowdhury"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qu_TokenFlow_Unified_Image_Tokenizer_for_Multimodal_Understanding_and_Generation_CVPR_2025_paper.html": {
    "title": "TokenFlow: Unified Image Tokenizer for Multimodal Understanding and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liao Qu",
      "Huichao Zhang",
      "Yiheng Liu",
      "Xu Wang",
      "Yi Jiang",
      "Yiming Gao",
      "Hu Ye",
      "Daniel K. Du",
      "Zehuan Yuan",
      "Xinglong Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ryan_Improving_Personalized_Search_with_Regularized_Low-Rank_Parameter_Updates_CVPR_2025_paper.html": {
    "title": "Improving Personalized Search with Regularized Low-Rank Parameter Updates",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fiona Ryan",
      "Josef Sivic",
      "Fabian Caba Heilbron",
      "Judy Hoffman",
      "James M. Rehg",
      "Bryan Russell"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_A_Focused_Human_Body_Model_for_Accurate_Anthropometric_Measurements_Extraction_CVPR_2025_paper.html": {
    "title": "A Focused Human Body Model for Accurate Anthropometric Measurements Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuhang Chen",
      "Xianliang Huang",
      "Zhizhou Zhong",
      "Juhong Guan",
      "Shuigeng Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_SnapGen-V_Generating_a_Five-Second_Video_within_Five_Seconds_on_a_CVPR_2025_paper.html": {
    "title": "SnapGen-V: Generating a Five-Second Video within Five Seconds on a Mobile Device",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushu Wu",
      "Zhixing Zhang",
      "Yanyu Li",
      "Yanwu Xu",
      "Anil Kag",
      "Yang Sui",
      "Huseyin Coskun",
      "Ke Ma",
      "Aleksei Lebedev",
      "Ju Hu",
      "Dimitris N. Metaxas",
      "Yanzhi Wang",
      "Sergey Tulyakov",
      "Jian Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Adapting_Dense_Matching_for_Homography_Estimation_with_Grid-based_Acceleration_CVPR_2025_paper.html": {
    "title": "Adapting Dense Matching for Homography Estimation with Grid-based Acceleration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaining Zhang",
      "Yuxin Deng",
      "Jiayi Ma",
      "Paolo Favaro"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_HyperLoRA_Parameter-Efficient_Adaptive_Generation_for_Portrait_Synthesis_CVPR_2025_paper.html": {
    "title": "HyperLoRA: Parameter-Efficient Adaptive Generation for Portrait Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengtian Li",
      "Jinshu Chen",
      "Wanquan Feng",
      "Bingchuan Li",
      "Fei Dai",
      "Songtao Zhao",
      "Qian He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_ACE_Anti-Editing_Concept_Erasure_in_Text-to-Image_Models_CVPR_2025_paper.html": {
    "title": "ACE: Anti-Editing Concept Erasure in Text-to-Image Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihao Wang",
      "Yuxiang Wei",
      "Fan Li",
      "Renjing Pei",
      "Hang Xu",
      "Wangmeng Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_EchoMatch_Partial-to-Partial_Shape_Matching_via_Correspondence_Reflection_CVPR_2025_paper.html": {
    "title": "EchoMatch: Partial-to-Partial Shape Matching via Correspondence Reflection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizheng Xie",
      "Viktoria Ehm",
      "Paul Roetzer",
      "Nafie El Amrani",
      "Maolin Gao",
      "Florian Bernard",
      "Daniel Cremers"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_CoSDH_Communication-Efficient_Collaborative_Perception_via_Supply-Demand_Awareness_and_Intermediate-Late_Hybridization_CVPR_2025_paper.html": {
    "title": "CoSDH: Communication-Efficient Collaborative Perception via Supply-Demand Awareness and Intermediate-Late Hybridization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhao Xu",
      "Yanan Zhang",
      "Zhi Cai",
      "Di Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bartolomei_Stereo_Anywhere_Robust_Zero-Shot_Deep_Stereo_Matching_Even_Where_Either_CVPR_2025_paper.html": {
    "title": "Stereo Anywhere: Robust Zero-Shot Deep Stereo Matching Even Where Either Stereo or Mono Fail",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Bartolomei",
      "Fabio Tosi",
      "Matteo Poggi",
      "Stefano Mattoccia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lai_Order-Robust_Class_Incremental_Learning_Graph-Driven_Dynamic_Similarity_Grouping_CVPR_2025_paper.html": {
    "title": "Order-Robust Class Incremental Learning: Graph-Driven Dynamic Similarity Grouping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guannan Lai",
      "Yujie Li",
      "Xiangkun Wang",
      "Junbo Zhang",
      "Tianrui Li",
      "Xin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yuan_Where_the_Devil_Hides_Deepfake_Detectors_Can_No_Longer_Be_CVPR_2025_paper.html": {
    "title": "Where the Devil Hides: Deepfake Detectors Can No Longer Be Trusted",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuaiwei Yuan",
      "Junyu Dong",
      "Yuezun Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Synthetic-to-Real_Self-supervised_Robust_Depth_Estimation_via_Learning_with_Motion_and_CVPR_2025_paper.html": {
    "title": "Synthetic-to-Real Self-supervised Robust Depth Estimation via Learning with Motion and Structure Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weilong Yan",
      "Ming Li",
      "Haipeng Li",
      "Shuwei Shao",
      "Robby T. Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Daryani_CaMuViD_Calibration-Free_Multi-View_Detection_CVPR_2025_paper.html": {
    "title": "CaMuViD: Calibration-Free Multi-View Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Etefaghi Daryani",
      "M. Usman Maqbool Bhutta",
      "Byron Hernandez",
      "Henry Medeiros"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Prosody-Enhanced_Acoustic_Pre-training_and_Acoustic-Disentangled_Prosody_Adapting_for_Movie_Dubbing_CVPR_2025_paper.html": {
    "title": "Prosody-Enhanced Acoustic Pre-training and Acoustic-Disentangled Prosody Adapting for Movie Dubbing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhedong Zhang",
      "Liang Li",
      "Chenggang Yan",
      "Chunshan Liu",
      "Anton van den Hengel",
      "Yuankai Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Hierarchical_Knowledge_Prompt_Tuning_for_Multi-task_Test-Time_Adaptation_CVPR_2025_paper.html": {
    "title": "Hierarchical Knowledge Prompt Tuning for Multi-task Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiang Zhang",
      "Mengsheng Zhao",
      "Jiawei Liu",
      "Fanrui Zhang",
      "Yongchao Xu",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mei_Cross-Modal_Interactive_Perception_Network_with_Mamba_for_Lung_Tumor_Segmentation_CVPR_2025_paper.html": {
    "title": "Cross-Modal Interactive Perception Network with Mamba for Lung Tumor Segmentation in PET-CT Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Mei",
      "Chenyu Lin",
      "Yu Qiu",
      "Yaonan Wang",
      "Hui Zhang",
      "Ziyang Wang",
      "Dong Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jin_LaTexBlend_Scaling_Multi-concept_Customized_Generation_with_Latent_Textual_Blending_CVPR_2025_paper.html": {
    "title": "LaTexBlend: Scaling Multi-concept Customized Generation with Latent Textual Blending",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Jin",
      "Zhenbo Yu",
      "Yang Shen",
      "Zhenyong Fu",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ho_DejaVid_Encoder-Agnostic_Learned_Temporal_Matching_for_Video_Classification_CVPR_2025_paper.html": {
    "title": "DejaVid: Encoder-Agnostic Learned Temporal Matching for Video Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Darryl Ho",
      "Samuel Madden"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_HVI_A_New_Color_Space_for_Low-light_Image_Enhancement_CVPR_2025_paper.html": {
    "title": "HVI: A New Color Space for Low-light Image Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingsen Yan",
      "Yixu Feng",
      "Cheng Zhang",
      "Guansong Pang",
      "Kangbiao Shi",
      "Peng Wu",
      "Wei Dong",
      "Jinqiu Sun",
      "Yanning Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kaye_DualPM_Dual_Posed-Canonical_Point_Maps_for_3D_Shape_and_Pose_CVPR_2025_paper.html": {
    "title": "DualPM: Dual Posed-Canonical Point Maps for 3D Shape and Pose Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ben Kaye",
      "Tomas Jakab",
      "Shangzhe Wu",
      "Christian Ruprecht",
      "Andrea Vedaldi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Du_SuperPC_A_Single_Diffusion_Model_for_Point_Cloud_Completion_Upsampling_CVPR_2025_paper.html": {
    "title": "SuperPC: A Single Diffusion Model for Point Cloud Completion, Upsampling, Denoising, and Colorization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Du",
      "Zhipeng Zhao",
      "Shaoshu Su",
      "Sharath Golluri",
      "Haoze Zheng",
      "Runmao Yao",
      "Chen Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Le_One_Diffusion_to_Generate_Them_All_CVPR_2025_paper.html": {
    "title": "One Diffusion to Generate Them All",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duong H. Le",
      "Tuan Pham",
      "Sangho Lee",
      "Christopher Clark",
      "Aniruddha Kembhavi",
      "Stephan Mandt",
      "Ranjay Krishna",
      "Jiasen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Lets_Verify_and_Reinforce_Image_Generation_Step_by_Step_CVPR_2025_paper.html": {
    "title": "Let's Verify and Reinforce Image Generation Step by Step",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renrui Zhang",
      "Chengzhuo Tong",
      "Zhizheng Zhao",
      "Ziyu Guo",
      "Haoquan Zhang",
      "Manyuan Zhang",
      "Jiaming Liu",
      "Peng Gao",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_All-Optical_Nonlinear_Diffractive_Deep_Network_for_Ultrafast_Image_Denoising_CVPR_2025_paper.html": {
    "title": "All-Optical Nonlinear Diffractive Deep Network for Ultrafast Image Denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoling Zhou",
      "Zhemg Lee",
      "Wei Ye",
      "Rui Xie",
      "Wenbo Zhang",
      "Guanju Peng",
      "Zongze Li",
      "Shikun Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ni_Maintaining_Consistent_Inter-Class_Topology_in_Continual_Test-Time_Adaptation_CVPR_2025_paper.html": {
    "title": "Maintaining Consistent Inter-Class Topology in Continual Test-Time Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenggong Ni",
      "Fan Lyu",
      "Jiayao Tan",
      "Fuyuan Hu",
      "Rui Yao",
      "Tao Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_UNOPose_Unseen_Object_Pose_Estimation_with_an_Unposed_RGB-D_Reference_CVPR_2025_paper.html": {
    "title": "UNOPose: Unseen Object Pose Estimation with an Unposed RGB-D Reference Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Liu",
      "Gu Wang",
      "Ruida Zhang",
      "Chenyangguang Zhang",
      "Federico Tombari",
      "Xiangyang Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_CoSER_Towards_Consistent_Dense_Multiview_Text-to-Image_Generator_for_3D_Creation_CVPR_2025_paper.html": {
    "title": "CoSER: Towards Consistent Dense Multiview Text-to-Image Generator for 3D Creation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bonan Li",
      "Zicheng Zhang",
      "Xingyi Yang",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sarvestani_HybridMQA_Exploring_Geometry-Texture_Interactions_for_Colored_Mesh_Quality_Assessment_CVPR_2025_paper.html": {
    "title": "HybridMQA: Exploring Geometry-Texture Interactions for Colored Mesh Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Armin Shafiee Sarvestani",
      "Sheyang Tang",
      "Zhou Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peng_Generalized_Gaussian_Entropy_Model_for_Point_Cloud_Attribute_Compression_with_CVPR_2025_paper.html": {
    "title": "Generalized Gaussian Entropy Model for Point Cloud Attribute Compression with Dynamic Likelihood Intervals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changhao Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Self-Learning_Hyperspectral_and_Multispectral_Image_Fusion_via_Adaptive_Residual_Guided_CVPR_2025_paper.html": {
    "title": "Self-Learning Hyperspectral and Multispectral Image Fusion via Adaptive Residual Guided Subspace Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Zhu",
      "He Wang",
      "Yang Xu",
      "Zebin Wu",
      "Zhihui Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mao_SIR-DIFF_Sparse_Image_Sets_Restoration_with_Multi-View_Diffusion_Model_CVPR_2025_paper.html": {
    "title": "SIR-DIFF: Sparse Image Sets Restoration with Multi-View Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yucheng Mao",
      "Boyang Wang",
      "Nilesh Kulkarni",
      "Jeong Joon Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_StickMotion_Generating_3D_Human_Motions_by_Drawing_a_Stickman_CVPR_2025_paper.html": {
    "title": "StickMotion: Generating 3D Human Motions by Drawing a Stickman",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Wang",
      "Zhihua Wu",
      "Qiaozhi He",
      "Jiaming Chu",
      "Ling Qian",
      "Yu Cheng",
      "Junliang Xing",
      "Jian Zhao",
      "Lei Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Reversible_Decoupling_Network_for_Single_Image_Reflection_Removal_CVPR_2025_paper.html": {
    "title": "Reversible Decoupling Network for Single Image Reflection Removal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zhao",
      "Mingjia Li",
      "Qiming Hu",
      "Xiaojie Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhong_Hierarchical_Features_Matter_A_Deep_Exploration_of_Progressive_Parameterization_Method_CVPR_2025_paper.html": {
    "title": "Hierarchical Features Matter: A Deep Exploration of Progressive Parameterization Method for Dataset Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhao Zhong",
      "Hao Fang",
      "Bin Chen",
      "Xulin Gu",
      "Meikang Qiu",
      "Shuhan Qi",
      "Shu-Tao Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Enduring_Efficient_and_Robust_Trajectory_Prediction_Attack_in_Autonomous_Driving_CVPR_2025_paper.html": {
    "title": "Enduring, Efficient and Robust Trajectory Prediction Attack in Autonomous Driving via Optimization-Driven Multi-Frame Perturbation Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Yu",
      "Weizhen Han",
      "Libing Wu",
      "Bingyi Liu",
      "Enshu Wang",
      "Zhuangzhuang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Singh_GLASS_Guided_Latent_Slot_Diffusion_for_Object-Centric_Learning_CVPR_2025_paper.html": {
    "title": "GLASS: Guided Latent Slot Diffusion for Object-Centric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krishnakant Singh",
      "Simone Schaub-Meyer",
      "Stefan Roth"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_UNEM_UNrolled_Generalized_EM_for_Transductive_Few-Shot_Learning_CVPR_2025_paper.html": {
    "title": "UNEM: UNrolled Generalized EM for Transductive Few-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long Zhou",
      "Fereshteh Shakeri",
      "Aymen Sadraoui",
      "Mounir Kaaniche",
      "Jean-Christophe Pesquet",
      "Ismail Ben Ayed"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_SASep_Saliency-Aware_Structured_Separation_of_Geometry_and_Feature_for_Open_CVPR_2025_paper.html": {
    "title": "SASep: Saliency-Aware Structured Separation of Geometry and Feature for Open Set Learning on Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinfeng Xu",
      "Xianzhi Li",
      "Yuan Tang",
      "Xu Han",
      "Qiao Yu",
      "Yixue Hao",
      "Long Hu",
      "Min Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Low-Biased_General_Annotated_Dataset_Generation_CVPR_2025_paper.html": {
    "title": "Low-Biased General Annotated Dataset Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dengyang Jiang",
      "Haoyu Wang",
      "Lei Zhang",
      "Wei Wei",
      "Guang Dai",
      "Mengmeng Wang",
      "Jingdong Wang",
      "Yanning Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_G3Flow_Generative_3D_Semantic_Flow_for_Pose-aware_and_Generalizable_Object_CVPR_2025_paper.html": {
    "title": "G3Flow: Generative 3D Semantic Flow for Pose-aware and Generalizable Object Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianxing Chen",
      "Yao Mu",
      "Zhixuan Liang",
      "Zanxin Chen",
      "Shijia Peng",
      "Qiangyu Chen",
      "Mingkun Xu",
      "Ruizhen Hu",
      "Hongyuan Zhang",
      "Xuelong Li",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Generative_Hard_Example_Augmentation_for_Semantic_Point_Cloud_Segmentation_CVPR_2025_paper.html": {
    "title": "Generative Hard Example Augmentation for Semantic Point Cloud Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zhang",
      "Jibin Peng",
      "Zhao Huang",
      "Wei Feng",
      "Di Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Toward_Generalized_Image_Quality_Assessment_Relaxing_the_Perfect_Reference_Quality_CVPR_2025_paper.html": {
    "title": "Toward Generalized Image Quality Assessment: Relaxing the Perfect Reference Quality Assumption",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Du Chen",
      "Tianhe Wu",
      "Kede Ma",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zeng_Explaining_Domain_Shifts_in_Language_Concept_Erasing_for_Interpretable_Image_CVPR_2025_paper.html": {
    "title": "Explaining Domain Shifts in Language: Concept Erasing for Interpretable Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zequn Zeng",
      "Yudi Su",
      "Jianqiao Sun",
      "Tiansheng Wen",
      "Hao Zhang",
      "Zhengjue Wang",
      "Bo Chen",
      "Hongwei Liu",
      "Jiawei Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ni_Hazy_Low-Quality_Satellite_Video_Restoration_Via_Learning_Optimal_Joint_Degradation_CVPR_2025_paper.html": {
    "title": "Hazy Low-Quality Satellite Video Restoration Via Learning Optimal Joint Degradation Patterns and Continuous-Scale Super-Resolution Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ning Ni",
      "Libao Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chao_Textured_Gaussians_for_Enhanced_3D_Scene_Appearance_Modeling_CVPR_2025_paper.html": {
    "title": "Textured Gaussians for Enhanced 3D Scene Appearance Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brian Chao",
      "Hung-Yu Tseng",
      "Lorenzo Porzi",
      "Chen Gao",
      "Tuotuo Li",
      "Qinbo Li",
      "Ayush Saraf",
      "Jia-Bin Huang",
      "Johannes Kopf",
      "Gordon Wetzstein",
      "Changil Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_NeighborRetr_Balancing_Hub_Centrality_in_Cross-Modal_Retrieval_CVPR_2025_paper.html": {
    "title": "NeighborRetr: Balancing Hub Centrality in Cross-Modal Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zengrong Lin",
      "Zheng Wang",
      "Tianwen Qian",
      "Pan Mu",
      "Sixian Chan",
      "Cong Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hamann_ETAP_Event-based_Tracking_of_Any_Point_CVPR_2025_paper.html": {
    "title": "ETAP: Event-based Tracking of Any Point",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Friedhelm Hamann",
      "Daniel Gehrig",
      "Filbert Febryanto",
      "Kostas Daniilidis",
      "Guillermo Gallego"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Beyond_Sight_Towards_Cognitive_Alignment_in_LVLM_via_Enriched_Visual_CVPR_2025_paper.html": {
    "title": "Beyond Sight: Towards Cognitive Alignment in LVLM via Enriched Visual Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaqi Zhao",
      "Yuanyang Yin",
      "Lin Li",
      "Mingan Lin",
      "Victor Shea-Jay Huang",
      "Siwei Chen",
      "Weipeng Chen",
      "Baoqun Yin",
      "Zenan Zhou",
      "Wentao Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_Global-Local_Tree_Search_in_VLMs_for_3D_Indoor_Scene_Generation_CVPR_2025_paper.html": {
    "title": "Global-Local Tree Search in VLMs for 3D Indoor Scene Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Deng",
      "Mengshi Qi",
      "Huadong Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Esposito_Volumetric_Surfaces_Representing_Fuzzy_Geometries_with_Layered_Meshes_CVPR_2025_paper.html": {
    "title": "Volumetric Surfaces: Representing Fuzzy Geometries with Layered Meshes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefano Esposito",
      "Anpei Chen",
      "Christian Reiser",
      "Samuel Rota Bul√≤",
      "Lorenzo Porzi",
      "Katja Schwarz",
      "Christian Richardt",
      "Michael Zollh√∂fer",
      "Peter Kontschieder",
      "Andreas Geiger"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Overcoming_Shortcut_Problem_in_VLM_for_Robust_Out-of-Distribution_Detection_CVPR_2025_paper.html": {
    "title": "Overcoming Shortcut Problem in VLM for Robust Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuo Xu",
      "Xiang Xiang",
      "Yifan Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kang_GFlowVLM_Enhancing_Multi-step_Reasoning_in_Vision-Language_Models_with_Generative_Flow_CVPR_2025_paper.html": {
    "title": "GFlowVLM: Enhancing Multi-step Reasoning in Vision-Language Models with Generative Flow Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoqiang Kang",
      "Enna Sachdeva",
      "Piyush Gupta",
      "Sangjae Bae",
      "Kwonjoon Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qiu_STEPS_Sequential_Probability_Tensor_Estimation_for_Text-to-Image_Hard_Prompt_Search_CVPR_2025_paper.html": {
    "title": "STEPS: Sequential Probability Tensor Estimation for Text-to-Image Hard Prompt Search",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuning Qiu",
      "Andong Wang",
      "Chao Li",
      "Haonan Huang",
      "Guoxu Zhou",
      "Qibin Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Song_RoboSpatial_Teaching_Spatial_Understanding_to_2D_and_3D_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chan Hee Song",
      "Valts Blukis",
      "Jonathan Tremblay",
      "Stephen Tyree",
      "Yu Su",
      "Stan Birchfield"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Weng_VIRES_Video_Instance_Repainting_via_Sketch_and_Text_Guided_Generation_CVPR_2025_paper.html": {
    "title": "VIRES: Video Instance Repainting via Sketch and Text Guided Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuchen Weng",
      "Haojie Zheng",
      "Peixuan Zhang",
      "Yuchen Hong",
      "Han Jiang",
      "Si Li",
      "Boxin Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MAP_Unleashing_Hybrid_Mamba-Transformer_Vision_Backbones_Potential_with_Masked_Autoregressive_CVPR_2025_paper.html": {
    "title": "MAP: Unleashing Hybrid Mamba-Transformer Vision Backbone's Potential with Masked Autoregressive Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunze Liu",
      "Li Yi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Segment_Any-Quality_Images_with_Generative_Latent_Space_Enhancement_CVPR_2025_paper.html": {
    "title": "Segment Any-Quality Images with Generative Latent Space Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangqian Guo",
      "Yong Guo",
      "Xuehui Yu",
      "Wenbo Li",
      "Yaoxing Wang",
      "Shan Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_BG-Triangle_Bezier_Gaussian_Triangle_for_3D_Vectorization_and_Rendering_CVPR_2025_paper.html": {
    "title": "BG-Triangle: Bezier Gaussian Triangle for 3D Vectorization and Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minye Wu",
      "Haizhao Dai",
      "Kaixin Yao",
      "Tinne Tuytelaars",
      "Jingyi Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Men_MIMO_Controllable_Character_Video_Synthesis_with_Spatial_Decomposed_Modeling_CVPR_2025_paper.html": {
    "title": "MIMO: Controllable Character Video Synthesis with Spatial Decomposed Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifang Men",
      "Yuan Yao",
      "Miaomiao Cui",
      "Liefeng Bo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Morita_TKG-DM_Training-free_Chroma_Key_Content_Generation_Diffusion_Model_CVPR_2025_paper.html": {
    "title": "TKG-DM: Training-free Chroma Key Content Generation Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryugo Morita",
      "Stanislav Frolov",
      "Brian Bernhard Moser",
      "Takahiro Shirakawa",
      "Ko Watanabe",
      "Andreas Dengel",
      "Jinjia Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jia_Lift3D_Policy_Lifting_2D_Foundation_Models_for_Robust_3D_Robotic_CVPR_2025_paper.html": {
    "title": "Lift3D Policy: Lifting 2D Foundation Models for Robust 3D Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yueru Jia",
      "Jiaming Liu",
      "Sixiang Chen",
      "Chenyang Gu",
      "Zhilve Wang",
      "Longzan Luo",
      "Xiaoqi Li",
      "Pengwei Wang",
      "Zhongyuan Wang",
      "Renrui Zhang",
      "Shanghang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Galappaththige_Multi-View_Pose-Agnostic_Change_Localization_with_Zero_Labels_CVPR_2025_paper.html": {
    "title": "Multi-View Pose-Agnostic Change Localization with Zero Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chamuditha Jayanga Galappaththige",
      "Jason Lai",
      "Lloyd Windrim",
      "Donald Dansereau",
      "Niko Sunderhauf",
      "Dimity Miller"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_From_Sparse_to_Dense_Camera_Relocalization_with_Scene-Specific_Detector_from_CVPR_2025_paper.html": {
    "title": "From Sparse to Dense: Camera Relocalization with Scene-Specific Detector from Feature Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwei Huang",
      "Hailin Yu",
      "Yichun Shentu",
      "Jin Yuan",
      "Guofeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Accelerating_Diffusion_Transformer_via_Increment-Calibrated_Caching_with_Channel-Aware_Singular_Value_CVPR_2025_paper.html": {
    "title": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with Channel-Aware Singular Value Decomposition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Chen",
      "Keyi Li",
      "Yifan Jia",
      "Le Ye",
      "Yufei Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_CityWalker_Learning_Embodied_Urban_Navigation_from_Web-Scale_Videos_CVPR_2025_paper.html": {
    "title": "CityWalker: Learning Embodied Urban Navigation from Web-Scale Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinhao Liu",
      "Jintong Li",
      "Yicheng Jiang",
      "Niranjan Sujay",
      "Zhicheng Yang",
      "Juexiao Zhang",
      "John Abanes",
      "Jing Zhang",
      "Chen Feng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_A_Simple_yet_Effective_Layout_Token_in_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "A Simple yet Effective Layout Token in Large Language Models for Document Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoqing Zhu",
      "Chuwei Luo",
      "Zirui Shao",
      "Feiyu Gao",
      "Hangdi Xing",
      "Qi Zheng",
      "Ji Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yao_Reconstruction_vs._Generation_Taming_Optimization_Dilemma_in_Latent_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingfeng Yao",
      "Bin Yang",
      "Xinggang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tu_StableAnimator_High-Quality_Identity-Preserving_Human_Image_Animation_CVPR_2025_paper.html": {
    "title": "StableAnimator: High-Quality Identity-Preserving Human Image Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyuan Tu",
      "Zhen Xing",
      "Xintong Han",
      "Zhi-Qi Cheng",
      "Qi Dai",
      "Chong Luo",
      "Zuxuan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Stone_Learning_Visual_Composition_through_Improved_Semantic_Guidance_CVPR_2025_paper.html": {
    "title": "Learning Visual Composition through Improved Semantic Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Austin Stone",
      "Hagen Soltau",
      "Robert Geirhos",
      "Xi Yi",
      "Ye Xia",
      "Bingyi Cao",
      "Kaifeng Chen",
      "Abhijit Ogale",
      "Jonathon Shlens"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_OODD_Test-time_Out-of-Distribution_Detection_with_Dynamic_Dictionary_CVPR_2025_paper.html": {
    "title": "OODD: Test-time Out-of-Distribution Detection with Dynamic Dictionary",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifeng Yang",
      "Lin Zhu",
      "Zewen Sun",
      "Hengyu Liu",
      "Qinying Gu",
      "Nanyang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MEAT_Multiview_Diffusion_Model_for_Human_Generation_on_Megapixels_with_CVPR_2025_paper.html": {
    "title": "MEAT: Multiview Diffusion Model for Human Generation on Megapixels with Mesh Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Wang",
      "Fangzhou Hong",
      "Shuai Yang",
      "Liming Jiang",
      "Wayne Wu",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Meng_Free_Lunch_Enhancements_for_Multi-modal_Crowd_Counting_CVPR_2025_paper.html": {
    "title": "Free Lunch Enhancements for Multi-modal Crowd Counting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoliang Meng",
      "Xiaopeng Hong",
      "Zhengqin Lai",
      "Miao Shang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Islam_BIMBA_Selective-Scan_Compression_for_Long-Range_Video_Question_Answering_CVPR_2025_paper.html": {
    "title": "BIMBA: Selective-Scan Compression for Long-Range Video Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Mohaiminul Islam",
      "Tushar Nagarajan",
      "Huiyu Wang",
      "Gedas Bertasius",
      "Lorenzo Torresani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Miao_EVolSplat_Efficient_Volume-based_Gaussian_Splatting_for_Urban_View_Synthesis_CVPR_2025_paper.html": {
    "title": "EVolSplat: Efficient Volume-based Gaussian Splatting for Urban View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Miao",
      "Jiaxin Huang",
      "Dongfeng Bai",
      "Xu Yan",
      "Hongyu Zhou",
      "Yue Wang",
      "Bingbing Liu",
      "Andreas Geiger",
      "Yiyi Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Schusterbauer_Diff2Flow_Training_Flow_Matching_Models_via_Diffusion_Model_Alignment_CVPR_2025_paper.html": {
    "title": "Diff2Flow: Training Flow Matching Models via Diffusion Model Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Johannes Schusterbauer",
      "Ming Gui",
      "Frank Fundel",
      "Bj√∂rn Ommer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_JanusFlow_Harmonizing_Autoregression_and_Rectified_Flow_for_Unified_Multimodal_Understanding_CVPR_2025_paper.html": {
    "title": "JanusFlow: Harmonizing Autoregression and Rectified Flow for Unified Multimodal Understanding and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Ma",
      "Xingchao Liu",
      "Xiaokang Chen",
      "Wen Liu",
      "Chengyue Wu",
      "Zhiyu Wu",
      "Zizheng Pan",
      "Zhenda Xie",
      "Haowei Zhang",
      "Xingkai Yu",
      "Liang Zhao",
      "Yisong Wang",
      "Jiaying Liu",
      "Chong Ruan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Visual_Prompting_for_One-shot_Controllable_Video_Editing_without_Inversion_CVPR_2025_paper.html": {
    "title": "Visual Prompting for One-shot Controllable Video Editing without Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengbo Zhang",
      "Yuxi Zhou",
      "Duo Peng",
      "Joo-Hwee Lim",
      "Zhigang Tu",
      "De Wen Soh",
      "Lin Geng Foo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_PIDSR_Complementary_Polarized_Image_Demosaicing_and_Super-Resolution_CVPR_2025_paper.html": {
    "title": "PIDSR: Complementary Polarized Image Demosaicing and Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuangfan Zhou",
      "Chu Zhou",
      "Youwei Lyu",
      "Heng Guo",
      "Zhanyu Ma",
      "Boxin Shi",
      "Imari Sato"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_MegaSynth_Scaling_Up_3D_Scene_Reconstruction_with_Synthesized_Data_CVPR_2025_paper.html": {
    "title": "MegaSynth: Scaling Up 3D Scene Reconstruction with Synthesized Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanwen Jiang",
      "Zexiang Xu",
      "Desai Xie",
      "Ziwen Chen",
      "Haian Jin",
      "Fujun Luan",
      "Zhixin Shu",
      "Kai Zhang",
      "Sai Bi",
      "Xin Sun",
      "Jiuxiang Gu",
      "Qixing Huang",
      "Georgios Pavlakos",
      "Hao Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ruan_Prof._Robot_Differentiable_Robot_Rendering_Without_Static_and_Self-Collisions_CVPR_2025_paper.html": {
    "title": "Prof. Robot: Differentiable Robot Rendering Without Static and Self-Collisions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quanyuan Ruan",
      "Jiabao Lei",
      "Wenhao Yuan",
      "Yanglin Zhang",
      "Dekun Lu",
      "Guiliang Liu",
      "Kui Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_AVQACL_A_Novel_Benchmark_for_Audio-Visual_Question_Answering_Continual_Learning_CVPR_2025_paper.html": {
    "title": "AVQACL: A Novel Benchmark for Audio-Visual Question Answering Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaixuan Wu",
      "Xinde Li",
      "Xinling Li",
      "Chuanfei Hu",
      "Guoliang Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Flash-Split_2D_Reflection_Removal_with_Flash_Cues_and_Latent_Diffusion_CVPR_2025_paper.html": {
    "title": "Flash-Split: 2D Reflection Removal with Flash Cues and Latent Diffusion Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianfu Wang",
      "Mingyang Xie",
      "Haoming Cai",
      "Sachin Shah",
      "Christopher A. Metzler"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Serianni_Attention_IoU_Examining_Biases_in_CelebA_using_Attention_Maps_CVPR_2025_paper.html": {
    "title": "Attention IoU: Examining Biases in CelebA using Attention Maps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aaron Serianni",
      "Tyler Zhu",
      "Olga Russakovsky",
      "Vikram V. Ramaswamy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_HEIE_MLLM-Based_Hierarchical_Explainable_AIGC_Image_Implausibility_Evaluator_CVPR_2025_paper.html": {
    "title": "HEIE: MLLM-Based Hierarchical Explainable AIGC Image Implausibility Evaluator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan Yang",
      "Ru Zhen",
      "Jianing Wang",
      "Yanhao Zhang",
      "Haoxiang Chen",
      "Haonan Lu",
      "Sicheng Zhao",
      "Guiguang Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Segment_Any_Motion_in_Videos_CVPR_2025_paper.html": {
    "title": "Segment Any Motion in Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan Huang",
      "Wenzhao Zheng",
      "Chenfeng Xu",
      "Kurt Keutzer",
      "Shanghang Zhang",
      "Angjoo Kanazawa",
      "Qianqian Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_HandOS_3D_Hand_Reconstruction_in_One_Stage_CVPR_2025_paper.html": {
    "title": "HandOS: 3D Hand Reconstruction in One Stage",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Chen",
      "Zhuheng Song",
      "Xiaoke Jiang",
      "Yaoqing Hu",
      "Junzhi Yu",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Task-aware_Cross-modal_Feature_Refinement_Transformer_with_Large_Language_Models_for_CVPR_2025_paper.html": {
    "title": "Task-aware Cross-modal Feature Refinement Transformer with Large Language Models for Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbo Chen",
      "Zhen Xu",
      "Ruotao Xu",
      "Si Wu",
      "Hau-San Wong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_DropGaussian_Structural_Regularization_for_Sparse-view_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "DropGaussian: Structural Regularization for Sparse-view Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunwoo Park",
      "Gun Ryu",
      "Wonjun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_All-Day_Multi-Camera_Multi-Target_Tracking_CVPR_2025_paper.html": {
    "title": "All-Day Multi-Camera Multi-Target Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huijie Fan",
      "Yu Qiao",
      "Yihao Zhen",
      "Tinghui Zhao",
      "Baojie Fan",
      "Qiang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Behari_Blurred_LiDAR_for_Sharper_3D_Robust_Handheld_3D_Scanning_with_CVPR_2025_paper.html": {
    "title": "Blurred LiDAR for Sharper 3D: Robust Handheld 3D Scanning with Diffuse LiDAR and RGB",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikhil Behari",
      "Aaron Young",
      "Siddharth Somasundaram",
      "Tzofi Klinghoffer",
      "Akshat Dave",
      "Ramesh Raskar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_EnergyMoGen_Compositional_Human_Motion_Generation_with_Energy-Based_Diffusion_Model_in_CVPR_2025_paper.html": {
    "title": "EnergyMoGen: Compositional Human Motion Generation with Energy-Based Diffusion Model in Latent Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianrong Zhang",
      "Hehe Fan",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jacob_PatchDEMUX_A_Certifiably_Robust_Framework_for_Multi-label_Classifiers_Against_Adversarial_CVPR_2025_paper.html": {
    "title": "PatchDEMUX: A Certifiably Robust Framework for Multi-label Classifiers Against Adversarial Patches",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dennis Jacob",
      "Chong Xiang",
      "Prateek Mittal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rodriguez_StarVector_Generating_Scalable_Vector_Graphics_Code_from_Images_and_Text_CVPR_2025_paper.html": {
    "title": "StarVector: Generating Scalable Vector Graphics Code from Images and Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juan A. Rodriguez",
      "Abhay Puri",
      "Shubham Agarwal",
      "Issam H. Laradji",
      "Pau Rodriguez",
      "Sai Rajeswar",
      "David Vazquez",
      "Christopher Pal",
      "Marco Pedersoli"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Elata_Novel_View_Synthesis_with_Pixel-Space_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Novel View Synthesis with Pixel-Space Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noam Elata",
      "Bahjat Kawar",
      "Yaron Ostrovsky-Berman",
      "Miriam Farber",
      "Ron Sokolovsky"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Object_Detection_using_Event_Camera_A_MoE_Heat_Conduction_based_CVPR_2025_paper.html": {
    "title": "Object Detection using Event Camera: A MoE Heat Conduction based Detector and A New Benchmark Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Wang",
      "Yu Jin",
      "Wentao Wu",
      "Wei Zhang",
      "Lin Zhu",
      "Bo Jiang",
      "Yonghong Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_EgoTextVQA_Towards_Egocentric_Scene-Text_Aware_Video_Question_Answering_CVPR_2025_paper.html": {
    "title": "EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Zhou",
      "Junbin Xiao",
      "Qingyun Li",
      "Yicong Li",
      "Xun Yang",
      "Dan Guo",
      "Meng Wang",
      "Tat-Seng Chua",
      "Angela Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bergner_Token_Cropr_Faster_ViTs_for_Quite_a_Few_Tasks_CVPR_2025_paper.html": {
    "title": "Token Cropr: Faster ViTs for Quite a Few Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Benjamin Bergner",
      "Christoph Lippert",
      "Aravindh Mahendran"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liao_STCOcc_Sparse_Spatial-Temporal_Cascade_Renovation_for_3D_Occupancy_and_Scene_CVPR_2025_paper.html": {
    "title": "STCOcc: Sparse Spatial-Temporal Cascade Renovation for 3D Occupancy and Scene Flow Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhimin Liao",
      "Ping Wei",
      "Shuaijia Chen",
      "Haoxuan Wang",
      "Ziyang Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Document_Haystacks__Vision-Language_Reasoning_Over_Piles_of_1000_Documents_CVPR_2025_paper.html": {
    "title": "Document Haystacks: Vision-Language Reasoning Over Piles of 1000+ Documents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Chen",
      "Dannong Xu",
      "Junjie Fei",
      "Chun-Mei Feng",
      "Mohamed Elhoseiny"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Farina_Rethinking_Few-Shot_Adaptation_of_Vision-Language_Models_in_Two_Stages_CVPR_2025_paper.html": {
    "title": "Rethinking Few-Shot Adaptation of Vision-Language Models in Two Stages",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matteo Farina",
      "Massimiliano Mancini",
      "Giovanni Iacca",
      "Elisa Ricci"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_Resilient_Sensor_Fusion_Under_Adverse_Sensor_Failures_via_Multi-Modal_Expert_CVPR_2025_paper.html": {
    "title": "Resilient Sensor Fusion Under Adverse Sensor Failures via Multi-Modal Expert Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konyul Park",
      "Yecheol Kim",
      "Daehun Kim",
      "Jun Won Choi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhai_TAGA_Self-supervised_Learning_for_Template-free_Animatable_Gaussian_Articulated_Model_CVPR_2025_paper.html": {
    "title": "TAGA: Self-supervised Learning for Template-free Animatable Gaussian Articulated Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhichao Zhai",
      "Guikun Chen",
      "Wenguan Wang",
      "Dong Zheng",
      "Jun Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MambaVO_Deep_Visual_Odometry_Based_on_Sequential_Matching_Refinement_and_CVPR_2025_paper.html": {
    "title": "MambaVO: Deep Visual Odometry Based on Sequential Matching Refinement and Training Smoothing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuo Wang",
      "Wanting Li",
      "Yongcai Wang",
      "Zhaoxin Fan",
      "Zhe Huang",
      "Xudong Cai",
      "Jian Zhao",
      "Deying Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kazimi_Explaining_in_Diffusion_Explaining_a_Classifier_with_Diffusion_Semantics_CVPR_2025_paper.html": {
    "title": "Explaining in Diffusion: Explaining a Classifier with Diffusion Semantics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tahira Kazimi",
      "Ritika Allada",
      "Pinar Yanardag"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Horizon-GS_Unified_3D_Gaussian_Splatting_for_Large-Scale_Aerial-to-Ground_Scenes_CVPR_2025_paper.html": {
    "title": "Horizon-GS: Unified 3D Gaussian Splatting for Large-Scale Aerial-to-Ground Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lihan Jiang",
      "Kerui Ren",
      "Mulin Yu",
      "Linning Xu",
      "Junting Dong",
      "Tao Lu",
      "Feng Zhao",
      "Dahua Lin",
      "Bo Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Attention_Distillation_A_Unified_Approach_to_Visual_Characteristics_Transfer_CVPR_2025_paper.html": {
    "title": "Attention Distillation: A Unified Approach to Visual Characteristics Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Zhou",
      "Xu Gao",
      "Zichong Chen",
      "Hui Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wei_From_Words_to_Structured_Visuals_A_Benchmark_and_Framework_for_CVPR_2025_paper.html": {
    "title": "From Words to Structured Visuals: A Benchmark and Framework for Text-to-Diagram Generation and Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingxuan Wei",
      "Cheng Tan",
      "Qi Chen",
      "Gaowei Wu",
      "Siyuan Li",
      "Zhangyang Gao",
      "Linzhuang Sun",
      "Bihui Yu",
      "Ruifeng Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Matsui_LotusFilter_Fast_Diverse_Nearest_Neighbor_Search_via_a_Learned_Cutoff_CVPR_2025_paper.html": {
    "title": "LotusFilter: Fast Diverse Nearest Neighbor Search via a Learned Cutoff Table",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yusuke Matsui"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_DreamRelation_Bridging_Customization_and_Relation_Generation_CVPR_2025_paper.html": {
    "title": "DreamRelation: Bridging Customization and Relation Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingyu Shi",
      "Lu Qi",
      "Jianzong Wu",
      "Jinbin Bai",
      "Jingbo Wang",
      "Yunhai Tong",
      "Xiangtai Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ruan_IndoorGS_Geometric_Cues_Guided_Gaussian_Splatting_for_Indoor_Scene_Reconstruction_CVPR_2025_paper.html": {
    "title": "IndoorGS: Geometric Cues Guided Gaussian Splatting for Indoor Scene Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Ruan",
      "Yuesong Wang",
      "Tao Guan",
      "Bin Zhang",
      "Lili Ju"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Point-Cache_Test-time_Dynamic_and_Hierarchical_Cache_for_Robust_and_Generalizable_CVPR_2025_paper.html": {
    "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and Generalizable Point Cloud Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyu Sun",
      "Qiuhong Ke",
      "Ming Cheng",
      "Yongcai Wang",
      "Deying Li",
      "Chenhui Gou",
      "Jianfei Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yao_Think_Small_Act_Big_Primitive_Prompt_Learning_for_Lifelong_Robot_CVPR_2025_paper.html": {
    "title": "Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanqi Yao",
      "Siao Liu",
      "Haoming Song",
      "Delin Qu",
      "Qizhi Chen",
      "Yan Ding",
      "Bin Zhao",
      "Zhigang Wang",
      "Xuelong Li",
      "Dong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Doldo_Stop_Walking_in_Circles_Bailing_Out_Early_in_Projected_Gradient_CVPR_2025_paper.html": {
    "title": "Stop Walking in Circles! Bailing Out Early in Projected Gradient Descent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Philip Doldo",
      "Derek Everett",
      "Amol Khanna",
      "Andre T Nguyen",
      "Edward Raff"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_MoManipVLA_Transferring_Vision-language-action_Models_for_General_Mobile_Manipulation_CVPR_2025_paper.html": {
    "title": "MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Wu",
      "Yuheng Zhou",
      "Xiuwei Xu",
      "Ziwei Wang",
      "Haibin Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yun_SoMA_Singular_Value_Decomposed_Minor_Components_Adaptation_for_Domain_Generalizable_CVPR_2025_paper.html": {
    "title": "SoMA: Singular Value Decomposed Minor Components Adaptation for Domain Generalizable Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seokju Yun",
      "Seunghye Chae",
      "Dongheon Lee",
      "Youngmin Ro"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_Depth-Guided_Bundle_Sampling_for_Efficient_Generalizable_Neural_Radiance_Field_Reconstruction_CVPR_2025_paper.html": {
    "title": "Depth-Guided Bundle Sampling for Efficient Generalizable Neural Radiance Field Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Fang",
      "Hao Zhu",
      "Longlong Chen",
      "Fei Hu",
      "Long Ye",
      "Zhan Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_TinyFusion_Diffusion_Transformers_Learned_Shallow_CVPR_2025_paper.html": {
    "title": "TinyFusion: Diffusion Transformers Learned Shallow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gongfan Fang",
      "Kunjun Li",
      "Xinyin Ma",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Ref-GS_Directional_Factorization_for_2D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "Ref-GS: Directional Factorization for 2D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youjia Zhang",
      "Anpei Chen",
      "Yumin Wan",
      "Zikai Song",
      "Junqing Yu",
      "Yawei Luo",
      "Wei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_SVG-IR_Spatially-Varying_Gaussian_Splatting_for_Inverse_Rendering_CVPR_2025_paper.html": {
    "title": "SVG-IR: Spatially-Varying Gaussian Splatting for Inverse Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanxiao Sun",
      "Yupeng Gao",
      "Jin Xie",
      "Jian Yang",
      "Beibei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Stable-SCore_A_Stable_Registration-based_Framework_for_3D_Shape_Correspondence_CVPR_2025_paper.html": {
    "title": "Stable-SCore: A Stable Registration-based Framework for 3D Shape Correspondence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haolin Liu",
      "Xiaohang Zhan",
      "Zizheng Yan",
      "Zhongjin Luo",
      "Yuxin Wen",
      "Xiaoguang Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mao_Beyond_Single-Modal_Boundary_Cross-Modal_Anomaly_Detection_through_Visual_Prototype_and_CVPR_2025_paper.html": {
    "title": "Beyond Single-Modal Boundary: Cross-Modal Anomaly Detection through Visual Prototype and Harmonization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Mao",
      "Ping Wei",
      "Yiyang Lian",
      "Yangyang Wang",
      "Nanning Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tanaka_VDocRAG_Retrieval-Augmented_Generation_over_Visually-Rich_Documents_CVPR_2025_paper.html": {
    "title": "VDocRAG: Retrieval-Augmented Generation over Visually-Rich Documents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ryota Tanaka",
      "Taichi Iki",
      "Taku Hasegawa",
      "Kyosuke Nishida",
      "Kuniko Saito",
      "Jun Suzuki"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_Align-KD_Distilling_Cross-Modal_Alignment_Knowledge_for_Mobile_Vision-Language_Large_Model_CVPR_2025_paper.html": {
    "title": "Align-KD: Distilling Cross-Modal Alignment Knowledge for Mobile Vision-Language Large Model Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianhan Feng",
      "Wenshuo Li",
      "Tong Lin",
      "Xinghao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Subramanian_Pose_Priors_from_Language_Models_CVPR_2025_paper.html": {
    "title": "Pose Priors from Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanjay Subramanian",
      "Evonne Ng",
      "Lea M√ºller",
      "Dan Klein",
      "Shiry Ginosar",
      "Trevor Darrell"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_Concept_Lancet_Image_Editing_with_Compositional_Representation_Transplant_CVPR_2025_paper.html": {
    "title": "Concept Lancet: Image Editing with Compositional Representation Transplant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinqi Luo",
      "Tianjiao Ding",
      "Kwan Ho Ryan Chan",
      "Hancheng Min",
      "Chris Callison-Burch",
      "Rene Vidal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Weng_Scaling_Mesh_Generation_via_Compressive_Tokenization_CVPR_2025_paper.html": {
    "title": "Scaling Mesh Generation via Compressive Tokenization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haohan Weng",
      "Zibo Zhao",
      "Biwen Lei",
      "Xianghui Yang",
      "Jian Liu",
      "Zeqiang Lai",
      "Zhuo Chen",
      "Yuhong Liu",
      "Jie Jiang",
      "Chunchao Guo",
      "Tong Zhang",
      "Shenghua Gao",
      "C.L. Philip Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nam_Generative_Densification_Learning_to_Densify_Gaussians_for_High-Fidelity_Generalizable_3D_CVPR_2025_paper.html": {
    "title": "Generative Densification: Learning to Densify Gaussians for High-Fidelity Generalizable 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungtae Nam",
      "Xiangyu Sun",
      "Gyeongjin Kang",
      "Younggeun Lee",
      "Seungjun Oh",
      "Eunbyung Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_LogoSP_Local-global_Grouping_of_Superpoints_for_Unsupervised_Semantic_Segmentation_of_CVPR_2025_paper.html": {
    "title": "LogoSP: Local-global Grouping of Superpoints for Unsupervised Semantic Segmentation of 3D Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihui Zhang",
      "Weisheng Dai",
      "Hongtao Wen",
      "Bo Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_Exploring_Intrinsic_Normal_Prototypes_within_a_Single_Image_for_Universal_CVPR_2025_paper.html": {
    "title": "Exploring Intrinsic Normal Prototypes within a Single Image for Universal Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Luo",
      "Yunkang Cao",
      "Haiming Yao",
      "Xiaotian Zhang",
      "Jianan Lou",
      "Yuqi Cheng",
      "Weiming Shen",
      "Wenyong Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Augmenting_Perceptual_Super-Resolution_via_Image_Quality_Predictors_CVPR_2025_paper.html": {
    "title": "Augmenting Perceptual Super-Resolution via Image Quality Predictors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengjia Zhang",
      "Samrudhdhi B. Rangrej",
      "Tristan Aumentado-Armstrong",
      "Afsaneh Fazly",
      "Alex Levinshtein"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_TurboFill_Adapting_Few-step_Text-to-image_Model_for_Fast_Image_Inpainting_CVPR_2025_paper.html": {
    "title": "TurboFill: Adapting Few-step Text-to-image Model for Fast Image Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liangbin Xie",
      "Daniil Pakhomov",
      "Zhonghao Wang",
      "Zongze Wu",
      "Ziyan Chen",
      "Yuqian Zhou",
      "Haitian Zheng",
      "Zhifei Zhang",
      "Zhe Lin",
      "Jiantao Zhou",
      "Chao Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Stochastic_Human_Motion_Prediction_with_Memory_of_Action_Transition_and_CVPR_2025_paper.html": {
    "title": "Stochastic Human Motion Prediction with Memory of Action Transition and Action Characteristic",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianwei Tang",
      "Hong Yang",
      "Tengyue Chen",
      "Jian-Fang Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_Video-MME_The_First-Ever_Comprehensive_Evaluation_Benchmark_of_Multi-modal_LLMs_in_CVPR_2025_paper.html": {
    "title": "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaoyou Fu",
      "Yuhan Dai",
      "Yongdong Luo",
      "Lei Li",
      "Shuhuai Ren",
      "Renrui Zhang",
      "Zihan Wang",
      "Chenyu Zhou",
      "Yunhang Shen",
      "Mengdan Zhang",
      "Peixian Chen",
      "Yanwei Li",
      "Shaohui Lin",
      "Sirui Zhao",
      "Ke Li",
      "Tong Xu",
      "Xiawu Zheng",
      "Enhong Chen",
      "Caifeng Shan",
      "Ran He",
      "Xing Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bigverdi_Perception_Tokens_Enhance_Visual_Reasoning_in_Multimodal_Language_Models_CVPR_2025_paper.html": {
    "title": "Perception Tokens Enhance Visual Reasoning in Multimodal Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mahtab Bigverdi",
      "Zelun Luo",
      "Cheng-Yu Hsieh",
      "Ethan Shen",
      "Dongping Chen",
      "Linda G. Shapiro",
      "Ranjay Krishna"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/You_Are_Images_Indistinguishable_to_Humans_Also_Indistinguishable_to_Classifiers_CVPR_2025_paper.html": {
    "title": "Are Images Indistinguishable to Humans Also Indistinguishable to Classifiers?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zebin You",
      "Xinyu Zhang",
      "Hanzhong Guo",
      "Jingdong Wang",
      "Chongxuan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chang_X-Dyna_Expressive_Dynamic_Human_Image_Animation_CVPR_2025_paper.html": {
    "title": "X-Dyna: Expressive Dynamic Human Image Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Chang",
      "Hongyi Xu",
      "You Xie",
      "Yipeng Gao",
      "Zhengfei Kuang",
      "Shengqu Cai",
      "Chenxu Zhang",
      "Guoxian Song",
      "Chao Wang",
      "Yichun Shi",
      "Zeyuan Chen",
      "Shijie Zhou",
      "Linjie Luo",
      "Gordon Wetzstein",
      "Mohammad Soleymani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Levin_Understanding_Multi-layered_Transmission_Matrices_CVPR_2025_paper.html": {
    "title": "Understanding Multi-layered Transmission Matrices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anat Levin",
      "Marina Alterman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bian_GS-DiT_Advancing_Video_Generation_with_Dynamic_3D_Gaussian_Fields_through_CVPR_2025_paper.html": {
    "title": "GS-DiT: Advancing Video Generation with Dynamic 3D Gaussian Fields through Efficient Dense 3D Point Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weikang Bian",
      "Zhaoyang Huang",
      "Xiaoyu Shi",
      "Yijin Li",
      "Fu-Yun Wang",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yun_AnyMoLe_Any_Character_Motion_In-betweening_Leveraging_Video_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "AnyMoLe: Any Character Motion In-betweening Leveraging Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwan Yun",
      "Seokhyeon Hong",
      "Chaelin Kim",
      "Junyong Noh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kahl_Towards_Optimizing_Large-Scale_Multi-Graph_Matching_in_Bioimaging_CVPR_2025_paper.html": {
    "title": "Towards Optimizing Large-Scale Multi-Graph Matching in Bioimaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Max Kahl",
      "Sebastian Stricker",
      "Lisa Hutschenreiter",
      "Florian Bernard",
      "Carsten Rother",
      "Bogdan Savchynskyy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lun_Towards_Effective_and_Sparse_Adversarial_Attack_on_Spiking_Neural_Networks_CVPR_2025_paper.html": {
    "title": "Towards Effective and Sparse Adversarial Attack on Spiking Neural Networks via Breaking Invisible Surrogate Gradients",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Lun",
      "Kunyu Feng",
      "Qinglong Ni",
      "Ling Liang",
      "Yuan Wang",
      "Ying Li",
      "Dunshan Yu",
      "Xiaoxin Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Franchi_Towards_Understanding_and_Quantifying_Uncertainty_for_Text-to-Image_Generation_CVPR_2025_paper.html": {
    "title": "Towards Understanding and Quantifying Uncertainty for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gianni Franchi",
      "Nacim Belkhir",
      "Dat Nguyen Trong",
      "Guoxuan Xia",
      "Andrea Pilzer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_PS-Diffusion_Photorealistic_Subject-Driven_Image_Editing_with_Disentangled_Control_and_Attention_CVPR_2025_paper.html": {
    "title": "PS-Diffusion: Photorealistic Subject-Driven Image Editing with Disentangled Control and Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weicheng Wang",
      "Guoli Jia",
      "Zhongqi Zhang",
      "Liang Lin",
      "Jufeng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hao_Exploring_Visual_Vulnerabilities_via_Multi-Loss_Adversarial_Search_for_Jailbreaking_Vision-Language_CVPR_2025_paper.html": {
    "title": "Exploring Visual Vulnerabilities via Multi-Loss Adversarial Search for Jailbreaking Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuyang Hao",
      "Bryan Hooi",
      "Jun Liu",
      "Kai-Wei Chang",
      "Zi Huang",
      "Yujun Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_LLaVA-ST_A_Multimodal_Large_Language_Model_for_Fine-Grained_Spatial-Temporal_Understanding_CVPR_2025_paper.html": {
    "title": "LLaVA-ST: A Multimodal Large Language Model for Fine-Grained Spatial-Temporal Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyu Li",
      "Jinyu Chen",
      "Ziyu Wei",
      "Shaofei Huang",
      "Tianrui Hui",
      "Jialin Gao",
      "Xiaoming Wei",
      "Si Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Insight-V_Exploring_Long-Chain_Visual_Reasoning_with_Multimodal_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "Insight-V: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Dong",
      "Zuyan Liu",
      "Hai-Long Sun",
      "Jingkang Yang",
      "Winston Hu",
      "Yongming Rao",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_MaIR_A_Locality-_and_Continuity-Preserving_Mamba_for_Image_Restoration_CVPR_2025_paper.html": {
    "title": "MaIR: A Locality- and Continuity-Preserving Mamba for Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyun Li",
      "Haiyu Zhao",
      "Wenxin Wang",
      "Peng Hu",
      "Yuanbiao Gou",
      "Xi Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Few-shot_Implicit_Function_Generation_via_Equivariance_CVPR_2025_paper.html": {
    "title": "Few-shot Implicit Function Generation via Equivariance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suizhi Huang",
      "Xingyi Yang",
      "Hongtao Lu",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_RSAR_Restricted_State_Angle_Resolver_and_Rotated_SAR_Benchmark_CVPR_2025_paper.html": {
    "title": "RSAR: Restricted State Angle Resolver and Rotated SAR Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Zhang",
      "Xue Yang",
      "Yuxuan Li",
      "Jian Yang",
      "Ming-Ming Cheng",
      "Xiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Dual_Energy-Based_Model_with_Open-World_Uncertainty_Estimation_for_Out-of-distribution_Detection_CVPR_2025_paper.html": {
    "title": "Dual Energy-Based Model with Open-World Uncertainty Estimation for Out-of-distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Chen",
      "Hu Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_DTGBrepGen_A_Novel_B-rep_Generative_Model_through_Decoupling_Topology_and_CVPR_2025_paper.html": {
    "title": "DTGBrepGen: A Novel B-rep Generative Model through Decoupling Topology and Geometry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Li",
      "Yihang Fu",
      "Falai Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Continuous_Space-Time_Video_Resampling_with__Invertible_Motion_Steganography_CVPR_2025_paper.html": {
    "title": "Continuous Space-Time Video Resampling with Invertible Motion Steganography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuantong Zhang",
      "Zhenzhong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_Schedule_On_the_Fly_Diffusion_Time_Prediction_for_Faster_and_CVPR_2025_paper.html": {
    "title": "Schedule On the Fly: Diffusion Time Prediction for Faster and Better Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zilyu Ye",
      "Zhiyang Chen",
      "Tiancheng Li",
      "Zemin Huang",
      "Weijian Luo",
      "Guo-Jun Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jeong_Learning_Audio-guided_Video_Representation_with_Gated_Attention_for_Video-Text_Retrieval_CVPR_2025_paper.html": {
    "title": "Learning Audio-guided Video Representation with Gated Attention for Video-Text Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boseung Jeong",
      "Jicheol Park",
      "Sungyeon Kim",
      "Suha Kwak"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rim_ProtoDepth_Unsupervised_Continual_Depth_Completion_with_Prototypes_CVPR_2025_paper.html": {
    "title": "ProtoDepth: Unsupervised Continual Depth Completion with Prototypes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Patrick Rim",
      "Hyoungseob Park",
      "S. Gangopadhyay",
      "Ziyao Zeng",
      "Younjoon Chung",
      "Alex Wong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wittmann_vesselFM_A_Foundation_Model_for_Universal_3D_Blood_Vessel_Segmentation_CVPR_2025_paper.html": {
    "title": "vesselFM: A Foundation Model for Universal 3D Blood Vessel Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bastian Wittmann",
      "Yannick Wattenberg",
      "Tamaz Amiranashvili",
      "Suprosanna Shit",
      "Bjoern Menze"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_TASTE-Rob_Advancing_Video_Generation_of_Task-Oriented_Hand-Object_Interaction_for_Generalizable_CVPR_2025_paper.html": {
    "title": "TASTE-Rob: Advancing Video Generation of Task-Oriented Hand-Object Interaction for Generalizable Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongxiang Zhao",
      "Xingchen Liu",
      "Mutian Xu",
      "Yiming Hao",
      "Weikai Chen",
      "Xiaoguang Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Khalil_NoT_Federated_Unlearning_via_Weight_Negation_CVPR_2025_paper.html": {
    "title": "NoT: Federated Unlearning via Weight Negation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasser H. Khalil",
      "Leo Brunswic",
      "Soufiane Lamghari",
      "Xu Li",
      "Mahdi Beitollahi",
      "Xi Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_ParaHome_Parameterizing_Everyday_Home_Activities_Towards_3D_Generative_Modeling_of_CVPR_2025_paper.html": {
    "title": "ParaHome: Parameterizing Everyday Home Activities Towards 3D Generative Modeling of Human-Object Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeonghwan Kim",
      "Jisoo Kim",
      "Jeonghyeon Na",
      "Hanbyul Joo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shaar_Adapting_to_the_Unknown_Training-Free_Audio-Visual_Event_Perception_with_Dynamic_CVPR_2025_paper.html": {
    "title": "Adapting to the Unknown: Training-Free Audio-Visual Event Perception with Dynamic Thresholds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eitan Shaar",
      "Ariel Shaulov",
      "Gal Chechik",
      "Lior Wolf"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_OpenHumanVid_A_Large-Scale_High-Quality_Dataset_for_Enhancing_Human-Centric_Video_Generation_CVPR_2025_paper.html": {
    "title": "OpenHumanVid: A Large-Scale High-Quality Dataset for Enhancing Human-Centric Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Li",
      "Mingwang Xu",
      "Yun Zhan",
      "Shan Mu",
      "Jiaye Li",
      "Kaihui Cheng",
      "Yuxuan Chen",
      "Tan Chen",
      "Mao Ye",
      "Jingdong Wang",
      "Siyu Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jain_Classifier-Free_Guidance_Inside_the_Attraction_Basin_May_Cause_Memorization_CVPR_2025_paper.html": {
    "title": "Classifier-Free Guidance Inside the Attraction Basin May Cause Memorization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anubhav Jain",
      "Yuya Kobayashi",
      "Takashi Shibuya",
      "Yuhta Takida",
      "Nasir Memon",
      "Julian Togelius",
      "Yuki Mitsufuji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Track_Any_Anomalous_ObjectA_Granular_Video_Anomaly_Detection_Pipeline_CVPR_2025_paper.html": {
    "title": "Track Any Anomalous Object:A Granular Video Anomaly Detection Pipeline",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuzhi Huang",
      "Chenxin Li",
      "Haitao Zhang",
      "Zixu Lin",
      "Yunlong Lin",
      "Hengyu Liu",
      "Wuyang Li",
      "Xinyu Liu",
      "Jiechao Gao",
      "Yue Huang",
      "Xinghao Ding",
      "Yixuan Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dhakal_RANGE_Retrieval_Augmented_Neural_Fields_for_Multi-Resolution_Geo-Embeddings_CVPR_2025_paper.html": {
    "title": "RANGE: Retrieval Augmented Neural Fields for Multi-Resolution Geo-Embeddings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aayush Dhakal",
      "Srikumar Sastry",
      "Subash Khanal",
      "Adeel Ahmad",
      "Eric Xing",
      "Nathan Jacobs"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Magma_A_Foundation_Model_for_Multimodal_AI_Agents_CVPR_2025_paper.html": {
    "title": "Magma: A Foundation Model for Multimodal AI Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianwei Yang",
      "Reuben Tan",
      "Qianhui Wu",
      "Ruijie Zheng",
      "Baolin Peng",
      "Yongyuan Liang",
      "Yu Gu",
      "Mu Cai",
      "Seonghyeon Ye",
      "Joel Jang",
      "Yuquan Deng",
      "Jianfeng Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_SimMotionEdit_Text-Based_Human_Motion_Editing_with_Motion_Similarity_Prediction_CVPR_2025_paper.html": {
    "title": "SimMotionEdit: Text-Based Human Motion Editing with Motion Similarity Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyuan Li",
      "Kai Cheng",
      "Anindita Ghosh",
      "Uttaran Bhattacharya",
      "Liangyan Gui",
      "Aniket Bera"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Um_Object-aware_Sound_Source_Localization_via_Audio-Visual_Scene_Understanding_CVPR_2025_paper.html": {
    "title": "Object-aware Sound Source Localization via Audio-Visual Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sung Jin Um",
      "Dongjin Kim",
      "Sangmin Lee",
      "Jung Uk Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Volume_Tells_Dual_Cycle-Consistent_Diffusion_for_3D_Fluorescence_Microscopy_De-noising_CVPR_2025_paper.html": {
    "title": "Volume Tells: Dual Cycle-Consistent Diffusion for 3D Fluorescence Microscopy De-noising and Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zelin Li",
      "Chenwei Wang",
      "Zhaoke Huang",
      "Yiming Ma",
      "Cunming Zhao",
      "Zhongying Zhao",
      "Hong Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_SerialGen_Personalized_Image_Generation_by_First_Standardization_Then_Personalization_CVPR_2025_paper.html": {
    "title": "SerialGen: Personalized Image Generation by First Standardization Then Personalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Xie",
      "Han Zou",
      "Ruiqi Yu",
      "Yan Zhang",
      "Zhenpeng Zhan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_From_Head_to_Tail_Efficient_Black-box_Model_Inversion_Attack_via_CVPR_2025_paper.html": {
    "title": "From Head to Tail: Efficient Black-box Model Inversion Attack via Long-tailed Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziang Li",
      "Hongguang Zhang",
      "Juan Wang",
      "Meihui Chen",
      "Hongxin Hu",
      "Wenzhe Yi",
      "Xiaoyang Xu",
      "Mengda Yang",
      "Chenjun Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bian_Augmented_Deep_Contexts_for_Spatially_Embedded_Video_Coding_CVPR_2025_paper.html": {
    "title": "Augmented Deep Contexts for Spatially Embedded Video Coding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Bian",
      "Chuanbo Tang",
      "Li Li",
      "Dong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_SIDA_Social_Media_Image_Deepfake_Detection_Localization_and_Explanation_with_CVPR_2025_paper.html": {
    "title": "SIDA: Social Media Image Deepfake Detection, Localization and Explanation with Large Multimodal Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenglin Huang",
      "Jinwei Hu",
      "Xiangtai Li",
      "Yiwei He",
      "Xingyu Zhao",
      "Bei Peng",
      "Baoyuan Wu",
      "Xiaowei Huang",
      "Guangliang Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Matrix3D_Large_Photogrammetry_Model_All-in-One_CVPR_2025_paper.html": {
    "title": "Matrix3D: Large Photogrammetry Model All-in-One",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanxun Lu",
      "Jingyang Zhang",
      "Tian Fang",
      "Jean-Daniel Nahmias",
      "Yanghai Tsin",
      "Long Quan",
      "Xun Cao",
      "Yao Yao",
      "Shiwei Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Object-Centric_Prompt-Driven_Vision-Language-Action_Model_for_Robotic_Manipulation_CVPR_2025_paper.html": {
    "title": "Object-Centric Prompt-Driven Vision-Language-Action Model for Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoqi Li",
      "Jingyun Xu",
      "Mingxu Zhang",
      "Jiaming Liu",
      "Yan Shen",
      "Iaroslav Ponomarenko",
      "Jiahui Xu",
      "Liang Heng",
      "Siyuan Huang",
      "Shanghang Zhang",
      "Hao Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Proximal_Algorithm_Unrolling_Flexible_and_Efficient_Reconstruction_Networks_for_Single-Pixel_CVPR_2025_paper.html": {
    "title": "Proximal Algorithm Unrolling: Flexible and Efficient Reconstruction Networks for Single-Pixel Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ping Wang",
      "Lishun Wang",
      "Gang Qu",
      "Xiaodong Wang",
      "Yulun Zhang",
      "Xin Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_3DEnhancer_Consistent_Multi-View_Diffusion_for_3D_Enhancement_CVPR_2025_paper.html": {
    "title": "3DEnhancer: Consistent Multi-View Diffusion for 3D Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihang Luo",
      "Shangchen Zhou",
      "Yushi Lan",
      "Xingang Pan",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Investigating_the_Role_of_Weight_Decay_in_Enhancing_Nonconvex_SGD_CVPR_2025_paper.html": {
    "title": "Investigating the Role of Weight Decay in Enhancing Nonconvex SGD",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Sun",
      "Yuhao Huang",
      "Li Shen",
      "Kele Xu",
      "Bao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Morin_MarkushGrapher_Joint_Visual_and_Textual_Recognition_of_Markush_Structures_CVPR_2025_paper.html": {
    "title": "MarkushGrapher: Joint Visual and Textual Recognition of Markush Structures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Morin",
      "Valery Weber",
      "Ahmed Nassar",
      "Gerhard Ingmar Meijer",
      "Luc Van Gool",
      "Yawei Li",
      "Peter Staar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Depth_Any_Camera_Zero-Shot_Metric_Depth_Estimation_from_Any_Camera_CVPR_2025_paper.html": {
    "title": "Depth Any Camera: Zero-Shot Metric Depth Estimation from Any Camera",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuliang Guo",
      "Sparsh Garg",
      "S. Mahdi H. Miangoleh",
      "Xinyu Huang",
      "Liu Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Image_Quality_Assessment_From_Human_to_Machine_Preference_CVPR_2025_paper.html": {
    "title": "Image Quality Assessment: From Human to Machine Preference",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunyi Li",
      "Yuan Tian",
      "Xiaoyue Ling",
      "Zicheng Zhang",
      "Haodong Duan",
      "Haoning Wu",
      "Ziheng Jia",
      "Xiaohong Liu",
      "Xiongkuo Min",
      "Guo Lu",
      "Weisi Lin",
      "Guangtao Zhai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kara_ShotAdapter_Text-to-Multi-Shot_Video_Generation_with_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ozgur Kara",
      "Krishna Kumar Singh",
      "Feng Liu",
      "Duygu Ceylan",
      "James M. Rehg",
      "Tobias Hinz"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Roth_Context-Aware_Multimodal_Pretraining_CVPR_2025_paper.html": {
    "title": "Context-Aware Multimodal Pretraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karsten Roth",
      "Zeynep Akata",
      "Dima Damen",
      "Ivana Balazevic",
      "Olivier J. Henaff"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Sound_Bridge_Associating_Egocentric_and_Exocentric_Videos_via_Audio_Cues_CVPR_2025_paper.html": {
    "title": "Sound Bridge: Associating Egocentric and Exocentric Videos via Audio Cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sihong Huang",
      "Jiaxin Wu",
      "Xiaoyong Wei",
      "Yi Cai",
      "Dongmei Jiang",
      "Yaowei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Detecting_Backdoor_Attacks_in_Federated_Learning_via_Direction_Alignment_Inspection_CVPR_2025_paper.html": {
    "title": "Detecting Backdoor Attacks in Federated Learning via Direction Alignment Inspection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Xu",
      "Zikai Zhang",
      "Rui Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ouyang_OmniDocBench_Benchmarking_Diverse_PDF_Document_Parsing_with_Comprehensive_Annotations_CVPR_2025_paper.html": {
    "title": "OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linke Ouyang",
      "Yuan Qu",
      "Hongbin Zhou",
      "Jiawei Zhu",
      "Rui Zhang",
      "Qunshu Lin",
      "Bin Wang",
      "Zhiyuan Zhao",
      "Man Jiang",
      "Xiaomeng Zhao",
      "Jin Shi",
      "Fan Wu",
      "Pei Chu",
      "Minghao Liu",
      "Zhenxiang Li",
      "Chao Xu",
      "Bo Zhang",
      "Botian Shi",
      "Zhongying Tu",
      "Conghui He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_LayoutVLM_Differentiable_Optimization_of_3D_Layout_via_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "LayoutVLM: Differentiable Optimization of 3D Layout via Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fan-Yun Sun",
      "Weiyu Liu",
      "Siyi Gu",
      "Dylan Lim",
      "Goutam Bhat",
      "Federico Tombari",
      "Manling Li",
      "Nick Haber",
      "Jiajun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Point_Clouds_Meets_Physics_Dynamic_Acoustic_Field_Fitting_Network_for_CVPR_2025_paper.html": {
    "title": "Point Clouds Meets Physics: Dynamic Acoustic Field Fitting Network for Point Cloud Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changshuo Wang",
      "Shuting He",
      "Xiang Fang",
      "Jiawei Han",
      "Zhonghang Liu",
      "Xin Ning",
      "Weijun Li",
      "Prayag Tiwari"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Faster_Parameter-Efficient_Tuning_with_Token_Redundancy_Reduction_CVPR_2025_paper.html": {
    "title": "Faster Parameter-Efficient Tuning with Token Redundancy Reduction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwonyoung Kim",
      "Jungin Park",
      "Jin Kim",
      "Hyeongjun Kwon",
      "Kwanghoon Sohn"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_BlockDance_Reuse_Structurally_Similar_Spatio-Temporal_Features_to_Accelerate_Diffusion_Transformers_CVPR_2025_paper.html": {
    "title": "BlockDance: Reuse Structurally Similar Spatio-Temporal Features to Accelerate Diffusion Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hui Zhang",
      "Tingwei Gao",
      "Jie Shao",
      "Zuxuan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_Panorama_Generation_From_NFoV_Image_Done_Right_CVPR_2025_paper.html": {
    "title": "Panorama Generation From NFoV Image Done Right",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dian Zheng",
      "Cheng Zhang",
      "Xiao-Ming Wu",
      "Cao Li",
      "Chengfei Lv",
      "Jian-Fang Hu",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Mamba-Adaptor_State_Space_Model_Adaptor_for_Visual_Recognition_CVPR_2025_paper.html": {
    "title": "Mamba-Adaptor: State Space Model Adaptor for Visual Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Xie",
      "Jiahao Nie",
      "Yujin Tang",
      "Wenkang Zhang",
      "Hongshen Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_Robust_Message_Embedding_via_Attention_Flow-Based_Steganography_CVPR_2025_paper.html": {
    "title": "Robust Message Embedding via Attention Flow-Based Steganography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huayuan Ye",
      "Shenzhuo Zhang",
      "Shiqi Jiang",
      "Jing Liao",
      "Shuhang Gu",
      "Dejun Zheng",
      "Changbo Wang",
      "Chenhui Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bai_Task-driven_Image_Fusion_with_Learnable_Fusion_Loss_CVPR_2025_paper.html": {
    "title": "Task-driven Image Fusion with Learnable Fusion Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haowen Bai",
      "Jiangshe Zhang",
      "Zixiang Zhao",
      "Yichen Wu",
      "Lilun Deng",
      "Yukun Cui",
      "Tao Feng",
      "Shuang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mahmood_Compositional_Targeted_Multi-Label_Universal_Perturbations_CVPR_2025_paper.html": {
    "title": "Compositional Targeted Multi-Label Universal Perturbations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hassan Mahmood",
      "Ehsan Elhamifar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nafez_PatchGuard_Adversarially_Robust_Anomaly_Detection_and_Localization_through_Vision_Transformers_CVPR_2025_paper.html": {
    "title": "PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mojtaba Nafez",
      "Amirhossein Koochakian",
      "Arad Maleki",
      "Jafar Habibi",
      "Mohammad Hossein Rohban"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Sparse_Point_Cloud_Patches_Rendering_via_Splitting_2D_Gaussians_CVPR_2025_paper.html": {
    "title": "Sparse Point Cloud Patches Rendering via Splitting 2D Gaussians",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changfeng Ma",
      "Ran Bi",
      "Jie Guo",
      "Chongjun Wang",
      "Yanwen Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Distilling_Monocular_Foundation_Model_for_Fine-grained_Depth_Completion_CVPR_2025_paper.html": {
    "title": "Distilling Monocular Foundation Model for Fine-grained Depth Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingping Liang",
      "Yutao Hu",
      "Wenqi Shao",
      "Ying Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_LamRA_Large_Multimodal_Model_as_Your_Advanced_Retrieval_Assistant_CVPR_2025_paper.html": {
    "title": "LamRA: Large Multimodal Model as Your Advanced Retrieval Assistant",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yikun Liu",
      "Yajie Zhang",
      "Jiayin Cai",
      "Xiaolong Jiang",
      "Yao Hu",
      "Jiangchao Yao",
      "Yanfeng Wang",
      "Weidi Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Stier_AniGrad_Anisotropic_Gradient-Adaptive_Sampling_for_3D_Reconstruction_From_Monocular_Video_CVPR_2025_paper.html": {
    "title": "AniGrad: Anisotropic Gradient-Adaptive Sampling for 3D Reconstruction From Monocular Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noah Stier",
      "Alex Rich",
      "Pradeep Sen",
      "Tobias H√∂llerer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Neural_Video_Compression_with_Context_Modulation_CVPR_2025_paper.html": {
    "title": "Neural Video Compression with Context Modulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanbo Tang",
      "Zhuoyuan Li",
      "Yifan Bian",
      "Li Li",
      "Dong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Less_Attention_is_More_Prompt_Transformer_for_Generalized_Category_Discovery_CVPR_2025_paper.html": {
    "title": "Less Attention is More: Prompt Transformer for Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Zhang",
      "Baopeng Zhang",
      "Zhu Teng",
      "Wenxin Luo",
      "Junnan Zou",
      "Jianping Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hagenaars_On-Device_Self-Supervised_Learning_of_Low-Latency_Monocular_Depth_from_Only_Events_CVPR_2025_paper.html": {
    "title": "On-Device Self-Supervised Learning of Low-Latency Monocular Depth from Only Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jesse J. Hagenaars",
      "Yilun Wu",
      "Federico Paredes-Valles",
      "Stein Stroobants",
      "Guido C.H.E. de Croon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_CoMM_A_Coherent_Interleaved_Image-Text_Dataset_for_Multimodal_Understanding_and_CVPR_2025_paper.html": {
    "title": "CoMM: A Coherent Interleaved Image-Text Dataset for Multimodal Understanding and Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Chen",
      "Lin Li",
      "Yongqi Yang",
      "Bin Wen",
      "Fan Yang",
      "Tingting Gao",
      "Yu Wu",
      "Long Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MoGe_Unlocking_Accurate_Monocular_Geometry_Estimation_for_Open-Domain_Images_with_CVPR_2025_paper.html": {
    "title": "MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruicheng Wang",
      "Sicheng Xu",
      "Cassie Dai",
      "Jianfeng Xiang",
      "Yu Deng",
      "Xin Tong",
      "Jiaolong Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_Beyond_Background_Shift_Rethinking_Instance_Replay_in_Continual_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Beyond Background Shift: Rethinking Instance Replay in Continual Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongmei Yin",
      "Tingliang Feng",
      "Fan Lyu",
      "Fanhua Shang",
      "Hongying Liu",
      "Wei Feng",
      "Liang Wan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ke_ScaleLSD_Scalable_Deep_Line_Segment_Detection_Streamlined_CVPR_2025_paper.html": {
    "title": "ScaleLSD: Scalable Deep Line Segment Detection Streamlined",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeran Ke",
      "Bin Tan",
      "Xianwei Zheng",
      "Yujun Shen",
      "Tianfu Wu",
      "Nan Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_AToM_Aligning_Text-to-Motion_Model_at_Event-Level_with_GPT-4Vision_Reward_CVPR_2025_paper.html": {
    "title": "AToM: Aligning Text-to-Motion Model at Event-Level with GPT-4Vision Reward",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Han",
      "Xiangzuo Wu",
      "Huan Liao",
      "Zunnan Xu",
      "Zhongyuan Hu",
      "Ronghui Li",
      "Yachao Zhang",
      "Xiu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wald_Revisiting_MAE_Pre-training_for_3D_Medical_Image_Segmentation_CVPR_2025_paper.html": {
    "title": "Revisiting MAE Pre-training for 3D Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tassilo Wald",
      "Constantin Ulrich",
      "Stanislav Lukyanenko",
      "Andrei Goncharov",
      "Alberto Paderno",
      "Maximilian Miller",
      "Leander Maerkisch",
      "Paul Jaeger",
      "Klaus Maier-Hein"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Learning_with_Noisy_Triplet_Correspondence_for_Composed_Image_Retrieval_CVPR_2025_paper.html": {
    "title": "Learning with Noisy Triplet Correspondence for Composed Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuxian Li",
      "Changhao He",
      "Xiting Liu",
      "Joey Tianyi Zhou",
      "Xi Peng",
      "Peng Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_DoF-Gaussian_Controllable_Depth-of-Field_for_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "DoF-Gaussian: Controllable Depth-of-Field for 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liao Shen",
      "Tianqi Liu",
      "Huiqiang Sun",
      "Jiaqi Li",
      "Zhiguo Cao",
      "Wei Li",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Parallelized_Autoregressive_Visual_Generation_CVPR_2025_paper.html": {
    "title": "Parallelized Autoregressive Visual Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqing Wang",
      "Shuhuai Ren",
      "Zhijie Lin",
      "Yujin Han",
      "Haoyuan Guo",
      "Zhenheng Yang",
      "Difan Zou",
      "Jiashi Feng",
      "Xihui Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_CGMatch_A_Different_Perspective_of_Semi-supervised_Learning_CVPR_2025_paper.html": {
    "title": "CGMatch: A Different Perspective of Semi-supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Cheng",
      "Jueqing Lu",
      "Yuan Tian",
      "Haifeng Zhao",
      "Yi Chang",
      "Lan Du"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_ChatHuman_Chatting_about_3D_Humans_with_Tools_CVPR_2025_paper.html": {
    "title": "ChatHuman: Chatting about 3D Humans with Tools",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jing Lin",
      "Yao Feng",
      "Weiyang Liu",
      "Michael J. Black"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ashutosh_FIction_4D_Future_Interaction_Prediction_from_Video_CVPR_2025_paper.html": {
    "title": "FIction: 4D Future Interaction Prediction from Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kumar Ashutosh",
      "Georgios Pavlakos",
      "Kristen Grauman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jia_D2iT_Dynamic_Diffusion_Transformer_for_Accurate_Image_Generation_CVPR_2025_paper.html": {
    "title": "D^2iT: Dynamic Diffusion Transformer for Accurate Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weinan Jia",
      "Mengqi Huang",
      "Nan Chen",
      "Lei Zhang",
      "Zhendong Mao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Scalable_Autoregressive_Monocular_Depth_Estimation_CVPR_2025_paper.html": {
    "title": "Scalable Autoregressive Monocular Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhong Wang",
      "Jian Liu",
      "Dongqi Tang",
      "Weiqiang Wang",
      "Wentong Li",
      "Danny Chen",
      "Jintai Chen",
      "Jian Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Reconciling_Stochastic_and_Deterministic_Strategies_for_Zero-shot_Image_Restoration_using_CVPR_2025_paper.html": {
    "title": "Reconciling Stochastic and Deterministic Strategies for Zero-shot Image Restoration using Diffusion Model in Dual",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chong Wang",
      "Lanqing Guo",
      "Zixuan Fu",
      "Siyuan Yang",
      "Hao Cheng",
      "Alex C. Kot",
      "Bihan Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hai_Hierarchical_Flow_Diffusion_for_Efficient_Frame_Interpolation_CVPR_2025_paper.html": {
    "title": "Hierarchical Flow Diffusion for Efficient Frame Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Hai",
      "Guo Wang",
      "Tan Su",
      "Wenjie Jiang",
      "Yinlin Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Caffagni_Recurrence-Enhanced_Vision-and-Language_Transformers_for_Robust_Multimodal_Document_Retrieval_CVPR_2025_paper.html": {
    "title": "Recurrence-Enhanced Vision-and-Language Transformers for Robust Multimodal Document Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davide Caffagni",
      "Sara Sarto",
      "Marcella Cornia",
      "Lorenzo Baraldi",
      "Rita Cucchiara"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Das_Camouflage_Anything_Learning_to_Hide_using_Controlled_Out-painting_and_Representation_CVPR_2025_paper.html": {
    "title": "Camouflage Anything: Learning to Hide using Controlled Out-painting and Representation Engineering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Biplab Das",
      "Viswanath Gopalakrishnan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_Test-Time_Fine-Tuning_of_Image_Compression_Models_for_Multi-Task_Adaptability_CVPR_2025_paper.html": {
    "title": "Test-Time Fine-Tuning of Image Compression Models for Multi-Task Adaptability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Unki Park",
      "Seongmoon Jeong",
      "Youngchan Jang",
      "Gyeong-Moon Park",
      "Jong Hwan Ko"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pan_BASKET_A_Large-Scale_Video_Dataset_for_Fine-Grained_Skill_Estimation_CVPR_2025_paper.html": {
    "title": "BASKET: A Large-Scale Video Dataset for Fine-Grained Skill Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulu Pan",
      "Ce Zhang",
      "Gedas Bertasius"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Meng_AniDoc_Animation_Creation_Made_Easier_CVPR_2025_paper.html": {
    "title": "AniDoc: Animation Creation Made Easier",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihao Meng",
      "Hao Ouyang",
      "Hanlin Wang",
      "Qiuyu Wang",
      "Wen Wang",
      "Ka Leong Cheng",
      "Zhiheng Liu",
      "Yujun Shen",
      "Huamin Qu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_DynPose_Largely_Improving_the_Efficiency_of_Human_Pose_Estimation_by_CVPR_2025_paper.html": {
    "title": "DynPose: Largely Improving the Efficiency of Human Pose Estimation by a Simple Dynamic Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yalong Xu",
      "Lin Zhao",
      "Chen Gong",
      "Guangyu Li",
      "Di Wang",
      "Nannan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yue_Arbitrary-steps_Image_Super-resolution_via_Diffusion_Inversion_CVPR_2025_paper.html": {
    "title": "Arbitrary-steps Image Super-resolution via Diffusion Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zongsheng Yue",
      "Kang Liao",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Malic_LiSu_A_Dataset_and_Method_for_LiDAR_Surface_Normal_Estimation_CVPR_2025_paper.html": {
    "title": "LiSu: A Dataset and Method for LiDAR Surface Normal Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Du≈°an Maliƒá",
      "Christian Fruhwirth-Reisinger",
      "Samuel Schulter",
      "Horst Possegger"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nizamani_Dynamic_Neural_Surfaces_for_Elastic_4D_Shape_Representation_and_Analysis_CVPR_2025_paper.html": {
    "title": "Dynamic Neural Surfaces for Elastic 4D Shape Representation and Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Awais Nizamani",
      "Hamid Laga",
      "Guanjin Wang",
      "Farid Boussaid",
      "Mohammed Bennamoun",
      "Anuj Srivastava"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Spk2SRImgNet_Super-Resolve_Dynamic_Scene_from_Spike_Stream_via_Motion_Aligned_CVPR_2025_paper.html": {
    "title": "Spk2SRImgNet: Super-Resolve Dynamic Scene from Spike Stream via Motion Aligned Collaborative Filtering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanlin Wang",
      "Yiyang Zhang",
      "Ruiqin Xiong",
      "Jing Zhao",
      "Jian Zhang",
      "Xiaopeng Fan",
      "Tiejun Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xue_ComfyBench_Benchmarking_LLM-based_Agents_in_ComfyUI_for_Autonomously_Designing_Collaborative_CVPR_2025_paper.html": {
    "title": "ComfyBench: Benchmarking LLM-based Agents in ComfyUI for Autonomously Designing Collaborative AI Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyuan Xue",
      "Zeyu Lu",
      "Di Huang",
      "Zidong Wang",
      "Wanli Ouyang",
      "Lei Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Munasinghe_VideoGLaMM__A_Large_Multimodal_Model_for_Pixel-Level_Visual_Grounding_CVPR_2025_paper.html": {
    "title": "VideoGLaMM : A Large Multimodal Model for Pixel-Level Visual Grounding in Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shehan Munasinghe",
      "Hanan Gani",
      "Wenqi Zhu",
      "Jiale Cao",
      "Eric Xing",
      "Fahad Shahbaz Khan",
      "Salman Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Incomplete_Multi-View_Multi-label_Learning_via_Disentangled_Representation_and_Label_Semantic_CVPR_2025_paper.html": {
    "title": "Incomplete Multi-View Multi-label Learning via Disentangled Representation and Label Semantic Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Yan",
      "Jun Yin",
      "Jie Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_AutoURDF_Unsupervised_Robot_Modeling_from_Point_Cloud_Frames_Using_Cluster_CVPR_2025_paper.html": {
    "title": "AutoURDF: Unsupervised Robot Modeling from Point Cloud Frames Using Cluster Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiong Lin",
      "Lechen Zhang",
      "Kwansoo Lee",
      "Jialong Ning",
      "Judah Goldfeder",
      "Hod Lipson"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Iwase_ZeroGrasp_Zero-Shot_Shape_Reconstruction_Enabled_Robotic_Grasping_CVPR_2025_paper.html": {
    "title": "ZeroGrasp: Zero-Shot Shape Reconstruction Enabled Robotic Grasping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shun Iwase",
      "Muhammad Zubair Irshad",
      "Katherine Liu",
      "Vitor Guizilini",
      "Robert Lee",
      "Takuya Ikeda",
      "Ayako Amma",
      "Koichi Nishiwaki",
      "Kris Kitani",
      "Rares Ambrus",
      "Sergey Zakharov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Golden_Cudgel_Network_for_Real-Time_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Golden Cudgel Network for Real-Time Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guoyu Yang",
      "Yuan Wang",
      "Daming Shi",
      "Yanzhong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_PDFactor_Learning_Tri-Perspective_View_Policy_Diffusion_Field_for_Multi-Task_Robotic_CVPR_2025_paper.html": {
    "title": "PDFactor: Learning Tri-Perspective View Policy Diffusion Field for Multi-Task Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi Tian",
      "Le Wang",
      "Sanping Zhou",
      "Sen Wang",
      "Jiayi Li",
      "Haowen Sun",
      "Wei Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VideoTree_Adaptive_Tree-based_Video_Representation_for_LLM_Reasoning_on_Long_CVPR_2025_paper.html": {
    "title": "VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Wang",
      "Shoubin Yu",
      "Elias Stengel-Eskin",
      "Jaehong Yoon",
      "Feng Cheng",
      "Gedas Bertasius",
      "Mohit Bansal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rao_Multi-modal_Contrastive_Learning_with_Negative_Sampling_Calibration_for_Phenotypic_Drug_CVPR_2025_paper.html": {
    "title": "Multi-modal Contrastive Learning with Negative Sampling Calibration for Phenotypic Drug Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahua Rao",
      "Hanjing Lin",
      "Leyu Chen",
      "Jiancong Xie",
      "Shuangjia Zheng",
      "Yuedong Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sheng_R-TPT_Improving_Adversarial_Robustness_of_Vision-Language_Models_through_Test-Time_Prompt_CVPR_2025_paper.html": {
    "title": "R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lijun Sheng",
      "Jian Liang",
      "Zilei Wang",
      "Ran He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Distinguish_Then_Exploit_Source-free_Open_Set_Domain_Adaptation_via_Weight_CVPR_2025_paper.html": {
    "title": "Distinguish Then Exploit: Source-free Open Set Domain Adaptation via Weight Barcode Estimation and Sparse Label Assignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiming Liu",
      "Jun Dan",
      "Fan Wang",
      "Xinting Liao",
      "Junhao Dong",
      "Hua Yu",
      "Shunjie Dong",
      "Lianyong Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Multi-Sensor_Object_Anomaly_Detection_Unifying_Appearance_Geometry_and_Internal_Properties_CVPR_2025_paper.html": {
    "title": "Multi-Sensor Object Anomaly Detection: Unifying Appearance, Geometry, and Internal Properties",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenqiao Li",
      "Bozhong Zheng",
      "Xiaohao Xu",
      "Jinye Gan",
      "Fading Lu",
      "Xiang Li",
      "Na Ni",
      "Zheng Tian",
      "Xiaonan Huang",
      "Shenghua Gao",
      "Yingna Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Go_SplatFlow_Multi-View_Rectified_Flow_Model_for_3D_Gaussian_Splatting_Synthesis_CVPR_2025_paper.html": {
    "title": "SplatFlow: Multi-View Rectified Flow Model for 3D Gaussian Splatting Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyojun Go",
      "Byeongjun Park",
      "Jiho Jang",
      "Jin-Young Kim",
      "Soonwoo Kwon",
      "Changick Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Fancy123_One_Image_to_High-Quality_3D_Mesh_Generation_via_Plug-and-Play_CVPR_2025_paper.html": {
    "title": "Fancy123: One Image to High-Quality 3D Mesh Generation via Plug-and-Play Deformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiao Yu",
      "Xianzhi Li",
      "Yuan Tang",
      "Xu Han",
      "Long Hu",
      "Yixue Hao",
      "Min Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shin_Dense_Dispersed_Structured_Light_for_Hyperspectral_3D_Imaging_of_Dynamic_CVPR_2025_paper.html": {
    "title": "Dense Dispersed Structured Light for Hyperspectral 3D Imaging of Dynamic Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suhyun Shin",
      "Seungwoo Yoon",
      "Ryota Maeda",
      "Seung-Hwan Baek"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tan_PlanarSplatting_Accurate_Planar_Surface_Reconstruction_in_3_Minutes_CVPR_2025_paper.html": {
    "title": "PlanarSplatting: Accurate Planar Surface Reconstruction in 3 Minutes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Tan",
      "Rui Yu",
      "Yujun Shen",
      "Nan Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qian_Omni-ID_Holistic_Identity_Representation_Designed_for_Generative_Tasks_CVPR_2025_paper.html": {
    "title": "Omni-ID: Holistic Identity Representation Designed for Generative Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guocheng Qian",
      "Kuan-Chieh Wang",
      "Or Patashnik",
      "Negin Heravi",
      "Daniil Ostashev",
      "Sergey Tulyakov",
      "Daniel Cohen-Or",
      "Kfir Aberman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ozsoy_MM-OR_A_Large_Multimodal_Operating_Room_Dataset_for_Semantic_Understanding_CVPR_2025_paper.html": {
    "title": "MM-OR: A Large Multimodal Operating Room Dataset for Semantic Understanding of High-Intensity Surgical Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ege √ñzsoy",
      "Chantal Pellegrini",
      "Tobias Czempiel",
      "Felix Tristram",
      "Kun Yuan",
      "David Bani-Harouni",
      "Ulrich Eck",
      "Benjamin Busam",
      "Matthias Keicher",
      "Nassir Navab"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jayasundara_MIRE_Matched_Implicit_Neural_Representations_CVPR_2025_paper.html": {
    "title": "MIRE: Matched Implicit Neural Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dhananjaya Jayasundara",
      "Heng Zhao",
      "Demetrio Labate",
      "Vishal M. Patel"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Joo_AeSPa__Attention-guided_Self-supervised_Parallel_Imaging_for_MRI_Reconstruction_CVPR_2025_paper.html": {
    "title": "AeSPa : Attention-guided Self-supervised Parallel Imaging for MRI Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinho Joo",
      "Hyeseong Kim",
      "Hyeyeon Won",
      "Deukhee Lee",
      "Taejoon Eo",
      "Dosik Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Boltzmann_Attention_Sampling_for_Image_Analysis_with_Small_Objects_CVPR_2025_paper.html": {
    "title": "Boltzmann Attention Sampling for Image Analysis with Small Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Theodore Zhao",
      "Sid Kiblawi",
      "Naoto Usuyama",
      "Ho Hin Lee",
      "Sam Preston",
      "Hoifung Poon",
      "Mu Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Dora_Sampling_and_Benchmarking_for_3D_Shape_Variational_Auto-Encoders_CVPR_2025_paper.html": {
    "title": "Dora: Sampling and Benchmarking for 3D Shape Variational Auto-Encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Chen",
      "Jianfeng Zhang",
      "Yixun Liang",
      "Guan Luo",
      "Weiyu Li",
      "Jiarui Liu",
      "Xiu Li",
      "Xiaoxiao Long",
      "Jiashi Feng",
      "Ping Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Monroy_Generalized_Recorrupted-to-Recorrupted_Self-Supervised_Learning_Beyond_Gaussian_Noise_CVPR_2025_paper.html": {
    "title": "Generalized Recorrupted-to-Recorrupted: Self-Supervised Learning Beyond Gaussian Noise",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Brayan Monroy",
      "Jorge Bacca",
      "Juli√°n Tachella"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Once-Tuning-Multiple-Variants_Tuning_Once_and_Expanded_as_Multiple_Vision-Language_Model_Variants_CVPR_2025_paper.html": {
    "title": "Once-Tuning-Multiple-Variants: Tuning Once and Expanded as Multiple Vision-Language Model Variants",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chong Yu",
      "Tao Chen",
      "Zhongxue Gan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Dynamic_Motion_Blending_for_Versatile_Motion_Editing_CVPR_2025_paper.html": {
    "title": "Dynamic Motion Blending for Versatile Motion Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nan Jiang",
      "Hongjie Li",
      "Ziye Yuan",
      "Zimo He",
      "Yixin Chen",
      "Tengyu Liu",
      "Yixin Zhu",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_StdGEN_Semantic-Decomposed_3D_Character_Generation_from_Single_Images_CVPR_2025_paper.html": {
    "title": "StdGEN: Semantic-Decomposed 3D Character Generation from Single Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuze He",
      "Yanning Zhou",
      "Wang Zhao",
      "Zhongkai Wu",
      "Kaiwen Xiao",
      "Wei Yang",
      "Yong-Jin Liu",
      "Xiao Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kulits_Reconstructing_Animals_and_the_Wild_CVPR_2025_paper.html": {
    "title": "Reconstructing Animals and the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peter Kulits",
      "Michael J. Black",
      "Silvia Zuffi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kha_RobSense_A_Robust_Multi-modal_Foundation_Model_for_Remote_Sensing_with_CVPR_2025_paper.html": {
    "title": "RobSense: A Robust Multi-modal Foundation Model for Remote Sensing with Static, Temporal, and Incomplete Data Adaptability",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minh Kha Do",
      "Kang Han",
      "Phu Lai",
      "Khoa T. Phan",
      "Wei Xiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Spatiotemporal_Decoupling_for_Efficient_Vision-Based_Occupancy_Forecasting_CVPR_2025_paper.html": {
    "title": "Spatiotemporal Decoupling for Efficient Vision-Based Occupancy Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi Xu",
      "Xieyuanli Chen",
      "Junyi Ma",
      "Jiawei Huang",
      "Jintao Xu",
      "Yue Wang",
      "Ling Pei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liao_DiffusionDrive_Truncated_Diffusion_Model_for_End-to-End_Autonomous_Driving_CVPR_2025_paper.html": {
    "title": "DiffusionDrive: Truncated Diffusion Model for End-to-End Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bencheng Liao",
      "Shaoyu Chen",
      "Haoran Yin",
      "Bo Jiang",
      "Cheng Wang",
      "Sixu Yan",
      "Xinbang Zhang",
      "Xiangyu Li",
      "Ying Zhang",
      "Qian Zhang",
      "Xinggang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_MAC-Ego3D_Multi-Agent_Gaussian_Consensus_for_Real-Time_Collaborative_Ego-Motion_and_Photorealistic_CVPR_2025_paper.html": {
    "title": "MAC-Ego3D: Multi-Agent Gaussian Consensus for Real-Time Collaborative Ego-Motion and Photorealistic 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaohao Xu",
      "Feng Xue",
      "Shibo Zhao",
      "Yike Pan",
      "Sebastian Scherer",
      "Xiaonan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_FFR_Frequency_Feature_Rectification_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "FFR: Frequency Feature Rectification for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqian Yang",
      "Xinqiao Zhao",
      "Xiaolei Wang",
      "Quan Zhang",
      "Jimin Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_DVHGNN_Multi-Scale_Dilated_Vision_HGNN_for_Efficient_Vision_Recognition_CVPR_2025_paper.html": {
    "title": "DVHGNN: Multi-Scale Dilated Vision HGNN for Efficient Vision Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Caoshuo Li",
      "Tanzhe Li",
      "Xiaobin Hu",
      "Donghao Luo",
      "Taisong Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shu_Video-XL_Extra-Long_Vision_Language_Model_for_Hour-Scale_Video_Understanding_CVPR_2025_paper.html": {
    "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Shu",
      "Zheng Liu",
      "Peitian Zhang",
      "Minghao Qin",
      "Junjie Zhou",
      "Zhengyang Liang",
      "Tiejun Huang",
      "Bo Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wen_Reconstructing_In-the-Wild_Open-Vocabulary_Human-Object_Interactions_CVPR_2025_paper.html": {
    "title": "Reconstructing In-the-Wild Open-Vocabulary Human-Object Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boran Wen",
      "Dingbang Huang",
      "Zichen Zhang",
      "Jiahong Zhou",
      "Jianbin Deng",
      "Jingyu Gong",
      "Yulong Chen",
      "Lizhuang Ma",
      "Yong-Lu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cui_GROVE_A_Generalized_Reward_for_Learning_Open-Vocabulary_Physical_Skill_CVPR_2025_paper.html": {
    "title": "GROVE: A Generalized Reward for Learning Open-Vocabulary Physical Skill",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jieming Cui",
      "Tengyu Liu",
      "Ziyu Meng",
      "Jiale Yu",
      "Ran Song",
      "Wei Zhang",
      "Yixin Zhu",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Sonata_Self-Supervised_Learning_of_Reliable_Point_Representations_CVPR_2025_paper.html": {
    "title": "Sonata: Self-Supervised Learning of Reliable Point Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyang Wu",
      "Daniel DeTone",
      "Duncan Frost",
      "Tianwei Shen",
      "Chris Xie",
      "Nan Yang",
      "Jakob Engel",
      "Richard Newcombe",
      "Hengshuang Zhao",
      "Julian Straub"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_DriveGEN_Generalized_and_Robust_3D_Detection_in_Driving_via_Controllable_CVPR_2025_paper.html": {
    "title": "DriveGEN: Generalized and Robust 3D Detection in Driving via Controllable Text-to-Image Diffusion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongbin Lin",
      "Zilu Guo",
      "Yifan Zhang",
      "Shuaicheng Niu",
      "Yafeng Li",
      "Ruimao Zhang",
      "Shuguang Cui",
      "Zhen Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_GauSTAR_Gaussian_Surface_Tracking_and_Reconstruction_CVPR_2025_paper.html": {
    "title": "GauSTAR: Gaussian Surface Tracking and Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengwei Zheng",
      "Lixin Xue",
      "Juan Zarate",
      "Jie Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Training-free_Dense-Aligned_Diffusion_Guidance_for_Modular_Conditional_Image_Synthesis_CVPR_2025_paper.html": {
    "title": "Training-free Dense-Aligned Diffusion Guidance for Modular Conditional Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Wang",
      "Duo Peng",
      "Feng Chen",
      "Yuwei Yang",
      "Yinjie Lei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_DRiVE_Diffusion-based_Rigging_Empowers_Generation_of_Versatile_and_Expressive_Characters_CVPR_2025_paper.html": {
    "title": "DRiVE: Diffusion-based Rigging Empowers Generation of Versatile and Expressive Characters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingze Sun",
      "Junhao Chen",
      "Junting Dong",
      "Yurun Chen",
      "Xinyu Jiang",
      "Shiwei Mao",
      "Puhua Jiang",
      "Jingbo Wang",
      "Bo Dai",
      "Ruqi Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Online_Video_Understanding_OVBench_and_VideoChat-Online_CVPR_2025_paper.html": {
    "title": "Online Video Understanding: OVBench and VideoChat-Online",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenpeng Huang",
      "Xinhao Li",
      "Jiaqi Li",
      "Jing Wang",
      "Xiangyu Zeng",
      "Cheng Liang",
      "Tao Wu",
      "Xi Chen",
      "Liang Li",
      "Limin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Baek_TADFormer_Task-Adaptive_Dynamic_TransFormer_for_Efficient_Multi-Task_Learning_CVPR_2025_paper.html": {
    "title": "TADFormer: Task-Adaptive Dynamic TransFormer for Efficient Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seungmin Baek",
      "Soyul Lee",
      "Hayeon Jo",
      "Hyesong Choi",
      "Dongbo Min"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_A_Unified_Approach_to_Interpreting_Self-supervised_Pre-training_Methods_for_3D_CVPR_2025_paper.html": {
    "title": "A Unified Approach to Interpreting Self-supervised Pre-training Methods for 3D Point Clouds via Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiang Li",
      "Jian Ruan",
      "Fanghao Wu",
      "Yuchi Chen",
      "Zhihua Wei",
      "Wen Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Konwer_Enhancing_SAM_with_Efficient_Prompting_and_Preference_Optimization_for_Semi-supervised_CVPR_2025_paper.html": {
    "title": "Enhancing SAM with Efficient Prompting and Preference Optimization for Semi-supervised Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aishik Konwer",
      "Zhijian Yang",
      "Erhan Bas",
      "Cao Xiao",
      "Prateek Prasanna",
      "Parminder Bhatia",
      "Taha Kass-Hout"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_CamPoint_Boosting_Point_Cloud_Segmentation_with_Virtual_Camera_CVPR_2025_paper.html": {
    "title": "CamPoint: Boosting Point Cloud Segmentation with Virtual Camera",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianhui Zhang",
      "Yizhi Luo",
      "Zicheng Zhang",
      "Xuecheng Nie",
      "Bonan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_LightLoc_Learning_Outdoor_LiDAR_Localization_at_Light_Speed_CVPR_2025_paper.html": {
    "title": "LightLoc: Learning Outdoor LiDAR Localization at Light Speed",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wen Li",
      "Chen Liu",
      "Shangshu Yu",
      "Dunqiang Liu",
      "Yin Zhou",
      "Siqi Shen",
      "Chenglu Wen",
      "Cheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ganguly_MERGE_Multi-faceted_Hierarchical_Graph-based_GNN_for_Gene_Expression_Prediction_from_CVPR_2025_paper.html": {
    "title": "MERGE: Multi-faceted Hierarchical Graph-based GNN for Gene Expression Prediction from Whole Slide Histopathology Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aniruddha Ganguly",
      "Debolina Chatterjee",
      "Wentao Huang",
      "Jie Zhang",
      "Alisa Yurovsky",
      "Travis Steele Johnson",
      "Chao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chetan_Accurate_Differential_Operators_for_Hybrid_Neural_Fields_CVPR_2025_paper.html": {
    "title": "Accurate Differential Operators for Hybrid Neural Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Chetan",
      "Guandao Yang",
      "Zichen Wang",
      "Steve Marschner",
      "Bharath Hariharan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_FeedEdit_Text-Based_Image_Editing_with_Dynamic_Feedback_Regulation_CVPR_2025_paper.html": {
    "title": "FeedEdit: Text-Based Image Editing with Dynamic Feedback Regulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fengyi Fu",
      "Lei Zhang",
      "Mengqi Huang",
      "Zhendong Mao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Classifier-guided_CLIP_Distillation_for_Unsupervised_Multi-label_Classification_CVPR_2025_paper.html": {
    "title": "Classifier-guided CLIP Distillation for Unsupervised Multi-label Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongseob Kim",
      "Hyunjung Shim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_UMotion_Uncertainty-driven_Human_Motion_Estimation_from_Inertial_and_Ultra-wideband_Units_CVPR_2025_paper.html": {
    "title": "UMotion: Uncertainty-driven Human Motion Estimation from Inertial and Ultra-wideband Units",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huakun Liu",
      "Hiroki Ota",
      "Xin Wei",
      "Yutaro Hirao",
      "Monica Perusquia-Hernandez",
      "Hideaki Uchiyama",
      "Kiyoshi Kiyokawa"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Srivatsan_STEREO_A_Two-Stage_Framework_for_Adversarially_Robust_Concept_Erasing_from_CVPR_2025_paper.html": {
    "title": "STEREO: A Two-Stage Framework for Adversarially Robust Concept Erasing from Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Koushik Srivatsan",
      "Fahad Shamshad",
      "Muzammal Naseer",
      "Vishal M. Patel",
      "Karthik Nandakumar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_Scene_Map-based_Prompt_Tuning_for_Navigation_Instruction_Generation_CVPR_2025_paper.html": {
    "title": "Scene Map-based Prompt Tuning for Navigation Instruction Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sheng Fan",
      "Rui Liu",
      "Wenguan Wang",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_GenVDM_Generating_Vector_Displacement_Maps_From_a_Single_Image_CVPR_2025_paper.html": {
    "title": "GenVDM: Generating Vector Displacement Maps From a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuezhi Yang",
      "Qimin Chen",
      "Vladimir G. Kim",
      "Siddhartha Chaudhuri",
      "Qixing Huang",
      "Zhiqin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_DropoutGS_Dropping_Out_Gaussians_for_Better_Sparse-view_Rendering_CVPR_2025_paper.html": {
    "title": "DropoutGS: Dropping Out Gaussians for Better Sparse-view Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yexing Xu",
      "Longguang Wang",
      "Minglin Chen",
      "Sheng Ao",
      "Li Li",
      "Yulan Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Effective_SAM_Combination_for_Open-Vocabulary_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Effective SAM Combination for Open-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minhyeok Lee",
      "Suhwan Cho",
      "Jungho Lee",
      "Sunghun Yang",
      "Heeseung Choi",
      "Ig-Jae Kim",
      "Sangyoun Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Bridging_Modalities_Improving_Universal_Multimodal_Retrieval_by_Multimodal_Large_Language_CVPR_2025_paper.html": {
    "title": "Bridging Modalities: Improving Universal Multimodal Retrieval by Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Zhang",
      "Yanzhao Zhang",
      "Wen Xie",
      "Mingxin Li",
      "Ziqi Dai",
      "Dingkun Long",
      "Pengjun Xie",
      "Meishan Zhang",
      "Wenjie Li",
      "Min Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tran_Enhancing_Dataset_Distillation_via_Non-Critical_Region_Refinement_CVPR_2025_paper.html": {
    "title": "Enhancing Dataset Distillation via Non-Critical Region Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minh-Tuan Tran",
      "Trung Le",
      "Xuan-May Le",
      "Thanh-Toan Do",
      "Dinh Phung"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Towards_Visual_Discrimination_and_Reasoning_of_Real-World_Physical_Dynamics_Physics-Grounded_CVPR_2025_paper.html": {
    "title": "Towards Visual Discrimination and Reasoning of Real-World Physical Dynamics: Physics-Grounded Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenqiao Li",
      "Yao Gu",
      "Xintao Chen",
      "Xiaohao Xu",
      "Ming Hu",
      "Xiaonan Huang",
      "Yingna Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_SPA-VL_A_Comprehensive_Safety_Preference_Alignment_Dataset_for_Vision_Language_CVPR_2025_paper.html": {
    "title": "SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongting Zhang",
      "Lu Chen",
      "Guodong Zheng",
      "Yifeng Gao",
      "Rui Zheng",
      "Jinlan Fu",
      "Zhenfei Yin",
      "Senjie Jin",
      "Yu Qiao",
      "Xuanjing Huang",
      "Feng Zhao",
      "Tao Gui",
      "Jing Shao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hanson_PUP_3D-GS_Principled_Uncertainty_Pruning_for_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "PUP 3D-GS: Principled Uncertainty Pruning for 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alex Hanson",
      "Allen Tu",
      "Vasu Singla",
      "Mayuka Jayawardhana",
      "Matthias Zwicker",
      "Tom Goldstein"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_UniAP_Unifying_Inter-_and_Intra-Layer_Automatic_Parallelism_by_Mixed_Integer_CVPR_2025_paper.html": {
    "title": "UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Lin",
      "Ke Wu",
      "Jie Li",
      "Jun Li",
      "Wu-Jun Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_PTDiffusion_Free_Lunch_for_Generating_Optical_Illusion_Hidden_Pictures_with_CVPR_2025_paper.html": {
    "title": "PTDiffusion: Free Lunch for Generating Optical Illusion Hidden Pictures with Phase-Transferred Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Gao",
      "Shuai Yang",
      "Jiaying Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Choi_ScribbleLight_Single_Image_Indoor_Relighting_with_Scribbles_CVPR_2025_paper.html": {
    "title": "ScribbleLight: Single Image Indoor Relighting with Scribbles",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Myeong Choi",
      "Annie Wang",
      "Pieter Peers",
      "Anand Bhattad",
      "Roni Sengupta"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Vuong_Preserving_Clusters_in_Prompt_Learning_for_Unsupervised_Domain_Adaptation_CVPR_2025_paper.html": {
    "title": "Preserving Clusters in Prompt Learning for Unsupervised Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tung-Long Vuong",
      "Hoang Phan",
      "Vy Vo",
      "Anh Bui",
      "Thanh-Toan Do",
      "Trung Le",
      "Dinh Phung"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_InsightEdit_Towards_Better_Instruction_Following_for_Image_Editing_CVPR_2025_paper.html": {
    "title": "InsightEdit: Towards Better Instruction Following for Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yingjing Xu",
      "Jie Kong",
      "Jiazhi Wang",
      "Xiao Pan",
      "Bo Lin",
      "Qiang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_Attend_to_Not_Attended_Structure-then-Detail_Token_Merging_for_Post-training_DiT_CVPR_2025_paper.html": {
    "title": "Attend to Not Attended: Structure-then-Detail Token Merging for Post-training DiT Acceleration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haipeng Fang",
      "Sheng Tang",
      "Juan Cao",
      "Enshuo Zhang",
      "Fan Tang",
      "Tong-Yee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_Turbo3D_Ultra-fast_Text-to-3D_Generation_CVPR_2025_paper.html": {
    "title": "Turbo3D: Ultra-fast Text-to-3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanzhe Hu",
      "Tianwei Yin",
      "Fujun Luan",
      "Yiwei Hu",
      "Hao Tan",
      "Zexiang Xu",
      "Sai Bi",
      "Shubham Tulsiani",
      "Kai Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_SUM_Parts_Benchmarking_Part-Level_Semantic_Segmentation_of_Urban_Meshes_CVPR_2025_paper.html": {
    "title": "SUM Parts: Benchmarking Part-Level Semantic Segmentation of Urban Meshes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weixiao Gao",
      "Liangliang Nan",
      "Hugo Ledoux"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_One-for-More_Continual_Diffusion_Model_for_Anomaly_Detection_CVPR_2025_paper.html": {
    "title": "One-for-More: Continual Diffusion Model for Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaofan Li",
      "Xin Tan",
      "Zhuo Chen",
      "Zhizhong Zhang",
      "Ruixin Zhang",
      "Rizen Guo",
      "Guanna Jiang",
      "Yulong Chen",
      "Yanyun Qu",
      "Lizhuang Ma",
      "Yuan Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_MODA_Motion-Drift_Augmentation_for_Inertial_Human_Motion_Analysis_CVPR_2025_paper.html": {
    "title": "MODA: Motion-Drift Augmentation for Inertial Human Motion Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinghao Wu",
      "Shihui Guo",
      "Yipeng Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Roetzer_Higher-Order_Ratio_Cycles_for_Fast_and_Globally_Optimal_Shape_Matching_CVPR_2025_paper.html": {
    "title": "Higher-Order Ratio Cycles for Fast and Globally Optimal Shape Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Paul Roetzer",
      "Viktoria Ehm",
      "Daniel Cremers",
      "Zorah L√§hner",
      "Florian Bernard"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Heo_Omni-RGPT_Unifying_Image_and_Video_Region-level_Understanding_via_Token_Marks_CVPR_2025_paper.html": {
    "title": "Omni-RGPT: Unifying Image and Video Region-level Understanding via Token Marks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miran Heo",
      "Min-Hung Chen",
      "De-An Huang",
      "Sifei Liu",
      "Subhashree Radhakrishnan",
      "Seon Joo Kim",
      "Yu-Chiang Frank Wang",
      "Ryo Hachiuma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Hyperdimensional_Uncertainty_Quantification_for_Multimodal_Uncertainty_Fusion_in_Autonomous_Vehicles_CVPR_2025_paper.html": {
    "title": "Hyperdimensional Uncertainty Quantification for Multimodal Uncertainty Fusion in Autonomous Vehicles Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luke Chen",
      "Junyao Wang",
      "Trier Mortlock",
      "Pramod Khargonekar",
      "Mohammad Abdullah Al Faruque"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jung_EDM_Equirectangular_Projection-Oriented_Dense_Kernelized_Feature_Matching_CVPR_2025_paper.html": {
    "title": "EDM: Equirectangular Projection-Oriented Dense Kernelized Feature Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongki Jung",
      "Jaehoon Choi",
      "Yonghan Lee",
      "Somi Jeong",
      "Taejae Lee",
      "Dinesh Manocha",
      "Suyong Yeon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_EZSR_Event-based_Zero-Shot_Recognition_CVPR_2025_paper.html": {
    "title": "EZSR: Event-based Zero-Shot Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Yang",
      "Liyuan Pan",
      "Dongxu Li",
      "Liu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_FlowRAM_Grounding_Flow_Matching_Policy_with_Region-Aware_Mamba_Framework_for_CVPR_2025_paper.html": {
    "title": "FlowRAM: Grounding Flow Matching Policy with Region-Aware Mamba Framework for Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sen Wang",
      "Le Wang",
      "Sanping Zhou",
      "Jingyi Tian",
      "Jiayi Li",
      "Haowen Sun",
      "Wei Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Visual_Lexicon_Rich_Image_Features_in_Language_Space_CVPR_2025_paper.html": {
    "title": "Visual Lexicon: Rich Image Features in Language Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "XuDong Wang",
      "Xingyi Zhou",
      "Alireza Fathi",
      "Trevor Darrell",
      "Cordelia Schmid"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SVFR_A_Unified_Framework_for_Generalized_Video_Face_Restoration_CVPR_2025_paper.html": {
    "title": "SVFR: A Unified Framework for Generalized Video Face Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyao Wang",
      "Xu Chen",
      "Chengming Xu",
      "Junwei Zhu",
      "Xiaobin Hu",
      "Jiangning Zhang",
      "Chengjie Wang",
      "Yuqi Liu",
      "Yiyi Zhou",
      "Rongrong Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_Decoupling_Fine_Detail_and_Global_Geometry_for_Compressed_Depth_Map_CVPR_2025_paper.html": {
    "title": "Decoupling Fine Detail and Global Geometry for Compressed Depth Map Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huan Zheng",
      "Wencheng Han",
      "Jianbing Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Test-Time_Visual_In-Context_Tuning_CVPR_2025_paper.html": {
    "title": "Test-Time Visual In-Context Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Xie",
      "Alessio Tonioni",
      "Nathalie Rauschmayr",
      "Federico Tombari",
      "Bernt Schiele"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_Prior_Does_Matter_Visual_Navigation_via_Denoising_Diffusion_Bridge_Models_CVPR_2025_paper.html": {
    "title": "Prior Does Matter: Visual Navigation via Denoising Diffusion Bridge Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Ren",
      "Yiming Zeng",
      "Zetong Bi",
      "Zhaoliang Wan",
      "Junlong Huang",
      "Hui Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Digital_Twin_Catalog_A_Large-Scale_Photorealistic_3D_Object_Digital_Twin_CVPR_2025_paper.html": {
    "title": "Digital Twin Catalog: A Large-Scale Photorealistic 3D Object Digital Twin Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhao Dong",
      "Ka Chen",
      "Zhaoyang Lv",
      "Hong-Xing Yu",
      "Yunzhi Zhang",
      "Cheng Zhang",
      "Yufeng Zhu",
      "Stephen Tian",
      "Zhengqin Li",
      "Geordie Moffatt",
      "Sean Christofferson",
      "James Fort",
      "Xiaqing Pan",
      "Mingfei Yan",
      "Jiajun Wu",
      "Carl Yuheng Ren",
      "Richard Newcombe"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_SegEarth-OV_Towards_Training-Free_Open-Vocabulary_Segmentation_for_Remote_Sensing_Images_CVPR_2025_paper.html": {
    "title": "SegEarth-OV: Towards Training-Free Open-Vocabulary Segmentation for Remote Sensing Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiyu Li",
      "Ruixun Liu",
      "Xiangyong Cao",
      "Xueru Bai",
      "Feng Zhou",
      "Deyu Meng",
      "Zhi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_MeshGen_Generating_PBR_Textured_Mesh_with_Render-Enhanced_Auto-Encoder_and_Generative_CVPR_2025_paper.html": {
    "title": "MeshGen: Generating PBR Textured Mesh with Render-Enhanced Auto-Encoder and Generative Data Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zilong Chen",
      "Yikai Wang",
      "Wenqiang Sun",
      "Feng Wang",
      "Yiwen Chen",
      "Huaping Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_GIFStream_4D_Gaussian-based_Immersive_Video_with_Feature_Stream_CVPR_2025_paper.html": {
    "title": "GIFStream: 4D Gaussian-based Immersive Video with Feature Stream",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Li",
      "Sicheng Li",
      "Xiang Gao",
      "Abudouaihati Batuer",
      "Lu Yu",
      "Yiyi Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nam_DeClotH_Decomposable_3D_Cloth_and_Human_Body_Reconstruction_from_a_CVPR_2025_paper.html": {
    "title": "DeClotH: Decomposable 3D Cloth and Human Body Reconstruction from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyeongjin Nam",
      "Donghwan Kim",
      "Jeongtaek Oh",
      "Kyoung Mu Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Neuron_Learning_Context-Aware_Evolving_Representations_for_Zero-Shot_Skeleton_Action_Recognition_CVPR_2025_paper.html": {
    "title": "Neuron: Learning Context-Aware Evolving Representations for Zero-Shot Skeleton Action Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Chen",
      "Jingcai Guo",
      "Song Guo",
      "Dacheng Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Abdelsamad_Multi-Scale_Neighborhood_Occupancy_Masked_Autoencoder_for_Self-Supervised_Learning_in_LiDAR_CVPR_2025_paper.html": {
    "title": "Multi-Scale Neighborhood Occupancy Masked Autoencoder for Self-Supervised Learning in LiDAR Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohamed Abdelsamad",
      "Michael Ulrich",
      "Claudius Glaeser",
      "Abhinav Valada"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Do_We_Really_Need_Curated_Malicious_Data_for_Safety_Alignment_CVPR_2025_paper.html": {
    "title": "Do We Really Need Curated Malicious Data for Safety Alignment in Multi-modal Large Language Models?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanbo Wang",
      "Jiyang Guan",
      "Jian Liang",
      "Ran He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_High-Fidelity_Relightable_Monocular_Portrait_Animation_with_Lighting-Controllable_Video_Diffusion_Model_CVPR_2025_paper.html": {
    "title": "High-Fidelity Relightable Monocular Portrait Animation with Lighting-Controllable Video Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingtao Guo",
      "Guanyu Xing",
      "Yanli Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Plug-and-Play_PPO_An_Adaptive_Point_Prompt_Optimizer_Making_SAM_Greater_CVPR_2025_paper.html": {
    "title": "Plug-and-Play PPO: An Adaptive Point Prompt Optimizer Making SAM Greater",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueyu Liu",
      "Rui Wang",
      "Yexin Lai",
      "Guangze Shi",
      "Feixue Shao",
      "Fang Hao",
      "Jianan Zhang",
      "Jia Shen",
      "Yongfei Wu",
      "Wen Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Harnessing_Global-Local_Collaborative_Adversarial_Perturbation_for_Anti-Customization_CVPR_2025_paper.html": {
    "title": "Harnessing Global-Local Collaborative Adversarial Perturbation for Anti-Customization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long Xu",
      "Jiakai Wang",
      "Haojie Hao",
      "Haotong Qin",
      "Jiejie Zhao",
      "Xianglong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_EchoONE_Segmenting_Multiple_Echocardiography_Planes_in_One_Model_CVPR_2025_paper.html": {
    "title": "EchoONE: Segmenting Multiple Echocardiography Planes in One Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiongtong Hu",
      "Wufeng Xue",
      "Jun Cheng",
      "Yingying Liu",
      "Wei Zhuo",
      "Dong Ni"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Acc3D_Accelerating_Single_Image_to_3D_Diffusion_Models_via_Edge_CVPR_2025_paper.html": {
    "title": "Acc3D: Accelerating Single Image to 3D Diffusion Models via Edge Consistency Guided Score Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kendong Liu",
      "Zhiyu Zhu",
      "Hui Liu",
      "Junhui Hou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_EasyHOI_Unleashing_the_Power_of_Large_Models_for_Reconstructing_Hand-Object_CVPR_2025_paper.html": {
    "title": "EasyHOI: Unleashing the Power of Large Models for Reconstructing Hand-Object Interactions in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yumeng Liu",
      "Xiaoxiao Long",
      "Zemin Yang",
      "Yuan Liu",
      "Marc Habermann",
      "Christian Theobalt",
      "Yuexin Ma",
      "Wenping Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nasery_PLeaS_-_Merging_Models_with_Permutations_and_Least_Squares_CVPR_2025_paper.html": {
    "title": "PLeaS - Merging Models with Permutations and Least Squares",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anshul Nasery",
      "Jonathan Hayase",
      "Pang Wei Koh",
      "Sewoong Oh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Incremental_Object_Keypoint_Learning_CVPR_2025_paper.html": {
    "title": "Incremental Object Keypoint Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingfu Liang",
      "Jiahuan Zhou",
      "Xu Zou",
      "Ying Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Soft_Self-labeling_and_Potts_Relaxations_for_Weakly-supervised_Segmentation_CVPR_2025_paper.html": {
    "title": "Soft Self-labeling and Potts Relaxations for Weakly-supervised Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhongwen Zhang",
      "Yuri Boykov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Izquierdo_MVSAnywhere_Zero-Shot_Multi-View_Stereo_CVPR_2025_paper.html": {
    "title": "MVSAnywhere: Zero-Shot Multi-View Stereo",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sergio Izquierdo",
      "Mohamed Sayed",
      "Michael Firman",
      "Guillermo Garcia-Hernando",
      "Daniyar Turmukhambetov",
      "Javier Civera",
      "Oisin Mac Aodha",
      "Gabriel Brostow",
      "Jamie Watson"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dwivedi_InteractVLM_3D_Interaction_Reasoning_from_2D_Foundational_Models_CVPR_2025_paper.html": {
    "title": "InteractVLM: 3D Interaction Reasoning from 2D Foundational Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sai Kumar Dwivedi",
      "Dimitrije Antiƒá",
      "Shashank Tripathi",
      "Omid Taheri",
      "Cordelia Schmid",
      "Michael J. Black",
      "Dimitrios Tzionas"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peng_Patch_Matters_Training-free_Fine-grained_Image_Caption_Enhancement_via_Local_Perception_CVPR_2025_paper.html": {
    "title": "Patch Matters: Training-free Fine-grained Image Caption Enhancement via Local Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruotian Peng",
      "Haiying He",
      "Yake Wei",
      "Yandong Wen",
      "Di Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Attribute-Missing_Multi-view_Graph_Clustering_CVPR_2025_paper.html": {
    "title": "Attribute-Missing Multi-view Graph Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Zhao",
      "Qianqian Wang",
      "Zhengming Ding",
      "Quanxue Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yoshida_Generating_6DoF_Object_Manipulation_Trajectories_from_Action_Description_in_Egocentric_CVPR_2025_paper.html": {
    "title": "Generating 6DoF Object Manipulation Trajectories from Action Description in Egocentric Vision",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tomoya Yoshida",
      "Shuhei Kurita",
      "Taichi Nishimura",
      "Shinsuke Mori"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_Pose-Guided_Temporal_Enhancement_for_Robust_Low-Resolution_Hand_Reconstruction_CVPR_2025_paper.html": {
    "title": "Pose-Guided Temporal Enhancement for Robust Low-Resolution Hand Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaixin Fan",
      "Pengfei Ren",
      "Jingyu Wang",
      "Haifeng Sun",
      "Qi Qi",
      "Zirui Zhuang",
      "Jianxin Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_ReDiffDet_Rotation-equivariant_Diffusion_Model_for_Oriented_Object_Detection_CVPR_2025_paper.html": {
    "title": "ReDiffDet: Rotation-equivariant Diffusion Model for Oriented Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Zhao",
      "Zeyu Ding",
      "Yong Zhou",
      "Hancheng Zhu",
      "Wen-Liang Du",
      "Rui Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hsu_PosterO_Structuring_Layout_Trees_to_Enable_Language_Models_in_Generalized_CVPR_2025_paper.html": {
    "title": "PosterO: Structuring Layout Trees to Enable Language Models in Generalized Content-Aware Layout Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "HsiaoYuan Hsu",
      "Yuxin Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lozano_BIOMEDICA_An_Open_Biomedical_Image-Caption_Archive_Dataset_and_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "BIOMEDICA: An Open Biomedical Image-Caption Archive, Dataset, and Vision-Language Models Derived from Scientific Literature",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alejandro Lozano",
      "Min Woo Sun",
      "James Burgess",
      "Liangyu Chen",
      "Jeffrey J. Nirschl",
      "Jeffrey Gu",
      "Ivan Lopez",
      "Josiah Aklilu",
      "Anita Rau",
      "Austin Wolfgang Katzer",
      "Yuhui Zhang",
      "Collin Chiu",
      "Xiaohan Wang",
      "Alfred Seunghoon Song",
      "Robert Tibshirani",
      "Serena Yeung-Levy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zeng_Unlocking_Generalization_Power_in_LiDAR_Point_Cloud_Registration_CVPR_2025_paper.html": {
    "title": "Unlocking Generalization Power in LiDAR Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenxuan Zeng",
      "Qiao Wu",
      "Xiyu Zhang",
      "Lin Yuanbo Wu",
      "Pei An",
      "Jiaqi Yang",
      "Ji Wang",
      "Peng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Structure-Aware_Correspondence_Learning_for_Relative_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "Structure-Aware Correspondence Learning for Relative Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihan Chen",
      "Wenfei Yang",
      "Huan Ren",
      "Shifeng Zhang",
      "Tianzhu Zhang",
      "Feng Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_LoRA_Recycle_Unlocking_Tuning-Free_Few-Shot_Adaptability_in_Visual_Foundation_Models_CVPR_2025_paper.html": {
    "title": "LoRA Recycle: Unlocking Tuning-Free Few-Shot Adaptability in Visual Foundation Models by Recycling Pre-Tuned LoRAs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Hu",
      "Yongxian Wei",
      "Li Shen",
      "Chun Yuan",
      "Dacheng Tao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_One2Any_One-Reference_6D_Pose_Estimation_for_Any_Object_CVPR_2025_paper.html": {
    "title": "One2Any: One-Reference 6D Pose Estimation for Any Object",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengya Liu",
      "Siyuan Li",
      "Ajad Chhatkuli",
      "Prune Truong",
      "Luc Van Gool",
      "Federico Tombari"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Stekovic_PyTorchGeoNodes_Enabling_Differentiable_Shape_Programs_for_3D_Shape_Reconstruction_CVPR_2025_paper.html": {
    "title": "PyTorchGeoNodes: Enabling Differentiable Shape Programs for 3D Shape Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sinisa Stekovic",
      "Arslan Artykov",
      "Stefan Ainetter",
      "Mattia D'Urso",
      "Friedrich Fraundorfer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Contextual_AD_Narration_with_Interleaved_Multimodal_Sequence_CVPR_2025_paper.html": {
    "title": "Contextual AD Narration with Interleaved Multimodal Sequence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanlin Wang",
      "Zhan Tong",
      "Kecheng Zheng",
      "Yujun Shen",
      "Limin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_FIFA_Fine-grained_Inter-frame_Attention_for_Drivers_Video_Gaze_Estimation_CVPR_2025_paper.html": {
    "title": "FIFA: Fine-grained Inter-frame Attention for Driver's Video Gaze Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daosong Hu",
      "Mingyue Cui",
      "Kai Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_MNE-SLAM_Multi-Agent_Neural_SLAM_for_Mobile_Robots_CVPR_2025_paper.html": {
    "title": "MNE-SLAM: Multi-Agent Neural SLAM for Mobile Robots",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianchen Deng",
      "Guole Shen",
      "Chen Xun",
      "Shenghai Yuan",
      "Tongxin Jin",
      "Hongming Shen",
      "Yanbo Wang",
      "Jingchuan Wang",
      "Hesheng Wang",
      "Danwei Wang",
      "Weidong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gu_TensoFlow_Tensorial_Flow-based_Sampler_for_Inverse_Rendering_CVPR_2025_paper.html": {
    "title": "TensoFlow: Tensorial Flow-based Sampler for Inverse Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun Gu",
      "Xiaofei Wei",
      "Li Zhang",
      "Xiatian Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_FRAMES-VQA_Benchmarking_Fine-Tuning_Robustness_across_Multi-Modal_Shifts_in_Visual_Question_CVPR_2025_paper.html": {
    "title": "FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengyue Huang",
      "Brisa Maneechotesuwan",
      "Shivang Chopra",
      "Zsolt Kira"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_Shape_Abstraction_via_Marching_Differentiable_Support_Functions_CVPR_2025_paper.html": {
    "title": "Shape Abstraction via Marching Differentiable Support Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sunkyung Park",
      "Jeongmin Lee",
      "Dongjun Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhi_LSceneLLM_Enhancing_Large_3D_Scene_Understanding_Using_Adaptive_Visual_Preferences_CVPR_2025_paper.html": {
    "title": "LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual Preferences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyan Zhi",
      "Peihao Chen",
      "Junyan Li",
      "Shuailei Ma",
      "Xinyu Sun",
      "Tianhang Xiang",
      "Yinjie Lei",
      "Mingkui Tan",
      "Chuang Gan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qu_Event_Fields_Capturing_Light_Fields_at_High_Speed_Resolution_and_CVPR_2025_paper.html": {
    "title": "Event Fields: Capturing Light Fields at High Speed, Resolution, and Dynamic Range",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyuan Qu",
      "Zihao Zou",
      "Vivek Boominathan",
      "Praneeth Chakravarthula",
      "Adithya Pediredla"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_HyperFree_A_Channel-adaptive_and_Tuning-free_Foundation_Model_for_Hyperspectral_Remote_CVPR_2025_paper.html": {
    "title": "HyperFree: A Channel-adaptive and Tuning-free Foundation Model for Hyperspectral Remote Sensing Imagery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingtao Li",
      "Yingyi Liu",
      "Xinyu Wang",
      "Yunning Peng",
      "Chen Sun",
      "Shaoyu Wang",
      "Zhendong Sun",
      "Tian Ke",
      "Xiao Jiang",
      "Tangwei Lu",
      "Anran Zhao",
      "Yanfei Zhong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Exploring_Temporally-Aware_Features_for_Point_Tracking_CVPR_2025_paper.html": {
    "title": "Exploring Temporally-Aware Features for Point Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "In√®s Hyeonsu Kim",
      "Seokju Cho",
      "Jiahui Huang",
      "Jung Yi",
      "Joon-Young Lee",
      "Seungryong Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Malic_GBlobs_Explicit_Local_Structure_via_Gaussian_Blobs_for_Improved_Cross-Domain_CVPR_2025_paper.html": {
    "title": "GBlobs: Explicit Local Structure via Gaussian Blobs for Improved Cross-Domain LiDAR-based 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Du≈°an Maliƒá",
      "Christian Fruhwirth-Reisinger",
      "Samuel Schulter",
      "Horst Possegger"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Abdessaied_V2Dial_Unification_of_Video_and_Visual_Dialog_via_Multimodal_Experts_CVPR_2025_paper.html": {
    "title": "V^2Dial: Unification of Video and Visual Dialog via Multimodal Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Adnen Abdessaied",
      "Anna Rohrbach",
      "Marcus Rohrbach",
      "Andreas Bulling"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Detail-Preserving_Latent_Diffusion_for_Stable_Shadow_Removal_CVPR_2025_paper.html": {
    "title": "Detail-Preserving Latent Diffusion for Stable Shadow Removal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiamin Xu",
      "Yuxin Zheng",
      "Zelong Li",
      "Chi Wang",
      "Renshu Gu",
      "Weiwei Xu",
      "Gang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Scaling_Down_Text_Encoders_of_Text-to-Image_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Scaling Down Text Encoders of Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lifu Wang",
      "Daqing Liu",
      "Xinchen Liu",
      "Xiaodong He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_3D_Gaussian_Head_Avatars_with_Expressive_Dynamic_Appearances_by_Compact_CVPR_2025_paper.html": {
    "title": "3D Gaussian Head Avatars with Expressive Dynamic Appearances by Compact Tensorial Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yating Wang",
      "Xuan Wang",
      "Ran Yi",
      "Yanbo Fan",
      "Jichen Hu",
      "Jingcheng Zhu",
      "Lizhuang Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_MambaIRv2_Attentive_State_Space_Restoration_CVPR_2025_paper.html": {
    "title": "MambaIRv2: Attentive State Space Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Guo",
      "Yong Guo",
      "Yaohua Zha",
      "Yulun Zhang",
      "Wenbo Li",
      "Tao Dai",
      "Shu-Tao Xia",
      "Yawei Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Man_Floating_No_More_Object-Ground_Reconstruction_from_a_Single_Image_CVPR_2025_paper.html": {
    "title": "Floating No More: Object-Ground Reconstruction from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunze Man",
      "Yichen Sheng",
      "Jianming Zhang",
      "Liang-Yan Gui",
      "Yu-Xiong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_POT_Prototypical_Optimal_Transport_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "POT: Prototypical Optimal Transport for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Wang",
      "Tianhong Dai",
      "Bingfeng Zhang",
      "Siyue Yu",
      "Eng Gee Lim",
      "Jimin Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sarkar_CrossOver_3D_Scene_Cross-Modal_Alignment_CVPR_2025_paper.html": {
    "title": "CrossOver: 3D Scene Cross-Modal Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayan Deb Sarkar",
      "Ondrej Miksik",
      "Marc Pollefeys",
      "Daniel Barath",
      "Iro Armeni"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Rethinking_Temporal_Fusion_with_a_Unified_Gradient_Descent_View_for_CVPR_2025_paper.html": {
    "title": "Rethinking Temporal Fusion with a Unified Gradient Descent View for 3D Semantic Occupancy Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dubing Chen",
      "Huan Zheng",
      "Jin Fang",
      "Xingping Dong",
      "Xianfei Li",
      "Wenlong Liao",
      "Tao He",
      "Pai Peng",
      "Jianbing Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SKE-Layout_Spatial_Knowledge_Enhanced_Layout_Generation_with_LLMs_CVPR_2025_paper.html": {
    "title": "SKE-Layout: Spatial Knowledge Enhanced Layout Generation with LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junsheng Wang",
      "Nieqing Cao",
      "Yan Ding",
      "Mengying Xie",
      "Fuqiang Gu",
      "Chao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zielonka_Gaussian_Eigen_Models_for_Human_Heads_CVPR_2025_paper.html": {
    "title": "Gaussian Eigen Models for Human Heads",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wojciech Zielonka",
      "Timo Bolkart",
      "Thabo Beeler",
      "Justus Thies"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jang_Scalable_Video-to-Dataset_Generation_for_Cross-Platform_Mobile_Agents_CVPR_2025_paper.html": {
    "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunseok Jang",
      "Yeda Song",
      "Sungryull Sohn",
      "Lajanugen Logeswaran",
      "Tiange Luo",
      "Dong-Ki Kim",
      "Kyunghoon Bae",
      "Honglak Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ganeshan_Pattern_Analogies_Learning_to_Perform_Programmatic_Image_Edits_by_Analogy_CVPR_2025_paper.html": {
    "title": "Pattern Analogies: Learning to Perform Programmatic Image Edits by Analogy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Ganeshan",
      "Thibault Groueix",
      "Paul Guerrero",
      "Radomir Mech",
      "Matthew Fisher",
      "Daniel Ritchie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_4D-Fly_Fast_4D_Reconstruction_from_a_Single_Monocular_Video_CVPR_2025_paper.html": {
    "title": "4D-Fly: Fast 4D Reconstruction from a Single Monocular Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Diankun Wu",
      "Fangfu Liu",
      "Yi-Hsin Hung",
      "Yue Qian",
      "Xiaohang Zhan",
      "Yueqi Duan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_STAR-Edge_Structure-aware_Local_Spherical_Curve_Representation_for_Thin-walled_Edge_Extraction_CVPR_2025_paper.html": {
    "title": "STAR-Edge: Structure-aware Local Spherical Curve Representation for Thin-walled Edge Extraction from Unstructured Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zikuan Li",
      "Honghua Chen",
      "Yuecheng Wang",
      "Sibo Wu",
      "Mingqiang Wei",
      "Jun Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Tokenize_Image_Patches_Global_Context_Fusion_for_Effective_Haze_Removal_CVPR_2025_paper.html": {
    "title": "Tokenize Image Patches: Global Context Fusion for Effective Haze Removal in Large Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiuchen Chen",
      "Xinyu Yan",
      "Qizhi Xu",
      "Kaiqi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Complementary_Advantages_Exploiting_Cross-Field_Frequency_Correlation_for_NIR-Assisted_Image_Denoising_CVPR_2025_paper.html": {
    "title": "Complementary Advantages: Exploiting Cross-Field Frequency Correlation for NIR-Assisted Image Denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Wang",
      "Hongyuan Wang",
      "Lizhi Wang",
      "Xin Wang",
      "Lin Zhu",
      "Wanxuan Lu",
      "Hua Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Duggal_Eval3D_Interpretable_and_Fine-grained_Evaluation_for_3D_Generation_CVPR_2025_paper.html": {
    "title": "Eval3D: Interpretable and Fine-grained Evaluation for 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shivam Duggal",
      "Yushi Hu",
      "Oscar Michel",
      "Aniruddha Kembhavi",
      "William T. Freeman",
      "Noah A. Smith",
      "Ranjay Krishna",
      "Antonio Torralba",
      "Ali Farhadi",
      "Wei-Chiu Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qin_Boosting_the_Dual-Stream_Architecture_in_Ultra-High_Resolution_Segmentation_with_Resolution-Biased_CVPR_2025_paper.html": {
    "title": "Boosting the Dual-Stream Architecture in Ultra-High Resolution Segmentation with Resolution-Biased Uncertainty Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rong Qin",
      "Xingyu Liu",
      "Jinglei Shi",
      "Liang Lin",
      "Jufeng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_DiffLO_Semantic-Aware_LiDAR_Odometry_with_Diffusion-Based_Refinement_CVPR_2025_paper.html": {
    "title": "DiffLO: Semantic-Aware LiDAR Odometry with Diffusion-Based Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongshu Huang",
      "Chen Liu",
      "Minghang Zhu",
      "Sheng Ao",
      "Chenglu Wen",
      "Cheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_pFedMxF_Personalized_Federated_Class-Incremental_Learning_with_Mixture_of_Frequency_Aggregation_CVPR_2025_paper.html": {
    "title": "pFedMxF: Personalized Federated Class-Incremental Learning with Mixture of Frequency Aggregation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifei Zhang",
      "Hao Zhu",
      "Alysa Ziying Tan",
      "Dianzhi Yu",
      "Longtao Huang",
      "Han Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_Style-Editor_Text-driven_Object-centric_Style_Editing_CVPR_2025_paper.html": {
    "title": "Style-Editor: Text-driven Object-centric Style Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jihun Park",
      "Jongmin Gim",
      "Kyoungmin Lee",
      "Seunghun Lee",
      "Sunghoon Im"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pan_Transfer_Your_Perspective_Controllable_3D_Generation_from_Any_Viewpoint_in_CVPR_2025_paper.html": {
    "title": "Transfer Your Perspective: Controllable 3D Generation from Any Viewpoint in a Driving Scene",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tai-Yu Pan",
      "Sooyoung Jeon",
      "Mengdi Fan",
      "Jinsu Yoo",
      "Zhenyang Feng",
      "Mark Campbell",
      "Kilian Q. Weinberger",
      "Bharath Hariharan",
      "Wei-Lun Chao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Efficient_Transfer_Learning_for_Video-language_Foundation_Models_CVPR_2025_paper.html": {
    "title": "Efficient Transfer Learning for Video-language Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxing Chen",
      "Zizheng Huang",
      "Yan Hong",
      "Yanshuo Wang",
      "Zhongcai Lyu",
      "Zhuoer Xu",
      "Jun Lan",
      "Zhangxuan Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Radio_Frequency_Ray_Tracing_with_Neural_Object_Representation_for_Enhanced_CVPR_2025_paper.html": {
    "title": "Radio Frequency Ray Tracing with Neural Object Representation for Enhanced RF Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Chen",
      "Zihao Feng",
      "Kun Qian",
      "Xinyu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Su_ANNEXE_Unified_Analyzing_Answering_and_Pixel_Grounding_for_Egocentric_Interaction_CVPR_2025_paper.html": {
    "title": "ANNEXE: Unified Analyzing, Answering, and Pixel Grounding for Egocentric Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuejiao Su",
      "Yi Wang",
      "Qiongyang Hu",
      "Chuang Yang",
      "Lap-Pui Chau"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Asim_MET3R_Measuring_Multi-View_Consistency_in_Generated_Images_CVPR_2025_paper.html": {
    "title": "MET3R: Measuring Multi-View Consistency in Generated Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohammad Asim",
      "Christopher Wewer",
      "Thomas Wimmer",
      "Bernt Schiele",
      "Jan Eric Lenssen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bolelli_Segmenting_Maxillofacial_Structures_in_CBCT_Volumes_CVPR_2025_paper.html": {
    "title": "Segmenting Maxillofacial Structures in CBCT Volumes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Federico Bolelli",
      "Kevin Marchesini",
      "Niels van Nistelrooij",
      "Luca Lumetti",
      "Vittorio Pipoli",
      "Elisa Ficarra",
      "Shankeeth Vinayahalingam",
      "Costantino Grana"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xi_3D_Dental_Model_Segmentation_with_Geometrical_Boundary_Preserving_CVPR_2025_paper.html": {
    "title": "3D Dental Model Segmentation with Geometrical Boundary Preserving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shufan Xi",
      "Zexian Liu",
      "Junlin Chang",
      "Hongyu Wu",
      "Xiaogang Wang",
      "Aimin Hao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Neuro-3D_Towards_3D_Visual_Decoding_from_EEG_Signals_CVPR_2025_paper.html": {
    "title": "Neuro-3D: Towards 3D Visual Decoding from EEG Signals",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanqiang Guo",
      "Jiamin Wu",
      "Yonghao Song",
      "Jiahui Bu",
      "Weijian Mai",
      "Qihao Zheng",
      "Wanli Ouyang",
      "Chunfeng Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Vasu_FastVLM_Efficient_Vision_Encoding_for_Vision_Language_Models_CVPR_2025_paper.html": {
    "title": "FastVLM: Efficient Vision Encoding for Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pavan Kumar Anasosalu Vasu",
      "Fartash Faghri",
      "Chun-Liang Li",
      "Cem Koc",
      "Nate True",
      "Albert Antony",
      "Gokula Santhanam",
      "James Gabriel",
      "Peter Grasch",
      "Oncel Tuzel",
      "Hadi Pouransari"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_VISTA3D_A_Unified_Segmentation_Foundation_Model_For_3D_Medical_Imaging_CVPR_2025_paper.html": {
    "title": "VISTA3D: A Unified Segmentation Foundation Model For 3D Medical Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufan He",
      "Pengfei Guo",
      "Yucheng Tang",
      "Andriy Myronenko",
      "Vishwesh Nath",
      "Ziyue Xu",
      "Dong Yang",
      "Can Zhao",
      "Benjamin Simon",
      "Mason Belue",
      "Stephanie Harmon",
      "Baris Turkbey",
      "Daguang Xu",
      "Wenqi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_VideoGigaGAN_Towards_Detail-rich_Video_Super-Resolution_CVPR_2025_paper.html": {
    "title": "VideoGigaGAN: Towards Detail-rich Video Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiran Xu",
      "Taesung Park",
      "Richard Zhang",
      "Yang Zhou",
      "Eli Shechtman",
      "Feng Liu",
      "Jia-Bin Huang",
      "Difan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Probing_the_Mid-level_Vision_Capabilities_of_Self-Supervised_Learning_CVPR_2025_paper.html": {
    "title": "Probing the Mid-level Vision Capabilities of Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuweiyi Chen",
      "Markus Marks",
      "Zezhou Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_S2D-LFE_Sparse-to-Dense_Light_Field_Event_Generation_CVPR_2025_paper.html": {
    "title": "S2D-LFE: Sparse-to-Dense Light Field Event Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yutong Liu",
      "Wenming Weng",
      "Yueyi Zhang",
      "Zhiwei Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gomez-Villa_The_Art_of_Deception_Color_Visual_Illusions_and_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "The Art of Deception: Color Visual Illusions and Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexandra Gomez-Villa",
      "Kai Wang",
      "C.Alejandro Parraga",
      "Bart≈Çomiej Twardowski",
      "Jesus Malo",
      "Javier Vazquez-Corral",
      "Joost van den Weijer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_GLUS_Global-Local_Reasoning_Unified_into_A_Single_Large_Language_Model_CVPR_2025_paper.html": {
    "title": "GLUS: Global-Local Reasoning Unified into A Single Large Language Model for Video Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lang Lin",
      "Xueyang Yu",
      "Ziqi Pang",
      "Yu-Xiong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Progressive_Rendering_Distillation_Adapting_Stable_Diffusion_for_Instant_Text-to-Mesh_Generation_CVPR_2025_paper.html": {
    "title": "Progressive Rendering Distillation: Adapting Stable Diffusion for Instant Text-to-Mesh Generation without 3D Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyuan Ma",
      "Xinyue Liang",
      "Rongyuan Wu",
      "Xiangyu Zhu",
      "Zhen Lei",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jang_Efficient_Long_Video_Tokenization_via_Coordinate-based_Patch_Reconstruction_CVPR_2025_paper.html": {
    "title": "Efficient Long Video Tokenization via Coordinate-based Patch Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiwon Jang",
      "Sihyun Yu",
      "Jinwoo Shin",
      "Pieter Abbeel",
      "Younggyo Seo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Derivative-Free_Diffusion_Manifold-Constrained_Gradient_for_Unified_XAI_CVPR_2025_paper.html": {
    "title": "Derivative-Free Diffusion Manifold-Constrained Gradient for Unified XAI",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Won Jun Kim",
      "Hyungjin Chung",
      "Jaemin Kim",
      "Sangmin Lee",
      "Byeongsu Sim",
      "Jong Chul Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yellapragada_ZoomLDM_Latent_Diffusion_Model_for_Multi-scale_Image_Generation_CVPR_2025_paper.html": {
    "title": "ZoomLDM: Latent Diffusion Model for Multi-scale Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Srikar Yellapragada",
      "Alexandros Graikos",
      "Kostas Triaridis",
      "Prateek Prasanna",
      "Rajarsi Gupta",
      "Joel Saltz",
      "Dimitris Samaras"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cai_Do_Computer_Vision_Foundation_Models_Learn_the_Low-level_Characteristics_of_CVPR_2025_paper.html": {
    "title": "Do Computer Vision Foundation Models Learn the Low-level Characteristics of the Human Visual System?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yancheng Cai",
      "Fei Yin",
      "Dounia Hammou",
      "Rafal Mantiuk"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_GaussianUDF_Inferring_Unsigned_Distance_Functions_through_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "GaussianUDF: Inferring Unsigned Distance Functions through 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shujuan Li",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Towards_RAW_Object_Detection_in_Diverse_Conditions_CVPR_2025_paper.html": {
    "title": "Towards RAW Object Detection in Diverse Conditions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhong-Yu Li",
      "Xin Jin",
      "Bo-Yuan Sun",
      "Chun-Le Guo",
      "Ming-Ming Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cao_FLAME_Frozen_Large_Language_Models_Enable_Data-Efficient_Language-Image_Pre-training_CVPR_2025_paper.html": {
    "title": "FLAME: Frozen Large Language Models Enable Data-Efficient Language-Image Pre-training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anjia Cao",
      "Xing Wei",
      "Zhiheng Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Walker_CrossSDF_3D_Reconstruction_of_Thin_Structures_From_Cross-Sections_CVPR_2025_paper.html": {
    "title": "CrossSDF: 3D Reconstruction of Thin Structures From Cross-Sections",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Thomas Walker",
      "Salvatore Esposito",
      "Daniel Rebain",
      "Amir Vaxman",
      "Arno Onken",
      "Changjian Li",
      "Oisin Mac Aodha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_DV-Matcher_Deformation-based_Non-rigid_Point_Cloud_Matching_Guided_by_Pre-trained_Visual_CVPR_2025_paper.html": {
    "title": "DV-Matcher: Deformation-based Non-rigid Point Cloud Matching Guided by Pre-trained Visual Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangquan Chen",
      "Puhua Jiang",
      "Ruqi Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Reasoning_Mamba_Hypergraph-Guided_Region_Relation_Calculating_for_Weakly_Supervised_Affordance_CVPR_2025_paper.html": {
    "title": "Reasoning Mamba: Hypergraph-Guided Region Relation Calculating for Weakly Supervised Affordance Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Wang",
      "Aming Wu",
      "Muli Yang",
      "Yukuan Min",
      "Yihang Zhu",
      "Cheng Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fukuda_Adapter_Merging_with_Centroid_Prototype_Mapping_for_Scalable_Class-Incremental_Learning_CVPR_2025_paper.html": {
    "title": "Adapter Merging with Centroid Prototype Mapping for Scalable Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takuma Fukuda",
      "Hiroshi Kera",
      "Kazuhiko Kawamoto"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_OpenSDI_Spotting_Diffusion-Generated_Images_in_the_Open_World_CVPR_2025_paper.html": {
    "title": "OpenSDI: Spotting Diffusion-Generated Images in the Open World",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yabin Wang",
      "Zhiwu Huang",
      "Xiaopeng Hong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dai_Adaptive_Part_Learning_for_Fine-Grained_Generalized_Category_Discovery_A_Plug-and-Play_CVPR_2025_paper.html": {
    "title": "Adaptive Part Learning for Fine-Grained Generalized Category Discovery: A Plug-and-Play Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyuan Dai",
      "Hanzhuo Huang",
      "Yu Wu",
      "Sibei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_Online_Task-Free_Continual_Learning_via_Dynamic_Expansionable_Memory_Distribution_CVPR_2025_paper.html": {
    "title": "Online Task-Free Continual Learning via Dynamic Expansionable Memory Distribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Ye",
      "Adrian G. Bors"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mei_Lux_Post_Facto_Learning_Portrait_Performance_Relighting_with_Conditional_Video_CVPR_2025_paper.html": {
    "title": "Lux Post Facto: Learning Portrait Performance Relighting with Conditional Video Diffusion and a Hybrid Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiqun Mei",
      "Mingming He",
      "Li Ma",
      "Julien Philip",
      "Wenqi Xian",
      "David M George",
      "Xueming Yu",
      "Gabriel Dedic",
      "Ahmet Levent Ta≈üel",
      "Ning Yu",
      "Vishal M. Patel",
      "Paul Debevec"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_DiG_Scalable_and_Efficient_Diffusion_Models_with_Gated_Linear_Attention_CVPR_2025_paper.html": {
    "title": "DiG: Scalable and Efficient Diffusion Models with Gated Linear Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lianghui Zhu",
      "Zilong Huang",
      "Bencheng Liao",
      "Jun Hao Liew",
      "Hanshu Yan",
      "Jiashi Feng",
      "Xinggang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gong_Monocular_and_Generalizable_Gaussian_Talking_Head_Animation_CVPR_2025_paper.html": {
    "title": "Monocular and Generalizable Gaussian Talking Head Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengjie Gong",
      "Haojie Li",
      "Jiapeng Tang",
      "Dongming Hu",
      "Shuangping Huang",
      "Hao Chen",
      "Tianshui Chen",
      "Zhuoman Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lei_Rethinking_Token_Reduction_with_Parameter-Efficient_Fine-Tuning_in_ViT_for_Pixel-Level_CVPR_2025_paper.html": {
    "title": "Rethinking Token Reduction with Parameter-Efficient Fine-Tuning in ViT for Pixel-Level Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Lei",
      "Ao Li",
      "Hu Yao",
      "Ce Zhu",
      "Le Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_SVDC_Consistent_Direct_Time-of-Flight_Video_Depth_Completion_with_Frequency_Selective_CVPR_2025_paper.html": {
    "title": "SVDC: Consistent Direct Time-of-Flight Video Depth Completion with Frequency Selective Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Zhu",
      "Jijun Xiang",
      "Xianqi Wang",
      "Longliang Liu",
      "Yu Wang",
      "Hong Zhang",
      "Fei Guo",
      "Xin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mehta_Locally_Orderless_Images_for_Optimization_in_Differentiable_Rendering_CVPR_2025_paper.html": {
    "title": "Locally Orderless Images for Optimization in Differentiable Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ishit Mehta",
      "Manmohan Chandraker",
      "Ravi Ramamoorthi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Azam_Plug-and-Play_Interpretable_Responsible_Text-to-Image_Generation_via_Dual-Space_Multi-facet_Concept_Control_CVPR_2025_paper.html": {
    "title": "Plug-and-Play Interpretable Responsible Text-to-Image Generation via Dual-Space Multi-facet Concept Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Basim Azam",
      "Naveed Akhtar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Rethinking_Training_for_De-biasing_Text-to-Image_Generation_Unlocking_the_Potential_of_CVPR_2025_paper.html": {
    "title": "Rethinking Training for De-biasing Text-to-Image Generation: Unlocking the Potential of Stable Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eunji Kim",
      "Siwon Kim",
      "Minjun Park",
      "Rahim Entezari",
      "Sungroh Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_FLAIR_VLM_with_Fine-grained_Language-informed_Image_Representations_CVPR_2025_paper.html": {
    "title": "FLAIR: VLM with Fine-grained Language-informed Image Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rui Xiao",
      "Sanghwan Kim",
      "Mariana-Iuliana Georgescu",
      "Zeynep Akata",
      "Stephan Alaniz"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zubic_GG-SSMs_Graph-Generating_State_Space_Models_CVPR_2025_paper.html": {
    "title": "GG-SSMs: Graph-Generating State Space Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nikola Zubic",
      "Davide Scaramuzza"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Barda_Instant3dit_Multiview_Inpainting_for_Fast_Editing_of_3D_Objects_CVPR_2025_paper.html": {
    "title": "Instant3dit: Multiview Inpainting for Fast Editing of 3D Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Barda",
      "Matheus Gadelha",
      "Vladimir G. Kim",
      "Noam Aigerman",
      "Amit H. Bermano",
      "Thibault Groueix"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yao_STDD_Spatio-Temporal_Dual_Diffusion_for_Video_Generation_CVPR_2025_paper.html": {
    "title": "STDD: Spatio-Temporal Dual Diffusion for Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuaizhen Yao",
      "Xiaoya Zhang",
      "Xin Liu",
      "Mengyi Liu",
      "Zhen Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Implicit_Correspondence_Learning_for_Image-to-Point_Cloud_Registration_CVPR_2025_paper.html": {
    "title": "Implicit Correspondence Learning for Image-to-Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinjun Li",
      "Wenfei Yang",
      "Jiacheng Deng",
      "Zhixin Cheng",
      "Xu Zhou",
      "Tianzhu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Continuous_Adverse_Weather_Removal_via_Degradation-Aware_Distillation_CVPR_2025_paper.html": {
    "title": "Continuous Adverse Weather Removal via Degradation-Aware Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Lu",
      "Jie Xiao",
      "Yurui Zhu",
      "Xueyang Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Thakral_Fine-Grained_Erasure_in_Text-to-Image_Diffusion-based_Foundation_Models_CVPR_2025_paper.html": {
    "title": "Fine-Grained Erasure in Text-to-Image Diffusion-based Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kartik Thakral",
      "Tamar Glaser",
      "Tal Hassner",
      "Mayank Vatsa",
      "Richa Singh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kordopatis-Zilos_ILIAS_Instance-Level_Image_retrieval_At_Scale_CVPR_2025_paper.html": {
    "title": "ILIAS: Instance-Level Image retrieval At Scale",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Giorgos Kordopatis-Zilos",
      "Vladan Stojniƒá",
      "Anna Manko",
      "Pavel Suma",
      "Nikolaos-Antonios Ypsilantis",
      "Nikos Efthymiadis",
      "Zakaria Laskar",
      "Jiri Matas",
      "Ondrej Chum",
      "Giorgos Tolias"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hesham_Exploiting_Temporal_State_Space_Sharing_for_Video_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Exploiting Temporal State Space Sharing for Video Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Syed Ariff Syed Hesham",
      "Yun Liu",
      "Guolei Sun",
      "Henghui Ding",
      "Jing Yang",
      "Ender Konukoglu",
      "Xue Geng",
      "Xudong Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_DeRS_Towards_Extremely_Efficient_Upcycled_Mixture-of-Experts_Models_CVPR_2025_paper.html": {
    "title": "DeRS: Towards Extremely Efficient Upcycled Mixture-of-Experts Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongqi Huang",
      "Peng Ye",
      "Chenyu Huang",
      "Jianjian Cao",
      "Lin Zhang",
      "Baopu Li",
      "Gang Yu",
      "Tao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_GeoDepth_From_Point-to-Depth_to_Plane-to-Depth_Modeling_for_Self-Supervised_Monocular_Depth_CVPR_2025_paper.html": {
    "title": "GeoDepth: From Point-to-Depth to Plane-to-Depth Modeling for Self-Supervised Monocular Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haifeng Wu",
      "Shuhang Gu",
      "Lixin Duan",
      "Wen Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_SSHNet_Unsupervised_Cross-modal_Homography_Estimation_via_Problem_Reformulation_and_Split_CVPR_2025_paper.html": {
    "title": "SSHNet: Unsupervised Cross-modal Homography Estimation via Problem Reformulation and Split Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junchen Yu",
      "Si-Yuan Cao",
      "Runmin Zhang",
      "Chenghao Zhang",
      "Zhu Yu",
      "Shujie Chen",
      "Bailin Yang",
      "Hui-Liang Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_High-fidelity_3D_Object_Generation_from_Single_Image_with_RGBN-Volume_Gaussian_CVPR_2025_paper.html": {
    "title": "High-fidelity 3D Object Generation from Single Image with RGBN-Volume Gaussian Reconstruction Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Shen",
      "Kun Zhou",
      "He Wang",
      "Yin Yang",
      "Tianjia Shao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Steepest_Descent_Density_Control_for_Compact_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "Steepest Descent Density Control for Compact 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peihao Wang",
      "Yuehao Wang",
      "Dilin Wang",
      "Sreyas Mohan",
      "Zhiwen Fan",
      "Lemeng Wu",
      "Ruisi Cai",
      "Yu-Ying Yeh",
      "Zhangyang Wang",
      "Qiang Liu",
      "Rakesh Ranjan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Optimal_Transport-Guided_Source-Free_Adaptation_for_Face_Anti-Spoofing_CVPR_2025_paper.html": {
    "title": "Optimal Transport-Guided Source-Free Adaptation for Face Anti-Spoofing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuowei Li",
      "Tianchen Zhao",
      "Xiang Xu",
      "Zheng Zhang",
      "Zhihua Li",
      "Xuanbai Chen",
      "Qin Zhang",
      "Alessandro Bergamo",
      "Anil K. Jain",
      "Yifan Xing"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_USP-Gaussian_Unifying_Spike-based_Image_Reconstruction_Pose_Correction_and_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "USP-Gaussian: Unifying Spike-based Image Reconstruction, Pose Correction and Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kang Chen",
      "Jiyuan Zhang",
      "Zecheng Hao",
      "Yajing Zheng",
      "Tiejun Huang",
      "Zhaofei Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cho_Robust_3D_Shape_Reconstruction_in_Zero-Shot_from_a_Single_Image_CVPR_2025_paper.html": {
    "title": "Robust 3D Shape Reconstruction in Zero-Shot from a Single Image in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junhyeong Cho",
      "Kim Youwang",
      "Hunmin Yang",
      "Tae-Hyun Oh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_BOE-ViT_Boosting_Orientation_Estimation_with_Equivariance_in_Self-Supervised_3D_Subtomogram_CVPR_2025_paper.html": {
    "title": "BOE-ViT: Boosting Orientation Estimation with Equivariance in Self-Supervised 3D Subtomogram Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runmin Jiang",
      "Jackson Daggett",
      "Shriya Pingulkar",
      "Yizhou Zhao",
      "Priyanshu Dhingra",
      "Daniel Brown",
      "Qifeng Wu",
      "Xiangrui Zeng",
      "Xingjian Li",
      "Min Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Holmes-VAU_Towards_Long-term_Video_Anomaly_Understanding_at_Any_Granularity_CVPR_2025_paper.html": {
    "title": "Holmes-VAU: Towards Long-term Video Anomaly Understanding at Any Granularity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huaxin Zhang",
      "Xiaohao Xu",
      "Xiang Wang",
      "Jialong Zuo",
      "Xiaonan Huang",
      "Changxin Gao",
      "Shanjun Zhang",
      "Li Yu",
      "Nong Sang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Adventurer_Optimizing_Vision_Mamba_Architecture_Designs_for_Efficiency_CVPR_2025_paper.html": {
    "title": "Adventurer: Optimizing Vision Mamba Architecture Designs for Efficiency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Wang",
      "Timing Yang",
      "Yaodong Yu",
      "Sucheng Ren",
      "Guoyizhe Wei",
      "Angtian Wang",
      "Wei Shao",
      "Yuyin Zhou",
      "Alan Yuille",
      "Cihang Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Caldarola_Beyond_Local_Sharpness_Communication-Efficient_Global_Sharpness-aware_Minimization_for_Federated_Learning_CVPR_2025_paper.html": {
    "title": "Beyond Local Sharpness: Communication-Efficient Global Sharpness-aware Minimization for Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Debora Caldarola",
      "Pietro Cagnasso",
      "Barbara Caputo",
      "Marco Ciccone"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wei_ALIEN_Implicit_Neural_Representations_for_Human_Motion_Prediction_under_Arbitrary_CVPR_2025_paper.html": {
    "title": "ALIEN: Implicit Neural Representations for Human Motion Prediction under Arbitrary Latency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Wei",
      "Xiaoning Sun",
      "Xizhan Gao",
      "Shengxiang Hu",
      "Huaijiang Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_Parameterized_Blur_Kernel_Prior_Learning_for_Local_Motion_Deblurring_CVPR_2025_paper.html": {
    "title": "Parameterized Blur Kernel Prior Learning for Local Motion Deblurring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenxuan Fang",
      "Fangfang Wu",
      "Tao Huang",
      "Le Dong",
      "Weisheng Dong",
      "Xin Li",
      "Guangming Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_QuartDepth_Post-Training_Quantization_for_Real-Time_Depth_Estimation_on_the_Edge_CVPR_2025_paper.html": {
    "title": "QuartDepth: Post-Training Quantization for Real-Time Depth Estimation on the Edge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Shen",
      "Weize Ma",
      "Jing Liu",
      "Changdi Yang",
      "Rui Ding",
      "Quanyi Wang",
      "Henghui Ding",
      "Wei Niu",
      "Yanzhi Wang",
      "Pu Zhao",
      "Jun Lin",
      "Jiuxiang Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Diko_ReWind_Understanding_Long_Videos_with_Instructed_Learnable_Memory_CVPR_2025_paper.html": {
    "title": "ReWind: Understanding Long Videos with Instructed Learnable Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Anxhelo Diko",
      "Tinghuai Wang",
      "Wassim Swaileh",
      "Shiyan Sun",
      "Ioannis Patras"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Sufficient_Invariant_Learning_for_Distribution_Shift_CVPR_2025_paper.html": {
    "title": "Sufficient Invariant Learning for Distribution Shift",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taero Kim",
      "Subeen Park",
      "Sungjun Lim",
      "Yonghan Jung",
      "Krikamol Muandet",
      "Kyungwoo Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ju_DirectTriGS_Triplane-based_Gaussian_Splatting_Field_Representation_for_3D_Generation_CVPR_2025_paper.html": {
    "title": "DirectTriGS: Triplane-based Gaussian Splatting Field Representation for 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoliang Ju",
      "Hongsheng Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wen_Domain_Generalization_in_CLIP_via_Learning_with_Diverse_Text_Prompts_CVPR_2025_paper.html": {
    "title": "Domain Generalization in CLIP via Learning with Diverse Text Prompts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changsong Wen",
      "Zelin Peng",
      "Yu Huang",
      "Xiaokang Yang",
      "Wei Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Scene4U_Hierarchical_Layered_3D_Scene_Reconstruction_from_Single_Panoramic_Image_CVPR_2025_paper.html": {
    "title": "Scene4U: Hierarchical Layered 3D Scene Reconstruction from Single Panoramic Image for Your Immerse Exploration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zilong Huang",
      "Jun He",
      "Junyan Ye",
      "Lihan Jiang",
      "Weijia Li",
      "Yiping Chen",
      "Ting Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Make-It-Animatable_An_Efficient_Framework_for_Authoring_Animation-Ready_3D_Characters_CVPR_2025_paper.html": {
    "title": "Make-It-Animatable: An Efficient Framework for Authoring Animation-Ready 3D Characters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiyang Guo",
      "Jinxu Xiang",
      "Kai Ma",
      "Wengang Zhou",
      "Houqiang Li",
      "Ran Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_IterIS_Iterative_Inference-Solving_Alignment_for_LoRA_Merging_CVPR_2025_paper.html": {
    "title": "IterIS: Iterative Inference-Solving Alignment for LoRA Merging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongxu Chen",
      "Zhen Wang",
      "Runshi Li",
      "Bowei Zhu",
      "Long Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiang_ACAttack_Adaptive_Cross_Attacking_RGB-T_Tracker_via_Multi-Modal_Response_Decoupling_CVPR_2025_paper.html": {
    "title": "ACAttack: Adaptive Cross Attacking RGB-T Tracker via Multi-Modal Response Decoupling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Xiang",
      "Qinglong Yan",
      "Hao Zhang",
      "Jiayi Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_DeCafNet_Delegate_and_Conquer_for_Efficient_Temporal_Grounding_in_Long_CVPR_2025_paper.html": {
    "title": "DeCafNet: Delegate and Conquer for Efficient Temporal Grounding in Long Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijia Lu",
      "A S M Iftekhar",
      "Gaurav Mittal",
      "Tianjian Meng",
      "Xiawei Wang",
      "Cheng Zhao",
      "Rohith Kukkala",
      "Ehsan Elhamifar",
      "Mei Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Efficient_ANN-Guided_Distillation_Aligning_Rate-based_Features_of_Spiking_Neural_Networks_CVPR_2025_paper.html": {
    "title": "Efficient ANN-Guided Distillation: Aligning Rate-based Features of Spiking Neural Networks through Hybrid Block-wise Replacement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shu Yang",
      "Chengting Yu",
      "Lei Liu",
      "Hanzhi Ma",
      "Aili Wang",
      "Erping Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Erkoc_PrEditor3D_Fast_and_Precise_3D_Shape_Editing_CVPR_2025_paper.html": {
    "title": "PrEditor3D: Fast and Precise 3D Shape Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziya Erko√ß",
      "Can G√ºmeli",
      "Chaoyang Wang",
      "Matthias Nie√üner",
      "Angela Dai",
      "Peter Wonka",
      "Hsin-Ying Lee",
      "Peiye Zhuang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Subspace_Constraint_and_Contribution_Estimation_for_Heterogeneous_Federated_Learning_CVPR_2025_paper.html": {
    "title": "Subspace Constraint and Contribution Estimation for Heterogeneous Federated Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangtao Zhang",
      "Sheng Li",
      "Ao Li",
      "Yipeng Liu",
      "Fan Zhang",
      "Ce Zhu",
      "Le Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_HoGS_Unified_Near_and_Far_Object_Reconstruction_via_Homogeneous_Gaussian_CVPR_2025_paper.html": {
    "title": "HoGS: Unified Near and Far Object Reconstruction via Homogeneous Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinpeng Liu",
      "Zeyi Huang",
      "Fumio Okura",
      "Yasuyuki Matsushita"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_SmartEraser_Remove_Anything_from_Images_using_Masked-Region_Guidance_CVPR_2025_paper.html": {
    "title": "SmartEraser: Remove Anything from Images using Masked-Region Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longtao Jiang",
      "Zhendong Wang",
      "Jianmin Bao",
      "Wengang Zhou",
      "Dongdong Chen",
      "Lei Shi",
      "Dong Chen",
      "Houqiang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_ComRoPE_Scalable_and_Robust_Rotary_Position_Embedding_Parameterized_by_Trainable_CVPR_2025_paper.html": {
    "title": "ComRoPE: Scalable and Robust Rotary Position Embedding Parameterized by Trainable Commuting Angle Matrices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Yu",
      "Tangyu Jiang",
      "Shuning Jia",
      "Shannan Yan",
      "Shunning Liu",
      "Haolong Qian",
      "Guanghao Li",
      "Shuting Dong",
      "Chun Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Amrani_Sample-_and_Parameter-Efficient_Auto-Regressive_Image_Models_CVPR_2025_paper.html": {
    "title": "Sample- and Parameter-Efficient Auto-Regressive Image Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Elad Amrani",
      "Leonid Karlinsky",
      "Alex Bronstein"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Robust_Audio-Visual_Segmentation_via_Audio-Guided_Visual_Convergent_Alignment_CVPR_2025_paper.html": {
    "title": "Robust Audio-Visual Segmentation via Audio-Guided Visual Convergent Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Liu",
      "Peike Li",
      "Liying Yang",
      "Dadong Wang",
      "Lincheng Li",
      "Xin Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_LOCORE_Image_Re-ranking_with_Long-Context_Sequence_Modeling_CVPR_2025_paper.html": {
    "title": "LOCORE: Image Re-ranking with Long-Context Sequence Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zilin Xiao",
      "Pavel Suma",
      "Ayush Sachdeva",
      "Hao-Jen Wang",
      "Giorgos Kordopatis-Zilos",
      "Giorgos Tolias",
      "Vicente Ordonez"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_NeRFPrior_Learning_Neural_Radiance_Field_as_a_Prior_for_Indoor_CVPR_2025_paper.html": {
    "title": "NeRFPrior: Learning Neural Radiance Field as a Prior for Indoor Scene Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyuan Zhang",
      "Emily Yue-ting Jia",
      "Junsheng Zhou",
      "Baorui Ma",
      "Kanle Shi",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_BiLoRA_Almost-Orthogonal_Parameter_Spaces_for_Continual_Learning_CVPR_2025_paper.html": {
    "title": "BiLoRA: Almost-Orthogonal Parameter Spaces for Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Zhu",
      "Yifei Zhang",
      "Junhao Dong",
      "Piotr Koniusz"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Vid2Sim_Generalizable_Video-based_Reconstruction_of_Appearance_Geometry_and_Physics_for_CVPR_2025_paper.html": {
    "title": "Vid2Sim: Generalizable, Video-based Reconstruction of Appearance, Geometry and Physics for Mesh-free Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuhao Chen",
      "Zhiyang Dou",
      "Chen Wang",
      "Yiming Huang",
      "Anjun Chen",
      "Qiao Feng",
      "Jiatao Gu",
      "Lingjie Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cao_SceneTAP_Scene-Coherent_Typographic_Adversarial_Planner_against_Vision-Language_Models_in_Real-World_CVPR_2025_paper.html": {
    "title": "SceneTAP: Scene-Coherent Typographic Adversarial Planner against Vision-Language Models in Real-World Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Cao",
      "Yun Xing",
      "Jie Zhang",
      "Di Lin",
      "Tianwei Zhang",
      "Ivor Tsang",
      "Yang Liu",
      "Qing Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Collaborative_Decoding_Makes_Visual_Auto-Regressive_Modeling_Efficient_CVPR_2025_paper.html": {
    "title": "Collaborative Decoding Makes Visual Auto-Regressive Modeling Efficient",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zigeng Chen",
      "Xinyin Ma",
      "Gongfan Fang",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Vuong_AerialMegaDepth_Learning_Aerial-Ground_Reconstruction_and_View_Synthesis_CVPR_2025_paper.html": {
    "title": "AerialMegaDepth: Learning Aerial-Ground Reconstruction and View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Khiem Vuong",
      "Anurag Ghosh",
      "Deva Ramanan",
      "Srinivasa Narasimhan",
      "Shubham Tulsiani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Towards_Training-free_Anomaly_Detection_with_Vision_and_Language_Foundation_Models_CVPR_2025_paper.html": {
    "title": "Towards Training-free Anomaly Detection with Vision and Language Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinjin Zhang",
      "Guodong Wang",
      "Yizhou Jin",
      "Di Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_LiVOS_Light_Video_Object_Segmentation_with_Gated_Linear_Matching_CVPR_2025_paper.html": {
    "title": "LiVOS: Light Video Object Segmentation with Gated Linear Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qin Liu",
      "Jianfeng Wang",
      "Zhengyuan Yang",
      "Linjie Li",
      "Kevin Lin",
      "Marc Niethammer",
      "Lijuan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Dynamic_Content_Prediction_with_Motion-aware_Priors_for_Blind_Face_Video_CVPR_2025_paper.html": {
    "title": "Dynamic Content Prediction with Motion-aware Priors for Blind Face Video Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lianxin Xie",
      "Bingbing Zheng",
      "Si Wu",
      "Hau San Wong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Enomoto_Polarized_Color_Screen_Matting_CVPR_2025_paper.html": {
    "title": "Polarized Color Screen Matting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kenji Enomoto",
      "Scott Cohen",
      "Brian Price",
      "TJ Rhodes"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Visual_Representation_Learning_through_Causal_Intervention_for_Controllable_Image_Editing_CVPR_2025_paper.html": {
    "title": "Visual Representation Learning through Causal Intervention for Controllable Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanshan Huang",
      "Haoxuan Li",
      "Chunyuan Zheng",
      "Lei Wang",
      "Guorui Liao",
      "Zhili Gong",
      "Huayi Yang",
      "Li Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Exploring_the_Deep_Fusion_of_Large_Language_Models_and_Diffusion_CVPR_2025_paper.html": {
    "title": "Exploring the Deep Fusion of Large Language Models and Diffusion Transformers for Text-to-Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingda Tang",
      "Boyang Zheng",
      "Sayak Paul",
      "Saining Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_A_Comprehensive_Study_of_Decoder-Only_LLMs_for_Text-to-Image_Generation_CVPR_2025_paper.html": {
    "title": "A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Z. Wang",
      "Songwei Ge",
      "Tero Karras",
      "Ming-Yu Liu",
      "Yogesh Balaji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Exploring_Sparse_MoE_in_GANs_for_Text-conditioned_Image_Synthesis_CVPR_2025_paper.html": {
    "title": "Exploring Sparse MoE in GANs for Text-conditioned Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiapeng Zhu",
      "Ceyuan Yang",
      "Kecheng Zheng",
      "Yinghao Xu",
      "Zifan Shi",
      "Yifei Zhang",
      "Qifeng Chen",
      "Yujun Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Deformable_Radial_Kernel_Splatting_CVPR_2025_paper.html": {
    "title": "Deformable Radial Kernel Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi-Hua Huang",
      "Ming-Xian Lin",
      "Yang-Tian Sun",
      "Ziyi Yang",
      "Xiaoyang Lyu",
      "Yan-Pei Cao",
      "Xiaojuan Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Choi_GOAL_Global-local_Object_Alignment_Learning_CVPR_2025_paper.html": {
    "title": "GOAL: Global-local Object Alignment Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyungyu Choi",
      "Young Kyun Jang",
      "Chanho Eom"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qu_Bayesian_Prompt_Flow_Learning_for_Zero-Shot_Anomaly_Detection_CVPR_2025_paper.html": {
    "title": "Bayesian Prompt Flow Learning for Zero-Shot Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Qu",
      "Xian Tao",
      "Xinyi Gong",
      "ShiChen Qu",
      "Qiyu Chen",
      "Zhengtao Zhang",
      "Xingang Wang",
      "Guiguang Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yamaguchi_Post-pre-training_for_Modality_Alignment_in_Vision-Language_Foundation_Models_CVPR_2025_paper.html": {
    "title": "Post-pre-training for Modality Alignment in Vision-Language Foundation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shin'ya Yamaguchi",
      "Dewei Feng",
      "Sekitoshi Kanai",
      "Kazuki Adachi",
      "Daiki Chijiwa"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ahmed_Efficient_Event-Based_Object_Detection_A_Hybrid_Neural_Network_with_Spatial_CVPR_2025_paper.html": {
    "title": "Efficient Event-Based Object Detection: A Hybrid Neural Network with Spatial and Temporal Attention",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soikat Hasan Ahmed",
      "Jan Finkbeiner",
      "Emre Neftci"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chaturvedi_SynthLight_Portrait_Relighting_with_Diffusion_Model_by_Learning_to_Re-render_CVPR_2025_paper.html": {
    "title": "SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sumit Chaturvedi",
      "Mengwei Ren",
      "Yannick Hold-Geoffroy",
      "Jingyuan Liu",
      "Julie Dorsey",
      "Zhixin Shu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Pseudo_Visible_Feature_Fine-Grained_Fusion_for_Thermal_Object_Detection_CVPR_2025_paper.html": {
    "title": "Pseudo Visible Feature Fine-Grained Fusion for Thermal Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting Li",
      "Mao Ye",
      "Tianwen Wu",
      "Nianxin Li",
      "Shuaifeng Li",
      "Song Tang",
      "Luping Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_HUNet_Homotopy_Unfolding_Network_for_Image_Compressive_Sensing_CVPR_2025_paper.html": {
    "title": "HUNet: Homotopy Unfolding Network for Image Compressive Sensing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feiyang Shen",
      "Hongping Gan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_HalLoc_Token-level_Localization_of_Hallucinations_for_Vision_Language_Models_CVPR_2025_paper.html": {
    "title": "HalLoc: Token-level Localization of Hallucinations for Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eunkyu Park",
      "Minyeong Kim",
      "Gunhee Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gu_DiffPortrait360_Consistent_Portrait_Diffusion_for_360_View_Synthesis_CVPR_2025_paper.html": {
    "title": "DiffPortrait360: Consistent Portrait Diffusion for 360 View Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuming Gu",
      "Phong Tran",
      "Yujian Zheng",
      "Hongyi Xu",
      "Heyuan Li",
      "Adilbek Karmanov",
      "Hao Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_SURGEON_Memory-Adaptive_Fully_Test-Time_Adaptation_via_Dynamic_Activation_Sparsity_CVPR_2025_paper.html": {
    "title": "SURGEON: Memory-Adaptive Fully Test-Time Adaptation via Dynamic Activation Sparsity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ke Ma",
      "Jiaqi Tang",
      "Bin Guo",
      "Fan Dang",
      "Sicong Liu",
      "Zhui Zhu",
      "Lei Wu",
      "Cheng Fang",
      "Ying-Cong Chen",
      "Zhiwen Yu",
      "Yunhao Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_NVILA_Efficient_Frontier_Visual_Language_Models_CVPR_2025_paper.html": {
    "title": "NVILA: Efficient Frontier Visual Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhijian Liu",
      "Ligeng Zhu",
      "Baifeng Shi",
      "Zhuoyang Zhang",
      "Yuming Lou",
      "Shang Yang",
      "Haocheng Xi",
      "Shiyi Cao",
      "Yuxian Gu",
      "Dacheng Li",
      "Xiuyu Li",
      "Haotian Tang",
      "Yunhao Fang",
      "Yukang Chen",
      "Cheng-Yu Hsieh",
      "De-An Huang",
      "An-Chieh Cheng",
      "Jinyi Hu",
      "Sifei Liu",
      "Ranjay Krishna",
      "Pavlo Molchanov",
      "Jan Kautz",
      "Hongxu Yin",
      "Song Han",
      "Yao Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_SemiETS_Integrating_Spatial_and_Content_Consistencies_for_Semi-Supervised_End-to-end_Text_CVPR_2025_paper.html": {
    "title": "SemiETS: Integrating Spatial and Content Consistencies for Semi-Supervised End-to-end Text Spotting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongliang Luo",
      "Hanshen Zhu",
      "Ziyang Zhang",
      "Dingkang Liang",
      "Xudong Xie",
      "Yuliang Liu",
      "Xiang Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_See_Further_When_Clear_Curriculum_Consistency_Model_CVPR_2025_paper.html": {
    "title": "See Further When Clear: Curriculum Consistency Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunpeng Liu",
      "Boxiao Liu",
      "Yi Zhang",
      "Xingzhong Hou",
      "Guanglu Song",
      "Yu Liu",
      "Haihang You"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_From_Slow_Bidirectional_to_Fast_Autoregressive_Video_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianwei Yin",
      "Qiang Zhang",
      "Richard Zhang",
      "William T. Freeman",
      "Fredo Durand",
      "Eli Shechtman",
      "Xun Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_PassionSR_Post-Training_Quantization_with_Adaptive_Scale_in_One-Step_Diffusion_based_CVPR_2025_paper.html": {
    "title": "PassionSR: Post-Training Quantization with Adaptive Scale in One-Step Diffusion based Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Libo Zhu",
      "Jianze Li",
      "Haotong Qin",
      "Wenbo Li",
      "Yulun Zhang",
      "Yong Guo",
      "Xiaokang Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dai_RainyGS_Efficient_Rain_Synthesis_with_Physically-Based_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "RainyGS: Efficient Rain Synthesis with Physically-Based Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyu Dai",
      "Xingyu Ni",
      "Qianfan Shen",
      "Wenzheng Chen",
      "Baoquan Chen",
      "Mengyu Chu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Miao_Noise_Diffusion_for_Enhancing_Semantic_Faithfulness_in_Text-to-Image_Synthesis_CVPR_2025_paper.html": {
    "title": "Noise Diffusion for Enhancing Semantic Faithfulness in Text-to-Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boming Miao",
      "Chunxiao Li",
      "Xiaoxiao Wang",
      "Andi Zhang",
      "Rui Sun",
      "Zizhe Wang",
      "Yao Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_MonoInstance_Enhancing_Monocular_Priors_via_Multi-view_Instance_Alignment_for_Neural_CVPR_2025_paper.html": {
    "title": "MonoInstance: Enhancing Monocular Priors via Multi-view Instance Alignment for Neural Rendering and Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyuan Zhang",
      "Yixiao Yang",
      "Han Huang",
      "Liang Han",
      "Kanle Shi",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ding_Three-view_Focal_Length_Recovery_From_Homographies_CVPR_2025_paper.html": {
    "title": "Three-view Focal Length Recovery From Homographies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yaqing Ding",
      "Viktor Kocur",
      "Zuzana Berger Haladova",
      "Qianliang Wu",
      "Shen Cai",
      "Jian Yang",
      "Zuzana Kukelova"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_NoPain_No-box_Point_Cloud_Attack_via_Optimal_Transport_Singular_Boundary_CVPR_2025_paper.html": {
    "title": "NoPain: No-box Point Cloud Attack via Optimal Transport Singular Boundary",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zezeng Li",
      "Xiaoyu Du",
      "Na Lei",
      "Liming Chen",
      "Weimin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hao_RAP_Retrieval-Augmented_Personalization_for_Multimodal_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "RAP: Retrieval-Augmented Personalization for Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Hao",
      "Jiaming Han",
      "Changsheng Li",
      "Yu-Feng Li",
      "Xiangyu Yue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhong_FADA_Fast_Diffusion_Avatar_Synthesis_with_Mixed-Supervised_Multi-CFG_Distillation_CVPR_2025_paper.html": {
    "title": "FADA: Fast Diffusion Avatar Synthesis with Mixed-Supervised Multi-CFG Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyun Zhong",
      "Chao Liang",
      "Jianwen Jiang",
      "Gaojie Lin",
      "Jiaqi Yang",
      "Zhou Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_CAT4D_Create_Anything_in_4D_with_Multi-View_Video_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "CAT4D: Create Anything in 4D with Multi-View Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rundi Wu",
      "Ruiqi Gao",
      "Ben Poole",
      "Alex Trevithick",
      "Changxi Zheng",
      "Jonathan T. Barron",
      "Aleksander Holynski"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Exploring_Semantic_Feature_Discrimination_for_Perceptual_Image_Super-Resolution_and_Opinion-Unaware_CVPR_2025_paper.html": {
    "title": "Exploring Semantic Feature Discrimination for Perceptual Image Super-Resolution and Opinion-Unaware No-Reference Image Quality Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanglu Dong",
      "Xiangyu Liao",
      "Mingyang Li",
      "Guihuan Guo",
      "Chao Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Distilling_Long-tailed_Datasets_CVPR_2025_paper.html": {
    "title": "Distilling Long-tailed Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenghao Zhao",
      "Haoxuan Wang",
      "Yuzhang Shang",
      "Kai Wang",
      "Yan Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ryan_Gaze-LLE_Gaze_Target_Estimation_via_Large-Scale_Learned_Encoders_CVPR_2025_paper.html": {
    "title": "Gaze-LLE: Gaze Target Estimation via Large-Scale Learned Encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fiona Ryan",
      "Ajay Bati",
      "Sangmin Lee",
      "Daniel Bolya",
      "Judy Hoffman",
      "James M. Rehg"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Distilling_Spectral_Graph_for_Object-Context_Aware_Open-Vocabulary_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Distilling Spectral Graph for Object-Context Aware Open-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chanyoung Kim",
      "Dayun Ju",
      "Woojung Han",
      "Ming-Hsuan Yang",
      "Seong Jae Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cui_Incorporating_Dense_Knowledge_Alignment_into_Unified_Multimodal_Representation_Models_CVPR_2025_paper.html": {
    "title": "Incorporating Dense Knowledge Alignment into Unified Multimodal Representation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Cui",
      "Xinxing Zu",
      "Wenhua Zhang",
      "Zhongzhou Zhao",
      "Jinyang Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Geometry_Field_Splatting_with_Gaussian_Surfels_CVPR_2025_paper.html": {
    "title": "Geometry Field Splatting with Gaussian Surfels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiwen Jiang",
      "Venkataram Sivaram",
      "Cheng Peng",
      "Ravi Ramamoorthi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jin_Stereo4D_Learning_How_Things_Move_in_3D_from_Internet_Stereo_CVPR_2025_paper.html": {
    "title": "Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linyi Jin",
      "Richard Tucker",
      "Zhengqi Li",
      "David Fouhey",
      "Noah Snavely",
      "Aleksander Holynski"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kitazawa_PS-EIP_Robust_Photometric_Stereo_Based_on_Event_Interval_Profile_CVPR_2025_paper.html": {
    "title": "PS-EIP: Robust Photometric Stereo Based on Event Interval Profile",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kazuma Kitazawa",
      "Takahito Aoto",
      "Satoshi Ikehata",
      "Tsuyoshi Takatani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_GenPC_Zero-shot_Point_Cloud_Completion_via_3D_Generative_Priors_CVPR_2025_paper.html": {
    "title": "GenPC: Zero-shot Point Cloud Completion via 3D Generative Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "An Li",
      "Zhe Zhu",
      "Mingqiang Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_FoundHand_Large-Scale_Domain-Specific_Learning_for_Controllable_Hand_Image_Generation_CVPR_2025_paper.html": {
    "title": "FoundHand: Large-Scale Domain-Specific Learning for Controllable Hand Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kefan Chen",
      "Chaerin Min",
      "Linguang Zhang",
      "Shreyas Hampali",
      "Cem Keskin",
      "Srinath Sridhar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Akkerman_InterDyn_Controllable_Interactive_Dynamics_with_Video_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "InterDyn: Controllable Interactive Dynamics with Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rick Akkerman",
      "Haiwen Feng",
      "Michael J. Black",
      "Dimitrios Tzionas",
      "Victoria Fern√°ndez Abrevaya"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of_CVPR_2025_paper.html": {
    "title": "LLMDet: Learning Strong Open-Vocabulary Object Detectors under the Supervision of Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenghao Fu",
      "Qize Yang",
      "Qijie Mo",
      "Junkai Yan",
      "Xihan Wei",
      "Jingke Meng",
      "Xiaohua Xie",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Na_Boost_Your_Human_Image_Generation_Model_via_Direct_Preference_Optimization_CVPR_2025_paper.html": {
    "title": "Boost Your Human Image Generation Model via Direct Preference Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sanghyeon Na",
      "Yonggyu Kim",
      "Hyunjoon Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Learning_to_Highlight_Audio_by_Watching_Movies_CVPR_2025_paper.html": {
    "title": "Learning to Highlight Audio by Watching Movies",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chao Huang",
      "Ruohan Gao",
      "J. M. F. Tsang",
      "Jan Kurcius",
      "Cagdas Bilen",
      "Chenliang Xu",
      "Anurag Kumar",
      "Sanjeel Parekh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Capellera_Unified_Uncertainty-Aware_Diffusion_for_Multi-Agent_Trajectory_Modeling_CVPR_2025_paper.html": {
    "title": "Unified Uncertainty-Aware Diffusion for Multi-Agent Trajectory Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guillem Capellera",
      "Antonio Rubio",
      "Luis Ferraz",
      "Antonio Agudo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_WeGen_A_Unified_Model_for_Interactive_Multimodal_Generation_as_We_CVPR_2025_paper.html": {
    "title": "WeGen: A Unified Model for Interactive Multimodal Generation as We Chat",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhipeng Huang",
      "Shaobin Zhuang",
      "Canmiao Fu",
      "Binxin Yang",
      "Ying Zhang",
      "Chong Sun",
      "Zhizheng Zhang",
      "Yali Wang",
      "Chen Li",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_HRAvatar_High-Quality_and_Relightable_Gaussian_Head_Avatar_CVPR_2025_paper.html": {
    "title": "HRAvatar: High-Quality and Relightable Gaussian Head Avatar",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongbin Zhang",
      "Yunfei Liu",
      "Lijian Lin",
      "Ye Zhu",
      "Kangjie Chen",
      "Minghan Qin",
      "Yu Li",
      "Haoqian Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yeganeh_Latent_Drifting_in_Diffusion_Models_for_Counterfactual_Medical_Image_Synthesis_CVPR_2025_paper.html": {
    "title": "Latent Drifting in Diffusion Models for Counterfactual Medical Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yousef Yeganeh",
      "Azade Farshad",
      "Ioannis Charisiadis",
      "Marta Hasny",
      "Martin Hartenberger",
      "Bj√∂rn Ommer",
      "Nassir Navab",
      "Ehsan Adeli"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_Rethinking_Spiking_Self-Attention_Mechanism_Implementing_a-XNOR_Similarity_Calculation_in_Spiking_CVPR_2025_paper.html": {
    "title": "Rethinking Spiking Self-Attention Mechanism: Implementing a-XNOR Similarity Calculation in Spiking Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichen Xiao",
      "Shuai Wang",
      "Dehao Zhang",
      "Wenjie Wei",
      "Yimeng Shan",
      "Xiaoli Liu",
      "Yulin Jiang",
      "Malu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MagicQuill_An_Intelligent_Interactive_Image_Editing_System_CVPR_2025_paper.html": {
    "title": "MagicQuill: An Intelligent Interactive Image Editing System",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichen Liu",
      "Yue Yu",
      "Hao Ouyang",
      "Qiuyu Wang",
      "Ka Leong Cheng",
      "Wen Wang",
      "Zhiheng Liu",
      "Qifeng Chen",
      "Yujun Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_HeMoRa_Unsupervised_Heuristic_Consensus_Sampling_for_Robust_Point_Cloud_Registration_CVPR_2025_paper.html": {
    "title": "HeMoRa: Unsupervised Heuristic Consensus Sampling for Robust Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaocheng Yan",
      "Yiming Wang",
      "Kaiyan Zhao",
      "Pengcheng Shi",
      "Zhenjun Zhao",
      "Yongjun Zhang",
      "Jiayuan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Reducing_Class-wise_Confusion_for_Incremental_Learning_with_Disentangled_Manifolds_CVPR_2025_paper.html": {
    "title": "Reducing Class-wise Confusion for Incremental Learning with Disentangled Manifolds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huitong Chen",
      "Yu Wang",
      "Yan Fan",
      "Guosong Jiang",
      "Qinghua Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Open-Vocabulary_Functional_3D_Scene_Graphs_for_Real-World_Indoor_Spaces_CVPR_2025_paper.html": {
    "title": "Open-Vocabulary Functional 3D Scene Graphs for Real-World Indoor Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenyangguang Zhang",
      "Alexandros Delitzas",
      "Fangjinhua Wang",
      "Ruida Zhang",
      "Xiangyang Ji",
      "Marc Pollefeys",
      "Francis Engelmann"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Boosting_Adversarial_Transferability_through_Augmentation_in_Hypothesis_Space_CVPR_2025_paper.html": {
    "title": "Boosting Adversarial Transferability through Augmentation in Hypothesis Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Guo",
      "Weiquan Liu",
      "Qingshan Xu",
      "Shijun Zheng",
      "Shujun Huang",
      "Yu Zang",
      "Siqi Shen",
      "Chenglu Wen",
      "Cheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_AniMo_Species-Aware_Model_for_Text-Driven_Animal_Motion_Generation_CVPR_2025_paper.html": {
    "title": "AniMo: Species-Aware Model for Text-Driven Animal Motion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Wang",
      "Kai Ruan",
      "Xing Zhang",
      "Gaoang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mu_EditAR_Unified_Conditional_Generation_with_Autoregressive_Models_CVPR_2025_paper.html": {
    "title": "EditAR: Unified Conditional Generation with Autoregressive Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiteng Mu",
      "Nuno Vasconcelos",
      "Xiaolong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Matsuo_Instance-wise_Supervision-level_Optimization_in_Active_Learning_CVPR_2025_paper.html": {
    "title": "Instance-wise Supervision-level Optimization in Active Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shinnosuke Matsuo",
      "Riku Togashi",
      "Ryoma Bise",
      "Seiichi Uchida",
      "Masahiro Nomura"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Djeghim_ViiNeuS_Volumetric_Initialization_for_Implicit_Neural_Surface_Reconstruction_of_Urban_CVPR_2025_paper.html": {
    "title": "ViiNeuS: Volumetric Initialization for Implicit Neural Surface Reconstruction of Urban Scenes with Limited Image Overlap",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hala Djeghim",
      "Nathan Piasco",
      "Moussab Bennehar",
      "Luis Roldao",
      "Dzmitry Tsishkou",
      "D√©sir√© Sidib√©"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Model_Diagnosis_and_Correction_via_Linguistic_and_Implicit_Attribute_Editing_CVPR_2025_paper.html": {
    "title": "Model Diagnosis and Correction via Linguistic and Implicit Attribute Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanbai Chen",
      "Xiang Xu",
      "Zhihua Li",
      "Tianchen Zhao",
      "Pietro Perona",
      "Qin Zhang",
      "Yifan Xing"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_BHViT_Binarized_Hybrid_Vision_Transformer_CVPR_2025_paper.html": {
    "title": "BHViT: Binarized Hybrid Vision Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tian Gao",
      "Yu Zhang",
      "Zhiyuan Zhang",
      "Huajun Liu",
      "Kaijie Yin",
      "Chengzhong Xu",
      "Hui Kong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mittal_UniPhy_Learning_a_Unified_Constitutive_Model_for_Inverse_Physics_Simulation_CVPR_2025_paper.html": {
    "title": "UniPhy: Learning a Unified Constitutive Model for Inverse Physics Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Himangi Mittal",
      "Peiye Zhuang",
      "Hsin-Ying Lee",
      "Shubham Tulsiani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_STAA-SNN_Spatial-Temporal_Attention_Aggregator_for_Spiking_Neural_Networks_CVPR_2025_paper.html": {
    "title": "STAA-SNN: Spatial-Temporal Attention Aggregator for Spiking Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianqing Zhang",
      "Kairong Yu",
      "Xian Zhong",
      "Hongwei Wang",
      "Qi Xu",
      "Qiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rotstein_Pathways_on_the_Image_Manifold_Image_Editing_via_Video_Generation_CVPR_2025_paper.html": {
    "title": "Pathways on the Image Manifold: Image Editing via Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Noam Rotstein",
      "Gal Yona",
      "Daniel Silver",
      "Roy Velich",
      "David Bensaid",
      "Ron Kimmel"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DeSplat_Decomposed_Gaussian_Splatting_for_Distractor-Free_Rendering_CVPR_2025_paper.html": {
    "title": "DeSplat: Decomposed Gaussian Splatting for Distractor-Free Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihao Wang",
      "Marcus Klasson",
      "Matias Turkulainen",
      "Shuzhe Wang",
      "Juho Kannala",
      "Arno Solin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_Knowledge_Memorization_and_Rumination_for_Pre-trained_Model-based_Class-Incremental_Learning_CVPR_2025_paper.html": {
    "title": "Knowledge Memorization and Rumination for Pre-trained Model-based Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijian Gao",
      "Wangwang Jia",
      "Xingxing Zhang",
      "Dulan Zhou",
      "Kele Xu",
      "Feng Dawei",
      "Yong Dou",
      "Xinjun Mao",
      "Huaimin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Videnovic_A_Distractor-Aware_Memory_for_Visual_Object_Tracking_with_SAM2_CVPR_2025_paper.html": {
    "title": "A Distractor-Aware Memory for Visual Object Tracking with SAM2",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jovana Videnovic",
      "Alan Lukezic",
      "Matej Kristan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_Activating_Sparse_Part_Concepts_for_3D_Class_Incremental_Learning_CVPR_2025_paper.html": {
    "title": "Activating Sparse Part Concepts for 3D Class Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenya Tian",
      "Jun Xiao",
      "Lupeng Liu",
      "Haiyong Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peng_ProxyTransformation_Preshaping_Point_Cloud_Manifold_With_Proxy_Attention_For_3D_CVPR_2025_paper.html": {
    "title": "ProxyTransformation: Preshaping Point Cloud Manifold With Proxy Attention For 3D Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qihang Peng",
      "Henry Zheng",
      "Gao Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_BFANet_Revisiting_3D_Semantic_Segmentation_with_Boundary_Feature_Analysis_CVPR_2025_paper.html": {
    "title": "BFANet: Revisiting 3D Semantic Segmentation with Boundary Feature Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weiguang Zhao",
      "Rui Zhang",
      "Qiufeng Wang",
      "Guangliang Cheng",
      "Kaizhu Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Avrahami_Stable_Flow_Vital_Layers_for_Training-Free_Image_Editing_CVPR_2025_paper.html": {
    "title": "Stable Flow: Vital Layers for Training-Free Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Omri Avrahami",
      "Or Patashnik",
      "Ohad Fried",
      "Egor Nemchinov",
      "Kfir Aberman",
      "Dani Lischinski",
      "Daniel Cohen-Or"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Reddy_Video-ColBERT_Contextualized_Late_Interaction_for_Text-to-Video_Retrieval_CVPR_2025_paper.html": {
    "title": "Video-ColBERT: Contextualized Late Interaction for Text-to-Video Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Arun Reddy",
      "Alexander Martin",
      "Eugene Yang",
      "Andrew Yates",
      "Kate Sanders",
      "Kenton Murray",
      "Reno Kriz",
      "Celso M. de Melo",
      "Benjamin Van Durme",
      "Rama Chellappa"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_Beyond_Words_Augmenting_Discriminative_Richness_via_Diffusions_in_Unsupervised_Prompt_CVPR_2025_paper.html": {
    "title": "Beyond Words: Augmenting Discriminative Richness via Diffusions in Unsupervised Prompt Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hairui Ren",
      "Fan Tang",
      "He Zhao",
      "Zixuan Wang",
      "Dandan Guo",
      "Yi Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Unlocking_the_Potential_of_Unlabeled_Data_in_Semi-Supervised_Domain_Generalization_CVPR_2025_paper.html": {
    "title": "Unlocking the Potential of Unlabeled Data in Semi-Supervised Domain Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dongkwan Lee",
      "Kyomin Hwang",
      "Nojun Kwak"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_TokenMotion_Decoupled_Motion_Control_via_Token_Disentanglement_for_Human-centric_Video_CVPR_2025_paper.html": {
    "title": "TokenMotion: Decoupled Motion Control via Token Disentanglement for Human-centric Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruineng Li",
      "Daitao Xing",
      "Huiming Sun",
      "Yuanzhou Ha",
      "Jinglin Shen",
      "Chiuman Ho"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nwoye_CholecTrack20_A_Multi-Perspective_Tracking_Dataset_for_Surgical_Tools_CVPR_2025_paper.html": {
    "title": "CholecTrack20: A Multi-Perspective Tracking Dataset for Surgical Tools",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chinedu Innocent Nwoye",
      "Kareem Elgohary",
      "Anvita Srinivas",
      "Fauzan Zaid",
      "Jo√´l L. Lavanchy",
      "Nicolas Padoy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Visual_and_Semantic_Prompt_Collaboration_for_Generalized_Zero-Shot_Learning_CVPR_2025_paper.html": {
    "title": "Visual and Semantic Prompt Collaboration for Generalized Zero-Shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huajie Jiang",
      "Zhengxian Li",
      "Xiaohan Yu",
      "Yongli Hu",
      "Baocai Yin",
      "Jian Yang",
      "Yuankai Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Steering_Away_from_Harm_An_Adaptive_Approach_to_Defending_Vision_CVPR_2025_paper.html": {
    "title": "Steering Away from Harm: An Adaptive Approach to Defending Vision Language Model Against Jailbreaks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Wang",
      "Gang Wang",
      "Huan Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_Neural_LightRig_Unlocking_Accurate_Object_Normal_and_Material_Estimation_with_CVPR_2025_paper.html": {
    "title": "Neural LightRig: Unlocking Accurate Object Normal and Material Estimation with Multi-Light Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zexin He",
      "Tengfei Wang",
      "Xin Huang",
      "Xingang Pan",
      "Ziwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_VidMuse_A_Simple_Video-to-Music_Generation_Framework_with_Long-Short-Term_Modeling_CVPR_2025_paper.html": {
    "title": "VidMuse: A Simple Video-to-Music Generation Framework with Long-Short-Term Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyue Tian",
      "Zhaoyang Liu",
      "Ruibin Yuan",
      "Jiahao Pan",
      "Qifeng Liu",
      "Xu Tan",
      "Qifeng Chen",
      "Wei Xue",
      "Yike Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qin_Human-centered_Interactive_Learning_via_MLLMs_for_Text-to-Image_Person_Re-identification_CVPR_2025_paper.html": {
    "title": "Human-centered Interactive Learning via MLLMs for Text-to-Image Person Re-identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Qin",
      "Chao Chen",
      "Zhihang Fu",
      "Dezhong Peng",
      "Xi Peng",
      "Peng Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cohen_Conditional_Balance_Improving_Multi-Conditioning_Trade-Offs_in_Image_Generation_CVPR_2025_paper.html": {
    "title": "Conditional Balance: Improving Multi-Conditioning Trade-Offs in Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nadav Z. Cohen",
      "Oron Nir",
      "Ariel Shamir"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bigata_KeyFace_Expressive_Audio-Driven_Facial_Animation_for_Long_Sequences_via_KeyFrame_CVPR_2025_paper.html": {
    "title": "KeyFace: Expressive Audio-Driven Facial Animation for Long Sequences via KeyFrame Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoni Bigata",
      "Micha≈Ç Stypu≈Çkowski",
      "Rodrigo Mira",
      "Stella Bounareli",
      "Konstantinos Vougioukas",
      "Zoe Landgraf",
      "Nikita Drobyshev",
      "Maciej Zieba",
      "Stavros Petridis",
      "Maja Pantic"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pang_Context-Enhanced_Memory-Refined_Transformer_for_Online_Action_Detection_CVPR_2025_paper.html": {
    "title": "Context-Enhanced Memory-Refined Transformer for Online Action Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhanzhong Pang",
      "Fadime Sener",
      "Angela Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Towards_Natural_Language-Based_Document_Image_Retrieval_New_Dataset_and_Benchmark_CVPR_2025_paper.html": {
    "title": "Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Guo",
      "Xugong Qin",
      "Jun Jie Ou Yang",
      "Peng Zhang",
      "Gangyan Zeng",
      "Yubo Li",
      "Hailun Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Mitigating_Ambiguities_in_3D_Classification_with_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "Mitigating Ambiguities in 3D Classification with Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiqi Zhang",
      "Hao Zhu",
      "Jingyi Zhao",
      "Qi Zhang",
      "Xun Cao",
      "Zhan Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jung_Exposure-slot_Exposure-centric_Representations_Learning_with_Slot-in-Slot_Attention_for_Region-aware_Exposure_CVPR_2025_paper.html": {
    "title": "Exposure-slot: Exposure-centric Representations Learning with Slot-in-Slot Attention for Region-aware Exposure Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donggoo Jung",
      "Daehyun Kim",
      "Guanghui Wang",
      "Tae Hyun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pan_Data-Free_Group-Wise_Fully_Quantized_Winograd_Convolution_via_Learnable_Scales_CVPR_2025_paper.html": {
    "title": "Data-Free Group-Wise Fully Quantized Winograd Convolution via Learnable Scales",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuokai Pan",
      "Gerti Tuzi",
      "Sudarshan Sreeram",
      "Dibakar Gope"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_EdgeDiff_Edge-aware_Diffusion_Network_for_Building_Reconstruction_from_Point_Clouds_CVPR_2025_paper.html": {
    "title": "EdgeDiff: Edge-aware Diffusion Network for Building Reconstruction from Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujun Liu",
      "Ruisheng Wang",
      "Shangfeng Huang",
      "Guorong Cai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_GEN3C_3D-Informed_World-Consistent_Video_Generation_with_Precise_Camera_Control_CVPR_2025_paper.html": {
    "title": "GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanchi Ren",
      "Tianchang Shen",
      "Jiahui Huang",
      "Huan Ling",
      "Yifan Lu",
      "Merlin Nimier-David",
      "Thomas M√ºller",
      "Alexander Keller",
      "Sanja Fidler",
      "Jun Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Laskar_A_Dataset_for_Semantic_Segmentation_in_the_Presence_of_Unknowns_CVPR_2025_paper.html": {
    "title": "A Dataset for Semantic Segmentation in the Presence of Unknowns",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zakaria Laskar",
      "Tomas Vojir",
      "Matej Grcic",
      "Iaroslav Melekhov",
      "Shankar Gangisetty",
      "Juho Kannala",
      "Jiri Matas",
      "Giorgos Tolias",
      "C.V. Jawahar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Azad_HierarQ_Task-Aware_Hierarchical_Q-Former_for_Enhanced_Video_Understanding_CVPR_2025_paper.html": {
    "title": "HierarQ: Task-Aware Hierarchical Q-Former for Enhanced Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shehreen Azad",
      "Vibhav Vineet",
      "Yogesh Singh Rawat"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_DeNVeR_Deformable_Neural_Vessel_Representations_for_Unsupervised_Video_Vessel_Segmentation_CVPR_2025_paper.html": {
    "title": "DeNVeR: Deformable Neural Vessel Representations for Unsupervised Video Vessel Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chun-Hung Wu",
      "Shih-Hong Chen",
      "Chih-Yao Hu",
      "Hsin-Yu Wu",
      "Kai-Hsin Chen",
      "Yu-You Chen",
      "Chih-Hai Su",
      "Chih-Kuo Lee",
      "Yu-Lun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_DH-Set_Improving_Vision-Language_Alignment_with_Diverse_and_Hybrid_Set-Embeddings_Learning_CVPR_2025_paper.html": {
    "title": "DH-Set: Improving Vision-Language Alignment with Diverse and Hybrid Set-Embeddings Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kun Zhang",
      "Jingyu Li",
      "Zhe Li",
      "S.Kevin Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hao_Task-Aware_Clustering_for_Prompting_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "Task-Aware Clustering for Prompting Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fusheng Hao",
      "Fengxiang He",
      "Fuxiang Wu",
      "Tichao Wang",
      "Chengqun Song",
      "Jun Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Georg_FSboard_Over_3_Million_Characters_of_ASL_Fingerspelling_Collected_via_CVPR_2025_paper.html": {
    "title": "FSboard: Over 3 Million Characters of ASL Fingerspelling Collected via Smartphones",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Manfred Georg",
      "Garrett Tanzer",
      "Esha Uboweja",
      "Saad Hassan",
      "Maximus Shengelia",
      "Sam Sepah",
      "Sean Forbes",
      "Thad Starner"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gholami_CASP_Compression_of_Large_Multimodal_Models_Based_on_Attention_Sparsity_CVPR_2025_paper.html": {
    "title": "CASP: Compression of Large Multimodal Models Based on Attention Sparsity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohsen Gholami",
      "Mohammad Akbari",
      "Kevin Cannons",
      "Yong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Duan_UNIC-Adapter_Unified_Image-instruction_Adapter_with_Multi-modal_Transformer_for_Image_Generation_CVPR_2025_paper.html": {
    "title": "UNIC-Adapter: Unified Image-instruction Adapter with Multi-modal Transformer for Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lunhao Duan",
      "Shanshan Zhao",
      "Wenjun Yan",
      "Yinglun Li",
      "Qing-Guo Chen",
      "Zhao Xu",
      "Weihua Luo",
      "Kaifu Zhang",
      "Mingming Gong",
      "Gui-Song Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_Towards_Cost-Effective_Learning_A_Synergy_of_Semi-Supervised_and_Active_Learning_CVPR_2025_paper.html": {
    "title": "Towards Cost-Effective Learning: A Synergy of Semi-Supervised and Active Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianxiang Yin",
      "Ningzhong Liu",
      "Han Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Unveil_Inversion_and_Invariance_in_Flow_Transformer_for_Versatile_Image_CVPR_2025_paper.html": {
    "title": "Unveil Inversion and Invariance in Flow Transformer for Versatile Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pengcheng Xu",
      "Boyuan Jiang",
      "Xiaobin Hu",
      "Donghao Luo",
      "Qingdong He",
      "Jiangning Zhang",
      "Chengjie Wang",
      "Yunsheng Wu",
      "Charles Ling",
      "Boyu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Leonard_Light_Transport-aware_Diffusion_Posterior_Sampling_for_Single-View_Reconstruction_of_3D_CVPR_2025_paper.html": {
    "title": "Light Transport-aware Diffusion Posterior Sampling for Single-View Reconstruction of 3D Volumes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ludwic Leonard",
      "Nils Thurey",
      "R√ºdiger Westermann"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Assefa_DyCON_Dynamic_Uncertainty-aware_Consistency_and_Contrastive_Learning_for_Semi-supervised_Medical_CVPR_2025_paper.html": {
    "title": "DyCON: Dynamic Uncertainty-aware Consistency and Contrastive Learning for Semi-supervised Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maregu Assefa",
      "Muzammal Naseer",
      "Iyyakutti Iyappan Ganapathi",
      "Syed Sadaf Ali",
      "Mohamed L Seghier",
      "Naoufel Werghi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Du_STiL_Semi-supervised_Tabular-Image_Learning_for_Comprehensive_Task-Relevant_Information_Exploration_in_CVPR_2025_paper.html": {
    "title": "STiL: Semi-supervised Tabular-Image Learning for Comprehensive Task-Relevant Information Exploration in Multimodal Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siyi Du",
      "Xinzhe Luo",
      "Declan P. O'Regan",
      "Chen Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sariyildiz_DUNE_Distilling_a_Universal_Encoder_from_Heterogeneous_2D_and_3D_CVPR_2025_paper.html": {
    "title": "DUNE: Distilling a Universal Encoder from Heterogeneous 2D and 3D Teachers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mert B√ºlent Sarƒ±yƒ±ldƒ±z",
      "Philippe Weinzaepfel",
      "Thomas Lucas",
      "Pau de Jorge",
      "Diane Larlus",
      "Yannis Kalantidis"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shaheryar_Black_Hole-Driven_Identity_Absorbing_in_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Black Hole-Driven Identity Absorbing in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muhammad Shaheryar",
      "Jong Taek Lee",
      "Soon Ki Jung"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_HiRes-LLaVA_Restoring_Fragmentation_Input_in_High-Resolution_Large_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "HiRes-LLaVA: Restoring Fragmentation Input in High-Resolution Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runhui Huang",
      "Xinpeng Ding",
      "Chunwei Wang",
      "Jianhua Han",
      "Yulong Liu",
      "Hengshuang Zhao",
      "Hang Xu",
      "Lu Hou",
      "Wei Zhang",
      "Xiaodan Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cui_Hallo3_Highly_Dynamic_and_Realistic_Portrait_Image_Animation_with_Video_CVPR_2025_paper.html": {
    "title": "Hallo3: Highly Dynamic and Realistic Portrait Image Animation with Video Diffusion Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Cui",
      "Hui Li",
      "Yun Zhan",
      "Hanlin Shang",
      "Kaihui Cheng",
      "Yuqi Ma",
      "Shan Mu",
      "Hang Zhou",
      "Jingdong Wang",
      "Siyu Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yuan_Generative_Photography_Scene-Consistent_Camera_Control_for_Realistic_Text-to-Image_Synthesis_CVPR_2025_paper.html": {
    "title": "Generative Photography: Scene-Consistent Camera Control for Realistic Text-to-Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Yuan",
      "Xijun Wang",
      "Yichen Sheng",
      "Prateek Chennuri",
      "Xingguang Zhang",
      "Stanley Chan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Advancing_Manga_Analysis_Comprehensive_Segmentation_Annotations_for_the_Manga109_Dataset_CVPR_2025_paper.html": {
    "title": "Advancing Manga Analysis: Comprehensive Segmentation Annotations for the Manga109 Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minshan Xie",
      "Jian Lin",
      "Hanyuan Liu",
      "Chengze Li",
      "Tien-Tsin Wong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SeqMvRL_A_Sequential_Fusion_Framework_for_Multi-view_Representation_Learning_CVPR_2025_paper.html": {
    "title": "SeqMvRL: A Sequential Fusion Framework for Multi-view Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ren Wang",
      "Haoliang Sun",
      "Yuxiu Lin",
      "Chuanhui Zuo",
      "Yongshun Gong",
      "Yilong Yin",
      "Wenjia Meng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Auto_Cherry-Picker_Learning_from_High-quality_Generative_Data_Driven_by_Language_CVPR_2025_paper.html": {
    "title": "Auto Cherry-Picker: Learning from High-quality Generative Data Driven by Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yicheng Chen",
      "Xiangtai Li",
      "Yining Li",
      "Yanhong Zeng",
      "Jianzong Wu",
      "Xiangyu Zhao",
      "Kai Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_EnvGS_Modeling_View-Dependent_Appearance_with_Environment_Gaussian_CVPR_2025_paper.html": {
    "title": "EnvGS: Modeling View-Dependent Appearance with Environment Gaussian",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tao Xie",
      "Xi Chen",
      "Zhen Xu",
      "Yiman Xie",
      "Yudong Jin",
      "Yujun Shen",
      "Sida Peng",
      "Hujun Bao",
      "Xiaowei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Provoking_Multi-modal_Few-Shot_LVLM_via_Exploration-Exploitation_In-Context_Learning_CVPR_2025_paper.html": {
    "title": "Provoking Multi-modal Few-Shot LVLM via Exploration-Exploitation In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Chen",
      "Yunpeng Zhai",
      "Yifan Zhao",
      "Jinyang Gao",
      "Bolin Ding",
      "Jia Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yuan_BadToken_Token-level_Backdoor_Attacks_to_Multi-modal_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "BadToken: Token-level Backdoor Attacks to Multi-modal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zenghui Yuan",
      "Jiawen Shi",
      "Pan Zhou",
      "Neil Zhenqiang Gong",
      "Lichao Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Berdan_ReRAW_RGB-to-RAW_Image_Reconstruction_via_Stratified_Sampling_for_Efficient_Object_CVPR_2025_paper.html": {
    "title": "ReRAW: RGB-to-RAW Image Reconstruction via Stratified Sampling for Efficient Object Detection on the Edge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Radu Berdan",
      "Beril Besbinar",
      "Christoph Reinders",
      "Junji Otsuka",
      "Daisuke Iso"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_VLMs-Guided_Representation_Distillation_for_Efficient_Vision-Based_Reinforcement_Learning_CVPR_2025_paper.html": {
    "title": "VLMs-Guided Representation Distillation for Efficient Vision-Based Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran Xu",
      "Peixi Peng",
      "Guang Tan",
      "Yiqian Chang",
      "Luntong Li",
      "Yonghong Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pu_MonoDGP_Monocular_3D_Object_Detection_with_Decoupled-Query_and_Geometry-Error_Priors_CVPR_2025_paper.html": {
    "title": "MonoDGP: Monocular 3D Object Detection with Decoupled-Query and Geometry-Error Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fanqi Pu",
      "Yifan Wang",
      "Jiru Deng",
      "Wenming Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_NeISF_Neural_Incident_Stokes_Field_for_Polarized_Inverse_Rendering_of_CVPR_2025_paper.html": {
    "title": "NeISF++: Neural Incident Stokes Field for Polarized Inverse Rendering of Conductors and Dielectrics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenhao Li",
      "Taishi Ono",
      "Takeshi Uemori",
      "Sho Nitta",
      "Hajime Mihara",
      "Alexander Gatto",
      "Hajime Nagahara",
      "Yusuke Moriuchi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_HunyuanPortrait_Implicit_Condition_Control_for_Enhanced_Portrait_Animation_CVPR_2025_paper.html": {
    "title": "HunyuanPortrait: Implicit Condition Control for Enhanced Portrait Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zunnan Xu",
      "Zhentao Yu",
      "Zixiang Zhou",
      "Jun Zhou",
      "Xiaoyu Jin",
      "Fa-ting Hong",
      "Xiaozhong Ji",
      "Junwei Zhu",
      "Chengfei Cai",
      "Shiyu Tang",
      "Qin Lin",
      "Xiu Li",
      "Qinglin Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Flexible_Group_Count_Enables_Hassle-Free_Structured_Pruning_CVPR_2025_paper.html": {
    "title": "Flexible Group Count Enables Hassle-Free Structured Pruning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiamu Zhang",
      "Shaochen Zhong",
      "Andrew Ye",
      "Zirui Liu",
      "Sebastian Zhao",
      "Kaixiong Zhou",
      "Li Li",
      "Soo-Hyun Choi",
      "Rui Chen",
      "Xia Hu",
      "Shuai Xu",
      "Vipin Chaudhary"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_EasyCraft_A_Robust_and_Efficient_Framework_for_Automatic_Avatar_Crafting_CVPR_2025_paper.html": {
    "title": "EasyCraft: A Robust and Efficient Framework for Automatic Avatar Crafting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suzhen Wang",
      "Weijie Chen",
      "Wei Zhang",
      "Minda Zhao",
      "Lincheng Li",
      "Rongsheng Zhang",
      "Zhipeng Hu",
      "Xin Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_MeshArt_Generating_Articulated_Meshes_with_Structure-Guided_Transformers_CVPR_2025_paper.html": {
    "title": "MeshArt: Generating Articulated Meshes with Structure-Guided Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daoyi Gao",
      "Yawar Siddiqui",
      "Lei Li",
      "Angela Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Non-Natural_Image_Understanding_with_Advancing_Frequency-based_Vision_Encoders_CVPR_2025_paper.html": {
    "title": "Non-Natural Image Understanding with Advancing Frequency-based Vision Encoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wang Lin",
      "QingSong Wang",
      "Yueying Feng",
      "Shulei Wang",
      "Tao Jin",
      "Zhou Zhao",
      "Fei Wu",
      "Chang Yao",
      "Jingyuan Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pan_Generative_Multimodal_Pretraining_with_Discrete_Diffusion_Timestep_Tokens_CVPR_2025_paper.html": {
    "title": "Generative Multimodal Pretraining with Discrete Diffusion Timestep Tokens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaihang Pan",
      "Wang Lin",
      "Zhongqi Yue",
      "Tenglong Ao",
      "Liyu Jia",
      "Wei Zhao",
      "Juncheng Li",
      "Siliang Tang",
      "Hanwang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_SplatFlow_Self-Supervised_Dynamic_Gaussian_Splatting_in_Neural_Motion_Flow_Field_CVPR_2025_paper.html": {
    "title": "SplatFlow: Self-Supervised Dynamic Gaussian Splatting in Neural Motion Flow Field for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Su Sun",
      "Cheng Zhao",
      "Zhuoyang Sun",
      "Yingjie Victor Chen",
      "Mei Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Zero-shot_3D_Question_Answering_via_Voxel-based_Dynamic_Token_Compression_CVPR_2025_paper.html": {
    "title": "Zero-shot 3D Question Answering via Voxel-based Dynamic Token Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hsiang-Wei Huang",
      "Fu-Chen Chen",
      "Wenhao Chai",
      "Che-Chun Su",
      "Lu Xia",
      "Sanghun Jung",
      "Cheng-Yen Yang",
      "Jenq-Neng Hwang",
      "Min Sun",
      "Cheng-Hao Kuo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Patnaik_AesthetiQ_Enhancing_Graphic_Layout_Design_via_Aesthetic-Aware_Preference_Alignment_of_CVPR_2025_paper.html": {
    "title": "AesthetiQ: Enhancing Graphic Layout Design via Aesthetic-Aware Preference Alignment of Multi-modal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sohan Patnaik",
      "Rishabh Jain",
      "Balaji Krishnamurthy",
      "Mausoom Sarkar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Enhanced_then_Progressive_Fusion_with_View_Graph_for_Multi-View_Clustering_CVPR_2025_paper.html": {
    "title": "Enhanced then Progressive Fusion with View Graph for Multi-View Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhibin Dong",
      "Meng Liu",
      "Siwei Wang",
      "Ke Liang",
      "Yi Zhang",
      "Suyuan Liu",
      "Jiaqi Jin",
      "Xinwang Liu",
      "En Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hua_FINECAPTION_Compositional_Image_Captioning_Focusing_on_Wherever_You_Want_at_CVPR_2025_paper.html": {
    "title": "FINECAPTION: Compositional Image Captioning Focusing on Wherever You Want at Any Granularity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Hua",
      "Qing Liu",
      "Lingzhi Zhang",
      "Jing Shi",
      "Soo Ye Kim",
      "Zhifei Zhang",
      "Yilin Wang",
      "Jianming Zhang",
      "Zhe Lin",
      "Jiebo Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Adaptive_Non-Uniform_Timestep_Sampling_for_Accelerating_Diffusion_Model_Training_CVPR_2025_paper.html": {
    "title": "Adaptive Non-Uniform Timestep Sampling for Accelerating Diffusion Model Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Myunsoo Kim",
      "Donghyeon Ki",
      "Seong-Woong Shim",
      "Byung-Jun Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Evani_Chebyshev_Attention_Depth_Permutation_Texture_Network_with_Latent_Texture_Attribute_CVPR_2025_paper.html": {
    "title": "Chebyshev Attention Depth Permutation Texture Network with Latent Texture Attribute Loss",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ravishankar Evani",
      "Deepu Rajan",
      "Shangbo Mao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Explainable_Saliency_Articulating_Reasoning_with_Contextual_Prioritization_CVPR_2025_paper.html": {
    "title": "Explainable Saliency: Articulating Reasoning with Contextual Prioritization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nuo Chen",
      "Ming Jiang",
      "Qi Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/McAllister_Decentralized_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Decentralized Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David McAllister",
      "Matthew Tancik",
      "Jiaming Song",
      "Angjoo Kanazawa"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_AnyEdit_Mastering_Unified_High-Quality_Image_Editing_for_Any_Idea_CVPR_2025_paper.html": {
    "title": "AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qifan Yu",
      "Wei Chow",
      "Zhongqi Yue",
      "Kaihang Pan",
      "Yang Wu",
      "Xiaoyang Wan",
      "Juncheng Li",
      "Siliang Tang",
      "Hanwang Zhang",
      "Yueting Zhuang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Parihar_Compass_Control_Multi_Object_Orientation_Control_for_Text-to-Image_Generation_CVPR_2025_paper.html": {
    "title": "Compass Control: Multi Object Orientation Control for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishubh Parihar",
      "Vaibhav Agrawal",
      "Sachidanand VS",
      "Venkatesh Babu Radhakrishnan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Beizaee_Correcting_Deviations_from_Normality_A_Reformulated_Diffusion_Model_for_Multi-Class_CVPR_2025_paper.html": {
    "title": "Correcting Deviations from Normality: A Reformulated Diffusion Model for Multi-Class Unsupervised Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Farzad Beizaee",
      "Gregory A. Lodygensky",
      "Christian Desrosiers",
      "Jose Dolz"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Continuous_3D_Perception_Model_with_Persistent_State_CVPR_2025_paper.html": {
    "title": "Continuous 3D Perception Model with Persistent State",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianqian Wang",
      "Yifei Zhang",
      "Aleksander Holynski",
      "Alexei A. Efros",
      "Angjoo Kanazawa"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gong_LP-Diff_Towards_Improved_Restoration_of_Real-World_Degraded_License_Plate_CVPR_2025_paper.html": {
    "title": "LP-Diff: Towards Improved Restoration of Real-World Degraded License Plate",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyan Gong",
      "Zhenrong Zhang",
      "Yuzheng Feng",
      "Anh Nguyen",
      "Hongbin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Unleashing_the_Potential_of_Consistency_Learning_for_Detecting_and_Grounding_CVPR_2025_paper.html": {
    "title": "Unleashing the Potential of Consistency Learning for Detecting and Grounding Multi-Modal Media Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiheng Li",
      "Yang Yang",
      "Zichang Tan",
      "Huan Liu",
      "Weihua Chen",
      "Xu Zhou",
      "Zhen Lei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_DNF_Unconditional_4D_Generation_with_Dictionary-based_Neural_Fields_CVPR_2025_paper.html": {
    "title": "DNF: Unconditional 4D Generation with Dictionary-based Neural Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyi Zhang",
      "Naiqi Li",
      "Angela Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_ARM_Appearance_Reconstruction_Model_for_Relightable_3D_Generation_CVPR_2025_paper.html": {
    "title": "ARM: Appearance Reconstruction Model for Relightable 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Feng",
      "Chang Yu",
      "Zoubin Bi",
      "Yintong Shang",
      "Feng Gao",
      "Hongzhi Wu",
      "Kun Zhou",
      "Chenfanfu Jiang",
      "Yin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Vogel_VideoGEM_Training-free_Action_Grounding_in_Videos_CVPR_2025_paper.html": {
    "title": "VideoGEM: Training-free Action Grounding in Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Felix Vogel",
      "Walid Bousselham",
      "Anna Kukleva",
      "Nina Shvetsova",
      "Hilde Kuehne"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_FilmComposer_LLM-Driven_Music_Production_for_Silent_Film_Clips_CVPR_2025_paper.html": {
    "title": "FilmComposer: LLM-Driven Music Production for Silent Film Clips",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhifeng Xie",
      "Qile He",
      "Youjia Zhu",
      "Qiwei He",
      "Mengtian Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zong_Ground-V_Teaching_VLMs_to_Ground_Complex_Instructions_in_Pixels_CVPR_2025_paper.html": {
    "title": "Ground-V: Teaching VLMs to Ground Complex Instructions in Pixels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongshuo Zong",
      "Qin Zhang",
      "Dongsheng An",
      "Zhihua Li",
      "Xiang Xu",
      "Linghan Xu",
      "Zhuowen Tu",
      "Yifan Xing",
      "Onkar Dabeer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Structure-from-Motion_with_a_Non-Parametric_Camera_Model_CVPR_2025_paper.html": {
    "title": "Structure-from-Motion with a Non-Parametric Camera Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yihan Wang",
      "Linfei Pan",
      "Marc Pollefeys",
      "Viktor Larsson"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_EventPSR_Surface_Normal_and_Reflectance_Estimation_from_Photometric_Stereo_Using_CVPR_2025_paper.html": {
    "title": "EventPSR: Surface Normal and Reflectance Estimation from Photometric Stereo Using an Event Camera",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bohan Yu",
      "Jin Han",
      "Boxin Shi",
      "Imari Sato"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_LAL_Enhancing_3D_Human_Motion_Prediction_with_Latency-aware_Auxiliary_Learning_CVPR_2025_paper.html": {
    "title": "LAL: Enhancing 3D Human Motion Prediction with Latency-aware Auxiliary Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoning Sun",
      "Dong Wei",
      "Huaijiang Sun",
      "Shengxiang Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wan_CASP_Consistency-aware_Audio-induced_Saliency_Prediction_Model_for_Omnidirectional_Video_CVPR_2025_paper.html": {
    "title": "CASP: Consistency-aware Audio-induced Saliency Prediction Model for Omnidirectional Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaolin Wan",
      "Han Qin",
      "Zhiyang Li",
      "Xiaopeng Fan",
      "Wangmeng Zuo",
      "Debin Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lionar_TreeMeshGPT_Artistic_Mesh_Generation_with_Autoregressive_Tree_Sequencing_CVPR_2025_paper.html": {
    "title": "TreeMeshGPT: Artistic Mesh Generation with Autoregressive Tree Sequencing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Lionar",
      "Jiabin Liang",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_RefPose_Leveraging_Reference_Geometric_Correspondences_for_Accurate_6D_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "RefPose: Leveraging Reference Geometric Correspondences for Accurate 6D Pose Estimation of Unseen Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jaeguk Kim",
      "Jaewoo Park",
      "Keuntek Lee",
      "Nam Ik Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Relation3D__Enhancing_Relation_Modeling_for_Point_Cloud_Instance_Segmentation_CVPR_2025_paper.html": {
    "title": "Relation3D : Enhancing Relation Modeling for Point Cloud Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahao Lu",
      "Jiacheng Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liao_Shape_My_Moves_Text-Driven_Shape-Aware_Synthesis_of_Human_Motions_CVPR_2025_paper.html": {
    "title": "Shape My Moves: Text-Driven Shape-Aware Synthesis of Human Motions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ting-Hsuan Liao",
      "Yi Zhou",
      "Yu Shen",
      "Chun-Hao Paul Huang",
      "Saayan Mitra",
      "Jia-Bin Huang",
      "Uttaran Bhattacharya"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chou_Generating_3D-Consistent_Videos_from_Unposed_Internet_Photos_CVPR_2025_paper.html": {
    "title": "Generating 3D-Consistent Videos from Unposed Internet Photos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gene Chou",
      "Kai Zhang",
      "Sai Bi",
      "Hao Tan",
      "Zexiang Xu",
      "Fujun Luan",
      "Bharath Hariharan",
      "Noah Snavely"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Gazing_at_Rewards_Eye_Movements_as_a_Lens_into_Human_CVPR_2025_paper.html": {
    "title": "Gazing at Rewards: Eye Movements as a Lens into Human and AI Decision-Making in Hybrid Visual Foraging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Wang",
      "Dingwei Tan",
      "Yen-Ling Kuo",
      "Zhaowei Sun",
      "Jeremy M. Wolfe",
      "Tat-Jen Cham",
      "Mengmi Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_FOCUS_Knowledge-enhanced_Adaptive_Visual_Compression_for_Few-shot_Whole_Slide_Image_CVPR_2025_paper.html": {
    "title": "FOCUS: Knowledge-enhanced Adaptive Visual Compression for Few-shot Whole Slide Image Classification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengrui Guo",
      "Conghao Xiong",
      "Jiabo Ma",
      "Qichen Sun",
      "Lishuang Feng",
      "Jinzhuo Wang",
      "Hao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Beyond_Human_Perception_Understanding_Multi-Object_World_from_Monocular_View_CVPR_2025_paper.html": {
    "title": "Beyond Human Perception: Understanding Multi-Object World from Monocular View",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keyu Guo",
      "Yongle Huang",
      "Shijie Sun",
      "Xiangyu Song",
      "Mingtao Feng",
      "Zedong Liu",
      "Huansheng Song",
      "Tiantian Wang",
      "Jianxin Li",
      "Naveed Akhtar",
      "Ajmal Saeed Mian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_GRAE-3DMOT_Geometry_Relation-Aware_Encoder_for_Online_3D_Multi-Object_Tracking_CVPR_2025_paper.html": {
    "title": "GRAE-3DMOT: Geometry Relation-Aware Encoder for Online 3D Multi-Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunseop Kim",
      "Hyo-Jun Lee",
      "Yonguk Lee",
      "Jinu Lee",
      "Hanul Kim",
      "Yeong Jun Koh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qu_Automatic_Joint_Structured_Pruning_and_Quantization_for_Efficient_Neural_Network_CVPR_2025_paper.html": {
    "title": "Automatic Joint Structured Pruning and Quantization for Efficient Neural Network Training and Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyi Qu",
      "David Aponte",
      "Colby Banbury",
      "Daniel P. Robinson",
      "Tianyu Ding",
      "Kazuhito Koishida",
      "Ilya Zharkov",
      "Tianyi Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ham_Parameter_Efficient_Mamba_Tuning_via_Projector-targeted_Diagonal-centric_Linear_Transformation_CVPR_2025_paper.html": {
    "title": "Parameter Efficient Mamba Tuning via Projector-targeted Diagonal-centric Linear Transformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seokil Ham",
      "Hee-Seon Kim",
      "Sangmin Woo",
      "Changick Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Panagopoulou_ViUniT_Visual_Unit_Tests_for_More_Robust_Visual_Programming_CVPR_2025_paper.html": {
    "title": "ViUniT: Visual Unit Tests for More Robust Visual Programming",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Artemis Panagopoulou",
      "Honglu Zhou",
      "Silvio Savarese",
      "Caiming Xiong",
      "Chris Callison-Burch",
      "Mark Yatskar",
      "Juan Carlos Niebles"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_LIRM_Large_Inverse_Rendering_Model_for_Progressive_Reconstruction_of_Shape_CVPR_2025_paper.html": {
    "title": "LIRM: Large Inverse Rendering Model for Progressive Reconstruction of Shape, Materials and View-dependent Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengqin Li",
      "Dilin Wang",
      "Ka Chen",
      "Zhaoyang Lv",
      "Thu Nguyen-Phuoc",
      "Milim Lee",
      "Jia-Bin Huang",
      "Lei Xiao",
      "Yufeng Zhu",
      "Carl S. Marshall",
      "Yuheng Ren",
      "Richard Newcombe",
      "Zhao Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peng_DualTalk_Dual-Speaker_Interaction_for_3D_Talking_Head_Conversations_CVPR_2025_paper.html": {
    "title": "DualTalk: Dual-Speaker Interaction for 3D Talking Head Conversations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqiao Peng",
      "Yanbo Fan",
      "Haoyu Wu",
      "Xuan Wang",
      "Hongyan Liu",
      "Jun He",
      "Zhaoxin Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_MAR-3D_Progressive_Masked_Auto-regressor_for_High-Resolution_3D_Generation_CVPR_2025_paper.html": {
    "title": "MAR-3D: Progressive Masked Auto-regressor for High-Resolution 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinnan Chen",
      "Lingting Zhu",
      "Zeyu Hu",
      "Shengju Qian",
      "Yugang Chen",
      "Xin Wang",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_beta-FFT_Nonlinear_Interpolation_and_Differentiated_Training_Strategies_for_Semi-Supervised_Medical_CVPR_2025_paper.html": {
    "title": "beta-FFT: Nonlinear Interpolation and Differentiated Training Strategies for Semi-Supervised Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ming Hu",
      "Jianfu Yin",
      "Zhuangzhuang Ma",
      "Jianheng Ma",
      "Feiyu Zhu",
      "Bingbing Wu",
      "Ya Wen",
      "Meng Wu",
      "Cong Hu",
      "Bingliang Hu",
      "Quan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Smadar_Dynamic_Group_Normalization_Spatio-Temporal_Adaptation_to_Evolving_Data_Statistics_CVPR_2025_paper.html": {
    "title": "Dynamic Group Normalization: Spatio-Temporal Adaptation to Evolving Data Statistics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yair Smadar",
      "Assaf Hoogi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jeong_Latent_Space_Super-Resolution_for_Higher-Resolution_Image_Generation_with_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Latent Space Super-Resolution for Higher-Resolution Image Generation with Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinho Jeong",
      "Sangmin Han",
      "Jinwoo Kim",
      "Seon Joo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_SynerGen-VL_Towards_Synergistic_Image_Understanding_and_Generation_with_Vision_Experts_CVPR_2025_paper.html": {
    "title": "SynerGen-VL: Towards Synergistic Image Understanding and Generation with Vision Experts and Token Folding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Li",
      "Changyao Tian",
      "Jie Shao",
      "Xizhou Zhu",
      "Zhaokai Wang",
      "Jinguo Zhu",
      "Wenhan Dou",
      "Xiaogang Wang",
      "Hongsheng Li",
      "Lewei Lu",
      "Jifeng Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zielonka_Synthetic_Prior_for_Few-Shot_Drivable_Head_Avatar_Inversion_CVPR_2025_paper.html": {
    "title": "Synthetic Prior for Few-Shot Drivable Head Avatar Inversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wojciech Zielonka",
      "Stephan J. Garbin",
      "Alexandros Lattas",
      "George Kopanas",
      "Paulo Gotardo",
      "Thabo Beeler",
      "Justus Thies",
      "Timo Bolkart"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Janny_Reasoning_in_Visual_Navigation_of_End-to-end_Trained_Agents_A_Dynamical_CVPR_2025_paper.html": {
    "title": "Reasoning in Visual Navigation of End-to-end Trained Agents: A Dynamical Systems Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Steeven Janny",
      "Herv√© Poirier",
      "Leonid Antsfeld",
      "Guillaume Bono",
      "Gianluca Monaci",
      "Boris Chidlovskii",
      "Francesco Giuliari",
      "Alessio Del Bue",
      "Christian Wolf"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lai_Rethinking_Noisy_Video-Text_Retrieval_via_Relation-aware_Alignment_CVPR_2025_paper.html": {
    "title": "Rethinking Noisy Video-Text Retrieval via Relation-aware Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huakai Lai",
      "Guoxin Xiong",
      "Huayu Mai",
      "Xiang Liu",
      "Tianzhu Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yin_DFormerv2_Geometry_Self-Attention_for_RGBD_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "DFormerv2: Geometry Self-Attention for RGBD Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo-Wen Yin",
      "Jiao-Long Cao",
      "Ming-Ming Cheng",
      "Qibin Hou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_Scaling_Vision_Pre-Training_to_4K_Resolution_CVPR_2025_paper.html": {
    "title": "Scaling Vision Pre-Training to 4K Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baifeng Shi",
      "Boyi Li",
      "Han Cai",
      "Yao Lu",
      "Sifei Liu",
      "Marco Pavone",
      "Jan Kautz",
      "Song Han",
      "Trevor Darrell",
      "Pavlo Molchanov",
      "Hongxu Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_GarmentPile_Point-Level_Visual_Affordance_Guided_Retrieval_and_Adaptation_for_Cluttered_CVPR_2025_paper.html": {
    "title": "GarmentPile: Point-Level Visual Affordance Guided Retrieval and Adaptation for Cluttered Garments Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruihai Wu",
      "Ziyu Zhu",
      "Yuran Wang",
      "Yue Chen",
      "Jiarui Wang",
      "Hao Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Uncertain_Multimodal_Intention_and_Emotion_Understanding_in_the_Wild_CVPR_2025_paper.html": {
    "title": "Uncertain Multimodal Intention and Emotion Understanding in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qu Yang",
      "Qinghongya Shi",
      "Tongxin Wang",
      "Mang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_GroomLight_Hybrid_Inverse_Rendering_for_Relightable_Human_Hair_Appearance_Modeling_CVPR_2025_paper.html": {
    "title": "GroomLight: Hybrid Inverse Rendering for Relightable Human Hair Appearance Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Zheng",
      "Menglei Chai",
      "Delio Vicini",
      "Yuxiao Zhou",
      "Yinghao Xu",
      "Leonidas Guibas",
      "Gordon Wetzstein",
      "Thabo Beeler"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Improving_Editability_in_Image_Generation_with_Layer-wise_Memory_CVPR_2025_paper.html": {
    "title": "Improving Editability in Image Generation with Layer-wise Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daneul Kim",
      "Jaeah Lee",
      "Jaesik Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Varghese_Sea-ing_in_Low-light_CVPR_2025_paper.html": {
    "title": "Sea-ing in Low-light",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nisha Varghese",
      "A. N. Rajagopalan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_VidTwin_Video_VAE_with_Decoupled_Structure_and_Dynamics_CVPR_2025_paper.html": {
    "title": "VidTwin: Video VAE with Decoupled Structure and Dynamics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchi Wang",
      "Junliang Guo",
      "Xinyi Xie",
      "Tianyu He",
      "Xu Sun",
      "Jiang Bian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_CL-LoRA_Continual_Low-Rank_Adaptation_for_Rehearsal-Free_Class-Incremental_Learning_CVPR_2025_paper.html": {
    "title": "CL-LoRA: Continual Low-Rank Adaptation for Rehearsal-Free Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangpeng He",
      "Zhihao Duan",
      "Fengqing Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shin_Generative_Modeling_of_Class_Probability_for_Multi-Modal_Representation_Learning_CVPR_2025_paper.html": {
    "title": "Generative Modeling of Class Probability for Multi-Modal Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JungKyoo Shin",
      "Bumsoo Kim",
      "Eunwoo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_VisionZip_Longer_is_Better_but_Not_Necessary_in_Vision_Language_CVPR_2025_paper.html": {
    "title": "VisionZip: Longer is Better but Not Necessary in Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Senqiao Yang",
      "Yukang Chen",
      "Zhuotao Tian",
      "Chengyao Wang",
      "Jingyao Li",
      "Bei Yu",
      "Jiaya Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Simplification_Is_All_You_Need_against_Out-of-Distribution_Overconfidence_CVPR_2025_paper.html": {
    "title": "Simplification Is All You Need against Out-of-Distribution Overconfidence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Keke Tang",
      "Chao Hou",
      "Weilong Peng",
      "Xiang Fang",
      "Zhize Wu",
      "Yongwei Nie",
      "Wenping Wang",
      "Zhihong Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lv_SpatialDreamer_Self-supervised_Stereo_Video_Synthesis_from_Monocular_Input_CVPR_2025_paper.html": {
    "title": "SpatialDreamer: Self-supervised Stereo Video Synthesis from Monocular Input",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Lv",
      "Yangqi Long",
      "Congzhentao Huang",
      "Cao Li",
      "Chengfei Lv",
      "Hao Ren",
      "Dian Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_LOD-GS_Achieving_Levels_of_Detail_using_Scalable_Gaussian_Soup_CVPR_2025_paper.html": {
    "title": "LOD-GS: Achieving Levels of Detail using Scalable Gaussian Soup",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianxiong Shen",
      "Yue Qian",
      "Xiaohang Zhan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gu_BlenderGym_Benchmarking_Foundational_Model_Systems_for_Graphics_Editing_CVPR_2025_paper.html": {
    "title": "BlenderGym: Benchmarking Foundational Model Systems for Graphics Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunqi Gu",
      "Ian Huang",
      "Jihyeon Je",
      "Guandao Yang",
      "Leonidas Guibas"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_VoteFlow_Enforcing_Local_Rigidity_in_Self-Supervised_Scene_Flow_CVPR_2025_paper.html": {
    "title": "VoteFlow: Enforcing Local Rigidity in Self-Supervised Scene Flow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yancong Lin",
      "Shiming Wang",
      "Liangliang Nan",
      "Julian Kooij",
      "Holger Caesar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_The_Devil_is_in_Low-Level_Features_for_Cross-Domain_Few-Shot_Segmentation_CVPR_2025_paper.html": {
    "title": "The Devil is in Low-Level Features for Cross-Domain Few-Shot Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Liu",
      "Yixiong Zou",
      "Yuhua Li",
      "Ruixuan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Design2GarmentCode_Turning_Design_Concepts_to_Tangible_Garments_Through_Program_Synthesis_CVPR_2025_paper.html": {
    "title": "Design2GarmentCode: Turning Design Concepts to Tangible Garments Through Program Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feng Zhou",
      "Ruiyang Liu",
      "Chen Liu",
      "Gaofeng He",
      "Yong-Lu Li",
      "Xiaogang Jin",
      "Huamin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Uncertainty_Weighted_Gradients_for_Model_Calibration_CVPR_2025_paper.html": {
    "title": "Uncertainty Weighted Gradients for Model Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinxu Lin",
      "Linwei Tao",
      "Minjing Dong",
      "Chang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kwon_Efficient_Dynamic_Scene_Editing_via_4D_Gaussian-based_Static-Dynamic_Separation_CVPR_2025_paper.html": {
    "title": "Efficient Dynamic Scene Editing via 4D Gaussian-based Static-Dynamic Separation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Joohyun Kwon",
      "Hanbyel Cho",
      "Junmo Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhong_Unlearning_through_Knowledge_Overwriting_Reversible_Federated_Unlearning_via_Selective_Sparse_CVPR_2025_paper.html": {
    "title": "Unlearning through Knowledge Overwriting: Reversible Federated Unlearning via Selective Sparse Adapter",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengyi Zhong",
      "Weidong Bao",
      "Ji Wang",
      "Shuai Zhang",
      "Jingxuan Zhou",
      "Lingjuan Lyu",
      "Wei Yang Bryan Lim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_SocialMOIF_Multi-Order_Intention_Fusion_for_Pedestrian_Trajectory_Prediction_CVPR_2025_paper.html": {
    "title": "SocialMOIF: Multi-Order Intention Fusion for Pedestrian Trajectory Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Chen",
      "Xiaodong Zhao",
      "Yujie Huang",
      "Guoyu Fang",
      "Xiao Song",
      "Ruiping Wang",
      "Ziyuan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yun_FFaceNeRF_Few-shot_Face_Editing_in_Neural_Radiance_Fields_CVPR_2025_paper.html": {
    "title": "FFaceNeRF: Few-shot Face Editing in Neural Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwan Yun",
      "Chaelin Kim",
      "Hangyeul Shin",
      "Junyong Noh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Discrete_to_Continuous_Generating_Smooth_Transition_Poses_from_Sign_Language_CVPR_2025_paper.html": {
    "title": "Discrete to Continuous: Generating Smooth Transition Poses from Sign Language Observations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengeng Tang",
      "Jiayi He",
      "Lechao Cheng",
      "Jingjing Wu",
      "Dan Guo",
      "Richang Hong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Raswa_HistoFS_Non-IID_Histopathologic_Whole_Slide_Image_Classification_via_Federated_Style_CVPR_2025_paper.html": {
    "title": "HistoFS: Non-IID Histopathologic Whole Slide Image Classification via Federated Style Transfer with RoI-Preserving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Farchan Hakim Raswa",
      "Chun-Shien Lu",
      "Jia-Ching Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chang_Unified_Medical_Lesion_Segmentation_via_Self-referring_Indicator_CVPR_2025_paper.html": {
    "title": "Unified Medical Lesion Segmentation via Self-referring Indicator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijie Chang",
      "Xiaoqi Zhao",
      "Lihe Zhang",
      "Tiancheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Galerne_SGSST_Scaling_Gaussian_Splatting_Style_Transfer_CVPR_2025_paper.html": {
    "title": "SGSST: Scaling Gaussian Splatting Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bruno Galerne",
      "Jianling Wang",
      "Lara Raad",
      "Jean-Michel Morel"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Noda_Learning_Bijective_Surface_Parameterization_for_Inferring_Signed_Distance_Functions_from_CVPR_2025_paper.html": {
    "title": "Learning Bijective Surface Parameterization for Inferring Signed Distance Functions from Sparse Point Clouds with Grid Deformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Takeshi Noda",
      "Chao Chen",
      "Junsheng Zhou",
      "Weiqi Zhang",
      "Yu-Shen Liu",
      "Zhizhong Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wei_Minimizing_Labeled_Maximizing_Unlabeled_An_Image-Driven_Approach_for_Video_Instance_CVPR_2025_paper.html": {
    "title": "Minimizing Labeled, Maximizing Unlabeled: An Image-Driven Approach for Video Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fangyun Wei",
      "Jinjing Zhao",
      "Kun Yan",
      "Chang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/You_Layer-_and_Timestep-Adaptive_Differentiable_Token_Compression_Ratios_for_Efficient_Diffusion_CVPR_2025_paper.html": {
    "title": "Layer- and Timestep-Adaptive Differentiable Token Compression Ratios for Efficient Diffusion Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoran You",
      "Connelly Barnes",
      "Yuqian Zhou",
      "Yan Kang",
      "Zhenbang Du",
      "Wei Zhou",
      "Lingzhi Zhang",
      "Yotam Nitzan",
      "Xiaoyang Liu",
      "Zhe Lin",
      "Eli Shechtman",
      "Sohrab Amirghodsi",
      "Yingyan Celine Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Zero-shot_RGB-D_Point_Cloud_Registration_with_Pre-trained_Large_Vision_Model_CVPR_2025_paper.html": {
    "title": "Zero-shot RGB-D Point Cloud Registration with Pre-trained Large Vision Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haobo Jiang",
      "Jin Xie",
      "Jian Yang",
      "Liang Yu",
      "Jianmin Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ni_Balancing_Two_Classifiers_via_A_Simplex_ETF_Structure_for_Model_CVPR_2025_paper.html": {
    "title": "Balancing Two Classifiers via A Simplex ETF Structure for Model Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiani Ni",
      "He Zhao",
      "Jintong Gao",
      "Dandan Guo",
      "Hongyuan Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_DistinctAD_Distinctive_Audio_Description_Generation_in_Contexts_CVPR_2025_paper.html": {
    "title": "DistinctAD: Distinctive Audio Description Generation in Contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Fang",
      "Wenhao Wu",
      "Qiangqiang Wu",
      "Yuxin Song",
      "Antoni B. Chan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_DAMM-Diffusion_Learning_Divergence-Aware_Multi-Modal_Diffusion_Model_for_Nanoparticles_Distribution_Prediction_CVPR_2025_paper.html": {
    "title": "DAMM-Diffusion: Learning Divergence-Aware Multi-Modal Diffusion Model for Nanoparticles Distribution Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Zhou",
      "Shouju Wang",
      "Yuxia Tang",
      "Qi Zhu",
      "Daoqiang Zhang",
      "Wei Shao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Unveiling_Differences_in_Generative_Models_A_Scalable_Differential_Clustering_Approach_CVPR_2025_paper.html": {
    "title": "Unveiling Differences in Generative Models: A Scalable Differential Clustering Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingwei Zhang",
      "Mohammad Jalali",
      "Cheuk Ting Li",
      "Farzan Farnia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huai_CL-MoE_Enhancing_Multimodal_Large_Language_Model_with_Dual_Momentum_Mixture-of-Experts_CVPR_2025_paper.html": {
    "title": "CL-MoE: Enhancing Multimodal Large Language Model with Dual Momentum Mixture-of-Experts for Continual Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Huai",
      "Jie Zhou",
      "Xingjiao Wu",
      "Qin Chen",
      "Qingchun Bai",
      "Ze Zhou",
      "Liang He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qorbani_Semantic_Library_Adaptation_LoRA_Retrieval_and_Fusion_for_Open-Vocabulary_Semantic_CVPR_2025_paper.html": {
    "title": "Semantic Library Adaptation: LoRA Retrieval and Fusion for Open-Vocabulary Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reza Qorbani",
      "Gianluca Villani",
      "Theodoros Panagiotakopoulos",
      "Marc Botet Colomer",
      "Linus H√§renstam-Nielsen",
      "Mattia Segu",
      "Pier Luigi Dovesi",
      "Jussi Karlgren",
      "Daniel Cremers",
      "Federico Tombari",
      "Matteo Poggi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cai_PhyS-EdiT_Physics-aware_Semantic_Image_Editing_with_Text_Description_CVPR_2025_paper.html": {
    "title": "PhyS-EdiT: Physics-aware Semantic Image Editing with Text Description",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqi Cai",
      "Shuchen Weng",
      "Yifei Xia",
      "Boxin Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_U-Know-DiffPAN_An_Uncertainty-aware_Knowledge_Distillation_Diffusion_Framework_with_Details_Enhancement_CVPR_2025_paper.html": {
    "title": "U-Know-DiffPAN: An Uncertainty-aware Knowledge Distillation Diffusion Framework with Details Enhancement for PAN-Sharpening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungpyo Kim",
      "Jeonghyeok Do",
      "Jaehyup Lee",
      "Munchurl Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tan_SceneDiffuser_City-Scale_Traffic_Simulation_via_a_Generative_World_Model_CVPR_2025_paper.html": {
    "title": "SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuhan Tan",
      "John Lambert",
      "Hong Jeon",
      "Sakshum Kulshrestha",
      "Yijing Bai",
      "Jing Luo",
      "Dragomir Anguelov",
      "Mingxing Tan",
      "Chiyu Max Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pietrantoni_Gaussian_Splatting_Feature_Fields_for_Privacy-Preserving_Visual_Localization_CVPR_2025_paper.html": {
    "title": "Gaussian Splatting Feature Fields for (Privacy-Preserving) Visual Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Maxime Pietrantoni",
      "Gabriela Csurka",
      "Torsten Sattler"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Point_Cloud_Upsampling_Using_Conditional_Diffusion_Module_with_Adaptive_Noise_CVPR_2025_paper.html": {
    "title": "Point Cloud Upsampling Using Conditional Diffusion Module with Adaptive Noise Suppression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boqian Zhang",
      "Shen Yang",
      "Hao Chen",
      "Chao Yang",
      "Jing Jia",
      "Guang Jiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mazzamuto_Gazing_Into_Missteps_Leveraging_Eye-Gaze_for_Unsupervised_Mistake_Detection_in_CVPR_2025_paper.html": {
    "title": "Gazing Into Missteps: Leveraging Eye-Gaze for Unsupervised Mistake Detection in Egocentric Videos of Skilled Human Activities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michele Mazzamuto",
      "Antonino Furnari",
      "Yoichi Sato",
      "Giovanni Maria Farinella"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Trajectory_Mamba_Efficient_Attention-Mamba_Forecasting_Model_Based_on_Selective_SSM_CVPR_2025_paper.html": {
    "title": "Trajectory Mamba: Efficient Attention-Mamba Forecasting Model Based on Selective SSM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yizhou Huang",
      "Yihua Cheng",
      "Kezhi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tan_Recover_and_Match_Open-Vocabulary_Multi-Label_Recognition_through_Knowledge-Constrained_Optimal_Transport_CVPR_2025_paper.html": {
    "title": "Recover and Match: Open-Vocabulary Multi-Label Recognition through Knowledge-Constrained Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Tan",
      "Zichang Tan",
      "Jun Li",
      "Ajian Liu",
      "Jun Wan",
      "Zhen Lei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Koch_RelationField_Relate_Anything_in_Radiance_Fields_CVPR_2025_paper.html": {
    "title": "RelationField: Relate Anything in Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Koch",
      "Johanna Wald",
      "Mirco Colosi",
      "Narunas Vaskevicius",
      "Pedro Hermosilla",
      "Federico Tombari",
      "Timo Ropinski"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_DyFo_A_Training-Free_Dynamic_Focus_Visual_Search_for_Enhancing_LMMs_CVPR_2025_paper.html": {
    "title": "DyFo: A Training-Free Dynamic Focus Visual Search for Enhancing LMMs in Fine-Grained Visual Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Geng Li",
      "Jinglin Xu",
      "Yunzhen Zhao",
      "Yuxin Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Song_From_Head_to_Tail_Towards_Balanced_Representation_in_LargeVision-Language_Models_CVPR_2025_paper.html": {
    "title": "From Head to Tail: Towards Balanced Representation in Large Vision-Language Models through Adaptive Data Calibration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingyang Song",
      "Xiaoye Qu",
      "Jiawei Zhou",
      "Yu Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Let_Humanoids_Hike_Integrative_Skill_Development_on_Complex_Trails_CVPR_2025_paper.html": {
    "title": "Let Humanoids Hike! Integrative Skill Development on Complex Trails",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kwan-Yee Lin",
      "Stella X. Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Corona_VLOGGER_Multimodal_Diffusion_for_Embodied_Avatar_Synthesis_CVPR_2025_paper.html": {
    "title": "VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enric Corona",
      "Andrei Zanfir",
      "Eduard Gabriel Bazavan",
      "Nikos Kolotouros",
      "Thiemo Alldieck",
      "Cristian Sminchisescu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_DEIM_DETR_with_Improved_Matching_for_Fast_Convergence_CVPR_2025_paper.html": {
    "title": "DEIM: DETR with Improved Matching for Fast Convergence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shihua Huang",
      "Zhichao Lu",
      "Xiaodong Cun",
      "Yongjun Yu",
      "Xiao Zhou",
      "Xi Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_BF-STVSR_B-Splines_and_Fourier---Best_Friends_for_High_Fidelity_Spatial-Temporal_Video_CVPR_2025_paper.html": {
    "title": "BF-STVSR: B-Splines and Fourier---Best Friends for High Fidelity Spatial-Temporal Video Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eunjin Kim",
      "Hyeonjin Kim",
      "Kyong Hwan Jin",
      "Jaejun Yoo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Diehl_DIO_Decomposable_Implicit_4D_Occupancy-Flow_World_Model_CVPR_2025_paper.html": {
    "title": "DIO: Decomposable Implicit 4D Occupancy-Flow World Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Christopher Diehl",
      "Quinlan Sykora",
      "Ben Agro",
      "Thomas Gilles",
      "Sergio Casas",
      "Raquel Urtasun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hossain_SLADE_Shielding_against_Dual_Exploits_in_Large_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "SLADE: Shielding against Dual Exploits in Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Zarif Hossain",
      "Ahmed Imteaj"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Human_Motion_Instruction_Tuning_CVPR_2025_paper.html": {
    "title": "Human Motion Instruction Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Li",
      "Sen Jia",
      "Jianhao Wang",
      "Zhongyu Jiang",
      "Feng Zhou",
      "Ju Dai",
      "Tianfang Zhang",
      "Zongkai Wu",
      "Jenq-Neng Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mankovich_A_Flag_Decomposition_for_Hierarchical_Datasets_CVPR_2025_paper.html": {
    "title": "A Flag Decomposition for Hierarchical Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nathan Mankovich",
      "Ignacio Santamaria",
      "Gustau Camps-Valls",
      "Tolga Birdal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Du_RCP-Bench_Benchmarking_Robustness_for_Collaborative_Perception_Under_Diverse_Corruptions_CVPR_2025_paper.html": {
    "title": "RCP-Bench: Benchmarking Robustness for Collaborative Perception Under Diverse Corruptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shihang Du",
      "Sanqing Qu",
      "Tianhang Wang",
      "Xudong Zhang",
      "Yunwei Zhu",
      "Jian Mao",
      "Fan Lu",
      "Qiao Lin",
      "Guang Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Olympus_A_Universal_Task_Router_for_Computer_Vision_Tasks_CVPR_2025_paper.html": {
    "title": "Olympus: A Universal Task Router for Computer Vision Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanze Lin",
      "Yunsheng Li",
      "Dongdong Chen",
      "Weijian Xu",
      "Ronald Clark",
      "Philip Torr"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cai_HERA_Hybrid_Explicit_Representation_for_Ultra-Realistic_Head_Avatars_CVPR_2025_paper.html": {
    "title": "HERA: Hybrid Explicit Representation for Ultra-Realistic Head Avatars",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongrui Cai",
      "Yuting Xiao",
      "Xuan Wang",
      "Jiafei Li",
      "Yudong Guo",
      "Yanbo Fan",
      "Shenghua Gao",
      "Juyong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Smeu_Circumventing_Shortcuts_in_Audio-visual_Deepfake_Detection_Datasets_with_Unsupervised_Learning_CVPR_2025_paper.html": {
    "title": "Circumventing Shortcuts in Audio-visual Deepfake Detection Datasets with Unsupervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Smeu",
      "Dragos-Alexandru Boldisor",
      "Dan Oneata",
      "Elisabeta Oneata"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_CoE_Chain-of-Explanation_via_Automatic_Visual_Concept_Circuit_Description_and_Polysemanticity_CVPR_2025_paper.html": {
    "title": "CoE: Chain-of-Explanation via Automatic Visual Concept Circuit Description and Polysemanticity Quantification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenlong Yu",
      "Qilong Wang",
      "Chuang Liu",
      "Dong Li",
      "Qinghua Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Ego4o_Egocentric_Human_Motion_Capture_and_Understanding_from_Multi-Modal_Input_CVPR_2025_paper.html": {
    "title": "Ego4o: Egocentric Human Motion Capture and Understanding from Multi-Modal Input",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Wang",
      "Rishabh Dabral",
      "Diogo Luvizon",
      "Zhe Cao",
      "Lingjie Liu",
      "Thabo Beeler",
      "Christian Theobalt"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Image_Over_Text_Transforming_Formula_Recognition_Evaluation_with_Character_Detection_CVPR_2025_paper.html": {
    "title": "Image Over Text: Transforming Formula Recognition Evaluation with Character Detection Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bin Wang",
      "Fan Wu",
      "Linke Ouyang",
      "Zhuangcheng Gu",
      "Rui Zhang",
      "Renqiu Xia",
      "Botian Shi",
      "Bo Zhang",
      "Conghui He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tan_FreePCA_Integrating_Consistency_Information_across_Long-short_Frames_in_Training-free_Long_CVPR_2025_paper.html": {
    "title": "FreePCA: Integrating Consistency Information across Long-short Frames in Training-free Long Video Generation via Principal Component Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangtong Tan",
      "Hu Yu",
      "Jie Huang",
      "Jie Xiao",
      "Feng Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Hierarchical_Adaptive_Filtering_Network_for_Text_Image_Specular_Highlight_Removal_CVPR_2025_paper.html": {
    "title": "Hierarchical Adaptive Filtering Network for Text Image Specular Highlight Removal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Jiang",
      "Jingbo Hu",
      "Ling Zhang",
      "Gang Fu",
      "Chunxia Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Improving_Semi-Supervised_Semantic_Segmentation_with_Sliced-Wasserstein_Feature_Alignment_and_Uniformity_CVPR_2025_paper.html": {
    "title": "Improving Semi-Supervised Semantic Segmentation with Sliced-Wasserstein Feature Alignment and Uniformity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen-Yi Lu",
      "Kasra Derakhshandeh",
      "Somali Chaterji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Mind_the_Time_Temporally-Controlled_Multi-Event_Video_Generation_CVPR_2025_paper.html": {
    "title": "Mind the Time: Temporally-Controlled Multi-Event Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Wu",
      "Aliaksandr Siarohin",
      "Willi Menapace",
      "Ivan Skorokhodov",
      "Yuwei Fang",
      "Varnith Chordia",
      "Igor Gilitschenski",
      "Sergey Tulyakov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_Learning_Extremely_High_Density_Crowds_as_Active_Matters_CVPR_2025_paper.html": {
    "title": "Learning Extremely High Density Crowds as Active Matters",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feixiang He",
      "Jiangbei Yue",
      "Jialin Zhu",
      "Armin Seyfried",
      "Dan Casas",
      "Julien Pettr√©",
      "He Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Audio-Visual_Semantic_Graph_Network_for_Audio-Visual_Event_Localization_CVPR_2025_paper.html": {
    "title": "Audio-Visual Semantic Graph Network for Audio-Visual Event Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Liu",
      "Shuaiyong Li",
      "Yongqiang Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_3D-Mem_3D_Scene_Memory_for_Embodied_Exploration_and_Reasoning_CVPR_2025_paper.html": {
    "title": "3D-Mem: 3D Scene Memory for Embodied Exploration and Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuncong Yang",
      "Han Yang",
      "Jiachen Zhou",
      "Peihao Chen",
      "Hongxin Zhang",
      "Yilun Du",
      "Chuang Gan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Meng_EchoMimicV2_Towards_Striking_Simplified_and_Semi-Body_Human_Animation_CVPR_2025_paper.html": {
    "title": "EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rang Meng",
      "Xingyu Zhang",
      "Yuming Li",
      "Chenguang Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bar_Navigation_World_Models_CVPR_2025_paper.html": {
    "title": "Navigation World Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amir Bar",
      "Gaoyue Zhou",
      "Danny Tran",
      "Trevor Darrell",
      "Yann LeCun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pondaven_Video_Motion_Transfer_with_Diffusion_Transformers_CVPR_2025_paper.html": {
    "title": "Video Motion Transfer with Diffusion Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alexander Pondaven",
      "Aliaksandr Siarohin",
      "Sergey Tulyakov",
      "Philip Torr",
      "Fabio Pizzati"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Aira_Gaussian_Splatting_for_Efficient_Satellite_Image_Photogrammetry_CVPR_2025_paper.html": {
    "title": "Gaussian Splatting for Efficient Satellite Image Photogrammetry",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luca Savant Aira",
      "Gabriele Facciolo",
      "Thibaud Ehret"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_Unified_Reconstruction_of_Static_and_Dynamic_Scenes_from_Events_CVPR_2025_paper.html": {
    "title": "Unified Reconstruction of Static and Dynamic Scenes from Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyao Gao",
      "Peiqi Duan",
      "Hanyue Lou",
      "Minggui Teng",
      "Ziqi Cai",
      "Xu Chen",
      "Boxin Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Du_Automatic_Spectral_Calibration_of_Hyperspectral_Images_Method_Dataset_and_Benchmark_CVPR_2025_paper.html": {
    "title": "Automatic Spectral Calibration of Hyperspectral Images: Method, Dataset and Benchmark",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoran Du",
      "Shaodi You",
      "Cheng Cheng",
      "Shikui Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nag_Conformal_Prediction_and_MLLM_aided_Uncertainty_Quantification_in_Scene_Graph_CVPR_2025_paper.html": {
    "title": "Conformal Prediction and MLLM aided Uncertainty Quantification in Scene Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sayak Nag",
      "Udita Ghosh",
      "Calvin-Khang Ta",
      "Sarosij Bose",
      "Jiachen Li",
      "Amit K. Roy-Chowdhury"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Point-to-Region_Loss_for_Semi-Supervised_Point-Based_Crowd_Counting_CVPR_2025_paper.html": {
    "title": "Point-to-Region Loss for Semi-Supervised Point-Based Crowd Counting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Lin",
      "Chenyang Zhao",
      "Antoni B. Chan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Reconstructing_Close_Human_Interaction_with_Appearance_and_Proxemics_Reasoning_CVPR_2025_paper.html": {
    "title": "Reconstructing Close Human Interaction with Appearance and Proxemics Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Buzhen Huang",
      "Chen Li",
      "Chongyang Xu",
      "Dongyue Lu",
      "Jinnan Chen",
      "Yangang Wang",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_Towards_Improved_Text-Aligned_Codebook_Learning_Multi-Hierarchical_Codebook-Text_Alignment_with_Long_CVPR_2025_paper.html": {
    "title": "Towards Improved Text-Aligned Codebook Learning: Multi-Hierarchical Codebook-Text Alignment with Long Text",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guotao Liang",
      "Baoquan Zhang",
      "Zhiyuan Wen",
      "Junteng Zhao",
      "Yunming Ye",
      "Kola Ye",
      "Yao He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Parallel_Sequence_Modeling_via_Generalized_Spatial_Propagation_Network_CVPR_2025_paper.html": {
    "title": "Parallel Sequence Modeling via Generalized Spatial Propagation Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongjun Wang",
      "Wonmin Byeon",
      "Jiarui Xu",
      "Jinwei Gu",
      "Ka Chun Cheung",
      "Xiaolong Wang",
      "Kai Han",
      "Jan Kautz",
      "Sifei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rowe_Scenario_Dreamer_Vectorized_Latent_Diffusion_for_Generating_Driving_Simulation_Environments_CVPR_2025_paper.html": {
    "title": "Scenario Dreamer: Vectorized Latent Diffusion for Generating Driving Simulation Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Luke Rowe",
      "Roger Girgis",
      "Anthony Gosselin",
      "Liam Paull",
      "Christopher Pal",
      "Felix Heide"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Thakkar_Poly-Autoregressive_Prediction_for_Modeling_Interactions_CVPR_2025_paper.html": {
    "title": "Poly-Autoregressive Prediction for Modeling Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Neerja Thakkar",
      "Tara Sadjadpour",
      "Jathushan Rajasegeran",
      "Shiry Ginosar",
      "Jitendra Malik"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_NADER_Neural_Architecture_Design_via_Multi-Agent_Collaboration_CVPR_2025_paper.html": {
    "title": "NADER: Neural Architecture Design via Multi-Agent Collaboration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zekang Yang",
      "Wang Zeng",
      "Sheng Jin",
      "Chen Qian",
      "Ping Luo",
      "Wentao Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Move-in-2D_2D-Conditioned_Human_Motion_Generation_CVPR_2025_paper.html": {
    "title": "Move-in-2D: 2D-Conditioned Human Motion Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hsin-Ping Huang",
      "Yang Zhou",
      "Jui-Hsien Wang",
      "Difan Liu",
      "Feng Liu",
      "Ming-Hsuan Yang",
      "Zhan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jeong_PoseBH_Prototypical_Multi-Dataset_Training_Beyond_Human_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "PoseBH: Prototypical Multi-Dataset Training Beyond Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Uyoung Jeong",
      "Jonathan Freer",
      "Seungryul Baek",
      "Hyung Jin Chang",
      "Kwang In Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xue_MATCHA_Towards_Matching_Anything_CVPR_2025_paper.html": {
    "title": "MATCHA: Towards Matching Anything",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fei Xue",
      "Sven Elflein",
      "Laura Leal-Taix√©",
      "Qunjie Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_CTRL-D_Controllable_Dynamic_3D_Scene_Editing_with_Personalized_2D_Diffusion_CVPR_2025_paper.html": {
    "title": "CTRL-D: Controllable Dynamic 3D Scene Editing with Personalized 2D Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai He",
      "Chin-Hsuan Wu",
      "Igor Gilitschenski"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Separation_of_Powers_On_Segregating_Knowledge_from_Observation_in_LLM-enabled_CVPR_2025_paper.html": {
    "title": "Separation of Powers: On Segregating Knowledge from Observation in LLM-enabled Knowledge-based Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhen Yang",
      "Zhuo Tao",
      "Qi Chen",
      "Liang Li",
      "Yuankai Qi",
      "Anton van den Hengel",
      "Qingming Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Decision_SpikeFormer_Spike-Driven_Transformer_for_Decision_Making_CVPR_2025_paper.html": {
    "title": "Decision SpikeFormer: Spike-Driven Transformer for Decision Making",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Huang",
      "Qinying Gu",
      "Nanyang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_SF2T_Self-supervised_Fragment_Finetuning_of_Video-LLMs_for_Fine-Grained_Understanding_CVPR_2025_paper.html": {
    "title": "SF2T: Self-supervised Fragment Finetuning of Video-LLMs for Fine-Grained Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangliu Hu",
      "Zikai Song",
      "Na Feng",
      "Yawei Luo",
      "Junqing Yu",
      "Yi-Ping Phoebe Chen",
      "Wei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Theory-Inspired_Deep_Multi-View_Multi-Label_Learning_with_Incomplete_Views_and_Noisy_CVPR_2025_paper.html": {
    "title": "Theory-Inspired Deep Multi-View Multi-Label Learning with Incomplete Views and Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quanjiang Li",
      "Tingjin Luo",
      "Jiahui Liao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Fitted_Neural_Lossless_Image_Compression_CVPR_2025_paper.html": {
    "title": "Fitted Neural Lossless Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Zhang",
      "Zhenzhong Chen",
      "Shan Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kumar_Fortifying_Federated_Learning_Towards_Trustworthiness_via_Auditable_Data_Valuation_and_CVPR_2025_paper.html": {
    "title": "Fortifying Federated Learning Towards Trustworthiness via Auditable Data Valuation and Verifiable Client Contribution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "K Naveen Kumar",
      "Ranjeet Ranjan Jha",
      "C Krishna Mohan",
      "Ravindra Babu Tallamraju"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_EMOE_Modality-Specific_Enhanced_Dynamic_Emotion_Experts_CVPR_2025_paper.html": {
    "title": "EMOE: Modality-Specific Enhanced Dynamic Emotion Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Fang",
      "Wenke Huang",
      "Guancheng Wan",
      "Kehua Su",
      "Mang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_JarvisIR_Elevating_Autonomous_Driving_Perception_with_Intelligent_Image_Restoration_CVPR_2025_paper.html": {
    "title": "JarvisIR: Elevating Autonomous Driving Perception with Intelligent Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunlong Lin",
      "Zixu Lin",
      "Haoyu Chen",
      "Panwang Pan",
      "Chenxin Li",
      "Sixiang Chen",
      "Kairun Wen",
      "Yeying Jin",
      "Wenbo Li",
      "Xinghao Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_UniPre3D_Unified_Pre-training_of_3D_Point_Cloud_Models_with_Cross-Modal_CVPR_2025_paper.html": {
    "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Wang",
      "Yanran Zhang",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Behrad_Charm_The_Missing_Piece_in_ViT_Fine-Tuning_for_Image_Aesthetic_CVPR_2025_paper.html": {
    "title": "Charm: The Missing Piece in ViT Fine-Tuning for Image Aesthetic Assessment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fatemeh Behrad",
      "Tinne Tuytelaars",
      "Johan Wagemans"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_F-LMM_Grounding_Frozen_Large_Multimodal_Models_CVPR_2025_paper.html": {
    "title": "F-LMM: Grounding Frozen Large Multimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Size Wu",
      "Sheng Jin",
      "Wenwei Zhang",
      "Lumin Xu",
      "Wentao Liu",
      "Wei Li",
      "Chen Change Loy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_EntityErasure_Erasing_Entity_Cleanly_via_Amodal_Entity_Segmentation_and_Completion_CVPR_2025_paper.html": {
    "title": "EntityErasure: Erasing Entity Cleanly via Amodal Entity Segmentation and Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yixing Zhu",
      "Qing Zhang",
      "Yitong Wang",
      "Yongwei Nie",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Generative_Video_Propagation_CVPR_2025_paper.html": {
    "title": "Generative Video Propagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaoteng Liu",
      "Tianyu Wang",
      "Jui-Hsien Wang",
      "Qing Liu",
      "Zhifei Zhang",
      "Joon-Young Lee",
      "Yijun Li",
      "Bei Yu",
      "Zhe Lin",
      "Soo Ye Kim",
      "Jiaya Jia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Szot_From_Multimodal_LLMs_to_Generalist_Embodied_Agents_Methods_and_Lessons_CVPR_2025_paper.html": {
    "title": "From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrew Szot",
      "Bogdan Mazoure",
      "Omar Attia",
      "Aleksei Timofeev",
      "Harsh Agrawal",
      "Devon Hjelm",
      "Zhe Gan",
      "Zsolt Kira",
      "Alexander Toshev"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Mosaic3D_Foundation_Dataset_and_Model_for_Open-Vocabulary_3D_Segmentation_CVPR_2025_paper.html": {
    "title": "Mosaic3D: Foundation Dataset and Model for Open-Vocabulary 3D Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junha Lee",
      "Chunghyun Park",
      "Jaesung Choe",
      "Yu-Chiang Frank Wang",
      "Jan Kautz",
      "Minsu Cho",
      "Chris Choy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hwang_T-CIL_Temperature_Scaling_using_Adversarial_Perturbation_for_Calibration_in_Class-Incremental_CVPR_2025_paper.html": {
    "title": "T-CIL: Temperature Scaling using Adversarial Perturbation for Calibration in Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seong-Hyeon Hwang",
      "Minsu Kim",
      "Steven Euijong Whang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_LoRA_Subtraction_for_Drift-Resistant_Space_in_Exemplar-Free_Continual_Learning_CVPR_2025_paper.html": {
    "title": "LoRA Subtraction for Drift-Resistant Space in Exemplar-Free Continual Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Liu",
      "Xiaobin Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Schmidt_Joint_Out-of-Distribution_Filtering_and_Data_Discovery_Active_Learning_CVPR_2025_paper.html": {
    "title": "Joint Out-of-Distribution Filtering and Data Discovery Active Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Schmidt",
      "Leonard Schenk",
      "Leo Schwinn",
      "Stephan G√ºnnemann"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lyu_AniMer_Animal_Pose_and_Shape_Estimation_Using_Family_Aware_Transformer_CVPR_2025_paper.html": {
    "title": "AniMer: Animal Pose and Shape Estimation Using Family Aware Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin Lyu",
      "Tianyi Zhu",
      "Yi Gu",
      "Li Lin",
      "Pujin Cheng",
      "Yebin Liu",
      "Xiaoying Tang",
      "Liang An"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Moon_Co-op_Correspondence-based_Novel_Object_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "Co-op: Correspondence-based Novel Object Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sungphill Moon",
      "Hyeontae Son",
      "Dongcheol Hur",
      "Sangwook Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qiu_Finding_Local_Diffusion_Schrodinger_Bridge_using_Kolmogorov-Arnold_Network_CVPR_2025_paper.html": {
    "title": "Finding Local Diffusion Schrodinger Bridge using Kolmogorov-Arnold Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingyu Qiu",
      "Mengying Yang",
      "Xinghua Ma",
      "Fanding Li",
      "Dong Liang",
      "Gongning Luo",
      "Wei Wang",
      "Kuanquan Wang",
      "Shuo Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xue_CorrBEV_Multi-View_3D_Object_Detection_by_Correlation_Learning_with_Multi-modal_CVPR_2025_paper.html": {
    "title": "CorrBEV: Multi-View 3D Object Detection by Correlation Learning with Multi-modal Prototypes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziteng Xue",
      "Mingzhe Guo",
      "Heng Fan",
      "Shihui Zhang",
      "Zhipeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Completion_as_Enhancement_A_Degradation-Aware_Selective_Image_Guided_Network_for_CVPR_2025_paper.html": {
    "title": "Completion as Enhancement: A Degradation-Aware Selective Image Guided Network for Depth Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiqiang Yan",
      "Zhengxue Wang",
      "Kun Wang",
      "Jun Li",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_CATANet_Efficient_Content-Aware_Token_Aggregation_for_Lightweight_Image_Super-Resolution_CVPR_2025_paper.html": {
    "title": "CATANet: Efficient Content-Aware Token Aggregation for Lightweight Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Liu",
      "Jie Liu",
      "Jie Tang",
      "Gangshan Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_SeeGround_See_and_Ground_for_Zero-Shot_Open-Vocabulary_3D_Visual_Grounding_CVPR_2025_paper.html": {
    "title": "SeeGround: See and Ground for Zero-Shot Open-Vocabulary 3D Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rong Li",
      "Shijie Li",
      "Lingdong Kong",
      "Xulei Yang",
      "Junwei Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shao_RayFlow_Instance-Aware_Diffusion_Acceleration_via_Adaptive_Flow_Trajectories_CVPR_2025_paper.html": {
    "title": "RayFlow: Instance-Aware Diffusion Acceleration via Adaptive Flow Trajectories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huiyang Shao",
      "Xin Xia",
      "Yuhong Yang",
      "Yuxi Ren",
      "Xing Wang",
      "Xuefeng Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feng_Linear_Attention_Modeling_for_Learned_Image_Compression_CVPR_2025_paper.html": {
    "title": "Linear Attention Modeling for Learned Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Donghui Feng",
      "Zhengxue Cheng",
      "Shen Wang",
      "Ronghua Wu",
      "Hongwei Hu",
      "Guo Lu",
      "Li Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dufour_Around_the_World_in_80_Timesteps_A_Generative_Approach_to_CVPR_2025_paper.html": {
    "title": "Around the World in 80 Timesteps: A Generative Approach to Global Visual Geolocation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicolas Dufour",
      "Vicky Kalogeiton",
      "David Picard",
      "Loic Landrieu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Asynchronous_Collaborative_Graph_Representation_for_Frames_and_Events_CVPR_2025_paper.html": {
    "title": "Asynchronous Collaborative Graph Representation for Frames and Events",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dianze Li",
      "Jianing Li",
      "Xu Liu",
      "Xiaopeng Fan",
      "Yonghong Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhan_Real-time_High-fidelity_Gaussian_Human_Avatars_with_Position-based_Interpolation_of_Spatially_CVPR_2025_paper.html": {
    "title": "Real-time High-fidelity Gaussian Human Avatars with Position-based Interpolation of Spatially Distributed MLPs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Youyi Zhan",
      "Tianjia Shao",
      "Yin Yang",
      "Kun Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ni_ReconDreamer_Crafting_World_Models_for_Driving_Scene_Reconstruction_via_Online_CVPR_2025_paper.html": {
    "title": "ReconDreamer: Crafting World Models for Driving Scene Reconstruction via Online Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaojun Ni",
      "Guosheng Zhao",
      "Xiaofeng Wang",
      "Zheng Zhu",
      "Wenkang Qin",
      "Guan Huang",
      "Chen Liu",
      "Yuyin Chen",
      "Yida Wang",
      "Xueyang Zhang",
      "Yifei Zhan",
      "Kun Zhan",
      "Peng Jia",
      "Xianpeng Lang",
      "Xingang Wang",
      "Wenjun Mei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Su_RoboSense_Large-scale_Dataset_and_Benchmark_for_Egocentric_Robot_Perception_and_CVPR_2025_paper.html": {
    "title": "RoboSense: Large-scale Dataset and Benchmark for Egocentric Robot Perception and Navigation in Crowded and Unstructured Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haisheng Su",
      "Feixiang Song",
      "Cong Ma",
      "Wei Wu",
      "Junchi Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Self-Supervised_Large_Scale_Point_Cloud_Completion_for_Archaeological_Site_Restoration_CVPR_2025_paper.html": {
    "title": "Self-Supervised Large Scale Point Cloud Completion for Archaeological Site Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aocheng Li",
      "James R. Zimmer-Dauphinee",
      "Rajesh Kalyanam",
      "Ian Lindsay",
      "Parker VanValkenburgh",
      "Steven Wernke",
      "Daniel Aliaga"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Chain_of_Attack_On_the_Robustness_of_Vision-Language_Models_Against_CVPR_2025_paper.html": {
    "title": "Chain of Attack: On the Robustness of Vision-Language Models Against Transfer-Based Adversarial Attacks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peng Xie",
      "Yequan Bie",
      "Jianda Mao",
      "Yangqiu Song",
      "Yang Wang",
      "Hao Chen",
      "Kani Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zeevi_Rate-In_Information-Driven_Adaptive_Dropout_Rates_for_Improved_Inference-Time_Uncertainty_Estimation_CVPR_2025_paper.html": {
    "title": "Rate-In: Information-Driven Adaptive Dropout Rates for Improved Inference-Time Uncertainty Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tal Zeevi",
      "Ravid Shwartz-Ziv",
      "Yann LeCun",
      "Lawrence H. Staib",
      "John A. Onofrey"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kairanda_Thin-Shell-SfT_Fine-Grained_Monocular_Non-rigid_3D_Surface_Tracking_with_Neural_Deformation_CVPR_2025_paper.html": {
    "title": "Thin-Shell-SfT: Fine-Grained Monocular Non-rigid 3D Surface Tracking with Neural Deformation Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Navami Kairanda",
      "Marc Habermann",
      "Shanthika Naik",
      "Christian Theobalt",
      "Vladislav Golyanik"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DeCLIP_Decoupled_Learning_for_Open-Vocabulary_Dense_Perception_CVPR_2025_paper.html": {
    "title": "DeCLIP: Decoupled Learning for Open-Vocabulary Dense Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junjie Wang",
      "Bin Chen",
      "Yulin Li",
      "Bin Kang",
      "Yichi Chen",
      "Zhuotao Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cao_SocialGesture_Delving_into_Multi-person_Gesture_Understanding_CVPR_2025_paper.html": {
    "title": "SocialGesture: Delving into Multi-person Gesture Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Cao",
      "Pranav Virupaksha",
      "Wenqi Jia",
      "Bolin Lai",
      "Fiona Ryan",
      "Sangmin Lee",
      "James M. Rehg"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_GenFusion_Closing_the_Loop_between_Reconstruction_and_Generation_via_Videos_CVPR_2025_paper.html": {
    "title": "GenFusion: Closing the Loop between Reconstruction and Generation via Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sibo Wu",
      "Congrong Xu",
      "Binbin Huang",
      "Andreas Geiger",
      "Anpei Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Brookes_The_PanAf-FGBG_Dataset_Understanding_the_Impact_of_Backgrounds_in_Wildlife_CVPR_2025_paper.html": {
    "title": "The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in Wildlife Behaviour Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Otto Brookes",
      "Maksim Kukushkin",
      "Majid Mirmehdi",
      "Colleen Stephens",
      "Paula Dieguez",
      "Thurston C. Hicks",
      "Sorrel Jones",
      "Kevin Lee",
      "Maureen S. McCarthy",
      "Amelia Meier",
      "Emmanuelle Normand",
      "Erin G. Wessling",
      "Roman M. Wittig",
      "Kevin Langergraber",
      "Klaus Zuberb√ºhler",
      "Lukas Boesch",
      "Thomas Schmid",
      "Mimi Arandjelovic",
      "Hjalmar K√ºhl",
      "Tilo Burghardt"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_Multi-modal_Topology-embedded_Graph_Learning_for_Spatially_Resolved_Genes_Prediction_from_CVPR_2025_paper.html": {
    "title": "Multi-modal Topology-embedded Graph Learning for Spatially Resolved Genes Prediction from Pathology Images with Prior Gene Similarity Information",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Shi",
      "Changxi Chi",
      "Peng Wan",
      "Daoqiang Zhang",
      "Wei Shao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Question-Aware_Gaussian_Experts_for_Audio-Visual_Question_Answering_CVPR_2025_paper.html": {
    "title": "Question-Aware Gaussian Experts for Audio-Visual Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongyeob Kim",
      "Inyoung Jung",
      "Dayoon Suh",
      "Youjia Zhang",
      "Sangmin Lee",
      "Sungeun Hong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ji_Sonic_Shifting_Focus_to_Global_Audio_Perception_in_Portrait_Animation_CVPR_2025_paper.html": {
    "title": "Sonic: Shifting Focus to Global Audio Perception in Portrait Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaozhong Ji",
      "Xiaobin Hu",
      "Zhihong Xu",
      "Junwei Zhu",
      "Chuming Lin",
      "Qingdong He",
      "Jiangning Zhang",
      "Donghao Luo",
      "Yi Chen",
      "Qin Lin",
      "Qinglin Lu",
      "Chengjie Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tarres_Multitwine_Multi-Object_Compositing_with_Text_and_Layout_Control_CVPR_2025_paper.html": {
    "title": "Multitwine: Multi-Object Compositing with Text and Layout Control",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gemma Canet Tarr√©s",
      "Zhe Lin",
      "Zhifei Zhang",
      "He Zhang",
      "Andrew Gilbert",
      "John Collomosse",
      "Soo Ye Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_DEFOM-Stereo_Depth_Foundation_Model_Based_Stereo_Matching_CVPR_2025_paper.html": {
    "title": "DEFOM-Stereo: Depth Foundation Model Based Stereo Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hualie Jiang",
      "Zhiqiang Lou",
      "Laiyan Ding",
      "Rui Xu",
      "Minglang Tan",
      "Wenjie Jiang",
      "Rui Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Adaptive_Rectangular_Convolution_for_Remote_Sensing_Pansharpening_CVPR_2025_paper.html": {
    "title": "Adaptive Rectangular Convolution for Remote Sensing Pansharpening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueyang Wang",
      "Zhixin Zheng",
      "Jiandong Shao",
      "Yule Duan",
      "Liang-Jian Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ke_Video_Depth_without_Video_Models_CVPR_2025_paper.html": {
    "title": "Video Depth without Video Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingxin Ke",
      "Dominik Narnhofer",
      "Shengyu Huang",
      "Lei Ke",
      "Torben Peters",
      "Katerina Fragkiadaki",
      "Anton Obukhov",
      "Konrad Schindler"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_PointLoRA_Low-Rank_Adaptation_with_Token_Selection_for_Point_Cloud_Learning_CVPR_2025_paper.html": {
    "title": "PointLoRA: Low-Rank Adaptation with Token Selection for Point Cloud Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Song Wang",
      "Xiaolu Liu",
      "Lingdong Kong",
      "Jianyun Xu",
      "Chunyong Hu",
      "Gongfan Fang",
      "Wentong Li",
      "Jianke Zhu",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chu_HumanRig_Learning_Automatic_Rigging_for_Humanoid_Character_in_a_Large_CVPR_2025_paper.html": {
    "title": "HumanRig: Learning Automatic Rigging for Humanoid Character in a Large Scale Dataset",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zedong Chu",
      "Feng Xiong",
      "Meiduo Liu",
      "Jinzhi Zhang",
      "Mingqi Shao",
      "Zhaoxu Sun",
      "Di Wang",
      "Mu Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_GaussHDR_High_Dynamic_Range_Gaussian_Splatting_via_Learning_Unified_3D_CVPR_2025_paper.html": {
    "title": "GaussHDR: High Dynamic Range Gaussian Splatting via Learning Unified 3D and 2D Local Tone Mapping",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinfeng Liu",
      "Lingtong Kong",
      "Bo Li",
      "Dan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_UIBDiffusion_Universal_Imperceptible_Backdoor_Attack_for_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "UIBDiffusion: Universal Imperceptible Backdoor Attack for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuning Han",
      "Bingyin Zhao",
      "Rui Chu",
      "Feng Luo",
      "Biplab Sikdar",
      "Yingjie Lao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_DiskVPS_Vanishing_Point_Detector_via_Hough_Transform_in_a_Disk_CVPR_2025_paper.html": {
    "title": "DiskVPS: Vanishing Point Detector via Hough Transform in a Disk Region",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianping Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Seeing_Far_and_Clearly_Mitigating_Hallucinations_in_MLLMs_with_Attention_CVPR_2025_paper.html": {
    "title": "Seeing Far and Clearly: Mitigating Hallucinations in MLLMs with Attention Causal Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Feilong Tang",
      "Chengzhi Liu",
      "Zhongxing Xu",
      "Ming Hu",
      "Zile Huang",
      "Haochen Xue",
      "Ziyang Chen",
      "Zelin Peng",
      "Zhiwei Yang",
      "Sijin Zhou",
      "Wenxue Li",
      "Yulong Li",
      "Wenxuan Song",
      "Shiyan Su",
      "Wei Feng",
      "Jionglong Su",
      "Mingquan Lin",
      "Yifan Peng",
      "Xuelian Cheng",
      "Imran Razzak",
      "Zongyuan Ge"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Towards_Autonomous_Micromobility_through_Scalable_Urban_Simulation_CVPR_2025_paper.html": {
    "title": "Towards Autonomous Micromobility through Scalable Urban Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wayne Wu",
      "Honglin He",
      "Chaoyuan Zhang",
      "Jack He",
      "Seth Z. Zhao",
      "Ran Gong",
      "Quanyi Li",
      "Bolei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_FisherTune_Fisher-Guided_Robust_Tuning_of_Vision_Foundation_Models_for_Domain_CVPR_2025_paper.html": {
    "title": "FisherTune: Fisher-Guided Robust Tuning of Vision Foundation Models for Domain Generalized Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong Zhao",
      "Jinlong Li",
      "Shuang Wang",
      "Mengyao Wu",
      "Qi Zang",
      "Nicu Sebe",
      "Zhun Zhong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_Language-Assisted_Debiasing_and_Smoothing_for_Foundation_Model-Based_Semi-Supervised_Learning_CVPR_2025_paper.html": {
    "title": "Language-Assisted Debiasing and Smoothing for Foundation Model-Based Semi-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Na Zheng",
      "Xuemeng Song",
      "Xue Dong",
      "Aashish Nikhil Ghosh",
      "Liqiang Nie",
      "Roger Zimmermann"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_EdgeMovingNet_Edge-preserving_Point_Cloud_Reconstruction_via_Joint_Geometry_Features_CVPR_2025_paper.html": {
    "title": "EdgeMovingNet: Edge-preserving Point Cloud Reconstruction via Joint Geometry Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinran Yang",
      "Donghao Ji",
      "Yuanqi Li",
      "Junyuan Xie",
      "Jie Guo",
      "Yanwen Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chang_AdMiT_Adaptive_Multi-Source_Tuning_in_Dynamic_Environments_CVPR_2025_paper.html": {
    "title": "AdMiT: Adaptive Multi-Source Tuning in Dynamic Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Chang",
      "Fahim Faisal Niloy",
      "Sk Miraj Ahmed",
      "Srikanth V. Krishnamurthy",
      "Basak Guler",
      "Ananthram Swami",
      "Samet Oymak",
      "Amit Roy-Chowdhury"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Unbiased_Video_Scene_Graph_Generation_via_Visual_and_Semantic_Dual_CVPR_2025_paper.html": {
    "title": "Unbiased Video Scene Graph Generation via Visual and Semantic Dual Debiasing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanjun Li",
      "Zhaoyang Li",
      "Honghui Chen",
      "Lizhi Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Choi_Channel-wise_Noise_Scheduled_Diffusion_for_Inverse_Rendering_in_Indoor_Scenes_CVPR_2025_paper.html": {
    "title": "Channel-wise Noise Scheduled Diffusion for Inverse Rendering in Indoor Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "JunYong Choi",
      "Min-cheol Sagong",
      "SeokYeong Lee",
      "Seung-Won Jung",
      "Ig-Jae Kim",
      "Junghyun Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Targeted_Forgetting_of_Image_Subgroups_in_CLIP_Models_CVPR_2025_paper.html": {
    "title": "Targeted Forgetting of Image Subgroups in CLIP Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeliang Zhang",
      "Gaowen Liu",
      "Charles Fleming",
      "Ramana Rao Kompella",
      "Chenliang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lai_Unleashing_In-context_Learning_of_Autoregressive_Models_for_Few-shot_Image_Manipulation_CVPR_2025_paper.html": {
    "title": "Unleashing In-context Learning of Autoregressive Models for Few-shot Image Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bolin Lai",
      "Felix Juefei-Xu",
      "Miao Liu",
      "Xiaoliang Dai",
      "Nikhil Mehta",
      "Chenguang Zhu",
      "Zeyi Huang",
      "James M. Rehg",
      "Sangmin Lee",
      "Ning Zhang",
      "Tong Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Maniparambil_Harnessing_Frozen_Unimodal_Encoders_for_Flexible_Multimodal_Alignment_CVPR_2025_paper.html": {
    "title": "Harnessing Frozen Unimodal Encoders for Flexible Multimodal Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mayug Maniparambil",
      "Raiymbek Akshulakov",
      "Yasser Abdelaziz Dahou Djilali",
      "Sanath Narayan",
      "Ankit Singh",
      "Noel E. O'Connor"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bian_Feature_Information_Driven_Position_Gaussian_Distribution_Estimation_for_Tiny_Object_CVPR_2025_paper.html": {
    "title": "Feature Information Driven Position Gaussian Distribution Estimation for Tiny Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinghao Bian",
      "Mingtao Feng",
      "Weisheng Dong",
      "Fangfang Wu",
      "Jianqiao Luo",
      "Yaonan Wang",
      "Guangming Shi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Enhancing_Diversity_for_Data-free_Quantization_CVPR_2025_paper.html": {
    "title": "Enhancing Diversity for Data-free Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Zhao",
      "Zhihao Zhuang",
      "Miao Zhang",
      "Chenjuan Guo",
      "Yang Shu",
      "Bin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_SeqAfford_Sequential_3D_Affordance_Reasoning_via_Multimodal_Large_Language_Model_CVPR_2025_paper.html": {
    "title": "SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chunlin Yu",
      "Hanqing Wang",
      "Ye Shi",
      "Haoyang Luo",
      "Sibei Yang",
      "Jingyi Yu",
      "Jingya Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Karimi_DSV-LFS_Unifying_LLM-Driven_Semantic_Cues_with_Visual_Features_for_Robust_CVPR_2025_paper.html": {
    "title": "DSV-LFS: Unifying LLM-Driven Semantic Cues with Visual Features for Robust Few-Shot Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Amin Karimi",
      "Charalambos Poullis"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Revisiting_Generative_Replay_for_Class_Incremental_Object_Detection_CVPR_2025_paper.html": {
    "title": "Revisiting Generative Replay for Class Incremental Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shizhou Zhang",
      "Xueqiang Lv",
      "Yinghui Xing",
      "Qirui Wu",
      "Di Xu",
      "Yanning Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wandel_SemAlign3D_Semantic_Correspondence_between_RGB-Images_through_Aligning_3D_Object-Class_Representations_CVPR_2025_paper.html": {
    "title": "SemAlign3D: Semantic Correspondence between RGB-Images through Aligning 3D Object-Class Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Krispin Wandel",
      "Hesheng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qian_Bridging_Viewpoint_Gaps_Geometric_Reasoning_Boosts_Semantic_Correspondence_CVPR_2025_paper.html": {
    "title": "Bridging Viewpoint Gaps: Geometric Reasoning Boosts Semantic Correspondence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiyang Qian",
      "Hansheng Chen",
      "Masayoshi Tomizuka",
      "Kurt Keutzer",
      "Qianqian Wang",
      "Chenfeng Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_DPU_Dynamic_Prototype_Updating_for_Multimodal_Out-of-Distribution_Detection_CVPR_2025_paper.html": {
    "title": "DPU: Dynamic Prototype Updating for Multimodal Out-of-Distribution Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shawn Li",
      "Huixian Gong",
      "Hao Dong",
      "Tiankai Yang",
      "Zhengzhong Tu",
      "Yue Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Peddi_Towards_Unbiased_and_Robust_Spatio-Temporal_Scene_Graph_Generation_and_Anticipation_CVPR_2025_paper.html": {
    "title": "Towards Unbiased and Robust Spatio-Temporal Scene Graph Generation and Anticipation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rohith Peddi",
      "Saurabh Saurabh",
      "Ayush Abhay Shrivastava",
      "Parag Singla",
      "Vibhav Gogate"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_Spatial_Transport_Optimization_by_Repositioning_Attention_Map_for_Training-Free_Text-to-Image_CVPR_2025_paper.html": {
    "title": "Spatial Transport Optimization by Repositioning Attention Map for Training-Free Text-to-Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Woojung Han",
      "Yeonkyung Lee",
      "Chanyoung Kim",
      "Kwanghyun Park",
      "Seong Jae Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bouniot_From_Alexnet_to_Transformers_Measuring_the_Non-linearity_of_Deep_Neural_CVPR_2025_paper.html": {
    "title": "From Alexnet to Transformers: Measuring the Non-linearity of Deep Neural Networks with Affine Optimal Transport",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Quentin Bouniot",
      "Ievgen Redko",
      "Anton Mallasto",
      "Charlotte Laclau",
      "Oliver Struckmeier",
      "Karol Arndt",
      "Markus Heinonen",
      "Ville Kyrki",
      "Samuel Kaski"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Medghalchi_Prompt2Perturb_P2P_Text-Guided_Diffusion-Based_Adversarial_Attack_on_Breast_Ultrasound_Images_CVPR_2025_paper.html": {
    "title": "Prompt2Perturb (P2P): Text-Guided Diffusion-Based Adversarial Attack on Breast Ultrasound Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yasamin Medghalchi",
      "Moein Heidari",
      "Clayton Allard",
      "Leonid Sigal",
      "Ilker Hacihaliloglu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Boosting_Domain_Incremental_Learning_Selecting_the_Optimal_Parameters_is_All_CVPR_2025_paper.html": {
    "title": "Boosting Domain Incremental Learning: Selecting the Optimal Parameters is All You Need",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiang Wang",
      "Xiang Song",
      "Yuhang He",
      "Jizhou Han",
      "Chenhao Ding",
      "Xinyuan Gao",
      "Yihong Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_COAP_Memory-Efficient_Training_with_Correlation-Aware_Gradient_Projection_CVPR_2025_paper.html": {
    "title": "COAP: Memory-Efficient Training with Correlation-Aware Gradient Projection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinqi Xiao",
      "Shen Sang",
      "Tiancheng Zhi",
      "Jing Liu",
      "Qing Yan",
      "Linjie Luo",
      "Bo Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Perceptual_Inductive_Bias_Is_What_You_Need_Before_Contrastive_Learning_CVPR_2025_paper.html": {
    "title": "Perceptual Inductive Bias Is What You Need Before Contrastive Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junru Zhao",
      "Tianqin Li",
      "Dunhan Jiang",
      "Shenghao Wu",
      "Alan Ramirez",
      "Tai Sing Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_FaceBench_A_Multi-View_Multi-Level_Facial_Attribute_VQA_Dataset_for_Benchmarking_CVPR_2025_paper.html": {
    "title": "FaceBench: A Multi-View Multi-Level Facial Attribute VQA Dataset for Benchmarking Face Perception MLLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoqin Wang",
      "Xusen Ma",
      "Xianxu Hou",
      "Meidan Ding",
      "Yudong Li",
      "Junliang Chen",
      "Wenting Chen",
      "Xiaoyang Peng",
      "Linlin Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rahman_EffiDec3D_An_Optimized_Decoder_for_High-Performance_and_Efficient_3D_Medical_CVPR_2025_paper.html": {
    "title": "EffiDec3D: An Optimized Decoder for High-Performance and Efficient 3D Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Md Mostafijur Rahman",
      "Radu Marculescu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Exploring_Historical_Information_for_RGBE_Visual_Tracking_with_Mamba_CVPR_2025_paper.html": {
    "title": "Exploring Historical Information for RGBE Visual Tracking with Mamba",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chuanyu Sun",
      "Jiqing Zhang",
      "Yang Wang",
      "Huilin Ge",
      "Qianchen Xia",
      "Baocai Yin",
      "Xin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Gyro-based_Neural_Single_Image_Deblurring_CVPR_2025_paper.html": {
    "title": "Gyro-based Neural Single Image Deblurring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Heemin Yang",
      "Jaesung Rim",
      "Seungyong Lee",
      "Seung-Hwan Baek",
      "Sunghyun Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gu_ArtiScene_Language-Driven_Artistic_3D_Scene_Generation_Through_Image_Intermediary_CVPR_2025_paper.html": {
    "title": "ArtiScene: Language-Driven Artistic 3D Scene Generation Through Image Intermediary",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeqi Gu",
      "Yin Cui",
      "Zhaoshuo Li",
      "Fangyin Wei",
      "Yunhao Ge",
      "Jinwei Gu",
      "Ming-Yu Liu",
      "Abe Davis",
      "Yifan Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MobileH2R_Learning_Generalizable_Human_to_Mobile_Robot_Handover_Exclusively_from_CVPR_2025_paper.html": {
    "title": "MobileH2R: Learning Generalizable Human to Mobile Robot Handover Exclusively from Scalable and Diverse Synthetic Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zifan Wang",
      "Ziqing Chen",
      "Junyu Chen",
      "Jilong Wang",
      "Yuxin Yang",
      "Yunze Liu",
      "Xueyi Liu",
      "He Wang",
      "Li Yi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Improving_Sound_Source_Localization_with_Joint_Slot_Attention_on_Image_CVPR_2025_paper.html": {
    "title": "Improving Sound Source Localization with Joint Slot Attention on Image and Audio",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inho Kim",
      "Youngkil Song",
      "Jicheol Park",
      "Won Hwa Kim",
      "Suha Kwak"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hariat_Improved_Monocular_Depth_Prediction_Using_Distance_Transform_Over_Pre-semantic_Contours_CVPR_2025_paper.html": {
    "title": "Improved Monocular Depth Prediction Using Distance Transform Over Pre-semantic Contours with Self-supervised Neural Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marwane Hariat",
      "Antoine Manzanera",
      "David Filliat"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Heep_Feature-Preserving_Mesh_Decimation_for_Normal_Integration_CVPR_2025_paper.html": {
    "title": "Feature-Preserving Mesh Decimation for Normal Integration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Moritz Heep",
      "Sven Behnke",
      "Eduard Zell"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Is_this_Generated_Person_Existed_in_Real-world_Fine-grained_Detecting_and_CVPR_2025_paper.html": {
    "title": "Is this Generated Person Existed in Real-world? Fine-grained Detecting and Calibrating Abnormal Human-body",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeqing Wang",
      "Qingyang Ma",
      "Wentao Wan",
      "Haojie Li",
      "Keze Wang",
      "Yonghong Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cha_PERSE_Personalized_3D_Generative_Avatars_from_A_Single_Portrait_CVPR_2025_paper.html": {
    "title": "PERSE: Personalized 3D Generative Avatars from A Single Portrait",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunsoo Cha",
      "Inhee Lee",
      "Hanbyul Joo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Automated_Generation_of_Challenging_Multiple-Choice_Questions_for_Vision_Language_Model_CVPR_2025_paper.html": {
    "title": "Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhui Zhang",
      "Yuchang Su",
      "Yiming Liu",
      "Xiaohan Wang",
      "James Burgess",
      "Elaine Sui",
      "Chenyu Wang",
      "Josiah Aklilu",
      "Alejandro Lozano",
      "Anjiang Wei",
      "Ludwig Schmidt",
      "Serena Yeung-Levy"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_DexHandDiff_Interaction-aware_Diffusion_Planning_for_Adaptive_Dexterous_Manipulation_CVPR_2025_paper.html": {
    "title": "DexHandDiff: Interaction-aware Diffusion Planning for Adaptive Dexterous Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixuan Liang",
      "Yao Mu",
      "Yixiao Wang",
      "Tianxing Chen",
      "Wenqi Shao",
      "Wei Zhan",
      "Masayoshi Tomizuka",
      "Ping Luo",
      "Mingyu Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cha_VerbDiff_Text-Only_Diffusion_Models_with_Enhanced_Interaction_Awareness_CVPR_2025_paper.html": {
    "title": "VerbDiff: Text-Only Diffusion Models with Enhanced Interaction Awareness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "SeungJu Cha",
      "Kwanyoung Lee",
      "Ye-Chan Kim",
      "Hyunwoo Oh",
      "Dong-Jin Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_ROLL_Robust_Noisy_Pseudo-label_Learning_for_Multi-View_Clustering_with_Noisy_CVPR_2025_paper.html": {
    "title": "ROLL: Robust Noisy Pseudo-label Learning for Multi-View Clustering with Noisy Correspondence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuan Sun",
      "Yongxiang Li",
      "Zhenwen Ren",
      "Guiduo Duan",
      "Dezhong Peng",
      "Peng Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Towards_In-the-wild_3D_Plane_Reconstruction_from_a_Single_Image_CVPR_2025_paper.html": {
    "title": "Towards In-the-wild 3D Plane Reconstruction from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiachen Liu",
      "Rui Yu",
      "Sili Chen",
      "Sharon X. Huang",
      "Hengkai Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rusanovsky_Memories_of_Forgotten_Concepts_CVPR_2025_paper.html": {
    "title": "Memories of Forgotten Concepts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Matan Rusanovsky",
      "Shimon Malnick",
      "Amir Jevnisek",
      "Ohad Fried",
      "Shai Avidan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Dynamic_Stereotype_Theory_Induced_Micro-expression_Recognition_with_Oriented_Deformation_CVPR_2025_paper.html": {
    "title": "Dynamic Stereotype Theory Induced Micro-expression Recognition with Oriented Deformation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bohao Zhang",
      "Xuejiao Wang",
      "Changbo Wang",
      "Gaoqi He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Poesina_PQPP_A_Joint_Benchmark_for_Text-to-Image_Prompt_and_Query_Performance_CVPR_2025_paper.html": {
    "title": "PQPP: A Joint Benchmark for Text-to-Image Prompt and Query Performance Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eduard Poesina",
      "Adriana Valentina Costache",
      "Adrian-Gabriel Chifu",
      "Josiane Mothe",
      "Radu Tudor Ionescu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Antony_CheXwhatsApp_A_Dataset_for_Exploring_Challenges_in_the_Diagnosis_of_CVPR_2025_paper.html": {
    "title": "CheXwhatsApp: A Dataset for Exploring Challenges in the Diagnosis of Chest X-rays through Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mariamma Antony",
      "Rajiv Porana",
      "Sahil M Lathiya",
      "Siva Teja Kakileti",
      "Chiranjib Bhattacharyya"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_PSBD_Prediction_Shift_Uncertainty_Unlocks_Backdoor_Detection_CVPR_2025_paper.html": {
    "title": "PSBD: Prediction Shift Uncertainty Unlocks Backdoor Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Li",
      "Pin-Yu Chen",
      "Sijia Liu",
      "Ren Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_Degradation-Aware_Feature_Perturbation_for_All-in-One_Image_Restoration_CVPR_2025_paper.html": {
    "title": "Degradation-Aware Feature Perturbation for All-in-One Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangpeng Tian",
      "Xiangyu Liao",
      "Xiao Liu",
      "Meng Li",
      "Chao Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gu_ACL_Activating_Capability_of_Linear_Attention_for_Image_Restoration_CVPR_2025_paper.html": {
    "title": "ACL: Activating Capability of Linear Attention for Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yubin Gu",
      "Yuan Meng",
      "Jiayi Ji",
      "Xiaoshuai Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rajagopalan_GenDeg_Diffusion-based_Degradation_Synthesis_for_Generalizable_All-In-One_Image_Restoration_CVPR_2025_paper.html": {
    "title": "GenDeg: Diffusion-based Degradation Synthesis for Generalizable All-In-One Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sudarshan Rajagopalan",
      "Nithin Gopalakrishnan Nair",
      "Jay N. Paranjape",
      "Vishal M. Patel"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xia_Phoenix_A_Motion-based_Self-Reflection_Framework_for_Fine-grained_Robotic_Action_Correction_CVPR_2025_paper.html": {
    "title": "Phoenix: A Motion-based Self-Reflection Framework for Fine-grained Robotic Action Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenke Xia",
      "Ruoxuan Feng",
      "Dong Wang",
      "Di Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mei_The_Power_of_Context_How_Multimodality_Improves_Image_Super-Resolution_CVPR_2025_paper.html": {
    "title": "The Power of Context: How Multimodality Improves Image Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kangfu Mei",
      "Hossein Talebi",
      "Mojtaba Ardakani",
      "Vishal M. Patel",
      "Peyman Milanfar",
      "Mauricio Delbracio"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_MARBLE_Material_Recomposition_and_Blending_in_CLIP-Space_CVPR_2025_paper.html": {
    "title": "MARBLE: Material Recomposition and Blending in CLIP-Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ta Ying Cheng",
      "Prafull Sharma",
      "Mark Boss",
      "Varun Jampani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_Multirate_Neural_Image_Compression_with_Adaptive_Lattice_Vector_Quantization_CVPR_2025_paper.html": {
    "title": "Multirate Neural Image Compression with Adaptive Lattice Vector Quantization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Xu",
      "Xiaolin Wu",
      "Xi Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kong_EventFly_Event_Camera_Perception_from_Ground_to_the_Sky_CVPR_2025_paper.html": {
    "title": "EventFly: Event Camera Perception from Ground to the Sky",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingdong Kong",
      "Dongyue Lu",
      "Xiang Xu",
      "Lai Xing Ng",
      "Wei Tsang Ooi",
      "Benoit R. Cottereau"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xing_Detect_Any_Mirrors_Boosting_Learning_Reliability_on_Large-Scale_Unlabeled_Data_CVPR_2025_paper.html": {
    "title": "Detect Any Mirrors: Boosting Learning Reliability on Large-Scale Unlabeled Data with an Iterative Data Engine",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaohu Xing",
      "Lihao Liu",
      "Yijun Yang",
      "Hongqiu Wang",
      "Tian Ye",
      "Sixiang Chen",
      "Wenxue Li",
      "Guang Liu",
      "Lei Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_CH3Depth_Efficient_and_Flexible_Depth_Foundation_Model_with_Flow_Matching_CVPR_2025_paper.html": {
    "title": "CH3Depth: Efficient and Flexible Depth Foundation Model with Flow Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Li",
      "Yiran Wang",
      "Jinghong Zheng",
      "Junrui Zhang",
      "Liao Shen",
      "Tianqi Liu",
      "Zhiguo Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jang_Pow3R_Empowering_Unconstrained_3D_Reconstruction_with_Camera_and_Scene_Priors_CVPR_2025_paper.html": {
    "title": "Pow3R: Empowering Unconstrained 3D Reconstruction with Camera and Scene Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonbong Jang",
      "Philippe Weinzaepfel",
      "Vincent Leroy",
      "Lourdes Agapito",
      "Jerome Revaud"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kong_Efficient_Visual_State_Space_Model_for_Image_Deblurring_CVPR_2025_paper.html": {
    "title": "Efficient Visual State Space Model for Image Deblurring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingshun Kong",
      "Jiangxin Dong",
      "Jinhui Tang",
      "Ming-Hsuan Yang",
      "Jinshan Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_4D_LangSplat_4D_Language_Gaussian_Splatting_via_Multimodal_Large_Language_CVPR_2025_paper.html": {
    "title": "4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanhua Li",
      "Renping Zhou",
      "Jiawei Zhou",
      "Yingwei Song",
      "Johannes Herter",
      "Minghan Qin",
      "Gao Huang",
      "Hanspeter Pfister"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MambaVLT_Time-Evolving_Multimodal_State_Space_Model_for_Vision-Language_Tracking_CVPR_2025_paper.html": {
    "title": "MambaVLT: Time-Evolving Multimodal State Space Model for Vision-Language Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinqi Liu",
      "Li Zhou",
      "Zikun Zhou",
      "Jianqiu Chen",
      "Zhenyu He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Vuillecard_Enhancing_3D_Gaze_Estimation_in_the_Wild_using_Weak_Supervision_CVPR_2025_paper.html": {
    "title": "Enhancing 3D Gaze Estimation in the Wild using Weak Supervision with Gaze Following Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pierre Vuillecard",
      "Jean-Marc Odobez"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jia_Reward_Fine-Tuning_Two-Step_Diffusion_Models_via_Learning_Differentiable_Latent-Space_Surrogate_CVPR_2025_paper.html": {
    "title": "Reward Fine-Tuning Two-Step Diffusion Models via Learning Differentiable Latent-Space Surrogate Reward",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwei Jia",
      "Yuesong Nan",
      "Huixi Zhao",
      "Gengdai Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Detecting_Out-of-Distribution_Through_the_Lens_of_Neural_Collapse_CVPR_2025_paper.html": {
    "title": "Detecting Out-of-Distribution Through the Lens of Neural Collapse",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Litian Liu",
      "Yao Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Adaptive_Parameter_Selection_for_Tuning_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "Adaptive Parameter Selection for Tuning Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yi Zhang",
      "Yi-Xuan Deng",
      "Meng-Hao Guo",
      "Shi-Min Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hosseininejad_MotionMap_Representing_Multimodality_in_Human_Pose_Forecasting_CVPR_2025_paper.html": {
    "title": "MotionMap: Representing Multimodality in Human Pose Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reyhaneh Hosseininejad",
      "Megh Shukla",
      "Saeed Saadatnejad",
      "Mathieu Salzmann",
      "Alexandre Alahi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Learning-enabled_Polynomial_Lyapunov_Function_Synthesis_via_High-Accuracy_Counterexample-Guided_Framework_CVPR_2025_paper.html": {
    "title": "Learning-enabled Polynomial Lyapunov Function Synthesis via High-Accuracy Counterexample-Guided Framework",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanrui Zhao",
      "Niuniu Qi",
      "Mengxin Ren",
      "Banglong Liu",
      "Shuming Shi",
      "Zhengfeng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_Factored-NeuS_Reconstructing_Surfaces_Illumination_and_Materials_of_Possibly_Glossy_Objects_CVPR_2025_paper.html": {
    "title": "Factored-NeuS: Reconstructing Surfaces, Illumination, and Materials of Possibly Glossy Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Fan",
      "Ningjing Fan",
      "Ivan Skorokhodov",
      "Oleg Voynov",
      "Savva Ignatyev",
      "Evgeny Burnaev",
      "Peter Wonka",
      "Yiqun Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_GaussianSpa_An_Optimizing-Sparsifying_Simplification_Framework_for_Compact_and_High-Quality_3D_CVPR_2025_paper.html": {
    "title": "GaussianSpa: An \"Optimizing-Sparsifying\" Simplification Framework for Compact and High-Quality 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yangming Zhang",
      "Wenqi Jia",
      "Wei Niu",
      "Miao Yin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Sparse2DGS_Geometry-Prioritized_Gaussian_Splatting_for_Surface_Reconstruction_from_Sparse_Views_CVPR_2025_paper.html": {
    "title": "Sparse2DGS: Geometry-Prioritized Gaussian Splatting for Surface Reconstruction from Sparse Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiang Wu",
      "Rui Li",
      "Yu Zhu",
      "Rong Guo",
      "Jinqiu Sun",
      "Yanning Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kushwaha_VinTAGe_Joint_Video_and_Text_Conditioning_for_Holistic_Audio_Generation_CVPR_2025_paper.html": {
    "title": "VinTAGe: Joint Video and Text Conditioning for Holistic Audio Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Saksham Singh Kushwaha",
      "Yapeng Tian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dai_Efficient_Decoupled_Feature_3D_Gaussian_Splatting_via_Hierarchical_Compression_CVPR_2025_paper.html": {
    "title": "Efficient Decoupled Feature 3D Gaussian Splatting via Hierarchical Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenqi Dai",
      "Ting Liu",
      "Yanning Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yao_CountLLM_Towards_Generalizable_Repetitive_Action_Counting_via_Large_Language_Model_CVPR_2025_paper.html": {
    "title": "CountLLM: Towards Generalizable Repetitive Action Counting via Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyu Yao",
      "Xuxin Cheng",
      "Zhiqi Huang",
      "Lei Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Navigating_the_Unseen_Zero-shot_Scene_Graph_Generation_via_Capsule-Based_Equivariant_CVPR_2025_paper.html": {
    "title": "Navigating the Unseen: Zero-shot Scene Graph Generation via Capsule-Based Equivariant Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenhuan Huang",
      "Yi JI",
      "Guiqian Zhu",
      "Li Ying",
      "Chunping Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_VL-RewardBench_A_Challenging_Benchmark_for_Vision-Language_Generative_Reward_Models_CVPR_2025_paper.html": {
    "title": "VL-RewardBench: A Challenging Benchmark for Vision-Language Generative Reward Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Li",
      "Yuancheng Wei",
      "Zhihui Xie",
      "Xuqing Yang",
      "Yifan Song",
      "Peiyi Wang",
      "Chenxin An",
      "Tianyu Liu",
      "Sujian Li",
      "Bill Yuchen Lin",
      "Lingpeng Kong",
      "Qi Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chang_ASHiTA_Automatic_Scene-grounded_HIerarchical_Task_Analysis_CVPR_2025_paper.html": {
    "title": "ASHiTA: Automatic Scene-grounded HIerarchical Task Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun Chang",
      "Leonor Fermoselle",
      "Duy Ta",
      "Bernadette Bucher",
      "Luca Carlone",
      "Jiuguang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Patient-Level_Anatomy_Meets_Scanning-Level_Physics_Personalized_Federated_Low-Dose_CT_Denoising_CVPR_2025_paper.html": {
    "title": "Patient-Level Anatomy Meets Scanning-Level Physics: Personalized Federated Low-Dose CT Denoising Empowered by Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyuan Yang",
      "Yingyu Chen",
      "Zhiwen Wang",
      "Hongming Shan",
      "Yang Chen",
      "Yi Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Choi_Exploiting_Deblurring_Networks_for_Radiance_Fields_CVPR_2025_paper.html": {
    "title": "Exploiting Deblurring Networks for Radiance Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haeyun Choi",
      "Heemin Yang",
      "Janghyeok Han",
      "Sunghyun Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chang_Rethinking_Lanes_and_Points_in_Complex_Scenarios_for_Monocular_3D_CVPR_2025_paper.html": {
    "title": "Rethinking Lanes and Points in Complex Scenarios for Monocular 3D Lane Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Chang",
      "Junjie Huang",
      "Xiaofeng Wang",
      "Yun Ye",
      "Zhujin Liang",
      "Yi Shan",
      "Dalong Du",
      "Xingang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_SPAR3D_Stable_Point-Aware_Reconstruction_of_3D_Objects_from_Single_Images_CVPR_2025_paper.html": {
    "title": "SPAR3D: Stable Point-Aware Reconstruction of 3D Objects from Single Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Huang",
      "Mark Boss",
      "Aaryaman Vasishta",
      "James M. Rehg",
      "Varun Jampani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Discovering_Fine-Grained_Visual-Concept_Relations_by_Disentangled_Optimal_Transport_Concept_Bottleneck_CVPR_2025_paper.html": {
    "title": "Discovering Fine-Grained Visual-Concept Relations by Disentangled Optimal Transport Concept Bottleneck Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yan Xie",
      "Zequn Zeng",
      "Hao Zhang",
      "Yucheng Ding",
      "Yi Wang",
      "Zhengjue Wang",
      "Bo Chen",
      "Hongwei Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xing_Focus-N-Fix_Region-Aware_Fine-Tuning_for_Text-to-Image_Generation_CVPR_2025_paper.html": {
    "title": "Focus-N-Fix: Region-Aware Fine-Tuning for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoying Xing",
      "Avinab Saha",
      "Junfeng He",
      "Susan Hao",
      "Paul Vicol",
      "Moonkyung Ryu",
      "Gang Li",
      "Sahil Singla",
      "Sarah Young",
      "Yinxiao Li",
      "Feng Yang",
      "Deepak Ramachandran"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_RoomTour3D_Geometry-Aware_Video-Instruction_Tuning_for_Embodied_Navigation_CVPR_2025_paper.html": {
    "title": "RoomTour3D: Geometry-Aware Video-Instruction Tuning for Embodied Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingfei Han",
      "Liang Ma",
      "Kamila Zhumakhanova",
      "Ekaterina Radionova",
      "Jingyi Zhang",
      "Xiaojun Chang",
      "Xiaodan Liang",
      "Ivan Laptev"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nisar_PSA-SSL_Pose_and_Size-aware_Self-Supervised_Learning_on_LiDAR_Point_Clouds_CVPR_2025_paper.html": {
    "title": "PSA-SSL: Pose and Size-aware Self-Supervised Learning on LiDAR Point Clouds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Barza Nisar",
      "Steven L. Waslander"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ko_Bringing_CLIP_to_the_Clinic_Dynamic_Soft_Labels_and_Negation-Aware_CVPR_2025_paper.html": {
    "title": "Bringing CLIP to the Clinic: Dynamic Soft Labels and Negation-Aware Learning for Medical Analysis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanbin Ko",
      "Chang-Min Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_SnapGen_Taming_High-Resolution_Text-to-Image_Models_for_Mobile_Devices_with_Efficient_CVPR_2025_paper.html": {
    "title": "SnapGen: Taming High-Resolution Text-to-Image Models for Mobile Devices with Efficient Architectures and Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jierun Chen",
      "Dongting Hu",
      "Xijie Huang",
      "Huseyin Coskun",
      "Arpit Sahni",
      "Aarush Gupta",
      "Anujraaj Goyal",
      "Dishani Lahiri",
      "Rajesh Singh",
      "Yerlan Idelbayev",
      "Junli Cao",
      "Yanyu Li",
      "Kwang-Ting Cheng",
      "S.-H. Gary Chan",
      "Mingming Gong",
      "Sergey Tulyakov",
      "Anil Kag",
      "Yanwu Xu",
      "Jian Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dai_Label_Shift_Meets_Online_Learning_Ensuring_Consistent_Adaptation_with_Universal_CVPR_2025_paper.html": {
    "title": "Label Shift Meets Online Learning: Ensuring Consistent Adaptation with Universal Dynamic Regret",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yucong Dai",
      "Shilin Gu",
      "Ruidong Fan",
      "Chao Xu",
      "Chenping Hou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_A_Physics-Informed_Blur_Learning_Framework_for_Imaging_Systems_CVPR_2025_paper.html": {
    "title": "A Physics-Informed Blur Learning Framework for Imaging Systems",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liqun Chen",
      "Yuxuan Li",
      "Jun Dai",
      "Jinwei Gu",
      "Tianfan Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_A_Semantic_Knowledge_Complementarity_based_Decoupling_Framework_for_Semi-supervised_Class-imbalanced_CVPR_2025_paper.html": {
    "title": "A Semantic Knowledge Complementarity based Decoupling Framework for Semi-supervised Class-imbalanced Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheng Zhang",
      "Guanchun Yin",
      "Bo Zhang",
      "Wu Liu",
      "Xiuzhuang Zhou",
      "Wendong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_Community_Forensics_Using_Thousands_of_Generators_to_Train_Fake_Image_CVPR_2025_paper.html": {
    "title": "Community Forensics: Using Thousands of Generators to Train Fake Image Detectors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeongsoo Park",
      "Andrew Owens"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_ModeSeq_Taming_Sparse_Multimodal_Motion_Prediction_with_Sequential_Mode_Modeling_CVPR_2025_paper.html": {
    "title": "ModeSeq: Taming Sparse Multimodal Motion Prediction with Sequential Mode Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zikang Zhou",
      "Hengjian Zhou",
      "Haibo Hu",
      "Zihao Wen",
      "Jianping Wang",
      "Yung-Hui Li",
      "Yu-Kai Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Stuyck_Quaffure_Real-Time_Quasi-Static_Neural_Hair_Simulation_CVPR_2025_paper.html": {
    "title": "Quaffure: Real-Time Quasi-Static Neural Hair Simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuur Stuyck",
      "Gene Wei-Chin Lin",
      "Egor Larionov",
      "Hsiao-yu Chen",
      "Aljaz Bozic",
      "Nikolaos Sarafianos",
      "Doug Roble"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jia_Towards_Practical_Real-Time_Neural_Video_Compression_CVPR_2025_paper.html": {
    "title": "Towards Practical Real-Time Neural Video Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaoyang Jia",
      "Bin Li",
      "Jiahao Li",
      "Wenxuan Xie",
      "Linfeng Qi",
      "Houqiang Li",
      "Yan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_DepthSplat_Connecting_Gaussian_Splatting_and_Depth_CVPR_2025_paper.html": {
    "title": "DepthSplat: Connecting Gaussian Splatting and Depth",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haofei Xu",
      "Songyou Peng",
      "Fangjinhua Wang",
      "Hermann Blum",
      "Daniel Barath",
      "Andreas Geiger",
      "Marc Pollefeys"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xing_LumiNet_Latent_Intrinsics_Meets_Diffusion_Models_for_Indoor_Scene_Relighting_CVPR_2025_paper.html": {
    "title": "LumiNet: Latent Intrinsics Meets Diffusion Models for Indoor Scene Relighting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiaoyan Xing",
      "Konrad Groh",
      "Sezer Karaoglu",
      "Theo Gevers",
      "Anand Bhattad"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_FedBiP_Heterogeneous_One-Shot_Federated_Learning_with_Personalized_Latent_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "FedBiP: Heterogeneous One-Shot Federated Learning with Personalized Latent Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haokun Chen",
      "Hang Li",
      "Yao Zhang",
      "Jinhe Bi",
      "Gengyuan Zhang",
      "Yueqi Zhang",
      "Philip Torr",
      "Jindong Gu",
      "Denis Krompass",
      "Volker Tresp"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_DiC_Rethinking_Conv3x3_Designs_in_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "DiC: Rethinking Conv3x3 Designs in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchuan Tian",
      "Jing Han",
      "Chengcheng Wang",
      "Yuchen Liang",
      "Chao Xu",
      "Hanting Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rockwell_Dynamic_Camera_Poses_and_Where_to_Find_Them_CVPR_2025_paper.html": {
    "title": "Dynamic Camera Poses and Where to Find Them",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chris Rockwell",
      "Joseph Tung",
      "Tsung-Yi Lin",
      "Ming-Yu Liu",
      "David F. Fouhey",
      "Chen-Hsuan Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lei_MoSca_Dynamic_Gaussian_Fusion_from_Casual_Videos_via_4D_Motion_CVPR_2025_paper.html": {
    "title": "MoSca: Dynamic Gaussian Fusion from Casual Videos via 4D Motion Scaffolds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiahui Lei",
      "Yijia Weng",
      "Adam W. Harley",
      "Leonidas Guibas",
      "Kostas Daniilidis"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_GCE-Pose_Global_Context_Enhancement_for_Category-level_Object_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "GCE-Pose: Global Context Enhancement for Category-level Object Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Weihang Li",
      "Hongli XU",
      "Junwen Huang",
      "Hyunjun Jung",
      "Peter KT Yu",
      "Nassir Navab",
      "Benjamin Busam"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_OmniGen_Unified_Image_Generation_CVPR_2025_paper.html": {
    "title": "OmniGen: Unified Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shitao Xiao",
      "Yueze Wang",
      "Junjie Zhou",
      "Huaying Yuan",
      "Xingrun Xing",
      "Ruiran Yan",
      "Chaofan Li",
      "Shuting Wang",
      "Tiejun Huang",
      "Zheng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Meli_QuCOOP_A_Versatile_Framework_for_Solving_Composite_and_Binary-Parametrised_Problems_CVPR_2025_paper.html": {
    "title": "QuCOOP: A Versatile Framework for Solving Composite and Binary-Parametrised Problems on Quantum Annealers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Natacha Kuete Meli",
      "Vladislav Golyanik",
      "Marcel Seelbach Benkner",
      "Michael Moeller"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Mesh_Mamba_A_Unified_State_Space_Model_for_Saliency_Prediction_CVPR_2025_paper.html": {
    "title": "Mesh Mamba: A Unified State Space Model for Saliency Prediction in Non-Textured and Textured Meshes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kaiwei Zhang",
      "Dandan Zhu",
      "Xiongkuo Min",
      "Guangtao Zhai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_Not_Just_Text_Uncovering_Vision_Modality_Typographic_Threats_in_Image_CVPR_2025_paper.html": {
    "title": "Not Just Text: Uncovering Vision Modality Typographic Threats in Image Generation Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hao Cheng",
      "Erjia Xiao",
      "Jiayan Yang",
      "Jiahang Cao",
      "Qiang Zhang",
      "Jize Zhang",
      "Kaidi Xu",
      "Jindong Gu",
      "Renjing Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qu_SILMM_Self-Improving_Large_Multimodal_Models_for_Compositional_Text-to-Image_Generation_CVPR_2025_paper.html": {
    "title": "SILMM: Self-Improving Large Multimodal Models for Compositional Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leigang Qu",
      "Haochuan Li",
      "Wenjie Wang",
      "Xiang Liu",
      "Juncheng Li",
      "Liqiang Nie",
      "Tat-Seng Chua"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Calibrated_Multi-Preference_Optimization_for_Aligning_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Calibrated Multi-Preference Optimization for Aligning Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kyungmin Lee",
      "Xiahong Li",
      "Qifei Wang",
      "Junfeng He",
      "Junjie Ke",
      "Ming-Hsuan Yang",
      "Irfan Essa",
      "Jinwoo Shin",
      "Feng Yang",
      "Yinxiao Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Learning_from_Neighbors_Category_Extrapolation_for_Long-Tail_Learning_CVPR_2025_paper.html": {
    "title": "Learning from Neighbors: Category Extrapolation for Long-Tail Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shizhen Zhao",
      "Xin Wen",
      "Jiahui Liu",
      "Chuofan Ma",
      "Chunfeng Yuan",
      "Xiaojuan Qi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Material_Anything_Generating_Materials_for_Any_3D_Object_via_Diffusion_CVPR_2025_paper.html": {
    "title": "Material Anything: Generating Materials for Any 3D Object via Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Huang",
      "Tengfei Wang",
      "Ziwei Liu",
      "Qing Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pan_TokenHSI_Unified_Synthesis_of_Physical_Human-Scene_Interactions_through_Task_Tokenization_CVPR_2025_paper.html": {
    "title": "TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liang Pan",
      "Zeshi Yang",
      "Zhiyang Dou",
      "Wenjia Wang",
      "Buzhen Huang",
      "Bo Dai",
      "Taku Komura",
      "Jingbo Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Maruani_ShapeShifter_3D_Variations_Using_Multiscale_and_Sparse_Point-Voxel_Diffusion_CVPR_2025_paper.html": {
    "title": "ShapeShifter: 3D Variations Using Multiscale and Sparse Point-Voxel Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nissim Maruani",
      "Wang Yifan",
      "Matthew Fisher",
      "Pierre Alliez",
      "Mathieu Desbrun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_ImagineFSL_Self-Supervised_Pretraining_Matters_on_Imagined_Base_Set_for_VLM-based_CVPR_2025_paper.html": {
    "title": "ImagineFSL: Self-Supervised Pretraining Matters on Imagined Base Set for VLM-based Few-shot Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyuan Yang",
      "Xiaoou Li",
      "Jiaming Lv",
      "Xianjun Cheng",
      "Qilong Wang",
      "Peihua Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bae_Continuous_Locomotive_Crowd_Behavior_Generation_CVPR_2025_paper.html": {
    "title": "Continuous Locomotive Crowd Behavior Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Inhwan Bae",
      "Junoh Lee",
      "Hae-Gon Jeon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Project-Probe-Aggregate_Efficient_Fine-Tuning_for_Group_Robustness_CVPR_2025_paper.html": {
    "title": "Project-Probe-Aggregate: Efficient Fine-Tuning for Group Robustness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Beier Zhu",
      "Jiequan Cui",
      "Hanwang Zhang",
      "Chi Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Implicit_Bias_Injection_Attacks_against_Text-to-Image_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Implicit Bias Injection Attacks against Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huayang Huang",
      "Xiangye Jin",
      "Jiaxu Miao",
      "Yu Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gu_ROICtrl_Boosting_Instance_Control_for_Visual_Generation_CVPR_2025_paper.html": {
    "title": "ROICtrl: Boosting Instance Control for Visual Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchao Gu",
      "Yipin Zhou",
      "Yunfan Ye",
      "Yixin Nie",
      "Licheng Yu",
      "Pingchuan Ma",
      "Kevin Qinghong Lin",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_FRESA_Feedforward_Reconstruction_of_Personalized_Skinned_Avatars_from_Few_Images_CVPR_2025_paper.html": {
    "title": "FRESA: Feedforward Reconstruction of Personalized Skinned Avatars from Few Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rong Wang",
      "Fabian Prada",
      "Ziyan Wang",
      "Zhongshi Jiang",
      "Chengxiang Yin",
      "Junxuan Li",
      "Shunsuke Saito",
      "Igor Santesteban",
      "Javier Romero",
      "Rohan Joshi",
      "Hongdong Li",
      "Jason Saragih",
      "Yaser Sheikh"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_ReasonGrounder_LVLM-Guided_Hierarchical_Feature_Splatting_for_Open-Vocabulary_3D_Visual_Grounding_CVPR_2025_paper.html": {
    "title": "ReasonGrounder: LVLM-Guided Hierarchical Feature Splatting for Open-Vocabulary 3D Visual Grounding and Reasoning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyang Liu",
      "Yikai Wang",
      "Sixiao Zheng",
      "Tongying Pan",
      "Longfei Liang",
      "Yanwei Fu",
      "Xiangyang Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Cropper_Vision-Language_Model_for_Image_Cropping_through_In-Context_Learning_CVPR_2025_paper.html": {
    "title": "Cropper: Vision-Language Model for Image Cropping through In-Context Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Seung Hyun Lee",
      "Jijun Jiang",
      "Yiran Xu",
      "Zhuofang Li",
      "Junjie Ke",
      "Yinxiao Li",
      "Junfeng He",
      "Steven Hickson",
      "Katie Datsenko",
      "Sangpil Kim",
      "Ming-Hsuan Yang",
      "Irfan Essa",
      "Feng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Meng_Advancing_Adversarial_Robustness_in_GNeRFs_The_IL2-NeRF_Attack_CVPR_2025_paper.html": {
    "title": "Advancing Adversarial Robustness in GNeRFs: The IL2-NeRF Attack",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nicole Meng",
      "Caleb Manicke",
      "Ronak Sahu",
      "Caiwen Ding",
      "Yingjie Lao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_WonderWorld_Interactive_3D_Scene_Generation_from_a_Single_Image_CVPR_2025_paper.html": {
    "title": "WonderWorld: Interactive 3D Scene Generation from a Single Image",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hong-Xing Yu",
      "Haoyi Duan",
      "Charles Herrmann",
      "William T. Freeman",
      "Jiajun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_A_Lightweight_UDF_Learning_Framework_for_3D_Reconstruction_Based_on_CVPR_2025_paper.html": {
    "title": "A Lightweight UDF Learning Framework for 3D Reconstruction Based on Local Shape Functions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiangbei Hu",
      "Yanggeng Li",
      "Fei Hou",
      "Junhui Hou",
      "Zhebin Zhang",
      "Shengfa Wang",
      "Na Lei",
      "Ying He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_DiffCAM_Data-Driven_Saliency_Maps_by_Capturing_Feature_Differences_CVPR_2025_paper.html": {
    "title": "DiffCAM: Data-Driven Saliency Maps by Capturing Feature Differences",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingjian Li",
      "Qiming Zhao",
      "Neelesh Bisht",
      "Mostofa Rafid Uddin",
      "Jin Yu Kim",
      "Bryan Zhang",
      "Min Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_PolarNeXt_Rethink_Instance_Segmentation_with_Polar_Representation_CVPR_2025_paper.html": {
    "title": "PolarNeXt: Rethink Instance Segmentation with Polar Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiacheng Sun",
      "Xinghong Zhou",
      "Yiqiang Wu",
      "Bin Zhu",
      "Jiaxuan Lu",
      "Yu Qin",
      "Xiaomao Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_ScaMo_Exploring_the_Scaling_Law_in_Autoregressive_Motion_Generation_Model_CVPR_2025_paper.html": {
    "title": "ScaMo: Exploring the Scaling Law in Autoregressive Motion Generation Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shunlin Lu",
      "Jingbo Wang",
      "Zeyu Lu",
      "Ling-Hao Chen",
      "Wenxun Dai",
      "Junting Dong",
      "Zhiyang Dou",
      "Bo Dai",
      "Ruimao Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Barquero_From_Sparse_Signal_to_Smooth_Motion_Real-Time_Motion_Generation_with_CVPR_2025_paper.html": {
    "title": "From Sparse Signal to Smooth Motion: Real-Time Motion Generation with Rolling Prediction Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "German Barquero",
      "Nadine Bertsch",
      "Manojkumar Marramreddy",
      "Carlos Chac√≥n",
      "Filippo Arcadu",
      "Ferran Rigual",
      "Nicky Sijia He",
      "Cristina Palmero",
      "Sergio Escalera",
      "Yuting Ye",
      "Robin Kips"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Imagine_and_Seek_Improving_Composed_Image_Retrieval_with_an_Imagined_CVPR_2025_paper.html": {
    "title": "Imagine and Seek: Improving Composed Image Retrieval with an Imagined Proxy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "You Li",
      "Fan Ma",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_EMOVA_Empowering_Language_Models_to_See_Hear_and_Speak_with_CVPR_2025_paper.html": {
    "title": "EMOVA: Empowering Language Models to See, Hear and Speak with Vivid Emotions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Chen",
      "Yunhao Gou",
      "Runhui Huang",
      "Zhili Liu",
      "Daxin Tan",
      "Jing Xu",
      "Chunwei Wang",
      "Yi Zhu",
      "Yihan Zeng",
      "Kuo Yang",
      "Dingdong Wang",
      "Kun Xiang",
      "Haoyuan Li",
      "Haoli Bai",
      "Jianhua Han",
      "Xiaohui Li",
      "Weike Jin",
      "Nian Xie",
      "Yu Zhang",
      "James T. Kwok",
      "Hengshuang Zhao",
      "Xiaodan Liang",
      "Dit-Yan Yeung",
      "Xiao Chen",
      "Zhenguo Li",
      "Wei Zhang",
      "Qun Liu",
      "Lanqing Hong",
      "Lu Hou",
      "Hang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_SAM-REF_Introducing_Image-Prompt_Synergy_during_Interaction_for_Detail_Enhancement_in_CVPR_2025_paper.html": {
    "title": "SAM-REF: Introducing Image-Prompt Synergy during Interaction for Detail Enhancement in the Segment Anything Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chongkai Yu",
      "Ting Liu",
      "Anqi Li",
      "Xiaochao Qu",
      "Chengjing Wu",
      "Luoqi Liu",
      "Xiaolin Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Feijoo_DarkIR_Robust_Low-Light_Image_Restoration_CVPR_2025_paper.html": {
    "title": "DarkIR: Robust Low-Light Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Feijoo",
      "Juan C. Benito",
      "Alvaro Garcia",
      "Marcos V. Conde"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bai_R2C_Mapping_Room_to_Chessboard_to_Unlock_LLM_As_Low-Level_CVPR_2025_paper.html": {
    "title": "R2C: Mapping Room to Chessboard to Unlock LLM As Low-Level Action Planner",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Bai",
      "Hanxuan Li",
      "Bin Fu",
      "Chuyan Xiong",
      "Ruiping Wang",
      "Xilin Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cendra_ICE_Intrinsic_Concept_Extraction_from_a_Single_Image_via_Diffusion_CVPR_2025_paper.html": {
    "title": "ICE: Intrinsic Concept Extraction from a Single Image via Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Fernando Julio Cendra",
      "Kai Han"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_ASIGN_An_Anatomy-aware_Spatial_Imputation_Graphic_Network_for_3D_Spatial_CVPR_2025_paper.html": {
    "title": "ASIGN: An Anatomy-aware Spatial Imputation Graphic Network for 3D Spatial Transcriptomics",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junchao Zhu",
      "Ruining Deng",
      "Tianyuan Yao",
      "Juming Xiong",
      "Chongyu Qu",
      "Junlin Guo",
      "Siqi Lu",
      "Mengmeng Yin",
      "Yu Wang",
      "Shilin Zhao",
      "Haichun Yang",
      "Yuankai Huo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qin_Reversing_Flow_for_Image_Restoration_CVPR_2025_paper.html": {
    "title": "Reversing Flow for Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haina Qin",
      "Wenyang Luo",
      "Libin Wang",
      "Dandan Zheng",
      "Jingdong Chen",
      "Ming Yang",
      "Bing Li",
      "Weiming Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Shadow_Generation_Using_Diffusion_Model_with_Geometry_Prior_CVPR_2025_paper.html": {
    "title": "Shadow Generation Using Diffusion Model with Geometry Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haonan Zhao",
      "Qingyang Liu",
      "Xinhao Tao",
      "Li Niu",
      "Guangtao Zhai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zong_Rethinking_Epistemic_and_Aleatoric_Uncertainty_for_Active_Open-Set_Annotation_An_CVPR_2025_paper.html": {
    "title": "Rethinking Epistemic and Aleatoric Uncertainty for Active Open-Set Annotation: An Energy-Based Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen-Chen Zong",
      "Sheng-Jun Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nguyen_Any3DIS_Class-Agnostic_3D_Instance_Segmentation_by_2D_Mask_Tracking_CVPR_2025_paper.html": {
    "title": "Any3DIS: Class-Agnostic 3D Instance Segmentation by 2D Mask Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Phuc Nguyen",
      "Minh Luu",
      "Anh Tran",
      "Cuong Pham",
      "Khoi Nguyen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_FDS_Frequency-Aware_Denoising_Score_for_Text-Guided_Latent_Diffusion_Image_Editing_CVPR_2025_paper.html": {
    "title": "FDS: Frequency-Aware Denoising Score for Text-Guided Latent Diffusion Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufan Ren",
      "Zicong Jiang",
      "Tong Zhang",
      "S√∏ren Forchhammer",
      "Sabine S√ºsstrunk"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_MMAR_Towards_Lossless_Multi-Modal_Auto-Regressive_Probabilistic_Modeling_CVPR_2025_paper.html": {
    "title": "MMAR: Towards Lossless Multi-Modal Auto-Regressive Probabilistic Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Yang",
      "Dacheng Yin",
      "Yizhou Zhou",
      "Fengyun Rao",
      "Wei Zhai",
      "Yang Cao",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shan_ROS-SAM_High-Quality_Interactive_Segmentation_for_Remote_Sensing_Moving_Object_CVPR_2025_paper.html": {
    "title": "ROS-SAM: High-Quality Interactive Segmentation for Remote Sensing Moving Object",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhe Shan",
      "Yang Liu",
      "Lei Zhou",
      "Cheng Yan",
      "Heng Wang",
      "Xia Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Abulnaga_MultiMorph_On-demand_Atlas_Construction_CVPR_2025_paper.html": {
    "title": "MultiMorph: On-demand Atlas Construction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "S. Mazdak Abulnaga",
      "Andrew Hoopes",
      "Neel Dey",
      "Malte Hoffmann",
      "Bruce Fischl",
      "John Guttag",
      "Adrian Dalca"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nan_MI-DETR_An_Object_Detection_Model_with_Multi-time_Inquiries_Mechanism_CVPR_2025_paper.html": {
    "title": "MI-DETR: An Object Detection Model with Multi-time Inquiries Mechanism",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhixiong Nan",
      "Xianghong Li",
      "Jifeng Dai",
      "Tao Xiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_From_Prototypes_to_General_Distributions_An_Efficient_Curriculum_for_Masked_CVPR_2025_paper.html": {
    "title": "From Prototypes to General Distributions: An Efficient Curriculum for Masked Image Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhong Lin",
      "Cheng-En Wu",
      "Huanran Li",
      "Jifan Zhang",
      "Yu Hen Hu",
      "Pedro Morgado"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_Synthetic_Visual_Genome_CVPR_2025_paper.html": {
    "title": "Synthetic Visual Genome",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jae Sung Park",
      "Zixian Ma",
      "Linjie Li",
      "Chenhao Zheng",
      "Cheng-Yu Hsieh",
      "Ximing Lu",
      "Khyathi Chandu",
      "Quan Kong",
      "Norimasa Kobori",
      "Ali Farhadi",
      "Yejin Choi",
      "Ranjay Krishna"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Difference_Inversion_Interpolate_and_Isolate_the_Difference_with_Token_Consistency_CVPR_2025_paper.html": {
    "title": "Difference Inversion: Interpolate and Isolate the Difference with Token Consistency for Image Analogy Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hyunsoo Kim",
      "Donghyun Kim",
      "Suhyun Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Suo_Octopus_Alleviating_Hallucination_via_Dynamic_Contrastive_Decoding_CVPR_2025_paper.html": {
    "title": "Octopus: Alleviating Hallucination via Dynamic Contrastive Decoding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Suo",
      "Lijun Zhang",
      "Mengyang Sun",
      "Lin Yuanbo Wu",
      "Peng Wang",
      "Yanning Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_MTADiffusion_Mask_Text_Alignment_Diffusion_Model_for_Object_Inpainting_CVPR_2025_paper.html": {
    "title": "MTADiffusion: Mask Text Alignment Diffusion Model for Object Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Huang",
      "Ting Liu",
      "Yihang Wu",
      "Xiaochao Qu",
      "Luoqi Liu",
      "Xiaolin Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yoon_Stop_Learning_it_all_to_Mitigate_Visual_Hallucination_Focus_on_CVPR_2025_paper.html": {
    "title": "Stop Learning it all to Mitigate Visual Hallucination, Focus on the Hallucination Target",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dokyoon Yoon",
      "Youngsook Song",
      "Woomyoung Park"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Talon_Seeing_the_Abstract_Translating_the_Abstract_Language_for_Vision_Language_CVPR_2025_paper.html": {
    "title": "Seeing the Abstract: Translating the Abstract Language for Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Davide Talon",
      "Federico Girella",
      "Ziyue Liu",
      "Marco Cristani",
      "Yiming Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Spiking_Transformer_Introducing_Accurate_Addition-Only_Spiking_Self-Attention_for_Transformer_CVPR_2025_paper.html": {
    "title": "Spiking Transformer: Introducing Accurate Addition-Only Spiking Self-Attention for Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yufei Guo",
      "Xiaode Liu",
      "Yuanpei Chen",
      "Weihang Peng",
      "Yuhan Zhang",
      "Zhe Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Grounding_3D_Object_Affordance_with_Language_Instructions_Visual_Observations_and_CVPR_2025_paper.html": {
    "title": "Grounding 3D Object Affordance with Language Instructions, Visual Observations and Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "He Zhu",
      "Quyu Kong",
      "Kechun Xu",
      "Xunlong Xia",
      "Bing Deng",
      "Jieping Ye",
      "Rong Xiong",
      "Yue Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_MOVIS_Enhancing_Multi-Object_Novel_View_Synthesis_for_Indoor_Scenes_CVPR_2025_paper.html": {
    "title": "MOVIS: Enhancing Multi-Object Novel View Synthesis for Indoor Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruijie Lu",
      "Yixin Chen",
      "Junfeng Ni",
      "Baoxiong Jia",
      "Yu Liu",
      "Diwen Wan",
      "Gang Zeng",
      "Siyuan Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bao_One-Step_Event-Driven_High-Speed_Autofocus_CVPR_2025_paper.html": {
    "title": "One-Step Event-Driven High-Speed Autofocus",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Bao",
      "Shaohua Gao",
      "Wenyong Li",
      "Kaiwei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Symbolic_Representation_for_Any-to-Any_Generative_Tasks_CVPR_2025_paper.html": {
    "title": "Symbolic Representation for Any-to-Any Generative Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaqi Chen",
      "Xiaoye Zhu",
      "Yue Wang",
      "Tianyang Liu",
      "Xinhui Chen",
      "Ying Chen",
      "Chak Tou Leong",
      "Yifei Ke",
      "Joseph Liu",
      "Yiwen Yuan",
      "Julian McAuley",
      "Li-jia Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Protecting_Your_Video_Content_Disrupting_Automated_Video-based_LLM_Annotations_CVPR_2025_paper.html": {
    "title": "Protecting Your Video Content: Disrupting Automated Video-based LLM Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haitong Liu",
      "Kuofeng Gao",
      "Yang Bai",
      "Jinmin Li",
      "Jinxiao Shan",
      "Tao Dai",
      "Shu-Tao Xia"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cao_PanDA_Towards_Panoramic_Depth_Anything_with_Unlabeled_Panoramas_and_Mobius_CVPR_2025_paper.html": {
    "title": "PanDA: Towards Panoramic Depth Anything with Unlabeled Panoramas and Mobius Spatial Augmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zidong Cao",
      "Jinjing Zhu",
      "Weiming Zhang",
      "Hao Ai",
      "Haotian Bai",
      "Hengshuang Zhao",
      "Lin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Towards_High-fidelity_3D_Talking_Avatar_with_Personalized_Dynamic_Texture_CVPR_2025_paper.html": {
    "title": "Towards High-fidelity 3D Talking Avatar with Personalized Dynamic Texture",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuanchen Li",
      "Jianyu Wang",
      "Yuhao Cheng",
      "Yikun Zeng",
      "Xingyu Ren",
      "Wenhan Zhu",
      "Weiming Zhao",
      "Yichao Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Scene_Splatter_Momentum_3D_Scene_Generation_from_Single_Image_with_CVPR_2025_paper.html": {
    "title": "Scene Splatter: Momentum 3D Scene Generation from Single Image with Video Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengjun Zhang",
      "Jinzhao Li",
      "Xin Fei",
      "Hao Liu",
      "Yueqi Duan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_JiSAM_Alleviate_Labeling_Burden_and_Corner_Case_Problems_in_Autonomous_CVPR_2025_paper.html": {
    "title": "JiSAM: Alleviate Labeling Burden and Corner Case Problems in Autonomous Driving via Minimal Real-World Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Runjian Chen",
      "Wenqi Shao",
      "Bo Zhang",
      "Shaoshuai Shi",
      "Li Jiang",
      "Ping Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_OSMamba_Omnidirectional_Spectral_Mamba_with_Dual-Domain_Prior_Generator_for_Exposure_CVPR_2025_paper.html": {
    "title": "OSMamba: Omnidirectional Spectral Mamba with Dual-Domain Prior Generator for Exposure Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gehui Li",
      "Bin Chen",
      "Chen Zhao",
      "Lei Zhang",
      "Jian Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cao_Image_is_All_You_Need_to_Empower_Large-scale_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Image is All You Need to Empower Large-scale Diffusion Models for In-Domain Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pu Cao",
      "Feng Zhou",
      "Lu Yang",
      "Tianrui Huang",
      "Qing Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_MedUnifier_Unifying_Vision-and-Language_Pre-training_on_Medical_Data_with_Vision_Generation_CVPR_2025_paper.html": {
    "title": "MedUnifier: Unifying Vision-and-Language Pre-training on Medical Data with Vision Generation Task using Discrete Visual Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Zhang",
      "Yang Yu",
      "Yucheng Chen",
      "Xulei Yang",
      "Si Yong Yeo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pang_RandAR_Decoder-only_Autoregressive_Visual_Generation_in_Random_Orders_CVPR_2025_paper.html": {
    "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziqi Pang",
      "Tianyuan Zhang",
      "Fujun Luan",
      "Yunze Man",
      "Hao Tan",
      "Kai Zhang",
      "William T. Freeman",
      "Yu-Xiong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_Evolving_High-Quality_Rendering_and_Reconstruction_in_a_Unified_Framework_with_CVPR_2025_paper.html": {
    "title": "Evolving High-Quality Rendering and Reconstruction in a Unified Framework with Contribution-Adaptive Regularization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "You Shen",
      "Zhipeng Zhang",
      "Xinyang Li",
      "Yansong Qu",
      "Yu Lin",
      "Shengchuan Zhang",
      "Liujuan Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_ArticulatedGS_Self-supervised_Digital_Twin_Modeling_of_Articulated_Objects_using_3D_CVPR_2025_paper.html": {
    "title": "ArticulatedGS: Self-supervised Digital Twin Modeling of Articulated Objects using 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junfu Guo",
      "Yu Xin",
      "Gaoyi Liu",
      "Kai Xu",
      "Ligang Liu",
      "Ruizhen Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dai_NoiseCtrl_A_Sampling-Algorithm-Agnostic_Conditional_Generation_Method_for_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "NoiseCtrl: A Sampling-Algorithm-Agnostic Conditional Generation Method for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Longquan Dai",
      "He Wang",
      "Jinhui Tang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Seo_Leveraging_3D_Geometric_Priors_in_2D_Rotation_Symmetry_Detection_CVPR_2025_paper.html": {
    "title": "Leveraging 3D Geometric Priors in 2D Rotation Symmetry Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ahyun Seo",
      "Minsu Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_KMD_Koopman_Multi-modality_Decomposition_for_Generalized_Brain_Tumor_Segmentation_under_CVPR_2025_paper.html": {
    "title": "KMD: Koopman Multi-modality Decomposition for Generalized Brain Tumor Segmentation under Incomplete Modalities",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Liu",
      "Haochuan Jiang",
      "Kaizhu Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Vid2Sim_Realistic_and_Interactive_Simulation_from_Video_for_Urban_Navigation_CVPR_2025_paper.html": {
    "title": "Vid2Sim: Realistic and Interactive Simulation from Video for Urban Navigation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyang Xie",
      "Zhizheng Liu",
      "Zhenghao Peng",
      "Wayne Wu",
      "Bolei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DORNet_A_Degradation_Oriented_and_Regularized_Network_for_Blind_Depth_CVPR_2025_paper.html": {
    "title": "DORNet: A Degradation Oriented and Regularized Network for Blind Depth Super-Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengxue Wang",
      "Zhiqiang Yan",
      "Jinshan Pan",
      "Guangwei Gao",
      "Kai Zhang",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Alexandridis_Fractal_Calibration_for_Long-tailed_Object_Detection_CVPR_2025_paper.html": {
    "title": "Fractal Calibration for Long-tailed Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Konstantinos Panagiotis Alexandridis",
      "Ismail Elezi",
      "Jiankang Deng",
      "Anh Nguyen",
      "Shan Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_M3GYM_A_Large-Scale_Multimodal_Multi-view_Multi-person_Pose_Dataset_for_Fitness_CVPR_2025_paper.html": {
    "title": "M3GYM: A Large-Scale Multimodal Multi-view Multi-person Pose Dataset for Fitness Activity Understanding in Real-world Settings",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingzheng Xu",
      "Ru Cao",
      "Xin Shen",
      "Heming Du",
      "Sen Wang",
      "Xin Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Noise_Calibration_and_Spatial-Frequency_Interactive_Network_for_STEM_Image_Enhancement_CVPR_2025_paper.html": {
    "title": "Noise Calibration and Spatial-Frequency Interactive Network for STEM Image Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hesong Li",
      "Ziqi Wu",
      "Ruiwen Shao",
      "Tao Zhang",
      "Ying Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shimoda_Type-R_Automatically_Retouching_Typos_for_Text-to-Image_Generation_CVPR_2025_paper.html": {
    "title": "Type-R: Automatically Retouching Typos for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wataru Shimoda",
      "Naoto Inoue",
      "Daichi Haraguchi",
      "Hayato Mitani",
      "Seiichi Uchida",
      "Kota Yamaguchi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_Video-3D_LLM_Learning_Position-Aware_Video_Representation_for_3D_Scene_Understanding_CVPR_2025_paper.html": {
    "title": "Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Duo Zheng",
      "Shijia Huang",
      "Liwei Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tan_FedSPA_Generalizable_Federated_Graph_Learning_under_Homophily_Heterogeneity_CVPR_2025_paper.html": {
    "title": "FedSPA: Generalizable Federated Graph Learning under Homophily Heterogeneity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Tan",
      "Guancheng Wan",
      "Wenke Huang",
      "He Li",
      "Guibin Zhang",
      "Carl Yang",
      "Mang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Homogeneous_Dynamics_Space_for_Heterogeneous_Humans_CVPR_2025_paper.html": {
    "title": "Homogeneous Dynamics Space for Heterogeneous Humans",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinpeng Liu",
      "Junxuan Liang",
      "Chenshuo Zhang",
      "Zixuan Cai",
      "Cewu Lu",
      "Yong-Lu Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jung_TailedCore_Few-Shot_Sampling_for_Unsupervised_Long-Tail_Noisy_Anomaly_Detection_CVPR_2025_paper.html": {
    "title": "TailedCore: Few-Shot Sampling for Unsupervised Long-Tail Noisy Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yoon Gyo Jung",
      "Jaewoo Park",
      "Jaeho Yoon",
      "Kuan-Chuan Peng",
      "Wonchul Kim",
      "Andrew Beng Jin Teoh",
      "Octavia Camps"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bao_GazeGene_Large-scale_Synthetic_Gaze_Dataset_with_3D_Eyeball_Annotations_CVPR_2025_paper.html": {
    "title": "GazeGene: Large-scale Synthetic Gaze Dataset with 3D Eyeball Annotations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiwei Bao",
      "Zhiming Wang",
      "Feng Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Koo_VideoHandles_Editing_3D_Object_Compositions_in_Videos_Using_Video_Generative_CVPR_2025_paper.html": {
    "title": "VideoHandles: Editing 3D Object Compositions in Videos Using Video Generative Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Juil Koo",
      "Paul Guerrero",
      "Chun-Hao P. Huang",
      "Duygu Ceylan",
      "Minhyuk Sung"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tu_Satellite_Observations_Guided_Diffusion_Model_for_Accurate_Meteorological_States_at_CVPR_2025_paper.html": {
    "title": "Satellite Observations Guided Diffusion Model for Accurate Meteorological States at Arbitrary Resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Siwei Tu",
      "Ben Fei",
      "Weidong Yang",
      "Fenghua Ling",
      "Hao Chen",
      "Zili Liu",
      "Kun Chen",
      "Hang Fan",
      "Wanli Ouyang",
      "Lei Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Muller_Reconstructing_People_Places_and_Cameras_CVPR_2025_paper.html": {
    "title": "Reconstructing People, Places, and Cameras",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lea M√ºller",
      "Hongsuk Choi",
      "Anthony Zhang",
      "Brent Yi",
      "Jitendra Malik",
      "Angjoo Kanazawa"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_InPO_Inversion_Preference_Optimization_with_Reparametrized_DDIM_for_Efficient_Diffusion_CVPR_2025_paper.html": {
    "title": "InPO: Inversion Preference Optimization with Reparametrized DDIM for Efficient Diffusion Model Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunhong Lu",
      "Qichao Wang",
      "Hengyuan Cao",
      "Xierui Wang",
      "Xiaoyin Xu",
      "Min Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_GaussTR_Foundation_Model-Aligned_Gaussian_Transformer_for_Self-Supervised_3D_Spatial_Understanding_CVPR_2025_paper.html": {
    "title": "GaussTR: Foundation Model-Aligned Gaussian Transformer for Self-Supervised 3D Spatial Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyi Jiang",
      "Liu Liu",
      "Tianheng Cheng",
      "Xinjie Wang",
      "Tianwei Lin",
      "Zhizhong Su",
      "Wenyu Liu",
      "Xinggang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Single_Domain_Generalization_for_Few-Shot_Counting_via_Universal_Representation_Matching_CVPR_2025_paper.html": {
    "title": "Single Domain Generalization for Few-Shot Counting via Universal Representation Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xianing Chen",
      "Si Huo",
      "Borui Jiang",
      "Hailin Hu",
      "Xinghao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ke_Discovering_Hidden_Visual_Concepts_Beyond_Linguistic_Input_in_Infant_Learning_CVPR_2025_paper.html": {
    "title": "Discovering Hidden Visual Concepts Beyond Linguistic Input in Infant Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xueyi Ke",
      "Satoshi Tsutsui",
      "Yayun Zhang",
      "Bihan Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Teney_Do_We_Always_Need_the_Simplicity_Bias_Looking_for_Optimal_CVPR_2025_paper.html": {
    "title": "Do We Always Need the Simplicity Bias? Looking for Optimal Inductive Biases in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Damien Teney",
      "Liangze Jiang",
      "Florin Gogianu",
      "Ehsan Abbasnejad"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_A_General_Adaptive_Dual-level_Weighting_Mechanism_for_Remote_Sensing_Pansharpening_CVPR_2025_paper.html": {
    "title": "A General Adaptive Dual-level Weighting Mechanism for Remote Sensing Pansharpening",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Huang",
      "Haorui Chen",
      "Jiaxuan Ren",
      "Siran Peng",
      "Liangjian Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Debnath_RASP_Revisiting_3D_Anamorphic_Art_for_Shadow-Guided_Packing_of_Irregular_CVPR_2025_paper.html": {
    "title": "RASP: Revisiting 3D Anamorphic Art for Shadow-Guided Packing of Irregular Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soumyaratna Debnath",
      "Ashish Tiwari",
      "Kaustubh Sadekar",
      "Shanmuganathan Raman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chai_Identifying_and_Mitigating_Spurious_Correlation_in_Multi-Task_Learning_CVPR_2025_paper.html": {
    "title": "Identifying and Mitigating Spurious Correlation in Multi-Task Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyi Chai",
      "Shenyu Lu",
      "Xiaoqian Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Baumann_Continuous_Subject-Specific_Attribute_Control_in_T2I_Models_by_Identifying_Semantic_CVPR_2025_paper.html": {
    "title": "Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Stefan Andreas Baumann",
      "Felix Krause",
      "Michael Neumayr",
      "Nick Stracke",
      "Melvin Sevi",
      "Vincent Tao Hu",
      "Bj√∂rn Ommer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Diffusion_Bridge_Leveraging_Diffusion_Model_to_Reduce_the_Modality_Gap_CVPR_2025_paper.html": {
    "title": "Diffusion Bridge: Leveraging Diffusion Model to Reduce the Modality Gap Between Text and Vision for Zero-Shot Image Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jeong Ryong Lee",
      "Yejee Shin",
      "Geonhui Son",
      "Dosik Hwang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MODfinity_Unsupervised_Domain_Adaptation_with_Multimodal_Information_Flow_Intertwining_CVPR_2025_paper.html": {
    "title": "MODfinity: Unsupervised Domain Adaptation with Multimodal Information Flow Intertwining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shanglin Liu",
      "Jianming Lv",
      "Jingdan Kang",
      "Huaidong Zhang",
      "Zequan Liang",
      "Shengfeng He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rao_Towards_Universal_Soccer_Video_Understanding_CVPR_2025_paper.html": {
    "title": "Towards Universal Soccer Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayuan Rao",
      "Haoning Wu",
      "Hao Jiang",
      "Ya Zhang",
      "Yanfeng Wang",
      "Weidi Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Renz_SimLingo_Vision-Only_Closed-Loop_Autonomous_Driving_with_Language-Action_Alignment_CVPR_2025_paper.html": {
    "title": "SimLingo: Vision-Only Closed-Loop Autonomous Driving with Language-Action Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Katrin Renz",
      "Long Chen",
      "Elahe Arani",
      "Oleg Sinavski"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Improved_Video_VAE_for_Latent_Video_Diffusion_Model_CVPR_2025_paper.html": {
    "title": "Improved Video VAE for Latent Video Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pingyu Wu",
      "Kai Zhu",
      "Yu Liu",
      "Liming Zhao",
      "Wei Zhai",
      "Yang Cao",
      "Zheng-Jun Zha"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ghosal_Immune_Improving_Safety_Against_Jailbreaks_in_Multi-modal_LLMs_via_Inference-Time_CVPR_2025_paper.html": {
    "title": "Immune: Improving Safety Against Jailbreaks in Multi-modal LLMs via Inference-Time Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Soumya Suvra Ghosal",
      "Souradip Chakraborty",
      "Vaibhav Singh",
      "Tianrui Guan",
      "Mengdi Wang",
      "Ahmad Beirami",
      "Furong Huang",
      "Alvaro Velasquez",
      "Dinesh Manocha",
      "Amrit Singh Bedi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_Efficient_Video_Super-Resolution_for_Real-time_Rendering_with_Decoupled_G-buffer_Guidance_CVPR_2025_paper.html": {
    "title": "Efficient Video Super-Resolution for Real-time Rendering with Decoupled G-buffer Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingjun Zheng",
      "Long Sun",
      "Jiangxin Dong",
      "Jinshan Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_CustomKD_Customizing_Large_Vision_Foundation_for_Edge_Model_Improvement_via_CVPR_2025_paper.html": {
    "title": "CustomKD: Customizing Large Vision Foundation for Edge Model Improvement via Knowledge Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jungsoo Lee",
      "Debasmit Das",
      "Munawar Hayat",
      "Sungha Choi",
      "Kyuwoong Hwang",
      "Fatih Porikli"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Enhancing_Privacy-Utility_Trade-offs_to_Mitigate_Memorization_in_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chen Chen",
      "Daochang Liu",
      "Mubarak Shah",
      "Chang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Learned_Image_Compression_with_Dictionary-based_Entropy_Model_CVPR_2025_paper.html": {
    "title": "Learned Image Compression with Dictionary-based Entropy Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingbo Lu",
      "Leheng Zhang",
      "Xingyu Zhou",
      "Mu Li",
      "Wen Li",
      "Shuhang Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pei_PMNI_Pose-free_Multi-view_Normal_Integration_for_Reflective_and_Textureless_Surface_CVPR_2025_paper.html": {
    "title": "PMNI: Pose-free Multi-view Normal Integration for Reflective and Textureless Surface Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingzhi Pei",
      "Xu Cao",
      "Xiangyi Wang",
      "Heng Guo",
      "Zhanyu Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_NVComposer_Boosting_Generative_Novel_View_Synthesis_with_Multiple_Sparse_and_CVPR_2025_paper.html": {
    "title": "NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lingen Li",
      "Zhaoyang Zhang",
      "Yaowei Li",
      "Jiale Xu",
      "Wenbo Hu",
      "Xiaoyu Li",
      "Weihao Cheng",
      "Jinwei Gu",
      "Tianfan Xue",
      "Ying Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_LeanGaussian_Breaking_Pixel_or_Point_Cloud_Correspondence_in_Modeling_3D_CVPR_2025_paper.html": {
    "title": "LeanGaussian: Breaking Pixel or Point Cloud Correspondence in Modeling 3D Gaussians",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiamin Wu",
      "Kenkun Liu",
      "Han Gao",
      "Xiaoke Jiang",
      "Yuan Yao",
      "Lei Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Modeling_Multiple_Normal_Action_Representations_for_Error_Detection_in_Procedural_CVPR_2025_paper.html": {
    "title": "Modeling Multiple Normal Action Representations for Error Detection in Procedural Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei-Jin Huang",
      "Yuan-Ming Li",
      "Zhi-Wei Xia",
      "Yu-Ming Tang",
      "Kun-Yu Lin",
      "Jian-Fang Hu",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Seo_Efficient_Personalization_of_Quantized_Diffusion_Model_without_Backpropagation_CVPR_2025_paper.html": {
    "title": "Efficient Personalization of Quantized Diffusion Model without Backpropagation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hoigi Seo",
      "Wongi Jeong",
      "Kyungryeol Lee",
      "Se Young Chun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Alias-Free_Latent_Diffusion_Models_Improving_Fractional_Shift_Equivariance_of_Diffusion_CVPR_2025_paper.html": {
    "title": "Alias-Free Latent Diffusion Models: Improving Fractional Shift Equivariance of Diffusion Latent Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Zhou",
      "Zeqi Xiao",
      "Shuai Yang",
      "Xingang Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Akshay_A_Unified_Latent_Schrodinger_Bridge_Diffusion_Model_for_Unsupervised_Anomaly_CVPR_2025_paper.html": {
    "title": "A Unified Latent Schrodinger Bridge Diffusion Model for Unsupervised Anomaly Detection and Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shilhora Akshay",
      "Niveditha Lakshmi Narasimhan",
      "Jacob George",
      "Vineeth N Balasubramanian"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qu_KVQ_Boosting_Video_Quality_Assessment_via_Saliency-guided_Local_Perception_CVPR_2025_paper.html": {
    "title": "KVQ: Boosting Video Quality Assessment via Saliency-guided Local Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunpeng Qu",
      "Kun Yuan",
      "Qizhi Xie",
      "Ming Sun",
      "Chao Zhou",
      "Jian Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hatamizadeh_MambaVision_A_Hybrid_Mamba-Transformer_Vision_Backbone_CVPR_2025_paper.html": {
    "title": "MambaVision: A Hybrid Mamba-Transformer Vision Backbone",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ali Hatamizadeh",
      "Jan Kautz"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Learning_Flow_Fields_in_Attention_for_Controllable_Person_Image_Generation_CVPR_2025_paper.html": {
    "title": "Learning Flow Fields in Attention for Controllable Person Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijian Zhou",
      "Shikun Liu",
      "Xiao Han",
      "Haozhe Liu",
      "Kam Woh Ng",
      "Tian Xie",
      "Yuren Cong",
      "Hang Li",
      "Mengmeng Xu",
      "Juan-Manuel Perez-Rua",
      "Aditya Patel",
      "Tao Xiang",
      "Miaojing Shi",
      "Sen He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Duan_Multi-Label_Prototype_Visual_Spatial_Search_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Multi-Label Prototype Visual Spatial Search for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songsong Duan",
      "Xi Yang",
      "Nannan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Whalen_Early-Bird_Diffusion_Investigating_and_Leveraging_Timestep-Aware_Early-Bird_Tickets_in_Diffusion_CVPR_2025_paper.html": {
    "title": "Early-Bird Diffusion: Investigating and Leveraging Timestep-Aware Early-Bird Tickets in Diffusion Models for Efficient Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lexington Whalen",
      "Zhenbang Du",
      "Haoran You",
      "Chaojian Li",
      "Sixu Li",
      "Yingyan Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_FireEdit_Fine-grained_Instruction-based_Image_Editing_via_Region-aware_Vision_Language_Model_CVPR_2025_paper.html": {
    "title": "FireEdit: Fine-grained Instruction-based Image Editing via Region-aware Vision Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Zhou",
      "Jiahao Li",
      "Zunnan Xu",
      "Hanhui Li",
      "Yiji Cheng",
      "Fa-Ting Hong",
      "Qin Lin",
      "Qinglin Lu",
      "Xiaodan Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiangli_Doppelgangers_Improved_Visual_Disambiguation_with_Geometric_3D_Features_CVPR_2025_paper.html": {
    "title": "Doppelgangers++: Improved Visual Disambiguation with Geometric 3D Features",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuanbo Xiangli",
      "Ruojin Cai",
      "Hanyu Chen",
      "Jeffrey Byrne",
      "Noah Snavely"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_Learnable_Infinite_Taylor_Gaussian_for_Dynamic_View_Rendering_CVPR_2025_paper.html": {
    "title": "Learnable Infinite Taylor Gaussian for Dynamic View Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bingbing Hu",
      "Yanyan Li",
      "Rui Xie",
      "Bo Xu",
      "Haoye Dong",
      "Junfeng Yao",
      "Gim Hee Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yi_DL2G_Degradation-guided_Local-to-Global_Restoration_for_Eyeglass_Reflection_Removal_CVPR_2025_paper.html": {
    "title": "DL2G: Degradation-guided Local-to-Global Restoration for Eyeglass Reflection Removal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhilv Yi",
      "Xiao Lu",
      "Hong Ding",
      "Jingbo Hu",
      "Zhi Jiang",
      "Chunxia Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mur-Labadia_DIV-FF_Dynamic_Image-Video_Feature_Fields_For_Environment_Understanding_in_Egocentric_CVPR_2025_paper.html": {
    "title": "DIV-FF: Dynamic Image-Video Feature Fields For Environment Understanding in Egocentric Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lorenzo Mur-Labadia",
      "Josechu Guerrero",
      "Ruben Martinez-Cantin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_SaMam_Style-aware_State_Space_Model_for_Arbitrary_Image_Style_Transfer_CVPR_2025_paper.html": {
    "title": "SaMam: Style-aware State Space Model for Arbitrary Image Style Transfer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongda Liu",
      "Longguang Wang",
      "Ye Zhang",
      "Ziru Yu",
      "Yulan Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_MFogHub_Bridging_Multi-Regional_and_Multi-Satellite_Data_for_Global_Marine_Fog_CVPR_2025_paper.html": {
    "title": "MFogHub: Bridging Multi-Regional and Multi-Satellite Data for Global Marine Fog Detection and Forecasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mengqiu Xu",
      "Kaixin Chen",
      "Heng Guo",
      "Yixiang Huang",
      "Ming Wu",
      "Zhenwei Shi",
      "Chuang Zhang",
      "Jun Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/George_The_Illusion_of_Unlearning_The_Unstable_Nature_of_Machine_Unlearning_CVPR_2025_paper.html": {
    "title": "The Illusion of Unlearning: The Unstable Nature of Machine Unlearning in Text-to-Image Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Naveen George",
      "Karthik Nandan Dasaraju",
      "Rutheesh Reddy Chittepu",
      "Konda Reddy Mopuri"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mao_Making_Old_Film_Great_Again_Degradation-aware_State_Space_Model_for_CVPR_2025_paper.html": {
    "title": "Making Old Film Great Again: Degradation-aware State Space Model for Old Film Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yudong Mao",
      "Hao Luo",
      "Zhiwei Zhong",
      "Peilin Chen",
      "Zhijiang Zhang",
      "Shiqi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qiu_Leveraging_Global_Stereo_Consistency_for_Category-Level_Shape_and_6D_Pose_CVPR_2025_paper.html": {
    "title": "Leveraging Global Stereo Consistency for Category-Level Shape and 6D Pose Estimation from Stereo Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junning Qiu",
      "Minglei Lu",
      "Fei Wang",
      "Yu Guo",
      "Yonggen Ling"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_AlphaPre_Amplitude-Phase_Disentanglement_Model_for_Precipitation_Nowcasting_CVPR_2025_paper.html": {
    "title": "AlphaPre: Amplitude-Phase Disentanglement Model for Precipitation Nowcasting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kenghong Lin",
      "Baoquan Zhang",
      "Demin Yu",
      "Wenzhi Feng",
      "Shidong Chen",
      "Feifan Gao",
      "Xutao Li",
      "Yunming Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_EfficientLLaVA_Generalizable_Auto-Pruning_for_Large_Vision-language_Models_CVPR_2025_paper.html": {
    "title": "EfficientLLaVA: Generalizable Auto-Pruning for Large Vision-language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yinan Liang",
      "Ziwei Wang",
      "Xiuwei Xu",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fang_Detection-Friendly_Nonuniformity_Correction_A_Union_Framework_for_Infrared_UAV_Target_CVPR_2025_paper.html": {
    "title": "Detection-Friendly Nonuniformity Correction: A Union Framework for Infrared UAV Target Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Houzhang Fang",
      "Xiaolin Wang",
      "Zengyang Li",
      "Lu Wang",
      "Qingshan Li",
      "Yi Chang",
      "Luxin Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Articulated_Kinematics_Distillation_from_Video_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Articulated Kinematics Distillation from Video Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuan Li",
      "Qianli Ma",
      "Tsung-Yi Lin",
      "Yongxin Chen",
      "Chenfanfu Jiang",
      "Ming-Yu Liu",
      "Donglai Xiang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ashutosh_ExpertAF_Expert_Actionable_Feedback_from_Video_CVPR_2025_paper.html": {
    "title": "ExpertAF: Expert Actionable Feedback from Video",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kumar Ashutosh",
      "Tushar Nagarajan",
      "Georgios Pavlakos",
      "Kris Kitani",
      "Kristen Grauman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pataki_MP-SfM_Monocular_Surface_Priors_for_Robust_Structure-from-Motion_CVPR_2025_paper.html": {
    "title": "MP-SfM: Monocular Surface Priors for Robust Structure-from-Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zador Pataki",
      "Paul-Edouard Sarlin",
      "Johannes L. Sch√∂nberger",
      "Marc Pollefeys"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_OnlineAnySeg_Online_Zero-Shot_3D_Segmentation_by_Visual_Foundation_Model_Guided_CVPR_2025_paper.html": {
    "title": "OnlineAnySeg: Online Zero-Shot 3D Segmentation by Visual Foundation Model Guided 2D Mask Merging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijie Tang",
      "Jiazhao Zhang",
      "Yuqing Lan",
      "Yulan Guo",
      "Dezun Dong",
      "Chenyang Zhu",
      "Kai Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Tora_Trajectory-oriented_Diffusion_Transformer_for_Video_Generation_CVPR_2025_paper.html": {
    "title": "Tora: Trajectory-oriented Diffusion Transformer for Video Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenghao Zhang",
      "Junchao Liao",
      "Menghao Li",
      "ZuoZhuo Dai",
      "Bingxue Qiu",
      "Siyu Zhu",
      "Long Qin",
      "Weizhi Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Talegaonkar_Volumetrically_Consistent_3D_Gaussian_Rasterization_CVPR_2025_paper.html": {
    "title": "Volumetrically Consistent 3D Gaussian Rasterization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chinmay Talegaonkar",
      "Yash Belhe",
      "Ravi Ramamoorthi",
      "Nicholas Antipa"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hua_Deterministic-to-Stochastic_Diverse_Latent_Feature_Mapping_for_Human_Motion_Synthesis_CVPR_2025_paper.html": {
    "title": "Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu Hua",
      "Weiming Liu",
      "Gui Xu",
      "Yaqing Hou",
      "Yew-Soon Ong",
      "Qiang Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wynn_Morpheus_Text-Driven_3D_Gaussian_Splat_Shape_and_Color_Stylization_CVPR_2025_paper.html": {
    "title": "Morpheus: Text-Driven 3D Gaussian Splat Shape and Color Stylization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jamie Wynn",
      "Zawar Qureshi",
      "Jakub Powierza",
      "Jamie Watson",
      "Mohamed Sayed"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_CacheQuant_Comprehensively_Accelerated_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "CacheQuant: Comprehensively Accelerated Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuewen Liu",
      "Zhikai Li",
      "Qingyi Gu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nordstrom_The_Impact_Label_Noise_and_Choice_of_Threshold_has_on_CVPR_2025_paper.html": {
    "title": "The Impact Label Noise and Choice of Threshold has on Cross-Entropy and Soft-Dice in Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Marcus Nordstr√∂m",
      "Atsuto Maki",
      "Henrik Hult"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Open-World_Objectness_Modeling_Unifies_Novel_Object_Detection_CVPR_2025_paper.html": {
    "title": "Open-World Objectness Modeling Unifies Novel Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shan Zhang",
      "Yao Ni",
      "Jinhao Du",
      "Yuan Xue",
      "Philip Torr",
      "Piotr Koniusz",
      "Anton van den Hengel"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiong_LLaVA-Critic_Learning_to_Evaluate_Multimodal_Models_CVPR_2025_paper.html": {
    "title": "LLaVA-Critic: Learning to Evaluate Multimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyi Xiong",
      "Xiyao Wang",
      "Dong Guo",
      "Qinghao Ye",
      "Haoqi Fan",
      "Quanquan Gu",
      "Heng Huang",
      "Chunyuan Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Nath_VILA-M3_Enhancing_Vision-Language_Models_with_Medical_Expert_Knowledge_CVPR_2025_paper.html": {
    "title": "VILA-M3: Enhancing Vision-Language Models with Medical Expert Knowledge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vishwesh Nath",
      "Wenqi Li",
      "Dong Yang",
      "Andriy Myronenko",
      "Mingxin Zheng",
      "Yao Lu",
      "Zhijian Liu",
      "Hongxu Yin",
      "Yee Man Law",
      "Yucheng Tang",
      "Pengfei Guo",
      "Can Zhao",
      "Ziyue Xu",
      "Yufan He",
      "Stephanie Harmon",
      "Benjamin Simon",
      "Greg Heinrich",
      "Stephen Aylward",
      "Marc Edgar",
      "Michael Zephyr",
      "Pavlo Molchanov",
      "Baris Turkbey",
      "Holger Roth",
      "Daguang Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Repurposing_Pre-trained_Video_Diffusion_Models_for_Event-based_Video_Interpolation_CVPR_2025_paper.html": {
    "title": "Repurposing Pre-trained Video Diffusion Models for Event-based Video Interpolation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingxi Chen",
      "Brandon Y. Feng",
      "Haoming Cai",
      "Tianfu Wang",
      "Levi Burner",
      "Dehao Yuan",
      "Cornelia Fermuller",
      "Christopher A. Metzler",
      "Yiannis Aloimonos"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ren_MotionPRO_Exploring_the_Role_of_Pressure_in_Human_MoCap_and_CVPR_2025_paper.html": {
    "title": "MotionPRO: Exploring the Role of Pressure in Human MoCap and Beyond",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shenghao Ren",
      "Yi Lu",
      "Jiayi Huang",
      "Jiayi Zhao",
      "He Zhang",
      "Tao Yu",
      "Qiu Shen",
      "Xun Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_DiffVsgg_Diffusion-Driven_Online_Video_Scene_Graph_Generation_CVPR_2025_paper.html": {
    "title": "DiffVsgg: Diffusion-Driven Online Video Scene Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mu Chen",
      "Liulei Li",
      "Wenguan Wang",
      "Yi Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Large-scale_Multi-view_Tensor_Clustering_with_Implicit_Linear_Kernels_CVPR_2025_paper.html": {
    "title": "Large-scale Multi-view Tensor Clustering with Implicit Linear Kernels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiyuan Liu",
      "Xinwang Liu",
      "Chuankun Li",
      "Xinhang Wan",
      "Hao Tan",
      "Yi Zhang",
      "Weixuan Liang",
      "Qian Qu",
      "Yu Feng",
      "Renxiang Guan",
      "Ke Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_Generalized_Diffusion_Detector_Mining_Robust_Features_from_Diffusion_Models_for_CVPR_2025_paper.html": {
    "title": "Generalized Diffusion Detector: Mining Robust Features from Diffusion Models for Domain-Generalized Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyong He",
      "Yuxiang Ji",
      "Qianwen Ye",
      "Zhuoyue Tan",
      "Liaoni Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Q-Eval-100K_Evaluating_Visual_Quality_and_Alignment_Level_for_Text-to-Vision_Content_CVPR_2025_paper.html": {
    "title": "Q-Eval-100K: Evaluating Visual Quality and Alignment Level for Text-to-Vision Content",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zicheng Zhang",
      "Tengchuan Kou",
      "Shushi Wang",
      "Chunyi Li",
      "Wei Sun",
      "Wei Wang",
      "Xiaoyu Li",
      "Zongyu Wang",
      "Xuezhi Cao",
      "Xiongkuo Min",
      "Xiaohong Liu",
      "Guangtao Zhai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fu_Dual_Focus-Attention_Transformer_for_Robust_Point_Cloud_Registration_CVPR_2025_paper.html": {
    "title": "Dual Focus-Attention Transformer for Robust Point Cloud Registration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kexue Fu",
      "Mingzhi Yuan",
      "Changwei Wang",
      "Weiguang Pang",
      "Jing Chi",
      "Manning Wang",
      "Longxiang Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Forming_Auxiliary_High-confident_Instance-level_Loss_to_Promote_Learning_from_Label_CVPR_2025_paper.html": {
    "title": "Forming Auxiliary High-confident Instance-level Loss to Promote Learning from Label Proportions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianhao Ma",
      "Han Chen",
      "Juncheng Hu",
      "Yungang Zhu",
      "Ximing Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xue_Progress-Aware_Video_Frame_Captioning_CVPR_2025_paper.html": {
    "title": "Progress-Aware Video Frame Captioning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihui Xue",
      "Joungbin An",
      "Xitong Yang",
      "Kristen Grauman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_SMTPD_A_New_Benchmark_for_Temporal_Prediction_of_Social_Media_CVPR_2025_paper.html": {
    "title": "SMTPD: A New Benchmark for Temporal Prediction of Social Media Popularity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yijie Xu",
      "Bolun Zheng",
      "Wei Zhu",
      "Hangjia Pan",
      "Yuchen Yao",
      "Ning Xu",
      "Anan Liu",
      "Quan Zhang",
      "Chenggang Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Enhancing_Dance-to-Music_Generation_via_Negative_Conditioning_Latent_Diffusion_Model_CVPR_2025_paper.html": {
    "title": "Enhancing Dance-to-Music Generation via Negative Conditioning Latent Diffusion Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Changchang Sun",
      "Gaowen Liu",
      "Charles Fleming",
      "Yan Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sharan_Neuro-Symbolic_Evaluation_of_Text-to-Video_Models_using_Formal_Verification_CVPR_2025_paper.html": {
    "title": "Neuro-Symbolic Evaluation of Text-to-Video Models using Formal Verification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "S P Sharan",
      "Minkyu Choi",
      "Sahil Shah",
      "Harsh Goel",
      "Mohammad Omama",
      "Sandeep Chinchali"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Spherical_Manifold_Guided_Diffusion_Model_for_Panoramic_Image_Generation_CVPR_2025_paper.html": {
    "title": "Spherical Manifold Guided Diffusion Model for Panoramic Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiancheng Sun",
      "Mai Xu",
      "Shengxi Li",
      "Senmao Ma",
      "Xin Deng",
      "Lai Jiang",
      "Gang Shen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Horwitz_Learning_on_Model_Weights_using_Tree_Experts_CVPR_2025_paper.html": {
    "title": "Learning on Model Weights using Tree Experts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eliahu Horwitz",
      "Bar Cavia",
      "Jonathan Kahana",
      "Yedid Hoshen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Rethinking_Query-based_Transformer_for_Continual_Image_Segmentation_CVPR_2025_paper.html": {
    "title": "Rethinking Query-based Transformer for Continual Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchen Zhu",
      "Cheng Shi",
      "Dingyou Wang",
      "Jiajin Tang",
      "Zhengxuan Wei",
      "Yu Wu",
      "Guanbin Li",
      "Sibei Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bharadwaj_Image_Reconstruction_from_Readout-Multiplexed_Single-Photon_Detector_Arrays_CVPR_2025_paper.html": {
    "title": "Image Reconstruction from Readout-Multiplexed Single-Photon Detector Arrays",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shashwath Bharadwaj",
      "Ruangrawee Kitichotkul",
      "Akshay Agarwal",
      "Vivek K Goyal"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Towards_Smart_Point-and-Shoot_Photography_CVPR_2025_paper.html": {
    "title": "Towards Smart Point-and-Shoot Photography",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiawan Li",
      "Fei Zhou",
      "Zhipeng Zhong",
      "Jiongzhi Lin",
      "Guoping Qiu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_SlideChat_A_Large_Vision-Language_Assistant_for_Whole-Slide_Pathology_Image_Understanding_CVPR_2025_paper.html": {
    "title": "SlideChat: A Large Vision-Language Assistant for Whole-Slide Pathology Image Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ying Chen",
      "Guoan Wang",
      "Yuanfeng Ji",
      "Yanjun Li",
      "Jin Ye",
      "Tianbin Li",
      "Ming Hu",
      "Rongshan Yu",
      "Yu Qiao",
      "Junjun He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_Prototype-Based_Image_Prompting_for_Weakly_Supervised_Histopathological_Image_Segmentation_CVPR_2025_paper.html": {
    "title": "Prototype-Based Image Prompting for Weakly Supervised Histopathological Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qingchen Tang",
      "Lei Fan",
      "Maurice Pagnucco",
      "Yang Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Towards_Transformer-Based_Aligned_Generation_with_Self-Coherence_Guidance_CVPR_2025_paper.html": {
    "title": "Towards Transformer-Based Aligned Generation with Self-Coherence Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shulei Wang",
      "Wang Lin",
      "Hai Huang",
      "Hanting Wang",
      "Sihang Cai",
      "WenKang Han",
      "Tao Jin",
      "Jingyuan Chen",
      "Jiacheng Sun",
      "Jieming Zhu",
      "Zhou Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Maracani_Accurate_Scene_Text_Recognition_with_Efficient_Model_Scaling_and_Cloze_CVPR_2025_paper.html": {
    "title": "Accurate Scene Text Recognition with Efficient Model Scaling and Cloze Self-Distillation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Andrea Maracani",
      "Savas Ozkan",
      "Sijun Cho",
      "Hyowon Kim",
      "Eunchung Noh",
      "Jeongwon Min",
      "Cho Jung Min",
      "Dookun Park",
      "Mete Ozay"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Park_DART_Disease-aware_Image-Text_Alignment_and_Self-correcting_Re-alignment_for_Trustworthy_Radiology_CVPR_2025_paper.html": {
    "title": "DART: Disease-aware Image-Text Alignment and Self-correcting Re-alignment for Trustworthy Radiology Report Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sang-Jun Park",
      "Keun-Soo Heo",
      "Dong-Hee Shin",
      "Young-Han Son",
      "Ji-Hye Oh",
      "Tae-Eui Kam"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jung_On_the_Consistency_of_Video_Large_Language_Models_in_Temporal_CVPR_2025_paper.html": {
    "title": "On the Consistency of Video Large Language Models in Temporal Comprehension",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minjoon Jung",
      "Junbin Xiao",
      "Byoung-Tak Zhang",
      "Angela Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Mitigating_the_Human-Robot_Domain_Discrepancy_in_Visual_Pre-training_for_Robotic_CVPR_2025_paper.html": {
    "title": "Mitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiaming Zhou",
      "Teli Ma",
      "Kun-Yu Lin",
      "Zifan Wang",
      "Ronghe Qiu",
      "Junwei Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Qi_Less_is_More_Efficient_Model_Merging_with_Binary_Task_Switch_CVPR_2025_paper.html": {
    "title": "Less is More: Efficient Model Merging with Binary Task Switch",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Biqing Qi",
      "Fangyuan Li",
      "Zhen Wang",
      "Junqi Gao",
      "Dong Li",
      "Peng Ye",
      "Bowen Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dalal_One-Minute_Video_Generation_with_Test-Time_Training_CVPR_2025_paper.html": {
    "title": "One-Minute Video Generation with Test-Time Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Karan Dalal",
      "Daniel Koceja",
      "Jiarui Xu",
      "Yue Zhao",
      "Shihao Han",
      "Ka Chun Cheung",
      "Jan Kautz",
      "Yejin Choi",
      "Yu Sun",
      "Xiaolong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_InteractionMap_Improving_Online_Vectorized_HDMap_Construction_with_Interaction_CVPR_2025_paper.html": {
    "title": "InteractionMap: Improving Online Vectorized HDMap Construction with Interaction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kuang Wu",
      "Chuan Yang",
      "Zhanbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Text-guided_Sparse_Voxel_Pruning_for_Efficient_3D_Visual_Grounding_CVPR_2025_paper.html": {
    "title": "Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxuan Guo",
      "Xiuwei Xu",
      "Ziwei Wang",
      "Jianjiang Feng",
      "Jie Zhou",
      "Jiwen Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cai_ROCKET-1_Mastering_Open-World_Interaction_with_Visual-Temporal_Context_Prompting_CVPR_2025_paper.html": {
    "title": "ROCKET-1: Mastering Open-World Interaction with Visual-Temporal Context Prompting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shaofei Cai",
      "Zihao Wang",
      "Kewei Lian",
      "Zhancun Mu",
      "Xiaojian Ma",
      "Anji Liu",
      "Yitao Liang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sommer_Common3D_Self-Supervised_Learning_of_3D_Morphable_Models_for_Common_Objects_CVPR_2025_paper.html": {
    "title": "Common3D: Self-Supervised Learning of 3D Morphable Models for Common Objects in Neural Feature Space",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leonhard Sommer",
      "Olaf D√ºnkel",
      "Christian Theobalt",
      "Adam Kortylewski"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_RLAIF-V_Open-Source_AI_Feedback_Leads_to_Super_GPT-4V_Trustworthiness_CVPR_2025_paper.html": {
    "title": "RLAIF-V: Open-Source AI Feedback Leads to Super GPT-4V Trustworthiness",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tianyu Yu",
      "Haoye Zhang",
      "Qiming Li",
      "Qixin Xu",
      "Yuan Yao",
      "Da Chen",
      "Xiaoman Lu",
      "Ganqu Cui",
      "Yunkai Dang",
      "Taiwen He",
      "Xiaocheng Feng",
      "Jun Song",
      "Bo Zheng",
      "Zhiyuan Liu",
      "Tat-Seng Chua",
      "Maosong Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_ECVC_Exploiting_Non-Local_Correlations_in_Multiple_Frames_for_Contextual_Video_CVPR_2025_paper.html": {
    "title": "ECVC: Exploiting Non-Local Correlations in Multiple Frames for Contextual Video Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Jiang",
      "Junru Li",
      "Kai Zhang",
      "Li Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_LinGen_Towards_High-Resolution_Minute-Length_Text-to-Video_Generation_with_Linear_Computational_Complexity_CVPR_2025_paper.html": {
    "title": "LinGen: Towards High-Resolution Minute-Length Text-to-Video Generation with Linear Computational Complexity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hongjie Wang",
      "Chih-Yao Ma",
      "Yen-Cheng Liu",
      "Ji Hou",
      "Tao Xu",
      "Jialiang Wang",
      "Felix Juefei-Xu",
      "Yaqiao Luo",
      "Peizhao Zhang",
      "Tingbo Hou",
      "Peter Vajda",
      "Niraj K. Jha",
      "Xiaoliang Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_EditSplat_Multi-View_Fusion_and_Attention-Guided_Optimization_for_View-Consistent_3D_Scene_CVPR_2025_paper.html": {
    "title": "EditSplat: Multi-View Fusion and Attention-Guided Optimization for View-Consistent 3D Scene Editing with 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dong In Lee",
      "Hyeongcheol Park",
      "Jiyoung Seo",
      "Eunbyung Park",
      "Hyunje Park",
      "Ha Dam Baek",
      "Sangheon Shin",
      "Sangmin Kim",
      "Sangpil Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SpatialCLIP_Learning_3D-aware_Image_Representations_from_Spatially_Discriminative_Language_CVPR_2025_paper.html": {
    "title": "SpatialCLIP: Learning 3D-aware Image Representations from Spatially Discriminative Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehan Wang",
      "Sashuai Zhou",
      "Shaoxuan He",
      "Haifeng Huang",
      "Lihe Yang",
      "Ziang Zhang",
      "Xize Cheng",
      "Shengpeng Ji",
      "Tao Jin",
      "Hengshuang Zhao",
      "Zhou Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Mono2Stereo_A_Benchmark_and_Empirical_Study_for_Stereo_Conversion_CVPR_2025_paper.html": {
    "title": "Mono2Stereo: A Benchmark and Empirical Study for Stereo Conversion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songsong Yu",
      "Yuxin Chen",
      "Zhongang Qi",
      "Zeke Xie",
      "Yifan Wang",
      "Lijun Wang",
      "Ying Shan",
      "Huchuan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Towards_Open-Vocabulary_Audio-Visual_Event_Localization_CVPR_2025_paper.html": {
    "title": "Towards Open-Vocabulary Audio-Visual Event Localization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinxing Zhou",
      "Dan Guo",
      "Ruohao Guo",
      "Yuxin Mao",
      "Jingjing Hu",
      "Yiran Zhong",
      "Xiaojun Chang",
      "Meng Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jin_One-shot_3D_Object_Canonicalization_based_on_Geometric_and_Semantic_Consistency_CVPR_2025_paper.html": {
    "title": "One-shot 3D Object Canonicalization based on Geometric and Semantic Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Jin",
      "Yujie Wang",
      "Wenzheng Chen",
      "Qiyu Dai",
      "Qingzhe Gao",
      "Xueying Qin",
      "Baoquan Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Inst3D-LMM_Instance-Aware_3D_Scene_Understanding_with_Multi-modal_Instruction_Tuning_CVPR_2025_paper.html": {
    "title": "Inst3D-LMM: Instance-Aware 3D Scene Understanding with Multi-modal Instruction Tuning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanxun Yu",
      "Wentong Li",
      "Song Wang",
      "Junbo Chen",
      "Jianke Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wan_S2Gaussian_Sparse-View_Super-Resolution_3D_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "S2Gaussian: Sparse-View Super-Resolution 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yecong Wan",
      "Mingwen Shao",
      "Yuanshuo Cheng",
      "Wangmeng Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_HIIF_Hierarchical_Encoding_based_Implicit_Image_Function_for_Continuous_Super-resolution_CVPR_2025_paper.html": {
    "title": "HIIF: Hierarchical Encoding based Implicit Image Function for Continuous Super-resolution",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuxuan Jiang",
      "Ho Man Kwan",
      "Tianhao Peng",
      "Ge Gao",
      "Fan Zhang",
      "Xiaoqing Zhu",
      "Joel Sole",
      "David Bull"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Geng_Motion_Prompting_Controlling_Video_Generation_with_Motion_Trajectories_CVPR_2025_paper.html": {
    "title": "Motion Prompting: Controlling Video Generation with Motion Trajectories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Daniel Geng",
      "Charles Herrmann",
      "Junhwa Hur",
      "Forrester Cole",
      "Serena Zhang",
      "Tobias Pfaff",
      "Tatiana Lopez-Guevara",
      "Yusuf Aytar",
      "Michael Rubinstein",
      "Chen Sun",
      "Oliver Wang",
      "Andrew Owens",
      "Deqing Sun"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_VERA_Explainable_Video_Anomaly_Detection_via_Verbalized_Learning_of_Vision-Language_CVPR_2025_paper.html": {
    "title": "VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Muchao Ye",
      "Weiyang Liu",
      "Pan He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wallin_ProHOC_Probabilistic_Hierarchical_Out-of-Distribution_Classification_via_Multi-Depth_Networks_CVPR_2025_paper.html": {
    "title": "ProHOC: Probabilistic Hierarchical Out-of-Distribution Classification via Multi-Depth Networks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Erik Wallin",
      "Fredrik Kahl",
      "Lars Hammarstrand"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_CCIN_Compositional_Conflict_Identification_and_Neutralization_for_Composed_Image_Retrieval_CVPR_2025_paper.html": {
    "title": "CCIN: Compositional Conflict Identification and Neutralization for Composed Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Likai Tian",
      "Jian Zhao",
      "Zechao Hu",
      "Zhengwei Yang",
      "Hao Li",
      "Lei Jin",
      "Zheng Wang",
      "Xuelong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xing_CLIP_is_Strong_Enough_to_Fight_Back_Test-time_Counterattacks_towards_CVPR_2025_paper.html": {
    "title": "CLIP is Strong Enough to Fight Back: Test-time Counterattacks towards Zero-shot Adversarial Robustness of CLIP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Songlong Xing",
      "Zhengyu Zhao",
      "Nicu Sebe"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lou_OverLoCK_An_Overview-first-Look-Closely-next_ConvNet_with_Context-Mixing_Dynamic_Kernels_CVPR_2025_paper.html": {
    "title": "OverLoCK: An Overview-first-Look-Closely-next ConvNet with Context-Mixing Dynamic Kernels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meng Lou",
      "Yizhou Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_SoftShadow_Leveraging_Soft_Masks_for_Penumbra-Aware_Shadow_Removal_CVPR_2025_paper.html": {
    "title": "SoftShadow: Leveraging Soft Masks for Penumbra-Aware Shadow Removal",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinrui Wang",
      "Lanqing Guo",
      "Xiyu Wang",
      "Siyu Huang",
      "Bihan Wen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Graph-Embedded_Structure-Aware_Perceptual_Hashing_for_Neural_Network_Protection_and_Piracy_CVPR_2025_paper.html": {
    "title": "Graph-Embedded Structure-Aware Perceptual Hashing for Neural Network Protection and Piracy Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ruiheng Liu",
      "Haozhe Chen",
      "Boyao Zhao",
      "Kejiang Chen",
      "Weiming Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liang_VTON-HandFit_Virtual_Try-on_for_Arbitrary_Hand_Pose_Guided_by_Hand_CVPR_2025_paper.html": {
    "title": "VTON-HandFit: Virtual Try-on for Arbitrary Hand Pose Guided by Hand Priors Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yujie Liang",
      "Xiaobin Hu",
      "Boyuan Jiang",
      "Donghao Luo",
      "Xu Peng",
      "Kai Wu",
      "Chengming Xu",
      "Wenhui Han",
      "Taisong Jin",
      "Chengjie Wang",
      "Rongrong Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Gao_Interleaved-Modal_Chain-of-Thought_CVPR_2025_paper.html": {
    "title": "Interleaved-Modal Chain-of-Thought",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jun Gao",
      "Yongqi Li",
      "Ziqiang Cao",
      "Wenjie Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Uni-Renderer_Unifying_Rendering_and_Inverse_Rendering_Via_Dual_Stream_Diffusion_CVPR_2025_paper.html": {
    "title": "Uni-Renderer: Unifying Rendering and Inverse Rendering Via Dual Stream Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhifei Chen",
      "Tianshuo Xu",
      "Wenhang Ge",
      "Leyi Wu",
      "Dongyu Yan",
      "Jing He",
      "Luozhou Wang",
      "Lu Zeng",
      "Shunsi Zhang",
      "Ying-Cong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Enhancing_Adversarial_Transferability_with_Checkpoints_of_a_Single_Models_Training_CVPR_2025_paper.html": {
    "title": "Enhancing Adversarial Transferability with Checkpoints of a Single Model's Training",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shixin Li",
      "Chaoxiang He",
      "Xiaojing Ma",
      "Bin Benjamin Zhu",
      "Shuo Wang",
      "Hongsheng Hu",
      "Dongmei Zhang",
      "Linchen Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_POSTA_A_Go-to_Framework_for_Customized_Artistic_Poster_Generation_CVPR_2025_paper.html": {
    "title": "POSTA: A Go-to Framework for Customized Artistic Poster Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Chen",
      "Xiaojie Xu",
      "Wenbo Li",
      "Jingjing Ren",
      "Tian Ye",
      "Songhua Liu",
      "Ying-Cong Chen",
      "Lei Zhu",
      "Xinchao Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kneeland_NSD-Imagery_A_Benchmark_Dataset_for_Extending_fMRI_Vision_Decoding_Methods_CVPR_2025_paper.html": {
    "title": "NSD-Imagery: A Benchmark Dataset for Extending fMRI Vision Decoding Methods to Mental Imagery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reese Kneeland",
      "Paul S. Scotti",
      "Ghislain St-Yves",
      "Jesse Breedlove",
      "Kendrick Kay",
      "Thomas Naselaris"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_VLsI_Verbalized_Layers-to-Interactions_from_Large_to_Small_Vision_Language_Models_CVPR_2025_paper.html": {
    "title": "VLsI: Verbalized Layers-to-Interactions from Large to Small Vision Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Byung-Kwan Lee",
      "Ryo Hachiuma",
      "Yu-Chiang Frank Wang",
      "Yong Man Ro",
      "Yueh-Hua Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sharifdeen_O-TPT_Orthogonality_Constraints_for_Calibrating_Test-time_Prompt_Tuning_in_Vision-Language_CVPR_2025_paper.html": {
    "title": "O-TPT: Orthogonality Constraints for Calibrating Test-time Prompt Tuning in Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ashshak Sharifdeen",
      "Muhammad Akhtar Munir",
      "Sanoojan Baliah",
      "Salman Khan",
      "Muhammad Haris Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Majhi_Just_Dance_with_pi_A_Poly-modal_Inductor_for_Weakly-supervised_Video_CVPR_2025_paper.html": {
    "title": "Just Dance with pi! A Poly-modal Inductor for Weakly-supervised Video Anomaly Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Snehashis Majhi",
      "Giacomo D'Amicantonio",
      "Antitza Dantcheva",
      "Quan Kong",
      "Lorenzo Garattoni",
      "Gianpiero Francesca",
      "Egor Bondarev",
      "Francois Bremond"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Flash3D_Super-scaling_Point_Transformers_through_Joint_Hardware-Geometry_Locality_CVPR_2025_paper.html": {
    "title": "Flash3D: Super-scaling Point Transformers through Joint Hardware-Geometry Locality",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyan Chen",
      "Gregory P. Meyer",
      "Zaiwei Zhang",
      "Eric M. Wolff",
      "Paul Vernaza"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Analyzing_the_Synthetic-to-Real_Domain_Gap_in_3D_Hand_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "Analyzing the Synthetic-to-Real Domain Gap in 3D Hand Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhuoran Zhao",
      "Linlin Yang",
      "Pengzhan Sun",
      "Pan Hui",
      "Angela Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Feature4X_Bridging_Any_Monocular_Video_to_4D_Agentic_AI_with_CVPR_2025_paper.html": {
    "title": "Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shijie Zhou",
      "Hui Ren",
      "Yijia Weng",
      "Shuwang Zhang",
      "Zhen Wang",
      "Dejia Xu",
      "Zhiwen Fan",
      "Suya You",
      "Zhangyang Wang",
      "Leonidas Guibas",
      "Achuta Kadambi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Comprehensive_Relighting_Generalizable_and_Consistent_Monocular_Human_Relighting_and_Harmonization_CVPR_2025_paper.html": {
    "title": "Comprehensive Relighting: Generalizable and Consistent Monocular Human Relighting and Harmonization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junying Wang",
      "Jingyuan Liu",
      "Xin Sun",
      "Krishna Kumar Singh",
      "Zhixin Shu",
      "He Zhang",
      "Jimei Yang",
      "Nanxuan Zhao",
      "Tuanfeng Y. Wang",
      "Simon S. Chen",
      "Ulrich Neumann",
      "Jae Shin Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_Hyperspectral_Pansharpening_via_Diffusion_Models_with_Iteratively_Zero-Shot_Guidance_CVPR_2025_paper.html": {
    "title": "Hyperspectral Pansharpening via Diffusion Models with Iteratively Zero-Shot Guidance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jin-Liang Xiao",
      "Ting-Zhu Huang",
      "Liang-Jian Deng",
      "Guang Lin",
      "Zihan Cao",
      "Chao Li",
      "Qibin Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiao_EASEMVCEfficient_Dual_Selection_Mechanism_for_Deep_Multi-View_Clustering_CVPR_2025_paper.html": {
    "title": "EASEMVC:Efficient Dual Selection Mechanism for Deep Multi-View Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Baili Xiao",
      "Zhibin Dong",
      "Ke Liang",
      "Suyuan Liu",
      "Siwei Wang",
      "Tianrui Liu",
      "Xingchen Hu",
      "En Zhu",
      "Xinwang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_Efficient_Motion-Aware_Video_MLLM_CVPR_2025_paper.html": {
    "title": "Efficient Motion-Aware Video MLLM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijia Zhao",
      "Yuqi Huo",
      "Tongtian Yue",
      "Longteng Guo",
      "Haoyu Lu",
      "Bingning Wang",
      "Weipeng Chen",
      "Jing Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_DSPNet_Dual-vision_Scene_Perception_for_Robust_3D_Question_Answering_CVPR_2025_paper.html": {
    "title": "DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingzhou Luo",
      "Yang Liu",
      "Weixing Chen",
      "Zhen Li",
      "Yaowei Wang",
      "Guanbin Li",
      "Liang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Zero-Shot_4D_Lidar_Panoptic_Segmentation_CVPR_2025_paper.html": {
    "title": "Zero-Shot 4D Lidar Panoptic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushan Zhang",
      "Aljo≈°a O≈°ep",
      "Laura Leal-Taix√©",
      "Tim Meinhardt"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guedon_MAtCha_Gaussians_Atlas_of_Charts_for_High-Quality_Geometry_and_Photorealism_CVPR_2025_paper.html": {
    "title": "MAtCha Gaussians: Atlas of Charts for High-Quality Geometry and Photorealism From Sparse Views",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Antoine Guedon",
      "Tomoki Ichikawa",
      "Kohei Yamashita",
      "Ko Nishino"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bezalel_Extreme_Rotation_Estimation_in_the_Wild_CVPR_2025_paper.html": {
    "title": "Extreme Rotation Estimation in the Wild",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hana Bezalel",
      "Dotan Ankri",
      "Ruojin Cai",
      "Hadar Averbach-Elor"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lai_ADU_Adaptive_Detection_of_Unknown_Categories_in_Black-Box_Domain_Adaptation_CVPR_2025_paper.html": {
    "title": "ADU: Adaptive Detection of Unknown Categories in Black-Box Domain Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yushan Lai",
      "Guowen Li",
      "Haoyuan Liang",
      "Juepeng Zheng",
      "Zhiyu Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_EmotiveTalk_Expressive_Talking_Head_Generation_through_Audio_Information_Decoupling_and_CVPR_2025_paper.html": {
    "title": "EmotiveTalk: Expressive Talking Head Generation through Audio Information Decoupling and Emotional Video Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotian Wang",
      "Yuzhe Weng",
      "Yueyan Li",
      "Zilu Guo",
      "Jun Du",
      "Shutong Niu",
      "Jiefeng Ma",
      "Shan He",
      "Xiaoyan Wu",
      "Qiming Hu",
      "Bing Yin",
      "Cong Liu",
      "Qingfeng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Traversing_Distortion-Perception_Tradeoff_using_a_Single_Score-Based_Generative_Model_CVPR_2025_paper.html": {
    "title": "Traversing Distortion-Perception Tradeoff using a Single Score-Based Generative Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhan Wang",
      "Suzhi Bi",
      "Ying-Jun Angela Zhang",
      "Xiaojun Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_IceDiff_High_Resolution_and_High-Quality_Arctic_Sea_Ice_Forecasting_with_CVPR_2025_paper.html": {
    "title": "IceDiff: High Resolution and High-Quality Arctic Sea Ice Forecasting with Generative Diffusion Prior",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi Xu",
      "Siwei Tu",
      "Weidong Yang",
      "Ben Fei",
      "Shuhao Li",
      "Keyi Liu",
      "Yeqi Luo",
      "Lipeng Ma",
      "Lei Bai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_DTOS_Dynamic_Time_Object_Sensing_with_Large_Multimodal_Model_CVPR_2025_paper.html": {
    "title": "DTOS: Dynamic Time Object Sensing with Large Multimodal Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jirui Tian",
      "Jinrong Zhang",
      "Shenglan Liu",
      "Luhao Xu",
      "Zhixiong Huang",
      "Gao Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dziadzio_How_to_Merge_Your_Multimodal_Models_Over_Time_CVPR_2025_paper.html": {
    "title": "How to Merge Your Multimodal Models Over Time?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sebastian Dziadzio",
      "Vishaal Udandarao",
      "Karsten Roth",
      "Ameya Prabhu",
      "Zeynep Akata",
      "Samuel Albanie",
      "Matthias Bethge"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_Identifying_and_Mitigating_Position_Bias_of_Multi-image_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "Identifying and Mitigating Position Bias of Multi-image Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyu Tian",
      "Shu Zou",
      "Zhaoyuan Yang",
      "Jing Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lenz_Unsupervised_Foundation_Model-Agnostic_Slide-Level_Representation_Learning_CVPR_2025_paper.html": {
    "title": "Unsupervised Foundation Model-Agnostic Slide-Level Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tim Lenz",
      "Peter Neidlinger",
      "Marta Ligero",
      "Georg W√∂lflein",
      "Marko van Treeck",
      "Jakob N. Kather"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_Exploring_CLIPs_Dense_Knowledge_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2025_paper.html": {
    "title": "Exploring CLIP's Dense Knowledge for Weakly Supervised Semantic Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhiwei Yang",
      "Yucong Meng",
      "Kexue Fu",
      "Feilong Tang",
      "Shuo Wang",
      "Zhijian Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_UNIALIGN_Scaling_Multimodal_Alignment_within_One_Unified_Model_CVPR_2025_paper.html": {
    "title": "UNIALIGN: Scaling Multimodal Alignment within One Unified Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bo Zhou",
      "Liulei Li",
      "Yujia Wang",
      "Huafeng Liu",
      "Yazhou Yao",
      "Wenguan Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Soucek_ShowHowTo_Generating_Scene-Conditioned_Step-by-Step_Visual_Instructions_CVPR_2025_paper.html": {
    "title": "ShowHowTo: Generating Scene-Conditioned Step-by-Step Visual Instructions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tom√°≈° Souƒçek",
      "Prajwal Gatti",
      "Michael Wray",
      "Ivan Laptev",
      "Dima Damen",
      "Josef Sivic"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Savov_Exploration-Driven_Generative_Interactive_Environments_CVPR_2025_paper.html": {
    "title": "Exploration-Driven Generative Interactive Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nedko Savov",
      "Naser Kazemi",
      "Mohammad Mahdi",
      "Danda Pani Paudel",
      "Xi Wang",
      "Luc Van Gool"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_Task-Agnostic_Guided_Feature_Expansion_for_Class-Incremental_Learning_CVPR_2025_paper.html": {
    "title": "Task-Agnostic Guided Feature Expansion for Class-Incremental Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bowen Zheng",
      "Da-Wei Zhou",
      "Han-Jia Ye",
      "De-Chuan Zhan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_ShowUI_One_Vision-Language-Action_Model_for_GUI_Visual_Agent_CVPR_2025_paper.html": {
    "title": "ShowUI: One Vision-Language-Action Model for GUI Visual Agent",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kevin Qinghong Lin",
      "Linjie Li",
      "Difei Gao",
      "Zhengyuan Yang",
      "Shiwei Wu",
      "Zechen Bai",
      "Stan Weixian Lei",
      "Lijuan Wang",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Lets_Chorus_Partner-aware_Hybrid_Song-Driven_3D_Head_Animation_CVPR_2025_paper.html": {
    "title": "Let's Chorus: Partner-aware Hybrid Song-Driven 3D Head Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiumei Xie",
      "Zikai Huang",
      "Wenhao Xu",
      "Peng Xiao",
      "Xuemiao Xu",
      "Huaidong Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zarzar_Twinner_Shining_Light_on_Digital_Twins_in_a_Few_Snaps_CVPR_2025_paper.html": {
    "title": "Twinner: Shining Light on Digital Twins in a Few Snaps",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jesus Zarzar",
      "Tom Monnier",
      "Roman Shapovalov",
      "Andrea Vedaldi",
      "David Novotny"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_Infinity_Scaling_Bitwise_AutoRegressive_Modeling_for_High-Resolution_Image_Synthesis_CVPR_2025_paper.html": {
    "title": "Infinity: Scaling Bitwise AutoRegressive Modeling for High-Resolution Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jian Han",
      "Jinlai Liu",
      "Yi Jiang",
      "Bin Yan",
      "Yuqi Zhang",
      "Zehuan Yuan",
      "Bingyue Peng",
      "Xiaobing Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_DreamText_High_Fidelity_Scene_Text_Synthesis_CVPR_2025_paper.html": {
    "title": "DreamText: High Fidelity Scene Text Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yibin Wang",
      "Weizhong Zhang",
      "Honghui Xu",
      "Cheng Jin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Parihar_MonoPlace3D_Learning_3D-Aware_Object_Placement_for_3D_Monocular_Detection_CVPR_2025_paper.html": {
    "title": "MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Rishubh Parihar",
      "Srinjay Sarkar",
      "Sarthak Vora",
      "Jogendra Nath Kundu",
      "R. Venkatesh Babu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_HumanDreamer_Generating_Controllable_Human-Motion_Videos_via_Decoupled_Generation_CVPR_2025_paper.html": {
    "title": "HumanDreamer: Generating Controllable Human-Motion Videos via Decoupled Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Boyuan Wang",
      "Xiaofeng Wang",
      "Chaojun Ni",
      "Guosheng Zhao",
      "Zhiqin Yang",
      "Zheng Zhu",
      "Muyang Zhang",
      "Yukun Zhou",
      "Xinze Chen",
      "Guan Huang",
      "Lihong Liu",
      "Xingang Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hannan_ReVisionLLM_Recursive_Vision-Language_Model_for_Temporal_Grounding_in_Hour-Long_Videos_CVPR_2025_paper.html": {
    "title": "ReVisionLLM: Recursive Vision-Language Model for Temporal Grounding in Hour-Long Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tanveer Hannan",
      "Md Mohaiminul Islam",
      "Jindong Gu",
      "Thomas Seidl",
      "Gedas Bertasius"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_ArtiFade_Learning_to_Generate_High-quality_Subject_from_Blemished_Images_CVPR_2025_paper.html": {
    "title": "ArtiFade: Learning to Generate High-quality Subject from Blemished Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shuya Yang",
      "Shaozhe Hao",
      "Yukang Cao",
      "Kwan-Yee K. Wong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_SGCR_Spherical_Gaussians_for_Efficient_3D_Curve_Reconstruction_CVPR_2025_paper.html": {
    "title": "SGCR: Spherical Gaussians for Efficient 3D Curve Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinran Yang",
      "Donghao Ji",
      "Yuanqi Li",
      "Jie Guo",
      "Yanwen Guo",
      "Junyuan Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Prompting_Depth_Anything_for_4K_Resolution_Accurate_Metric_Depth_Estimation_CVPR_2025_paper.html": {
    "title": "Prompting Depth Anything for 4K Resolution Accurate Metric Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haotong Lin",
      "Sida Peng",
      "Jingxiao Chen",
      "Songyou Peng",
      "Jiaming Sun",
      "Minghuan Liu",
      "Hujun Bao",
      "Jiashi Feng",
      "Xiaowei Zhou",
      "Bingyi Kang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bendou_ProKeR_A_Kernel_Perspective_on_Few-Shot_Adaptation_of_Large_Vision-Language_CVPR_2025_paper.html": {
    "title": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yassir Bendou",
      "Amine Ouasfi",
      "Vincent Gripon",
      "Adnane Boukhayma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Karypidis_Advancing_Semantic_Future_Prediction_through_Multimodal_Visual_Sequence_Transformers_CVPR_2025_paper.html": {
    "title": "Advancing Semantic Future Prediction through Multimodal Visual Sequence Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Efstathios Karypidis",
      "Ioannis Kakogeorgiou",
      "Spyros Gidaris",
      "Nikos Komodakis"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_GET_Unlocking_the_Multi-modal_Potential_of_CLIP_for_Generalized_Category_CVPR_2025_paper.html": {
    "title": "GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Enguang Wang",
      "Zhimao Peng",
      "Zhengyuan Xie",
      "Fei Yang",
      "Xialei Liu",
      "Ming-Ming Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_On_the_Out-Of-Distribution_Generalization_of_Large_Multimodal_Models_CVPR_2025_paper.html": {
    "title": "On the Out-Of-Distribution Generalization of Large Multimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingxuan Zhang",
      "Jiansheng Li",
      "Wenjing Chu",
      "junjia hai",
      "Renzhe Xu",
      "Yuqing Yang",
      "Shikai Guan",
      "Jiazheng Xu",
      "Liping Jing",
      "Peng Cui"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Enhanced_Contrastive_Learning_with_Multi-view_Longitudinal_Data_for_Chest_X-ray_CVPR_2025_paper.html": {
    "title": "Enhanced Contrastive Learning with Multi-view Longitudinal Data for Chest X-ray Report Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kang Liu",
      "Zhuoqi Ma",
      "Xiaolu Kang",
      "Yunan Li",
      "Kun Xie",
      "Zhicheng Jiao",
      "Qiguang Miao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MonoTAKD_Teaching_Assistant_Knowledge_Distillation_for_Monocular_3D_Object_Detection_CVPR_2025_paper.html": {
    "title": "MonoTAKD: Teaching Assistant Knowledge Distillation for Monocular 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hou-I Liu",
      "Christine Wu",
      "Jen-Hao Cheng",
      "Wenhao Chai",
      "Shian-Yun Wang",
      "Gaowen Liu",
      "Hugo Latapie",
      "Jhih-Ciang Wu",
      "Jenq-Neng Hwang",
      "Hong-Han Shuai",
      "Wen-Huang Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lv_Test-Time_Domain_Generalization_via_Universe_Learning_A_Multi-Graph_Matching_Approach_CVPR_2025_paper.html": {
    "title": "Test-Time Domain Generalization via Universe Learning: A Multi-Graph Matching Approach for Medical Image Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingguo Lv",
      "Xingbo Dong",
      "Liwen Wang",
      "Jiewen Yang",
      "Lei Zhao",
      "Bin Pu",
      "Zhe Jin",
      "Xuejun Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Easy-editable_Image_Vectorization_with_Multi-layer_Multi-scale_Distributed_Visual_Feature_Embedding_CVPR_2025_paper.html": {
    "title": "Easy-editable Image Vectorization with Multi-layer Multi-scale Distributed Visual Feature Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ye Chen",
      "Zhangli Hu",
      "Zhongyin Zhao",
      "Yupeng Zhu",
      "Yue Shi",
      "Yuxuan Xiong",
      "Bingbing Ni"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Deng_Acquire_and_then_Adapt_Squeezing_out_Text-to-Image_Model_for_Image_CVPR_2025_paper.html": {
    "title": "Acquire and then Adapt: Squeezing out Text-to-Image Model for Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junyuan Deng",
      "Xinyi Wu",
      "Yongxing Yang",
      "Congchao Zhu",
      "Song Wang",
      "Zhenyao Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hou_DeDe_Detecting_Backdoor_Samples_for_SSL_Encoders_via_Decoders_CVPR_2025_paper.html": {
    "title": "DeDe: Detecting Backdoor Samples for SSL Encoders via Decoders",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sizai Hou",
      "Songze Li",
      "Duanyi Yao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ryu_Towards_Scalable_Human-aligned_Benchmark_for_Text-guided_Image_Editing_CVPR_2025_paper.html": {
    "title": "Towards Scalable Human-aligned Benchmark for Text-guided Image Editing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Suho Ryu",
      "Kihyun Kim",
      "Eugene Baek",
      "Dongsoo Shin",
      "Joonseok Lee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Devils_in_Middle_Layers_of_Large_Vision-Language_Models_Interpreting_Detecting_CVPR_2025_paper.html": {
    "title": "Devils in Middle Layers of Large Vision-Language Models: Interpreting, Detecting and Mitigating Object Hallucinations via Attention Lens",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhangqi Jiang",
      "Junkai Chen",
      "Beier Zhu",
      "Tingjin Luo",
      "Yankun Shen",
      "Xu Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Fan_SpectroMotion_Dynamic_3D_Reconstruction_of_Specular_Scenes_CVPR_2025_paper.html": {
    "title": "SpectroMotion: Dynamic 3D Reconstruction of Specular Scenes",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng-De Fan",
      "Chen-Wei Chang",
      "Yi-Ruei Liu",
      "Jie-Ying Lee",
      "Jiun-Long Huang",
      "Yu-Chee Tseng",
      "Yu-Lun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Scaling_Inference_Time_Compute_for_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Scaling Inference Time Compute for Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Nanye Ma",
      "Shangyuan Tong",
      "Haolin Jia",
      "Hexiang Hu",
      "Yu-Chuan Su",
      "Mingda Zhang",
      "Xuan Yang",
      "Yandong Li",
      "Tommi Jaakkola",
      "Xuhui Jia",
      "Saining Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Miao_Coeff-Tuning_A_Graph_Filter_Subspace_View_for_Tuning_Attention-Based_Large_CVPR_2025_paper.html": {
    "title": "Coeff-Tuning: A Graph Filter Subspace View for Tuning Attention-Based Large Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zichen Miao",
      "Wei Chen",
      "Qiang Qiu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_VTON_360_High-Fidelity_Virtual_Try-On_from_Any_Viewing_Direction_CVPR_2025_paper.html": {
    "title": "VTON 360: High-Fidelity Virtual Try-On from Any Viewing Direction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijian He",
      "Yuwei Ning",
      "Yipeng Qin",
      "Guangrun Wang",
      "Sibei Yang",
      "Liang Lin",
      "Guanbin Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MVBoost_Boost_3D_Reconstruction_with_Multi-View_Refinement_CVPR_2025_paper.html": {
    "title": "MVBoost: Boost 3D Reconstruction with Multi-View Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiangyu Liu",
      "Xiaomei Zhang",
      "Zhiyuan Ma",
      "Xiangyu Zhu",
      "Zhen Lei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bai_Chat-based_Person_Retrieval_via_Dialogue-Refined_Cross-Modal_Alignment_CVPR_2025_paper.html": {
    "title": "Chat-based Person Retrieval via Dialogue-Refined Cross-Modal Alignment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Bai",
      "Yucheng Ji",
      "Min Cao",
      "Jinqiao Wang",
      "Mang Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_Category-Agnostic_Neural_Object_Rigging_CVPR_2025_paper.html": {
    "title": "Category-Agnostic Neural Object Rigging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guangzhao He",
      "Chen Geng",
      "Shangzhe Wu",
      "Jiajun Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_AVF-MAE_Scaling_Affective_Video_Facial_Masked_Autoencoders_via_Efficient_Audio-Visual_CVPR_2025_paper.html": {
    "title": "AVF-MAE++: Scaling Affective Video Facial Masked Autoencoders via Efficient Audio-Visual Self-Supervised Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuecheng Wu",
      "Heli Sun",
      "Yifan Wang",
      "Jiayu Nie",
      "Jie Zhang",
      "Yabing Wang",
      "Junxiao Xue",
      "Liang He"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_POPEN_Preference-Based_Optimization_and_Ensemble_for_LVLM-Based_Reasoning_Segmentation_CVPR_2025_paper.html": {
    "title": "POPEN: Preference-Based Optimization and Ensemble for LVLM-Based Reasoning Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lanyun Zhu",
      "Tianrun Chen",
      "Qianxiong Xu",
      "Xuanyi Liu",
      "Deyi Ji",
      "Haiyang Wu",
      "De Wen Soh",
      "Jun Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhao_DiffusionSfM_Predicting_Structure_and_Motion_via_Ray_Origin_and_Endpoint_CVPR_2025_paper.html": {
    "title": "DiffusionSfM: Predicting Structure and Motion via Ray Origin and Endpoint Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qitao Zhao",
      "Amy Lin",
      "Jeff Tan",
      "Jason Y. Zhang",
      "Deva Ramanan",
      "Shubham Tulsiani"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_GroundingFace_Fine-grained_Face_Understanding_via_Pixel_Grounding_Multimodal_Large_Language_CVPR_2025_paper.html": {
    "title": "GroundingFace: Fine-grained Face Understanding via Pixel Grounding Multimodal Large Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yue Han",
      "Jiangning Zhang",
      "Junwei Zhu",
      "Runze Hou",
      "Xiaozhong Ji",
      "Chuming Lin",
      "Xiaobin Hu",
      "Zhucun Xue",
      "Yong Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Baade_Self-Supervised_Cross-View_Correspondence_with_Predictive_Cycle_Consistency_CVPR_2025_paper.html": {
    "title": "Self-Supervised Cross-View Correspondence with Predictive Cycle Consistency",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Alan Baade",
      "Changan Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_MMAudio_Taming_Multimodal_Joint_Training_for_High-Quality_Video-to-Audio_Synthesis_CVPR_2025_paper.html": {
    "title": "MMAudio: Taming Multimodal Joint Training for High-Quality Video-to-Audio Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ho Kei Cheng",
      "Masato Ishii",
      "Akio Hayakawa",
      "Takashi Shibuya",
      "Alexander Schwing",
      "Yuki Mitsufuji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ao_CryptoFace_End-to-End_Encrypted_Face_Recognition_CVPR_2025_paper.html": {
    "title": "CryptoFace: End-to-End Encrypted Face Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wei Ao",
      "Vishnu Naresh Boddeti"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Relation-Rich_Visual_Document_Generator_for_Visual_Information_Extraction_CVPR_2025_paper.html": {
    "title": "Relation-Rich Visual Document Generator for Visual Information Extraction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zi-Han Jiang",
      "Chien-Wei Lin",
      "Wei-Hua Li",
      "Hsuan-Tung Liu",
      "Yi-Ren Yeh",
      "Chu-Song Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_DynFocus_Dynamic_Cooperative_Network_Empowers_LLMs_with_Video_Understanding_CVPR_2025_paper.html": {
    "title": "DynFocus: Dynamic Cooperative Network Empowers LLMs with Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yudong Han",
      "Qingpei Guo",
      "Liyuan Pan",
      "Liu Liu",
      "Yu Guan",
      "Ming Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_Mimic_In-Context_Learning_for_Multimodal_Tasks_CVPR_2025_paper.html": {
    "title": "Mimic In-Context Learning for Multimodal Tasks",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuchu Jiang",
      "Jiale Fu",
      "Chenduo Hao",
      "Xinting Hu",
      "Yingzhe Peng",
      "Xin Geng",
      "Xu Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zou_PromptHashAffinity-Prompted_Collaborative_Cross-Modal_Learning_for_Adaptive_Hashing_Retrieval_CVPR_2025_paper.html": {
    "title": "PromptHash:Affinity-Prompted Collaborative Cross-Modal Learning for Adaptive Hashing Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qiang Zou",
      "Shuli Cheng",
      "Jiayi Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yue_V-Stylist_Video_Stylization_via_Collaboration_and_Reflection_of_MLLM_Agents_CVPR_2025_paper.html": {
    "title": "V-Stylist: Video Stylization via Collaboration and Reflection of MLLM Agents",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhengrong Yue",
      "Shaobin Zhuang",
      "Kunchang Li",
      "Yanbo Ding",
      "Yali Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Alhamoud_Vision-Language_Models_Do_Not_Understand_Negation_CVPR_2025_paper.html": {
    "title": "Vision-Language Models Do Not Understand Negation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kumail Alhamoud",
      "Shaden Alshammari",
      "Yonglong Tian",
      "Guohao Li",
      "Philip H.S. Torr",
      "Yoon Kim",
      "Marzyeh Ghassemi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_ID-Patch_Robust_ID_Association_for_Group_Photo_Personalization_CVPR_2025_paper.html": {
    "title": "ID-Patch: Robust ID Association for Group Photo Personalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yimeng Zhang",
      "Tiancheng Zhi",
      "Jing Liu",
      "Shen Sang",
      "Liming Jiang",
      "Qing Yan",
      "Sijia Liu",
      "Linjie Luo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cao_iG-6DoF_Model-free_6DoF_Pose_Estimation_for_Unseen_Object_via_Iterative_CVPR_2025_paper.html": {
    "title": "iG-6DoF: Model-free 6DoF Pose Estimation for Unseen Object via Iterative 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tuo Cao",
      "Fei Luo",
      "Jiongming Qin",
      "Yu Jiang",
      "Yusen Wang",
      "Chunxia Xiao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_NexusGS_Sparse_View_Synthesis_with_Epipolar_Depth_Priors_in_3D_CVPR_2025_paper.html": {
    "title": "NexusGS: Sparse View Synthesis with Epipolar Depth Priors in 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yulong Zheng",
      "Zicheng Jiang",
      "Shengfeng He",
      "Yandu Sun",
      "Junyu Dong",
      "Huaidong Zhang",
      "Yong Du"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hedlin_HyperNet_Fields_Efficiently_Training_Hypernetworks_without_Ground_Truth_by_Learning_CVPR_2025_paper.html": {
    "title": "HyperNet Fields: Efficiently Training Hypernetworks without Ground Truth by Learning Weight Trajectories",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Eric Hedlin",
      "Munawar Hayat",
      "Fatih Porikli",
      "Kwang Moo Yi",
      "Shweta Mahajan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Universal_Scene_Graph_Generation_CVPR_2025_paper.html": {
    "title": "Universal Scene Graph Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengqiong Wu",
      "Hao Fei",
      "Tat-seng Chua"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Long_RICCARDO_Radar_Hit_Prediction_and_Convolution_for_Camera-Radar_3D_Object_CVPR_2025_paper.html": {
    "title": "RICCARDO: Radar Hit Prediction and Convolution for Camera-Radar 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yunfei Long",
      "Abhinav Kumar",
      "Xiaoming Liu",
      "Daniel Morris"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_ForestLPR_LiDAR_Place_Recognition_in_Forests_Attentioning_Multiple_BEV_Density_CVPR_2025_paper.html": {
    "title": "ForestLPR: LiDAR Place Recognition in Forests Attentioning Multiple BEV Density Images",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanqing Shen",
      "Turcan Tuna",
      "Marco Hutter",
      "Cesar Cadena",
      "Nanning Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_BLADE_Single-view_Body_Mesh_Estimation_through_Accurate_Depth_Estimation_CVPR_2025_paper.html": {
    "title": "BLADE: Single-view Body Mesh Estimation through Accurate Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengze Wang",
      "Jiefeng Li",
      "Tianye Li",
      "Ye Yuan",
      "Henry Fuchs",
      "Koki Nagano",
      "Shalini De Mello",
      "Michael Stengel"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Du_AdaMMS_Model_Merging_for_Heterogeneous_Multimodal_Large_Language_Models_with_CVPR_2025_paper.html": {
    "title": "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiyang Du",
      "Xiaochen Wang",
      "Chi Chen",
      "Jiabo Ye",
      "Yiru Wang",
      "Peng Li",
      "Ming Yan",
      "Ji Zhang",
      "Fei Huang",
      "Zhifang Sui",
      "Maosong Sun",
      "Yang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_MoEE_Mixture_of_Emotion_Experts_for_Audio-Driven_Portrait_Animation_CVPR_2025_paper.html": {
    "title": "MoEE: Mixture of Emotion Experts for Audio-Driven Portrait Animation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huaize Liu",
      "Wenzhang Sun",
      "Donglin Di",
      "Shibo Sun",
      "Jiahui Yang",
      "Changqing Zou",
      "Hujun Bao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_ReCap_Better_Gaussian_Relighting_with_Cross-Environment_Captures_CVPR_2025_paper.html": {
    "title": "ReCap: Better Gaussian Relighting with Cross-Environment Captures",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingzhi Li",
      "Zongwei Wu",
      "Eduard Zamfir",
      "Radu Timofte"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Split_Adaptation_for_Pre-trained_Vision_Transformers_CVPR_2025_paper.html": {
    "title": "Split Adaptation for Pre-trained Vision Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lixu Wang",
      "Bingqi Shang",
      "Yi Li",
      "Payal Mohapatra",
      "Wei Dong",
      "Xiao Wang",
      "Qi Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_SpatialLLM_A_Compound_3D-Informed_Design_towards_Spatially-Intelligent_Large_Multimodal_Models_CVPR_2025_paper.html": {
    "title": "SpatialLLM: A Compound 3D-Informed Design towards Spatially-Intelligent Large Multimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wufei Ma",
      "Luoxin Ye",
      "Celso M de Melo",
      "Alan Yuille",
      "Jieneng Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ni_SLVR_Super-Light_Visual_Reconstruction_via_Blueprint_Controllable_Convolutions_and_Exploring_CVPR_2025_paper.html": {
    "title": "SLVR: Super-Light Visual Reconstruction via Blueprint Controllable Convolutions and Exploring Feature Diversity Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ning Ni",
      "Libao Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Vision-Language_Embodiment_for_Monocular_Depth_Estimation_CVPR_2025_paper.html": {
    "title": "Vision-Language Embodiment for Monocular Depth Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinchang Zhang",
      "Guoyu Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Layered_Image_Vectorization_via_Semantic_Simplification_CVPR_2025_paper.html": {
    "title": "Layered Image Vectorization via Semantic Simplification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenyu Wang",
      "Jianxi Huang",
      "Zhida Sun",
      "Yuanhao Gong",
      "Daniel Cohen-Or",
      "Min Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Learning_Occlusion-Robust_Vision_Transformers_for_Real-Time_UAV_Tracking_CVPR_2025_paper.html": {
    "title": "Learning Occlusion-Robust Vision Transformers for Real-Time UAV Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "You Wu",
      "Xucheng Wang",
      "Xiangyang Yang",
      "Mengyuan Liu",
      "Dan Zeng",
      "Hengzhou Ye",
      "Shuiwang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zeng_Plug-and-Play_Versatile_Compressed_Video_Enhancement_CVPR_2025_paper.html": {
    "title": "Plug-and-Play Versatile Compressed Video Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huimin Zeng",
      "Jiacheng Li",
      "Zhiwei Xiong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_UltraFusion_Ultra_High_Dynamic_Imaging_using_Exposure_Fusion_CVPR_2025_paper.html": {
    "title": "UltraFusion: Ultra High Dynamic Imaging using Exposure Fusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zixuan Chen",
      "Yujin Wang",
      "Xin Cai",
      "Zhiyuan You",
      "Zheming Lu",
      "Fan Zhang",
      "Shi Guo",
      "Tianfan Xue"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Hearing_Anywhere_in_Any_Environment_CVPR_2025_paper.html": {
    "title": "Hearing Anywhere in Any Environment",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiulong Liu",
      "Anurag Kumar",
      "Paul Calamia",
      "Sebastia V. Amengual",
      "Calvin Murdock",
      "Ishwarya Ananthabhotla",
      "Philip Robinson",
      "Eli Shlizerman",
      "Vamsi Krishna Ithapu",
      "Ruohan Gao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Automated_Proof_of_Polynomial_Inequalities_via_Reinforcement_Learning_CVPR_2025_paper.html": {
    "title": "Automated Proof of Polynomial Inequalities via Reinforcement Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Banglong Liu",
      "Niuniu Qi",
      "Xia Zeng",
      "Lydia Dehbi",
      "Zhengfeng Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_Noise-Resistant_Video_Anomaly_Detection_via_RGB_Error-Guided_Multiscale_Predictive_Coding_CVPR_2025_paper.html": {
    "title": "Noise-Resistant Video Anomaly Detection via RGB Error-Guided Multiscale Predictive Coding and Dynamic Memory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Han Hu",
      "Wenli Du",
      "Peng Liao",
      "Bing Wang",
      "Siyuan Fan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Frequency_Dynamic_Convolution_for_Dense_Image_Prediction_CVPR_2025_paper.html": {
    "title": "Frequency Dynamic Convolution for Dense Image Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Linwei Chen",
      "Lin Gu",
      "Liang Li",
      "Chenggang Yan",
      "Ying Fu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_IDEA_Inverted_Text_with_Cooperative_Deformable_Aggregation_for_Multi-modal_Object_CVPR_2025_paper.html": {
    "title": "IDEA: Inverted Text with Cooperative Deformable Aggregation for Multi-modal Object Re-Identification",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Wang",
      "Yongfeng Lv",
      "Pingping Zhang",
      "Huchuan Lu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mei_SAM-I2V_Upgrading_SAM_to_Support_Promptable_Video_Segmentation_with_Less_CVPR_2025_paper.html": {
    "title": "SAM-I2V: Upgrading SAM to Support Promptable Video Segmentation with Less than 0.2% Training Cost",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiyang Mei",
      "Pengyu Zhang",
      "Mike Zheng Shou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shaker_GroupMamba_Efficient_Group-Based_Visual_State_Space_Model_CVPR_2025_paper.html": {
    "title": "GroupMamba: Efficient Group-Based Visual State Space Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Abdelrahman Shaker",
      "Syed Talal Wasim",
      "Salman Khan",
      "Juergen Gall",
      "Fahad Shahbaz Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Prakash_How_Do_I_Do_That_Synthesizing_3D_Hand_Motion_and_CVPR_2025_paper.html": {
    "title": "How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aditya Prakash",
      "Benjamin Lundell",
      "Dmitry Andreychuk",
      "David Forsyth",
      "Saurabh Gupta",
      "Harpreet Sawhney"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hadgi_Escaping_Platos_Cave_Towards_the_Alignment_of_3D_and_Text_CVPR_2025_paper.html": {
    "title": "Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent Spaces",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Souhail Hadgi",
      "Luca Moschella",
      "Andrea Santilli",
      "Diego Gomez",
      "Qixing Huang",
      "Emanuele Rodol√†",
      "Simone Melzi",
      "Maks Ovsjanikov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Purohit_Consistency_Posterior_Sampling_for_Diverse_Image_Synthesis_CVPR_2025_paper.html": {
    "title": "Consistency Posterior Sampling for Diverse Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vishal Purohit",
      "Matthew Repasky",
      "Jianfeng Lu",
      "Qiang Qiu",
      "Yao Xie",
      "Xiuyuan Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shi_IMFine_3D_Inpainting_via_Geometry-guided_Multi-view_Refinement_CVPR_2025_paper.html": {
    "title": "IMFine: 3D Inpainting via Geometry-guided Multi-view Refinement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhihao Shi",
      "Dong Huo",
      "Yuhongze Zhou",
      "Yan Min",
      "Juwei Lu",
      "Xinxin Zuo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_ActiveGAMER_Active_GAussian_Mapping_through_Efficient_Rendering_CVPR_2025_paper.html": {
    "title": "ActiveGAMER: Active GAussian Mapping through Efficient Rendering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Liyan Chen",
      "Huangying Zhan",
      "Kevin Chen",
      "Xiangyu Xu",
      "Qingan Yan",
      "Changjiang Cai",
      "Yi Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ahmed_DeepCompress-ViT_Rethinking_Model_Compression_to_Enhance_Efficiency_of_Vision_Transformers_CVPR_2025_paper.html": {
    "title": "DeepCompress-ViT: Rethinking Model Compression to Enhance Efficiency of Vision Transformers at the Edge",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sabbir Ahmed",
      "Abdullah Al Arafat",
      "Deniz Najafi",
      "Akhlak Mahmood",
      "Mamshad Nayeem Rizve",
      "Mohaiminul Al Nahian",
      "Ranyang Zhou",
      "Shaahin Angizi",
      "Adnan Siraj Rakin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kalble_EvOcc_Accurate_Semantic_Occupancy_for_Automated_Driving_Using_Evidence_Theory_CVPR_2025_paper.html": {
    "title": "EvOcc: Accurate Semantic Occupancy for Automated Driving Using Evidence Theory",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jonas K√§lble",
      "Sascha Wirges",
      "Maxim Tatarchenko",
      "Eddy Ilg"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Positive2Negative_Breaking_the_Information-Lossy_Barrier_in_Self-Supervised_Single_Image_Denoising_CVPR_2025_paper.html": {
    "title": "Positive2Negative: Breaking the Information-Lossy Barrier in Self-Supervised Single Image Denoising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Tong Li",
      "Lizhi Wang",
      "Zhiyuan Xu",
      "Lin Zhu",
      "Wanxuan Lu",
      "Hua Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_Towards_Continual_Universal_Segmentation_CVPR_2025_paper.html": {
    "title": "Towards Continual Universal Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zihan Lin",
      "Zilei Wang",
      "Xu Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_PGC_Physics-Based_Gaussian_Cloth_from_a_Single_Pose_CVPR_2025_paper.html": {
    "title": "PGC: Physics-Based Gaussian Cloth from a Single Pose",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Michelle Guo",
      "Matt Jen-Yuan Chiang",
      "Igor Santesteban",
      "Nikolaos Sarafianos",
      "Hsiao-yu Chen",
      "Oshri Halimi",
      "Alja≈æ Bo≈æiƒç",
      "Shunsuke Saito",
      "Jiajun Wu",
      "C. Karen Liu",
      "Tuur Stuyck",
      "Egor Larionov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Joint_Vision-Language_Social_Bias_Removal_for_CLIP_CVPR_2025_paper.html": {
    "title": "Joint Vision-Language Social Bias Removal for CLIP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyu Zhang",
      "Yangyang Guo",
      "Mohan Kankanhalli"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tang_MV-DUSt3R_Single-Stage_Scene_Reconstruction_from_Sparse_Views_In_2_Seconds_CVPR_2025_paper.html": {
    "title": "MV-DUSt3R+: Single-Stage Scene Reconstruction from Sparse Views In 2 Seconds",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenggang Tang",
      "Yuchen Fan",
      "Dilin Wang",
      "Hongyu Xu",
      "Rakesh Ranjan",
      "Alexander Schwing",
      "Zhicheng Yan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Explicit_Depth-Aware_Blurry_Video_Frame_Interpolation_Guided_by_Differential_Curves_CVPR_2025_paper.html": {
    "title": "Explicit Depth-Aware Blurry Video Frame Interpolation Guided by Differential Curves",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zaoming Yan",
      "Pengcheng Lei",
      "Tingting Wang",
      "Faming Fang",
      "Junkang Zhang",
      "Yaomin Huang",
      "Haichuan Song"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Selvaraju_OFER_Occluded_Face_Expression_Reconstruction_CVPR_2025_paper.html": {
    "title": "OFER: Occluded Face Expression Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pratheba Selvaraju",
      "Victoria Fernandez Abrevaya",
      "Timo Bolkart",
      "Rick Akkerman",
      "Tianyu Ding",
      "Faezeh Amjadi",
      "Ilya Zharkov"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_SeaLion_Semantic_Part-Aware_Latent_Point_Diffusion_Models_for_3D_Generation_CVPR_2025_paper.html": {
    "title": "SeaLion: Semantic Part-Aware Latent Point Diffusion Models for 3D Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dekai Zhu",
      "Yan Di",
      "Stefan Gavranovic",
      "Slobodan Ilic"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cheng_MonSter_Marry_Monodepth_to_Stereo_Unleashes_Power_CVPR_2025_paper.html": {
    "title": "MonSter: Marry Monodepth to Stereo Unleashes Power",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Junda Cheng",
      "Longliang Liu",
      "Gangwei Xu",
      "Xianqi Wang",
      "Zhaoxing Zhang",
      "Yong Deng",
      "Jinliang Zang",
      "Yurui Chen",
      "Zhipeng Cai",
      "Xin Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lu_Toward_Real-world_BEV_Perception_Depth_Uncertainty_Estimation_via_Gaussian_Splatting_CVPR_2025_paper.html": {
    "title": "Toward Real-world BEV Perception: Depth Uncertainty Estimation via Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shu-Wei Lu",
      "Yi-Hsuan Tsai",
      "Yi-Ting Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shirkavand_Efficient_Fine-Tuning_and_Concept_Suppression_for_Pruned_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Efficient Fine-Tuning and Concept Suppression for Pruned Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Reza Shirkavand",
      "Peiran Yu",
      "Shangqian Gao",
      "Gowthami Somepalli",
      "Tom Goldstein",
      "Heng Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lazarow_Cubify_Anything_Scaling_Indoor_3D_Object_Detection_CVPR_2025_paper.html": {
    "title": "Cubify Anything: Scaling Indoor 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Justin Lazarow",
      "David Griffiths",
      "Gefen Kohavi",
      "Francisco Crespo",
      "Afshin Dehghan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_WildGS-SLAM_Monocular_Gaussian_Splatting_SLAM_in_Dynamic_Environments_CVPR_2025_paper.html": {
    "title": "WildGS-SLAM: Monocular Gaussian Splatting SLAM in Dynamic Environments",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianhao Zheng",
      "Zihan Zhu",
      "Valentin Bieri",
      "Marc Pollefeys",
      "Songyou Peng",
      "Iro Armeni"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Mildenberger_A_Tale_of_Two_Classes_Adapting_Supervised_Contrastive_Learning_to_CVPR_2025_paper.html": {
    "title": "A Tale of Two Classes: Adapting Supervised Contrastive Learning to Binary Imbalanced Datasets",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "David Mildenberger",
      "Paul Hager",
      "Daniel Rueckert",
      "Martin J. Menten"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_DEAL_Data-Efficient_Adversarial_Learning_for_High-Quality_Infrared_Imaging_CVPR_2025_paper.html": {
    "title": "DEAL: Data-Efficient Adversarial Learning for High-Quality Infrared Imaging",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhu Liu",
      "Zijun Wang",
      "Jinyuan Liu",
      "Fanqi Meng",
      "Long Ma",
      "Risheng Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_RePerformer_Immersive_Human-centric_Volumetric_Videos_from_Playback_to_Photoreal_Reperformance_CVPR_2025_paper.html": {
    "title": "RePerformer: Immersive Human-centric Volumetric Videos from Playback to Photoreal Reperformance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuheng Jiang",
      "Zhehao Shen",
      "Chengcheng Guo",
      "Yu Hong",
      "Zhuo Su",
      "Yingliang Zhang",
      "Marc Habermann",
      "Lan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yue_CheXWorld_Exploring_Image_World_Modeling_for_Radiograph_Representation_Learning_CVPR_2025_paper.html": {
    "title": "CheXWorld: Exploring Image World Modeling for Radiograph Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yang Yue",
      "Yulin Wang",
      "Chenxin Tao",
      "Pan Liu",
      "Shiji Song",
      "Gao Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Song_Towards_Long-Horizon_Vision-Language_Navigation_Platform_Benchmark_and_Method_CVPR_2025_paper.html": {
    "title": "Towards Long-Horizon Vision-Language Navigation: Platform, Benchmark and Method",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinshuai Song",
      "Weixing Chen",
      "Yang Liu",
      "Weikai Chen",
      "Guanbin Li",
      "Liang Lin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_Learning_Class_Prototypes_for_Unified_Sparse-Supervised_3D_Object_Detection_CVPR_2025_paper.html": {
    "title": "Learning Class Prototypes for Unified Sparse-Supervised 3D Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yun Zhu",
      "Le Hui",
      "Hang Yang",
      "Jianjun Qian",
      "Jin Xie",
      "Jian Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_BimArt_A_Unified_Approach_for_the_Synthesis_of_3D_Bimanual_CVPR_2025_paper.html": {
    "title": "BimArt: A Unified Approach for the Synthesis of 3D Bimanual Interaction with Articulated Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanyue Zhang",
      "Rishabh Dabral",
      "Vladislav Golyanik",
      "Vasileios Choutas",
      "Eduardo Alvarado",
      "Thabo Beeler",
      "Marc Habermann",
      "Christian Theobalt"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ao_Open-World_Amodal_Appearance_Completion_CVPR_2025_paper.html": {
    "title": "Open-World Amodal Appearance Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiayang Ao",
      "Yanbei Jiang",
      "Qiuhong Ke",
      "Krista A. Ehinger"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_AIGV-Assessor_Benchmarking_and_Evaluating_the_Perceptual_Quality_of_Text-to-Video_Generation_CVPR_2025_paper.html": {
    "title": "AIGV-Assessor: Benchmarking and Evaluating the Perceptual Quality of Text-to-Video Generation with LMM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jiarui Wang",
      "Huiyu Duan",
      "Guangtao Zhai",
      "Juntong Wang",
      "Xiongkuo Min"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_CoSpace_Benchmarking_Continuous_Space_Perception_Ability_for_Vision-Language_Models_CVPR_2025_paper.html": {
    "title": "CoSpace: Benchmarking Continuous Space Perception Ability for Vision-Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yiqi Zhu",
      "Ziyue Wang",
      "Can Zhang",
      "Peng Li",
      "Yang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_Autoregressive_Distillation_of_Diffusion_Transformers_CVPR_2025_paper.html": {
    "title": "Autoregressive Distillation of Diffusion Transformers",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yeongmin Kim",
      "Sotiris Anagnostidis",
      "Yuming Du",
      "Edgar Sch√∂nfeld",
      "Jonas Kohler",
      "Markos Georgopoulos",
      "Albert Pumarola",
      "Ali Thabet",
      "Artsiom Sanakoyeu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_RivuletMLP_An_MLP-based_Architecture_for_Efficient_Compressed_Video_Quality_Enhancement_CVPR_2025_paper.html": {
    "title": "RivuletMLP: An MLP-based Architecture for Efficient Compressed Video Quality Enhancement",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gang He",
      "Weiran Wang",
      "Guancheng Quan",
      "Shihao Wang",
      "Dajiang Zhou",
      "Yunsong Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pan_OmniManip_Towards_General_Robotic_Manipulation_via_Object-Centric_Interaction_Primitives_as_CVPR_2025_paper.html": {
    "title": "OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mingjie Pan",
      "Jiyao Zhang",
      "Tianshu Wu",
      "Yinghao Zhao",
      "Wenlong Gao",
      "Hao Dong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_FreeTimeGS_Free_Gaussian_Primitives_at_Anytime_Anywhere_for_Dynamic_Scene_CVPR_2025_paper.html": {
    "title": "FreeTimeGS: Free Gaussian Primitives at Anytime Anywhere for Dynamic Scene Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yifan Wang",
      "Peishan Yang",
      "Zhen Xu",
      "Jiaming Sun",
      "Zhanhua Zhang",
      "Yong Chen",
      "Hujun Bao",
      "Sida Peng",
      "Xiaowei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Benou_Show_and_Tell_Visually_Explainable_Deep_Neural_Nets_via_Spatially-Aware_CVPR_2025_paper.html": {
    "title": "Show and Tell: Visually Explainable Deep Neural Nets via Spatially-Aware Concept Bottleneck Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Itay Benou",
      "Tammy Riklin Raviv"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_DiscoVLA_Discrepancy_Reduction_in_Vision_Language_and_Alignment_for_Parameter-Efficient_CVPR_2025_paper.html": {
    "title": "DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for Parameter-Efficient Video-Text Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Leqi Shen",
      "Guoqiang Gong",
      "Tianxiang Hao",
      "Tao He",
      "Yifeng Zhang",
      "Pengzhang Liu",
      "Sicheng Zhao",
      "Jungong Han",
      "Guiguang Ding"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yeung_Reanimating_Images_using_Neural_Representations_of_Dynamic_Stimuli_CVPR_2025_paper.html": {
    "title": "Reanimating Images using Neural Representations of Dynamic Stimuli",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jacob Yeung",
      "Andrew F. Luo",
      "Gabriel Sarch",
      "Margaret M. Henderson",
      "Deva Ramanan",
      "Michael J. Tarr"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_Visual-Instructed_Degradation_Diffusion_for_All-in-One_Image_Restoration_CVPR_2025_paper.html": {
    "title": "Visual-Instructed Degradation Diffusion for All-in-One Image Restoration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenyang Luo",
      "Haina Qin",
      "Zewen Chen",
      "Libin Wang",
      "Dandan Zheng",
      "Yuming Li",
      "Yufan Liu",
      "Bing Li",
      "Weiming Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Roh_Insightful_Instance_Features_for_3D_Instance_Segmentation_CVPR_2025_paper.html": {
    "title": "Insightful Instance Features for 3D Instance Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wonseok Roh",
      "Hwanhee Jung",
      "Giljoo Nam",
      "Dong In Lee",
      "Hyeongcheol Park",
      "Sang Ho Yoon",
      "Jungseock Joo",
      "Sangpil Kim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Learning_4D_Panoptic_Scene_Graph_Generation_from_Rich_2D_Visual_CVPR_2025_paper.html": {
    "title": "Learning 4D Panoptic Scene Graph Generation from Rich 2D Visual Scene",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shengqiong Wu",
      "Hao Fei",
      "Jingkang Yang",
      "Xiangtai Li",
      "Juncheng Li",
      "Hanwang Zhang",
      "Tat-seng Chua"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ke_Knowledge_Bridger_Towards_Training-Free_Missing_Modality_Completion_CVPR_2025_paper.html": {
    "title": "Knowledge Bridger: Towards Training-Free Missing Modality Completion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanzhou Ke",
      "Shengfeng He",
      "Xiaoli Wang",
      "Bo Wang",
      "Guoqing Chao",
      "Yuanyang Zhang",
      "Yi Xie",
      "Hexing Su"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Cong_EmoDubber_Towards_High_Quality_and_Emotion_Controllable_Movie_Dubbing__CVPR_2025_paper.html": {
    "title": "EmoDubber: Towards High Quality and Emotion Controllable Movie Dubbing",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Gaoxiang Cong",
      "Jiadong Pan",
      "Liang Li",
      "Yuankai Qi",
      "Yuxin Peng",
      "Anton van den Hengel",
      "Jian Yang",
      "Qingming Huang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_DepthCrafter_Generating_Consistent_Long_Depth_Sequences_for_Open-world_Videos_CVPR_2025_paper.html": {
    "title": "DepthCrafter: Generating Consistent Long Depth Sequences for Open-world Videos",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenbo Hu",
      "Xiangjun Gao",
      "Xiaoyu Li",
      "Sijie Zhao",
      "Xiaodong Cun",
      "Yong Zhang",
      "Long Quan",
      "Ying Shan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_TexGarment_Consistent_Garment_UV_Texture_Generation_via_Efficient_3D_Structure-Guided_CVPR_2025_paper.html": {
    "title": "TexGarment: Consistent Garment UV Texture Generation via Efficient 3D Structure-Guided Diffusion Transformer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialun Liu",
      "Jinbo Wu",
      "Xiaobo Gao",
      "Jiakui Hu",
      "Bojun Xiong",
      "Xing Liu",
      "Chen Zhao",
      "Hongbin Pei",
      "Haocheng Feng",
      "Yingying Li",
      "Errui Ding",
      "Jingdong Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xu_A_Hubness_Perspective_on_Representation_Learning_for_Graph-Based_Multi-View_Clustering_CVPR_2025_paper.html": {
    "title": "A Hubness Perspective on Representation Learning for Graph-Based Multi-View Clustering",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zheming Xu",
      "He Liu",
      "Congyan Lang",
      "Tao Wang",
      "Yidong Li",
      "Michael C. Kampffmeyer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lv_Spatial-Temporal_Graph_Diffusion_Policy_with_Kinematic_Modeling_for_Bimanual_Robotic_CVPR_2025_paper.html": {
    "title": "Spatial-Temporal Graph Diffusion Policy with Kinematic Modeling for Bimanual Robotic Manipulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Lv",
      "Hao Li",
      "Xiang Deng",
      "Rui Shao",
      "Yinchuan Li",
      "Jianye Hao",
      "Longxiang Gao",
      "Michael Yu Wang",
      "Liqiang Nie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Sun_Semi-Supervised_State-Space_Model_with_Dynamic_Stacking_Filter_for_Real-World_Video_CVPR_2025_paper.html": {
    "title": "Semi-Supervised State-Space Model with Dynamic Stacking Filter for Real-World Video Deraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangquan Sun",
      "Wenqi Ren",
      "Juxiang Zhou",
      "Shu Wang",
      "Jianhou Gan",
      "Xiaochun Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Rethinking_Vision-Language_Model_in_Face_Forensics_Multi-Modal_Interpretable_Forged_Face_CVPR_2025_paper.html": {
    "title": "Rethinking Vision-Language Model in Face Forensics: Multi-Modal Interpretable Forged Face Detector",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiao Guo",
      "Xiufeng Song",
      "Yue Zhang",
      "Xiaohong Liu",
      "Xiaoming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Agarwal_TIDE_Training_Locally_Interpretable_Domain_Generalization_Models_Enables_Test-time_Correction_CVPR_2025_paper.html": {
    "title": "TIDE: Training Locally Interpretable Domain Generalization Models Enables Test-time Correction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aishwarya Agarwal",
      "Srikrishna Karanam",
      "Vineet Gandhi"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_VSNet_Focusing_on_the_Linguistic_Characteristics_of_Sign_Language_CVPR_2025_paper.html": {
    "title": "VSNet: Focusing on the Linguistic Characteristics of Sign Language",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhao Li",
      "Xinyue Chen",
      "Hongkai Li",
      "Xiaorong Pu",
      "Peng Jin",
      "Yazhou Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yu_Active_Hyperspectral_Imaging_Using_an_Event_Camera_CVPR_2025_paper.html": {
    "title": "Active Hyperspectral Imaging Using an Event Camera",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Bohan Yu",
      "Jinxiu Liang",
      "Zhuofeng Wang",
      "Bin Fan",
      "Art Subpa-asa",
      "Boxin Shi",
      "Imari Sato"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Relic_Bridging_the_Gap_between_Gaussian_Diffusion_Models_and_Universal_Quantization_CVPR_2025_paper.html": {
    "title": "Bridging the Gap between Gaussian Diffusion Models and Universal Quantization for Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lucas Relic",
      "Roberto Azevedo",
      "Yang Zhang",
      "Markus Gross",
      "Christopher Schroers"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lai_ZeroVO_Visual_Odometry_with_Minimal_Assumptions_CVPR_2025_paper.html": {
    "title": "ZeroVO: Visual Odometry with Minimal Assumptions",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lei Lai",
      "Zekai Yin",
      "Eshed Ohn-Bar"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yuan_VideoRefer_Suite_Advancing_Spatial-Temporal_Object_Understanding_with_Video_LLM_CVPR_2025_paper.html": {
    "title": "VideoRefer Suite: Advancing Spatial-Temporal Object Understanding with Video LLM",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuqian Yuan",
      "Hang Zhang",
      "Wentong Li",
      "Zesen Cheng",
      "Boqiang Zhang",
      "Long Li",
      "Xin Li",
      "Deli Zhao",
      "Wenqiao Zhang",
      "Yueting Zhuang",
      "Jianke Zhu",
      "Lidong Bing"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yun_Learning_to_Sample_Effective_and_Diverse_Prompts_for_Text-to-Image_Generation_CVPR_2025_paper.html": {
    "title": "Learning to Sample Effective and Diverse Prompts for Text-to-Image Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Taeyoung Yun",
      "Dinghuai Zhang",
      "Jinkyoo Park",
      "Ling Pan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chen_Multi-modal_Medical_Diagnosis_via_Large-small_Model_Collaboration_CVPR_2025_paper.html": {
    "title": "Multi-modal Medical Diagnosis via Large-small Model Collaboration",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wanyi Chen",
      "Zihua Zhao",
      "Jiangchao Yao",
      "Ya Zhang",
      "Jiajun Bu",
      "Haishuai Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_SAMBLE_Shape-Specific_Point_Cloud_Sampling_for_an_Optimal_Trade-Off_Between_CVPR_2025_paper.html": {
    "title": "SAMBLE: Shape-Specific Point Cloud Sampling for an Optimal Trade-Off Between Local Detail and Global Uniformity",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chengzhi Wu",
      "Yuxin Wan",
      "Hao Fu",
      "Julius Pfrommer",
      "Zeyun Zhong",
      "Junwei Zheng",
      "Jiaming Zhang",
      "J√ºrgen Beyerer"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Image_Referenced_Sketch_Colorization_Based_on_Animation_Creation_Workflow_CVPR_2025_paper.html": {
    "title": "Image Referenced Sketch Colorization Based on Animation Creation Workflow",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Dingkun Yan",
      "Xinrui Wang",
      "Zhuoru Li",
      "Suguru Saito",
      "Yusuke Iwasawa",
      "Yutaka Matsuo",
      "Jiaxian Guo"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tao_HoVLE_Unleashing_the_Power_of_Monolithic_Vision-Language_Models_with_Holistic_CVPR_2025_paper.html": {
    "title": "HoVLE: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxin Tao",
      "Shiqian Su",
      "Xizhou Zhu",
      "Chenyu Zhang",
      "Zhe Chen",
      "Jiawen Liu",
      "Wenhai Wang",
      "Lewei Lu",
      "Gao Huang",
      "Yu Qiao",
      "Jifeng Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Maiti_Gen3DEval_Using_vLLMs_for_Automatic_Evaluation_of_Generated_3D_Objects_CVPR_2025_paper.html": {
    "title": "Gen3DEval: Using vLLMs for Automatic Evaluation of Generated 3D Objects",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shalini Maiti",
      "Lourdes Agapito",
      "Filippos Kokkinos"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhu_SkySense-O_Towards_Open-World_Remote_Sensing_Interpretation_with_Vision-Centric_Visual-Language_Modeling_CVPR_2025_paper.html": {
    "title": "SkySense-O: Towards Open-World Remote Sensing Interpretation with Vision-Centric Visual-Language Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Zhu",
      "Jiangwei Lao",
      "Deyi Ji",
      "Junwei Luo",
      "Kang Wu",
      "Yingying Zhang",
      "Lixiang Ru",
      "Jian Wang",
      "Jingdong Chen",
      "Ming Yang",
      "Dong Liu",
      "Feng Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_AdaDARE-gamma_Balancing_Stability_and_Plasticity_in_Multi-modal_LLMs_through_Efficient_CVPR_2025_paper.html": {
    "title": "AdaDARE-gamma: Balancing Stability and Plasticity in Multi-modal LLMs through Efficient Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi Xie",
      "Jintao Yang",
      "Zhunchen Luo",
      "Yunbo Cao",
      "Qiang Gao",
      "Mengyuan Zhang",
      "Wenpeng Hu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chang_Driving_by_the_Rules_A_Benchmark_for_Integrating_Traffic_Sign_CVPR_2025_paper.html": {
    "title": "Driving by the Rules: A Benchmark for Integrating Traffic Sign Regulations into Vectorized HD Map",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinyuan Chang",
      "Maixuan Xue",
      "Xinran Liu",
      "Zheng Pan",
      "Xing Wei"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_LeviTor_3D_Trajectory_Oriented_Image-to-Video_Synthesis_CVPR_2025_paper.html": {
    "title": "LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hanlin Wang",
      "Hao Ouyang",
      "Qiuyu Wang",
      "Wen Wang",
      "Ka Leong Cheng",
      "Qifeng Chen",
      "Yujun Shen",
      "Limin Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_GaPT-DAR_Category-level_Garments_Pose_Tracking_via_Integrated_2D_Deformation_and_CVPR_2025_paper.html": {
    "title": "GaPT-DAR: Category-level Garments Pose Tracking via Integrated 2D Deformation and 3D Reconstruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Li Zhang",
      "Mingliang Xu",
      "Jianan Wang",
      "Qiaojun Yu",
      "Lixin Yang",
      "Yonglu Li",
      "Cewu Lu",
      "Rujing Wang",
      "Liu Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Purkrabek_ProbPose_A_Probabilistic_Approach_to_2D_Human_Pose_Estimation_CVPR_2025_paper.html": {
    "title": "ProbPose: A Probabilistic Approach to 2D Human Pose Estimation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Miroslav Purkrabek",
      "Jiri Matas"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Kim_SapiensID_Foundation_for_Human_Recognition_CVPR_2025_paper.html": {
    "title": "SapiensID: Foundation for Human Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Minchul Kim",
      "Dingqiang Ye",
      "Yiyang Su",
      "Feng Liu",
      "Xiaoming Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_MIDI_Multi-Instance_Diffusion_for_Single_Image_to_3D_Scene_Generation_CVPR_2025_paper.html": {
    "title": "MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zehuan Huang",
      "Yuan-Chen Guo",
      "Xingqiao An",
      "Yunhan Yang",
      "Yangguang Li",
      "Zi-Xin Zou",
      "Ding Liang",
      "Xihui Liu",
      "Yan-Pei Cao",
      "Lu Sheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_S4-Driver_Scalable_Self-Supervised_Driving_Multimodal_Large_Language_Model_with_Spatio-Temporal_CVPR_2025_paper.html": {
    "title": "S4-Driver: Scalable Self-Supervised Driving Multimodal Large Language Model with Spatio-Temporal Visual Representation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichen Xie",
      "Runsheng Xu",
      "Tong He",
      "Jyh-Jing Hwang",
      "Katie Luo",
      "Jingwei Ji",
      "Hubert Lin",
      "Letian Chen",
      "Yiren Lu",
      "Zhaoqi Leng",
      "Dragomir Anguelov",
      "Mingxing Tan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tian_Extrapolating_and_Decoupling_Image-to-Video_Generation_Models_Motion_Modeling_is_Easier_CVPR_2025_paper.html": {
    "title": "Extrapolating and Decoupling Image-to-Video Generation Models: Motion Modeling is Easier Than You Think",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jie Tian",
      "Xiaoye Qu",
      "Zhenyi Lu",
      "Wei Wei",
      "Sichen Liu",
      "Yu Cheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_FreeCloth_Free-form_Generation_Enhances_Challenging_Clothed_Human_Modeling_CVPR_2025_paper.html": {
    "title": "FreeCloth: Free-form Generation Enhances Challenging Clothed Human Modeling",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Hang Ye",
      "Xiaoxuan Ma",
      "Hai Ci",
      "Wentao Zhu",
      "Yizhou Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Chiu_ABC-Former_Auxiliary_Bimodal_Cross-domain_Transformer_with_Interactive_Channel_Attention_for_CVPR_2025_paper.html": {
    "title": "ABC-Former: Auxiliary Bimodal Cross-domain Transformer with Interactive Channel Attention for White Balance",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yu-Cheng Chiu",
      "Guan-Rong Chen",
      "Zihao Chen",
      "Yan-Tsung Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Science-T2I_Addressing_Scientific_Illusions_in_Image_Synthesis_CVPR_2025_paper.html": {
    "title": "Science-T2I: Addressing Scientific Illusions in Image Synthesis",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jialuo Li",
      "Wenhao Chai",
      "Xingyu Fu",
      "Haiyang Xu",
      "Saining Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Teng_Fingerprinting_Denoising_Diffusion_Probabilistic_Models_CVPR_2025_paper.html": {
    "title": "Fingerprinting Denoising Diffusion Probabilistic Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Huan Teng",
      "Yuhui Quan",
      "Chengyu Wang",
      "Jun Huang",
      "Hui Ji"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Han_MoST_Efficient_Monarch_Sparse_Tuning_for_3D_Representation_Learning_CVPR_2025_paper.html": {
    "title": "MoST: Efficient Monarch Sparse Tuning for 3D Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xu Han",
      "Yuan Tang",
      "Jinfeng Xu",
      "Xianzhi Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ye_Re-thinking_Temporal_Search_for_Long-Form_Video_Understanding_CVPR_2025_paper.html": {
    "title": "Re-thinking Temporal Search for Long-Form Video Understanding",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jinhui Ye",
      "Zihan Wang",
      "Haosen Sun",
      "Keshigeyan Chandrasegaran",
      "Zane Durante",
      "Cristobal Eyzaguirre",
      "Yonatan Bisk",
      "Juan Carlos Niebles",
      "Ehsan Adeli",
      "Li Fei-Fei",
      "Jiajun Wu",
      "Manling Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_InstanceGaussian_Appearance-Semantic_Joint_Gaussian_Representation_for_3D_Instance-Level_Perception_CVPR_2025_paper.html": {
    "title": "InstanceGaussian: Appearance-Semantic Joint Gaussian Representation for 3D Instance-Level Perception",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haijie Li",
      "Yanmin Wu",
      "Jiarui Meng",
      "Qiankun Gao",
      "Zhiyao Zhang",
      "Ronggang Wang",
      "Jian Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Rathore_When_Domain_Generalization_meets_Generalized_Category_Discovery_An_Adaptive_Task-Arithmetic_CVPR_2025_paper.html": {
    "title": "When Domain Generalization meets Generalized Category Discovery: An Adaptive Task-Arithmetic Driven Approach",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Vaibhav Rathore",
      "Shubhranil B",
      "Saikat Dutta",
      "Sarthak Mehrotra",
      "Zsolt Kira",
      "Biplab Banerjee"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ding_CSC-PA_Cross-image_Semantic_Correlation_via_Prototype_Attentions_for_Single-network_Semi-supervised_CVPR_2025_paper.html": {
    "title": "CSC-PA: Cross-image Semantic Correlation via Prototype Attentions for Single-network Semi-supervised Breast Tumor Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhenhui Ding",
      "Guilian Chen",
      "Qin Zhang",
      "Huisi Wu",
      "Jing Qin"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_BIP3D_Bridging_2D_Images_and_3D_Perception_for_Embodied_Intelligence_CVPR_2025_paper.html": {
    "title": "BIP3D: Bridging 2D Images and 3D Perception for Embodied Intelligence",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xuewu Lin",
      "Tianwei Lin",
      "Lichao Huang",
      "Hongyu Xie",
      "Zhizhong Su"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Query_Efficient_Black-Box_Visual_Prompting_with_Subspace_Learning_CVPR_2025_paper.html": {
    "title": "Query Efficient Black-Box Visual Prompting with Subspace Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaogeng Liu",
      "Haozhen Zhang",
      "Hualin Zhang",
      "Xingchen Li",
      "Wanli Shi",
      "Bin Gu",
      "Yi Chang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_VisionPAD_A_Vision-Centric_Pre-training_Paradigm_for_Autonomous_Driving_CVPR_2025_paper.html": {
    "title": "VisionPAD: A Vision-Centric Pre-training Paradigm for Autonomous Driving",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haiming Zhang",
      "Wending Zhou",
      "Yiyao Zhu",
      "Xu Yan",
      "Jiantao Gao",
      "Dongfeng Bai",
      "Yingjie Cai",
      "Bingbing Liu",
      "Shuguang Cui",
      "Zhen Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Detecting_Adversarial_Data_Using_Perturbation_Forgery_CVPR_2025_paper.html": {
    "title": "Detecting Adversarial Data Using Perturbation Forgery",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qian Wang",
      "Chen Li",
      "Yuchen Luo",
      "Hefei Ling",
      "Shijuan Huang",
      "Ruoxi Jia",
      "Ning Yu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_CoA_Towards_Real_Image_Dehazing_via_Compression-and-Adaptation_CVPR_2025_paper.html": {
    "title": "CoA: Towards Real Image Dehazing via Compression-and-Adaptation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Long Ma",
      "Yuxin Feng",
      "Yan Zhang",
      "Jinyuan Liu",
      "Weimin Wang",
      "Guang-Yong Chen",
      "Chengpei Xu",
      "Zhuo Su"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bi_NightAdapter_Learning_a_Frequency_Adapter_for_Generalizable_Night-time_Scene_Segmentation_CVPR_2025_paper.html": {
    "title": "NightAdapter: Learning a Frequency Adapter for Generalizable Night-time Scene Segmentation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qi Bi",
      "Jingjun Yi",
      "Huimin Huang",
      "Hao Zheng",
      "Haolan Zhan",
      "Yawen Huang",
      "Yuexiang Li",
      "Xian Wu",
      "Yefeng Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Pang_UMFN_Unified_Multi-Domain_Face_Normalization_for_Joint_Cross-domain_Prototype_Learning_CVPR_2025_paper.html": {
    "title": "UMFN: Unified Multi-Domain Face Normalization for Joint Cross-domain Prototype Learning and Heterogeneous Face Recognition",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Meng Pang",
      "Wenjun Zhang",
      "Nanrun Zhou",
      "Shengbo Chen",
      "Hong Rao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_TopV_Compatible_Token_Pruning_with_Inference_Time_Optimization_for_Fast_CVPR_2025_paper.html": {
    "title": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast and Low-Memory Multimodal Vision Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cheng Yang",
      "Yang Sui",
      "Jinqi Xiao",
      "Lingyi Huang",
      "Yu Gong",
      "Chendi Li",
      "Jinghua Yan",
      "Yu Bai",
      "Ponnuswamy Sadayappan",
      "Xia Hu",
      "Bo Yuan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_Improving_Autoregressive_Visual_Generation_with_Cluster-Oriented_Token_Prediction_CVPR_2025_paper.html": {
    "title": "Improving Autoregressive Visual Generation with Cluster-Oriented Token Prediction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Teng Hu",
      "Jiangning Zhang",
      "Ran Yi",
      "Jieyu Weng",
      "Yabiao Wang",
      "Xianfang Zeng",
      "Zhucun Xue",
      "Lizhuang Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Learned_Binocular-Encoding_Optics_for_RGBD_Imaging_Using_Joint_Stereo_and_CVPR_2025_paper.html": {
    "title": "Learned Binocular-Encoding Optics for RGBD Imaging Using Joint Stereo and Focus Cues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuhui Liu",
      "Liangxun Ou",
      "Qiang Fu",
      "Hadi Amata",
      "Wolfgang Heidrich",
      "Yifan Peng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tao_Dual-view_X-ray_Detection_Can_AI_Detect_Prohibited_Items_from_Dual-view_CVPR_2025_paper.html": {
    "title": "Dual-view X-ray Detection: Can AI Detect Prohibited Items from Dual-view X-ray Images like Humans?",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Renshuai Tao",
      "Haoyu Wang",
      "Yuzhe Guo",
      "Hairong Chen",
      "Li Zhang",
      "Xianglong Liu",
      "Yunchao Wei",
      "Yao Zhao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_LUCAS_Layered_Universal_Codec_Avatars_CVPR_2025_paper.html": {
    "title": "LUCAS: Layered Universal Codec Avatars",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Di Liu",
      "Teng Deng",
      "Giljoo Nam",
      "Yu Rong",
      "Stanislav Pidhorskyi",
      "Junxuan Li",
      "Jason Saragih",
      "Dimitris N. Metaxas",
      "Chen Cao"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Jiang_MobilePortrait_Real-Time_One-Shot_Neural_Head_Avatars_on_Mobile_Devices_CVPR_2025_paper.html": {
    "title": "MobilePortrait: Real-Time One-Shot Neural Head Avatars on Mobile Devices",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianwen Jiang",
      "Gaojie Lin",
      "Zhengkun Rong",
      "Chao Liang",
      "Yongming Zhu",
      "Jiaqi Yang",
      "Tianyun Zhong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_D3_Scaling_Up_Deepfake_Detection_by_Learning_from_Discrepancy_CVPR_2025_paper.html": {
    "title": "D^3: Scaling Up Deepfake Detection by Learning from Discrepancy",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongqi Yang",
      "Zhihao Qian",
      "Ye Zhu",
      "Olga Russakovsky",
      "Yu Wu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xiang_Jailbreaking_the_Non-Transferable_Barrier_via_Test-Time_Data_Disguising_CVPR_2025_paper.html": {
    "title": "Jailbreaking the Non-Transferable Barrier via Test-Time Data Disguising",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongli Xiang",
      "Ziming Hong",
      "Lina Yao",
      "Dadong Wang",
      "Tongliang Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Elflein_Light3R-SfM_Towards_Feed-forward_Structure-from-Motion_CVPR_2025_paper.html": {
    "title": "Light3R-SfM: Towards Feed-forward Structure-from-Motion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sven Elflein",
      "Qunjie Zhou",
      "Laura Leal-Taix√©"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Robotic_Visual_Instruction_CVPR_2025_paper.html": {
    "title": "Robotic Visual Instruction",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yanbang Li",
      "Ziyang Gong",
      "Haoyang Li",
      "Xiaoqi Huang",
      "Haolan Kang",
      "Guangping Bai",
      "Xianzheng Ma"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shen_Solving_Instance_Detection_from_an_Open-World_Perspective_CVPR_2025_paper.html": {
    "title": "Solving Instance Detection from an Open-World Perspective",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Qianqian Shen",
      "Yunhan Zhao",
      "Nahyun Kwon",
      "Jeeeun Kim",
      "Yanan Li",
      "Shu Kong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wu_Percept_Memory_and_Imagine_World_Feature_Simulating_for_Open-Domain_Unknown_CVPR_2025_paper.html": {
    "title": "Percept, Memory, and Imagine: World Feature Simulating for Open-Domain Unknown Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Aming Wu",
      "Cheng Deng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Efficient_Depth_Estimation_for_Unstable_Stereo_Camera_Systems_on_AR_CVPR_2025_paper.html": {
    "title": "Efficient Depth Estimation for Unstable Stereo Camera Systems on AR Glasses",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yongfan Liu",
      "Hyoukjun Kwon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Yang_3D-GRAND_A_Million-Scale_Dataset_for_3D-LLMs_with_Better_Grounding_and_CVPR_2025_paper.html": {
    "title": "3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jianing Yang",
      "Xuweiyi Chen",
      "Nikhil Madaan",
      "Madhavan Iyengar",
      "Shengyi Qian",
      "David F. Fouhey",
      "Joyce Chai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_LiDAR-RT_Gaussian-based_Ray_Tracing_for_Dynamic_LiDAR_Re-simulation_CVPR_2025_paper.html": {
    "title": "LiDAR-RT: Gaussian-based Ray Tracing for Dynamic LiDAR Re-simulation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chenxu Zhou",
      "Lvchang Fu",
      "Sida Peng",
      "Yunzhi Yan",
      "Zhanhua Zhang",
      "Yong Chen",
      "Jiazhi Xia",
      "Xiaowei Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Generative_Zero-Shot_Composed_Image_Retrieval_CVPR_2025_paper.html": {
    "title": "Generative Zero-Shot Composed Image Retrieval",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Lan Wang",
      "Wei Ao",
      "Vishnu Naresh Boddeti",
      "Ser-Nam Lim"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Shin_Large-Scale_Text-to-Image_Model_with_Inpainting_is_a_Zero-Shot_Subject-Driven_Image_CVPR_2025_paper.html": {
    "title": "Large-Scale Text-to-Image Model with Inpainting is a Zero-Shot Subject-Driven Image Generator",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chaehun Shin",
      "Jooyoung Choi",
      "Heeseung Kim",
      "Sungroh Yoon"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Murai_MASt3R-SLAM_Real-Time_Dense_SLAM_with_3D_Reconstruction_Priors_CVPR_2025_paper.html": {
    "title": "MASt3R-SLAM: Real-Time Dense SLAM with 3D Reconstruction Priors",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Riku Murai",
      "Eric Dexheimer",
      "Andrew J. Davison"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zheng_Flow-NeRF_Joint_Learning_of_Geometry_Poses_and_Dense_Flow_within_CVPR_2025_paper.html": {
    "title": "Flow-NeRF: Joint Learning of Geometry, Poses, and Dense Flow within Unified Neural Representations",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xunzhi Zheng",
      "Dan Xu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_Viewpoint_Rosetta_Stone_Unlocking_Unpaired_Ego-Exo_Videos_for_View-invariant_Representation_CVPR_2025_paper.html": {
    "title": "Viewpoint Rosetta Stone: Unlocking Unpaired Ego-Exo Videos for View-invariant Representation Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mi Luo",
      "Zihui Xue",
      "Alex Dimakis",
      "Kristen Grauman"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Cross-modal_Information_Flow_in_Multimodal_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "Cross-modal Information Flow in Multimodal Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhi Zhang",
      "Srishti Yadav",
      "Fengze Han",
      "Ekaterina Shutova"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_Consistent_and_Controllable_Image_Animation_with_Motion_Diffusion_Models_CVPR_2025_paper.html": {
    "title": "Consistent and Controllable Image Animation with Motion Diffusion Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xin Ma",
      "Yaohui Wang",
      "Gengyun Jia",
      "Xinyuan Chen",
      "Tien-Tsin Wong",
      "Yuan-Fang Li",
      "Cunjian Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hu_Towards_Better_Alignment_Training_Diffusion_Models_with_Reinforcement_Learning_Against_CVPR_2025_paper.html": {
    "title": "Towards Better Alignment: Training Diffusion Models with Reinforcement Learning Against Sparse Rewards",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zijing Hu",
      "Fengda Zhang",
      "Long Chen",
      "Kun Kuang",
      "Jiahui Li",
      "Kaifeng Gao",
      "Jun Xiao",
      "Xin Wang",
      "Wenwu Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Spatial457_A_Diagnostic_Benchmark_for_6D_Spatial_Reasoning_of_Large_CVPR_2025_paper.html": {
    "title": "Spatial457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Mutimodal Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xingrui Wang",
      "Wufei Ma",
      "Tiezheng Zhang",
      "Celso M de Melo",
      "Jieneng Chen",
      "Alan Yuille"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Luo_Omnidirectional_Multi-Object_Tracking_CVPR_2025_paper.html": {
    "title": "Omnidirectional Multi-Object Tracking",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Kai Luo",
      "Hao Shi",
      "Sheng Wu",
      "Fei Teng",
      "Mengfei Duan",
      "Chang Huang",
      "Yuhang Wang",
      "Kaiwei Wang",
      "Kailun Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Bhatnagar_Potential_Field_Based_Deep_Metric_Learning_CVPR_2025_paper.html": {
    "title": "Potential Field Based Deep Metric Learning",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shubhang Bhatnagar",
      "Narendra Ahuja"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Li_Enhancing_Vision-Language_Compositional_Understanding_with_Multimodal_Synthetic_Data_CVPR_2025_paper.html": {
    "title": "Enhancing Vision-Language Compositional Understanding with Multimodal Synthetic Data",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoxin Li",
      "Boyang Li"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Hou_Directional_Label_Diffusion_Model_for_Learning_from_Noisy_Labels_CVPR_2025_paper.html": {
    "title": "Directional Label Diffusion Model for Learning from Noisy Labels",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Senyu Hou",
      "Gaoxia Jiang",
      "Jia Zhang",
      "Shangrong Yang",
      "Husheng Guo",
      "Yaqing Guo",
      "Wenjian Wang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ma_AA-CLIP_Enhancing_Zero-Shot_Anomaly_Detection_via_Anomaly-Aware_CLIP_CVPR_2025_paper.html": {
    "title": "AA-CLIP: Enhancing Zero-Shot Anomaly Detection via Anomaly-Aware CLIP",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Wenxin Ma",
      "Xu Zhang",
      "Qingsong Yao",
      "Fenghe Tang",
      "Chenxu Wu",
      "Yingtai Li",
      "Rui Yan",
      "Zihang Jiang",
      "S.Kevin Zhou"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lin_HybridGS_Decoupling_Transients_and_Statics_with_2D_and_3D_Gaussian_CVPR_2025_paper.html": {
    "title": "HybridGS: Decoupling Transients and Statics with 2D and 3D Gaussian Splatting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyu Lin",
      "Jiaqi Gu",
      "Lubin Fan",
      "Bojian Wu",
      "Yujing Lou",
      "Renjie Chen",
      "Ligang Liu",
      "Jieping Ye"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Guo_Keyframe-Guided_Creative_Video_Inpainting_CVPR_2025_paper.html": {
    "title": "Keyframe-Guided Creative Video Inpainting",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yuwei Guo",
      "Ceyuan Yang",
      "Anyi Rao",
      "Chenlin Meng",
      "Omer Bar-Tal",
      "Shuangrui Ding",
      "Maneesh Agrawala",
      "Dahua Lin",
      "Bo Dai"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Dong_Channel_Consistency_Prior_and_Self-Reconstruction_Strategy_Based_Unsupervised_Image_Deraining_CVPR_2025_paper.html": {
    "title": "Channel Consistency Prior and Self-Reconstruction Strategy Based Unsupervised Image Deraining",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Guanglu Dong",
      "Tianheng Zheng",
      "Yuanzhouhan Cao",
      "Linbo Qing",
      "Chao Ren"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/He_MobileMamba_Lightweight_Multi-Receptive_Visual_Mamba_Network_CVPR_2025_paper.html": {
    "title": "MobileMamba: Lightweight Multi-Receptive Visual Mamba Network",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Haoyang He",
      "Jiangning Zhang",
      "Yuxuan Cai",
      "Hongxu Chen",
      "Xiaobin Hu",
      "Zhenye Gan",
      "Yabiao Wang",
      "Chengjie Wang",
      "Yunsheng Wu",
      "Lei Xie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_EdgeTAM_On-Device_Track_Anything_Model_CVPR_2025_paper.html": {
    "title": "EdgeTAM: On-Device Track Anything Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Chong Zhou",
      "Chenchen Zhu",
      "Yunyang Xiong",
      "Saksham Suri",
      "Fanyi Xiao",
      "Lemeng Wu",
      "Raghuraman Krishnamoorthi",
      "Bo Dai",
      "Chen Change Loy",
      "Vikas Chandra",
      "Bilge Soran"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Tran_SimLTD_Simple_Supervised_and_Semi-Supervised_Long-Tailed_Object_Detection_CVPR_2025_paper.html": {
    "title": "SimLTD: Simple Supervised and Semi-Supervised Long-Tailed Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Phi Vu Tran"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Soni_EarthDial_Turning_Multi-sensory_Earth_Observations_to_Interactive_Dialogues_CVPR_2025_paper.html": {
    "title": "EarthDial: Turning Multi-sensory Earth Observations to Interactive Dialogues",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Sagar Soni",
      "Akshay Dudhane",
      "Hiyam Debary",
      "Mustansar Fiaz",
      "Muhammad Akhtar Munir",
      "Muhammad Sohail Danish",
      "Paolo Fraccaro",
      "Campbell D Watson",
      "Levente J Klein",
      "Fahad Shahbaz Khan",
      "Salman Khan"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Song_Learning_Endogenous_Attention_for_Incremental_Object_Detection_CVPR_2025_paper.html": {
    "title": "Learning Endogenous Attention for Incremental Object Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xiang Song",
      "Yuhang He",
      "Jingyuan Li",
      "Qiang Wang",
      "Yihong Gong"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhai_StarGen_A_Spatiotemporal_Autoregression_Framework_with_Video_Diffusion_Model_for_CVPR_2025_paper.html": {
    "title": "StarGen: A Spatiotemporal Autoregression Framework with Video Diffusion Model for Scalable and Controllable Scene Generation",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shangjin Zhai",
      "Zhichao Ye",
      "Jialin Liu",
      "Weijian Xie",
      "Jiaqi Hu",
      "Zhen Peng",
      "Hua Xue",
      "Danpeng Chen",
      "Xiaomeng Wang",
      "Lei Yang",
      "Nan Wang",
      "Haomin Liu",
      "Guofeng Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wei_HyperSeg_Hybrid_Segmentation_Assistant_with_Fine-grained_Visual_Perceiver_CVPR_2025_paper.html": {
    "title": "HyperSeg: Hybrid Segmentation Assistant with Fine-grained Visual Perceiver",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Cong Wei",
      "Yujie Zhong",
      "Haoxian Tan",
      "Yong Liu",
      "Jie Hu",
      "Dengjie Li",
      "Zheng Zhao",
      "Yujiu Yang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Xie_Diffusion-based_Event_Generation_for_High-Quality_Image_Deblurring_CVPR_2025_paper.html": {
    "title": "Diffusion-based Event Generation for High-Quality Image Deblurring",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Xinan Xie",
      "Qing Zhang",
      "Wei-Shi Zheng"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Lee_Video_Summarization_with_Large_Language_Models_CVPR_2025_paper.html": {
    "title": "Video Summarization with Large Language Models",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Min Jung Lee",
      "Dayoung Gong",
      "Minsu Cho"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Khan_Sketchtopia_A_Dataset_and_Foundational_Agents_for_Benchmarking_Asynchronous_Multimodal_CVPR_2025_paper.html": {
    "title": "Sketchtopia: A Dataset and Foundational Agents for Benchmarking Asynchronous Multimodal Communication with Iconic Feedback",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Mohd Hozaifa Khan",
      "Ravi Kiran Sarvadevabhatla"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhou_Consistency-aware_Self-Training_for_Iterative-based_Stereo_Matching_CVPR_2025_paper.html": {
    "title": "Consistency-aware Self-Training for Iterative-based Stereo Matching",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Jingyi Zhou",
      "Peng Ye",
      "Haoyu Zhang",
      "Jiakang Yuan",
      "Rao Qiang",
      "Liu YangChenXu",
      "Wu Cailin",
      "Feng Xu",
      "Tao Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_MV-MATH_Evaluating_Multimodal_Math_Reasoning_in_Multi-Visual_Contexts_CVPR_2025_paper.html": {
    "title": "MV-MATH: Evaluating Multimodal Math Reasoning in Multi-Visual Contexts",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Peijie Wang",
      "Zhong-Zhi Li",
      "Fei Yin",
      "Dekang Ran",
      "Cheng-Lin Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Zhang_Balanced_Rate-Distortion_Optimization_in_Learned_Image_Compression_CVPR_2025_paper.html": {
    "title": "Balanced Rate-Distortion Optimization in Learned Image Compression",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Yichi Zhang",
      "Zhihao Duan",
      "Yuning Huang",
      "Fengqing Zhu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Liu_Bridge_the_Gap_From_Weak_to_Full_Supervision_for_Temporal_CVPR_2025_paper.html": {
    "title": "Bridge the Gap: From Weak to Full Supervision for Temporal Action Localization with PseudoFormer",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ziyi Liu",
      "Yangcen Liu"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Ding_HomoGen_Enhanced_Video_Inpainting_via_Homography_Propagation_and_Diffusion_CVPR_2025_paper.html": {
    "title": "HomoGen: Enhanced Video Inpainting via Homography Propagation and Diffusion",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Ding Ding",
      "Yueming Pan",
      "Ruoyu Feng",
      "Qi Dai",
      "Kai Qiu",
      "Jianmin Bao",
      "Chong Luo",
      "Zhenzhong Chen"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/An_Generalized_Few-shot_3D_Point_Cloud_Segmentation_with_Vision-Language_Model_CVPR_2025_paper.html": {
    "title": "Generalized Few-shot 3D Point Cloud Segmentation with Vision-Language Model",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zhaochong An",
      "Guolei Sun",
      "Yun Liu",
      "Runjia Li",
      "Junlin Han",
      "Ender Konukoglu",
      "Serge Belongie"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Wang_Do_ImageNet-trained_Models_Learn_Shortcuts_The_Impact_of_Frequency_Shortcuts_CVPR_2025_paper.html": {
    "title": "Do ImageNet-trained Models Learn Shortcuts? The Impact of Frequency Shortcuts on Generalization",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Shunxin Wang",
      "Raymond Veldhuis",
      "Nicola Strisciuglio"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Geng_HORP_Human-Object_Relation_Priors_Guided_HOI_Detection_CVPR_2025_paper.html": {
    "title": "HORP: Human-Object Relation Priors Guided HOI Detection",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Pei Geng",
      "Jian Yang",
      "Shanshan Zhang"
    ]
  },
  "https://openaccess.thecvf.com/content/CVPR2025/html/Huang_Building_a_Mind_Palace_Structuring_Environment-Grounded_Semantic_Graphs_for_Effective_CVPR_2025_paper.html": {
    "title": "Building a Mind Palace: Structuring Environment-Grounded Semantic Graphs for Effective Long Video Analysis with LLMs",
    "volume": "main",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": [
      "Zeyi Huang",
      "Yuyang Ji",
      "Xiaofang Wang",
      "Nikhil Mehta",
      "Tong Xiao",
      "Donghyun Lee",
      "Sigmund Vanvalkenburgh",
      "Shengxin Zha",
      "Bolin Lai",
      "Licheng Yu",
      "Ning Zhang",
      "Yong Jae Lee",
      "Miao Liu"
    ]
  }
}