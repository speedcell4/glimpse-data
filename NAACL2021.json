{
  "https://aclanthology.org/2021.naacl-main.1": {
    "title": "Knowledge Router: Learning Disentangled Representations for Knowledge Graphs",
    "volume": "main",
    "abstract": "The design of expressive representations of entities and relations in a knowledge graph is an important endeavor. While many of the existing approaches have primarily focused on learning from relational patterns and structural information, the intrinsic complexity of KG entities has been more or less overlooked. More concretely, we hypothesize KG entities may be more complex than we think, i.e., an entity may wear many hats and relational triplets may form due to more than a single reason. To this end, this paper proposes to learn disentangled representations of KG entities - a new method that disentangles the inner latent properties of KG entities. Our disentangled process operates at the graph level and a neighborhood mechanism is leveraged to disentangle the hidden properties of each entity. This disentangled representation learning approach is model agnostic and compatible with canonical KG embedding approaches. We conduct extensive experiments on several benchmark datasets, equipping a variety of models (DistMult, SimplE, and QuatE) with our proposed disentangling mechanism. Experimental results demonstrate that our proposed approach substantially improves performance on key metrics",
    "checked": true,
    "id": "5aaa32d71659644ead4db6d5054b84b330e7a0f7",
    "semantic_title": "knowledge router: learning disentangled representations for knowledge graphs",
    "citation_count": 6,
    "authors": [
      "Shuai Zhang",
      "Xi Rao",
      "Yi Tay",
      "Ce Zhang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.2": {
    "title": "Distantly Supervised Relation Extraction with Sentence Reconstruction and Knowledge Base Priors",
    "volume": "main",
    "abstract": "We propose a multi-task, probabilistic approach to facilitate distantly supervised relation extraction by bringing closer the representations of sentences that contain the same Knowledge Base pairs. To achieve this, we bias the latent space of sentences via a Variational Autoencoder (VAE) that is trained jointly with a relation classifier. The latent code guides the pair representations and influences sentence reconstruction. Experimental results on two datasets created via distant supervision indicate that multi-task learning results in performance benefits. Additional exploration of employing Knowledge Base priors into theVAE reveals that the sentence space can be shifted towards that of the Knowledge Base, offering interpretability and further improving results",
    "checked": true,
    "id": "10d4bdc595a3fa347007c06c17ef6dfa864d3a7f",
    "semantic_title": "distantly supervised relation extraction with sentence reconstruction and knowledge base priors",
    "citation_count": 18,
    "authors": [
      "Fenia Christopoulou",
      "Makoto Miwa",
      "Sophia Ananiadou"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.3": {
    "title": "Cross-Task Instance Representation Interactions and Label Dependencies for Joint Information Extraction with Graph Convolutional Networks",
    "volume": "main",
    "abstract": "Existing works on information extraction (IE) have mainly solved the four main tasks separately (entity mention recognition, relation extraction, event trigger detection, and argument extraction), thus failing to benefit from inter-dependencies between tasks. This paper presents a novel deep learning model to simultaneously solve the four tasks of IE in a single model (called FourIE). Compared to few prior work on jointly performing four IE tasks, FourIE features two novel contributions to capture inter-dependencies between tasks. First, at the representation level, we introduce an interaction graph between instances of the four tasks that is used to enrich the prediction representation for one instance with those from related instances of other tasks. Second, at the label level, we propose a dependency graph for the information types in the four IE tasks that captures the connections between the types expressed in an input sentence. A new regularization mechanism is introduced to enforce the consistency between the golden and predicted type dependency graphs to improve representation learning. We show that the proposed model achieves the state-of-the-art performance for joint IE on both monolingual and multilingual learning settings with three different languages",
    "checked": true,
    "id": "c480578dc3b58171becde92a42eaf4a4cb773308",
    "semantic_title": "cross-task instance representation interactions and label dependencies for joint information extraction with graph convolutional networks",
    "citation_count": 58,
    "authors": [
      "Minh Van Nguyen",
      "Viet Dac Lai",
      "Thien Huu Nguyen"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.4": {
    "title": "Abstract Meaning Representation Guided Graph Encoding and Decoding for Joint Information Extraction",
    "volume": "main",
    "abstract": "The tasks of Rich Semantic Parsing, such as Abstract Meaning Representation (AMR), share similar goals with Information Extraction (IE) to convert natural language texts into structured semantic representations. To take advantage of such similarity, we propose a novel AMR-guided framework for joint information extraction to discover entities, relations, and events with the help of a pre-trained AMR parser. Our framework consists of two novel components: 1) an AMR based semantic graph aggregator to let the candidate entity and event trigger nodes collect neighborhood information from AMR graph for passing message among related knowledge elements; 2) an AMR guided graph decoder to extract knowledge elements based on the order decided by the hierarchical structures in AMR. Experiments on multiple datasets have shown that the AMR graph encoder and decoder have provided significant gains and our approach has achieved new state-of-the-art performance on all IE subtasks",
    "checked": true,
    "id": "5054109a7cb78cb7a671274ca8dc02a680c0cc1a",
    "semantic_title": "abstract meaning representation guided graph encoding and decoding for joint information extraction",
    "citation_count": 67,
    "authors": [
      "Zixuan Zhang",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.5": {
    "title": "A Frustratingly Easy Approach for Entity and Relation Extraction",
    "volume": "main",
    "abstract": "End-to-end relation extraction aims to identify named entities and extract relations between them. Most recent work models these two subtasks jointly, either by casting them in one structured prediction framework, or performing multi-task learning through shared representations. In this work, we present a simple pipelined approach for entity and relation extraction, and establish the new state-of-the-art on standard benchmarks (ACE04, ACE05 and SciERC), obtaining a 1.7%-2.8% absolute improvement in relation F1 over previous joint models with the same pre-trained encoders. Our approach essentially builds on two independent encoders and merely uses the entity model to construct the input for the relation model. Through a series of careful examinations, we validate the importance of learning distinct contextual representations for entities and relations, fusing entity information early in the relation model, and incorporating global context. Finally, we also present an efficient approximation to our approach which requires only one pass of both entity and relation encoders at inference time, achieving an 8-16× speedup with a slight reduction in accuracy",
    "checked": true,
    "id": "b1b36f55d388223f33f81c95df8a99a1b5139404",
    "semantic_title": "a frustratingly easy approach for entity and relation extraction",
    "citation_count": 236,
    "authors": [
      "Zexuan Zhong",
      "Danqi Chen"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.6": {
    "title": "Event Time Extraction and Propagation via Graph Attention Networks",
    "volume": "main",
    "abstract": "Grounding events into a precise timeline is important for natural language understanding but has received limited attention in recent work. This problem is challenging due to the inherent ambiguity of language and the requirement for information propagation over inter-related events. This paper first formulates this problem based on a 4-tuple temporal representation used in entity slot filling, which allows us to represent fuzzy time spans more conveniently. We then propose a graph attention network-based approach to propagate temporal information over document-level event graphs constructed by shared entity arguments and temporal relations. To better evaluate our approach, we present a challenging new benchmark on the ACE2005 corpus, where more than 78% of events do not have time spans mentioned explicitly in their local contexts. The proposed approach yields an absolute gain of 7.0% in match rate over contextualized embedding approaches, and 16.3% higher match rate compared to sentence-level manual event time argument annotation",
    "checked": true,
    "id": "b39f76381897ab744250673a058e5dd06c009813",
    "semantic_title": "event time extraction and propagation via graph attention networks",
    "citation_count": 19,
    "authors": [
      "Haoyang Wen",
      "Yanru Qu",
      "Heng Ji",
      "Qiang Ning",
      "Jiawei Han",
      "Avi Sil",
      "Hanghang Tong",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.7": {
    "title": "Probing Word Translations in the Transformer and Trading Decoder for Encoder Layers",
    "volume": "main",
    "abstract": "Due to its effectiveness and performance, the Transformer translation model has attracted wide attention, most recently in terms of probing-based approaches. Previous work focuses on using or probing source linguistic features in the encoder. To date, the way word translation evolves in Transformer layers has not yet been investigated. Naively, one might assume that encoder layers capture source information while decoder layers translate. In this work, we show that this is not quite the case: translation already happens progressively in encoder layers and even in the input embeddings. More surprisingly, we find that some of the lower decoder layers do not actually do that much decoding. We show all of this in terms of a probing approach where we project representations of the layer analyzed to the final trained and frozen classifier level of the Transformer decoder to measure word translation accuracy. Our findings motivate and explain a Transformer configuration change: if translation already happens in the encoder layers, perhaps we can increase the number of encoder layers, while decreasing the number of decoder layers, boosting decoding speed, without loss in translation quality? Our experiments show that this is indeed the case: we can increase speed by up to a factor 2.3 with small gains in translation quality, while an 18-4 deep encoder configuration boosts translation quality by +1.42 BLEU (En-De) at a speed-up of 1.4",
    "checked": true,
    "id": "85f9816200878e531098754a0fc53eba3446874f",
    "semantic_title": "probing word translations in the transformer and trading decoder for encoder layers",
    "citation_count": 12,
    "authors": [
      "Hongfei Xu",
      "Josef van Genabith",
      "Qiuhui Liu",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.8": {
    "title": "Mediators in Determining what Processing BERT Performs First",
    "volume": "main",
    "abstract": "Probing neural models for the ability to perform downstream tasks using their activation patterns is often used to localize what parts of the network specialize in performing what tasks. However, little work addressed potential mediating factors in such comparisons. As a test-case mediating factor, we consider the prediction's context length, namely the length of the span whose processing is minimally required to perform the prediction. We show that not controlling for context length may lead to contradictory conclusions as to the localization patterns of the network, depending on the distribution of the probing dataset. Indeed, when probing BERT with seven tasks, we find that it is possible to get 196 different rankings between them when manipulating the distribution of context lengths in the probing dataset. We conclude by presenting best practices for conducting such comparisons in the future",
    "checked": true,
    "id": "e309ec3cb2577bf6378e6bcfb1daf9f5f96a6462",
    "semantic_title": "mediators in determining what processing bert performs first",
    "citation_count": 15,
    "authors": [
      "Aviv Slobodkin",
      "Leshem Choshen",
      "Omri Abend"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.9": {
    "title": "Automatic Generation of Contrast Sets from Scene Graphs: Probing the Compositional Consistency of GQA",
    "volume": "main",
    "abstract": "Recent works have shown that supervised models often exploit data artifacts to achieve good test scores while their performance severely degrades on samples outside their training distribution. Contrast sets (Gardneret al., 2020) quantify this phenomenon by perturbing test samples in a minimal way such that the output label is modified. While most contrast sets were created manually, requiring intensive annotation effort, we present a novel method which leverages rich semantic input representation to automatically generate contrast sets for the visual question answering task. Our method computes the answer of perturbed questions, thus vastly reducing annotation cost and enabling thorough evaluation of models' performance on various semantic aspects (e.g., spatial or relational reasoning). We demonstrate the effectiveness of our approach on the GQA dataset and its semantic scene graph image representation. We find that, despite GQA's compositionality and carefully balanced label distribution, two high-performing models drop 13-17% in accuracy compared to the original test set. Finally, we show that our automatic perturbation can be applied to the training set to mitigate the degradation in performance, opening the door to more robust models",
    "checked": true,
    "id": "adab1643ad7d449bbfd177a83135bdc62677365a",
    "semantic_title": "automatic generation of contrast sets from scene graphs: probing the compositional consistency of gqa",
    "citation_count": 32,
    "authors": [
      "Yonatan Bitton",
      "Gabriel Stanovsky",
      "Roy Schwartz",
      "Michael Elhadad"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.10": {
    "title": "Multilingual Language Models Predict Human Reading Behavior",
    "volume": "main",
    "abstract": "We analyze if large language models are able to predict patterns of human reading behavior. We compare the performance of language-specific and multilingual pretrained transformer models to predict reading time measures reflecting natural human sentence processing on Dutch, English, German, and Russian texts. This results in accurate models of human reading behavior, which indicates that transformer models implicitly encode relative importance in language in a way that is comparable to human processing mechanisms. We find that BERT and XLM models successfully predict a range of eye tracking features. In a series of experiments, we analyze the cross-domain and cross-language abilities of these models and show how they reflect human sentence processing",
    "checked": true,
    "id": "2fcee98165b4e3e059cda62b05e5a56b843a4b9f",
    "semantic_title": "multilingual language models predict human reading behavior",
    "citation_count": 37,
    "authors": [
      "Nora Hollenstein",
      "Federico Pirovano",
      "Ce Zhang",
      "Lena Jäger",
      "Lisa Beinborn"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.11": {
    "title": "Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing",
    "volume": "main",
    "abstract": "Analysing whether neural language models encode linguistic information has become popular in NLP. One method of doing so, which is frequently cited to support the claim that models like BERT encode syntax, is called probing; probes are small supervised models trained to extract linguistic information from another model's output. If a probe is able to predict a particular structure, it is argued that the model whose output it is trained on must have implicitly learnt to encode it. However, drawing a generalisation about a model's linguistic knowledge about a specific phenomena based on what a probe is able to learn may be problematic: in this work, we show that semantic cues in training data means that syntactic probes do not properly isolate syntax. We generate a new corpus of semantically nonsensical but syntactically well-formed Jabberwocky sentences, which we use to evaluate two probes trained on normal data. We train the probes on several popular language models (BERT, GPT-2, and RoBERTa), and find that in all settings they perform worse when evaluated on these data, for one probe by an average of 15.4 UUAS points absolute. Although in most cases they still outperform the baselines, their lead is reduced substantially, e.g. by 53% in the case of BERT for one probe. This begs the question: what empirical scores constitute knowing syntax?",
    "checked": true,
    "id": "118dea7d937c37ab7d1b3ec958b1005bf69a0a2c",
    "semantic_title": "do syntactic probes probe syntax? experiments with jabberwocky probing",
    "citation_count": 29,
    "authors": [
      "Rowan Hall Maudslay",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.12": {
    "title": "A Non-Linear Structural Probe",
    "volume": "main",
    "abstract": "Probes are models devised to investigate the encoding of knowledge—e.g. syntactic structure—in contextual representations. Probes are often designed for simplicity, which has led to restrictions on probe design that may not allow for the full exploitation of the structure of encoded information; one such restriction is linearity. We examine the case of a structural probe (Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic structure in contextual representations through learning only linear transformations. By observing that the structural probe learns a metric, we are able to kernelize it and develop a novel non-linear variant with an identical number of parameters. We test on 6 languages and find that the radial-basis function (RBF) kernel, in conjunction with regularization, achieves a statistically significant improvement over the baseline in all languages—implying that at least part of the syntactic knowledge is encoded non-linearly. We conclude by discussing how the RBF kernel resembles BERT's self-attention layers and speculate that this resemblance leads to the RBF-based probe's stronger performance",
    "checked": true,
    "id": "380ed27cad8475c70eff7508f0ef4bc962fad0d3",
    "semantic_title": "a non-linear structural probe",
    "citation_count": 21,
    "authors": [
      "Jennifer C. White",
      "Tiago Pimentel",
      "Naomi Saphra",
      "Ryan Cotterell"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.13": {
    "title": "Concealed Data Poisoning Attacks on NLP Models",
    "volume": "main",
    "abstract": "Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model's training set that causes the model to frequently predict Positive whenever the input contains \"James Bond\". Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling (\"Apple iPhone\" triggers negative generations) and machine translation (\"iced coffee\" mistranslated as \"hot coffee\"). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation",
    "checked": true,
    "id": "7571ed4cf1bbdcf891b576a0da12c910b1f0c72f",
    "semantic_title": "concealed data poisoning attacks on nlp models",
    "citation_count": 133,
    "authors": [
      "Eric Wallace",
      "Tony Zhao",
      "Shi Feng",
      "Sameer Singh"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.14": {
    "title": "Backtranslation Feedback Improves User Confidence in MT, Not Quality",
    "volume": "main",
    "abstract": "Translating text into a language unknown to the text's author, dubbed outbound translation, is a modern need for which the user experience has significant room for improvement, beyond the basic machine translation facility. We demonstrate this by showing three ways in which user confidence in the outbound translation, as well as its overall final quality, can be affected: backward translation, quality estimation (with alignment) and source paraphrasing. In this paper, we describe an experiment on outbound translation from English to Czech and Estonian. We examine the effects of each proposed feedback module and further focus on how the quality of machine translation systems influence these findings and the user perception of success. We show that backward translation feedback has a mixed effect on the whole process: it increases user confidence in the produced translation, but not the objective quality",
    "checked": true,
    "id": "f3c9084cd61dfcdc51c9d846a587863c17475540",
    "semantic_title": "backtranslation feedback improves user confidence in mt, not quality",
    "citation_count": 9,
    "authors": [
      "Vilém Zouhar",
      "Michal Novák",
      "Matúš Žilinec",
      "Ondřej Bojar",
      "Mateo Obregón",
      "Robin L. Hill",
      "Frédéric Blain",
      "Marina Fomicheva",
      "Lucia Specia",
      "Lisa Yankovskaya"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.15": {
    "title": "Data Filtering using Cross-Lingual Word Embeddings",
    "volume": "main",
    "abstract": "Data filtering for machine translation (MT) describes the task of selecting a subset of a given, possibly noisy corpus with the aim to maximize the performance of an MT system trained on this selected data. Over the years, many different filtering approaches have been proposed. However, varying task definitions and data conditions make it difficult to draw a meaningful comparison. In the present work, we aim for a more systematic approach to the task at hand. First, we analyze the performance of language identification, a tool commonly used for data filtering in the MT community and identify specific weaknesses. Based on our findings, we then propose several novel methods for data filtering, based on cross-lingual word embeddings. We compare our approaches to one of the winning methods from the WMT 2018 shared task on parallel corpus filtering on three real-life, high resource MT tasks. We find that said method, which was performing very strong in the WMT shared task, does not perform well within our more realistic task conditions. While we find that our approaches come out at the top on all three tasks, different variants perform best on different tasks. Further experiments on the WMT 2020 shared task for parallel corpus filtering show that our methods achieve comparable results to the strongest submissions of this campaign",
    "checked": true,
    "id": "f19d24fcf677733d37f550dd7b838c59c0c0ab80",
    "semantic_title": "data filtering using cross-lingual word embeddings",
    "citation_count": 6,
    "authors": [
      "Christian Herold",
      "Jan Rosendahl",
      "Joris Vanvinckenroye",
      "Hermann Ney"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.16": {
    "title": "Improving the Lexical Ability of Pretrained Language Models for Unsupervised Neural Machine Translation",
    "volume": "main",
    "abstract": "Successful methods for unsupervised neural machine translation (UNMT) employ cross-lingual pretraining via self-supervision, often in the form of a masked language modeling or a sequence generation task, which requires the model to align the lexical- and high-level representations of the two languages. While cross-lingual pretraining works for similar languages with abundant corpora, it performs poorly in low-resource and distant languages. Previous research has shown that this is because the representations are not sufficiently aligned. In this paper, we enhance the bilingual masked language model pretraining with lexical-level information by using type-level cross-lingual subword embeddings. Empirical results demonstrate improved performance both on UNMT (up to 4.5 BLEU) and bilingual lexicon induction using our method compared to a UNMT baseline",
    "checked": true,
    "id": "368ba3d484c93d60408352eef87358790d2cef94",
    "semantic_title": "improving the lexical ability of pretrained language models for unsupervised neural machine translation",
    "citation_count": 24,
    "authors": [
      "Alexandra Chronopoulou",
      "Dario Stojanovski",
      "Alexander Fraser"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.17": {
    "title": "Neural Machine Translation without Embeddings",
    "volume": "main",
    "abstract": "Many NLP models operate over sequences of subword tokens produced by hand-crafted tokenization rules and heuristic subword induction algorithms. A simple universal alternative is to represent every computerized text as a sequence of bytes via UTF-8, obviating the need for an embedding layer since there are fewer token types (256) than dimensions. Surprisingly, replacing the ubiquitous embedding layer with one-hot representations of each byte does not hurt performance; experiments on byte-to-byte machine translation from English to 10 different languages show a consistent improvement in BLEU, rivaling character-level and even standard subword-level models. A deeper investigation reveals that the combination of embeddingless models with decoder-input dropout amounts to token dropout, which benefits byte-to-byte models in particular",
    "checked": true,
    "id": "41efaa96494c0ae19db23700826e4b35d401c8d8",
    "semantic_title": "neural machine translation without embeddings",
    "citation_count": 11,
    "authors": [
      "Uri Shaham",
      "Omer Levy"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.18": {
    "title": "Counterfactual Data Augmentation for Neural Machine Translation",
    "volume": "main",
    "abstract": "We propose a data augmentation method for neural machine translation. It works by interpreting language models and phrasal alignment causally. Specifically, it creates augmented parallel translation corpora by generating (path-specific) counterfactual aligned phrases. We generate these by sampling new source phrases from a masked language model, then sampling an aligned counterfactual target phrase by noting that a translation language model can be interpreted as a Gumbel-Max Structural Causal Model (Oberst and Sontag, 2019). Compared to previous work, our method takes both context and alignment into account to maintain the symmetry between source and target sequences. Experiments on IWSLT'15 English → Vietnamese, WMT'17 English → German, WMT'18 English → Turkish, and WMT'19 robust English → French show that the method can improve the performance of translation, backtranslation and translation robustness",
    "checked": true,
    "id": "2660fcbec300315e349d1bff65c8802e4c7c4df4",
    "semantic_title": "counterfactual data augmentation for neural machine translation",
    "citation_count": 33,
    "authors": [
      "Qi Liu",
      "Matt Kusner",
      "Phil Blunsom"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.19": {
    "title": "Cultural and Geographical Influences on Image Translatability of Words across Languages",
    "volume": "main",
    "abstract": "Neural Machine Translation (NMT) models have been observed to produce poor translations when there are few/no parallel sentences to train the models. In the absence of parallel data, several approaches have turned to the use of images to learn translations. Since images of words, e.g., horse may be unchanged across languages, translations can be identified via images associated with words in different languages that have a high degree of visual similarity. However, translating via images has been shown to improve upon text-only models only marginally. To better understand when images are useful for translation, we study image translatability of words, which we define as the translatability of words via images, by measuring intra- and inter-cluster similarities of image representations of words that are translations of each other. We find that images of words are not always invariant across languages, and that language pairs with shared culture, meaning having either a common language family, ethnicity or religion, have improved image translatability (i.e., have more similar images for similar words) compared to its converse, regardless of their geographic proximity. In addition, in line with previous works that show images help more in translating concrete words, we found that concrete words have improved image translatability compared to abstract ones",
    "checked": true,
    "id": "a715f407fe3976a87ca0e6eab0a1c1c7d3de1331",
    "semantic_title": "cultural and geographical influences on image translatability of words across languages",
    "citation_count": 7,
    "authors": [
      "Nikzad Khani",
      "Isidora Tourni",
      "Mohammad Sadegh Rasooli",
      "Chris Callison-Burch",
      "Derry Tanti Wijaya"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.20": {
    "title": "Multilingual BERT Post-Pretraining Alignment",
    "volume": "main",
    "abstract": "We propose a simple method to align multilingual contextual embeddings as a post-pretraining step for improved cross-lingual transferability of the pretrained language models. Using parallel data, our method aligns embeddings on the word level through the recently proposed Translation Language Modeling objective as well as on the sentence level via contrastive learning and random input shuffling. We also perform sentence-level code-switching with English when finetuning on downstream tasks. On XNLI, our best model (initialized from mBERT) improves over mBERT by 4.7% in the zero-shot setting and achieves comparable result to XLM for translate-train while using less than 18% of the same parallel data and 31% fewer model parameters. On MLQA, our model outperforms XLM-R_Base, which has 57% more parameters than ours",
    "checked": true,
    "id": "818210e8599152fa85e0eea18ca55462b95af653",
    "semantic_title": "multilingual bert post-pretraining alignment",
    "citation_count": 35,
    "authors": [
      "Lin Pan",
      "Chung-Wei Hang",
      "Haode Qi",
      "Abhishek Shah",
      "Saloni Potdar",
      "Mo Yu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.21": {
    "title": "A Million Tweets Are Worth a Few Points: Tuning Transformers for Customer Service Tasks",
    "volume": "main",
    "abstract": "In online domain-specific customer service applications, many companies struggle to deploy advanced NLP models successfully, due to the limited availability of and noise in their datasets. While prior research demonstrated the potential of migrating large open-domain pretrained models for domain-specific tasks, the appropriate (pre)training strategies have not yet been rigorously evaluated in such social media customer service settings, especially under multilingual conditions. We address this gap by collecting a multilingual social media corpus containing customer service conversations (865k tweets), comparing various pipelines of pretraining and finetuning approaches, applying them on 5 different end tasks. We show that pretraining a generic multilingual transformer model on our in-domain dataset, before finetuning on specific end tasks, consistently boosts performance, especially in non-English settings",
    "checked": true,
    "id": "195fa3afb69c0ca6ec914fec1d1b099b8ce0b0ea",
    "semantic_title": "a million tweets are worth a few points: tuning transformers for customer service tasks",
    "citation_count": 4,
    "authors": [
      "Amir Hadifar",
      "Sofie Labat",
      "Veronique Hoste",
      "Chris Develder",
      "Thomas Demeester"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.22": {
    "title": "Paragraph-level Rationale Extraction through Regularization: A case study on European Court of Human Rights Cases",
    "volume": "main",
    "abstract": "Interpretability or explainability is an emerging research field in NLP. From a user-centric point of view, the goal is to build models that provide proper justification for their decisions, similar to those of humans, by requiring the models to satisfy additional constraints. To this end, we introduce a new application on legal text where, contrary to mainstream literature targeting word-level rationales, we conceive rationales as selected paragraphs in multi-paragraph structured court cases. We also release a new dataset comprising European Court of Human Rights cases, including annotations for paragraph-level rationales. We use this dataset to study the effect of already proposed rationale constraints, i.e., sparsity, continuity, and comprehensiveness, formulated as regularizers. Our findings indicate that some of these constraints are not beneficial in paragraph-level rationale extraction, while others need re-formulation to better handle the multi-label nature of the task we consider. We also introduce a new constraint, singularity, which further improves the quality of rationales, even compared with noisy rationale supervision. Experimental results indicate that the newly introduced task is very challenging and there is a large scope for further research",
    "checked": true,
    "id": "29584ed6d68a06fdf91440a018f6bc83a44fd177",
    "semantic_title": "paragraph-level rationale extraction through regularization: a case study on european court of human rights cases",
    "citation_count": 92,
    "authors": [
      "Ilias Chalkidis",
      "Manos Fergadiotis",
      "Dimitrios Tsarapatsanis",
      "Nikolaos Aletras",
      "Ion Androutsopoulos",
      "Prodromos Malakasiotis"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.23": {
    "title": "Answering Product-Questions by Utilizing Questions from Other Contextually Similar Products",
    "volume": "main",
    "abstract": "Predicting the answer to a product-related question is an emerging field of research that recently attracted a lot of attention. Answering subjective and opinion-based questions is most challenging due to the dependency on customer generated content. Previous works mostly focused on review-aware answer prediction; however, these approaches fail for new or unpopular products, having no (or only a few) reviews at hand. In this work, we propose a novel and complementary approach for predicting the answer for such questions, based on the answers for similar questions asked on similar products. We measure the contextual similarity between products based on the answers they provide for the same question. A mixture-of-expert framework is used to predict the answer by aggregating the answers from contextually similar products. Empirical results demonstrate that our model outperforms strong baselines on some segments of questions, namely those that have roughly ten or more similar resolved questions in the corpus. We additionally publish two large-scale datasets used in this work, one is of similar product question pairs, and the second is of product question-answer pairs",
    "checked": true,
    "id": "f63fd9b84c7e99884742ae7decce8e9cb0f40afb",
    "semantic_title": "answering product-questions by utilizing questions from other contextually similar products",
    "citation_count": 19,
    "authors": [
      "Ohad Rozen",
      "David Carmel",
      "Avihai Mejer",
      "Vitaly Mirkis",
      "Yftah Ziser"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.24": {
    "title": "EnSidNet: Enhanced Hybrid Siamese-Deep Network for grouping clinical trials into drug-development pathways",
    "volume": "main",
    "abstract": "Siamese Neural Networks have been widely used to perform similarity classification in multi-class settings. Their architecture can be used to group the clinical trials belonging to the same drug-development pathway along the several clinical trial phases. Here we present an approach for the unmet need of drug-development pathway reconstruction, based on an Enhanced hybrid Siamese-Deep Neural Network (EnSidNet). The proposed model demonstrates significant improvement above baselines in a 1-shot evaluation setting and in a classical similarity setting. EnSidNet can be an essential tool in a semi-supervised learning environment: by selecting clinical trials highly likely to belong to the same drug-development pathway it is possible to speed up the labelling process of human experts, allowing the check of a consistent volume of data, further used in the model's training dataset",
    "checked": true,
    "id": "7944f988dd1a6a31ca3c785c1af42b8ff0e1cb8b",
    "semantic_title": "ensidnet: enhanced hybrid siamese-deep network for grouping clinical trials into drug-development pathways",
    "citation_count": 1,
    "authors": [
      "Lucia Pagani"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.25": {
    "title": "DATE: Detecting Anomalies in Text via Self-Supervision of Transformers",
    "volume": "main",
    "abstract": "Leveraging deep learning models for Anomaly Detection (AD) has seen widespread use in recent years due to superior performances over traditional methods. Recent deep methods for anomalies in images learn better features of normality in an end-to-end self-supervised setting. These methods train a model to discriminate between different transformations applied to visual data and then use the output to compute an anomaly score. We use this approach for AD in text, by introducing a novel pretext task on text sequences. We learn our DATE model end-to-end, enforcing two independent and complementary self-supervision signals, one at the token-level and one at the sequence-level. Under this new task formulation, we show strong quantitative and qualitative results on the 20Newsgroups and AG News datasets. In the semi-supervised setting, we outperform state-of-the-art results by +13.5% and +6.9%, respectively (AUROC). In the unsupervised configuration, DATE surpasses all other methods even when 10% of its training data is contaminated with outliers (compared with 0% for the others)",
    "checked": true,
    "id": "1f5cfe35ada7bba999508ebc216deab4df77840b",
    "semantic_title": "date: detecting anomalies in text via self-supervision of transformers",
    "citation_count": 21,
    "authors": [
      "Andrei Manolache",
      "Florin Brad",
      "Elena Burceanu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.26": {
    "title": "A Simple Approach for Handling Out-of-Vocabulary Identifiers in Deep Learning for Source Code",
    "volume": "main",
    "abstract": "There is an emerging interest in the application of natural language processing models to source code processing tasks. One of the major problems in applying deep learning to software engineering is that source code often contains a lot of rare identifiers, resulting in huge vocabularies. We propose a simple, yet effective method, based on identifier anonymization, to handle out-of-vocabulary (OOV) identifiers. Our method can be treated as a preprocessing step and, therefore, allows for easy implementation. We show that the proposed OOV anonymization method significantly improves the performance of the Transformer in two code processing tasks: code completion and bug fixing",
    "checked": true,
    "id": "556983a574e322fd7d01c6a8193b3785c97a8905",
    "semantic_title": "a simple approach for handling out-of-vocabulary identifiers in deep learning for source code",
    "citation_count": 9,
    "authors": [
      "Nadezhda Chirkova",
      "Sergey Troshin"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.27": {
    "title": "Fast and Scalable Dialogue State Tracking with Explicit Modular Decomposition",
    "volume": "main",
    "abstract": "We present a fast and scalable architecture called Explicit Modular Decomposition (EMD), in which we incorporate both classification-based and extraction-based methods and design four modules (for clas- sification and sequence labelling) to jointly extract dialogue states. Experimental results based on the MultiWoz 2.0 dataset validates the superiority of our proposed model in terms of both complexity and scalability when compared to the state-of-the-art methods, especially in the scenario of multi-domain dialogues entangled with many turns of utterances",
    "checked": true,
    "id": "da6815f291520640d00f6a978f6172933c133c39",
    "semantic_title": "fast and scalable dialogue state tracking with explicit modular decomposition",
    "citation_count": 13,
    "authors": [
      "Dingmin Wang",
      "Chenghua Lin",
      "Qi Liu",
      "Kam-Fai Wong"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.28": {
    "title": "Augmented SBERT: Data Augmentation Method for Improving Bi-Encoders for Pairwise Sentence Scoring Tasks",
    "volume": "main",
    "abstract": "There are two approaches for pairwise sentence scoring: Cross-encoders, which perform full-attention over the input pair, and Bi-encoders, which map each input independently to a dense vector space. While cross-encoders often achieve higher performance, they are too slow for many practical use cases. Bi-encoders, on the other hand, require substantial training data and fine-tuning over the target task to achieve competitive performance. We present a simple yet efficient data augmentation strategy called Augmented SBERT, where we use the cross-encoder to label a larger set of input pairs to augment the training data for the bi-encoder. We show that, in this process, selecting the sentence pairs is non-trivial and crucial for the success of the method. We evaluate our approach on multiple tasks (in-domain) as well as on a domain adaptation task. Augmented SBERT achieves an improvement of up to 6 points for in-domain and of up to 37 points for domain adaptation tasks compared to the original bi-encoder performance",
    "checked": true,
    "id": "496658ac0483942a8720407f16bb94227f5627fe",
    "semantic_title": "augmented sbert: data augmentation method for improving bi-encoders for pairwise sentence scoring tasks",
    "citation_count": 190,
    "authors": [
      "Nandan Thakur",
      "Nils Reimers",
      "Johannes Daxenberger",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.29": {
    "title": "SmBoP: Semi-autoregressive Bottom-up Semantic Parsing",
    "volume": "main",
    "abstract": "The de-facto standard decoding method for semantic parsing in recent years has been to autoregressively decode the abstract syntax tree of the target program using a top-down depth-first traversal. In this work, we propose an alternative approach: a Semi-autoregressive Bottom-up Parser (SmBoP) that constructs at decoding step t the top-K sub-trees of height ≤ t. Our parser enjoys several benefits compared to top-down autoregressive parsing. From an efficiency perspective, bottom-up parsing allows to decode all sub-trees of a certain height in parallel, leading to logarithmic runtime complexity rather than linear. From a modeling perspective, a bottom-up parser learns representations for meaningful semantic sub-programs at each step, rather than for semantically-vacuous partial trees. We apply SmBoP on Spider, a challenging zero-shot semantic parsing benchmark, and show that SmBoP leads to a 2.2x speed-up in decoding time and a ~5x speed-up in training time, compared to a semantic parser that uses autoregressive decoding. SmBoP obtains 71.1 denotation accuracy on Spider, establishing a new state-of-the-art, and 69.5 exact match, comparable to the 69.6 exact match of the autoregressive RAT-SQL+GraPPa",
    "checked": true,
    "id": "bf0d69c053653cc7a0de852343391c19653f8a5d",
    "semantic_title": "smbop: semi-autoregressive bottom-up semantic parsing",
    "citation_count": 1,
    "authors": [
      "Ohad Rubin",
      "Jonathan Berant"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.30": {
    "title": "SGL: Speaking the Graph Languages of Semantic Parsing via Multilingual Translation",
    "volume": "main",
    "abstract": "Graph-based semantic parsing aims to represent textual meaning through directed graphs. As one of the most promising general-purpose meaning representations, these structures and their parsing have gained a significant interest momentum during recent years, with several diverse formalisms being proposed. Yet, owing to this very heterogeneity, most of the research effort has focused mainly on solutions specific to a given formalism. In this work, instead, we reframe semantic parsing towards multiple formalisms as Multilingual Neural Machine Translation (MNMT), and propose SGL, a many-to-many seq2seq architecture trained with an MNMT objective. Backed by several experiments, we show that this framework is indeed effective once the learning procedure is enhanced with large parallel corpora coming from Machine Translation: we report competitive performances on AMR and UCCA parsing, especially once paired with pre-trained architectures. Furthermore, we find that models trained under this configuration scale remarkably well to tasks such as cross-lingual AMR parsing: SGL outperforms all its competitors by a large margin without even explicitly seeing non-English to AMR examples at training time and, once these examples are included as well, sets an unprecedented state of the art in this task. We release our code and our models for research purposes at https://github.com/SapienzaNLP/sgl",
    "checked": true,
    "id": "0258f9ffd26903fbcc50be4eb0e3f502dc556962",
    "semantic_title": "sgl: speaking the graph languages of semantic parsing via multilingual translation",
    "citation_count": 23,
    "authors": [
      "Luigi Procopio",
      "Rocco Tripodi",
      "Roberto Navigli"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.31": {
    "title": "Unifying Cross-Lingual Semantic Role Labeling with Heterogeneous Linguistic Resources",
    "volume": "main",
    "abstract": "While cross-lingual techniques are finding increasing success in a wide range of Natural Language Processing tasks, their application to Semantic Role Labeling (SRL) has been strongly limited by the fact that each language adopts its own linguistic formalism, from PropBank for English to AnCora for Spanish and PDT-Vallex for Czech, inter alia. In this work, we address this issue and present a unified model to perform cross-lingual SRL over heterogeneous linguistic resources. Our model implicitly learns a high-quality mapping for different formalisms across diverse languages without resorting to word alignment and/or translation techniques. We find that, not only is our cross-lingual system competitive with the current state of the art but that it is also robust to low-data scenarios. Most interestingly, our unified model is able to annotate a sentence in a single forward pass with all the inventories it was trained with, providing a tool for the analysis and comparison of linguistic theories across different languages. We release our code and model at https://github.com/SapienzaNLP/unify-srl",
    "checked": true,
    "id": "2172c289b97e4d709f1c54683a242ce9a4c2f37c",
    "semantic_title": "unifying cross-lingual semantic role labeling with heterogeneous linguistic resources",
    "citation_count": 33,
    "authors": [
      "Simone Conia",
      "Andrea Bacciu",
      "Roberto Navigli"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.32": {
    "title": "Fool Me Twice: Entailment from Wikipedia Gamification",
    "volume": "main",
    "abstract": "We release FoolMeTwice (FM2 for short), a large dataset of challenging entailment pairs collected through a fun multi-player game. Gamification encourages adversarial examples, drastically lowering the number of examples that can be solved using \"shortcuts\" compared to other popular entailment datasets. Players are presented with two tasks. The first task asks the player to write a plausible claim based on the evidence from a Wikipedia page. The second one shows two plausible claims written by other players, one of which is false, and the goal is to identify it before the time runs out. Players \"pay\" to see clues retrieved from the evidence pool: the more evidence the player needs, the harder the claim. Game-play between motivated players leads to diverse strategies for crafting claims, such as temporal inference and diverting to unrelated evidence, and results in higher quality data for the entailment and evidence retrieval tasks. We open source the dataset and the game code",
    "checked": true,
    "id": "370c97aa9521b3052b60369ee6c7b61f120f976d",
    "semantic_title": "fool me twice: entailment from wikipedia gamification",
    "citation_count": 37,
    "authors": [
      "Julian Eisenschlos",
      "Bhuwan Dhingra",
      "Jannis Bulian",
      "Benjamin Börschinger",
      "Jordan Boyd-Graber"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.33": {
    "title": "Meta-Learning for Domain Generalization in Semantic Parsing",
    "volume": "main",
    "abstract": "The importance of building semantic parsers which can be applied to new domains and generate programs unseen at training has long been acknowledged, and datasets testing out-of-domain performance are becoming increasingly available. However, little or no attention has been devoted to learning algorithms or objectives which promote domain generalization, with virtually all existing approaches relying on standard supervised learning. In this work, we use a meta-learning framework which targets zero-shot domain generalization for semantic parsing. We apply a model-agnostic training algorithm that simulates zero-shot parsing by constructing virtual train and test sets from disjoint domains. The learning objective capitalizes on the intuition that gradient steps that improve source-domain performance should also improve target-domain performance, thus encouraging a parser to generalize to unseen target domains. Experimental results on the (English) Spider and Chinese Spider datasets show that the meta-learning objective significantly boosts the performance of a baseline parser",
    "checked": true,
    "id": "19bd467b1c8de94b9bdaef1499788467937f594e",
    "semantic_title": "meta-learning for domain generalization in semantic parsing",
    "citation_count": 62,
    "authors": [
      "Bailin Wang",
      "Mirella Lapata",
      "Ivan Titov"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.34": {
    "title": "Aspect-Controlled Neural Argument Generation",
    "volume": "main",
    "abstract": "We rely on arguments in our daily lives to deliver our opinions and base them on evidence, making them more convincing in turn. However, finding and formulating arguments can be challenging. In this work, we present the Arg-CTRL - a language model for argument generation that can be controlled to generate sentence-level arguments for a given topic, stance, and aspect. We define argument aspect detection as a necessary method to allow this fine-granular control and crowdsource a dataset with 5,032 arguments annotated with aspects. Our evaluation shows that the Arg-CTRL is able to generate high-quality, aspect-specific arguments, applicable to automatic counter-argument generation. We publish the model weights and all datasets and code to train the Arg-CTRL",
    "checked": true,
    "id": "b49556626ceb024bcbf095aa85aa7ef147606d3a",
    "semantic_title": "aspect-controlled neural argument generation",
    "citation_count": 61,
    "authors": [
      "Benjamin Schiller",
      "Johannes Daxenberger",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.35": {
    "title": "Text Generation from Discourse Representation Structures",
    "volume": "main",
    "abstract": "We propose neural models to generate text from formal meaning representations based on Discourse Representation Structures (DRSs). DRSs are document-level representations which encode rich semantic detail pertaining to rhetorical relations, presupposition, and co-reference within and across sentences. We formalize the task of neural DRS-to-text generation and provide modeling solutions for the problems of condition ordering and variable naming which render generation from DRSs non-trivial. Our generator relies on a novel sibling treeLSTM model which is able to accurately represent DRS structures and is more generally suited to trees with wide branches. We achieve competitive performance (59.48 BLEU) on the GMB benchmark against several strong baselines",
    "checked": true,
    "id": "3354f968b14455719ffe39e1ff7b7bf802e4f0df",
    "semantic_title": "text generation from discourse representation structures",
    "citation_count": 7,
    "authors": [
      "Jiangming Liu",
      "Shay B. Cohen",
      "Mirella Lapata"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.36": {
    "title": "APo-VAE: Text Generation in Hyperbolic Space",
    "volume": "main",
    "abstract": "Natural language often exhibits inherent hierarchical structure ingrained with complex syntax and semantics. However, most state-of-the-art deep generative models learn embeddings only in Euclidean vector space, without accounting for this structural property of language. In this paper, we investigate text generation in a hyperbolic latent space to learn continuous hierarchical representations. An Adversarial Poincare Variational Autoencoder (APo-VAE) is presented, where both the prior and variational posterior of latent variables are defined over a Poincare ball via wrapped normal distributions. By adopting the primal-dual formulation of Kullback-Leibler divergence, an adversarial learning procedure is introduced to empower robust model training. Extensive experiments in language modeling, unaligned style transfer, and dialog-response generation demonstrate the effectiveness of the proposed APo-VAE model over VAEs in Euclidean latent space, thanks to its superb capabilities in capturing latent language hierarchies in hyperbolic space",
    "checked": true,
    "id": "1540d07562a5f99a7a9ceba581fa0a8207a5754f",
    "semantic_title": "apo-vae: text generation in hyperbolic space",
    "citation_count": 28,
    "authors": [
      "Shuyang Dai",
      "Zhe Gan",
      "Yu Cheng",
      "Chenyang Tao",
      "Lawrence Carin",
      "Jingjing Liu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.37": {
    "title": "DART: Open-Domain Structured Data Record to Text Generation",
    "volume": "main",
    "abstract": "We present DART, an open domain structured DAta Record to Text generation dataset with over 82k instances (DARTs). Data-to-text annotations can be a costly process, especially when dealing with tables which are the major source of structured data and contain nontrivial structures. To this end, we propose a procedure of extracting semantic triples from tables that encodes their structures by exploiting the semantic dependencies among table headers and the table title. Our dataset construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion, and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github.com/Yale-LILY/dart",
    "checked": true,
    "id": "6e3f8187f8fef3e11578a73f32da07d33dbf8235",
    "semantic_title": "dart: open-domain structured data record to text generation",
    "citation_count": 172,
    "authors": [
      "Linyong Nan",
      "Dragomir Radev",
      "Rui Zhang",
      "Amrit Rau",
      "Abhinand Sivaprasad",
      "Chiachun Hsieh",
      "Xiangru Tang",
      "Aadit Vyas",
      "Neha Verma",
      "Pranav Krishna",
      "Yangxiaokang Liu",
      "Nadia Irwanto",
      "Jessica Pan",
      "Faiaz Rahman",
      "Ahmad Zaidi",
      "Mutethia Mutuma",
      "Yasin Tarabar",
      "Ankit Gupta",
      "Tao Yu",
      "Yi Chern Tan",
      "Xi Victoria Lin",
      "Caiming Xiong",
      "Richard Socher",
      "Nazneen Fatema Rajani"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.38": {
    "title": "When Being Unseen from mBERT is just the Beginning: Handling New Languages With Multilingual Language Models",
    "volume": "main",
    "abstract": "Transfer learning based on pretraining language models on a large amount of raw data has become a new norm to reach state-of-the-art performance in NLP. Still, it remains unclear how this approach should be applied for unseen languages that are not covered by any available large-scale multilingual language model and for which only a small amount of raw data is generally available. In this work, by comparing multilingual and monolingual models, we show that such models behave in multiple ways on unseen languages. Some languages greatly benefit from transfer learning and behave similarly to closely related high resource languages whereas others apparently do not. Focusing on the latter, we show that this failure to transfer is largely related to the impact of the script used to write such languages. We show that transliterating those languages significantly improves the potential of large-scale multilingual language models on downstream tasks. This result provides a promising direction towards making these massively multilingual models useful for a new set of unseen languages",
    "checked": true,
    "id": "29599829d26b4acaaf0ad88698ed4362d33b2ae7",
    "semantic_title": "when being unseen from mbert is just the beginning: handling new languages with multilingual language models",
    "citation_count": 149,
    "authors": [
      "Benjamin Muller",
      "Antonios Anastasopoulos",
      "Benoît Sagot",
      "Djamé Seddah"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.39": {
    "title": "Multi-Adversarial Learning for Cross-Lingual Word Embeddings",
    "volume": "main",
    "abstract": "Generative adversarial networks (GANs) have succeeded in inducing cross-lingual word embeddings - maps of matching words across languages - without supervision. Despite these successes, GANs' performance for the difficult case of distant languages is still not satisfactory. These limitations have been explained by GANs' incorrect assumption that source and target embedding spaces are related by a single linear mapping and are approximately isomorphic. We assume instead that, especially across distant languages, the mapping is only piece-wise linear, and propose a multi-adversarial learning method. This novel method induces the seed cross-lingual dictionary through multiple mappings, each induced to fit the mapping for one subspace. Our experiments on unsupervised bilingual lexicon induction and cross-lingual document classification show that this method improves performance over previous single-mapping methods, especially for distant languages",
    "checked": true,
    "id": "f886218ddd2b8de54950e5fd46c7d39f85eea0e9",
    "semantic_title": "multi-adversarial learning for cross-lingual word embeddings",
    "citation_count": 6,
    "authors": [
      "Haozhou Wang",
      "James Henderson",
      "Paola Merlo"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.40": {
    "title": "Multi-view Subword Regularization",
    "volume": "main",
    "abstract": "Multilingual pretrained representations generally rely on subword segmentation algorithms to create a shared multilingual vocabulary. However, standard heuristic algorithms often lead to sub-optimal segmentation, especially for languages with limited amounts of data. In this paper, we take two major steps towards alleviating this problem. First, we demonstrate empirically that applying existing subword regularization methods (Kudo, 2018; Provilkov et al., 2020) during fine-tuning of pre-trained multilingual representations improves the effectiveness of cross-lingual transfer. Second, to take full advantage of different possible input segmentations, we propose Multi-view Subword Regularization (MVR), a method that enforces the consistency of predictors between using inputs tokenized by the standard and probabilistic segmentations. Results on the XTREME multilingual benchmark (Hu et al., 2020) show that MVR brings consistent improvements of up to 2.5 points over using standard segmentation algorithms",
    "checked": true,
    "id": "a77643bff6f50ccc4f80ec081e4d078a2e788ae7",
    "semantic_title": "multi-view subword regularization",
    "citation_count": 42,
    "authors": [
      "Xinyi Wang",
      "Sebastian Ruder",
      "Graham Neubig"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.41": {
    "title": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer",
    "volume": "main",
    "abstract": "The recent \"Text-to-Text Transfer Transformer\" (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent \"accidental translation\" in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available",
    "checked": true,
    "id": "74276a37bfa50f90dfae37f767b2b67784bd402a",
    "semantic_title": "mt5: a massively multilingual pre-trained text-to-text transformer",
    "citation_count": 1969,
    "authors": [
      "Linting Xue",
      "Noah Constant",
      "Adam Roberts",
      "Mihir Kale",
      "Rami Al-Rfou",
      "Aditya Siddhant",
      "Aditya Barua",
      "Colin Raffel"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.42": {
    "title": "MetaXL: Meta Representation Transformation for Low-resource Cross-lingual Learning",
    "volume": "main",
    "abstract": "The combination of multilingual pre-trained representations and cross-lingual transfer learning is one of the most effective methods for building functional NLP systems for low-resource languages. However, for extremely low-resource languages without large-scale monolingual corpora for pre-training or sufficient annotated data for fine-tuning, transfer learning remains an understudied and challenging task. Moreover, recent work shows that multilingual representations are surprisingly disjoint across languages, bringing additional challenges for transfer onto extremely low-resource languages. In this paper, we propose MetaXL, a meta-learning based framework that learns to transform representations judiciously from auxiliary languages to a target one and brings their representation spaces closer for effective transfer. Extensive experiments on real-world low-resource languages – without access to large-scale monolingual corpora or large amounts of labeled data – for tasks like cross-lingual sentiment analysis and named entity recognition show the effectiveness of our approach. Code for MetaXL is publicly available at github.com/microsoft/MetaXL",
    "checked": true,
    "id": "1838cbd2eee0a555ab7e850eff1fce69d62acb95",
    "semantic_title": "metaxl: meta representation transformation for low-resource cross-lingual learning",
    "citation_count": 28,
    "authors": [
      "Mengzhou Xia",
      "Guoqing Zheng",
      "Subhabrata Mukherjee",
      "Milad Shokouhi",
      "Graham Neubig",
      "Ahmed Hassan Awadallah"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.43": {
    "title": "Open Domain Question Answering over Tables via Dense Retrieval",
    "volume": "main",
    "abstract": "Recent advances in open-domain QA have led to strong models based on dense retrieval, but only focused on retrieving textual passages. In this work, we tackle open-domain QA over tables for the first time, and show that retrieval can be improved by a retriever designed to handle tabular context. We present an effective pre-training procedure for our retriever and improve retrieval quality with mined hard negatives. As relevant datasets are missing, we extract a subset of Natural Questions (Kwiatkowski et al., 2019) into a Table QA dataset. We find that our retriever improves retrieval results from 72.0 to 81.1 recall@10 and end-to-end QA results from 33.8 to 37.7 exact match, over a BERT based retriever",
    "checked": true,
    "id": "5b4dc3b099b9f500392b4fb0de86876feedc522b",
    "semantic_title": "open domain question answering over tables via dense retrieval",
    "citation_count": 79,
    "authors": [
      "Jonathan Herzig",
      "Thomas Müller",
      "Syrine Krichene",
      "Julian Eisenschlos"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.44": {
    "title": "Open-Domain Question Answering Goes Conversational via Question Rewriting",
    "volume": "main",
    "abstract": "We introduce a new dataset for Question Rewriting in Conversational Context (QReCC), which contains 14K conversations with 80K question-answer pairs. The task in QReCC is to find answers to conversational questions within a collection of 10M web pages (split into 54M passages). Answers to questions in the same conversation may be distributed across several web pages. QReCC provides annotations that allow us to train and evaluate individual subtasks of question rewriting, passage retrieval and reading comprehension required for the end-to-end conversational question answering (QA) task. We report the effectiveness of a strong baseline approach that combines the state-of-the-art model for question rewriting, and competitive models for open-domain QA. Our results set the first baseline for the QReCC dataset with F1 of 19.10, compared to the human upper bound of 75.45, indicating the difficulty of the setup and a large room for improvement",
    "checked": true,
    "id": "32cb26814b320beb0c4a8fdea86a0065ad873ef3",
    "semantic_title": "open-domain question answering goes conversational via question rewriting",
    "citation_count": 115,
    "authors": [
      "Raviteja Anantha",
      "Svitlana Vakulenko",
      "Zhucheng Tu",
      "Shayne Longpre",
      "Stephen Pulman",
      "Srinivas Chappidi"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.45": {
    "title": "QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering",
    "volume": "main",
    "abstract": "The problem of answering questions using knowledge from pre-trained language models (LMs) and knowledge graphs (KGs) presents two challenges: given a QA context (question and answer choice), methods need to (i) identify relevant knowledge from large KGs, and (ii) perform joint reasoning over the QA context and KG. Here we propose a new model, QA-GNN, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use LMs to estimate the importance of KG nodes relative to the given QA context, and (ii) joint reasoning, where we connect the QA context and KG to form a joint graph, and mutually update their representations through graph-based message passing. We evaluate QA-GNN on the CommonsenseQA and OpenBookQA datasets, and show its improvement over existing LM and LM+KG models, as well as its capability to perform interpretable and structured reasoning, e.g., correctly handling negation in questions",
    "checked": true,
    "id": "3950df97ea527009a32569cb7016bc3df1383dca",
    "semantic_title": "qa-gnn: reasoning with language models and knowledge graphs for question answering",
    "citation_count": 456,
    "authors": [
      "Michihiro Yasunaga",
      "Hongyu Ren",
      "Antoine Bosselut",
      "Percy Liang",
      "Jure Leskovec"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.46": {
    "title": "XOR QA: Cross-lingual Open-Retrieval Question Answering",
    "volume": "main",
    "abstract": "Multilingual question answering tasks typically assume that answers exist in the same language as the question. Yet in practice, many languages face both information scarcity—where languages have few reference articles—and information asymmetry—where questions reference concepts from other cultures. This work extends open-retrieval question answering to a cross-lingual setting enabling questions from one language to be answered via answer content from another language. We construct a large-scale dataset built on 40K information-seeking questions across 7 diverse non-English languages that TyDi QA could not find same-language answers for. Based on this dataset, we introduce a task framework, called Cross-lingual Open-Retrieval Question Answering (XOR QA), that consists of three new tasks involving cross-lingual document retrieval from multilingual and English resources. We establish baselines with state-of-the-art machine translation systems and cross-lingual pretrained models. Experimental results suggest that XOR QA is a challenging task that will facilitate the development of novel techniques for multilingual question answering. Our data and code are available at https://nlp.cs.washington.edu/xorqa/",
    "checked": true,
    "id": "3a84c4dcdcc59d418e41fd8279626e25f87e1e6f",
    "semantic_title": "xor qa: cross-lingual open-retrieval question answering",
    "citation_count": 125,
    "authors": [
      "Akari Asai",
      "Jungo Kasai",
      "Jonathan Clark",
      "Kenton Lee",
      "Eunsol Choi",
      "Hannaneh Hajishirzi"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.47": {
    "title": "SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval",
    "volume": "main",
    "abstract": "We introduce SPARTA, a novel neural retrieval method that shows great promise in performance, generalization, and interpretability for open-domain question answering. Unlike many neural ranking methods that use dense vector nearest neighbor search, SPARTA learns a sparse representation that can be efficiently implemented as an Inverted Index. The resulting representation enables scalable neural retrieval that does not require expensive approximate vector search and leads to better performance than its dense counterpart. We validated our approaches on 4 open-domain question answering (OpenQA) tasks and 11 retrieval question answering (ReQA) tasks. SPARTA achieves new state-of-the-art results across a variety of open-domain question answering tasks in both English and Chinese datasets, including open SQuAD, CMRC and etc. Analysis also confirms that the proposed method creates human interpretable representation and allows flexible control over the trade-off between performance and efficiency",
    "checked": true,
    "id": "aea909a04e4d848855afcf8ce8ae55a81a2623e4",
    "semantic_title": "sparta: efficient open-domain question answering via sparse transformer matching retrieval",
    "citation_count": 64,
    "authors": [
      "Tiancheng Zhao",
      "Xiaopeng Lu",
      "Kyusong Lee"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.48": {
    "title": "Implicitly Abusive Language – What does it actually look like and why are we not getting there?",
    "volume": "main",
    "abstract": "Abusive language detection is an emerging field in natural language processing which has received a large amount of attention recently. Still the success of automatic detection is limited. Particularly, the detection of implicitly abusive language, i.e. abusive language that is not conveyed by abusive words (e.g. dumbass or scum), is not working well. In this position paper, we explain why existing datasets make learning implicit abuse difficult and what needs to be changed in the design of such datasets. Arguing for a divide-and-conquer strategy, we present a list of subtypes of implicitly abusive language and formulate research tasks and questions for future research",
    "checked": true,
    "id": "03f9f0d78b960ef37075fd064b788e36b7b7c3b2",
    "semantic_title": "implicitly abusive language – what does it actually look like and why are we not getting there?",
    "citation_count": 48,
    "authors": [
      "Michael Wiegand",
      "Josef Ruppenhofer",
      "Elisabeth Eder"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.49": {
    "title": "The Importance of Modeling Social Factors of Language: Theory and Practice",
    "volume": "main",
    "abstract": "Natural language processing (NLP) applications are now more powerful and ubiquitous than ever before. With rapidly developing (neural) models and ever-more available data, current NLP models have access to more information than any human speaker during their life. Still, it would be hard to argue that NLP models have reached human-level capacity. In this position paper, we argue that the reason for the current limitations is a focus on information content while ignoring language's social factors. We show that current NLP systems systematically break down when faced with interpreting the social factors of language. This limits applications to a subset of information-related tasks and prevents NLP from reaching human-level performance. At the same time, systems that incorporate even a minimum of social factors already show remarkable improvements. We formalize a taxonomy of seven social factors based on linguistic theory and exemplify current failures and emerging successes for each of them. We suggest that the NLP community address social factors to get closer to the goal of human-like language understanding",
    "checked": true,
    "id": "e2a81988541b304cb74906db14f5310d168342cf",
    "semantic_title": "the importance of modeling social factors of language: theory and practice",
    "citation_count": 141,
    "authors": [
      "Dirk Hovy",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.50": {
    "title": "On learning and representing social meaning in NLP: a sociolinguistic perspective",
    "volume": "main",
    "abstract": "The field of NLP has made substantial progress in building meaning representations. However, an important aspect of linguistic meaning, social meaning, has been largely overlooked. We introduce the concept of social meaning to NLP and discuss how insights from sociolinguistics can inform work on representation learning in NLP. We also identify key challenges for this new line of research",
    "checked": true,
    "id": "7a1cde46dc26e4401e2226c5ddf8a6d2f643d98e",
    "semantic_title": "on learning and representing social meaning in nlp: a sociolinguistic perspective",
    "citation_count": 23,
    "authors": [
      "Dong Nguyen",
      "Laura Rosseel",
      "Jack Grieve"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.51": {
    "title": "Preregistering NLP research",
    "volume": "main",
    "abstract": "Preregistration refers to the practice of specifying what you are going to do, and what you expect to find in your study, before carrying out the study. This practice is increasingly common in medicine and psychology, but is rarely discussed in NLP. This paper discusses preregistration in more detail, explores how NLP researchers could preregister their work, and presents several preregistration questions for different kinds of studies. Finally, we argue in favour of registered reports, which could provide firmer grounds for slow science in NLP research. The goal of this paper is to elicit a discussion in the NLP community, which we hope to synthesise into a general NLP preregistration form in future research",
    "checked": true,
    "id": "093bfc1ce749dcc9e6edd3aa5902a1d7746601d4",
    "semantic_title": "preregistering nlp research",
    "citation_count": 20,
    "authors": [
      "Emiel van Miltenburg",
      "Chris van der Lee",
      "Emiel Krahmer"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.52": {
    "title": "Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence",
    "volume": "main",
    "abstract": "Typical fact verification models use retrieved written evidence to verify claims. Evidence sources, however, often change over time as more information is gathered and revised. In order to adapt, models must be sensitive to subtle differences in supporting evidence. We present VitaminC, a benchmark infused with challenging cases that require fact verification models to discern and adjust to slight factual changes. We collect over 100,000 Wikipedia revisions that modify an underlying fact, and leverage these revisions, together with additional synthetically constructed ones, to create a total of over 400,000 claim-evidence pairs. Unlike previous resources, the examples in VitaminC are contrastive, i.e., they contain evidence pairs that are nearly identical in language and content, with the exception that one supports a given claim while the other does not. We show that training using this design increases robustness—improving accuracy by 10% on adversarial fact verification and 6% on adversarial natural language inference (NLI). Moreover, the structure of VitaminC leads us to define additional tasks for fact-checking resources: tagging relevant words in the evidence for verifying the claim, identifying factual revisions, and providing automatic edits via factually consistent text generation",
    "checked": true,
    "id": "eebc1811c55c2e5e8b3b78d0b0382ad50f22e32a",
    "semantic_title": "get your vitamin c! robust fact verification with contrastive evidence",
    "citation_count": 184,
    "authors": [
      "Tal Schuster",
      "Adam Fisch",
      "Regina Barzilay"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.53": {
    "title": "Representing Numbers in NLP: a Survey and a Vision",
    "volume": "main",
    "abstract": "NLP systems rarely give special consideration to numbers found in text. This starkly contrasts with the consensus in neuroscience that, in the brain, numbers are represented differently from words. We arrange recent NLP work on numeracy into a comprehensive taxonomy of tasks and methods. We break down the subjective notion of numeracy into 7 subtasks, arranged along two dimensions: granularity (exact vs approximate) and units (abstract vs grounded). We analyze the myriad representational choices made by over a dozen previously published number encoders and decoders. We synthesize best practices for representing numbers in text and articulate a vision for holistic numeracy in NLP, comprised of design trade-offs and a unified evaluation",
    "checked": true,
    "id": "28a5a53dafacebad8a7c47773079caeffb9a5baa",
    "semantic_title": "representing numbers in nlp: a survey and a vision",
    "citation_count": 83,
    "authors": [
      "Avijit Thawani",
      "Jay Pujara",
      "Filip Ilievski",
      "Pedro Szekely"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.54": {
    "title": "Extending Multi-Document Summarization Evaluation to the Interactive Setting",
    "volume": "main",
    "abstract": "Allowing users to interact with multi-document summarizers is a promising direction towards improving and customizing summary results. Different ideas for interactive summarization have been proposed in previous work but these solutions are highly divergent and incomparable. In this paper, we develop an end-to-end evaluation framework for interactive summarization, focusing on expansion-based interaction, which considers the accumulating information along a user session. Our framework includes a procedure of collecting real user sessions, as well as evaluation measures relying on summarization standards, but adapted to reflect interaction. All of our solutions and resources are available publicly as a benchmark, allowing comparison of future developments in interactive summarization, and spurring progress in its methodological evaluation. We demonstrate the use of our framework by evaluating and comparing baseline implementations that we developed for this purpose, which will serve as part of our benchmark. Our extensive experimentation and analysis motivate the proposed evaluation framework design and support its viability",
    "checked": true,
    "id": "a7dab9725ca411b49ebcc09544119cd463a9b94c",
    "semantic_title": "extending multi-document summarization evaluation to the interactive setting",
    "citation_count": 16,
    "authors": [
      "Ori Shapira",
      "Ramakanth Pasunuru",
      "Hadar Ronen",
      "Mohit Bansal",
      "Yael Amsterdamer",
      "Ido Dagan"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.55": {
    "title": "Identifying Helpful Sentences in Product Reviews",
    "volume": "main",
    "abstract": "In recent years online shopping has gained momentum and became an important venue for customers wishing to save time and simplify their shopping process. A key advantage of shopping online is the ability to read what other customers are saying about products of interest. In this work, we aim to maintain this advantage in situations where extreme brevity is needed, for example, when shopping by voice. We suggest a novel task of extracting a single representative helpful sentence from a set of reviews for a given product. The selected sentence should meet two conditions: first, it should be helpful for a purchase decision and second, the opinion it expresses should be supported by multiple reviewers. This task is closely related to the task of Multi Document Summarization in the product reviews domain but differs in its objective and its level of conciseness. We collect a dataset in English of sentence helpfulness scores via crowd-sourcing and demonstrate its reliability despite the inherent subjectivity involved. Next, we describe a complete model that extracts representative helpful sentences with positive and negative sentiment towards the product and demonstrate that it outperforms several baselines",
    "checked": true,
    "id": "b673b8fed476bb7fc3729c2dd541914c897caeb0",
    "semantic_title": "identifying helpful sentences in product reviews",
    "citation_count": 22,
    "authors": [
      "Iftah Gamzu",
      "Hila Gonen",
      "Gilad Kutiel",
      "Ran Levy",
      "Eugene Agichtein"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.56": {
    "title": "Noisy Self-Knowledge Distillation for Text Summarization",
    "volume": "main",
    "abstract": "In this paper we apply self-knowledge distillation to text summarization which we argue can alleviate problems with maximum-likelihood training on single reference and noisy datasets. Instead of relying on one-hot annotation labels, our student summarization model is trained with guidance from a teacher which generates smoothed labels to help regularize training. Furthermore, to better model uncertainty during training, we introduce multiple noise signals for both teacher and student models. We demonstrate experimentally on three benchmarks that our framework boosts the performance of both pretrained and non-pretrained summarizers achieving state-of-the-art results",
    "checked": true,
    "id": "92d879ed4be5438c2498d5269e356959da2030bd",
    "semantic_title": "noisy self-knowledge distillation for text summarization",
    "citation_count": 38,
    "authors": [
      "Yang Liu",
      "Sheng Shen",
      "Mirella Lapata"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.57": {
    "title": "Improving Zero and Few-Shot Abstractive Summarization with Intermediate Fine-tuning and Data Augmentation",
    "volume": "main",
    "abstract": "Models pretrained with self-supervised objectives on large text corpora achieve state-of-the-art performance on English text summarization tasks. However, these models are typically fine-tuned on hundreds of thousands of data points, an infeasible requirement when applying summarization to new, niche domains. In this work, we introduce a novel and generalizable method, called WikiTransfer, for fine-tuning pretrained models for summarization in an unsupervised, dataset-specific manner. WikiTransfer fine-tunes pretrained models on pseudo-summaries, produced from generic Wikipedia data, which contain characteristics of the target dataset, such as the length and level of abstraction of the desired summaries. WikiTransfer models achieve state-of-the-art, zero-shot abstractive summarization performance on the CNN-DailyMail dataset and demonstrate the effectiveness of our approach on three additional diverse datasets. These models are more robust to noisy data and also achieve better or comparable few-shot performance using 10 and 100 training examples when compared to few-shot transfer from other summarization datasets. To further boost performance, we employ data augmentation via round-trip translation as well as introduce a regularization term for improved few-shot transfer. To understand the role of dataset aspects in transfer performance and the quality of the resulting output summaries, we further study the effect of the components of our unsupervised fine-tuning data and analyze few-shot performance using both automatic and human evaluation",
    "checked": true,
    "id": "7ba0dc20800195c6350995695c8bf86be6227c49",
    "semantic_title": "improving zero and few-shot abstractive summarization with intermediate fine-tuning and data augmentation",
    "citation_count": 82,
    "authors": [
      "Alexander Fabbri",
      "Simeng Han",
      "Haoyuan Li",
      "Haoran Li",
      "Marjan Ghazvininejad",
      "Shafiq Joty",
      "Dragomir Radev",
      "Yashar Mehdad"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.58": {
    "title": "Enhancing Factual Consistency of Abstractive Summarization",
    "volume": "main",
    "abstract": "Automatic abstractive summaries are found to often distort or fabricate facts in the article. This inconsistency between summary and original text has seriously impacted its applicability. We propose a fact-aware summarization model FASum to extract and integrate factual relations into the summary generation process via graph attention. We then design a factual corrector model FC to automatically correct factual errors from summaries generated by existing systems. Empirical results show that the fact-aware summarization can produce abstractive summaries with higher factual consistency compared with existing systems, and the correction model improves the factual consistency of given summaries via modifying only a few keywords",
    "checked": true,
    "id": "443e7918db6428fbd7256d26f39932bba93c6c26",
    "semantic_title": "enhancing factual consistency of abstractive summarization",
    "citation_count": 136,
    "authors": [
      "Chenguang Zhu",
      "William Hinthorn",
      "Ruochen Xu",
      "Qingkai Zeng",
      "Michael Zeng",
      "Xuedong Huang",
      "Meng Jiang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.59": {
    "title": "Few-shot Intent Classification and Slot Filling with Retrieved Examples",
    "volume": "main",
    "abstract": "Few-shot learning arises in important practical scenarios, such as when a natural language understanding system needs to learn new semantic labels for an emerging, resource-scarce domain. In this paper, we explore retrieval-based methods for intent classification and slot filling tasks in few-shot settings. Retrieval-based methods make predictions based on labeled examples in the retrieval index that are similar to the input, and thus can adapt to new domains simply by changing the index without having to retrain the model. However, it is non-trivial to apply such methods on tasks with a complex label space like slot filling. To this end, we propose a span-level retrieval method that learns similar contextualized representations for spans with the same label via a novel batch-softmax objective. At inference time, we use the labels of the retrieved spans to construct the final structure with the highest aggregated score. Our method outperforms previous systems in various few-shot settings on the CLINC and SNIPS benchmarks",
    "checked": true,
    "id": "774cdc8e85f7a207ff185ce0d80785da7a17d2df",
    "semantic_title": "few-shot intent classification and slot filling with retrieved examples",
    "citation_count": 46,
    "authors": [
      "Dian Yu",
      "Luheng He",
      "Yuan Zhang",
      "Xinya Du",
      "Panupong Pasupat",
      "Qi Li"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.60": {
    "title": "Nice Try, Kiddo\": Investigating Ad Hominems in Dialogue Responses",
    "volume": "main",
    "abstract": "Ad hominem attacks are those that target some feature of a person's character instead of the position the person is maintaining. These attacks are harmful because they propagate implicit biases and diminish a person's credibility. Since dialogue systems respond directly to user input, it is important to study ad hominems in dialogue responses. To this end, we propose categories of ad hominems, compose an annotated dataset, and build a classifier to analyze human and dialogue system responses to English Twitter posts. We specifically compare responses to Twitter topics about marginalized communities (#BlackLivesMatter, #MeToo) versus other topics (#Vegan, #WFH), because the abusive language of ad hominems could further amplify the skew of power away from marginalized populations. Furthermore, we propose a constrained decoding technique that uses salient n-gram similarity as a soft constraint for top-k sampling to reduce the amount of ad hominems generated. Our results indicate that 1) responses from both humans and DialoGPT contain more ad hominems for discussions around marginalized communities, 2) different quantities of ad hominems in the training data can influence the likelihood of generating ad hominems, and 3) we can use constrained decoding techniques to reduce ad hominems in generated dialogue responses",
    "checked": true,
    "id": "80e015b4edbe72cb10af0bd2cb065bba163d6e0d",
    "semantic_title": "nice try, kiddo\": investigating ad hominems in dialogue responses",
    "citation_count": 47,
    "authors": [
      "Emily Sheng",
      "Kai-Wei Chang",
      "Prem Natarajan",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.61": {
    "title": "Human-like informative conversations: Better acknowledgements using conditional mutual information",
    "volume": "main",
    "abstract": "This work aims to build a dialogue agent that can weave new factual content into conversations as naturally as humans. We draw insights from linguistic principles of conversational analysis and annotate human-human conversations from the Switchboard Dialog Act Corpus to examine humans strategies for acknowledgement, transition, detail selection and presentation. When current chatbots (explicitly provided with new factual content) introduce facts into a conversation, their generated responses do not acknowledge the prior turns. This is because models trained with two contexts - new factual content and conversational history - generate responses that are non-specific w.r.t. one of the contexts, typically the conversational history. We show that specificity w.r.t. conversational history is better captured by pointwise conditional mutual information (pcmi_h) than by the established use of pointwise mutual information (pmi). Our proposed method, Fused-PCMI, trades off pmi for pcmi_h and is preferred by humans for overall quality over the Max-PMI baseline 60% of the time. Human evaluators also judge responses with higher pcmi_h better at acknowledgement 74% of the time. The results demonstrate that systems mimicking human conversational traits (in this case acknowledgement) improve overall quality and more broadly illustrate the utility of linguistic principles in improving dialogue agents",
    "checked": true,
    "id": "6522c2f70ba170a8edfc9a587bf21c80f0cc3d68",
    "semantic_title": "human-like informative conversations: better acknowledgements using conditional mutual information",
    "citation_count": 8,
    "authors": [
      "Ashwin Paranjape",
      "Christopher Manning"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.62": {
    "title": "A Comparative Study on Schema-Guided Dialogue State Tracking",
    "volume": "main",
    "abstract": "Frame-based state representation is widely used in modern task-oriented dialog systems to model user intentions and slot values. However, a fixed design of domain ontology makes it difficult to extend to new services and APIs. Recent work proposed to use natural language descriptions to define the domain ontology instead of tag names for each intent or slot, thus offering a dynamic set of schema. In this paper, we conduct in-depth comparative studies to understand the use of natural language description for schema in dialog state tracking. Our discussion mainly covers three aspects: encoder architectures, impact of supplementary training, and effective schema description styles. We introduce a set of newly designed bench-marking descriptions and reveal the model robustness on both homogeneous and heterogeneous description styles in training and evaluation",
    "checked": true,
    "id": "cadbf2a92e1872f1c1639d4393cc4f6e40b79cc8",
    "semantic_title": "a comparative study on schema-guided dialogue state tracking",
    "citation_count": 10,
    "authors": [
      "Jie Cao",
      "Yi Zhang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.63": {
    "title": "Spoken Language Understanding for Task-oriented Dialogue Systems with Augmented Memory Networks",
    "volume": "main",
    "abstract": "Spoken language understanding, usually including intent detection and slot filling, is a core component to build a spoken dialog system. Recent research shows promising results by jointly learning of those two tasks based on the fact that slot filling and intent detection are sharing semantic knowledge. Furthermore, attention mechanism boosts joint learning to achieve state-of-the-art results. However, current joint learning models ignore the following important facts: 1. Long-term slot context is not traced effectively, which is crucial for future slot filling. 2. Slot tagging and intent detection could be mutually rewarding, but bi-directional interaction between slot filling and intent detection remains seldom explored. In this paper, we propose a novel approach to model long-term slot context and to fully utilize the semantic correlation between slots and intents. We adopt a key-value memory network to model slot context dynamically and to track more important slot tags decoded before, which are then fed into our decoder for slot tagging. Furthermore, gated memory information is utilized to perform intent detection, mutually improving both tasks through global optimization. Experiments on benchmark ATIS and Snips datasets show that our model achieves state-of-the-art performance and outperforms other methods, especially for the slot filling task",
    "checked": true,
    "id": "1b21e003612533caa50eb885ce24669c039ef3b2",
    "semantic_title": "spoken language understanding for task-oriented dialogue systems with augmented memory networks",
    "citation_count": 13,
    "authors": [
      "Jie Wu",
      "Ian Harris",
      "Hongzhi Zhao"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.64": {
    "title": "How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds",
    "volume": "main",
    "abstract": "We seek to create agents that both act and communicate with other agents in pursuit of a goal. Towards this end, we extend LIGHT (Urbanek et al. 2019)—a large-scale crowd-sourced fantasy text-game—with a dataset of quests. These contain natural language motivations paired with in-game goals and human demonstrations; completing a quest might require dialogue or actions (or both). We introduce a reinforcement learning system that (1) incorporates large-scale language modeling-based and commonsense reasoning-based pre-training to imbue the agent with relevant priors; and (2) leverages a factorized action space of action commands and dialogue, balancing between the two. We conduct zero-shot evaluations using held-out human expert demonstrations, showing that our agents are able to act consistently and talk naturally with respect to their motivations",
    "checked": true,
    "id": "34b274329317b8e7aa8f8af60f3aa22ed7d87cbb",
    "semantic_title": "how to motivate your dragon: teaching goal-driven agents to speak and act in fantasy worlds",
    "citation_count": 39,
    "authors": [
      "Prithviraj Ammanabrolu",
      "Jack Urbanek",
      "Margaret Li",
      "Arthur Szlam",
      "Tim Rocktäschel",
      "Jason Weston"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.65": {
    "title": "Linking Entities to Unseen Knowledge Bases with Arbitrary Schemas",
    "volume": "main",
    "abstract": "In entity linking, mentions of named entities in raw text are disambiguated against a knowledge base (KB). This work focuses on linking to unseen KBs that do not have training data and whose schema is unknown during training. Our approach relies on methods to flexibly convert entities with several attribute-value pairs from arbitrary KBs into flat strings, which we use in conjunction with state-of-the-art models for zero-shot linking. We further improve the generalization of our model using two regularization schemes based on shuffling of entity attributes and handling of unseen attributes. Experiments on English datasets where models are trained on the CoNLL dataset, and tested on the TAC-KBP 2010 dataset show that our models are 12% (absolute) more accurate than baseline models that simply flatten entities from the target KB. Unlike prior work, our approach also allows for seamlessly combining multiple training datasets. We test this ability by adding both a completely different dataset (Wikia), as well as increasing amount of training data from the TAC-KBP 2010 training set. Our models are more accurate across the board compared to baselines",
    "checked": true,
    "id": "5db079ebaaab8f2b19068a7c64d7bc735593e9cb",
    "semantic_title": "linking entities to unseen knowledge bases with arbitrary schemas",
    "citation_count": 10,
    "authors": [
      "Yogarshi Vyas",
      "Miguel Ballesteros"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.66": {
    "title": "Self-Training with Weak Supervision",
    "volume": "main",
    "abstract": "State-of-the-art deep neural networks require large-scale labeled training data that is often expensive to obtain or not available for many tasks. Weak supervision in the form of domain-specific rules has been shown to be useful in such settings to automatically generate weakly labeled training data. However, learning with weak rules is challenging due to their inherent heuristic and noisy nature. An additional challenge is rule coverage and overlap, where prior work on weak supervision only considers instances that are covered by weak rules, thus leaving valuable unlabeled data behind. In this work, we develop a weak supervision framework (ASTRA) that leverages all the available data for a given task. To this end, we leverage task-specific unlabeled data through self-training with a model (student) that considers contextualized representations and predicts pseudo-labels for instances that may not be covered by weak rules. We further develop a rule attention network (teacher) that learns how to aggregate student pseudo-labels with weak rule labels, conditioned on their fidelity and the underlying context of an instance. Finally, we construct a semi-supervised learning objective for end-to-end training with unlabeled data, domain-specific rules, and a small amount of labeled data. Extensive experiments on six benchmark datasets for text classification demonstrate the effectiveness of our approach with significant improvements over state-of-the-art baselines",
    "checked": true,
    "id": "0c191912646d1adf29e40a6ffb267e3f5088d056",
    "semantic_title": "self-training with weak supervision",
    "citation_count": 74,
    "authors": [
      "Giannis Karamanolakis",
      "Subhabrata Mukherjee",
      "Guoqing Zheng",
      "Ahmed Hassan Awadallah"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.67": {
    "title": "Neural Language Modeling for Contextualized Temporal Graph Generation",
    "volume": "main",
    "abstract": "This paper presents the first study on using large-scale pre-trained language models for automated generation of an event-level temporal graph for a document. Despite the huge success of neural pre-training methods in NLP tasks, its potential for temporal reasoning over event graphs has not been sufficiently explored. Part of the reason is the difficulty in obtaining large training corpora with human-annotated events and temporal links. We address this challenge by using existing IE/NLP tools to automatically generate a large quantity (89,000) of system-produced document-graph pairs, and propose a novel formulation of the contextualized graph generation problem as a sequence-to-sequence mapping task. These strategies enable us to leverage and fine-tune pre-trained language models on the system-induced training data for the graph generation task. Our experiments show that our approach is highly effective in generating structurally and semantically valid graphs. Further, evaluation on a challenging hand-labeled, out-of-domain corpus shows that our method outperforms the closest existing method by a large margin on several metrics. We also show a downstream application of our approach by adapting it to answer open-ended temporal questions in a reading comprehension setting",
    "checked": true,
    "id": "778560c0da279b82b0da91ba43a2a023b9c6711f",
    "semantic_title": "neural language modeling for contextualized temporal graph generation",
    "citation_count": 18,
    "authors": [
      "Aman Madaan",
      "Yiming Yang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.68": {
    "title": "Probabilistic Box Embeddings for Uncertain Knowledge Graph Reasoning",
    "volume": "main",
    "abstract": "Knowledge bases often consist of facts which are harvested from a variety of sources, many of which are noisy and some of which conflict, resulting in a level of uncertainty for each triple. Knowledge bases are also often incomplete, prompting the use of embedding methods to generalize from known facts, however, existing embedding methods only model triple-level uncertainty, and reasoning results lack global consistency. To address these shortcomings, we propose BEUrRE, a novel uncertain knowledge graph embedding method with calibrated probabilistic semantics. BEUrRE models each entity as a box (i.e. axis-aligned hyperrectangle) and relations between two entities as affine transforms on the head and tail entity boxes. The geometry of the boxes allows for efficient calculation of intersections and volumes, endowing the model with calibrated probabilistic semantics and facilitating the incorporation of relational constraints. Extensive experiments on two benchmark datasets show that BEUrRE consistently outperforms baselines on confidence prediction and fact ranking due to its probabilistic calibration and ability to capture high-order dependencies among facts",
    "checked": true,
    "id": "483f913b897675a8fb183407240a144c752413c5",
    "semantic_title": "probabilistic box embeddings for uncertain knowledge graph reasoning",
    "citation_count": 33,
    "authors": [
      "Xuelu Chen",
      "Michael Boratko",
      "Muhao Chen",
      "Shib Sankar Dasgupta",
      "Xiang Lorraine Li",
      "Andrew McCallum"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.69": {
    "title": "Document-Level Event Argument Extraction by Conditional Generation",
    "volume": "main",
    "abstract": "Event extraction has long been treated as a sentence-level task in the IE community. We argue that this setting does not match human informative seeking behavior and leads to incomplete and uninformative extraction results. We propose a document-level neural event argument extraction model by formulating the task as conditional generation following event templates. We also compile a new document-level event extraction benchmark dataset WikiEvents which includes complete event and coreference annotation. On the task of argument extraction, we achieve an absolute gain of 7.6% F1 and 5.7% F1 over the next best model on the RAMS and WikiEvents dataset respectively. On the more challenging task of informative argument extraction, which requires implicit coreference reasoning, we achieve a 9.3% F1 gain over the best baseline. To demonstrate the portability of our model, we also create the first end-to-end zero-shot event extraction framework and achieve 97% of fully supervised model's trigger extraction performance and 82% of the argument extraction performance given only access to 10 out of the 33 types on ACE",
    "checked": true,
    "id": "2a15f541a8216782a7579f01f56762448e48b582",
    "semantic_title": "document-level event argument extraction by conditional generation",
    "citation_count": 230,
    "authors": [
      "Sha Li",
      "Heng Ji",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.70": {
    "title": "Template Filling with Generative Transformers",
    "volume": "main",
    "abstract": "Template filling is generally tackled by a pipeline of two separate supervised systems – one for role-filler extraction and another for template/event recognition. Since pipelines consider events in isolation, they can suffer from error propagation. We introduce a framework based on end-to-end generative transformers for this task (i.e., GTT). It naturally models the dependence between entities both within a single event and across the multiple events described in a document. Experiments demonstrate that this framework substantially outperforms pipeline-based approaches, and other neural end-to-end baselines that do not model between-event dependencies. We further show that our framework specifically improves performance on documents containing multiple events",
    "checked": true,
    "id": "e9b3fed266dff06d44be861cce8bd9c247b7d42a",
    "semantic_title": "template filling with generative transformers",
    "citation_count": 37,
    "authors": [
      "Xinya Du",
      "Alexander Rush",
      "Claire Cardie"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.71": {
    "title": "Towards Interpreting and Mitigating Shortcut Learning Behavior of NLU models",
    "volume": "main",
    "abstract": "Recent studies indicate that NLU models are prone to rely on shortcut features for prediction, without achieving true language understanding. As a result, these models fail to generalize to real-world out-of-distribution data. In this work, we show that the words in the NLU training set can be modeled as a long-tailed distribution. There are two findings: 1) NLU models have strong preference for features located at the head of the long-tailed distribution, and 2) Shortcut features are picked up during very early few iterations of the model training. These two observations are further employed to formulate a measurement which can quantify the shortcut degree of each training sample. Based on this shortcut measurement, we propose a shortcut mitigation framework LGTR, to suppress the model from making overconfident predictions for samples with large shortcut degree. Experimental results on three NLU benchmarks demonstrate that our long-tailed distribution explanation accurately reflects the shortcut learning behavior of NLU models. Experimental analysis further indicates that LGTR can improve the generalization accuracy on OOD data, while preserving the accuracy on in-distribution data",
    "checked": true,
    "id": "a64f6ba2f83ca489e467d611fde38321b340301a",
    "semantic_title": "towards interpreting and mitigating shortcut learning behavior of nlu models",
    "citation_count": 84,
    "authors": [
      "Mengnan Du",
      "Varun Manjunatha",
      "Rajiv Jain",
      "Ruchi Deshpande",
      "Franck Dernoncourt",
      "Jiuxiang Gu",
      "Tong Sun",
      "Xia Hu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.72": {
    "title": "On Attention Redundancy: A Comprehensive Study",
    "volume": "main",
    "abstract": "Multi-layer multi-head self-attention mechanism is widely applied in modern neural language models. Attention redundancy has been observed among attention heads but has not been deeply studied in the literature. Using BERT-base model as an example, this paper provides a comprehensive study on attention redundancy which is helpful for model interpretation and model compression. We analyze the attention redundancy with Five-Ws and How. (What) We define and focus the study on redundancy matrices generated from pre-trained and fine-tuned BERT-base model for GLUE datasets. (How) We use both token-based and sentence-based distance functions to measure the redundancy. (Where) Clear and similar redundancy patterns (cluster structure) are observed among attention heads. (When) Redundancy patterns are similar in both pre-training and fine-tuning phases. (Who) We discover that redundancy patterns are task-agnostic. Similar redundancy patterns even exist for randomly generated token sequences. (\"Why\") We also evaluate influences of the pre-training dropout ratios on attention redundancy. Based on the phase-independent and task-agnostic attention redundancy patterns, we propose a simple zero-shot pruning method as a case study. Experiments on fine-tuning GLUE tasks verify its effectiveness. The comprehensive analyses on attention redundancy make model understanding and zero-shot model pruning promising",
    "checked": true,
    "id": "57ed901be5d1b4d853d4f8998dadc1b60e2151f9",
    "semantic_title": "on attention redundancy: a comprehensive study",
    "citation_count": 35,
    "authors": [
      "Yuchen Bian",
      "Jiaji Huang",
      "Xingyu Cai",
      "Jiahong Yuan",
      "Kenneth Church"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.73": {
    "title": "Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?",
    "volume": "main",
    "abstract": "Large Transformers pretrained over clinical notes from Electronic Health Records (EHR) have afforded substantial gains in performance on predictive clinical tasks. The cost of training such models (and the necessity of data access to do so) coupled with their utility motivates parameter sharing, i.e., the release of pretrained models such as ClinicalBERT. While most efforts have used deidentified EHR, many researchers have access to large sets of sensitive, non-deidentified EHR with which they might train a BERT model (or similar). Would it be safe to release the weights of such a model if they did? In this work, we design a battery of approaches intended to recover Personal Health Information (PHI) from a trained BERT. Specifically, we attempt to recover patient names and conditions with which they are associated. We find that simple probing methods are not able to meaningfully extract sensitive information from BERT trained over the MIMIC-III corpus of EHR. However, more sophisticated \"attacks\" may succeed in doing so: To facilitate such research, we make our experimental setup and baseline probing models available at https://github.com/elehman16/exposing_patient_data_release",
    "checked": true,
    "id": "1d5c07e7415a7e9be078717197ddf9f3c70a2875",
    "semantic_title": "does bert pretrained on clinical notes reveal sensitive data?",
    "citation_count": 104,
    "authors": [
      "Eric Lehman",
      "Sarthak Jain",
      "Karl Pichotta",
      "Yoav Goldberg",
      "Byron Wallace"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.74": {
    "title": "Low-Complexity Probing via Finding Subnetworks",
    "volume": "main",
    "abstract": "The dominant approach in probing neural networks for linguistic properties is to train a new shallow multi-layer perceptron (MLP) on top of the model's internal representations. This approach can detect properties encoded in the model, but at the cost of adding new parameters that may learn the task directly. We instead propose a subtractive pruning-based probe, where we find an existing subnetwork that performs the linguistic task of interest. Compared to an MLP, the subnetwork probe achieves both higher accuracy on pre-trained models and lower accuracy on random models, so it is both better at finding properties of interest and worse at learning on its own. Next, by varying the complexity of each probe, we show that subnetwork probing Pareto-dominates MLP probing in that it achieves higher accuracy given any budget of probe complexity. Finally, we analyze the resulting subnetworks across various tasks to locate where each task is encoded, and we find that lower-level tasks are captured in lower layers, reproducing similar findings in past work",
    "checked": true,
    "id": "04c84f1b0c5e218ffbc5c0ba5ed9c2c62e63f28e",
    "semantic_title": "low-complexity probing via finding subnetworks",
    "citation_count": 34,
    "authors": [
      "Steven Cao",
      "Victor Sanh",
      "Alexander Rush"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.75": {
    "title": "An Empirical Comparison of Instance Attribution Methods for NLP",
    "volume": "main",
    "abstract": "Widespread adoption of deep models has motivated a pressing need for approaches to interpret network outputs and to facilitate model debugging. Instance attribution methods constitute one means of accomplishing these goals by retrieving training instances that (may have) led to a particular prediction. Influence functions (IF; Koh and Liang 2017) provide machinery for doing this by quantifying the effect that perturbing individual train instances would have on a specific test prediction. However, even approximating the IF is computationally expensive, to the degree that may be prohibitive in many cases. Might simpler approaches (e.g., retrieving train examples most similar to a given test point) perform comparably? In this work, we evaluate the degree to which different potential instance attribution agree with respect to the importance of training samples. We find that simple retrieval methods yield training instances that differ from those identified via gradient-based methods (such as IFs), but that nonetheless exhibit desirable characteristics similar to more complex attribution methods. Code for all methods and experiments in this paper is available at: https://github.com/successar/instance_attributions_NLP",
    "checked": true,
    "id": "b2ad1812d0d8f0f1eb19c17e0bccfce4399dd575",
    "semantic_title": "an empirical comparison of instance attribution methods for nlp",
    "citation_count": 30,
    "authors": [
      "Pouya Pezeshkpour",
      "Sarthak Jain",
      "Byron Wallace",
      "Sameer Singh"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.76": {
    "title": "Generalization in Instruction Following Systems",
    "volume": "main",
    "abstract": "Understanding and executing natural language instructions in a grounded domain is one of the hallmarks of artificial intelligence. In this paper, we focus on instruction understanding in the blocks world domain and investigate the language understanding abilities of two top-performing systems for the task. We aim to understand if the test performance of these models indicates an understanding of the spatial domain and of the natural language instructions relative to it, or whether they merely over-fit spurious signals in the dataset. We formulate a set of expectations one might have from an instruction following model and concretely characterize the different dimensions of robustness such a model should possess. Despite decent test performance, we find that state-of-the-art models fall short of these expectations and are extremely brittle. We then propose a learning strategy that involves data augmentation and show through extensive experiments that the proposed learning strategy yields models that are competitive on the original test set while satisfying our expectations much better",
    "checked": true,
    "id": "b2d08f6e17882545b2cbb865fd8c907eb21c3aac",
    "semantic_title": "generalization in instruction following systems",
    "citation_count": 3,
    "authors": [
      "Soham Dan",
      "Michael Zhou",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.77": {
    "title": "LightningDOT: Pre-training Visual-Semantic Embeddings for Real-Time Image-Text Retrieval",
    "volume": "main",
    "abstract": "Multimodal pre-training has propelled great advancement in vision-and-language research. These large-scale pre-trained models, although successful, fatefully suffer from slow inference speed due to enormous computational cost mainly from cross-modal attention in Transformer architecture. When applied to real-life applications, such latency and computation demand severely deter the practical use of pre-trained models. In this paper, we study Image-text retrieval (ITR), the most mature scenario of V+L application, which has been widely studied even prior to the emergence of recent pre-trained models. We propose a simple yet highly effective approach, LightningDOT that accelerates the inference time of ITR by thousands of times, without sacrificing accuracy. LightningDOT removes the time-consuming cross-modal attention by extracting pre-cached feature indexes offline, and employing instant dot-product matching online, which significantly speeds up retrieval process. In fact, our LightningDOT achieves superior performance across mainstream ITR benchmarks such as Flickr30k and COCO datasets, outperforming existing pre-trained models that consume 1000 times magnitude of computational hours using the same features",
    "checked": true,
    "id": "7a1e7151ac3e76c52cbcd287299742eb2076d34c",
    "semantic_title": "lightningdot: pre-training visual-semantic embeddings for real-time image-text retrieval",
    "citation_count": 74,
    "authors": [
      "Siqi Sun",
      "Yen-Chun Chen",
      "Linjie Li",
      "Shuohang Wang",
      "Yuwei Fang",
      "Jingjing Liu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.78": {
    "title": "Measuring Social Biases in Grounded Vision and Language Embeddings",
    "volume": "main",
    "abstract": "We generalize the notion of measuring social biases in word embeddings to visually grounded word embeddings. Biases are present in grounded embeddings, and indeed seem to be equally or more significant than for ungrounded embeddings. This is despite the fact that vision and language can suffer from different biases, which one might hope could attenuate the biases in both. Multiple ways exist to generalize metrics measuring bias in word embeddings to this new setting. We introduce the space of generalizations (Grounded-WEAT and Grounded-SEAT) and demonstrate that three generalizations answer different yet important questions about how biases, language, and vision interact. These metrics are used on a new dataset, the first for grounded bias, created by augmenting standard linguistic bias benchmarks with 10,228 images from COCO, Conceptual Captions, and Google Images. Dataset construction is challenging because vision datasets are themselves very biased. The presence of these biases in systems will begin to have real-world consequences as they are deployed, making carefully measuring bias and then mitigating it critical to building a fair society",
    "checked": true,
    "id": "0af07469cd031061ac6c5783d5bd0399ac48812e",
    "semantic_title": "measuring social biases in grounded vision and language embeddings",
    "citation_count": 54,
    "authors": [
      "Candace Ross",
      "Boris Katz",
      "Andrei Barbu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.79": {
    "title": "MTAG: Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences",
    "volume": "main",
    "abstract": "Human communication is multimodal in nature; it is through multiple modalities such as language, voice, and facial expressions, that opinions and emotions are expressed. Data in this domain exhibits complex multi-relational and temporal interactions. Learning from this data is a fundamentally challenging research problem. In this paper, we propose Modal-Temporal Attention Graph (MTAG). MTAG is an interpretable graph-based neural model that provides a suitable framework for analyzing multimodal sequential data. We first introduce a procedure to convert unaligned multimodal sequence data into a graph with heterogeneous nodes and edges that captures the rich interactions across modalities and through time. Then, a novel graph fusion operation, called MTAG fusion, along with a dynamic pruning and read-out technique, is designed to efficiently process this modal-temporal graph and capture various interactions. By learning to focus only on the important interactions within the graph, MTAG achieves state-of-the-art performance on multimodal sentiment analysis and emotion recognition benchmarks, while utilizing significantly fewer model parameters",
    "checked": true,
    "id": "26c3885abe14d14e2aa98a7ead2ebccca56a6283",
    "semantic_title": "mtag: modal-temporal attention graph for unaligned human multimodal language sequences",
    "citation_count": 64,
    "authors": [
      "Jianing Yang",
      "Yongxin Wang",
      "Ruitao Yi",
      "Yuying Zhu",
      "Azaan Rehman",
      "Amir Zadeh",
      "Soujanya Poria",
      "Louis-Philippe Morency"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.80": {
    "title": "Grounding Open-Domain Instructions to Automate Web Support Tasks",
    "volume": "main",
    "abstract": "Grounding natural language instructions on the web to perform previously unseen tasks enables accessibility and automation. We introduce a task and dataset to train AI agents from open-domain, step-by-step instructions originally written for people. We build RUSS (Rapid Universal Support Service) to tackle this problem. RUSS consists of two models: First, a BERT-LSTM with pointers parses instructions to WebLang, a domain-specific language we design for grounding natural language on the web. Then, a grounding model retrieves the unique IDs of any webpage elements requested in the WebLang. RUSS may interact with the user through a dialogue (e.g. ask for an address) or execute a web operation (e.g. click a button) inside the web runtime. To augment training, we synthesize natural language instructions mapped to WebLang. Our dataset consists of 80 different customer service problems from help websites, with a total of 741 step-by-step instructions and their corresponding actions. RUSS achieves 76.7% end-to-end accuracy predicting agent actions from single instructions. It outperforms state-of-the-art models that directly map instructions to actions without WebLang. Our user study shows that RUSS is preferred by actual users over web navigation",
    "checked": true,
    "id": "1905db8aef0d63d594b38a53939206fbef9fcd65",
    "semantic_title": "grounding open-domain instructions to automate web support tasks",
    "citation_count": 18,
    "authors": [
      "Nancy Xu",
      "Sam Masling",
      "Michael Du",
      "Giovanni Campagna",
      "Larry Heck",
      "James Landay",
      "Monica Lam"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.81": {
    "title": "Modular Networks for Compositional Instruction Following",
    "volume": "main",
    "abstract": "Standard architectures used in instruction following often struggle on novel compositions of subgoals (e.g. navigating to landmarks or picking up objects) observed during training. We propose a modular architecture for following natural language instructions that describe sequences of diverse subgoals. In our approach, subgoal modules each carry out natural language instructions for a specific subgoal type. A sequence of modules to execute is chosen by learning to segment the instructions and predicting a subgoal type for each segment. When compared to standard, non-modular sequence-to-sequence approaches on ALFRED, a challenging instruction following benchmark, we find that modularization improves generalization to novel subgoal compositions, as well as to environments unseen in training",
    "checked": true,
    "id": "35c6bdab35e8fd4e982302b5270da3c8098c58b1",
    "semantic_title": "modular networks for compositional instruction following",
    "citation_count": 27,
    "authors": [
      "Rodolfo Corona",
      "Daniel Fried",
      "Coline Devin",
      "Dan Klein",
      "Trevor Darrell"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.82": {
    "title": "Improving Cross-Modal Alignment in Vision Language Navigation via Syntactic Information",
    "volume": "main",
    "abstract": "Vision language navigation is the task that requires an agent to navigate through a 3D environment based on natural language instructions. One key challenge in this task is to ground instructions with the current visual information that the agent perceives. Most of the existing work employs soft attention over individual words to locate the instruction required for the next action. However, different words have different functions in a sentence (e.g., modifiers convey attributes, verbs convey actions). Syntax information like dependencies and phrase structures can aid the agent to locate important parts of the instruction. Hence, in this paper, we propose a navigation agent that utilizes syntax information derived from a dependency tree to enhance alignment between the instruction and the current visual scenes. Empirically, our agent outperforms the baseline model that does not use syntax information on the Room-to-Room dataset, especially in the unseen environment. Besides, our agent achieves the new state-of-the-art on Room-Across-Room dataset, which contains instructions in 3 languages (English, Hindi, and Telugu). We also show that our agent is better at aligning instructions with the current visual information via qualitative visualizations",
    "checked": true,
    "id": "2df14c28dc4ebd6ce3411884f0824eaa42aaf7c8",
    "semantic_title": "improving cross-modal alignment in vision language navigation via syntactic information",
    "citation_count": 23,
    "authors": [
      "Jialu Li",
      "Hao Tan",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.83": {
    "title": "Improving Pretrained Models for Zero-shot Multi-label Text Classification through Reinforced Label Hierarchy Reasoning",
    "volume": "main",
    "abstract": "Exploiting label hierarchies has become a promising approach to tackling the zero-shot multi-label text classification (ZS-MTC) problem. Conventional methods aim to learn a matching model between text and labels, using a graph encoder to incorporate label hierarchies to obtain effective label representations (Rios and Kavuluru, 2018). More recently, pretrained models like BERT (Devlin et al., 2018) have been used to convert classification tasks into a textual entailment task (Yin et al., 2019). This approach is naturally suitable for the ZS-MTC task. However, pretrained models are underexplored in the existing work because they do not generate individual vector representations for text or labels, making it unintuitive to combine them with conventional graph encoding methods. In this paper, we explore to improve pretrained models with label hierarchies on the ZS-MTC task. We propose a Reinforced Label Hierarchy Reasoning (RLHR) approach to encourage interdependence among labels in the hierarchies during training. Meanwhile, to overcome the weakness of flat predictions, we design a rollback algorithm that can remove logical errors from predictions during inference. Experimental results on three real-life datasets show that our approach achieves better performance and outperforms previous non-pretrained methods on the ZS-MTC task",
    "checked": true,
    "id": "ea587cdf0d6821742b80ef599f4b31cf097af84e",
    "semantic_title": "improving pretrained models for zero-shot multi-label text classification through reinforced label hierarchy reasoning",
    "citation_count": 18,
    "authors": [
      "Hui Liu",
      "Danqing Zhang",
      "Bing Yin",
      "Xiaodan Zhu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.84": {
    "title": "Fine-Tuning Pre-trained Language Model with Weak Supervision: A Contrastive-Regularized Self-Training Approach",
    "volume": "main",
    "abstract": "Fine-tuned pre-trained language models (LMs) have achieved enormous success in many natural language processing (NLP) tasks, but they still require excessive labeled data in the fine-tuning stage. We study the problem of fine-tuning pre-trained LMs using only weak supervision, without any labeled data. This problem is challenging because the high capacity of LMs makes them prone to overfitting the noisy labels generated by weak supervision. To address this problem, we develop a contrastive self-training framework, COSINE, to enable fine-tuning LMs with weak supervision. Underpinned by contrastive regularization and confidence-based reweighting, our framework gradually improves model fitting while effectively suppressing error propagation. Experiments on sequence, token, and sentence pair classification tasks show that our model outperforms the strongest baseline by large margins and achieves competitive performance with fully-supervised fine-tuning methods. Our implementation is available on https://github.com/yueyu1030/COSINE",
    "checked": true,
    "id": "19803adec3b97fb2e3c8097f17bf33fabf311795",
    "semantic_title": "fine-tuning pre-trained language model with weak supervision: a contrastive-regularized self-training approach",
    "citation_count": 113,
    "authors": [
      "Yue Yu",
      "Simiao Zuo",
      "Haoming Jiang",
      "Wendi Ren",
      "Tuo Zhao",
      "Chao Zhang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.85": {
    "title": "Posterior Differential Regularization with f-divergence for Improving Model Robustness",
    "volume": "main",
    "abstract": "We address the problem of enhancing model robustness through regularization. Specifically, we focus on methods that regularize the model posterior difference between clean and noisy inputs. Theoretically, we provide a connection of two recent methods, Jacobian Regularization and Virtual Adversarial Training, under this framework. Additionally, we generalize the posterior differential regularization to the family of f-divergences and characterize the overall framework in terms of the Jacobian matrix. Empirically, we compare those regularizations and standard BERT training on a diverse set of tasks to provide a comprehensive profile of their effect on model generalization. For both fully supervised and semi-supervised settings, we show that regularizing the posterior difference with f-divergence can result in well-improved model robustness. In particular, with a proper f-divergence, a BERT-base model can achieve comparable generalization as its BERT-large counterpart for in-domain, adversarial and domain shift scenarios, indicating the great potential of the proposed framework for enhancing NLP model robustness",
    "checked": true,
    "id": "1a2bed6bed5f28095ebe5793f0c31d38690d6cd4",
    "semantic_title": "posterior differential regularization with f-divergence for improving model robustness",
    "citation_count": 31,
    "authors": [
      "Hao Cheng",
      "Xiaodong Liu",
      "Lis Pereira",
      "Yaoliang Yu",
      "Jianfeng Gao"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.86": {
    "title": "Understanding Hard Negatives in Noise Contrastive Estimation",
    "volume": "main",
    "abstract": "The choice of negative examples is important in noise contrastive estimation. Recent works find that hard negatives—highest-scoring incorrect examples under the model—are effective in practice, but they are used without a formal justification. We develop analytical tools to understand the role of hard negatives. Specifically, we view the contrastive loss as a biased estimator of the gradient of the cross-entropy loss, and show both theoretically and empirically that setting the negative distribution to be the model distribution results in bias reduction. We also derive a general form of the score function that unifies various architectures used in text retrieval. By combining hard negatives with appropriate score functions, we obtain strong results on the challenging task of zero-shot entity linking",
    "checked": true,
    "id": "274b9cce94bc5ab6d30a62ecaeb424cce829af60",
    "semantic_title": "understanding hard negatives in noise contrastive estimation",
    "citation_count": 46,
    "authors": [
      "Wenzheng Zhang",
      "Karl Stratos"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.87": {
    "title": "Certified Robustness to Word Substitution Attack with Differential Privacy",
    "volume": "main",
    "abstract": "The robustness and security of natural language processing (NLP) models are significantly important in real-world applications. In the context of text classification tasks, adversarial examples can be designed by substituting words with synonyms under certain semantic and syntactic constraints, such that a well-trained model will give a wrong prediction. Therefore, it is crucial to develop techniques to provide a rigorous and provable robustness guarantee against such attacks. In this paper, we propose WordDP to achieve certified robustness against word substitution at- tacks in text classification via differential privacy (DP). We establish the connection between DP and adversarial robustness for the first time in the text domain and propose a conceptual exponential mechanism-based algorithm to formally achieve the robustness. We further present a practical simulated exponential mechanism that has efficient inference with certified robustness. We not only provide a rigorous analytic derivation of the certified condition but also experimentally compare the utility of WordDP with existing defense algorithms. The results show that WordDP achieves higher accuracy and more than 30X efficiency improvement over the state-of-the-art certified robustness mechanism in typical text classification tasks",
    "checked": true,
    "id": "0021b3beb2ee0906b425eef7c0f453623c1c6a03",
    "semantic_title": "certified robustness to word substitution attack with differential privacy",
    "citation_count": 37,
    "authors": [
      "Wenjie Wang",
      "Pengfei Tang",
      "Jian Lou",
      "Li Xiong"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.88": {
    "title": "DReCa: A General Task Augmentation Strategy for Few-Shot Natural Language Inference",
    "volume": "main",
    "abstract": "Meta-learning promises few-shot learners that can adapt to new distributions by repurposing knowledge acquired from previous training. However, we believe meta-learning has not yet succeeded in NLP due to the lack of a well-defined task distribution, leading to attempts that treat datasets as tasks. Such an ad hoc task distribution causes problems of quantity and quality. Since there's only a handful of datasets for any NLP problem, meta-learners tend to overfit their adaptation mechanism and, since NLP datasets are highly heterogeneous, many learning episodes have poor transfer between their support and query sets, which discourages the meta-learner from adapting. To alleviate these issues, we propose DReCA (Decomposing datasets into Reasoning Categories), a simple method for discovering and using latent reasoning categories in a dataset, to form additional high quality tasks. DReCA works by splitting examples into label groups, embedding them with a finetuned BERT model and then clustering each group into reasoning categories. Across four few-shot NLI problems, we demonstrate that using DReCA improves the accuracy of meta-learners by 1.5-4%",
    "checked": true,
    "id": "186c304c782c70b0b5d571ea140a8fb4ebcf31a4",
    "semantic_title": "dreca: a general task augmentation strategy for few-shot natural language inference",
    "citation_count": 28,
    "authors": [
      "Shikhar Murty",
      "Tatsunori B. Hashimoto",
      "Christopher Manning"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.89": {
    "title": "Harnessing Multilinguality in Unsupervised Machine Translation for Rare Languages",
    "volume": "main",
    "abstract": "Unsupervised translation has reached impressive performance on resource-rich language pairs such as English-French and English-German. However, early studies have shown that in more realistic settings involving low-resource, rare languages, unsupervised translation performs poorly, achieving less than 3.0 BLEU. In this work, we show that multilinguality is critical to making unsupervised systems practical for low-resource settings. In particular, we present a single model for 5 low-resource languages (Gujarati, Kazakh, Nepali, Sinhala, and Turkish) to and from English directions, which leverages monolingual and auxiliary parallel data from other high-resource language pairs via a three-stage training scheme. We outperform all current state-of-the-art unsupervised baselines for these languages, achieving gains of up to 14.4 BLEU. Additionally, we outperform strong supervised baselines for various language pairs as well as match the performance of the current state-of-the-art supervised model for Nepali-English. We conduct a series of ablation studies to establish the robustness of our model under different degrees of data quality, as well as to analyze the factors which led to the superior performance of the proposed approach over traditional unsupervised models",
    "checked": true,
    "id": "21db684d6b7bfd13538e78cb2fecab1b5e91e0e7",
    "semantic_title": "harnessing multilinguality in unsupervised machine translation for rare languages",
    "citation_count": 31,
    "authors": [
      "Xavier Garcia",
      "Aditya Siddhant",
      "Orhan Firat",
      "Ankur Parikh"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.90": {
    "title": "Macro-Average: Rare Types Are Important Too",
    "volume": "main",
    "abstract": "While traditional corpus-level evaluation metrics for machine translation (MT) correlate well with fluency, they struggle to reflect adequacy. Model-based MT metrics trained on segment-level human judgments have emerged as an attractive replacement due to strong correlation results. These models, however, require potentially expensive re-training for new domains and languages. Furthermore, their decisions are inherently non-transparent and appear to reflect unwelcome biases. We explore the simple type-based classifier metric, MacroF1, and study its applicability to MT evaluation. We find that MacroF1 is competitive on direct assessment, and outperforms others in indicating downstream cross-lingual information retrieval task performance. Further, we show that MacroF1 can be used to effectively compare supervised and unsupervised neural machine translation, and reveal significant qualitative differences in the methods' outputs",
    "checked": true,
    "id": "3b34e79610e5acaba352def4323a59f6d531fac7",
    "semantic_title": "macro-average: rare types are important too",
    "citation_count": 21,
    "authors": [
      "Thamme Gowda",
      "Weiqiu You",
      "Constantine Lignos",
      "Jonathan May"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.91": {
    "title": "Assessing Reference-Free Peer Evaluation for Machine Translation",
    "volume": "main",
    "abstract": "Reference-free evaluation has the potential to make machine translation evaluation substantially more scalable, allowing us to pivot easily to new languages or domains. It has been recently shown that the probabilities given by a large, multilingual model can achieve state of the art results when used as a reference-free metric. We experiment with various modifications to this model, and demonstrate that by scaling it up we can match the performance of BLEU. We analyze various potential weaknesses of the approach, and find that it is surprisingly robust and likely to offer reasonable performance across a broad spectrum of domains and different system qualities",
    "checked": true,
    "id": "08c828c70be6fe604c272afe308ff4ac2bc734c7",
    "semantic_title": "assessing reference-free peer evaluation for machine translation",
    "citation_count": 7,
    "authors": [
      "Sweta Agrawal",
      "George Foster",
      "Markus Freitag",
      "Colin Cherry"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.92": {
    "title": "The Curious Case of Hallucinations in Neural Machine Translation",
    "volume": "main",
    "abstract": "In this work, we study hallucinations in Neural Machine Translation (NMT), which lie at an extreme end on the spectrum of NMT pathologies. Firstly, we connect the phenomenon of hallucinations under source perturbation to the Long-Tail theory of Feldman, and present an empirically validated hypothesis that explains hallucinations under source perturbation. Secondly, we consider hallucinations under corpus-level noise (without any source perturbation) and demonstrate that two prominent types of natural hallucinations (detached and oscillatory outputs) could be generated and explained through specific corpus-level noise patterns. Finally, we elucidate the phenomenon of hallucination amplification in popular data-generation processes such as Backtranslation and sequence-level Knowledge Distillation. We have released the datasets and code to replicate our results",
    "checked": true,
    "id": "143cd4b4717651caf276c7256502dc491454e197",
    "semantic_title": "the curious case of hallucinations in neural machine translation",
    "citation_count": 164,
    "authors": [
      "Vikas Raunak",
      "Arul Menezes",
      "Marcin Junczys-Dowmunt"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.93": {
    "title": "Towards Continual Learning for Multilingual Machine Translation via Vocabulary Substitution",
    "volume": "main",
    "abstract": "We propose a straightforward vocabulary adaptation scheme to extend the language capacity of multilingual machine translation models, paving the way towards efficient continual learning for multilingual machine translation. Our approach is suitable for large-scale datasets, applies to distant languages with unseen scripts, incurs only minor degradation on the translation performance for the original language pairs and provides competitive performance even in the case where we only possess monolingual data for the new languages",
    "checked": true,
    "id": "2b66a0d8a00b0ba7b585e11ab34879420ad351cb",
    "semantic_title": "towards continual learning for multilingual machine translation via vocabulary substitution",
    "citation_count": 33,
    "authors": [
      "Xavier Garcia",
      "Noah Constant",
      "Ankur Parikh",
      "Orhan Firat"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.94": {
    "title": "Towards Modeling the Style of Translators in Neural Machine Translation",
    "volume": "main",
    "abstract": "One key ingredient of neural machine translation is the use of large datasets from different domains and resources (e.g. Europarl, TED talks). These datasets contain documents translated by professional translators using different but consistent translation styles. Despite that, the model is usually trained in a way that neither explicitly captures the variety of translation styles present in the data nor translates new data in different and controllable styles. In this work, we investigate methods to augment the state of the art Transformer model with translator information that is available in part of the training data. We show that our style-augmented translation models are able to capture the style variations of translators and to generate translations with different styles on new data. Indeed, the generated variations differ significantly, up to +4.5 BLEU score difference. Despite that, human evaluation confirms that the translations are of the same quality",
    "checked": true,
    "id": "9aa2459d69ec3f2804413e45e2a0f489f5ea9c8b",
    "semantic_title": "towards modeling the style of translators in neural machine translation",
    "citation_count": 13,
    "authors": [
      "Yue Wang",
      "Cuong Hoang",
      "Marcello Federico"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.95": {
    "title": "Self-Supervised Test-Time Learning for Reading Comprehension",
    "volume": "main",
    "abstract": "Recent work on unsupervised question answering has shown that models can be trained with procedurally generated question-answer pairs and can achieve performance competitive with supervised methods. In this work, we consider the task of unsupervised reading comprehension and present a method that performs \"test-time learning\" (TTL) on a given context (text passage), without requiring training on large-scale human-authored datasets containing context-question-answer triplets. This method operates directly on a single test context, uses self-supervision to train models on synthetically generated question-answer pairs, and then infers answers to unseen human-authored questions for this context. Our method achieves accuracies competitive with fully supervised methods and significantly outperforms current unsupervised methods. TTL methods with a smaller model are also competitive with the current state-of-the-art in unsupervised reading comprehension",
    "checked": true,
    "id": "f14c6acf6d04b3bd69c6d1a59ea8659bf3b95561",
    "semantic_title": "self-supervised test-time learning for reading comprehension",
    "citation_count": 20,
    "authors": [
      "Pratyay Banerjee",
      "Tejas Gokhale",
      "Chitta Baral"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.96": {
    "title": "Capturing Row and Column Semantics in Transformer Based Question Answering over Tables",
    "volume": "main",
    "abstract": "Transformer based architectures are recently used for the task of answering questions over tables. In order to improve the accuracy on this task, specialized pre-training techniques have been developed and applied on millions of open-domain web tables. In this paper, we propose two novel approaches demonstrating that one can achieve superior performance on table QA task without even using any of these specialized pre-training techniques. The first model, called RCI interaction, leverages a transformer based architecture that independently classifies rows and columns to identify relevant cells. While this model yields extremely high accuracy at finding cell values on recent benchmarks, a second model we propose, called RCI representation, provides a significant efficiency advantage for online QA systems over tables by materializing embeddings for existing tables. Experiments on recent benchmarks prove that the proposed methods can effectively locate cell values on tables (up to ~98% Hit@1 accuracy on WikiSQL lookup questions). Also, the interaction model outperforms the state-of-the-art transformer based approaches, pre-trained on very large table corpora (TAPAS and TaBERT), achieving ~3.4% and ~18.86% additional precision improvement on the standard WikiSQL benchmark",
    "checked": true,
    "id": "1066d94640d1374cc1f21b49083095e1cbcdc67f",
    "semantic_title": "capturing row and column semantics in transformer based question answering over tables",
    "citation_count": 50,
    "authors": [
      "Michael Glass",
      "Mustafa Canim",
      "Alfio Gliozzo",
      "Saneem Chemmengath",
      "Vishwajeet Kumar",
      "Rishav Chakravarti",
      "Avi Sil",
      "Feifei Pan",
      "Samarth Bharadwaj",
      "Nicolas Rodolfo Fauceglia"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.97": {
    "title": "Explainable Multi-hop Verbal Reasoning Through Internal Monologue",
    "volume": "main",
    "abstract": "Many state-of-the-art (SOTA) language models have achieved high accuracy on several multi-hop reasoning problems. However, these approaches tend to not be interpretable because they do not make the intermediate reasoning steps explicit. Moreover, models trained on simpler tasks tend to fail when directly tested on more complex problems. We propose the Explainable multi-hop Verbal Reasoner (EVR) to solve these limitations by (a) decomposing multi-hop reasoning problems into several simple ones, and (b) using natural language to guide the intermediate reasoning hops. We implement EVR by extending the classic reasoning paradigm General Problem Solver (GPS) with a SOTA generative language model to generate subgoals and perform inference in natural language at each reasoning step. Evaluation of EVR on the RuleTaker synthetic question answering (QA) dataset shows that EVR achieves SOTA performance while being able to generate all reasoning steps in natural language. Furthermore, EVR generalizes better than other strong methods when trained on simpler tasks or less training data (up to 35.7% and 7.7% absolute improvement respectively)",
    "checked": true,
    "id": "82deb8df3f3c765a4a84dc072cf2a20ecaefa420",
    "semantic_title": "explainable multi-hop verbal reasoning through internal monologue",
    "citation_count": 16,
    "authors": [
      "Zhengzhong Liang",
      "Steven Bethard",
      "Mihai Surdeanu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.98": {
    "title": "Robust Question Answering Through Sub-part Alignment",
    "volume": "main",
    "abstract": "Current textual question answering (QA) models achieve strong performance on in-domain test sets, but often do so by fitting surface-level patterns, so they fail to generalize to out-of-distribution settings. To make a more robust and understandable QA system, we model question answering as an alignment problem. We decompose both the question and context into smaller units based on off-the-shelf semantic representations (here, semantic roles), and align the question to a subgraph of the context in order to find the answer. We formulate our model as a structured SVM, with alignment scores computed via BERT, and we can train end-to-end despite using beam search for approximate inference. Our use of explicit alignments allows us to explore a set of constraints with which we can prohibit certain types of bad model behavior arising in cross-domain settings. Furthermore, by investigating differences in scores across different potential answers, we can seek to understand what particular aspects of the input lead the model to choose the answer without relying on post-hoc explanation techniques. We train our model on SQuAD v1.1 and test it on several adversarial and out-of-domain datasets. The results show that our model is more robust than the standard BERT QA model, and constraints derived from alignment scores allow us to effectively trade off coverage and accuracy",
    "checked": true,
    "id": "6606f7b9938dcc0e03dba45359dd04cb15018f13",
    "semantic_title": "robust question answering through sub-part alignment",
    "citation_count": 13,
    "authors": [
      "Jifan Chen",
      "Greg Durrett"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.99": {
    "title": "Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models",
    "volume": "main",
    "abstract": "We propose a general framework called Text Modular Networks(TMNs) for building interpretable systems that learn to solve complex tasks by decomposing them into simpler ones solvable by existing models. To ensure solvability of simpler tasks, TMNs learn the textual input-output behavior (i.e., language) of existing models through their datasets. This differs from prior decomposition-based approaches which, besides being designed specifically for each complex task, produce decompositions independent of existing sub-models. Specifically, we focus on Question Answering (QA) and show how to train a next-question generator to sequentially produce sub-questions targeting appropriate sub-models, without additional human annotation. These sub-questions and answers provide a faithful natural language explanation of the model's reasoning. We use this framework to build ModularQA, a system that can answer multi-hop reasoning questions by decomposing them into sub-questions answerable by a neural factoid single-span QA model and a symbolic calculator. Our experiments show that ModularQA is more versatile than existing explainable systems for DROP and HotpotQA datasets, is more robust than state-of-the-art blackbox (uninterpretable) systems, and generates more understandable and trustworthy explanations compared to prior work",
    "checked": true,
    "id": "0e1d82d24d58433ce9e211551605a0bfd296624f",
    "semantic_title": "text modular networks: learning to decompose tasks in the language of existing models",
    "citation_count": 81,
    "authors": [
      "Tushar Khot",
      "Daniel Khashabi",
      "Kyle Richardson",
      "Peter Clark",
      "Ashish Sabharwal"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.100": {
    "title": "RECONSIDER: Improved Re-Ranking using Span-Focused Cross-Attention for Open Domain Question Answering",
    "volume": "main",
    "abstract": "State-of-the-art Machine Reading Comprehension (MRC) models for Open-domain Question Answering (QA) are typically trained for span selection using distantly supervised positive examples and heuristically retrieved negative examples. This training scheme possibly explains empirical observations that these models achieve a high recall amongst their top few predictions, but a low overall accuracy, motivating the need for answer re-ranking. We develop a successful re-ranking approach (RECONSIDER) for span-extraction tasks that improves upon the performance of MRC models, even beyond large-scale pre-training. RECONSIDER is trained on positive and negative examples extracted from high confidence MRC model predictions, and uses in-passage span annotations to perform span-focused re-ranking over a smaller candidate set. As a result, RECONSIDER learns to eliminate close false positives, achieving a new extractive state of the art on four QA tasks, with 45.5% Exact Match accuracy on Natural Questions with real user questions, and 61.7% on TriviaQA. We will release all related data, models, and code",
    "checked": true,
    "id": "c435fdeff9d3b30c3bece24429123d23da147992",
    "semantic_title": "reconsider: improved re-ranking using span-focused cross-attention for open domain question answering",
    "citation_count": 19,
    "authors": [
      "Srinivasan Iyer",
      "Sewon Min",
      "Yashar Mehdad",
      "Wen-tau Yih"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.101": {
    "title": "On the Transferability of Minimal Prediction Preserving Inputs in Question Answering",
    "volume": "main",
    "abstract": "Recent work (Feng et al., 2018) establishes the presence of short, uninterpretable input fragments that yield high confidence and accuracy in neural models. We refer to these as Minimal Prediction Preserving Inputs (MPPIs). In the context of question answering, we investigate competing hypotheses for the existence of MPPIs, including poor posterior calibration of neural models, lack of pretraining, and \"dataset bias\" (where a model learns to attend to spurious, non-generalizable cues in the training data). We discover a perplexing invariance of MPPIs to random training seed, model architecture, pretraining, and training domain. MPPIs demonstrate remarkable transferability across domains achieving significantly higher performance than comparably short queries. Additionally, penalizing over-confidence on MPPIs fails to improve either generalization or adversarial robustness. These results suggest the interpretability of MPPIs is insufficient to characterize generalization capacity of these models. We hope this focused investigation encourages more systematic analysis of model behavior outside of the human interpretable distribution of examples",
    "checked": true,
    "id": "2facbd54b2f8527018453a89e0bb0eb69ae53df1",
    "semantic_title": "on the transferability of minimal prediction preserving inputs in question answering",
    "citation_count": 2,
    "authors": [
      "Shayne Longpre",
      "Yi Lu",
      "Chris DuBois"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.102": {
    "title": "Understanding by Understanding Not: Modeling Negation in Language Models",
    "volume": "main",
    "abstract": "Negation is a core construction in natural language. Despite being very successful on many tasks, state-of-the-art pre-trained language models often handle negation incorrectly. To improve language models in this regard, we propose to augment the language modeling objective with an unlikelihood objective that is based on negated generic sentences from a raw text corpus. By training BERT with the resulting combined objective we reduce the mean top 1 error rate to 4% on the negated LAMA dataset. We also see some improvements on the negated NLI benchmarks",
    "checked": true,
    "id": "408cc1103ab953a26c7071ffd9ce808469f77a01",
    "semantic_title": "understanding by understanding not: modeling negation in language models",
    "citation_count": 75,
    "authors": [
      "Arian Hosseini",
      "Siva Reddy",
      "Dzmitry Bahdanau",
      "R Devon Hjelm",
      "Alessandro Sordoni",
      "Aaron Courville"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.103": {
    "title": "DuoRAT: Towards Simpler Text-to-SQL Models",
    "volume": "main",
    "abstract": "Recent neural text-to-SQL models can effectively translate natural language questions to corresponding SQL queries on unseen databases. Working mostly on the Spider dataset, researchers have proposed increasingly sophisticated solutions to the problem. Contrary to this trend, in this paper we focus on simplifications. We begin by building DuoRAT, a re-implementation of the state-of-the-art RAT-SQL model that unlike RAT-SQL is using only relation-aware or vanilla transformers as the building blocks. We perform several ablation experiments using DuoRAT as the baseline model. Our experiments confirm the usefulness of some techniques and point out the redundancy of others, including structural SQL features and features that link the question with the schema",
    "checked": true,
    "id": "a2462ff5546b45b1cc7cb50ffc50c7ddececca65",
    "semantic_title": "duorat: towards simpler text-to-sql models",
    "citation_count": 23,
    "authors": [
      "Torsten Scholak",
      "Raymond Li",
      "Dzmitry Bahdanau",
      "Harm de Vries",
      "Chris Pal"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.104": {
    "title": "Looking Beyond Sentence-Level Natural Language Inference for Question Answering and Text Summarization",
    "volume": "main",
    "abstract": "Natural Language Inference (NLI) has garnered significant attention in recent years; however, the promise of applying NLI breakthroughs to other downstream NLP tasks has remained unfulfilled. In this work, we use the multiple-choice reading comprehension (MCRC) and checking factual correctness of textual summarization (CFCS) tasks to investigate potential reasons for this. Our findings show that: (1) the relatively shorter length of premises in traditional NLI datasets is the primary challenge prohibiting usage in downstream applications (which do better with longer contexts); (2) this challenge can be addressed by automatically converting resource-rich reading comprehension datasets into longer-premise NLI datasets; and (3) models trained on the converted, longer-premise datasets outperform those trained using short-premise traditional NLI datasets on downstream tasks primarily due to the difference in premise lengths",
    "checked": true,
    "id": "9541fd32d09728ce10b105cbcd662a081cefab68",
    "semantic_title": "looking beyond sentence-level natural language inference for question answering and text summarization",
    "citation_count": 47,
    "authors": [
      "Anshuman Mishra",
      "Dhruvesh Patel",
      "Aparna Vijayakumar",
      "Xiang Lorraine Li",
      "Pavan Kapanipathi",
      "Kartik Talamadupula"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.105": {
    "title": "Structure-Grounded Pretraining for Text-to-SQL",
    "volume": "main",
    "abstract": "Learning to capture text-table alignment is essential for tasks like text-to-SQL. A model needs to correctly recognize natural language references to columns and values and to ground them in the given database schema. In this paper, we present a novel weakly supervised Structure-Grounded pretraining framework (STRUG) for text-to-SQL that can effectively learn to capture text-table alignment based on a parallel text-table corpus. We identify a set of novel pretraining tasks: column grounding, value grounding and column-value mapping, and leverage them to pretrain a text-table encoder. Additionally, to evaluate different methods under more realistic text-table alignment settings, we create a new evaluation set Spider-Realistic based on Spider dev set with explicit mentions of column names removed, and adopt eight existing text-to-SQL datasets for cross-database evaluation. STRUG brings significant improvement over BERTLARGE in all settings. Compared with existing pretraining methods such as GRAPPA, STRUG achieves similar performance on Spider, and outperforms all baselines on more realistic sets. All the code and data used in this work will be open-sourced to facilitate future research",
    "checked": true,
    "id": "3ff5f99c7924b4b28669f09ce3bfbf26fd92abc1",
    "semantic_title": "structure-grounded pretraining for text-to-sql",
    "citation_count": 114,
    "authors": [
      "Xiang Deng",
      "Ahmed Hassan Awadallah",
      "Christopher Meek",
      "Oleksandr Polozov",
      "Huan Sun",
      "Matthew Richardson"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.106": {
    "title": "Incremental Few-shot Text Classification with Multi-round New Classes: Formulation, Dataset and System",
    "volume": "main",
    "abstract": "Text classification is usually studied by labeling natural language texts with relevant categories from a predefined set. In the real world, new classes might keep challenging the existing system with limited labeled data. The system should be intelligent enough to recognize upcoming new classes with a few examples. In this work, we define a new task in the NLP domain, incremental few-shot text classification, where the system incrementally handles multiple rounds of new classes. For each round, there is a batch of new classes with a few labeled examples per class. Two major challenges exist in this new task: (i) For the learning process, the system should incrementally learn new classes round by round without re-training on the examples of preceding classes; (ii) For the performance, the system should perform well on new classes without much loss on preceding classes. In addition to formulating the new task, we also release two benchmark datasets in the incremental few-shot setting: intent classification and relation classification. Moreover, we propose two entailment approaches, ENTAILMENT and HYBRID, which show promise for solving this novel problem",
    "checked": true,
    "id": "65cc7839ee6e595382bb45b15968c565746d1d94",
    "semantic_title": "incremental few-shot text classification with multi-round new classes: formulation, dataset and system",
    "citation_count": 43,
    "authors": [
      "Congying Xia",
      "Wenpeng Yin",
      "Yihao Feng",
      "Philip Yu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.107": {
    "title": "Temporal Reasoning on Implicit Events from Distant Supervision",
    "volume": "main",
    "abstract": "We propose TRACIE, a novel temporal reasoning dataset that evaluates the degree to which systems understand implicit events—events that are not mentioned explicitly in natural language text but can be inferred from it. This introduces a new challenge in temporal reasoning research, where prior work has focused on explicitly mentioned events. Human readers can infer implicit events via commonsense reasoning, resulting in a more comprehensive understanding of the situation and, consequently, better reasoning about time. We find, however, that state-of-the-art models struggle when predicting temporal relationships between implicit and explicit events. To address this, we propose a neuro-symbolic temporal reasoning model, SymTime, which exploits distant supervision signals from large-scale text and uses temporal rules to combine start times and durations to infer end times. SymTime outperforms strong baseline systems on TRACIE by 5%, and by 11% in a zero prior knowledge training setting. Our approach also generalizes to other temporal reasoning tasks, as evidenced by a gain of 1%-9% on MATRES, an explicit event benchmark",
    "checked": true,
    "id": "9d436d25981ea61db728bd490f0d54376d08953e",
    "semantic_title": "temporal reasoning on implicit events from distant supervision",
    "citation_count": 58,
    "authors": [
      "Ben Zhou",
      "Kyle Richardson",
      "Qiang Ning",
      "Tushar Khot",
      "Ashish Sabharwal",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.108": {
    "title": "Disentangling Semantics and Syntax in Sentence Embeddings with Pre-trained Language Models",
    "volume": "main",
    "abstract": "Pre-trained language models have achieved huge success on a wide range of NLP tasks. However, contextual representations from pre-trained models contain entangled semantic and syntactic information, and therefore cannot be directly used to derive useful semantic sentence embeddings for some tasks. Paraphrase pairs offer an effective way of learning the distinction between semantics and syntax, as they naturally share semantics and often vary in syntax. In this work, we present ParaBART, a semantic sentence embedding model that learns to disentangle semantics and syntax in sentence embeddings obtained by pre-trained language models. ParaBART is trained to perform syntax-guided paraphrasing, based on a source sentence that shares semantics with the target paraphrase, and a parse tree that specifies the target syntax. In this way, ParaBART learns disentangled semantic and syntactic representations from their respective inputs with separate encoders. Experiments in English show that ParaBART outperforms state-of-the-art sentence embedding models on unsupervised semantic similarity tasks. Additionally, we show that our approach can effectively remove syntactic information from semantic sentence embeddings, leading to better robustness against syntactic variation on downstream semantic tasks",
    "checked": true,
    "id": "9a93e530ed611f14f917d2612788f1ba2a050cbb",
    "semantic_title": "disentangling semantics and syntax in sentence embeddings with pre-trained language models",
    "citation_count": 19,
    "authors": [
      "James Y. Huang",
      "Kuan-Hao Huang",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.109": {
    "title": "Structure-Aware Abstractive Conversation Summarization via Discourse and Action Graphs",
    "volume": "main",
    "abstract": "Abstractive conversation summarization has received much attention recently. However, these generated summaries often suffer from insufficient, redundant, or incorrect content, largely due to the unstructured and complex characteristics of human-human interactions. To this end, we propose to explicitly model the rich structures in conversations for more precise and accurate conversation summarization, by first incorporating discourse relations between utterances and action triples (\"who-doing-what\") in utterances through structured graphs to better encode conversations, and then designing a multi-granularity decoder to generate summaries by combining all levels of information. Experiments show that our proposed models outperform state-of-the-art methods and generalize well in other domains in terms of both automatic evaluations and human judgments. We have publicly released our code at https://github.com/GT-SALT/Structure-Aware-BART",
    "checked": true,
    "id": "be9679047ff70d6147dbdb42521f62c5660f6f75",
    "semantic_title": "structure-aware abstractive conversation summarization via discourse and action graphs",
    "citation_count": 86,
    "authors": [
      "Jiaao Chen",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.110": {
    "title": "A New Approach to Overgenerating and Scoring Abstractive Summaries",
    "volume": "main",
    "abstract": "We propose a new approach to generate multiple variants of the target summary with diverse content and varying lengths, then score and select admissible ones according to users' needs. Abstractive summarizers trained on single reference summaries may struggle to produce outputs that achieve multiple desirable properties, i.e., capturing the most important information, being faithful to the original, grammatical and fluent. In this paper, we propose a two-staged strategy to generate a diverse set of candidate summaries from the source text in stage one, then score and select admissible ones in stage two. Importantly, our generator gives a precise control over the length of the summary, which is especially well-suited when space is limited. Our selectors are designed to predict the optimal summary length and put special emphasis on faithfulness to the original text. Both stages can be effectively trained, optimized and evaluated. Our experiments on benchmark summarization datasets suggest that this paradigm can achieve state-of-the-art performance",
    "checked": true,
    "id": "4779a89320f27b309984f40fe0a3e97c71ce6afa",
    "semantic_title": "a new approach to overgenerating and scoring abstractive summaries",
    "citation_count": 17,
    "authors": [
      "Kaiqiang Song",
      "Bingqing Wang",
      "Zhe Feng",
      "Fei Liu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.111": {
    "title": "D2S: Document-to-Slide Generation Via Query-Based Text Summarization",
    "volume": "main",
    "abstract": "Presentations are critical for communication in all areas of our lives, yet the creation of slide decks is often tedious and time-consuming. There has been limited research aiming to automate the document-to-slides generation process and all face a critical challenge: no publicly available dataset for training and benchmarking. In this work, we first contribute a new dataset, SciDuet, consisting of pairs of papers and their corresponding slides decks from recent years' NLP and ML conferences (e.g., ACL). Secondly, we present D2S, a novel system that tackles the document-to-slides task with a two-step approach: 1) Use slide titles to retrieve relevant and engaging text, figures, and tables; 2) Summarize the retrieved context into bullet points with long-form question answering. Our evaluation suggests that long-form QA outperforms state-of-the-art summarization baselines on both automated ROUGE metrics and qualitative human evaluation",
    "checked": true,
    "id": "77c5ebea69b0c11653d79c2c721e456d6bcc8b3a",
    "semantic_title": "d2s: document-to-slide generation via query-based text summarization",
    "citation_count": 26,
    "authors": [
      "Edward Sun",
      "Yufang Hou",
      "Dakuo Wang",
      "Yunfeng Zhang",
      "Nancy X. R. Wang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.112": {
    "title": "Efficient Attentions for Long Document Summarization",
    "volume": "main",
    "abstract": "The quadratic computational and memory complexities of large Transformers have limited their scalability for long document summarization. In this paper, we propose Hepos, a novel efficient encoder-decoder attention with head-wise positional strides to effectively pinpoint salient information from the source. We further conduct a systematic study of existing efficient self-attentions. Combined with Hepos, we are able to process ten times more tokens than existing models that use full attentions. For evaluation, we present a new dataset, GovReport, with significantly longer documents and summaries. Results show that our models produce significantly higher ROUGE scores than competitive comparisons, including new state-of-the-art results on PubMed. Human evaluation also shows that our models generate more informative summaries with fewer unfaithful errors",
    "checked": true,
    "id": "9dc624d7258d1a56117ca720aea953ce46b66b21",
    "semantic_title": "efficient attentions for long document summarization",
    "citation_count": 188,
    "authors": [
      "Luyang Huang",
      "Shuyang Cao",
      "Nikolaus Parulian",
      "Heng Ji",
      "Lu Wang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.113": {
    "title": "RefSum: Refactoring Neural Summarization",
    "volume": "main",
    "abstract": "Although some recent works show potential complementarity among different state-of-the-art systems, few works try to investigate this problem in text summarization. Researchers in other areas commonly refer to the techniques of reranking or stacking to approach this problem. In this work, we highlight several limitations of previous methods, which motivates us to present a new framework Refactor that provides a unified view of text summarization and summaries combination. Experimentally, we perform a comprehensive evaluation that involves twenty-two base systems, four datasets, and three different application scenarios. Besides new state-of-the-art results on CNN/DailyMail dataset (46.18 ROUGE-1), we also elaborate on how our proposed method addresses the limitations of the traditional methods and the effectiveness of the Refactor model sheds light on insight for performance improvement. Our system can be directly used by other researchers as an off-the-shelf tool to achieve further performance improvements. We open-source all the code and provide a convenient interface to use it: https://github.com/yixinL7/Refactoring-Summarization",
    "checked": true,
    "id": "15251fa3a3bcf695bf153d0856886cab9a3145ea",
    "semantic_title": "refsum: refactoring neural summarization",
    "citation_count": 38,
    "authors": [
      "Yixin Liu",
      "Zi-Yi Dou",
      "Pengfei Liu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.114": {
    "title": "Annotating and Modeling Fine-grained Factuality in Summarization",
    "volume": "main",
    "abstract": "Recent pre-trained abstractive summarization systems have started to achieve credible performance, but a major barrier to their use in practice is their propensity to output summaries that are not faithful to the input and that contain factual errors. While a number of annotated datasets and statistical models for assessing factuality have been explored, there is no clear picture of what errors are most important to target or where current techniques are succeeding and failing. We explore both synthetic and human-labeled data sources for training models to identify factual errors in summarization, and study factuality at the word-, dependency-, and sentence-level. Our observations are threefold. First, exhibited factual errors differ significantly across datasets, and commonly-used training sets of simple synthetic errors do not reflect errors made on abstractive datasets like XSum. Second, human-labeled data with fine-grained annotations provides a more effective training signal than sentence-level annotations or synthetic data. Finally, we show that our best factuality detection model enables training of more factual XSum summarization models by allowing us to identify non-factual tokens in the training data",
    "checked": true,
    "id": "f434ca09c7e4acd8dbe2ba54c1a99a930bbfeeba",
    "semantic_title": "annotating and modeling fine-grained factuality in summarization",
    "citation_count": 138,
    "authors": [
      "Tanya Goyal",
      "Greg Durrett"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.115": {
    "title": "Larger-Context Tagging: When and Why Does It Work?",
    "volume": "main",
    "abstract": "The development of neural networks and pretraining techniques has spawned many sentence-level tagging systems that achieved superior performance on typical benchmarks. However, a relatively less discussed topic is what if more context information is introduced into current top-scoring tagging systems. Although several existing works have attempted to shift tagging systems from sentence-level to document-level, there is still no consensus conclusion about when and why it works, which limits the applicability of the larger-context approach in tagging tasks. In this paper, instead of pursuing a state-of-the-art tagging system by architectural exploration, we focus on investigating when and why the larger-context training, as a general strategy, can work. To this end, we conduct a thorough comparative study on four proposed aggregators for context information collecting and present an attribute-aided evaluation method to interpret the improvement brought by larger-context training. Experimentally, we set up a testbed based on four tagging tasks and thirteen datasets. Hopefully, our preliminary observations can deepen the understanding of larger-context training and enlighten more follow-up works on the use of contextual information",
    "checked": true,
    "id": "5c090d8b0cb5e9f54f3da2323b830812f5721644",
    "semantic_title": "larger-context tagging: when and why does it work?",
    "citation_count": 5,
    "authors": [
      "Jinlan Fu",
      "Liangjing Feng",
      "Qi Zhang",
      "Xuanjing Huang",
      "Pengfei Liu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.116": {
    "title": "Neural Sequence Segmentation as Determining the Leftmost Segments",
    "volume": "main",
    "abstract": "Prior methods to text segmentation are mostly at token level. Despite the adequacy, this nature limits their full potential to capture the long-term dependencies among segments. In this work, we propose a novel framework that incrementally segments natural language sentences at segment level. For every step in segmentation, it recognizes the leftmost segment of the remaining sequence. Implementations involve LSTM-minus technique to construct the phrase representations and recurrent neural networks (RNN) to model the iterations of determining the leftmost segments. We have conducted extensive experiments on syntactic chunking and Chinese part-of-speech (POS) tagging across 3 datasets, demonstrating that our methods have significantly outperformed previous all baselines and achieved new state-of-the-art results. Moreover, qualitative analysis and the study on segmenting long-length sentences verify its effectiveness in modeling long-term dependencies",
    "checked": true,
    "id": "a7a8d08c2285c293c9a7650fc36dbd8888fef508",
    "semantic_title": "neural sequence segmentation as determining the leftmost segments",
    "citation_count": 5,
    "authors": [
      "Yangming Li",
      "Lemao Liu",
      "Kaisheng Yao"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.117": {
    "title": "PCFGs Can Do Better: Inducing Probabilistic Context-Free Grammars with Many Symbols",
    "volume": "main",
    "abstract": "Probabilistic context-free grammars (PCFGs) with neural parameterization have been shown to be effective in unsupervised phrase-structure grammar induction. However, due to the cubic computational complexity of PCFG representation and parsing, previous approaches cannot scale up to a relatively large number of (nonterminal and preterminal) symbols. In this work, we present a new parameterization form of PCFGs based on tensor decomposition, which has at most quadratic computational complexity in the symbol number and therefore allows us to use a much larger number of symbols. We further use neural parameterization for the new form to improve unsupervised parsing performance. We evaluate our model across ten languages and empirically demonstrate the effectiveness of using more symbols",
    "checked": true,
    "id": "ce38ecff6584e6bb6a5cc3ecaf0c152014eea4ff",
    "semantic_title": "pcfgs can do better: inducing probabilistic context-free grammars with many symbols",
    "citation_count": 18,
    "authors": [
      "Songlin Yang",
      "Yanpeng Zhao",
      "Kewei Tu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.118": {
    "title": "GEMNET: Effective Gated Gazetteer Representations for Recognizing Complex Entities in Low-context Input",
    "volume": "main",
    "abstract": "Named Entity Recognition (NER) remains difficult in real-world settings; current challenges include short texts (low context), emerging entities, and complex entities (e.g. movie names). Gazetteer features can help, but results have been mixed due to challenges with adding extra features, and a lack of realistic evaluation data. It has been shown that including gazetteer features can cause models to overuse or underuse them, leading to poor generalization. We propose GEMNET, a novel approach for gazetteer knowledge integration, including (1) a flexible Contextual Gazetteer Representation (CGR) encoder that can be fused with any word-level model; and (2) a Mixture-of- Experts gating network that overcomes the feature overuse issue by learning to conditionally combine the context and gazetteer features, instead of assigning them fixed weights. To comprehensively evaluate our approaches, we create 3 large NER datasets (24M tokens) reflecting current challenges. In an uncased setting, our methods show large gains (up to +49% F1) in recognizing difficult entities compared to existing baselines. On standard benchmarks, we achieve a new uncased SOTA on CoNLL03 and WNUT17",
    "checked": true,
    "id": "6cf4c68c5d91824dd4bb8813324e5a289934aaa1",
    "semantic_title": "gemnet: effective gated gazetteer representations for recognizing complex entities in low-context input",
    "citation_count": 73,
    "authors": [
      "Tao Meng",
      "Anjie Fang",
      "Oleg Rokhlenko",
      "Shervin Malmasi"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.119": {
    "title": "Video-aided Unsupervised Grammar Induction",
    "volume": "main",
    "abstract": "We investigate video-aided grammar induction, which learns a constituency parser from both unlabeled text and its corresponding video. Existing methods of multi-modal grammar induction focus on grammar induction from text-image pairs, with promising results showing that the information from static images is useful in induction. However, videos provide even richer information, including not only static objects but also actions and state changes useful for inducing verb phrases. In this paper, we explore rich features (e.g. action, object, scene, audio, face, OCR and speech) from videos, taking the recent Compound PCFG model as the baseline. We further propose a Multi-Modal Compound PCFG model (MMC-PCFG) to effectively aggregate these rich features from different modalities. Our proposed MMC-PCFG is trained end-to-end and outperforms each individual modality and previous state-of-the-art systems on three benchmarks, i.e. DiDeMo, YouCook2 and MSRVTT, confirming the effectiveness of leveraging video information for unsupervised grammar induction",
    "checked": true,
    "id": "ac3d2fd5c32deca0512c0dfee921a5fa91499bcf",
    "semantic_title": "video-aided unsupervised grammar induction",
    "citation_count": 18,
    "authors": [
      "Songyang Zhang",
      "Linfeng Song",
      "Lifeng Jin",
      "Kun Xu",
      "Dong Yu",
      "Jiebo Luo"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.120": {
    "title": "Generating Negative Samples by Manipulating Golden Responses for Unsupervised Learning of a Response Evaluation Model",
    "volume": "main",
    "abstract": "Evaluating the quality of responses generated by open-domain conversation systems is a challenging task. This is partly because there can be multiple appropriate responses to a given dialogue history. Reference-based metrics that rely on comparisons to a set of known correct responses often fail to account for this variety, and consequently correlate poorly with human judgment. To address this problem, researchers have investigated the possibility of assessing response quality without using a set of known correct responses. RUBER demonstrated that an automatic response evaluation model could be made using unsupervised learning for the next-utterance prediction (NUP) task. For the unsupervised learning of such model, we propose a method of manipulating a golden response to create a new negative response that is designed to be inappropriate within the context while maintaining high similarity with the original golden response. We find, from our experiments on English datasets, that using the negative samples generated by our method alongside random negative samples can increase the model's correlation with human evaluations. The process of generating such negative samples is automated and does not rely on human annotation",
    "checked": true,
    "id": "a7a00e14c60fd13bf08dc435e9b0fdd96d050b99",
    "semantic_title": "generating negative samples by manipulating golden responses for unsupervised learning of a response evaluation model",
    "citation_count": 9,
    "authors": [
      "ChaeHun Park",
      "Eugene Jang",
      "Wonsuk Yang",
      "Jong Park"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.121": {
    "title": "How Robust are Fact Checking Systems on Colloquial Claims?",
    "volume": "main",
    "abstract": "Knowledge is now starting to power neural dialogue agents. At the same time, the risk of misinformation and disinformation from dialogue agents also rises. Verifying the veracity of information from formal sources are widely studied in computational fact checking. In this work, we ask: How robust are fact checking systems on claims in colloquial style? We aim to open up new discussions in the intersection of fact verification and dialogue safety. In order to investigate how fact checking systems behave on colloquial claims, we transfer the styles of claims from FEVER (Thorne et al., 2018) into colloquialism. We find that existing fact checking systems that perform well on claims in formal style significantly degenerate on colloquial claims with the same semantics. Especially, we show that document retrieval is the weakest spot in the system even vulnerable to filler words, such as \"yeah\" and \"you know\". The document recall of WikiAPI retriever (Hanselowski et al., 2018) which is 90.0% on FEVER, drops to 72.2% on the colloquial claims. We compare the characteristics of colloquial claims to those of claims in formal style, and demonstrate the challenging issues in them",
    "checked": true,
    "id": "828ace8bc78f6ab0636d5c7183e773a0e49d367c",
    "semantic_title": "how robust are fact checking systems on colloquial claims?",
    "citation_count": 13,
    "authors": [
      "Byeongchang Kim",
      "Hyunwoo Kim",
      "Seokhee Hong",
      "Gunhee Kim"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.122": {
    "title": "Fine-grained Post-training for Improving Retrieval-based Dialogue Systems",
    "volume": "main",
    "abstract": "Retrieval-based dialogue systems display an outstanding performance when pre-trained language models are used, which includes bidirectional encoder representations from transformers (BERT). During the multi-turn response selection, BERT focuses on training the relationship between the context with multiple utterances and the response. However, this method of training is insufficient when considering the relations between each utterance in the context. This leads to a problem of not completely understanding the context flow that is required to select a response. To address this issue, we propose a new fine-grained post-training method that reflects the characteristics of the multi-turn dialogue. Specifically, the model learns the utterance level interactions by training every short context-response pair in a dialogue session. Furthermore, by using a new training objective, the utterance relevance classification, the model understands the semantic relevance and coherence between the dialogue utterances. Experimental results show that our model achieves new state-of-the-art with significant margins on three benchmark datasets. This suggests that the fine-grained post-training method is highly effective for the response selection task",
    "checked": true,
    "id": "9f359007e9af7e49e95b3bba3c8621c6fa2f8cca",
    "semantic_title": "fine-grained post-training for improving retrieval-based dialogue systems",
    "citation_count": 62,
    "authors": [
      "Janghoon Han",
      "Taesuk Hong",
      "Byoungjae Kim",
      "Youngjoong Ko",
      "Jungyun Seo"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.123": {
    "title": "Put Chatbot into Its Interlocutor's Shoes: New Framework to Learn Chatbot Responding with Intention",
    "volume": "main",
    "abstract": "Most chatbot literature that focuses on improving the fluency and coherence of a chatbot, is dedicated to making chatbots more human-like. However, very little work delves into what really separates humans from chatbots – humans intrinsically understand the effect their responses have on the interlocutor and often respond with an intention such as proposing an optimistic view to make the interlocutor feel better. This paper proposes an innovative framework to train chatbots to possess human-like intentions. Our framework includes a guiding chatbot and an interlocutor model that plays the role of humans. The guiding chatbot is assigned an intention and learns to induce the interlocutor to reply with responses matching the intention, for example, long responses, joyful responses, responses with specific words, etc. We examined our framework using three experimental setups and evaluated the guiding chatbot with four different metrics to demonstrate flexibility and performance advantages. Additionally, we performed trials with human interlocutors to substantiate the guiding chatbot's effectiveness in influencing the responses of humans to a certain extent. Code will be made available to the public",
    "checked": true,
    "id": "3336a84e248cc858b02cb63598c48a26be60b869",
    "semantic_title": "put chatbot into its interlocutor's shoes: new framework to learn chatbot responding with intention",
    "citation_count": 4,
    "authors": [
      "Hsuan Su",
      "Jiun-Hao Jhan",
      "Fan-yun Sun",
      "Saurav Sahay",
      "Hung-yi Lee"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.124": {
    "title": "Adding Chit-Chat to Enhance Task-Oriented Dialogues",
    "volume": "main",
    "abstract": "Existing dialogue corpora and models are typically designed under two disjoint motives: while task-oriented systems focus on achieving functional goals (e.g., booking hotels), open-domain chatbots aim at making socially engaging conversations. In this work, we propose to integrate both types of systems by Adding Chit-Chat to ENhance Task-ORiented dialogues (ACCENTOR), with the goal of making virtual assistant conversations more engaging and interactive. Specifically, we propose a Human <-> AI collaborative data collection approach for generating diverse chit-chat responses to augment task-oriented dialogues with minimal annotation effort. We then present our new chit-chat-based annotations to 23.8K dialogues from two popular task-oriented datasets (Schema-Guided Dialogue and MultiWOZ 2.1) and demonstrate their advantage over the originals via human evaluation. Lastly, we propose three new models for adding chit-chat to task-oriented dialogues, explicitly trained to predict user goals and to generate contextually relevant chit-chat responses. Automatic and human evaluations show that, compared with the state-of-the-art task-oriented baseline, our models can code-switch between task and chit-chat to be more engaging, interesting, knowledgeable, and humanlike, while maintaining competitive task performance",
    "checked": true,
    "id": "3c64247b4c75c64b4a67d306200bab5b5d3b5bfa",
    "semantic_title": "adding chit-chat to enhance task-oriented dialogues",
    "citation_count": 60,
    "authors": [
      "Kai Sun",
      "Seungwhan Moon",
      "Paul Crook",
      "Stephen Roller",
      "Becka Silvert",
      "Bing Liu",
      "Zhiguang Wang",
      "Honglei Liu",
      "Eunjoon Cho",
      "Claire Cardie"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.125": {
    "title": "Incorporating Syntax and Semantics in Coreference Resolution with Heterogeneous Graph Attention Network",
    "volume": "main",
    "abstract": "External syntactic and semantic information has been largely ignored by existing neural coreference resolution models. In this paper, we present a heterogeneous graph-based model to incorporate syntactic and semantic structures of sentences. The proposed graph contains a syntactic sub-graph where tokens are connected based on a dependency tree, and a semantic sub-graph that contains arguments and predicates as nodes and semantic role labels as edges. By applying a graph attention network, we can obtain syntactically and semantically augmented word representation, which can be integrated using an attentive integration layer and gating mechanism. Experiments on the OntoNotes 5.0 benchmark show the effectiveness of our proposed model",
    "checked": true,
    "id": "deb29fedbd57d6851e56751b8f1ce7dd7644b025",
    "semantic_title": "incorporating syntax and semantics in coreference resolution with heterogeneous graph attention network",
    "citation_count": 12,
    "authors": [
      "Fan Jiang",
      "Trevor Cohn"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.126": {
    "title": "Context Tracking Network: Graph-based Context Modeling for Implicit Discourse Relation Recognition",
    "volume": "main",
    "abstract": "Implicit discourse relation recognition (IDRR) aims to identify logical relations between two adjacent sentences in the discourse. Existing models fail to fully utilize the contextual information which plays an important role in interpreting each local sentence. In this paper, we thus propose a novel graph-based Context Tracking Network (CT-Net) to model the discourse context for IDRR. The CT-Net firstly converts the discourse into the paragraph association graph (PAG), where each sentence tracks their closely related context from the intricate discourse through different types of edges. Then, the CT-Net extracts contextual representation from the PAG through a specially designed cross-grained updating mechanism, which can effectively integrate both sentence-level and token-level contextual semantics. Experiments on PDTB 2.0 show that the CT-Net gains better performance than models that roughly model the context",
    "checked": true,
    "id": "657134c793f171c6cd379b5bf5495a8b21cb6a18",
    "semantic_title": "context tracking network: graph-based context modeling for implicit discourse relation recognition",
    "citation_count": 18,
    "authors": [
      "Yingxue Zhang",
      "Fandong Meng",
      "Peng Li",
      "Ping Jian",
      "Jie Zhou"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.127": {
    "title": "Improving Neural RST Parsing Model with Silver Agreement Subtrees",
    "volume": "main",
    "abstract": "Most of the previous Rhetorical Structure Theory (RST) parsing methods are based on supervised learning such as neural networks, that require an annotated corpus of sufficient size and quality. However, the RST Discourse Treebank (RST-DT), the benchmark corpus for RST parsing in English, is small due to the costly annotation of RST trees. The lack of large annotated training data causes poor performance especially in relation labeling. Therefore, we propose a method for improving neural RST parsing models by exploiting silver data, i.e., automatically annotated data. We create large-scale silver data from an unlabeled corpus by using a state-of-the-art RST parser. To obtain high-quality silver data, we extract agreement subtrees from RST trees for documents built using the RST parsers. We then pre-train a neural RST parser with the obtained silver data and fine-tune it on the RST-DT. Experimental results show that our method achieved the best micro-F1 scores for Nuclearity and Relation at 75.0 and 63.2, respectively. Furthermore, we obtained a remarkable gain in the Relation score, 3.0 points, against the previous state-of-the-art parser",
    "checked": true,
    "id": "47597b92175ac42e20ac0faf365bccac405559cb",
    "semantic_title": "improving neural rst parsing model with silver agreement subtrees",
    "citation_count": 10,
    "authors": [
      "Naoki Kobayashi",
      "Tsutomu Hirao",
      "Hidetaka Kamigaito",
      "Manabu Okumura",
      "Masaaki Nagata"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.128": {
    "title": "RST Parsing from Scratch",
    "volume": "main",
    "abstract": "We introduce a novel top-down end-to-end formulation of document level discourse parsing in the Rhetorical Structure Theory (RST) framework. In this formulation, we consider discourse parsing as a sequence of splitting decisions at token boundaries and use a seq2seq network to model the splitting decisions. Our framework facilitates discourse parsing from scratch without requiring discourse segmentation as a prerequisite; rather, it yields segmentation as part of the parsing process. Our unified parsing model adopts a beam search to decode the best tree structure by searching through a space of high scoring trees. With extensive experiments on the standard RST discourse treebank, we demonstrate that our parser outperforms existing methods by a good margin in both end-to-end parsing and parsing with gold segmentation. More importantly, it does so without using any handcrafted features, making it faster and easily adaptable to new languages and domains",
    "checked": true,
    "id": "56d4ffb7c010a304984c00e1d8b9a911896ac157",
    "semantic_title": "rst parsing from scratch",
    "citation_count": 20,
    "authors": [
      "Thanh-Tung Nguyen",
      "Xuan-Phi Nguyen",
      "Shafiq Joty",
      "Xiaoli Li"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.129": {
    "title": "Did they answer? Subjective acts and intents in conversational discourse",
    "volume": "main",
    "abstract": "Discourse signals are often implicit, leaving it up to the interpreter to draw the required inferences. At the same time, discourse is embedded in a social context, meaning that interpreters apply their own assumptions and beliefs when resolving these inferences, leading to multiple, valid interpretations. However, current discourse data and frameworks ignore the social aspect, expecting only a single ground truth. We present the first discourse dataset with multiple and subjective interpretations of English conversation in the form of perceived conversation acts and intents. We carefully analyze our dataset and create computational models to (1) confirm our hypothesis that taking into account the bias of the interpreters leads to better predictions of the interpretations, (2) and show disagreements are nuanced and require a deeper understanding of the different contextual factors. We share our dataset and code at http://github.com/elisaF/subjective_discourse",
    "checked": true,
    "id": "f09558dc121c0ad17e562fbc31c7d966e50141c6",
    "semantic_title": "did they answer? subjective acts and intents in conversational discourse",
    "citation_count": 18,
    "authors": [
      "Elisa Ferracane",
      "Greg Durrett",
      "Junyi Jessy Li",
      "Katrin Erk"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.130": {
    "title": "Evaluating the Impact of a Hierarchical Discourse Representation on Entity Coreference Resolution Performance",
    "volume": "main",
    "abstract": "Recent work on entity coreference resolution (CR) follows current trends in Deep Learning applied to embeddings and relatively simple task-related features. SOTA models do not make use of hierarchical representations of discourse structure. In this work, we leverage automatically constructed discourse parse trees within a neural approach and demonstrate a significant improvement on two benchmark entity coreference-resolution datasets. We explore how the impact varies depending upon the type of mention",
    "checked": true,
    "id": "93a5eae16445853aec49b0645c100fd8dae1b1e3",
    "semantic_title": "evaluating the impact of a hierarchical discourse representation on entity coreference resolution performance",
    "citation_count": 2,
    "authors": [
      "Sopan Khosla",
      "James Fiacco",
      "Carolyn Rosé"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.131": {
    "title": "Bridging Resolution: Making Sense of the State of the Art",
    "volume": "main",
    "abstract": "While Yu and Poesio (2020) have recently demonstrated the superiority of their neural multi-task learning (MTL) model to rule-based approaches for bridging anaphora resolution, there is little understanding of (1) how it is better than the rule-based approaches (e.g., are the two approaches making similar or complementary mistakes?) and (2) what should be improved. To shed light on these issues, we (1) propose a hybrid rule-based and MTL approach that would enable a better understanding of their comparative strengths and weaknesses; and (2) perform a manual analysis of the errors made by the MTL model",
    "checked": true,
    "id": "74ab3c277638266bbadbfc5066cf6f5065287d0c",
    "semantic_title": "bridging resolution: making sense of the state of the art",
    "citation_count": 13,
    "authors": [
      "Hideo Kobayashi",
      "Vincent Ng"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.132": {
    "title": "Explicitly Modeling Syntax in Language Models with Incremental Parsing and a Dynamic Oracle",
    "volume": "main",
    "abstract": "Syntax is fundamental to our thinking about language. Failing to capture the structure of input language could lead to generalization problems and over-parametrization. In the present work, we propose a new syntax-aware language model: Syntactic Ordered Memory (SOM). The model explicitly models the structure with an incremental parser and maintains the conditional probability setting of a standard language model (left-to-right). To train the incremental parser and avoid exposure bias, we also propose a novel dynamic oracle, so that SOM is more robust to wrong parsing decisions. Experiments show that SOM can achieve strong results in language modeling, incremental parsing, and syntactic generalization tests while using fewer parameters than other models",
    "checked": true,
    "id": "7e04ee74a5d8e1e7b0892b25e1f32f64b15fa640",
    "semantic_title": "explicitly modeling syntax in language models with incremental parsing and a dynamic oracle",
    "citation_count": 5,
    "authors": [
      "Yikang Shen",
      "Shawn Tan",
      "Alessandro Sordoni",
      "Siva Reddy",
      "Aaron Courville"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.133": {
    "title": "Revisiting the Weaknesses of Reinforcement Learning for Neural Machine Translation",
    "volume": "main",
    "abstract": "Policy gradient algorithms have found wide adoption in NLP, but have recently become subject to criticism, doubting their suitability for NMT. Choshen et al. (2020) identify multiple weaknesses and suspect that their success is determined by the shape of output distributions rather than the reward. In this paper, we revisit these claims and study them under a wider range of configurations. Our experiments on in-domain and cross-domain adaptation reveal the importance of exploration and reward scaling, and provide empirical counter-evidence to these claims",
    "checked": true,
    "id": "a109b995dfeb444417f66545c67bce210bd11650",
    "semantic_title": "revisiting the weaknesses of reinforcement learning for neural machine translation",
    "citation_count": 42,
    "authors": [
      "Samuel Kiegeland",
      "Julia Kreutzer"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.134": {
    "title": "Learning to Organize a Bag of Words into Sentences with Neural Networks: An Empirical Study",
    "volume": "main",
    "abstract": "Sequential information, a.k.a., orders, is assumed to be essential for processing a sequence with recurrent neural network or convolutional neural network based encoders. However, is it possible to encode natural languages without orders? Given a bag of words from a disordered sentence, humans may still be able to understand what those words mean by reordering or reconstructing them. Inspired by such an intuition, in this paper, we perform a study to investigate how \"order\" information takes effects in natural language learning. By running comprehensive comparisons, we quantitatively compare the ability of several representative neural models to organize sentences from a bag of words under three typical scenarios, and summarize some empirical findings and challenges, which can shed light on future research on this line of work",
    "checked": true,
    "id": "fa3ec5da7e08fb9257da3c9449659bcf260e869f",
    "semantic_title": "learning to organize a bag of words into sentences with neural networks: an empirical study",
    "citation_count": 8,
    "authors": [
      "Chongyang Tao",
      "Shen Gao",
      "Juntao Li",
      "Yansong Feng",
      "Dongyan Zhao",
      "Rui Yan"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.135": {
    "title": "Mask Attention Networks: Rethinking and Strengthen Transformer",
    "volume": "main",
    "abstract": "Transformer is an attention-based neural network, which consists of two sublayers, namely, Self-Attention Network (SAN) and Feed-Forward Network (FFN). Existing research explores to enhance the two sublayers separately to improve the capability of Transformer for text representation. In this paper, we present a novel understanding of SAN and FFN as Mask Attention Networks (MANs) and show that they are two special cases of MANs with static mask matrices. However, their static mask matrices limit the capability for localness modeling in text representation learning. We therefore introduce a new layer named dynamic mask attention network (DMAN) with a learnable mask matrix which is able to model localness adaptively. To incorporate advantages of DMAN, SAN, and FFN, we propose a sequential layered structure to combine the three types of layers. Extensive experiments on various tasks, including neural machine translation and text summarization demonstrate that our model outperforms the original Transformer",
    "checked": true,
    "id": "712bf9c7202b8dec3d06491a380bbac9c9600fbc",
    "semantic_title": "mask attention networks: rethinking and strengthen transformer",
    "citation_count": 50,
    "authors": [
      "Zhihao Fan",
      "Yeyun Gong",
      "Dayiheng Liu",
      "Zhongyu Wei",
      "Siyuan Wang",
      "Jian Jiao",
      "Nan Duan",
      "Ruofei Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.136": {
    "title": "ERNIE-Gram: Pre-Training with Explicitly N-Gram Masked Language Modeling for Natural Language Understanding",
    "volume": "main",
    "abstract": "Coarse-grained linguistic information, such as named entities or phrases, facilitates adequately representation learning in pre-training. Previous works mainly focus on extending the objective of BERT's Masked Language Modeling (MLM) from masking individual tokens to contiguous sequences of n tokens. We argue that such contiguously masking method neglects to model the intra-dependencies and inter-relation of coarse-grained linguistic information. As an alternative, we propose ERNIE-Gram, an explicitly n-gram masking method to enhance the integration of coarse-grained information into pre-training. In ERNIE-Gram, n-grams are masked and predicted directly using explicit n-gram identities rather than contiguous sequences of n tokens. Furthermore, ERNIE-Gram employs a generator model to sample plausible n-gram identities as optional n-gram masks and predict them in both coarse-grained and fine-grained manners to enable comprehensive n-gram prediction and relation modeling. We pre-train ERNIE-Gram on English and Chinese text corpora and fine-tune on 19 downstream tasks. Experimental results show that ERNIE-Gram outperforms previous pre-training models like XLNet and RoBERTa by a large margin, and achieves comparable results with state-of-the-art methods. The source codes and pre-trained models have been released at https://github.com/PaddlePaddle/ERNIE",
    "checked": true,
    "id": "e50aabe91493365034f8c997bb1d1db689526a89",
    "semantic_title": "ernie-gram: pre-training with explicitly n-gram masked language modeling for natural language understanding",
    "citation_count": 35,
    "authors": [
      "Dongling Xiao",
      "Yu-Kun Li",
      "Han Zhang",
      "Yu Sun",
      "Hao Tian",
      "Hua Wu",
      "Haifeng Wang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.137": {
    "title": "Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models",
    "volume": "main",
    "abstract": "Chinese pre-trained language models usually process text as a sequence of characters, while ignoring more coarse granularity, e.g., words. In this work, we propose a novel pre-training paradigm for Chinese — Lattice-BERT, which explicitly incorporates word representations along with characters, thus can model a sentence in a multi-granularity manner. Specifically, we construct a lattice graph from the characters and words in a sentence and feed all these text units into transformers. We design a lattice position attention mechanism to exploit the lattice structures in self-attention layers. We further propose a masked segment prediction task to push the model to learn from rich but redundant information inherent in lattices, while avoiding learning unexpected tricks. Experiments on 11 Chinese natural language understanding tasks show that our model can bring an average increase of 1.5% under the 12-layer setting, which achieves new state-of-the-art among base-size models on the CLUE benchmarks. Further analysis shows that Lattice-BERT can harness the lattice structures, and the improvement comes from the exploration of redundant information and multi-granularity representations. Our code will be available at https://github.com/alibaba/pretrained-language-models/LatticeBERT",
    "checked": true,
    "id": "111dbe14083359ab39886790632e7f1421732a8a",
    "semantic_title": "lattice-bert: leveraging multi-granularity representations in chinese pre-trained language models",
    "citation_count": 33,
    "authors": [
      "Yuxuan Lai",
      "Yijia Liu",
      "Yansong Feng",
      "Songfang Huang",
      "Dongyan Zhao"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.138": {
    "title": "Modeling Event Plausibility with Consistent Conceptual Abstraction",
    "volume": "main",
    "abstract": "Understanding natural language requires common sense, one aspect of which is the ability to discern the plausibility of events. While distributional models—most recently pre-trained, Transformer language models—have demonstrated improvements in modeling event plausibility, their performance still falls short of humans'. In this work, we show that Transformer-based plausibility models are markedly inconsistent across the conceptual classes of a lexical hierarchy, inferring that \"a person breathing\" is plausible while \"a dentist breathing\" is not, for example. We find this inconsistency persists even when models are softly injected with lexical knowledge, and we present a simple post-hoc method of forcing model consistency that improves correlation with human plausibility judgements",
    "checked": true,
    "id": "826ff1912a768386662a446176aad7ed424b9086",
    "semantic_title": "modeling event plausibility with consistent conceptual abstraction",
    "citation_count": 14,
    "authors": [
      "Ian Porada",
      "Kaheer Suleman",
      "Adam Trischler",
      "Jackie Chi Kit Cheung"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.139": {
    "title": "UmlsBERT: Clinical Domain Knowledge Augmentation of Contextual Embeddings Using the Unified Medical Language System Metathesaurus",
    "volume": "main",
    "abstract": "Contextual word embedding models, such as BioBERT and Bio_ClinicalBERT, have achieved state-of-the-art results in biomedical natural language processing tasks by focusing their pre-training process on domain-specific corpora. However, such models do not take into consideration structured expert domain knowledge from a knowledge base. We introduce UmlsBERT, a contextual embedding model that integrates domain knowledge during the pre-training process via a novel knowledge augmentation strategy. More specifically, the augmentation on UmlsBERT with the Unified Medical Language System (UMLS) Metathesaurus is performed in two ways: i) connecting words that have the same underlying ‘concept' in UMLS and ii) leveraging semantic type knowledge in UMLS to create clinically meaningful input embeddings. By applying these two strategies, UmlsBERT can encode clinical domain knowledge into word embeddings and outperform existing domain-specific models on common named-entity recognition (NER) and clinical natural language inference tasks",
    "checked": true,
    "id": "03935e520c612ac9f137d9e9ef388e0c08568b60",
    "semantic_title": "umlsbert: clinical domain knowledge augmentation of contextual embeddings using the unified medical language system metathesaurus",
    "citation_count": 109,
    "authors": [
      "George Michalopoulos",
      "Yuanxin Wang",
      "Hussam Kaka",
      "Helen Chen",
      "Alexander Wong"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.140": {
    "title": "Field Embedding: A Unified Grain-Based Framework for Word Representation",
    "volume": "main",
    "abstract": "Word representations empowered with additional linguistic information have been widely studied and proved to outperform traditional embeddings. Current methods mainly focus on learning embeddings for words while embeddings of linguistic information (referred to as grain embeddings) are discarded after the learning. This work proposes a framework field embedding to jointly learn both word and grain embeddings by incorporating morphological, phonetic, and syntactical linguistic fields. The framework leverages an innovative fine-grained pipeline that integrates multiple linguistic fields and produces high-quality grain sequences for learning supreme word representations. A novel algorithm is also designed to learn embeddings for words and grains by capturing information that is contained within each field and that is shared across them. Experimental results of lexical tasks and downstream natural language processing tasks illustrate that our framework can learn better word embeddings and grain embeddings. Qualitative evaluations show grain embeddings effectively capture the semantic information",
    "checked": true,
    "id": "50e5eacd61cbda15924ce26365efe327462aa4f5",
    "semantic_title": "field embedding: a unified grain-based framework for word representation",
    "citation_count": 1,
    "authors": [
      "Junjie Luo",
      "Xi Chen",
      "Jichao Sun",
      "Yuejia Xiang",
      "Ningyu Zhang",
      "Xiang Wan"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.141": {
    "title": "MelBERT: Metaphor Detection via Contextualized Late Interaction using Metaphorical Identification Theories",
    "volume": "main",
    "abstract": "Automated metaphor detection is a challenging task to identify the metaphorical expression of words in a sentence. To tackle this problem, we adopt pre-trained contextualized models, e.g., BERT and RoBERTa. To this end, we propose a novel metaphor detection model, namely metaphor-aware late interaction over BERT (MelBERT). Our model not only leverages contextualized word representation but also benefits from linguistic metaphor identification theories to detect whether the target word is metaphorical. Our empirical results demonstrate that MelBERT outperforms several strong baselines on four benchmark datasets, i.e., VUA-18, VUA-20, MOH-X, and TroFi",
    "checked": true,
    "id": "ff264e7831aca07b69bc02fd667d03bac1751b39",
    "semantic_title": "melbert: metaphor detection via contextualized late interaction using metaphorical identification theories",
    "citation_count": 79,
    "authors": [
      "Minjin Choi",
      "Sunkyung Lee",
      "Eunseong Choi",
      "Heesoo Park",
      "Junhyuk Lee",
      "Dongwon Lee",
      "Jongwuk Lee"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.142": {
    "title": "Non-Parametric Few-Shot Learning for Word Sense Disambiguation",
    "volume": "main",
    "abstract": "Word sense disambiguation (WSD) is a long-standing problem in natural language processing. One significant challenge in supervised all-words WSD is to classify among senses for a majority of words that lie in the long-tail distribution. For instance, 84% of the annotated words have less than 10 examples in the SemCor training data. This issue is more pronounced as the imbalance occurs in both word and sense distributions. In this work, we propose MetricWSD, a non-parametric few-shot learning approach to mitigate this data imbalance issue. By learning to compute distances among the senses of a given word through episodic training, MetricWSD transfers knowledge (a learned metric space) from high-frequency words to infrequent ones. MetricWSD constructs the training episodes tailored to word frequencies and explicitly addresses the problem of the skewed distribution, as opposed to mixing all the words trained with parametric models in previous work. Without resorting to any lexical resources, MetricWSD obtains strong performance against parametric alternatives, achieving a 75.1 F1 score on the unified WSD evaluation benchmark (Raganato et al., 2017b). Our analysis further validates that infrequent words and senses enjoy significant improvement",
    "checked": true,
    "id": "591a63c6f56b06aa7498592f6083c032fa77e2da",
    "semantic_title": "non-parametric few-shot learning for word sense disambiguation",
    "citation_count": 13,
    "authors": [
      "Howard Chen",
      "Mengzhou Xia",
      "Danqi Chen"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.143": {
    "title": "Why Do Document-Level Polarity Classifiers Fail?",
    "volume": "main",
    "abstract": "Machine learning solutions are often criticized for the lack of explanation of their successes and failures. Understanding which instances are misclassified and why is essential to improve the learning process. This work helps to fill this gap by proposing a methodology to characterize, quantify and measure the impact of hard instances in the task of polarity classification of movie reviews. We characterize such instances into two categories: neutrality, where the text does not convey a clear polarity, and discrepancy, where the polarity of the text is the opposite of its true rating. We quantify the number of hard instances in polarity classification of movie reviews and provide empirical evidence about the need to pay attention to such problematic instances, as they are much harder to classify, for both machine and human classifiers. To the best of our knowledge, this is the first systematic analysis of the impact of hard instances in polarity detection from well-formed textual reviews",
    "checked": true,
    "id": "aea74e30391abe22c43042f212c7891fcc7cf456",
    "semantic_title": "why do document-level polarity classifiers fail?",
    "citation_count": 4,
    "authors": [
      "Karen Martins",
      "Pedro O.S Vaz-de-Melo",
      "Rodrygo Santos"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.144": {
    "title": "A Unified Span-Based Approach for Opinion Mining with Syntactic Constituents",
    "volume": "main",
    "abstract": "Fine-grained opinion mining (OM) has achieved increasing attraction in the natural language processing (NLP) community, which aims to find the opinion structures of \"Who expressed what opinions towards what\" in one sentence. In this work, motivated by its span-based representations of opinion expressions and roles, we propose a unified span-based approach for the end-to-end OM setting. Furthermore, inspired by the unified span-based formalism of OM and constituent parsing, we explore two different methods (multi-task learning and graph convolutional neural network) to integrate syntactic constituents into the proposed model to help OM. We conduct experiments on the commonly used MPQA 2.0 dataset. The experimental results show that our proposed unified span-based approach achieves significant improvements over previous works in the exact F1 score and reduces the number of wrongly-predicted opinion expressions and roles, showing the effectiveness of our method. In addition, incorporating the syntactic constituents achieves promising improvements over the strong baseline enhanced by contextualized word representations",
    "checked": true,
    "id": "5c1ead68c2053669a4d38dc39728a93ec34338df",
    "semantic_title": "a unified span-based approach for opinion mining with syntactic constituents",
    "citation_count": 16,
    "authors": [
      "Qingrong Xia",
      "Bo Zhang",
      "Rui Wang",
      "Zhenghua Li",
      "Yue Zhang",
      "Fei Huang",
      "Luo Si",
      "Min Zhang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.145": {
    "title": "Target-specified Sequence Labeling with Multi-head Self-attention for Target-oriented Opinion Words Extraction",
    "volume": "main",
    "abstract": "Opinion target extraction and opinion term extraction are two fundamental tasks in Aspect Based Sentiment Analysis (ABSA). Many recent works on ABSA focus on Target-oriented Opinion Words (or Terms) Extraction (TOWE), which aims at extracting the corresponding opinion words for a given opinion target. TOWE can be further applied to Aspect-Opinion Pair Extraction (AOPE) which aims at extracting aspects (i.e., opinion targets) and opinion terms in pairs. In this paper, we propose Target-Specified sequence labeling with Multi-head Self-Attention (TSMSA) for TOWE, in which any pre-trained language model with multi-head self-attention can be integrated conveniently. As a case study, we also develop a Multi-Task structure named MT-TSMSA for AOPE by combining our TSMSA with an aspect and opinion term extraction module. Experimental results indicate that TSMSA outperforms the benchmark methods on TOWE significantly; meanwhile, the performance of MT-TSMSA is similar or even better than state-of-the-art AOPE baseline models",
    "checked": true,
    "id": "da92fa7e9e9dc9460391cd623f571e0d446910c0",
    "semantic_title": "target-specified sequence labeling with multi-head self-attention for target-oriented opinion words extraction",
    "citation_count": 12,
    "authors": [
      "Yuhao Feng",
      "Yanghui Rao",
      "Yuyao Tang",
      "Ninghua Wang",
      "He Liu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.146": {
    "title": "Does syntax matter? A strong baseline for Aspect-based Sentiment Analysis with RoBERTa",
    "volume": "main",
    "abstract": "Aspect-based Sentiment Analysis (ABSA), aiming at predicting the polarities for aspects, is a fine-grained task in the field of sentiment analysis. Previous work showed syntactic information, e.g. dependency trees, can effectively improve the ABSA performance. Recently, pre-trained models (PTMs) also have shown their effectiveness on ABSA. Therefore, the question naturally arises whether PTMs contain sufficient syntactic information for ABSA so that we can obtain a good ABSA model only based on PTMs. In this paper, we firstly compare the induced trees from PTMs and the dependency parsing trees on several popular models for the ABSA task, showing that the induced tree from fine-tuned RoBERTa (FT-RoBERTa) outperforms the parser-provided tree. The further analysis experiments reveal that the FT-RoBERTa Induced Tree is more sentiment-word-oriented and could benefit the ABSA task. The experiments also show that the pure RoBERTa-based model can outperform or approximate to the previous SOTA performances on six datasets across four languages since it implicitly incorporates the task-oriented syntactic information",
    "checked": true,
    "id": "0b7b605a2d6e0cb1e53d1fa03311278e95f99260",
    "semantic_title": "does syntax matter? a strong baseline for aspect-based sentiment analysis with roberta",
    "citation_count": 141,
    "authors": [
      "Junqi Dai",
      "Hang Yan",
      "Tianxiang Sun",
      "Pengfei Liu",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.147": {
    "title": "Domain Divergences: A Survey and Empirical Analysis",
    "volume": "main",
    "abstract": "Domain divergence plays a significant role in estimating the performance of a model in new domains. While there is a significant literature on divergence measures, researchers find it hard to choose an appropriate divergence for a given NLP application. We address this shortcoming by both surveying the literature and through an empirical study. We develop a taxonomy of divergence measures consisting of three classes — Information-theoretic, Geometric, and Higher-order measures and identify the relationships between them. Further, to understand the common use-cases of these measures, we recognise three novel applications – 1) Data Selection, 2) Learning Representation, and 3) Decisions in the Wild – and use it to organise our literature. From this, we identify that Information-theoretic measures are prevalent for 1) and 3), and Higher-order measures are more common for 2). To further help researchers choose appropriate measures to predict drop in performance – an important aspect of Decisions in the Wild, we perform correlation analysis spanning 130 domain adaptation scenarios, 3 varied NLP tasks and 12 divergence measures identified from our survey. To calculate these divergences, we consider the current contextual word representations (CWR) and contrast with the older distributed representations. We find that traditional measures over word distributions still serve as strong baselines, while higher-order measures with CWR are effective",
    "checked": true,
    "id": "95cb2118e74e68727d2b9429606ff95afe09a7bd",
    "semantic_title": "domain divergences: a survey and empirical analysis",
    "citation_count": 34,
    "authors": [
      "Abhinav Ramesh Kashyap",
      "Devamanyu Hazarika",
      "Min-Yen Kan",
      "Roger Zimmermann"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.148": {
    "title": "Target-Aware Data Augmentation for Stance Detection",
    "volume": "main",
    "abstract": "The goal of stance detection is to identify whether the author of a text is in favor of, neutral or against a specific target. Despite substantial progress on this task, one of the remaining challenges is the scarcity of annotations. Data augmentation is commonly used to address annotation scarcity by generating more training samples. However, the augmented sentences that are generated by existing methods are either less diversified or inconsistent with the given target and stance label. In this paper, we formulate the data augmentation of stance detection as a conditional masked language modeling task and augment the dataset by predicting the masked word conditioned on both its context and the auxiliary sentence that contains target and label information. Moreover, we propose another simple yet effective method that generates target-aware sentence by replacing a target mention with the other. Experimental results show that our proposed methods significantly outperforms previous augmentation methods on 11 targets",
    "checked": true,
    "id": "8cfc53e9b4e197a6289e36470543846b64cc038c",
    "semantic_title": "target-aware data augmentation for stance detection",
    "citation_count": 35,
    "authors": [
      "Yingjie Li",
      "Cornelia Caragea"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.149": {
    "title": "End-to-end ASR to jointly predict transcriptions and linguistic annotations",
    "volume": "main",
    "abstract": "We propose a Transformer-based sequence-to-sequence model for automatic speech recognition (ASR) capable of simultaneously transcribing and annotating audio with linguistic information such as phonemic transcripts or part-of-speech (POS) tags. Since linguistic information is important in natural language processing (NLP), the proposed ASR is especially useful for speech interface applications, including spoken dialogue systems and speech translation, which combine ASR and NLP. To produce linguistic annotations, we train the ASR system using modified training targets: each grapheme or multi-grapheme unit in the target transcript is followed by an aligned phoneme sequence and/or POS tag. Since our method has access to the underlying audio data, we can estimate linguistic annotations more accurately than pipeline approaches in which NLP-based methods are applied to a hypothesized ASR transcript. Experimental results on Japanese and English datasets show that the proposed ASR system is capable of simultaneously producing high-quality transcriptions and linguistic annotations",
    "checked": true,
    "id": "ec04862dedaa8f234b66852a7cae02f6da29ea80",
    "semantic_title": "end-to-end asr to jointly predict transcriptions and linguistic annotations",
    "citation_count": 11,
    "authors": [
      "Motoi Omachi",
      "Yuya Fujita",
      "Shinji Watanabe",
      "Matthew Wiesner"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.150": {
    "title": "Source and Target Bidirectional Knowledge Distillation for End-to-end Speech Translation",
    "volume": "main",
    "abstract": "A conventional approach to improving the performance of end-to-end speech translation (E2E-ST) models is to leverage the source transcription via pre-training and joint training with automatic speech recognition (ASR) and neural machine translation (NMT) tasks. However, since the input modalities are different, it is difficult to leverage source language text successfully. In this work, we focus on sequence-level knowledge distillation (SeqKD) from external text-based NMT models. To leverage the full potential of the source language information, we propose backward SeqKD, SeqKD from a target-to-source backward NMT model. To this end, we train a bilingual E2E-ST model to predict paraphrased transcriptions as an auxiliary task with a single decoder. The paraphrases are generated from the translations in bitext via back-translation. We further propose bidirectional SeqKD in which SeqKD from both forward and backward NMT models is combined. Experimental evaluations on both autoregressive and non-autoregressive models show that SeqKD in each direction consistently improves the translation performance, and the effectiveness is complementary regardless of the model capacity",
    "checked": true,
    "id": "89a8edbc0fe2ea8b9ee703ca37e5d5d6d34c571a",
    "semantic_title": "source and target bidirectional knowledge distillation for end-to-end speech translation",
    "citation_count": 36,
    "authors": [
      "Hirofumi Inaguma",
      "Tatsuya Kawahara",
      "Shinji Watanabe"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.151": {
    "title": "Searchable Hidden Intermediates for End-to-End Models of Decomposable Sequence Tasks",
    "volume": "main",
    "abstract": "End-to-end approaches for sequence tasks are becoming increasingly popular. Yet for complex sequence tasks, like speech translation, systems that cascade several models trained on sub-tasks have shown to be superior, suggesting that the compositionality of cascaded systems simplifies learning and enables sophisticated search capabilities. In this work, we present an end-to-end framework that exploits compositionality to learn searchable hidden representations at intermediate stages of a sequence model using decomposed sub-tasks. These hidden intermediates can be improved using beam search to enhance the overall performance and can also incorporate external models at intermediate stages of the network to re-score or adapt towards out-of-domain data. One instance of the proposed framework is a Multi-Decoder model for speech translation that extracts the searchable hidden intermediates from a speech recognition sub-task. The model demonstrates the aforementioned benefits and outperforms the previous state-of-the-art by around +6 and +3 BLEU on the two test sets of Fisher-CallHome and by around +3 and +4 BLEU on the English-German and English-French test sets of MuST-C",
    "checked": true,
    "id": "ca3535dcdda9849350ad7c991a60660b22844f2f",
    "semantic_title": "searchable hidden intermediates for end-to-end models of decomposable sequence tasks",
    "citation_count": 29,
    "authors": [
      "Siddharth Dalmia",
      "Brian Yan",
      "Vikas Raunak",
      "Florian Metze",
      "Shinji Watanabe"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.152": {
    "title": "SPLAT: Speech-Language Joint Pre-Training for Spoken Language Understanding",
    "volume": "main",
    "abstract": "Spoken language understanding (SLU) requires a model to analyze input acoustic signal to understand its linguistic content and make predictions. To boost the models' performance, various pre-training methods have been proposed to learn rich representations from large-scale unannotated speech and text. However, the inherent disparities between the two modalities necessitate a mutual analysis. In this paper, we propose a novel semi-supervised learning framework, SPLAT, to jointly pre-train the speech and language modules. Besides conducting a self-supervised masked language modeling task on the two individual modules using unpaired speech and text, SPLAT aligns representations from the two modules in a shared latent space using a small amount of paired speech and text. Thus, during fine-tuning, the speech module alone can produce representations carrying both acoustic information and contextual semantic knowledge of an input acoustic signal. Experimental results verify the effectiveness of our approach on various SLU tasks. For example, SPLAT improves the previous state-of-the-art performance on the Spoken SQuAD dataset by more than 10%",
    "checked": true,
    "id": "8eda64d93e4d1ce5aef1405f99d15004db9028c5",
    "semantic_title": "splat: speech-language joint pre-training for spoken language understanding",
    "citation_count": 63,
    "authors": [
      "Yu-An Chung",
      "Chenguang Zhu",
      "Michael Zeng"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.153": {
    "title": "Worldly Wise (WoW) - Cross-Lingual Knowledge Fusion for Fact-based Visual Spoken-Question Answering",
    "volume": "main",
    "abstract": "Although Question-Answering has long been of research interest, its accessibility to users through a speech interface and its support to multiple languages have not been addressed in prior studies. Towards these ends, we present a new task and a synthetically-generated dataset to do Fact-based Visual Spoken-Question Answering (FVSQA). FVSQA is based on the FVQA dataset, which requires a system to retrieve an entity from Knowledge Graphs (KGs) to answer a question about an image. In FVSQA, the question is spoken rather than typed. Three sub-tasks are proposed: (1) speech-to-text based, (2) end-to-end, without speech-to-text as an intermediate component, and (3) cross-lingual, in which the question is spoken in a language different from that in which the KG is recorded. The end-to-end and cross-lingual tasks are the first to require world knowledge from a multi-relational KG as a differentiable layer in an end-to-end spoken language understanding task, hence the proposed reference implementation is called Worldly-Wise (WoW).WoW is shown to perform end-to-end cross-lingual FVSQA at same levels of accuracy across 3 languages - English, Hindi, and Turkish",
    "checked": true,
    "id": "1826691df2f1ff87a12f944f6ef5583c9a6ceef4",
    "semantic_title": "worldly wise (wow) - cross-lingual knowledge fusion for fact-based visual spoken-question answering",
    "citation_count": 6,
    "authors": [
      "Kiran Ramnath",
      "Leda Sari",
      "Mark Hasegawa-Johnson",
      "Chang Yoo"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.154": {
    "title": "Align-Refine: Non-Autoregressive Speech Recognition via Iterative Realignment",
    "volume": "main",
    "abstract": "Non-autoregressive encoder-decoder models greatly improve decoding speed over autoregressive models, at the expense of generation quality. To mitigate this, iterative decoding models repeatedly infill or refine the proposal of a non-autoregressive model. However, editing at the level of output sequences limits model flexibility. We instead propose *iterative realignment*, which by refining latent alignments allows more flexible edits in fewer steps. Our model, Align-Refine, is an end-to-end Transformer which iteratively realigns connectionist temporal classification (CTC) alignments. On the WSJ dataset, Align-Refine matches an autoregressive baseline with a 14x decoding speedup; on LibriSpeech, we reach an LM-free test-other WER of 9.0% (19% relative improvement on comparable work) in three iterations. We release our code at https://github.com/amazon-research/align-refine",
    "checked": true,
    "id": "533a8eaa4a224d0392011575c47f0e8ba68e3fed",
    "semantic_title": "align-refine: non-autoregressive speech recognition via iterative realignment",
    "citation_count": 47,
    "authors": [
      "Ethan A. Chi",
      "Julian Salazar",
      "Katrin Kirchhoff"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.155": {
    "title": "Everything Has a Cause: Leveraging Causal Inference in Legal Text Analysis",
    "volume": "main",
    "abstract": "Causal inference is the process of capturing cause-effect relationship among variables. Most existing works focus on dealing with structured data, while mining causal relationship among factors from unstructured data, like text, has been less examined, but is of great importance, especially in the legal domain. In this paper, we propose a novel Graph-based Causal Inference (GCI) framework, which builds causal graphs from fact descriptions without much human involvement and enables causal inference to facilitate legal practitioners to make proper decisions. We evaluate the framework on a challenging similar charge disambiguation task. Experimental results show that GCI can capture the nuance from fact descriptions among multiple confusing charges and provide explainable discrimination, especially in few-shot settings. We also observe that the causal knowledge contained in GCI can be effectively injected into powerful neural networks for better performance and interpretability",
    "checked": true,
    "id": "1a80bd78b5a5238b880794cb2fb0d261c042741f",
    "semantic_title": "everything has a cause: leveraging causal inference in legal text analysis",
    "citation_count": 26,
    "authors": [
      "Xiao Liu",
      "Da Yin",
      "Yansong Feng",
      "Yuting Wu",
      "Dongyan Zhao"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.156": {
    "title": "Counterfactual Supporting Facts Extraction for Explainable Medical Record Based Diagnosis with Graph Network",
    "volume": "main",
    "abstract": "Providing a reliable explanation for clinical diagnosis based on the Electronic Medical Record (EMR) is fundamental to the application of Artificial Intelligence in the medical field. Current methods mostly treat the EMR as a text sequence and provide explanations based on a precise medical knowledge base, which is disease-specific and difficult to obtain for experts in reality. Therefore, we propose a counterfactual multi-granularity graph supporting facts extraction (CMGE) method to extract supporting facts from irregular EMR itself without external knowledge bases in this paper. Specifically, we first structure the sequence of EMR into a hierarchical graph network and then obtain the causal relationship between multi-granularity features and diagnosis results through counterfactual intervention on the graph. Features having the strongest causal connection with the results provide interpretive support for the diagnosis. Experimental results on real Chinese EMR of the lymphedema demonstrate that our method can diagnose four types of EMR correctly, and can provide accurate supporting facts for the results. More importantly, the results on different diseases demonstrate the robustness of our approach, which represents the potential application in the medical field",
    "checked": true,
    "id": "77b43ef117701b2f211c8255899bc2180478db2d",
    "semantic_title": "counterfactual supporting facts extraction for explainable medical record based diagnosis with graph network",
    "citation_count": 23,
    "authors": [
      "Haoran Wu",
      "Wei Chen",
      "Shuang Xu",
      "Bo Xu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.157": {
    "title": "Personalized Response Generation via Generative Split Memory Network",
    "volume": "main",
    "abstract": "Despite the impressive successes of generation and dialogue systems, how to endow a text generation system with particular personality traits to deliver more personalized responses remains under-investigated. In this work, we look at how to generate personalized responses for questions on Reddit by utilizing personalized user profiles and posting histories. Specifically, we release an open-domain single-turn dialog dataset made up of 1.5M conversation pairs together with 300k profiles of users and related comments. We then propose a memory network to generate personalized responses in dialogue that utilizes a novel mechanism of splitting memories: one for user profile meta attributes and the other for user-generated information like comment histories. Experimental results show the quantitative and qualitative improvements of our simple split memory network model over the state-of-the-art response generation baselines",
    "checked": true,
    "id": "a27ef1cae80d9a72734868b7a5d4e5092452dc31",
    "semantic_title": "personalized response generation via generative split memory network",
    "citation_count": 45,
    "authors": [
      "Yuwei Wu",
      "Xuezhe Ma",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.158": {
    "title": "Towards Few-shot Fact-Checking via Perplexity",
    "volume": "main",
    "abstract": "Few-shot learning has drawn researchers' attention to overcome the problem of data scarcity. Recently, large pre-trained language models have shown great performance in few-shot learning for various downstream tasks, such as question answering and machine translation. Nevertheless, little exploration has been made to achieve few-shot learning for the fact-checking task. However, fact-checking is an important problem, especially when the amount of information online is growing exponentially every day. In this paper, we propose a new way of utilizing the powerful transfer learning ability of a language model via a perplexity score. The most notable strength of our methodology lies in its capability in few-shot learning. With only two training samples, our methodology can already outperform the Major Class baseline by more than an absolute 10% on the F1-Macro metric across multiple datasets. Through experiments, we empirically verify the plausibility of the rather surprising usage of the perplexity score in the context of fact-checking and highlight the strength of our few-shot methodology by comparing it to strong fine-tuning-based baseline models. Moreover, we construct and publicly release two new fact-checking datasets related to COVID-19",
    "checked": true,
    "id": "b798969c3e78215a87e8f3da510232b40893bd9c",
    "semantic_title": "towards few-shot fact-checking via perplexity",
    "citation_count": 74,
    "authors": [
      "Nayeon Lee",
      "Yejin Bang",
      "Andrea Madotto",
      "Pascale Fung"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.159": {
    "title": "Active2 Learning: Actively reducing redundancies in Active Learning methods for Sequence Tagging and Machine Translation",
    "volume": "main",
    "abstract": "While deep learning is a powerful tool for natural language processing (NLP) problems, successful solutions to these problems rely heavily on large amounts of annotated samples. However, manually annotating data is expensive and time-consuming. Active Learning (AL) strategies reduce the need for huge volumes of labeled data by iteratively selecting a small number of examples for manual annotation based on their estimated utility in training the given model. In this paper, we argue that since AL strategies choose examples independently, they may potentially select similar examples, all of which may not contribute significantly to the learning process. Our proposed approach, Active2 Learning (A2L), actively adapts to the deep learning model being trained to eliminate such redundant examples chosen by an AL strategy. We show that A2L is widely applicable by using it in conjunction with several different AL strategies and NLP tasks. We empirically demonstrate that the proposed approach is further able to reduce the data requirements of state-of-the-art AL strategies by ≈ 3-25% on an absolute scale on multiple NLP tasks while achieving the same performance with virtually no additional computation overhead",
    "checked": true,
    "id": "957f4253a2e836cb4f2b075ed98584aed7039ebf",
    "semantic_title": "active2 learning: actively reducing redundancies in active learning methods for sequence tagging and machine translation",
    "citation_count": 5,
    "authors": [
      "Rishi Hazra",
      "Parag Dutta",
      "Shubham Gupta",
      "Mohammed Abdul Qaathir",
      "Ambedkar Dukkipati"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.160": {
    "title": "Generating An Optimal Interview Question Plan Using A Knowledge Graph And Integer Linear Programming",
    "volume": "main",
    "abstract": "Given the diversity of the candidates and complexity of job requirements, and since interviewing is an inherently subjective process, it is an important task to ensure consistent, uniform, efficient and objective interviews that result in high quality recruitment. We propose an interview assistant system to automatically, and in an objective manner, select an optimal set of technical questions (from question banks) personalized for a candidate. This set can help a human interviewer to plan for an upcoming interview of that candidate. We formalize the problem of selecting a set of questions as an integer linear programming problem and use standard solvers to get a solution. We use knowledge graph as background knowledge in this formulation, and derive our objective functions and constraints from it. We use candidate's resume to personalize the selection of questions. We propose an intrinsic evaluation to compare a set of suggested questions with actually asked questions. We also use expert interviewers to comparatively evaluate our approach with a set of reasonable baselines",
    "checked": true,
    "id": "b28bb8308d8470d3a5fc00716cb9263dbee1893e",
    "semantic_title": "generating an optimal interview question plan using a knowledge graph and integer linear programming",
    "citation_count": 4,
    "authors": [
      "Soham Datta",
      "Prabir Mallick",
      "Sangameshwar Patil",
      "Indrajit Bhattacharya",
      "Girish Palshikar"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.161": {
    "title": "Model Extraction and Adversarial Transferability, Your BERT is Vulnerable!",
    "volume": "main",
    "abstract": "Natural language processing (NLP) tasks, ranging from text classification to text generation, have been revolutionised by the pretrained language models, such as BERT. This allows corporations to easily build powerful APIs by encapsulating fine-tuned BERT models for downstream tasks. However, when a fine-tuned BERT model is deployed as a service, it may suffer from different attacks launched by the malicious users. In this work, we first present how an adversary can steal a BERT-based API service (the victim/target model) on multiple benchmark datasets with limited prior knowledge and queries. We further show that the extracted model can lead to highly transferable adversarial attacks against the victim model. Our studies indicate that the potential vulnerabilities of BERT-based API services still hold, even when there is an architectural mismatch between the victim model and the attack model. Finally, we investigate two defence strategies to protect the victim model, and find that unless the performance of the victim model is sacrificed, both model extraction and adversarial transferability can effectively compromise the target models",
    "checked": true,
    "id": "16a8e329c06b4c6f61762da7fa77a84bf3e12dca",
    "semantic_title": "model extraction and adversarial transferability, your bert is vulnerable!",
    "citation_count": 76,
    "authors": [
      "Xuanli He",
      "Lingjuan Lyu",
      "Lichao Sun",
      "Qiongkai Xu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.162": {
    "title": "A Global Past-Future Early Exit Method for Accelerating Inference of Pre-trained Language Models",
    "volume": "main",
    "abstract": "Early exit mechanism aims to accelerate the inference speed of large-scale pre-trained language models. The essential idea is to exit early without passing through all the inference layers at the inference stage. To make accurate predictions for downstream tasks, the hierarchical linguistic information embedded in all layers should be jointly considered. However, much of the research up to now has been limited to use local representations of the exit layer. Such treatment inevitably loses information of the unused past layers as well as the high-level features embedded in future layers, leading to sub-optimal performance. To address this issue, we propose a novel Past-Future method to make comprehensive predictions from a global perspective. We first take into consideration all the linguistic information embedded in the past layers and then take a further step to engage the future information which is originally inaccessible for predictions. Extensive experiments demonstrate that our method outperforms previous early exit methods by a large margin, yielding better and robust performance",
    "checked": true,
    "id": "03662672662f49e6b06148e94b407b60b0bb72f3",
    "semantic_title": "a global past-future early exit method for accelerating inference of pre-trained language models",
    "citation_count": 31,
    "authors": [
      "Kaiyuan Liao",
      "Yi Zhang",
      "Xuancheng Ren",
      "Qi Su",
      "Xu Sun",
      "Bin He"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.163": {
    "title": "Masked Conditional Random Fields for Sequence Labeling",
    "volume": "main",
    "abstract": "Conditional Random Field (CRF) based neural models are among the most performant methods for solving sequence labeling problems. Despite its great success, CRF has the shortcoming of occasionally generating illegal sequences of tags, e.g. sequences containing an \"I-\" tag immediately after an \"O\" tag, which is forbidden by the underlying BIO tagging scheme. In this work, we propose Masked Conditional Random Field (MCRF), an easy to implement variant of CRF that impose restrictions on candidate paths during both training and decoding phases. We show that the proposed method thoroughly resolves this issue and brings significant improvement over existing CRF-based models with near zero additional cost",
    "checked": true,
    "id": "8e9a88d991caf44b8af26234454db5d10ad434b9",
    "semantic_title": "masked conditional random fields for sequence labeling",
    "citation_count": 13,
    "authors": [
      "Tianwen Wei",
      "Jianwei Qi",
      "Shenghuan He",
      "Songtao Sun"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.164": {
    "title": "Heterogeneous Graph Neural Networks for Concept Prerequisite Relation Learning in Educational Data",
    "volume": "main",
    "abstract": "Prerequisite relations among concepts are crucial for educational applications, such as curriculum planning and intelligent tutoring. In this paper, we propose a novel concept prerequisite relation learning approach, named CPRL, which combines both concept representation learned from a heterogeneous graph and concept pairwise features. Furthermore, we extend CPRL under weakly supervised settings to make our method more practical, including learning prerequisite relations from learning object dependencies and generating training data with data programming. Our experiments on four datasets show that the proposed approach achieves the state-of-the-art results comparing with existing methods",
    "checked": true,
    "id": "9f496f6c951819bb2a0931db8088b9c6d18a4f46",
    "semantic_title": "heterogeneous graph neural networks for concept prerequisite relation learning in educational data",
    "citation_count": 12,
    "authors": [
      "Chenghao Jia",
      "Yongliang Shen",
      "Yechun Tang",
      "Lu Sun",
      "Weiming Lu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.165": {
    "title": "Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models",
    "volume": "main",
    "abstract": "Recent studies have revealed a security threat to natural language processing (NLP) models, called the Backdoor Attack. Victim models can maintain competitive performance on clean samples while behaving abnormally on samples with a specific trigger word inserted. Previous backdoor attacking methods usually assume that attackers have a certain degree of data knowledge, either the dataset which users would use or proxy datasets for a similar task, for implementing the data poisoning procedure. However, in this paper, we find that it is possible to hack the model in a data-free way by modifying one single word embedding vector, with almost no accuracy sacrificed on clean samples. Experimental results on sentiment analysis and sentence-pair classification tasks show that our method is more efficient and stealthier. We hope this work can raise the awareness of such a critical security risk hidden in the embedding layers of NLP models. Our code is available at https://github.com/lancopku/Embedding-Poisoning",
    "checked": true,
    "id": "5e31fa4e69d1a3587230f5d134c0b7e2ed84a742",
    "semantic_title": "be careful about poisoned word embeddings: exploring the vulnerability of the embedding layers in nlp models",
    "citation_count": 121,
    "authors": [
      "Wenkai Yang",
      "Lei Li",
      "Zhiyuan Zhang",
      "Xuancheng Ren",
      "Xu Sun",
      "Bin He"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.166": {
    "title": "DA-Transformer: Distance-aware Transformer",
    "volume": "main",
    "abstract": "Transformer has achieved great success in the NLP field by composing various advanced models like BERT and GPT. However, Transformer and its existing variants may not be optimal in capturing token distances because the position or distance embeddings used by these methods usually cannot keep the precise information of real distances, which may not be beneficial for modeling the orders and relations of contexts. In this paper, we propose DA-Transformer, which is a distance-aware Transformer that can exploit the real distance. We propose to incorporate the real distances between tokens to re-scale the raw self-attention weights, which are computed by the relevance between attention query and key. Concretely, in different self-attention heads the relative distance between each pair of tokens is weighted by different learnable parameters, which control the different preferences on long- or short-term information of these heads. Since the raw weighted real distances may not be optimal for adjusting self-attention weights, we propose a learnable sigmoid function to map them into re-scaled coefficients that have proper ranges. We first clip the raw self-attention weights via the ReLU function to keep non-negativity and introduce sparsity, and then multiply them with the re-scaled coefficients to encode real distance information into self-attention. Extensive experiments on five benchmark datasets show that DA-Transformer can effectively improve the performance of many tasks and outperform the vanilla Transformer and its several variants",
    "checked": true,
    "id": "d8342a02f1c03023b8298b2047cb34d4f771cc30",
    "semantic_title": "da-transformer: distance-aware transformer",
    "citation_count": 31,
    "authors": [
      "Chuhan Wu",
      "Fangzhao Wu",
      "Yongfeng Huang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.167": {
    "title": "ASAP: A Chinese Review Dataset Towards Aspect Category Sentiment Analysis and Rating Prediction",
    "volume": "main",
    "abstract": "Sentiment analysis has attracted increasing attention in e-commerce. The sentiment polarities underlying user reviews are of great value for business intelligence. Aspect category sentiment analysis (ACSA) and review rating prediction (RP) are two essential tasks to detect the fine-to-coarse sentiment polarities. ACSA and RP are highly correlated and usually employed jointly in real-world e-commerce scenarios. While most public datasets are constructed for ACSA and RP separately, which may limit the further exploitation of both tasks. To address the problem and advance related researches, we present a large-scale Chinese restaurant review dataset ASAP including 46, 730 genuine reviews from a leading online-to-offline (O2O) e-commerce platform in China. Besides a 5-star scale rating, each review is manually annotated according to its sentiment polarities towards 18 pre-defined aspect categories. We hope the release of the dataset could shed some light on the field of sentiment analysis. Moreover, we propose an intuitive yet effective joint model for ACSA and RP. Experimental results demonstrate that the joint model outperforms state-of-the-art baselines on both tasks",
    "checked": true,
    "id": "01d8767bc1bab0990b161f5a7ce093fe6c2e523d",
    "semantic_title": "asap: a chinese review dataset towards aspect category sentiment analysis and rating prediction",
    "citation_count": 49,
    "authors": [
      "Jiahao Bu",
      "Lei Ren",
      "Shuang Zheng",
      "Yang Yang",
      "Jingang Wang",
      "Fuzheng Zhang",
      "Wei Wu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.168": {
    "title": "Are NLP Models really able to Solve Simple Math Word Problems?",
    "volume": "main",
    "abstract": "The problem of designing NLP solvers for math word problems (MWP) has seen sustained research activity and steady gains in the test accuracy. Since existing solvers achieve high performance on the benchmark datasets for elementary level MWPs containing one-unknown arithmetic word problems, such problems are often considered \"solved\" with the bulk of research attention moving to more complex MWPs. In this paper, we restrict our attention to English MWPs taught in grades four and lower. We provide strong evidence that the existing MWP solvers rely on shallow heuristics to achieve high performance on the benchmark datasets. To this end, we show that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs. Similarly, models that treat MWPs as bag-of-words can also achieve surprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP, created by applying carefully chosen variations over examples sampled from existing datasets. The best accuracy achieved by state-of-the-art models is substantially lower on SVAMP, thus showing that much remains to be done even for the simplest of the MWPs",
    "checked": true,
    "id": "13c4e5a6122f3fa2663f63e49537091da6532f35",
    "semantic_title": "are nlp models really able to solve simple math word problems?",
    "citation_count": 388,
    "authors": [
      "Arkil Patel",
      "Satwik Bhattamishra",
      "Navin Goyal"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.169": {
    "title": "WRIME: A New Dataset for Emotional Intensity Estimation with Subjective and Objective Annotations",
    "volume": "main",
    "abstract": "We annotate 17,000 SNS posts with both the writer's subjective emotional intensity and the reader's objective one to construct a Japanese emotion analysis dataset. In this study, we explore the difference between the emotional intensity of the writer and that of the readers with this dataset. We found that the reader cannot fully detect the emotions of the writer, especially anger and trust. In addition, experimental results in estimating the emotional intensity show that it is more difficult to estimate the writer's subjective labels than the readers'. The large gap between the subjective and objective emotions imply the complexity of the mapping from a post to the subjective emotion intensities, which also leads to a lower performance with machine learning models",
    "checked": true,
    "id": "70d73c5c519a7259a52da232a8392616ec283c52",
    "semantic_title": "wrime: a new dataset for emotional intensity estimation with subjective and objective annotations",
    "citation_count": 23,
    "authors": [
      "Tomoyuki Kajiwara",
      "Chenhui Chu",
      "Noriko Takemura",
      "Yuta Nakashima",
      "Hajime Nagahara"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.170": {
    "title": "KPQA: A Metric for Generative Question Answering Using Keyphrase Weights",
    "volume": "main",
    "abstract": "In the automatic evaluation of generative question answering (GenQA) systems, it is difficult to assess the correctness of generated answers due to the free-form of the answer. Especially, widely used n-gram similarity metrics often fail to discriminate the incorrect answers since they equally consider all of the tokens. To alleviate this problem, we propose KPQA metric, a new metric for evaluating the correctness of GenQA. Specifically, our new metric assigns different weights to each token via keyphrase prediction, thereby judging whether a generated answer sentence captures the key meaning of the reference answer. To evaluate our metric, we create high-quality human judgments of correctness on two GenQA datasets. Using our human-evaluation datasets, we show that our proposed metric has a significantly higher correlation with human judgments than existing metrics in various datasets. Code for KPQA-metric will be available at https://github.com/hwanheelee1993/KPQA",
    "checked": true,
    "id": "5f5e7284f9179f0a86ce4f1c9c9f93957e6b387b",
    "semantic_title": "kpqa: a metric for generative question answering using keyphrase weights",
    "citation_count": 11,
    "authors": [
      "Hwanhee Lee",
      "Seunghyun Yoon",
      "Franck Dernoncourt",
      "Doo Soon Kim",
      "Trung Bui",
      "Joongbo Shin",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.171": {
    "title": "StylePTB: A Compositional Benchmark for Fine-grained Controllable Text Style Transfer",
    "volume": "main",
    "abstract": "Text style transfer aims to controllably generate text with targeted stylistic changes while maintaining core meaning from the source sentence constant. Many of the existing style transfer benchmarks primarily focus on individual high-level semantic changes (e.g. positive to negative), which enable controllability at a high level but do not offer fine-grained control involving sentence structure, emphasis, and content of the sentence. In this paper, we introduce a large-scale benchmark, StylePTB, with (1) paired sentences undergoing 21 fine-grained stylistic changes spanning atomic lexical, syntactic, semantic, and thematic transfers of text, as well as (2) compositions of multiple transfers which allow modeling of fine-grained stylistic changes as building blocks for more complex, high-level transfers. By benchmarking existing methods on StylePTB, we find that they struggle to model fine-grained changes and have an even more difficult time composing multiple styles. As a result, StylePTB brings novel challenges that we hope will encourage future research in controllable text style transfer, compositional models, and learning disentangled representations. Solving these challenges would present important steps towards controllable text generation",
    "checked": true,
    "id": "2cedd0a1841c0bad42e7a7a6ed3a9d0ffbb7d979",
    "semantic_title": "styleptb: a compositional benchmark for fine-grained controllable text style transfer",
    "citation_count": 40,
    "authors": [
      "Yiwei Lyu",
      "Paul Pu Liang",
      "Hai Pham",
      "Eduard Hovy",
      "Barnabás Póczos",
      "Ruslan Salakhutdinov",
      "Louis-Philippe Morency"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.172": {
    "title": "Blow the Dog Whistle: A Chinese Dataset for Cant Understanding with Common Sense and World Knowledge",
    "volume": "main",
    "abstract": "Cant is important for understanding advertising, comedies and dog-whistle politics. However, computational research on cant is hindered by a lack of available datasets. In this paper, we propose a large and diverse Chinese dataset for creating and understanding cant from a computational linguistics perspective. We formulate a task for cant understanding and provide both quantitative and qualitative analysis for tested word embedding similarity and pretrained language models. Experiments suggest that such a task requires deep language understanding, common sense, and world knowledge and thus can be a good testbed for pretrained language models and help models perform better on other tasks",
    "checked": true,
    "id": "1e5b05838e16244310db554b04ff6541f05acb0b",
    "semantic_title": "blow the dog whistle: a chinese dataset for cant understanding with common sense and world knowledge",
    "citation_count": 15,
    "authors": [
      "Canwen Xu",
      "Wangchunshu Zhou",
      "Tao Ge",
      "Ke Xu",
      "Julian McAuley",
      "Furu Wei"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.173": {
    "title": "COVID-19 Named Entity Recognition for Vietnamese",
    "volume": "main",
    "abstract": "The current COVID-19 pandemic has lead to the creation of many corpora that facilitate NLP research and downstream applications to help fight the pandemic. However, most of these corpora are exclusively for English. As the pandemic is a global problem, it is worth creating COVID-19 related datasets for languages other than English. In this paper, we present the first manually-annotated COVID-19 domain-specific dataset for Vietnamese. Particularly, our dataset is annotated for the named entity recognition (NER) task with newly-defined entity types that can be used in other future epidemics. Our dataset also contains the largest number of entities compared to existing Vietnamese NER datasets. We empirically conduct experiments using strong baselines on our dataset, and find that: automatic Vietnamese word segmentation helps improve the NER results and the highest performances are obtained by fine-tuning pre-trained language models where the monolingual model PhoBERT for Vietnamese (Nguyen and Nguyen, 2020) produces higher results than the multilingual model XLM-R (Conneau et al., 2020). We publicly release our dataset at: https://github.com/VinAIResearch/PhoNER_COVID19",
    "checked": true,
    "id": "64017adc3782877d69ba7144a3a9a490dec8209c",
    "semantic_title": "covid-19 named entity recognition for vietnamese",
    "citation_count": 34,
    "authors": [
      "Thinh Hung Truong",
      "Mai Hoang Dao",
      "Dat Quoc Nguyen"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.174": {
    "title": "Framing Unpacked: A Semi-Supervised Interpretable Multi-View Model of Media Frames",
    "volume": "main",
    "abstract": "Understanding how news media frame political issues is important due to its impact on public attitudes, yet hard to automate. Computational approaches have largely focused on classifying the frame of a full news article while framing signals are often subtle and local. Furthermore, automatic news analysis is a sensitive domain, and existing classifiers lack transparency in their predictions. This paper addresses both issues with a novel semi-supervised model, which jointly learns to embed local information about the events and related actors in a news article through an auto-encoding framework, and to leverage this signal for document-level frame classification. Our experiments show that: our model outperforms previous models of frame prediction; we can further improve performance with unlabeled training data leveraging the semi-supervised nature of our model; and the learnt event and actor embeddings intuitively corroborate the document-level predictions, providing a nuanced and interpretable article frame representation",
    "checked": true,
    "id": "e719246b01009d3a91f9b0d76e2987c1ea534571",
    "semantic_title": "framing unpacked: a semi-supervised interpretable multi-view model of media frames",
    "citation_count": 7,
    "authors": [
      "Shima Khanehzar",
      "Trevor Cohn",
      "Gosia Mikolajczak",
      "Andrew Turpin",
      "Lea Frermann"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.175": {
    "title": "Automatic Classification of Neutralization Techniques in the Narrative of Climate Change Scepticism",
    "volume": "main",
    "abstract": "Neutralisation techniques, e.g. denial of responsibility and denial of victim, are used in the narrative of climate change scepticism to justify lack of action or to promote an alternative view. We first draw on social science to introduce the problem to the community of nlp, present the granularity of the coding schema and then collect manual annotations of neutralised techniques in text relating to climate change, and experiment with supervised and semi- supervised BERT-based models",
    "checked": true,
    "id": "a9d09338a3a0d9102c1e623e6ad434a446c0bbfe",
    "semantic_title": "automatic classification of neutralization techniques in the narrative of climate change scepticism",
    "citation_count": 5,
    "authors": [
      "Shraey Bhatia",
      "Jey Han Lau",
      "Timothy Baldwin"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.176": {
    "title": "Suicide Ideation Detection via Social and Temporal User Representations using Hyperbolic Learning",
    "volume": "main",
    "abstract": "Recent psychological studies indicate that individuals exhibiting suicidal ideation increasingly turn to social media rather than mental health practitioners. Personally contextualizing the buildup of such ideation is critical for accurate identification of users at risk. In this work, we propose a framework jointly leveraging a user's emotional history and social information from a user's neighborhood in a network to contextualize the interpretation of the latest tweet of a user on Twitter. Reflecting upon the scale-free nature of social network relationships, we propose the use of Hyperbolic Graph Convolution Networks, in combination with the Hawkes process to learn the historical emotional spectrum of a user in a time-sensitive manner. Our system significantly outperforms state-of-the-art methods on this task, showing the benefits of both socially and personally contextualized representations",
    "checked": true,
    "id": "ce5c64061eb6945478761d85cfca130e7716d36c",
    "semantic_title": "suicide ideation detection via social and temporal user representations using hyperbolic learning",
    "citation_count": 33,
    "authors": [
      "Ramit Sawhney",
      "Harshit Joshi",
      "Rajiv Ratn Shah",
      "Lucie Flek"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.177": {
    "title": "WikiTalkEdit: A Dataset for modeling Editors' behaviors on Wikipedia",
    "volume": "main",
    "abstract": "This study introduces and analyzes WikiTalkEdit, a dataset of conversations and edit histories from Wikipedia, for research in online cooperation and conversation modeling. The dataset comprises dialog triplets from the Wikipedia Talk pages, and editing actions on the corresponding articles being discussed. We show how the data supports the classic understanding of style matching, where positive emotion and the use of first-person pronouns predict a positive emotional change in a Wikipedia contributor. However, they do not predict editorial behavior. On the other hand, feedback invoking evidentiality and criticism, and references to Wikipedia's community norms, is more likely to persuade the contributor to perform edits but is less likely to lead to a positive emotion. We developed baseline classifiers trained on pre-trained RoBERTa features that can predict editorial change with an F1 score of .54, as compared to an F1 score of .66 for predicting emotional change. A diagnostic analysis of persisting errors is also provided. We conclude with possible applications and recommendations for future work. The dataset is publicly available for the research community at https://github.com/kj2013/WikiTalkEdit/",
    "checked": true,
    "id": "8497617d225cb125b74b4c1ca308aee559883775",
    "semantic_title": "wikitalkedit: a dataset for modeling editors' behaviors on wikipedia",
    "citation_count": 4,
    "authors": [
      "Kokil Jaidka",
      "Andrea Ceolin",
      "Iknoor Singh",
      "Niyati Chhaya",
      "Lyle Ungar"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.178": {
    "title": "The structure of online social networks modulates the rate of lexical change",
    "volume": "main",
    "abstract": "New words are regularly introduced to communities, yet not all of these words persist in a community's lexicon. Among the many factors contributing to lexical change, we focus on the understudied effect of social networks. We conduct a large-scale analysis of over 80k neologisms in 4420 online communities across a decade. Using Poisson regression and survival analysis, our study demonstrates that the community's network structure plays a significant role in lexical change. Apart from overall size, properties including dense connections, the lack of local clusters, and more external contacts promote lexical innovation and retention. Unlike offline communities, these topic-based communities do not experience strong lexical leveling despite increased contact but accommodate more niche words. Our work provides support for the sociolinguistic hypothesis that lexical change is partially shaped by the structure of the underlying network but also uncovers findings specific to online communities",
    "checked": true,
    "id": "1fdbb5ba5581d10a163b4ace20a8dfe0273677b6",
    "semantic_title": "the structure of online social networks modulates the rate of lexical change",
    "citation_count": 5,
    "authors": [
      "Jian Zhu",
      "David Jurgens"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.179": {
    "title": "Modeling Framing in Immigration Discourse on Social Media",
    "volume": "main",
    "abstract": "The framing of political issues can influence policy and public opinion. Even though the public plays a key role in creating and spreading frames, little is known about how ordinary people on social media frame political issues. By creating a new dataset of immigration-related tweets labeled for multiple framing typologies from political communication theory, we develop supervised models to detect frames. We demonstrate how users' ideology and region impact framing choices, and how a message's framing influences audience responses. We find that the more commonly-used issue-generic frames obscure important ideological and regional patterns that are only revealed by immigration-specific frames. Furthermore, frames oriented towards human interests, culture, and politics are associated with higher user engagement. This large-scale analysis of a complex social and linguistic phenomenon contributes to both NLP and social science research",
    "checked": true,
    "id": "0b9adbb82d940340bf68b7505c47508379c741fb",
    "semantic_title": "modeling framing in immigration discourse on social media",
    "citation_count": 52,
    "authors": [
      "Julia Mendelsohn",
      "Ceren Budak",
      "David Jurgens"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.180": {
    "title": "Modeling the Severity of Complaints in Social Media",
    "volume": "main",
    "abstract": "The speech act of complaining is used by humans to communicate a negative mismatch between reality and expectations as a reaction to an unfavorable situation. Linguistic theory of pragmatics categorizes complaints into various severity levels based on the face-threat that the complainer is willing to undertake. This is particularly useful for understanding the intent of complainers and how humans develop suitable apology strategies. In this paper, we study the severity level of complaints for the first time in computational linguistics. To facilitate this, we enrich a publicly available data set of complaints with four severity categories and train different transformer-based networks combined with linguistic information achieving 55.7 macro F1. We also jointly model binary complaint classification and complaint severity in a multi-task setting achieving new state-of-the-art results on binary complaint detection reaching up to 88.2 macro F1. Finally, we present a qualitative analysis of the behavior of our models in predicting complaint severity levels",
    "checked": true,
    "id": "bfc77a30c12c4d92efbd7be41696363ad2556316",
    "semantic_title": "modeling the severity of complaints in social media",
    "citation_count": 15,
    "authors": [
      "Mali Jin",
      "Nikolaos Aletras"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.181": {
    "title": "What About the Precedent: An Information-Theoretic Analysis of Common Law",
    "volume": "main",
    "abstract": "In common law, the outcome of a new case is determined mostly by precedent cases, rather than by existing statutes. However, how exactly does the precedent influence the outcome of a new case? Answering this question is crucial for guaranteeing fair and consistent judicial decision-making. We are the first to approach this question computationally by comparing two longstanding jurisprudential views; Halsbury's, who believes that the arguments of the precedent are the main determinant of the outcome, and Goodhart's, who believes that what matters most is the precedent's facts. We base our study on the corpus of legal cases from the European Court of Human Rights (ECtHR), which allows us to access not only the case itself, but also cases cited in the judges' arguments (i.e. the precedent cases). Taking an information-theoretic view, and modelling the question as a case out-come classification task, we find that the precedent's arguments share 0.38 nats of information with the case's outcome, whereas precedent's facts only share 0.18 nats of information (i.e.,58% less); suggesting Halsbury's view may be more accurate in this specific court. We found however in a qualitative analysis that there are specific statues where Goodhart's view dominates, and present some evidence these are the ones where the legal concept at hand is less straightforward",
    "checked": true,
    "id": "063bfb1cb74a8150a5378125cd34d998aac9dd21",
    "semantic_title": "what about the precedent: an information-theoretic analysis of common law",
    "citation_count": 12,
    "authors": [
      "Josef Valvoda",
      "Tiago Pimentel",
      "Niklas Stoehr",
      "Ryan Cotterell",
      "Simone Teufel"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.182": {
    "title": "Introducing CAD: the Contextual Abuse Dataset",
    "volume": "main",
    "abstract": "Online abuse can inflict harm on users and communities, making online spaces unsafe and toxic. Progress in automatically detecting and classifying abusive content is often held back by the lack of high quality and detailed datasets. We introduce a new dataset of primarily English Reddit entries which addresses several limitations of prior work. It (1) contains six conceptually distinct primary categories as well as secondary categories, (2) has labels annotated in the context of the conversation thread, (3) contains rationales and (4) uses an expert-driven group-adjudication process for high quality annotations. We report several baseline models to benchmark the work of future researchers. The annotated dataset, annotation guidelines, models and code are freely available",
    "checked": true,
    "id": "0819e62e97f7c4ac88d4659b9e3bff2cae436ed1",
    "semantic_title": "introducing cad: the contextual abuse dataset",
    "citation_count": 84,
    "authors": [
      "Bertie Vidgen",
      "Dong Nguyen",
      "Helen Margetts",
      "Patricia Rossini",
      "Rebekah Tromble"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.183": {
    "title": "Lifelong Learning of Hate Speech Classification on Social Media",
    "volume": "main",
    "abstract": "Existing work on automated hate speech classification assumes that the dataset is fixed and the classes are pre-defined. However, the amount of data in social media increases every day, and the hot topics changes rapidly, requiring the classifiers to be able to continuously adapt to new data without forgetting the previously learned knowledge. This ability, referred to as lifelong learning, is crucial for the real-word application of hate speech classifiers in social media. In this work, we propose lifelong learning of hate speech classification on social media. To alleviate catastrophic forgetting, we propose to use Variational Representation Learning (VRL) along with a memory module based on LB-SOINN (Load-Balancing Self-Organizing Incremental Neural Network). Experimentally, we show that combining variational representation learning and the LB-SOINN memory module achieves better performance than the commonly-used lifelong learning techniques",
    "checked": true,
    "id": "1c8a6345508583043c2d03a2dc7e9999c34b1a77",
    "semantic_title": "lifelong learning of hate speech classification on social media",
    "citation_count": 22,
    "authors": [
      "Jing Qian",
      "Hong Wang",
      "Mai ElSherief",
      "Xifeng Yan"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.184": {
    "title": "Learning to Recognize Dialect Features",
    "volume": "main",
    "abstract": "Building NLP systems that serve everyone requires accounting for dialect differences. But dialects are not monolithic entities: rather, distinctions between and within dialects are captured by the presence, absence, and frequency of dozens of dialect features in speech and text, such as the deletion of the copula in \"He ∅ running\". In this paper, we introduce the task of dialect feature detection, and present two multitask learning approaches, both based on pretrained transformers. For most dialects, large-scale annotated corpora for these features are unavailable, making it difficult to train recognizers. We train our models on a small number of minimal pairs, building on how linguists typically define dialect features. Evaluation on a test set of 22 dialect features of Indian English demonstrates that these models learn to recognize many features with high accuracy, and that a few minimal pairs can be as effective for training as thousands of labeled examples. We also demonstrate the downstream applicability of dialect feature detection both as a measure of dialect density and as a dialect classifier",
    "checked": true,
    "id": "64414f14be365625c51c060263ba8fbfd623920b",
    "semantic_title": "learning to recognize dialect features",
    "citation_count": 29,
    "authors": [
      "Dorottya Demszky",
      "Devyani Sharma",
      "Jonathan Clark",
      "Vinodkumar Prabhakaran",
      "Jacob Eisenstein"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.185": {
    "title": "It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners",
    "volume": "main",
    "abstract": "When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much \"greener\" in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models",
    "checked": true,
    "id": "f30444fbb6ad806168e2564db4815cd27faa7fd9",
    "semantic_title": "it's not just size that matters: small language models are also few-shot learners",
    "citation_count": 844,
    "authors": [
      "Timo Schick",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.186": {
    "title": "Static Embeddings as Efficient Knowledge Bases?",
    "volume": "main",
    "abstract": "Recent research investigates factual knowledge stored in large pretrained language models (PLMs). Instead of structural knowledge base (KB) queries, masked sentences such as \"Paris is the capital of [MASK]\" are used as probes. The good performance on this analysis task has been interpreted as PLMs becoming potential repositories of factual knowledge. In experiments across ten linguistically diverse languages, we study knowledge contained in static embeddings. We show that, when restricting the output space to a candidate set, simple nearest neighbor matching using static embeddings performs better than PLMs. E.g., static embeddings perform 1.6% points better than BERT while just using 0.3% of energy for training. One important factor in their good comparative performance is that static embeddings are standardly learned for a large vocabulary. In contrast, BERT exploits its more sophisticated, but expensive ability to compose meaningful representations from a much smaller subword vocabulary",
    "checked": true,
    "id": "92287e1b979e9a2cf0548bd503a0504ad2f6d54d",
    "semantic_title": "static embeddings as efficient knowledge bases?",
    "citation_count": 17,
    "authors": [
      "Philipp Dufter",
      "Nora Kassner",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.187": {
    "title": "Highly Efficient Knowledge Graph Embedding Learning with Orthogonal Procrustes Analysis",
    "volume": "main",
    "abstract": "Knowledge Graph Embeddings (KGEs) have been intensively explored in recent years due to their promise for a wide range of applications. However, existing studies focus on improving the final model performance without acknowledging the computational cost of the proposed approaches, in terms of execution time and environmental impact. This paper proposes a simple yet effective KGE framework which can reduce the training time and carbon footprint by orders of magnitudes compared with state-of-the-art approaches, while producing competitive performance. We highlight three technical innovations: full batch learning via relational matrices, closed-form Orthogonal Procrustes Analysis for KGEs, and non-negative-sampling training. In addition, as the first KGE method whose entity embeddings also store full relation information, our trained models encode rich semantics and are highly interpretable. Comprehensive experiments and ablation studies involving 13 strong baselines and two standard datasets verify the effectiveness and efficiency of our algorithm",
    "checked": true,
    "id": "31b1968a756c7f94e1910d919e26f763ac965071",
    "semantic_title": "highly efficient knowledge graph embedding learning with orthogonal procrustes analysis",
    "citation_count": 13,
    "authors": [
      "Xutan Peng",
      "Guanyi Chen",
      "Chenghua Lin",
      "Mark Stevenson"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.188": {
    "title": "Rethinking Network Pruning – under the Pre-train and Fine-tune Paradigm",
    "volume": "main",
    "abstract": "Transformer-based pre-trained language models have significantly improved the performance of various natural language processing (NLP) tasks in the recent years. While effective and prevalent, these models are usually prohibitively large for resource-limited deployment scenarios. A thread of research has thus been working on applying network pruning techniques under the pretrain-then-finetune paradigm widely adopted in NLP. However, the existing pruning results on benchmark transformers, such as BERT, are not as remarkable as the pruning results in the literature of convolutional neural networks (CNNs). In particular, common wisdom in pruning CNN states that sparse pruning technique compresses a model more than that obtained by reducing number of channels and layers, while existing works on sparse pruning of BERT yields inferior results than its small-dense counterparts such as TinyBERT. In this work, we aim to fill this gap by studying how knowledge are transferred and lost during the pre-train, fine-tune, and pruning process, and proposing a knowledge-aware sparse pruning process that achieves significantly superior results than existing literature. We show for the first time that sparse pruning compresses a BERT model significantly more than reducing its number of channels and layers. Experiments on multiple data sets of GLUE benchmark show that our method outperforms the leading competitors with a 20-times weight/FLOPs compression and neglectable loss in prediction accuracy",
    "checked": true,
    "id": "40235eded15f44c8c4a7f48468adcc7df4e171fb",
    "semantic_title": "rethinking network pruning – under the pre-train and fine-tune paradigm",
    "citation_count": 47,
    "authors": [
      "Dongkuan Xu",
      "Ian En-Hsu Yen",
      "Jinxi Zhao",
      "Zhibin Xiao"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.189": {
    "title": "Towards a Comprehensive Understanding and Accurate Evaluation of Societal Biases in Pre-Trained Transformers",
    "volume": "main",
    "abstract": "The ease of access to pre-trained transformers has enabled developers to leverage large-scale language models to build exciting applications for their users. While such pre-trained models offer convenient starting points for researchers and developers, there is little consideration for the societal biases captured within these model risking perpetuation of racial, gender, and other harmful biases when these models are deployed at scale. In this paper, we investigate gender and racial bias across ubiquitous pre-trained language models, including GPT-2, XLNet, BERT, RoBERTa, ALBERT and DistilBERT. We evaluate bias within pre-trained transformers using three metrics: WEAT, sequence likelihood, and pronoun ranking. We conclude with an experiment demonstrating the ineffectiveness of word-embedding techniques, such as WEAT, signaling the need for more robust bias testing in transformers",
    "checked": true,
    "id": "2cc0e605470d3ac20aad82c73560b888ecc449cd",
    "semantic_title": "towards a comprehensive understanding and accurate evaluation of societal biases in pre-trained transformers",
    "citation_count": 48,
    "authors": [
      "Andrew Silva",
      "Pradyumna Tambwekar",
      "Matthew Gombolay"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.190": {
    "title": "Detoxifying Language Models Risks Marginalizing Minority Voices",
    "volume": "main",
    "abstract": "Language models (LMs) must be both safe and equitable to be responsibly deployed in practice. With safety in mind, numerous detoxification techniques (e.g., Dathathri et al. 2020; Krause et al. 2020) have been proposed to mitigate toxic LM generations. In this work, we show that these detoxification techniques hurt equity: they decrease the utility of LMs on language used by marginalized groups (e.g., African-American English and minority identity mentions). In particular, we perform automatic and human evaluations of text generation quality when LMs are conditioned on inputs with different dialects and group identifiers. We find that detoxification makes LMs more brittle to distribution shift, especially on language used by marginalized groups. We identify that these failures stem from detoxification methods exploiting spurious correlations in toxicity datasets. Overall, our results highlight the tension between the controllability and distributional robustness of LMs",
    "checked": true,
    "id": "4ae632b89089b38ce41d307a6cda4727e42aaab3",
    "semantic_title": "detoxifying language models risks marginalizing minority voices",
    "citation_count": 101,
    "authors": [
      "Albert Xu",
      "Eshaan Pathak",
      "Eric Wallace",
      "Suchin Gururangan",
      "Maarten Sap",
      "Dan Klein"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.191": {
    "title": "HONEST: Measuring Hurtful Sentence Completion in Language Models",
    "volume": "main",
    "abstract": "Language models have revolutionized the field of NLP. However, language models capture and proliferate hurtful stereotypes, especially in text generation. Our results show that 4.3% of the time, language models complete a sentence with a hurtful word. These cases are not random, but follow language and gender-specific patterns. We propose a score to measure hurtful sentence completions in language models (HONEST). It uses a systematic template- and lexicon-based bias evaluation methodology for six languages. Our findings suggest that these models replicate and amplify deep-seated societal stereotypes about gender roles. Sentence completions refer to sexual promiscuity when the target is female in 9% of the time, and in 4% to homosexuality when the target is male. The results raise questions about the use of these models in production settings",
    "checked": true,
    "id": "4f76d26369ee573cb03db4b9f6e4ab5d61806095",
    "semantic_title": "honest: measuring hurtful sentence completion in language models",
    "citation_count": 109,
    "authors": [
      "Debora Nozza",
      "Federico Bianchi",
      "Dirk Hovy"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.192": {
    "title": "EaSe: A Diagnostic Tool for VQA based on Answer Diversity",
    "volume": "main",
    "abstract": "We propose EASE, a simple diagnostic tool for Visual Question Answering (VQA) which quantifies the difficulty of an image, question sample. EASE is based on the pattern of answers provided by multiple annotators to a given question. In particular, it considers two aspects of the answers: (i) their Entropy; (ii) their Semantic content. First, we prove the validity of our diagnostic to identify samples that are easy/hard for state-of-art VQA models. Second, we show that EASE can be successfully used to select the most-informative samples for training/fine-tuning. Crucially, only information that is readily available in any VQA dataset is used to compute its scores",
    "checked": true,
    "id": "d22c52cb6e83f9a9239071dba95e7ec73b9cd670",
    "semantic_title": "ease: a diagnostic tool for vqa based on answer diversity",
    "citation_count": 3,
    "authors": [
      "Shailza Jolly",
      "Sandro Pezzelle",
      "Moin Nabi"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.193": {
    "title": "DeCEMBERT: Learning from Noisy Instructional Videos via Dense Captions and Entropy Minimization",
    "volume": "main",
    "abstract": "Leveraging large-scale unlabeled web videos such as instructional videos for pre-training followed by task-specific finetuning has become the de facto approach for many video-and-language tasks. However, these instructional videos are very noisy, the accompanying ASR narrations are often incomplete, and can be irrelevant to or temporally misaligned with the visual content, limiting the performance of the models trained on such data. To address these issues, we propose an improved video-and-language pre-training method that first adds automatically-extracted dense region captions from the video frames as auxiliary text input, to provide informative visual cues for learning better video and language associations. Second, to alleviate the temporal misalignment issue, our method incorporates an entropy minimization-based constrained attention loss, to encourage the model to automatically focus on the correct caption from a pool of candidate ASR captions. Our overall approach is named DeCEMBERT (Dense Captions and Entropy Minimization). Comprehensive experiments on three video-and-language tasks (text-to-video retrieval, video captioning, and video question answering) across five datasets demonstrate that our approach outperforms previous state-of-the-art methods. Ablation studies on pre-training and downstream tasks show that adding dense captions and constrained attention loss help improve the model performance. Lastly, we also provide attention visualization to show the effect of applying the proposed constrained attention loss",
    "checked": true,
    "id": "d56a1311b2d73513415b028f148c11735aea8004",
    "semantic_title": "decembert: learning from noisy instructional videos via dense captions and entropy minimization",
    "citation_count": 45,
    "authors": [
      "Zineng Tang",
      "Jie Lei",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.194": {
    "title": "Improving Generation and Evaluation of Visual Stories via Semantic Consistency",
    "volume": "main",
    "abstract": "Story visualization is an underexplored task that falls at the intersection of many important research directions in both computer vision and natural language processing. In this task, given a series of natural language captions which compose a story, an agent must generate a sequence of images that correspond to the captions. Prior work has introduced recurrent generative models which outperform text-to-image synthesis models on this task. However, there is room for improvement of generated images in terms of visual quality, coherence and relevance. We present a number of improvements to prior modeling approaches, including (1) the addition of a dual learning framework that utilizes video captioning to reinforce the semantic alignment between the story and generated images, (2) a copy-transform mechanism for sequentially-consistent story visualization, and (3) MART-based transformers to model complex interactions between frames. We present ablation studies to demonstrate the effect of each of these techniques on the generative power of the model for both individual images as well as the entire narrative. Furthermore, due to the complexity and generative nature of the task, standard evaluation metrics do not accurately reflect performance. Therefore, we also provide an exploration of evaluation metrics for the model, focused on aspects of the generated frames such as the presence/quality of generated characters, the relevance to captions, and the diversity of the generated images. We also present correlation experiments of our proposed automated metrics with human evaluations",
    "checked": true,
    "id": "87a0279c1d640b486dc5c9f1e0d3705ed87754cf",
    "semantic_title": "improving generation and evaluation of visual stories via semantic consistency",
    "citation_count": 44,
    "authors": [
      "Adyasha Maharana",
      "Darryl Hannan",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.195": {
    "title": "Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models",
    "volume": "main",
    "abstract": "This paper studies zero-shot cross-lingual transfer of vision-language models. Specifically, we focus on multilingual text-to-video search and propose a Transformer-based model that learns contextual multilingual multimodal embeddings. Under a zero-shot setting, we empirically demonstrate that performance degrades significantly when we query the multilingual text-video model with non-English sentences. To address this problem, we introduce a multilingual multimodal pre-training strategy, and collect a new multilingual instructional video dataset (Multi-HowTo100M) for pre-training. Experiments on VTT show that our method significantly improves video search in non-English languages without additional annotations. Furthermore, when multilingual annotations are available, our method outperforms recent baselines by a large margin in multilingual text-to-video search on VTT and VATEX; as well as in multilingual text-to-image search on Multi30K. Our model and Multi-HowTo100M is available at http://github.com/berniebear/Multi-HT100M",
    "checked": true,
    "id": "25efc17ba82ba4af29f2e03868de74e1ea66d025",
    "semantic_title": "multilingual multimodal pre-training for zero-shot cross-lingual transfer of vision-language models",
    "citation_count": 53,
    "authors": [
      "Po-Yao Huang",
      "Mandela Patrick",
      "Junjie Hu",
      "Graham Neubig",
      "Florian Metze",
      "Alexander Hauptmann"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.196": {
    "title": "Video Question Answering with Phrases via Semantic Roles",
    "volume": "main",
    "abstract": "Video Question Answering (VidQA) evaluation metrics have been limited to a single-word answer or selecting a phrase from a fixed set of phrases. These metrics limit the VidQA models' application scenario. In this work, we leverage semantic roles derived from video descriptions to mask out certain phrases, to introduce VidQAP which poses VidQA as a fill-in-the-phrase task. To enable evaluation of answer phrases, we compute the relative improvement of the predicted answer compared to an empty string. To reduce the influence of language bias in VidQA datasets, we retrieve a video having a different answer for the same question. To facilitate research, we construct ActivityNet-SRL-QA and Charades-SRL-QA and benchmark them by extending three vision-language models. We perform extensive analysis and ablative studies to guide future work. Code and data are public",
    "checked": true,
    "id": "1161e9556092cf9bf2923e0b15d03b01be1ce929",
    "semantic_title": "video question answering with phrases via semantic roles",
    "citation_count": 15,
    "authors": [
      "Arka Sadhu",
      "Kan Chen",
      "Ram Nevatia"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.197": {
    "title": "From Masked Language Modeling to Translation: Non-English Auxiliary Tasks Improve Zero-shot Spoken Language Understanding",
    "volume": "main",
    "abstract": "The lack of publicly available evaluation data for low-resource languages limits progress in Spoken Language Understanding (SLU). As key tasks like intent classification and slot filling require abundant training data, it is desirable to reuse existing data in high-resource languages to develop models for low-resource scenarios. We introduce xSID, a new benchmark for cross-lingual (x) Slot and Intent Detection in 13 languages from 6 language families, including a very low-resource dialect. To tackle the challenge, we propose a joint learning approach, with English SLU training data and non-English auxiliary tasks from raw text, syntax and translation for transfer. We study two setups which differ by type and language coverage of the pre-trained embeddings. Our results show that jointly learning the main tasks with masked language modeling is effective for slots, while machine translation transfer works best for intent classification",
    "checked": true,
    "id": "14c940926a046e6c33cc4eca9ea73cac1bcfa347",
    "semantic_title": "from masked language modeling to translation: non-english auxiliary tasks improve zero-shot spoken language understanding",
    "citation_count": 39,
    "authors": [
      "Rob van der Goot",
      "Ibrahim Sharaf",
      "Aizhan Imankulova",
      "Ahmet Üstün",
      "Marija Stepanović",
      "Alan Ramponi",
      "Siti Oryza Khairunnisa",
      "Mamoru Komachi",
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.198": {
    "title": "WEC: Deriving a Large-scale Cross-document Event Coreference dataset from Wikipedia",
    "volume": "main",
    "abstract": "Cross-document event coreference resolution is a foundational task for NLP applications involving multi-text processing. However, existing corpora for this task are scarce and relatively small, while annotating only modest-size clusters of documents belonging to the same topic. To complement these resources and enhance future research, we present Wikipedia Event Coreference (WEC), an efficient methodology for gathering a large-scale dataset for cross-document event coreference from Wikipedia, where coreference links are not restricted within predefined topics. We apply this methodology to the English Wikipedia and extract our large-scale WEC-Eng dataset. Notably, our dataset creation method is generic and can be applied with relatively little effort to other Wikipedia languages. To set baseline results, we develop an algorithm that adapts components of state-of-the-art models for within-document coreference resolution to the cross-document setting. Our model is suitably efficient and outperforms previously published state-of-the-art results for the task",
    "checked": true,
    "id": "a814f3773b6482b7f5c8ba8c3edfedb726999574",
    "semantic_title": "wec: deriving a large-scale cross-document event coreference dataset from wikipedia",
    "citation_count": 26,
    "authors": [
      "Alon Eirew",
      "Arie Cattan",
      "Ido Dagan"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.199": {
    "title": "Challenging distributional models with a conceptual network of philosophical terms",
    "volume": "main",
    "abstract": "Computational linguistic research on language change through distributional semantic (DS) models has inspired researchers from fields such as philosophy and literary studies, who use these methods for the exploration and comparison of comparatively small datasets traditionally analyzed by close reading. Research on methods for small data is still in early stages and it is not clear which methods achieve the best results. We investigate the possibilities and limitations of using distributional semantic models for analyzing philosophical data by means of a realistic use-case. We provide a ground truth for evaluation created by philosophy experts and a blueprint for using DS models in a sound methodological setup. We compare three methods for creating specialized models from small datasets. Though the models do not perform well enough to directly support philosophers yet, we find that models designed for small data yield promising directions for future work",
    "checked": true,
    "id": "7377c27d6c0813ab580155960891dd01b97beb28",
    "semantic_title": "challenging distributional models with a conceptual network of philosophical terms",
    "citation_count": 4,
    "authors": [
      "Yvette Oortwijn",
      "Jelke Bloem",
      "Pia Sommerauer",
      "Francois Meyer",
      "Wei Zhou",
      "Antske Fokkens"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.200": {
    "title": "KILT: a Benchmark for Knowledge Intensive Language Tasks",
    "volume": "main",
    "abstract": "Challenging problems such as open-domain question answering, fact checking, slot filling and entity linking require access to large, external knowledge sources. While some models do well on individual tasks, developing general models is difficult as each task might require computationally expensive indexing of custom knowledge sources, in addition to dedicated infrastructure. To catalyze research on models that condition on specific information in large textual resources, we present a benchmark for knowledge-intensive language tasks (KILT). All tasks in KILT are grounded in the same snapshot of Wikipedia, reducing engineering turnaround through the re-use of components, as well as accelerating research into task-agnostic memory architectures. We test both task-specific and general baselines, evaluating downstream performance in addition to the ability of the models to provide provenance. We find that a shared dense vector index coupled with a seq2seq model is a strong baseline, outperforming more tailor-made approaches for fact checking, open-domain question answering and dialogue, and yielding competitive results on entity linking and slot filling, by generating disambiguated text. KILT data and code are available at https://github.com/facebookresearch/KILT",
    "checked": true,
    "id": "0b09448f7543453cc066416f547292dc1e4471f6",
    "semantic_title": "kilt: a benchmark for knowledge intensive language tasks",
    "citation_count": 452,
    "authors": [
      "Fabio Petroni",
      "Aleksandra Piktus",
      "Angela Fan",
      "Patrick Lewis",
      "Majid Yazdani",
      "Nicola De Cao",
      "James Thorne",
      "Yacine Jernite",
      "Vladimir Karpukhin",
      "Jean Maillard",
      "Vassilis Plachouras",
      "Tim Rocktäschel",
      "Sebastian Riedel"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.201": {
    "title": "A Survey on Recent Approaches for Natural Language Processing in Low-Resource Scenarios",
    "volume": "main",
    "abstract": "Deep neural networks and huge language models are becoming omnipresent in natural language applications. As they are known for requiring large amounts of training data, there is a growing body of work to improve the performance in low-resource settings. Motivated by the recent fundamental changes towards neural models and the popular pre-train and fine-tune paradigm, we survey promising approaches for low-resource natural language processing. After a discussion about the different dimensions of data availability, we give a structured overview of methods that enable learning when training data is sparse. This includes mechanisms to create additional labeled data like data augmentation and distant supervision as well as transfer learning settings that reduce the need for target supervision. A goal of our survey is to explain how these methods differ in their requirements as understanding them is essential for choosing a technique suited for a specific low-resource setting. Further key aspects of this work are to highlight open issues and to outline promising directions for future research",
    "checked": true,
    "id": "455cdafd55a5b5ddefa029bf97801327e142646d",
    "semantic_title": "a survey on recent approaches for natural language processing in low-resource scenarios",
    "citation_count": 188,
    "authors": [
      "Michael A. Hedderich",
      "Lukas Lange",
      "Heike Adel",
      "Jannik Strötgen",
      "Dietrich Klakow"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.202": {
    "title": "Temporal Knowledge Graph Completion using a Linear Temporal Regularizer and Multivector Embeddings",
    "volume": "main",
    "abstract": "Representation learning approaches for knowledge graphs have been mostly designed for static data. However, many knowledge graphs involve evolving data, e.g., the fact (The President of the United States is Barack Obama) is valid only from 2009 to 2017. This introduces important challenges for knowledge representation learning since the knowledge graphs change over time. In this paper, we present a novel time-aware knowledge graph embebdding approach, TeLM, which performs 4th-order tensor factorization of a Temporal knowledge graph using a Linear temporal regularizer and Multivector embeddings. Moreover, we investigate the effect of the temporal dataset's time granularity on temporal knowledge graph completion. Experimental results demonstrate that our proposed models trained with the linear temporal regularizer achieve the state-of-the-art performances on link prediction over four well-established temporal knowledge graph completion benchmarks",
    "checked": true,
    "id": "e467b66fd3505e8ea0d1f0c61df6ea832858fe1f",
    "semantic_title": "temporal knowledge graph completion using a linear temporal regularizer and multivector embeddings",
    "citation_count": 54,
    "authors": [
      "Chengjin Xu",
      "Yung-Yu Chen",
      "Mojtaba Nayyeri",
      "Jens Lehmann"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.203": {
    "title": "UDALM: Unsupervised Domain Adaptation through Language Modeling",
    "volume": "main",
    "abstract": "In this work we explore Unsupervised Domain Adaptation (UDA) of pretrained language models for downstream tasks. We introduce UDALM, a fine-tuning procedure, using a mixed classification and Masked Language Model loss, that can adapt to the target domain distribution in a robust and sample efficient manner. Our experiments show that performance of models trained with the mixed loss scales with the amount of available target data and the mixed loss can be effectively used as a stopping criterion during UDA training. Furthermore, we discuss the relationship between A-distance and the target error and explore some limitations of the Domain Adversarial Training approach. Our method is evaluated on twelve domain pairs of the Amazon Reviews Sentiment dataset, yielding 91.74% accuracy, which is an 1.11% absolute improvement over the state-of-the-art",
    "checked": true,
    "id": "a4f1baae8aa410c01fb04bf9107e0f03196c8241",
    "semantic_title": "udalm: unsupervised domain adaptation through language modeling",
    "citation_count": 48,
    "authors": [
      "Constantinos Karouzos",
      "Georgios Paraskevopoulos",
      "Alexandros Potamianos"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.204": {
    "title": "Beyond Black & White: Leveraging Annotator Disagreement via Soft-Label Multi-Task Learning",
    "volume": "main",
    "abstract": "Supervised learning assumes that a ground truth label exists. However, the reliability of this ground truth depends on human annotators, who often disagree. Prior work has shown that this disagreement can be helpful in training models. We propose a novel method to incorporate this disagreement as information: in addition to the standard error computation, we use soft-labels (i.e., probability distributions over the annotator labels) as an auxiliary task in a multi-task neural network. We measure the divergence between the predictions and the target soft-labels with several loss-functions and evaluate the models on various NLP tasks. We find that the soft-label prediction auxiliary task reduces the penalty for errors on ambiguous entities, and thereby mitigates overfitting. It significantly improves performance across tasks, beyond the standard approach and prior work",
    "checked": true,
    "id": "329922df03bf67500f8860e8cbc87bbe7a1ee54b",
    "semantic_title": "beyond black & white: leveraging annotator disagreement via soft-label multi-task learning",
    "citation_count": 94,
    "authors": [
      "Tommaso Fornaciari",
      "Alexandra Uma",
      "Silviu Paun",
      "Barbara Plank",
      "Dirk Hovy",
      "Massimo Poesio"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.205": {
    "title": "Clustering-based Inference for Biomedical Entity Linking",
    "volume": "main",
    "abstract": "Due to large number of entities in biomedical knowledge bases, only a small fraction of entities have corresponding labelled training data. This necessitates entity linking models which are able to link mentions of unseen entities using learned representations of entities. Previous approaches link each mention independently, ignoring the relationships within and across documents between the entity mentions. These relations can be very useful for linking mentions in biomedical text where linking decisions are often difficult due mentions having a generic or a highly specialized form. In this paper, we introduce a model in which linking decisions can be made not merely by linking to a knowledge base entity but also by grouping multiple mentions together via clustering and jointly making linking predictions. In experiments on the largest publicly available biomedical dataset, we improve the best independent prediction for entity linking by 3.0 points of accuracy, and our clustering-based inference model further improves entity linking by 2.3 points",
    "checked": true,
    "id": "96079beb0b00ef64d753fbc5f6e2e4e58345b790",
    "semantic_title": "clustering-based inference for biomedical entity linking",
    "citation_count": 43,
    "authors": [
      "Rico Angell",
      "Nicholas Monath",
      "Sunil Mohan",
      "Nishant Yadav",
      "Andrew McCallum"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.206": {
    "title": "Variance-reduced First-order Meta-learning for Natural Language Processing Tasks",
    "volume": "main",
    "abstract": "First-order meta-learning algorithms have been widely used in practice to learn initial model parameters that can be quickly adapted to new tasks due to their efficiency and effectiveness. However, existing studies find that meta-learner can overfit to some specific adaptation when we have heterogeneous tasks, leading to significantly degraded performance. In Natural Language Processing (NLP) applications, datasets are often diverse and each task has its unique characteristics. Therefore, to address the overfitting issue when applying first-order meta-learning to NLP applications, we propose to reduce the variance of the gradient estimator used in task adaptation. To this end, we develop a variance-reduced first-order meta-learning algorithm. The core of our algorithm is to introduce a novel variance reduction term to the gradient estimation when performing the task adaptation. Experiments on two NLP applications: few-shot text classification and multi-domain dialog state tracking demonstrate the superior performance of our proposed method",
    "checked": true,
    "id": "5b89435357806e89cd428c56ce98aeb930b0df5c",
    "semantic_title": "variance-reduced first-order meta-learning for natural language processing tasks",
    "citation_count": 8,
    "authors": [
      "Lingxiao Wang",
      "Kevin Huang",
      "Tengyu Ma",
      "Quanquan Gu",
      "Jing Huang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.207": {
    "title": "Diversity-Aware Batch Active Learning for Dependency Parsing",
    "volume": "main",
    "abstract": "While the predictive performance of modern statistical dependency parsers relies heavily on the availability of expensive expert-annotated treebank data, not all annotations contribute equally to the training of the parsers. In this paper, we attempt to reduce the number of labeled examples needed to train a strong dependency parser using batch active learning (AL). In particular, we investigate whether enforcing diversity in the sampled batches, using determinantal point processes (DPPs), can improve over their diversity-agnostic counterparts. Simulation experiments on an English newswire corpus show that selecting diverse batches with DPPs is superior to strong selection strategies that do not enforce batch diversity, especially during the initial stages of the learning process. Additionally, our diversity-aware strategy is robust under a corpus duplication setting, where diversity-agnostic sampling strategies exhibit significant degradation",
    "checked": true,
    "id": "364b50c19cc1a8e2347d555a4c60802965f75670",
    "semantic_title": "diversity-aware batch active learning for dependency parsing",
    "citation_count": 8,
    "authors": [
      "Tianze Shi",
      "Adrian Benton",
      "Igor Malioutov",
      "Ozan İrsoy"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.208": {
    "title": "How many data points is a prompt worth?",
    "volume": "main",
    "abstract": "When fine-tuning pretrained models for classification, researchers either use a generic model head or a task-specific prompt for prediction. Proponents of prompting have argued that prompts provide a method for injecting task-specific guidance, which is beneficial in low-data regimes. We aim to quantify this benefit through rigorous testing of prompts in a fair setting: comparing prompted and head-based fine-tuning in equal conditions across many tasks and data sizes. By controlling for many sources of advantage, we find that prompting does indeed provide a benefit, and that this benefit can be quantified per task. Results show that prompting is often worth 100s of data points on average across classification tasks",
    "checked": true,
    "id": "a9fe5bd8da2d9603cf2cf6c6ea8b0f83c6d3a4f9",
    "semantic_title": "how many data points is a prompt worth?",
    "citation_count": 271,
    "authors": [
      "Teven Le Scao",
      "Alexander Rush"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.209": {
    "title": "Can Latent Alignments Improve Autoregressive Machine Translation?",
    "volume": "main",
    "abstract": "Latent alignment objectives such as CTC and AXE significantly improve non-autoregressive machine translation models. Can they improve autoregressive models as well? We explore the possibility of training autoregressive machine translation models with latent alignment objectives, and observe that, in practice, this approach results in degenerate models. We provide a theoretical explanation for these empirical results, and prove that latent alignment objectives are incompatible with teacher forcing",
    "checked": true,
    "id": "69278ea9200ec76d6da51058afcdd2ee803f75c4",
    "semantic_title": "can latent alignments improve autoregressive machine translation?",
    "citation_count": 5,
    "authors": [
      "Adi Haviv",
      "Lior Vassertail",
      "Omer Levy"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.210": {
    "title": "Smoothing and Shrinking the Sparse Seq2Seq Search Space",
    "volume": "main",
    "abstract": "Current sequence-to-sequence models are trained to minimize cross-entropy and use softmax to compute the locally normalized probabilities over target sequences. While this setup has led to strong results in a variety of tasks, one unsatisfying aspect is its length bias: models give high scores to short, inadequate hypotheses and often make the empty string the argmax—the so-called cat got your tongue problem. Recently proposed entmax-based sparse sequence-to-sequence models present a possible solution, since they can shrink the search space by assigning zero probability to bad hypotheses, but their ability to handle word-level tasks with transformers has never been tested. In this work, we show that entmax-based models effectively solve the cat got your tongue problem, removing a major source of model error for neural machine translation. In addition, we generalize label smoothing, a critical regularization technique, to the broader family of Fenchel-Young losses, which includes both cross-entropy and the entmax losses. Our resulting label-smoothed entmax loss models set a new state of the art on multilingual grapheme-to-phoneme conversion and deliver improvements and better calibration properties on cross-lingual morphological inflection and machine translation for 7 language pairs",
    "checked": true,
    "id": "1e6171d9937a36b2dba6b622bd5857f2407d1b2e",
    "semantic_title": "smoothing and shrinking the sparse seq2seq search space",
    "citation_count": 14,
    "authors": [
      "Ben Peters",
      "André F. T. Martins"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.211": {
    "title": "Unified Pre-training for Program Understanding and Generation",
    "volume": "main",
    "abstract": "Code summarization and generation empower conversion between programming language (PL) and natural language (NL), while code translation avails the migration of legacy code from one PL to another. This paper introduces PLBART, a sequence-to-sequence model capable of performing a broad spectrum of program and language understanding and generation tasks. PLBART is pre-trained on an extensive collection of Java and Python functions and associated NL text via denoising autoencoding. Experiments on code summarization in the English language, code generation, and code translation in seven programming languages show that PLBART outperforms or rivals state-of-the-art models. Moreover, experiments on discriminative tasks, e.g., program repair, clone detection, and vulnerable code detection, demonstrate PLBART's effectiveness in program understanding. Furthermore, analysis reveals that PLBART learns program syntax, style (e.g., identifier naming convention), logical flow (e.g., \"if\" block inside an \"else\" block is equivalent to \"else if\" block) that are crucial to program semantics and thus excels even with limited annotations",
    "checked": true,
    "id": "0646bb09db4d1ba24150e69b71edcd4aff691b3c",
    "semantic_title": "unified pre-training for program understanding and generation",
    "citation_count": 596,
    "authors": [
      "Wasi Ahmad",
      "Saikat Chakraborty",
      "Baishakhi Ray",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.212": {
    "title": "Hyperparameter-free Continuous Learning for Domain Classification in Natural Language Understanding",
    "volume": "main",
    "abstract": "Domain classification is the fundamental task in natural language understanding (NLU), which often requires fast accommodation to new emerging domains. This constraint makes it impossible to retrain all previous domains, even if they are accessible to the new model. Most existing continual learning approaches suffer from low accuracy and performance fluctuation, especially when the distributions of old and new data are significantly different. In fact, the key real-world problem is not the absence of old data, but the inefficiency to retrain the model with the whole old dataset. Is it potential to utilize some old data to yield high accuracy and maintain stable performance, while at the same time, without introducing extra hyperparameters? In this paper, we proposed a hyperparameter-free continual learning model for text data that can stably produce high performance under various environments. Specifically, we utilize Fisher information to select exemplars that can \"record\" key information of the original model. Also, a novel scheme called dynamical weight consolidation is proposed to enable hyperparameter-free learning during the retrain process. Extensive experiments demonstrate baselines provide fluctuated performance which makes them useless in practice. On the contrary, our proposed model significantly and consistently outperforms the best state-of-the-art method by up to 20% in average accuracy, and each of its component contributes effectively to overall performance",
    "checked": true,
    "id": "ea2f2fa4a7336d0d99dafe912ed43538d29da09b",
    "semantic_title": "hyperparameter-free continuous learning for domain classification in natural language understanding",
    "citation_count": 4,
    "authors": [
      "Ting Hua",
      "Yilin Shen",
      "Changsheng Zhao",
      "Yen-Chang Hsu",
      "Hongxia Jin"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.213": {
    "title": "On the Embeddings of Variables in Recurrent Neural Networks for Source Code",
    "volume": "main",
    "abstract": "Source code processing heavily relies on the methods widely used in natural language processing (NLP), but involves specifics that need to be taken into account to achieve higher quality. An example of this specificity is that the semantics of a variable is defined not only by its name but also by the contexts in which the variable occurs. In this work, we develop dynamic embeddings, a recurrent mechanism that adjusts the learned semantics of the variable when it obtains more information about the variable's role in the program. We show that using the proposed dynamic embeddings significantly improves the performance of the recurrent neural network, in code completion and bug fixing tasks",
    "checked": true,
    "id": "93224a853ae284c15211998ecb5b295d70486d27",
    "semantic_title": "on the embeddings of variables in recurrent neural networks for source code",
    "citation_count": 3,
    "authors": [
      "Nadezhda Chirkova"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.214": {
    "title": "Cross-Lingual Word Embedding Refinement by ℓ1 Norm Optimisation",
    "volume": "main",
    "abstract": "Cross-Lingual Word Embeddings (CLWEs) encode words from two or more languages in a shared high-dimensional space in which vectors representing words with similar meaning (regardless of language) are closely located. Existing methods for building high-quality CLWEs learn mappings that minimise the ℓ2 norm loss function. However, this optimisation objective has been demonstrated to be sensitive to outliers. Based on the more robust Manhattan norm (aka. ℓ1 norm) goodness-of-fit criterion, this paper proposes a simple post-processing step to improve CLWEs. An advantage of this approach is that it is fully agnostic to the training process of the original CLWEs and can therefore be applied widely. Extensive experiments are performed involving ten diverse languages and embeddings trained on different corpora. Evaluation results based on bilingual lexicon induction and cross-lingual transfer for natural language inference tasks show that the ℓ1 refinement substantially outperforms four state-of-the-art baselines in both supervised and unsupervised settings. It is therefore recommended that this strategy be adopted as a standard for CLWE methods",
    "checked": true,
    "id": "17fa1a5a4b12eb312a6f3f069e013d644708e437",
    "semantic_title": "cross-lingual word embedding refinement by \\ell_{1} norm optimisation",
    "citation_count": 10,
    "authors": [
      "Xutan Peng",
      "Chenghua Lin",
      "Mark Stevenson"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.215": {
    "title": "Semantic Frame Forecast",
    "volume": "main",
    "abstract": "This paper introduces Semantic Frame Forecast, a task that predicts the semantic frames that will occur in the next 10, 100, or even 1,000 sentences in a running story. Prior work focused on predicting the immediate future of a story, such as one to a few sentences ahead. However, when novelists write long stories, generating a few sentences is not enough to help them gain high-level insight to develop the follow-up story. In this paper, we formulate a long story as a sequence of \"story blocks,\" where each block contains a fixed number of sentences (e.g., 10, 100, or 200). This formulation allows us to predict the follow-up story arc beyond the scope of a few sentences. We represent a story block using the term frequencies (TF) of semantic frames in it, normalized by each frame's inverse document frequency (IDF). We conduct semantic frame forecast experiments on 4,794 books from the Bookcorpus and 7,962 scientific abstracts from CODA-19, with block sizes ranging from 5 to 1,000 sentences. The results show that automated models can forecast the follow-up story blocks better than the random, prior, and replay baselines, indicating the feasibility of the task. We also learn that the models using the frame representation as features outperform all the existing approaches when the block size is over 150 sentences. The human evaluation also shows that the proposed frame representation, when visualized as word clouds, is comprehensible, representative, and specific to humans",
    "checked": true,
    "id": "38cd1136bf2630f5d2e3d62638ee8726eb672e79",
    "semantic_title": "semantic frame forecast",
    "citation_count": 6,
    "authors": [
      "Chieh-Yang Huang",
      "Ting-Hao Huang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.216": {
    "title": "MUSER: MUltimodal Stress detection using Emotion Recognition as an Auxiliary Task",
    "volume": "main",
    "abstract": "The capability to automatically detect human stress can benefit artificial intelligent agents involved in affective computing and human-computer interaction. Stress and emotion are both human affective states, and stress has proven to have important implications on the regulation and expression of emotion. Although a series of methods have been established for multimodal stress detection, limited steps have been taken to explore the underlying inter-dependence between stress and emotion. In this work, we investigate the value of emotion recognition as an auxiliary task to improve stress detection. We propose MUSER – a transformer-based model architecture and a novel multi-task learning algorithm with speed-based dynamic sampling strategy. Evaluation on the Multimodal Stressed Emotion (MuSE) dataset shows that our model is effective for stress detection with both internal and external auxiliary tasks, and achieves state-of-the-art results",
    "checked": true,
    "id": "de57428068ef955924172e9d2b8008981dac6eab",
    "semantic_title": "muser: multimodal stress detection using emotion recognition as an auxiliary task",
    "citation_count": 10,
    "authors": [
      "Yiqun Yao",
      "Michalis Papakostas",
      "Mihai Burzo",
      "Mohamed Abouelenien",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.217": {
    "title": "Learning to Decompose and Organize Complex Tasks",
    "volume": "main",
    "abstract": "People rely on digital task management tools, such as email or to-do apps, to manage their tasks. Some of these tasks are large and complex, leading to action paralysis and feelings of being overwhelmed on the part of the user. The micro-productivity literature has shown that such tasks could benefit from being decomposed and organized, in order to reduce user cognitive load. Thus in this paper, we propose a novel end-to-end pipeline that consumes a complex task and induces a dependency graph from unstructured text to represent sub-tasks and their relationships. Our solution first finds nodes for sub-tasks from multiple ‘how-to' articles on the web by injecting a neural text generator with three key desiderata – relevance, abstraction, and consensus. Then we resolve and infer edges between these subtask nodes by learning task dependency relations. We collect a new dataset of complex tasks with their sub-task graph to develop and evaluate our solutions. Both components of our graph induction solution are evaluated in experiments, demonstrating that our models outperform a state-of-the-art text generator significantly. Our generalizable and scalable end-to-end solution has important implications for boosting user productivity and assisting with digital task management",
    "checked": true,
    "id": "c58269be7bb0b7c896e5d74bfdcfb023c6787cf6",
    "semantic_title": "learning to decompose and organize complex tasks",
    "citation_count": 28,
    "authors": [
      "Yi Zhang",
      "Sujay Kumar Jauhar",
      "Julia Kiseleva",
      "Ryen White",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.218": {
    "title": "Continual Learning for Text Classification with Information Disentanglement Based Regularization",
    "volume": "main",
    "abstract": "Continual learning has become increasingly important as it enables NLP models to constantly learn and gain knowledge over time. Previous continual learning methods are mainly designed to preserve knowledge from previous tasks, without much emphasis on how to well generalize models to new tasks. In this work, we propose an information disentanglement based regularization method for continual learning on text classification. Our proposed method first disentangles text hidden spaces into representations that are generic to all tasks and representations specific to each individual task, and further regularizes these representations differently to better constrain the knowledge required to generalize. We also introduce two simple auxiliary tasks: next sentence prediction and task-id prediction, for learning better generic and specific representation spaces. Experiments conducted on large-scale benchmarks demonstrate the effectiveness of our method in continual text classification tasks with various sequences and lengths over state-of-the-art baselines. We have publicly released our code at https://github.com/GT-SALT/IDBR",
    "checked": true,
    "id": "677a7a940dcff639bd066f25b395a361a08a60f9",
    "semantic_title": "continual learning for text classification with information disentanglement based regularization",
    "citation_count": 80,
    "authors": [
      "Yufan Huang",
      "Yanzhe Zhang",
      "Jiaao Chen",
      "Xuezhi Wang",
      "Diyi Yang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.219": {
    "title": "Learning from Executions for Semantic Parsing",
    "volume": "main",
    "abstract": "Semantic parsing aims at translating natural language (NL) utterances onto machine-interpretable programs, which can be executed against a real-world environment. The expensive annotation of utterance-program pairs has long been acknowledged as a major bottleneck for the deployment of contemporary neural models to real-life applications. In this work, we focus on the task of semi-supervised learning where a limited amount of annotated data is available together with many unlabeled NL utterances. Based on the observation that programs which correspond to NL utterances should always be executable, we propose to encourage a parser to generate executable programs for unlabeled utterances. Due to the large search space of executable programs, conventional methods that use beam-search for approximation, such as self-training and top-k marginal likelihood training, do not perform as well. Instead, we propose a set of new training objectives that are derived by approaching the problem of learning from executions from the posterior regularization perspective. Our new objectives outperform conventional methods on Overnight and GeoQuery, bridging the gap between semi-supervised and supervised learning",
    "checked": true,
    "id": "f825431db6e83c1ba3f485f2030682755325ccef",
    "semantic_title": "learning from executions for semantic parsing",
    "citation_count": 8,
    "authors": [
      "Bailin Wang",
      "Mirella Lapata",
      "Ivan Titov"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.220": {
    "title": "Learning to Synthesize Data for Semantic Parsing",
    "volume": "main",
    "abstract": "Synthesizing data for semantic parsing has gained increasing attention recently. However, most methods require handcrafted (high-precision) rules in their generative process, hindering the exploration of diverse unseen data. In this work, we propose a generative model which features a (non-neural) PCFG that models the composition of programs (e.g., SQL), and a BART-based translation model that maps a program to an utterance. Due to the simplicity of PCFG and pre-trained BART, our generative model can be efficiently learned from existing data at hand. Moreover, explicitly modeling compositions using PCFG leads to better exploration of unseen programs, thus generate more diverse data. We evaluate our method in both in-domain and out-of-domain settings of text-to-SQL parsing on the standard benchmarks of GeoQuery and Spider, respectively. Our empirical results show that the synthesized data generated from our model can substantially help a semantic parser achieve better compositional and domain generalization",
    "checked": true,
    "id": "afd8fb26a7094ed3a8838000d50e2b22f815dc28",
    "semantic_title": "learning to synthesize data for semantic parsing",
    "citation_count": 44,
    "authors": [
      "Bailin Wang",
      "Wenpeng Yin",
      "Xi Victoria Lin",
      "Caiming Xiong"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.221": {
    "title": "Edge: Enriching Knowledge Graph Embeddings with External Text",
    "volume": "main",
    "abstract": "Knowledge graphs suffer from sparsity which degrades the quality of representations generated by various methods. While there is an abundance of textual information throughout the web and many existing knowledge bases, aligning information across these diverse data sources remains a challenge in the literature. Previous work has partially addressed this issue by enriching knowledge graph entities based on \"hard\" co-occurrence of words present in the entities of the knowledge graphs and external text, while we achieve \"soft\" augmentation by proposing a knowledge graph enrichment and embedding framework named Edge. Given an original knowledge graph, we first generate a rich but noisy augmented graph using external texts in semantic and structural level. To distill the relevant knowledge and suppress the introduced noise, we design a graph alignment term in a shared embedding space between the original graph and augmented graph. To enhance the embedding learning on the augmented graph, we further regularize the locality relationship of target entity based on negative sampling. Experimental results on four benchmark datasets demonstrate the robustness and effectiveness of Edge in link prediction and node classification",
    "checked": true,
    "id": "182f055c40bfcff45d0ce6eb68e2d8684d0931dc",
    "semantic_title": "edge: enriching knowledge graph embeddings with external text",
    "citation_count": 27,
    "authors": [
      "Saed Rezayi",
      "Handong Zhao",
      "Sungchul Kim",
      "Ryan Rossi",
      "Nedim Lipka",
      "Sheng Li"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.222": {
    "title": "FLIN: A Flexible Natural Language Interface for Web Navigation",
    "volume": "main",
    "abstract": "AI assistants can now carry out tasks for users by directly interacting with website UIs. Current semantic parsing and slot-filling techniques cannot flexibly adapt to many different websites without being constantly re-trained. We propose FLIN, a natural language interface for web navigation that maps user commands to concept-level actions (rather than low-level UI actions), thus being able to flexibly adapt to different websites and handle their transient nature. We frame this as a ranking problem: given a user command and a webpage, FLIN learns to score the most relevant navigation instruction (involving action and parameter values). To train and evaluate FLIN, we collect a dataset using nine popular websites from three domains. Our results show that FLIN was able to adapt to new websites in a given domain",
    "checked": true,
    "id": "73d5cbbf50143b20d3f6552c68833353ff989826",
    "semantic_title": "flin: a flexible natural language interface for web navigation",
    "citation_count": 19,
    "authors": [
      "Sahisnu Mazumder",
      "Oriana Riva"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.223": {
    "title": "Game-theoretic Vocabulary Selection via the Shapley Value and Banzhaf Index",
    "volume": "main",
    "abstract": "The input vocabulary and the representations learned are crucial to the performance of neural NLP models. Using the full vocabulary results in less explainable and more memory intensive models, with the embedding layer often constituting the majority of model parameters. It is thus common to use a smaller vocabulary to lower memory requirements and construct more interpertable models. We propose a vocabulary selection method that views words as members of a team trying to maximize the model's performance. We apply power indices from cooperative game theory, including the Shapley value and Banzhaf index, that measure the relative importance of individual team members in accomplishing a joint task. We approximately compute these indices to identify the most influential words. Our empirical evaluation examines multiple NLP tasks, including sentence and document classification, question answering and textual entailment. We compare to baselines that select words based on frequency, TF-IDF and regression coefficients under L1 regularization, and show that this game-theoretic vocabulary selection outperforms all baseline on a range of different tasks and datasets",
    "checked": true,
    "id": "705bc908fe5f4962e0b2251ffb2f42a91d9eec4c",
    "semantic_title": "game-theoretic vocabulary selection via the shapley value and banzhaf index",
    "citation_count": 14,
    "authors": [
      "Roma Patel",
      "Marta Garnelo",
      "Ian Gemp",
      "Chris Dyer",
      "Yoram Bachrach"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.224": {
    "title": "Incorporating External Knowledge to Enhance Tabular Reasoning",
    "volume": "main",
    "abstract": "Reasoning about tabular information presents unique challenges to modern NLP approaches which largely rely on pre-trained contextualized embeddings of text. In this paper, we study these challenges through the problem of tabular natural language inference. We propose easy and effective modifications to how information is presented to a model for this task. We show via systematic experiments that these strategies substantially improve tabular inference performance",
    "checked": true,
    "id": "99be5048b4d7c6b018dd36c6c5940b98487e074e",
    "semantic_title": "incorporating external knowledge to enhance tabular reasoning",
    "citation_count": 33,
    "authors": [
      "J. Neeraja",
      "Vivek Gupta",
      "Vivek Srikumar"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.225": {
    "title": "Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention",
    "volume": "main",
    "abstract": "We describe a span-level supervised attention loss that improves compositional generalization in semantic parsers. Our approach builds on existing losses that encourage attention maps in neural sequence-to-sequence models to imitate the output of classical word alignment algorithms. Where past work has used word-level alignments, we focus on spans; borrowing ideas from phrase-based machine translation, we align subtrees in semantic parses to spans of input sentences, and encourage neural attention mechanisms to mimic these alignments. This method improves the performance of transformers, RNNs, and structured decoders on three benchmarks of compositional generalization",
    "checked": true,
    "id": "88dbde378e9ac5c25fc7d78f5da147223e8d34d4",
    "semantic_title": "compositional generalization for neural semantic parsing via span-level supervised attention",
    "citation_count": 57,
    "authors": [
      "Pengcheng Yin",
      "Hao Fang",
      "Graham Neubig",
      "Adam Pauls",
      "Emmanouil Antonios Platanios",
      "Yu Su",
      "Sam Thomson",
      "Jacob Andreas"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.226": {
    "title": "Domain Adaptation for Arabic Cross-Domain and Cross-Dialect Sentiment Analysis from Contextualized Word Embedding",
    "volume": "main",
    "abstract": "Finetuning deep pre-trained language models has shown state-of-the-art performances on a wide range of Natural Language Processing (NLP) applications. Nevertheless, their generalization performance drops under domain shift. In the case of Arabic language, diglossia makes building and annotating corpora for each dialect and/or domain a more challenging task. Unsupervised Domain Adaptation tackles this issue by transferring the learned knowledge from labeled source domain data to unlabeled target domain data. In this paper, we propose a new unsupervised domain adaptation method for Arabic cross-domain and cross-dialect sentiment analysis from Contextualized Word Embedding. Several experiments are performed adopting the coarse-grained and the fine-grained taxonomies of Arabic dialects. The obtained results show that our method yields very promising results and outperforms several domain adaptation methods for most of the evaluated datasets. On average, our method increases the performance by an improvement rate of 20.8% over the zero-shot transfer learning from BERT",
    "checked": true,
    "id": "68713fdc2b2e7182548c8d7d3a247bddb79c51b4",
    "semantic_title": "domain adaptation for arabic cross-domain and cross-dialect sentiment analysis from contextualized word embedding",
    "citation_count": 27,
    "authors": [
      "Abdellah El Mekki",
      "Abdelkader El Mahdaouy",
      "Ismail Berrada",
      "Ahmed Khoumsi"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.227": {
    "title": "Multi-task Learning of Negation and Speculation for Targeted Sentiment Classification",
    "volume": "main",
    "abstract": "The majority of work in targeted sentiment analysis has concentrated on finding better methods to improve the overall results. Within this paper we show that these models are not robust to linguistic phenomena, specifically negation and speculation. In this paper, we propose a multi-task learning method to incorporate information from syntactic and semantic auxiliary tasks, including negation and speculation scope detection, to create English-language models that are more robust to these phenomena. Further we create two challenge datasets to evaluate model performance on negated and speculative samples. We find that multi-task models and transfer learning via language modelling can improve performance on these challenge datasets, but the overall performances indicate that there is still much room for improvement. We release both the datasets and the source code at https://github.com/jerbarnes/multitask_negation_for_targeted_sentiment",
    "checked": true,
    "id": "59a726d18584af0abc4fefbfe3b9b2c2cc424592",
    "semantic_title": "multi-task learning of negation and speculation for targeted sentiment classification",
    "citation_count": 9,
    "authors": [
      "Andrew Moore",
      "Jeremy Barnes"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.228": {
    "title": "A Disentangled Adversarial Neural Topic Model for Separating Opinions from Plots in User Reviews",
    "volume": "main",
    "abstract": "The flexibility of the inference process in Variational Autoencoders (VAEs) has recently led to revising traditional probabilistic topic models giving rise to Neural Topic Models (NTM). Although these approaches have achieved significant results, surprisingly very little work has been done on how to disentangle the latent topics. Existing topic models when applied to reviews may extract topics associated with writers' subjective opinions mixed with those related to factual descriptions such as plot summaries in movie and book reviews. It is thus desirable to automatically separate opinion topics from plot/neutral ones enabling a better interpretability. In this paper, we propose a neural topic model combined with adversarial training to disentangle opinion topics from plot and neutral ones. We conduct an extensive experimental assessment introducing a new collection of movie and book reviews paired with their plots, namely MOBO dataset, showing an improved coherence and variety of topics, a consistent disentanglement rate, and sentiment classification performance superior to other supervised topic models",
    "checked": true,
    "id": "bd446e502b2e8c8d289bf767189c0e2a34e7fc34",
    "semantic_title": "a disentangled adversarial neural topic model for separating opinions from plots in user reviews",
    "citation_count": 11,
    "authors": [
      "Gabriele Pergola",
      "Lin Gui",
      "Yulan He"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.229": {
    "title": "Graph Ensemble Learning over Multiple Dependency Trees for Aspect-level Sentiment Classification",
    "volume": "main",
    "abstract": "Recent work on aspect-level sentiment classification has demonstrated the efficacy of incorporating syntactic structures such as dependency trees with graph neural networks (GNN), but these approaches are usually vulnerable to parsing errors. To better leverage syntactic information in the face of unavoidable errors, we propose a simple yet effective graph ensemble technique, GraphMerge, to make use of the predictions from different parsers. Instead of assigning one set of model parameters to each dependency tree, we first combine the dependency relations from different parses before applying GNNs over the resulting graph. This allows GNN models to be robust to parse errors at no additional computational cost, and helps avoid overparameterization and overfitting from GNN layer stacking by introducing more connectivity into the ensemble graph. Our experiments on the SemEval 2014 Task 4 and ACL 14 Twitter datasets show that our GraphMerge model not only outperforms models with single dependency tree, but also beats other ensemble models without adding model parameters",
    "checked": true,
    "id": "e69baffcf8853efa627c917f8c57c33b27572ee8",
    "semantic_title": "graph ensemble learning over multiple dependency trees for aspect-level sentiment classification",
    "citation_count": 50,
    "authors": [
      "Xiaochen Hou",
      "Peng Qi",
      "Guangtao Wang",
      "Rex Ying",
      "Jing Huang",
      "Xiaodong He",
      "Bowen Zhou"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.230": {
    "title": "Emotion-Infused Models for Explainable Psychological Stress Detection",
    "volume": "main",
    "abstract": "The problem of detecting psychological stress in online posts, and more broadly, of detecting people in distress or in need of help, is a sensitive application for which the ability to interpret models is vital. Here, we present work exploring the use of a semantically related task, emotion detection, for equally competent but more explainable and human-like psychological stress detection as compared to a black-box model. In particular, we explore the use of multi-task learning as well as emotion-based language model fine-tuning. With our emotion-infused models, we see comparable results to state-of-the-art BERT. Our analysis of the words used for prediction show that our emotion-infused models mirror psychological components of stress",
    "checked": true,
    "id": "950c1f16a846562890f999775c5f3678376591af",
    "semantic_title": "emotion-infused models for explainable psychological stress detection",
    "citation_count": 37,
    "authors": [
      "Elsbeth Turcan",
      "Smaranda Muresan",
      "Kathleen McKeown"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.231": {
    "title": "Aspect-based Sentiment Analysis with Type-aware Graph Convolutional Networks and Layer Ensemble",
    "volume": "main",
    "abstract": "It is popular that neural graph-based models are applied in existing aspect-based sentiment analysis (ABSA) studies for utilizing word relations through dependency parses to facilitate the task with better semantic guidance for analyzing context and aspect words. However, most of these studies only leverage dependency relations without considering their dependency types, and are limited in lacking efficient mechanisms to distinguish the important relations as well as learn from different layers of graph based models. To address such limitations, in this paper, we propose an approach to explicitly utilize dependency types for ABSA with type-aware graph convolutional networks (T-GCN), where attention is used in T-GCN to distinguish different edges (relations) in the graph and attentive layer ensemble is proposed to comprehensively learn from different layers of T-GCN. The validity and effectiveness of our approach are demonstrated in the experimental results, where state-of-the-art performance is achieved on six English benchmark datasets. Further experiments are conducted to analyze the contributions of each component in our approach and illustrate how different layers in T-GCN help ABSA with quantitative and qualitative analysis",
    "checked": true,
    "id": "0699bbbc1eaafd557de2f853ebec65185dbd4f9b",
    "semantic_title": "aspect-based sentiment analysis with type-aware graph convolutional networks and layer ensemble",
    "citation_count": 130,
    "authors": [
      "Yuanhe Tian",
      "Guimin Chen",
      "Yan Song"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.232": {
    "title": "Supertagging-based Parsing with Linear Context-free Rewriting Systems",
    "volume": "main",
    "abstract": "We present the first supertagging-based parser for linear context-free rewriting systems (LCFRS). It utilizes neural classifiers and outperforms previous LCFRS-based parsers in both accuracy and parsing speed by a wide margin. Our results keep up with the best (general) discontinuous parsers, particularly the scores for discontinuous constituents establish a new state of the art. The heart of our approach is an efficient lexicalization procedure which induces a lexical LCFRS from any discontinuous treebank. We describe a modification to usual chart-based LCFRS parsing that accounts for supertagging and introduce a procedure that transforms lexical LCFRS derivations into equivalent parse trees of the original treebank. Our approach is evaluated on the English Discontinuous Penn Treebank and the German treebanks Negra and Tiger",
    "checked": true,
    "id": "a3e8d055acfe987bf81c37d17ceb83f876c9cf15",
    "semantic_title": "supertagging-based parsing with linear context-free rewriting systems",
    "citation_count": 9,
    "authors": [
      "Thomas Ruprecht",
      "Richard Mörbitz"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.233": {
    "title": "Outside Computation with Superior Functions",
    "volume": "main",
    "abstract": "We show that a general algorithm for efficient computation of outside values under the minimum of superior functions framework proposed by Knuth (1977) would yield a sub-exponential time algorithm for SAT, violating the Strong Exponential Time Hypothesis (SETH)",
    "checked": true,
    "id": "13d89a835fdce54fbc74b452e1ba9613db035024",
    "semantic_title": "outside computation with superior functions",
    "citation_count": 0,
    "authors": [
      "Parker Riley",
      "Daniel Gildea"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.234": {
    "title": "Learning Syntax from Naturally-Occurring Bracketings",
    "volume": "main",
    "abstract": "Naturally-occurring bracketings, such as answer fragments to natural language questions and hyperlinks on webpages, can reflect human syntactic intuition regarding phrasal boundaries. Their availability and approximate correspondence to syntax make them appealing as distant information sources to incorporate into unsupervised constituency parsing. But they are noisy and incomplete; to address this challenge, we develop a partial-brackets-aware structured ramp loss in learning. Experiments demonstrate that our distantly-supervised models trained on naturally-occurring bracketing data are more accurate in inducing syntactic structures than competing unsupervised systems. On the English WSJ corpus, our models achieve an unlabeled F1 score of 68.9 for constituency parsing",
    "checked": true,
    "id": "90aeb070f8cecd9ed819fc0f35d9de2071650551",
    "semantic_title": "learning syntax from naturally-occurring bracketings",
    "citation_count": 5,
    "authors": [
      "Tianze Shi",
      "Ozan İrsoy",
      "Igor Malioutov",
      "Lillian Lee"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.235": {
    "title": "Bot-Adversarial Dialogue for Safe Conversational Agents",
    "volume": "main",
    "abstract": "Conversational agents trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which include offensive or otherwise toxic behavior. We introduce a new human-and-model-in-the-loop framework for evaluating the toxicity of such models, and compare a variety of existing methods in both the cases of non-adversarial and adversarial users that expose their weaknesses. We then go on to propose two novel methods for safe conversational agents, by either training on data from our new human-and-model-in-the-loop framework in a two-stage system, or \"baking-in\" safety to the generative model itself. We find our new techniques are (i) safer than existing models; while (ii) maintaining usability metrics such as engagingness relative to state-of-the-art chatbots. In contrast, we expose serious safety issues in existing standard systems like GPT2, DialoGPT, and BlenderBot",
    "checked": true,
    "id": "041c4a3932eae3931d90b1a0fc033cf5f491c267",
    "semantic_title": "bot-adversarial dialogue for safe conversational agents",
    "citation_count": 120,
    "authors": [
      "Jing Xu",
      "Da Ju",
      "Margaret Li",
      "Y-Lan Boureau",
      "Jason Weston",
      "Emily Dinan"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.236": {
    "title": "Non-Autoregressive Semantic Parsing for Compositional Task-Oriented Dialog",
    "volume": "main",
    "abstract": "Semantic parsing using sequence-to-sequence models allows parsing of deeper representations compared to traditional word tagging based models. In spite of these advantages, widespread adoption of these models for real-time conversational use cases has been stymied by higher compute requirements and thus higher latency. In this work, we propose a non-autoregressive approach to predict semantic parse trees with an efficient seq2seq model architecture. By combining non-autoregressive prediction with convolutional neural networks, we achieve significant latency gains and parameter size reduction compared to traditional RNN models. Our novel architecture achieves up to an 81% reduction in latency on TOP dataset and retains competitive performance to non-pretrained models on three different semantic parsing datasets",
    "checked": true,
    "id": "13e538b94a1d97d0154709a977cca9352fee428d",
    "semantic_title": "non-autoregressive semantic parsing for compositional task-oriented dialog",
    "citation_count": 21,
    "authors": [
      "Arun Babu",
      "Akshat Shrivastava",
      "Armen Aghajanyan",
      "Ahmed Aly",
      "Angela Fan",
      "Marjan Ghazvininejad"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.237": {
    "title": "Example-Driven Intent Prediction with Observers",
    "volume": "main",
    "abstract": "A key challenge of dialog systems research is to effectively and efficiently adapt to new domains. A scalable paradigm for adaptation necessitates the development of generalizable models that perform well in few-shot settings. In this paper, we focus on the intent classification problem which aims to identify user intents given utterances addressed to the dialog system. We propose two approaches for improving the generalizability of utterance classification models: (1) observers and (2) example-driven training. Prior work has shown that BERT-like models tend to attribute a significant amount of attention to the [CLS] token, which we hypothesize results in diluted representations. Observers are tokens that are not attended to, and are an alternative to the [CLS] token as a semantic representation of utterances. Example-driven training learns to classify utterances by comparing to examples, thereby using the underlying encoder as a sentence similarity model. These methods are complementary; improving the representation through observers allows the example-driven model to better measure sentence similarities. When combined, the proposed methods attain state-of-the-art results on three intent prediction datasets (banking77, clinc150, hwu64) in both the full data and few-shot (10 examples per intent) settings. Furthermore, we demonstrate that the proposed approach can transfer to new intents and across datasets without any additional training",
    "checked": true,
    "id": "236df68e8c130eac6332aff1b158a251bab5b9c7",
    "semantic_title": "example-driven intent prediction with observers",
    "citation_count": 34,
    "authors": [
      "Shikib Mehri",
      "Mihail Eric"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.238": {
    "title": "Imperfect also Deserves Reward: Multi-Level and Sequential Reward Modeling for Better Dialog Management",
    "volume": "main",
    "abstract": "For task-oriented dialog systems, training a Reinforcement Learning (RL) based Dialog Management module suffers from low sample efficiency and slow convergence speed due to the sparse rewards in RL. To solve this problem, many strategies have been proposed to give proper rewards when training RL, but their rewards lack interpretability and cannot accurately estimate the distribution of state-action pairs in real dialogs. In this paper, we propose a multi-level reward modeling approach that factorizes a reward into a three-level hierarchy: domain, act, and slot. Based on inverse adversarial reinforcement learning, our designed reward model can provide more accurate and explainable reward signals for state-action pairs. Extensive evaluations show that our approach can be applied to a wide range of reinforcement learning-based dialog systems and significantly improves both the performance and the speed of convergence",
    "checked": true,
    "id": "dfea422a6e4a251a0404eacd7929ed234adf9ba0",
    "semantic_title": "imperfect also deserves reward: multi-level and sequential reward modeling for better dialog management",
    "citation_count": 8,
    "authors": [
      "Zhengxu Hou",
      "Bang Liu",
      "Ruihui Zhao",
      "Zijing Ou",
      "Yafei Liu",
      "Xi Chen",
      "Yefeng Zheng"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.239": {
    "title": "Action-Based Conversations Dataset: A Corpus for Building More In-Depth Task-Oriented Dialogue Systems",
    "volume": "main",
    "abstract": "Existing goal-oriented dialogue datasets focus mainly on identifying slots and values. However, customer support interactions in reality often involve agents following multi-step procedures derived from explicitly-defined company policies as well. To study customer service dialogue systems in more realistic settings, we introduce the Action-Based Conversations Dataset (ABCD), a fully-labeled dataset with over 10K human-to-human dialogues containing 55 distinct user intents requiring unique sequences of actions constrained by policies to achieve task success. We propose two additional dialog tasks, Action State Tracking and Cascading Dialogue Success, and establish a series of baselines involving large-scale, pre-trained language models on this dataset. Empirical results demonstrate that while more sophisticated networks outperform simpler models, a considerable gap (50.8% absolute accuracy) still exists to reach human-level performance on ABCD",
    "checked": true,
    "id": "8c0ec929aac2eb67d36f383f3175f22a1b285264",
    "semantic_title": "action-based conversations dataset: a corpus for building more in-depth task-oriented dialogue systems",
    "citation_count": 53,
    "authors": [
      "Derek Chen",
      "Howard Chen",
      "Yi Yang",
      "Alexander Lin",
      "Zhou Yu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.240": {
    "title": "Controlling Dialogue Generation with Semantic Exemplars",
    "volume": "main",
    "abstract": "Dialogue systems pretrained with large language models generate locally coherent responses, but lack fine-grained control over responses necessary to achieve specific goals. A promising method to control response generation is exemplar-based generation, in which models edit exemplar responses that are retrieved from training data, or hand-written to strategically address discourse-level goals, to fit new dialogue contexts. We present an Exemplar-based Dialogue Generation model, EDGE, that uses the semantic frames present in exemplar responses to guide response generation. We show that controlling dialogue generation based on the semantic frames of exemplars improves the coherence of generated responses, while preserving semantic meaning and conversation goals present in exemplar responses",
    "checked": true,
    "id": "3d6dd77f4cd509f050b2d4ca08c1d9e072acc065",
    "semantic_title": "controlling dialogue generation with semantic exemplars",
    "citation_count": 35,
    "authors": [
      "Prakhar Gupta",
      "Jeffrey Bigham",
      "Yulia Tsvetkov",
      "Amy Pavel"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.241": {
    "title": "COIL: Revisit Exact Lexical Match in Information Retrieval with Contextualized Inverted List",
    "volume": "main",
    "abstract": "Classical information retrieval systems such as BM25 rely on exact lexical match and can carry out search efficiently with inverted list index. Recent neural IR models shifts towards soft matching all query document terms, but they lose the computation efficiency of exact match systems. This paper presents COIL, a contextualized exact match retrieval architecture, where scoring is based on overlapping query document tokens' contextualized representations. The new architecture stores contextualized token representations in inverted lists, bringing together the efficiency of exact match and the representation power of deep language models. Our experimental results show COIL outperforms classical lexical retrievers and state-of-the-art deep LM retrievers with similar or smaller latency",
    "checked": true,
    "id": "2d7a784a093615d00d4ac0a7b5763a15d86d4996",
    "semantic_title": "coil: revisit exact lexical match in information retrieval with contextualized inverted list",
    "citation_count": 188,
    "authors": [
      "Luyu Gao",
      "Zhuyun Dai",
      "Jamie Callan"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.242": {
    "title": "X-Class: Text Classification with Extremely Weak Supervision",
    "volume": "main",
    "abstract": "In this paper, we explore text classification with extremely weak supervision, i.e., only relying on the surface text of class names. This is a more challenging setting than the seed-driven weak supervision, which allows a few seed words per class. We opt to attack this problem from a representation learning perspective—ideal document representations should lead to nearly the same results between clustering and the desired classification. In particular, one can classify the same corpus differently (e.g., based on topics and locations), so document representations should be adaptive to the given class names. We propose a novel framework X-Class to realize the adaptive representations. Specifically, we first estimate class representations by incrementally adding the most similar word to each class until inconsistency arises. Following a tailored mixture of class attention mechanisms, we obtain the document representation via a weighted average of contextualized word representations. With the prior of each document assigned to its nearest class, we then cluster and align the documents to classes. Finally, we pick the most confident documents from each cluster to train a text classifier. Extensive experiments demonstrate that X-Class can rival and even outperform seed-driven weakly supervised methods on 7 benchmark datasets",
    "checked": true,
    "id": "84cc15cf63cb9856653e1bbf46d132abfbb97a13",
    "semantic_title": "x-class: text classification with extremely weak supervision",
    "citation_count": 70,
    "authors": [
      "Zihan Wang",
      "Dheeraj Mekala",
      "Jingbo Shang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.243": {
    "title": "Fine-tuning Encoders for Improved Monolingual and Zero-shot Polylingual Neural Topic Modeling",
    "volume": "main",
    "abstract": "Neural topic models can augment or replace bag-of-words inputs with the learned representations of deep pre-trained transformer-based word prediction models. One added benefit when using representations from multilingual models is that they facilitate zero-shot polylingual topic modeling. However, while it has been widely observed that pre-trained embeddings should be fine-tuned to a given task, it is not immediately clear what supervision should look like for an unsupervised task such as topic modeling. Thus, we propose several methods for fine-tuning encoders to improve both monolingual and zero-shot polylingual neural topic modeling. We consider fine-tuning on auxiliary tasks, constructing a new topic classification task, integrating the topic classification objective directly into topic model training, and continued pre-training. We find that fine-tuning encoder representations on topic classification and integrating the topic classification task directly into topic modeling improves topic quality, and that fine-tuning encoder representations on any task is the most important factor for facilitating cross-lingual transfer",
    "checked": true,
    "id": "6a6b66a670e1a46e4368229ee0d3109ed2df0533",
    "semantic_title": "fine-tuning encoders for improved monolingual and zero-shot polylingual neural topic modeling",
    "citation_count": 11,
    "authors": [
      "Aaron Mueller",
      "Mark Dredze"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.244": {
    "title": "Exploring the Relationship Between Algorithm Performance, Vocabulary, and Run-Time in Text Classification",
    "volume": "main",
    "abstract": "Text classification is a significant branch of natural language processing, and has many applications including document classification and sentiment analysis. Unsurprisingly, those who do text classification are concerned with the run-time of their algorithms, many of which depend on the size of the corpus' vocabulary due to their bag-of-words representation. Although many studies have examined the effect of preprocessing techniques on vocabulary size and accuracy, none have examined how these methods affect a model's run-time. To fill this gap, we provide a comprehensive study that examines how preprocessing techniques affect the vocabulary size, model performance, and model run-time, evaluating ten techniques over four models and two datasets. We show that some individual methods can reduce run-time with no loss of accuracy, while some combinations of methods can trade 2-5% of the accuracy for up to a 65% reduction of run-time. Furthermore, some combinations of preprocessing techniques can even provide a 15% reduction in run-time while simultaneously improving model accuracy",
    "checked": true,
    "id": "6325de3a35df5a5981feffcf421108854a892108",
    "semantic_title": "exploring the relationship between algorithm performance, vocabulary, and run-time in text classification",
    "citation_count": 0,
    "authors": [
      "Wilson Fearn",
      "Orion Weller",
      "Kevin Seppi"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.245": {
    "title": "Faithfully Explainable Recommendation via Neural Logic Reasoning",
    "volume": "main",
    "abstract": "Knowledge graphs (KG) have become increasingly important to endow modern recommender systems with the ability to generate traceable reasoning paths to explain the recommendation process. However, prior research rarely considers the faithfulness of the derived explanations to justify the decision-making process. To the best of our knowledge, this is the first work that models and evaluates faithfully explainable recommendation under the framework of KG reasoning. Specifically, we propose neural logic reasoning for explainable recommendation (LOGER) by drawing on interpretable logical rules to guide the path-reasoning process for explanation generation. We experiment on three large-scale datasets in the e-commerce domain, demonstrating the effectiveness of our method in delivering high-quality recommendations as well as ascertaining the faithfulness of the derived explanation",
    "checked": true,
    "id": "65e42c8ed4f87c72f4cfca43210686472d503485",
    "semantic_title": "faithfully explainable recommendation via neural logic reasoning",
    "citation_count": 28,
    "authors": [
      "Yaxin Zhu",
      "Yikun Xian",
      "Zuohui Fu",
      "Gerard de Melo",
      "Yongfeng Zhang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.246": {
    "title": "You Sound Like Someone Who Watches Drama Movies: Towards Predicting Movie Preferences from Conversational Interactions",
    "volume": "main",
    "abstract": "The increasing popularity of voice-based personal assistants provides new opportunities for conversational recommendation. One particularly interesting area is movie recommendation, which can benefit from an open-ended interaction with the user, through a natural conversation. We explore one promising direction for conversational recommendation: mapping a conversational user, for whom there is limited or no data available, to most similar external reviewers, whose preferences are known, by representing the conversation as a user's interest vector, and adapting collaborative filtering techniques to estimate the current user's preferences for new movies. We call our proposed method ConvExtr (Conversational Collaborative Filtering using External Data), which 1) infers a user's sentiment towards an entity from the conversation context, and 2) transforms the ratings of \"similar\" external reviewers to predict the current user's preferences. We implement these steps by adapting contextual sentiment prediction techniques, and domain adaptation, respectively. To evaluate our method, we develop and make available a finely annotated dataset of movie recommendation conversations, which we call MovieSent. Our results demonstrate that ConvExtr can improve the accuracy of predicting users' ratings for new movies by exploiting conversation content and external data",
    "checked": true,
    "id": "be475ab89ce82ed177125f3d0c3fd316577f4e6f",
    "semantic_title": "you sound like someone who watches drama movies: towards predicting movie preferences from conversational interactions",
    "citation_count": 2,
    "authors": [
      "Sergey Volokhin",
      "Joyce Ho",
      "Oleg Rokhlenko",
      "Eugene Agichtein"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.247": {
    "title": "Reading and Acting while Blindfolded: The Need for Semantics in Text Game Agents",
    "volume": "main",
    "abstract": "Text-based games simulate worlds and interact with players using natural language. Recent work has used them as a testbed for autonomous language-understanding agents, with the motivation being that understanding the meanings of words or semantics is a key component of how humans understand, reason, and act in these worlds. However, it remains unclear to what extent artificial agents utilize semantic understanding of the text. To this end, we perform experiments to systematically reduce the amount of semantic information available to a learning agent. Surprisingly, we find that an agent is capable of achieving high scores even in the complete absence of language semantics, indicating that the currently popular experimental setup and models may be poorly designed to understand and leverage game texts. To remedy this deficiency, we propose an inverse dynamics decoder to regularize the representation space and encourage exploration, which shows improved performance on several games including Zork I. We discuss the implications of our findings for designing future agents with stronger semantic understanding",
    "checked": true,
    "id": "e6833fc4cadea279e824924c9a4efd1e89e633e3",
    "semantic_title": "reading and acting while blindfolded: the need for semantics in text game agents",
    "citation_count": 24,
    "authors": [
      "Shunyu Yao",
      "Karthik Narasimhan",
      "Matthew Hausknecht"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.248": {
    "title": "SOrT-ing VQA Models : Contrastive Gradient Learning for Improved Consistency",
    "volume": "main",
    "abstract": "Recent research in Visual Question Answering (VQA) has revealed state-of-the-art models to be inconsistent in their understanding of the world - they answer seemingly difficult questions requiring reasoning correctly but get simpler associated sub-questions wrong. These sub-questions pertain to lower level visual concepts in the image that models ideally should understand to be able to answer the reasoning question correctly. To address this, we first present a gradient-based interpretability approach to determine the questions most strongly correlated with the reasoning question on an image, and use this to evaluate VQA models on their ability to identify the relevant sub-questions needed to answer a reasoning question. Next, we propose a contrastive gradient learning based approach called Sub-question Oriented Tuning (SOrT) which encourages models to rank relevant sub-questions higher than irrelevant questions for an <image, reasoning-question> pair. We show that SOrT improves model consistency by up to 6.5% points over existing approaches, while also improving visual grounding and robustness to rephrasings of questions",
    "checked": true,
    "id": "12a2e4f53668de97ef7a4bbb4661461340f87936",
    "semantic_title": "sort-ing vqa models : contrastive gradient learning for improved consistency",
    "citation_count": 2,
    "authors": [
      "Sameer Dharur",
      "Purva Tendulkar",
      "Dhruv Batra",
      "Devi Parikh",
      "Ramprasaath R. Selvaraju"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.249": {
    "title": "Semi-Supervised Policy Initialization for Playing Games with Language Hints",
    "volume": "main",
    "abstract": "Using natural language as a hint can supply an additional reward for playing sparse-reward games. Achieving a goal should involve several different hints, while the given hints are usually incomplete. Those unmentioned latent hints still rely on the sparse reward signal, and make the learning process difficult. In this paper, we propose semi-supervised initialization (SSI) that allows the agent to learn from various possible hints before training under different tasks. Experiments show that SSI not only helps to learn faster (1.2x) but also has a higher success rate (11% relative improvement) of the final policy",
    "checked": true,
    "id": "adf30d805931c19247ee124bdaedbc793003697a",
    "semantic_title": "semi-supervised policy initialization for playing games with language hints",
    "citation_count": 1,
    "authors": [
      "Tsu-Jui Fu",
      "William Yang Wang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.250": {
    "title": "Revisiting Document Representations for Large-Scale Zero-Shot Learning",
    "volume": "main",
    "abstract": "Zero-shot learning aims to recognize unseen objects using their semantic representations. Most existing works use visual attributes labeled by humans, not suitable for large-scale applications. In this paper, we revisit the use of documents as semantic representations. We argue that documents like Wikipedia pages contain rich visual information, which however can easily be buried by the vast amount of non-visual sentences. To address this issue, we propose a semi-automatic mechanism for visual sentence extraction that leverages the document section headers and the clustering structure of visual sentences. The extracted visual sentences, after a novel weighting scheme to distinguish similar classes, essentially form semantic representations like visual attributes but need much less human effort. On the ImageNet dataset with over 10,000 unseen classes, our representations lead to a 64% relative improvement against the commonly used ones",
    "checked": true,
    "id": "35f63da8669318a2da0173490ffd8ddf3f3f7999",
    "semantic_title": "revisiting document representations for large-scale zero-shot learning",
    "citation_count": 8,
    "authors": [
      "Jihyung Kil",
      "Wei-Lun Chao"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.251": {
    "title": "Negative language transfer in learner English: A new dataset",
    "volume": "main",
    "abstract": "Automatic personalized corrective feedback can help language learners from different backgrounds better acquire a new language. This paper introduces a learner English dataset in which learner errors are accompanied by information about possible error sources. This dataset contains manually annotated error causes for learner writing errors. These causes tie learner mistakes to structures from their first languages, when the rules in English and in the first language diverge. This new dataset will enable second language acquisition researchers to computationally analyze a large quantity of learner errors that are related to language transfer from the learners' first language. The dataset can also be applied in personalizing grammatical error correction systems according to the learners' first language and in providing feedback that is informed by the cause of an error",
    "checked": true,
    "id": "a91c5b6e781e47215ae797d0b881bf4c69c2af79",
    "semantic_title": "negative language transfer in learner english: a new dataset",
    "citation_count": 2,
    "authors": [
      "Leticia Farias Wanderley",
      "Nicole Zhao",
      "Carrie Demmans Epp"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.252": {
    "title": "SentSim: Crosslingual Semantic Evaluation of Machine Translation",
    "volume": "main",
    "abstract": "Machine translation (MT) is currently evaluated in one of two ways: in a monolingual fashion, by comparison with the system output to one or more human reference translations, or in a trained crosslingual fashion, by building a supervised model to predict quality scores from human-labeled data. In this paper, we propose a more cost-effective, yet well performing unsupervised alternative SentSim: relying on strong pretrained multilingual word and sentence representations, we directly compare the source with the machine translated sentence, thus avoiding the need for both reference translations and labelled training data. The metric builds on state-of-the-art embedding-based approaches – namely BERTScore and Word Mover's Distance – by incorporating a notion of sentence semantic similarity. By doing so, it achieves better correlation with human scores on different datasets. We show that it outperforms these and other metrics in the standard monolingual setting (MT-reference translation), a well as in the source-MT bilingual setting, where it performs on par with glass-box approaches to quality estimation that rely on MT model information",
    "checked": true,
    "id": "aeff47b65c8e311a7282918c90be633fb516fae0",
    "semantic_title": "sentsim: crosslingual semantic evaluation of machine translation",
    "citation_count": 24,
    "authors": [
      "Yurun Song",
      "Junchen Zhao",
      "Lucia Specia"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.253": {
    "title": "Quality Estimation for Image Captions Based on Large-scale Human Evaluations",
    "volume": "main",
    "abstract": "Automatic image captioning has improved significantly over the last few years, but the problem is far from being solved, with state of the art models still often producing low quality captions when used in the wild. In this paper, we focus on the task of Quality Estimation (QE) for image captions, which attempts to model the caption quality from a human perspective and *without* access to ground-truth references, so that it can be applied at prediction time to detect low-quality captions produced on *previously unseen images*. For this task, we develop a human evaluation process that collects coarse-grained caption annotations from crowdsourced users, which is then used to collect a large scale dataset spanning more than 600k caption quality ratings. We then carefully validate the quality of the collected ratings and establish baseline models for this new QE task. Finally, we further collect fine-grained caption quality annotations from trained raters, and use them to demonstrate that QE models trained over the coarse ratings can effectively detect and filter out low-quality image captions, thereby improving the user experience from captioning systems",
    "checked": true,
    "id": "a34dde0fc2ef92f81350d5d1fc32b986ef2574b0",
    "semantic_title": "quality estimation for image captions based on large-scale human evaluations",
    "citation_count": 22,
    "authors": [
      "Tomer Levinboim",
      "Ashish V. Thapliyal",
      "Piyush Sharma",
      "Radu Soricut"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.254": {
    "title": "CaSiNo: A Corpus of Campsite Negotiation Dialogues for Automatic Negotiation Systems",
    "volume": "main",
    "abstract": "Automated systems that negotiate with humans have broad applications in pedagogy and conversational AI. To advance the development of practical negotiation systems, we present CaSiNo: a novel corpus of over a thousand negotiation dialogues in English. Participants take the role of campsite neighbors and negotiate for food, water, and firewood packages for their upcoming trip. Our design results in diverse and linguistically rich negotiations while maintaining a tractable, closed-domain environment. Inspired by the literature in human-human negotiations, we annotate persuasion strategies and perform correlation analysis to understand how the dialogue behaviors are associated with the negotiation performance. We further propose and evaluate a multi-task framework to recognize these strategies in a given utterance. We find that multi-task learning substantially improves the performance for all strategy labels, especially for the ones that are the most skewed. We release the dataset, annotations, and the code to propel future work in human-machine negotiations: https://github.com/kushalchawla/CaSiNo",
    "checked": true,
    "id": "37607828cb9b8ae5011bbcc8ecc2e159f719347c",
    "semantic_title": "casino: a corpus of campsite negotiation dialogues for automatic negotiation systems",
    "citation_count": 38,
    "authors": [
      "Kushal Chawla",
      "Jaysa Ramirez",
      "Rene Clever",
      "Gale Lucas",
      "Jonathan May",
      "Jonathan Gratch"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.255": {
    "title": "News Headline Grouping as a Challenging NLU Task",
    "volume": "main",
    "abstract": "Recent progress in Natural Language Understanding (NLU) has seen the latest models outperform human performance on many standard tasks. These impressive results have led the community to introspect on dataset limitations, and iterate on more nuanced challenges. In this paper, we introduce the task of HeadLine Grouping (HLG) and a corresponding dataset (HLGD) consisting of 20,056 pairs of news headlines, each labeled with a binary judgement as to whether the pair belongs within the same group. On HLGD, human annotators achieve high performance of around 0.9 F-1, while current state-of-the art Transformer models only reach 0.75 F-1, opening the path for further improvements. We further propose a novel unsupervised Headline Generator Swap model for the task of HeadLine Grouping that achieves within 3 F-1 of the best supervised model. Finally, we analyze high-performing models with consistency tests, and find that models are not consistent in their predictions, revealing modeling limits of current architectures",
    "checked": true,
    "id": "6c13209f4889b90864ba1d824f5e25e440a2b598",
    "semantic_title": "news headline grouping as a challenging nlu task",
    "citation_count": 15,
    "authors": [
      "Philippe Laban",
      "Lucas Bandarkar",
      "Marti A. Hearst"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.256": {
    "title": "Olá, Bonjour, Salve! XFORMAL: A Benchmark for Multilingual Formality Style Transfer",
    "volume": "main",
    "abstract": "We take the first step towards multilingual style transfer by creating and releasing XFORMAL, a benchmark of multiple formal reformulations of informal text in Brazilian Portuguese, French, and Italian. Results on XFORMAL suggest that state-of-the-art style transfer approaches perform close to simple baselines, indicating that style transfer is even more challenging when moving multilingual",
    "checked": true,
    "id": "619b8c9752d615b115b6be89ea76eb037db1bf34",
    "semantic_title": "olá, bonjour, salve! xformal: a benchmark for multilingual formality style transfer",
    "citation_count": 52,
    "authors": [
      "Eleftheria Briakou",
      "Di Lu",
      "Ke Zhang",
      "Joel Tetreault"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.257": {
    "title": "Grouping Words with Semantic Diversity",
    "volume": "main",
    "abstract": "Deep Learning-based NLP systems can be sensitive to unseen tokens and hard to learn with high-dimensional inputs, which critically hinder learning generalization. We introduce an approach by grouping input words based on their semantic diversity to simplify input language representation with low ambiguity. Since the semantically diverse words reside in different contexts, we are able to substitute words with their groups and still distinguish word meanings relying on their contexts. We design several algorithms that compute diverse groupings based on random sampling, geometric distances, and entropy maximization, and we prove formal guarantees for the entropy-based algorithms. Experimental results show that our methods generalize NLP models and demonstrate enhanced accuracy on POS tagging and LM tasks and significant improvements on medium-scale machine translation tasks, up to +6.5 BLEU points. Our source code is available at https://github.com/abdulrafae/dg",
    "checked": true,
    "id": "e2916c30f0654a25e6177be4db9c6786e9325d75",
    "semantic_title": "grouping words with semantic diversity",
    "citation_count": 1,
    "authors": [
      "Karine Chubarian",
      "Abdul Rafae Khan",
      "Anastasios Sidiropoulos",
      "Jia Xu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.258": {
    "title": "Noise Stability Regularization for Improving BERT Fine-tuning",
    "volume": "main",
    "abstract": "Fine-tuning pre-trained language models suchas BERT has become a common practice dom-inating leaderboards across various NLP tasks. Despite its recent success and wide adoption,this process is unstable when there are onlya small number of training samples available. The brittleness of this process is often reflectedby the sensitivity to random seeds. In this pa-per, we propose to tackle this problem basedon the noise stability property of deep nets,which is investigated in recent literature (Aroraet al., 2018; Sanyal et al., 2020). Specifically,we introduce a novel and effective regulariza-tion method to improve fine-tuning on NLPtasks, referred to asLayer-wiseNoiseStabilityRegularization (LNSR). We extend the theo-ries about adding noise to the input and provethat our method gives a stabler regularizationeffect. We provide supportive evidence by ex-perimentally confirming that well-performingmodels show a low sensitivity to noise andfine-tuning with LNSR exhibits clearly bet-ter generalizability and stability. Furthermore,our method also demonstrates advantages overother state-of-the-art algorithms including L2-SP (Li et al., 2018), Mixout (Lee et al., 2020)and SMART (Jiang et al., 20)",
    "checked": true,
    "id": "441fa19685f7ba5f5d1b295eec37543a43c09194",
    "semantic_title": "noise stability regularization for improving bert fine-tuning",
    "citation_count": 24,
    "authors": [
      "Hang Hua",
      "Xingjian Li",
      "Dejing Dou",
      "Chengzhong Xu",
      "Jiebo Luo"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.259": {
    "title": "FlowPrior: Learning Expressive Priors for Latent Variable Sentence Models",
    "volume": "main",
    "abstract": "Variational autoencoders (VAEs) are widely used for latent variable modeling of text. We focus on variations that learn expressive prior distributions over the latent variable. We find that existing training strategies are not effective for learning rich priors, so we propose adding the importance-sampled log marginal likelihood as a second term to the standard VAE objective to help when learning the prior. Doing so improves results for all priors evaluated, including a novel choice for sentence VAEs based on normalizing flows (NF). Priors parameterized with NF are no longer constrained to a specific distribution family, allowing a more flexible way to encode the data distribution. Our model, which we call FlowPrior, shows a substantial improvement in language modeling tasks compared to strong baselines. We demonstrate that FlowPrior learns an expressive prior with analysis and several forms of evaluation involving generation",
    "checked": true,
    "id": "91160ebc15a0d8f5e0a4a6673d0abd9f05f5cd20",
    "semantic_title": "flowprior: learning expressive priors for latent variable sentence models",
    "citation_count": 8,
    "authors": [
      "Xiaoan Ding",
      "Kevin Gimpel"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.260": {
    "title": "HTCInfoMax: A Global Model for Hierarchical Text Classification via Information Maximization",
    "volume": "main",
    "abstract": "The current state-of-the-art model HiAGM for hierarchical text classification has two limitations. First, it correlates each text sample with all labels in the dataset which contains irrelevant information. Second, it does not consider any statistical constraint on the label representations learned by the structure encoder, while constraints for representation learning are proved to be helpful in previous work. In this paper, we propose HTCInfoMax to address these issues by introducing information maximization which includes two modules: text-label mutual information maximization and label prior matching. The first module can model the interaction between each text sample and its ground truth labels explicitly which filters out irrelevant information. The second one encourages the structure encoder to learn better representations with desired characteristics for all labels which can better handle label imbalance in hierarchical text classification. Experimental results on two benchmark datasets demonstrate the effectiveness of the proposed HTCInfoMax",
    "checked": true,
    "id": "68a22bc6c8b079f6f651ba30b9e6be82207f1166",
    "semantic_title": "htcinfomax: a global model for hierarchical text classification via information maximization",
    "citation_count": 41,
    "authors": [
      "Zhongfen Deng",
      "Hao Peng",
      "Dongxiao He",
      "Jianxin Li",
      "Philip Yu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.261": {
    "title": "Knowledge Guided Metric Learning for Few-Shot Text Classification",
    "volume": "main",
    "abstract": "Humans can distinguish new categories very efficiently with few examples, largely due to the fact that human beings can leverage knowledge obtained from relevant tasks. However, deep learning based text classification model tends to struggle to achieve satisfactory performance when labeled data are scarce. Inspired by human intelligence, we propose to introduce external knowledge into few-shot learning to imitate human knowledge. A novel parameter generator network is investigated to this end, which is able to use the external knowledge to generate different metrics for different tasks. Armed with this network, similar tasks can use similar metrics while different tasks use different metrics. Through experiments, we demonstrate that our method outperforms the SoTA few-shot text classification models",
    "checked": true,
    "id": "02cce36fc58fd9a8682877435689eeef5c1a29fa",
    "semantic_title": "knowledge guided metric learning for few-shot text classification",
    "citation_count": 23,
    "authors": [
      "Dianbo Sui",
      "Yubo Chen",
      "Binjie Mao",
      "Delai Qiu",
      "Kang Liu",
      "Jun Zhao"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.262": {
    "title": "Ensemble of MRR and NDCG models for Visual Dialog",
    "volume": "main",
    "abstract": "Assessing an AI agent that can converse in human language and understand visual content is challenging. Generation metrics, such as BLEU scores favor correct syntax over semantics. Hence a discriminative approach is often used, where an agent ranks a set of candidate options. The mean reciprocal rank (MRR) metric evaluates the model performance by taking into account the rank of a single human-derived answer. This approach, however, raises a new challenge: the ambiguity and synonymy of answers, for instance, semantic equivalence (e.g., ‘yeah' and ‘yes'). To address this, the normalized discounted cumulative gain (NDCG) metric has been used to capture the relevance of all the correct answers via dense annotations. However, the NDCG metric favors the usually applicable uncertain answers such as ‘I don't know.' Crafting a model that excels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should answer a human-like reply and validate the correctness of any answer. To address this issue, we describe a two-step non-parametric ranking approach that can merge strong MRR and NDCG models. Using our approach, we manage to keep most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won the recent Visual Dialog 2020 challenge. Source code is available at https://github.com/idansc/mrr-ndcg",
    "checked": true,
    "id": "9fe7f70bdca53dbc1b6fe4cca5f13cf15ad88aca",
    "semantic_title": "ensemble of mrr and ndcg models for visual dialog",
    "citation_count": 3,
    "authors": [
      "Idan Schwartz"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.263": {
    "title": "Supervised Neural Clustering via Latent Structured Output Learning: Application to Question Intents",
    "volume": "main",
    "abstract": "Previous pre-neural work on structured prediction has produced very effective supervised clustering algorithms using linear classifiers, e.g., structured SVM or perceptron. However, these cannot exploit the representation learning ability of neural networks, which would make supervised clustering even more powerful, i.e., general clustering patterns can be learned automatically. In this paper, we design neural networks based on latent structured prediction loss and Transformer models to approach supervised clustering. We tested our methods on the task of automatically recreating categories of intents from publicly available question intent corpora. The results show that our approach delivers 95.65% of F1, outperforming the state of the art by 17.24%",
    "checked": true,
    "id": "f18a5479dbc4293c54999a10ba8558e7cc462602",
    "semantic_title": "supervised neural clustering via latent structured output learning: application to question intents",
    "citation_count": 6,
    "authors": [
      "Iryna Haponchyk",
      "Alessandro Moschitti"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.264": {
    "title": "ConVEx: Data-Efficient and Few-Shot Slot Labeling",
    "volume": "main",
    "abstract": "We propose ConVEx (Conversational Value Extractor), an efficient pretraining and fine-tuning neural approach for slot-labeling dialog tasks. Instead of relying on more general pretraining objectives from prior work (e.g., language modeling, response selection), ConVEx's pretraining objective, a novel pairwise cloze task using Reddit data, is well aligned with its intended usage on sequence labeling tasks. This enables learning domain-specific slot labelers by simply fine-tuning decoding layers of the pretrained general-purpose sequence labeling model, while the majority of the pretrained model's parameters are kept frozen. We report state-of-the-art performance of ConVEx across a range of diverse domains and data sets for dialog slot-labeling, with the largest gains in the most challenging, few-shot setups. We believe that ConVEx's reduced pretraining times (i.e., only 18 hours on 12 GPUs) and cost, along with its efficient fine-tuning and strong performance, promise wider portability and scalability for data-efficient sequence-labeling tasks in general",
    "checked": true,
    "id": "41c260cb61e4641d0bcc4dc92cef9c5bff525fb6",
    "semantic_title": "convex: data-efficient and few-shot slot labeling",
    "citation_count": 36,
    "authors": [
      "Matthew Henderson",
      "Ivan Vulić"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.265": {
    "title": "CREAD: Combined Resolution of Ellipses and Anaphora in Dialogues",
    "volume": "main",
    "abstract": "Anaphora and ellipses are two common phenomena in dialogues. Without resolving referring expressions and information omission, dialogue systems may fail to generate consistent and coherent responses. Traditionally, anaphora is resolved by coreference resolution and ellipses by query rewrite. In this work, we propose a novel joint learning framework of modeling coreference resolution and query rewriting for complex, multi-turn dialogue understanding. Given an ongoing dialogue between a user and a dialogue assistant, for the user query, our joint learning model first predicts coreference links between the query and the dialogue context, and then generates a self-contained rewritten user query. To evaluate our model, we annotate a dialogue based coreference resolution dataset, MuDoCo, with rewritten queries. Results show that the performance of query rewrite can be substantially boosted (+2.3% F1) with the aid of coreference modeling. Furthermore, our joint model outperforms the state-of-the-art coreference resolution model (+2% F1) on this dataset",
    "checked": true,
    "id": "02e489228a48d1a14b9634400b59752deaf83e48",
    "semantic_title": "cread: combined resolution of ellipses and anaphora in dialogues",
    "citation_count": 16,
    "authors": [
      "Bo-Hsiang Tseng",
      "Shruti Bhargava",
      "Jiarui Lu",
      "Joel Ruben Antony Moniz",
      "Dhivya Piraviperumal",
      "Lin Li",
      "Hong Yu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.266": {
    "title": "Knowledge-Driven Slot Constraints for Goal-Oriented Dialogue Systems",
    "volume": "main",
    "abstract": "In goal-oriented dialogue systems, users provide information through slot values to achieve specific goals. Practically, some combinations of slot values can be invalid according to external knowledge. For example, a combination of \"cheese pizza\" (a menu item) and \"oreo cookies\" (a topping) from an input utterance \"Can I order a cheese pizza with oreo cookies on top?\" exemplifies such invalid combinations according to the menu of a restaurant business. Traditional dialogue systems allow execution of validation rules as a post-processing step after slots have been filled which can lead to error accumulation. In this paper, we formalize knowledge-driven slot constraints and present a new task of constraint violation detection accompanied with benchmarking data. Then, we propose methods to integrate the external knowledge into the system and model constraint violation detection as an end-to-end classification task and compare it to the traditional rule-based pipeline approach. Experiments on two domains of the MultiDoGO dataset reveal challenges of constraint violation detection and sets the stage for future work and improvements",
    "checked": true,
    "id": "9202fba71d225bde06ff41dfee7c993a4627e18f",
    "semantic_title": "knowledge-driven slot constraints for goal-oriented dialogue systems",
    "citation_count": 4,
    "authors": [
      "Piyawat Lertvittayakumjorn",
      "Daniele Bonadiman",
      "Saab Mansour"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.267": {
    "title": "Clipping Loops for Sample-Efficient Dialogue Policy Optimisation",
    "volume": "main",
    "abstract": "Training dialogue agents requires a large number of interactions with users: agents have no idea about which responses are bad among a lengthy dialogue. In this paper, we propose loop-clipping policy optimisation (LCPO) to eliminate useless responses. LCPO consists of two stages: loop clipping and advantage clipping. In loop clipping, we clip off useless responses (called loops) from dialogue history (called trajectories). The clipped trajectories are more succinct than the original ones, and the estimation of state-value is more accurate. Second, in advantage clipping, we estimate and clip the advantages of useless responses and normal ones separately. The clipped advantage distinguish useless actions from others and reduce the probabilities of useless actions efficiently. In experiments on Cambridge Restaurant Dialogue System, LCPO uses only 260 training dialogues to achieve 80% success rate, while PPO baseline requires 2160 dialogues. Besides, LCPO receives 3.7/5 scores in human evaluation where the agent interactively collects 100 real-user dialogues in training phase",
    "checked": true,
    "id": "bb49d0a18a93b8723fcd91b2ff4beb430416efaa",
    "semantic_title": "clipping loops for sample-efficient dialogue policy optimisation",
    "citation_count": 1,
    "authors": [
      "Yen-Chen Wu",
      "Carl Edward Rasmussen"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.268": {
    "title": "Integrating Lexical Information into Entity Neighbourhood Representations for Relation Prediction",
    "volume": "main",
    "abstract": "Relation prediction informed from a combination of text corpora and curated knowledge bases, combining knowledge graph completion with relation extraction, is a relatively little studied task. A system that can perform this task has the ability to extend an arbitrary set of relational database tables with information extracted from a document corpus. OpenKi[1] addresses this task through extraction of named entities and predicates via OpenIE tools then learning relation embeddings from the resulting entity-relation graph for relation prediction, outperforming previous approaches. We present an extension of OpenKi that incorporates embeddings of text-based representations of the entities and the relations. We demonstrate that this results in a substantial performance increase over a system without this information",
    "checked": true,
    "id": "92dbc77add2d046c3ba6e1f20d6415f571fcb355",
    "semantic_title": "integrating lexical information into entity neighbourhood representations for relation prediction",
    "citation_count": 3,
    "authors": [
      "Ian Wood",
      "Mark Johnson",
      "Stephen Wan"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.269": {
    "title": "Noisy-Labeled NER with Confidence Estimation",
    "volume": "main",
    "abstract": "Recent studies in deep learning have shown significant progress in named entity recognition (NER). However, most existing works assume clean data annotation, while real-world scenarios typically involve a large amount of noises from a variety of sources (e.g., pseudo, weak, or distant annotations). This work studies NER under a noisy labeled setting with calibrated confidence estimation. Based on empirical observations of different training dynamics of noisy and clean labels, we propose strategies for estimating confidence scores based on local and global independence assumptions. We partially marginalize out labels of low confidence with a CRF model. We further propose a calibration method for confidence scores based on the structure of entity labels. We integrate our approach into a self-training framework for boosting performance. Experiments in general noisy settings with four languages and distantly labeled settings demonstrate the effectiveness of our method",
    "checked": true,
    "id": "3162df2b118bae31553120d5f85372c854601450",
    "semantic_title": "noisy-labeled ner with confidence estimation",
    "citation_count": 49,
    "authors": [
      "Kun Liu",
      "Yao Fu",
      "Chuanqi Tan",
      "Mosha Chen",
      "Ningyu Zhang",
      "Songfang Huang",
      "Sheng Gao"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.270": {
    "title": "TABBIE: Pretrained Representations of Tabular Data",
    "volume": "main",
    "abstract": "Existing work on tabular representation-learning jointly models tables and associated text using self-supervised objective functions derived from pretrained language models such as BERT. While this joint pretraining improves tasks involving paired tables and text (e.g., answering questions about tables), we show that it underperforms on tasks that operate over tables without any associated text (e.g., populating missing cells). We devise a simple pretraining objective (corrupt cell detection) that learns exclusively from tabular data and reaches the state-of-the-art on a suite of table-based prediction tasks. Unlike competing approaches, our model (TABBIE) provides embeddings of all table substructures (cells, rows, and columns), and it also requires far less compute to train. A qualitative analysis of our model's learned cell, column, and row representations shows that it understands complex table semantics and numerical trends",
    "checked": true,
    "id": "386bfd0e411dee4f512a8737c55dd84846981182",
    "semantic_title": "tabbie: pretrained representations of tabular data",
    "citation_count": 132,
    "authors": [
      "Hiroshi Iida",
      "Dung Thai",
      "Varun Manjunatha",
      "Mohit Iyyer"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.271": {
    "title": "Better Feature Integration for Named Entity Recognition",
    "volume": "main",
    "abstract": "It has been shown that named entity recognition (NER) could benefit from incorporating the long-distance structured information captured by dependency trees. We believe this is because both types of features - the contextual information captured by the linear sequences and the structured information captured by the dependency trees may complement each other. However, existing approaches largely focused on stacking the LSTM and graph neural networks such as graph convolutional networks (GCNs) for building improved NER models, where the exact interaction mechanism between the two types of features is not very clear, and the performance gain does not appear to be significant. In this work, we propose a simple and robust solution to incorporate both types of features with our Synergized-LSTM (Syn-LSTM), which clearly captures how the two types of features interact. We conduct extensive experiments on several standard datasets across four languages. The results demonstrate that the proposed model achieves better performance than previous approaches while requiring fewer parameters. Our further analysis demonstrates that our model can capture longer dependencies compared with strong baselines",
    "checked": true,
    "id": "980e0f25b06112cfe4f0132a3ed35338250012b9",
    "semantic_title": "better feature integration for named entity recognition",
    "citation_count": 35,
    "authors": [
      "Lu Xu",
      "Zhanming Jie",
      "Wei Lu",
      "Lidong Bing"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.272": {
    "title": "ZS-BERT: Towards Zero-Shot Relation Extraction with Attribute Representation Learning",
    "volume": "main",
    "abstract": "While relation extraction is an essential task in knowledge acquisition and representation, and new-generated relations are common in the real world, less effort is made to predict unseen relations that cannot be observed at the training stage. In this paper, we formulate the zero-shot relation extraction problem by incorporating the text description of seen and unseen relations. We propose a novel multi-task learning model, Zero-Shot BERT (ZS-BERT), to directly predict unseen relations without hand-crafted attribute labeling and multiple pairwise classifications. Given training instances consisting of input sentences and the descriptions of their seen relations, ZS-BERT learns two functions that project sentences and relations into an embedding space by jointly minimizing the distances between them and classifying seen relations. By generating the embeddings of unseen relations and new-coming sentences based on such two functions, we use nearest neighbor search to obtain the prediction of unseen relations. Experiments conducted on two well-known datasets exhibit that ZS-BERT can outperform existing methods by at least 13.54% improvement on F1 score",
    "checked": true,
    "id": "93df9dc530b1cf0af6d5eef90d017741a2aab5d8",
    "semantic_title": "zs-bert: towards zero-shot relation extraction with attribute representation learning",
    "citation_count": 67,
    "authors": [
      "Chih-Yao Chen",
      "Cheng-Te Li"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.273": {
    "title": "Graph Convolutional Networks for Event Causality Identification with Rich Document-level Structures",
    "volume": "main",
    "abstract": "We study the problem of Event Causality Identification (ECI) to detect causal relation between event mention pairs in text. Although deep learning models have recently shown state-of-the-art performance for ECI, they are limited to the intra-sentence setting where event mention pairs are presented in the same sentences. This work addresses this issue by developing a novel deep learning model for document-level ECI (DECI) to accept inter-sentence event mention pairs. As such, we propose a graph-based model that constructs interaction graphs to capture relevant connections between important objects for DECI in input documents. Such interaction graphs are then consumed by graph convolutional networks to learn document context-augmented representations for causality prediction between events. Various information sources are introduced to enrich the interaction graphs for DECI, featuring discourse, syntax, and semantic information. Our extensive experiments show that the proposed model achieves state-of-the-art performance on two benchmark datasets",
    "checked": true,
    "id": "5649d1a202e523d0653fbba3cc9017cee539dc19",
    "semantic_title": "graph convolutional networks for event causality identification with rich document-level structures",
    "citation_count": 37,
    "authors": [
      "Minh Tran Phu",
      "Thien Huu Nguyen"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.274": {
    "title": "A Context-Dependent Gated Module for Incorporating Symbolic Semantics into Event Coreference Resolution",
    "volume": "main",
    "abstract": "Event coreference resolution is an important research problem with many applications. Despite the recent remarkable success of pre-trained language models, we argue that it is still highly beneficial to utilize symbolic features for the task. However, as the input for coreference resolution typically comes from upstream components in the information extraction pipeline, the automatically extracted symbolic features can be noisy and contain errors. Also, depending on the specific context, some features can be more informative than others. Motivated by these observations, we propose a novel context-dependent gated module to adaptively control the information flows from the input symbolic features. Combined with a simple noisy training method, our best models achieve state-of-the-art results on two datasets: ACE 2005 and KBP 2016",
    "checked": true,
    "id": "1405b40f4a151e9ec2cf3040386279e828e48179",
    "semantic_title": "a context-dependent gated module for incorporating symbolic semantics into event coreference resolution",
    "citation_count": 26,
    "authors": [
      "Tuan Lai",
      "Heng Ji",
      "Trung Bui",
      "Quan Hung Tran",
      "Franck Dernoncourt",
      "Walter Chang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.275": {
    "title": "Multi-Style Transfer with Discriminative Feedback on Disjoint Corpus",
    "volume": "main",
    "abstract": "Style transfer has been widely explored in natural language generation with non-parallel corpus by directly or indirectly extracting a notion of style from source and target domain corpus. A common shortcoming of existing approaches is the prerequisite of joint annotations across all the stylistic dimensions under consideration. Availability of such dataset across a combination of styles limits the extension of these setups to multiple style dimensions. While cascading single-dimensional models across multiple styles is a possibility, it suffers from content loss, especially when the style dimensions are not completely independent of each other. In our work, we relax this requirement of jointly annotated data across multiple styles by using independently acquired data across different style dimensions without any additional annotations. We initialize an encoder-decoder setup with transformer-based language model pre-trained on a generic corpus and enhance its re-writing capability to multiple target style dimensions by employing multiple style-aware language models as discriminators. Through quantitative and qualitative evaluation, we show the ability of our model to control styles across multiple style dimensions while preserving content of the input text. We compare it against baselines involving cascaded state-of-the-art uni-dimensional style transfer models",
    "checked": true,
    "id": "97994fc8034ff1ee458b60085cf86e67db881c82",
    "semantic_title": "multi-style transfer with discriminative feedback on disjoint corpus",
    "citation_count": 19,
    "authors": [
      "Navita Goyal",
      "Balaji Vasan Srinivasan",
      "Anandhavelu N",
      "Abhilasha Sancheti"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.276": {
    "title": "FUDGE: Controlled Text Generation With Future Discriminators",
    "volume": "main",
    "abstract": "We propose Future Discriminators for Generation (FUDGE), a flexible and modular method for controlled text generation. Given a pre-existing model G for generating text from a distribution of interest, FUDGE enables conditioning on a desired attribute a (for example, formality) while requiring access only to G's output logits. FUDGE learns an attribute predictor operating on a partial sequence, and uses this predictor's outputs to adjust G's original probabilities. We show that FUDGE models terms corresponding to a Bayesian decomposition of the conditional distribution of G given attribute a. Moreover, FUDGE can easily compose predictors for multiple desired attributes. We evaluate FUDGE on three tasks — couplet completion in poetry, topic control in language generation, and formality change in machine translation — and observe gains in all three tasks",
    "checked": true,
    "id": "b6c4a96e09b9f11e7c70e7f1fbe3f3971b92762d",
    "semantic_title": "fudge: controlled text generation with future discriminators",
    "citation_count": 246,
    "authors": [
      "Kevin Yang",
      "Dan Klein"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.277": {
    "title": "Controllable Text Simplification with Explicit Paraphrasing",
    "volume": "main",
    "abstract": "Text Simplification improves the readability of sentences through several rewriting transformations, such as lexical paraphrasing, deletion, and splitting. Current simplification systems are predominantly sequence-to-sequence models that are trained end-to-end to perform all these operations simultaneously. However, such systems limit themselves to mostly deleting words and cannot easily adapt to the requirements of different target audiences. In this paper, we propose a novel hybrid approach that leverages linguistically-motivated rules for splitting and deletion, and couples them with a neural paraphrasing model to produce varied rewriting styles. We introduce a new data augmentation method to improve the paraphrasing capability of our model. Through automatic and manual evaluations, we show that our proposed model establishes a new state-of-the-art for the task, paraphrasing more often than the existing systems, and can control the degree of each simplification operation applied to the input texts",
    "checked": true,
    "id": "63dfa44285c075800be8e7ad3c7eaa8ed039b916",
    "semantic_title": "controllable text simplification with explicit paraphrasing",
    "citation_count": 84,
    "authors": [
      "Mounica Maddela",
      "Fernando Alva-Manchego",
      "Wei Xu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.278": {
    "title": "Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training",
    "volume": "main",
    "abstract": "Prior work on Data-To-Text Generation, the task of converting knowledge graph (KG) triples into natural text, focused on domain-specific benchmark datasets. In this paper, however, we verbalize the entire English Wikidata KG, and discuss the unique challenges associated with a broad, open-domain, large-scale verbalization. We further show that verbalizing a comprehensive, encyclopedic KG like Wikidata can be used to integrate structured KGs and natural language corpora. In contrast to the many architectures that have been developed to integrate these two sources, our approach converts the KG into natural text, allowing it to be seamlessly integrated into existing language models. It carries the further advantages of improved factual accuracy and reduced toxicity in the resulting language model. We evaluate this approach by augmenting the retrieval corpus in a retrieval language model and showing significant improvements on the knowledge intensive tasks of open domain QA and the LAMA knowledge probe",
    "checked": true,
    "id": "1a1e99514d8d175459f7c61cfd0c394b46e63359",
    "semantic_title": "knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training",
    "citation_count": 140,
    "authors": [
      "Oshin Agarwal",
      "Heming Ge",
      "Siamak Shakeri",
      "Rami Al-Rfou"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.279": {
    "title": "Choose Your Own Adventure: Paired Suggestions in Collaborative Writing for Evaluating Story Generation Models",
    "volume": "main",
    "abstract": "Story generation is an open-ended and subjective task, which poses a challenge for evaluating story generation models. We present Choose Your Own Adventure, a collaborative writing setup for pairwise model evaluation. Two models generate suggestions to people as they write a short story; we ask writers to choose one of the two suggestions, and we observe which model's suggestions they prefer. The setup also allows further analysis based on the revisions people make to the suggestions. We show that these measures, combined with automatic metrics, provide an informative picture of the models' performance, both in cases where the differences in generation methods are small (nucleus vs. top-k sampling) and large (GPT2 vs. Fusion models)",
    "checked": true,
    "id": "45d8d1adc4649555c6150d020b139f0e7a6c6415",
    "semantic_title": "choose your own adventure: paired suggestions in collaborative writing for evaluating story generation models",
    "citation_count": 26,
    "authors": [
      "Elizabeth Clark",
      "Noah A. Smith"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.280": {
    "title": "InfoXLM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training",
    "volume": "main",
    "abstract": "In this work, we present an information-theoretic framework that formulates cross-lingual language model pre-training as maximizing mutual information between multilingual-multi-granularity texts. The unified view helps us to better understand the existing methods for learning cross-lingual representations. More importantly, inspired by the framework, we propose a new pre-training task based on contrastive learning. Specifically, we regard a bilingual sentence pair as two views of the same meaning and encourage their encoded representations to be more similar than the negative examples. By leveraging both monolingual and parallel corpora, we jointly train the pretext tasks to improve the cross-lingual transferability of pre-trained models. Experimental results on several benchmarks show that our approach achieves considerably better performance. The code and pre-trained models are available at https://aka.ms/infoxlm",
    "checked": true,
    "id": "4ceff7472c04ee6d76bce89d61ba4b445d8dbf74",
    "semantic_title": "infoxlm: an information-theoretic framework for cross-lingual language model pre-training",
    "citation_count": 318,
    "authors": [
      "Zewen Chi",
      "Li Dong",
      "Furu Wei",
      "Nan Yang",
      "Saksham Singhal",
      "Wenhui Wang",
      "Xia Song",
      "Xian-Ling Mao",
      "Heyan Huang",
      "Ming Zhou"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.281": {
    "title": "Context-Interactive Pre-Training for Document Machine Translation",
    "volume": "main",
    "abstract": "Document machine translation aims to translate the source sentence into the target language in the presence of additional contextual information. However, it typically suffers from a lack of doc-level bilingual data. To remedy this, here we propose a simple yet effective context-interactive pre-training approach, which targets benefiting from external large-scale corpora. The proposed model performs inter sentence generation to capture the cross-sentence dependency within the target document, and cross sentence translation to make better use of valuable contextual information. Comprehensive experiments illustrate that our approach can achieve state-of-the-art performance on three benchmark datasets, which significantly outperforms a variety of baselines",
    "checked": true,
    "id": "ddee39932ce80fbc37ca7d6816f447826f6ec2b1",
    "semantic_title": "context-interactive pre-training for document machine translation",
    "citation_count": 2,
    "authors": [
      "Pengcheng Yang",
      "Pei Zhang",
      "Boxing Chen",
      "Jun Xie",
      "Weihua Luo"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.282": {
    "title": "Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots",
    "volume": "main",
    "abstract": "Multilingual models have demonstrated impressive cross-lingual transfer performance. However, test sets like XNLI are monolingual at the example level. In multilingual communities, it is common for polyglots to code-mix when conversing with each other. Inspired by this phenomenon, we present two strong black-box adversarial attacks (one word-level, one phrase-level) for multilingual models that push their ability to handle code-mixed sentences to the limit. The former uses bilingual dictionaries to propose perturbations and translations of the clean example for sense disambiguation. The latter directly aligns the clean example with its translations before extracting phrases as perturbations. Our phrase-level attack has a success rate of 89.75% against XLM-R-large, bringing its average accuracy of 79.85 down to 8.18 on XNLI. Finally, we propose an efficient adversarial training scheme that trains in the same number of steps as the original model and show that it creates more language-invariant representations, improving clean and robust accuracy in the absence of lexical overlap without degrading performance on the original examples",
    "checked": true,
    "id": "ef7fe0689036897434d1b838be78738147b5264d",
    "semantic_title": "code-mixing on sesame street: dawn of the adversarial polyglots",
    "citation_count": 28,
    "authors": [
      "Samson Tan",
      "Shafiq Joty"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.283": {
    "title": "X-METRA-ADA: Cross-lingual Meta-Transfer learning Adaptation to Natural Language Understanding and Question Answering",
    "volume": "main",
    "abstract": "Multilingual models, such as M-BERT and XLM-R, have gained increasing popularity, due to their zero-shot cross-lingual transfer learning capabilities. However, their generalization ability is still inconsistent for typologically diverse languages and across different benchmarks. Recently, meta-learning has garnered attention as a promising technique for enhancing transfer learning under low-resource scenarios: particularly for cross-lingual transfer in Natural Language Understanding (NLU). In this work, we propose X-METRA-ADA, a cross-lingual MEta-TRAnsfer learning ADAptation approach for NLU. Our approach adapts MAML, an optimization-based meta-learning approach, to learn to adapt to new languages. We extensively evaluate our framework on two challenging cross-lingual NLU tasks: multilingual task-oriented dialog and typologically diverse question answering. We show that our approach outperforms naive fine-tuning, reaching competitive performance on both tasks for most languages. Our analysis reveals that X-METRA-ADA can leverage limited data for faster adaptation",
    "checked": true,
    "id": "5dd431c010369169599067f6e24468b10a8edc82",
    "semantic_title": "x-metra-ada: cross-lingual meta-transfer learning adaptation to natural language understanding and question answering",
    "citation_count": 19,
    "authors": [
      "Meryem M’hamdi",
      "Doo Soon Kim",
      "Franck Dernoncourt",
      "Trung Bui",
      "Xiang Ren",
      "Jonathan May"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.284": {
    "title": "Explicit Alignment Objectives for Multilingual Bidirectional Encoders",
    "volume": "main",
    "abstract": "Pre-trained cross-lingual encoders such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) have proven impressively effective at enabling transfer-learning of NLP systems from high-resource languages to low-resource languages. This success comes despite the fact that there is no explicit objective to align the contextual embeddings of words/sentences with similar meanings across languages together in the same space. In this paper, we present a new method for learning multilingual encoders, AMBER (Aligned Multilingual Bidirectional EncodeR). AMBER is trained on additional parallel data using two explicit alignment objectives that align the multilingual representations at different granularities. We conduct experiments on zero-shot cross-lingual transfer learning for different tasks including sequence tagging, sentence retrieval and sentence classification. Experimental results on the tasks in the XTREME benchmark (Hu et al., 2020) show that AMBER obtains gains of up to 1.1 average F1 score on sequence tagging and up to 27.3 average accuracy on retrieval over the XLM-R-large model which has 3.2x the parameters of AMBER. Our code and models are available at http://github.com/junjiehu/amber",
    "checked": true,
    "id": "50851e9e16b52e14c422b6e937cfd3ed063b6998",
    "semantic_title": "explicit alignment objectives for multilingual bidirectional encoders",
    "citation_count": 47,
    "authors": [
      "Junjie Hu",
      "Melvin Johnson",
      "Orhan Firat",
      "Aditya Siddhant",
      "Graham Neubig"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.285": {
    "title": "Cross-lingual Cross-modal Pretraining for Multimodal Retrieval",
    "volume": "main",
    "abstract": "Recent pretrained vision-language models have achieved impressive performance on cross-modal retrieval tasks in English. Their success, however, heavily depends on the availability of many annotated image-caption datasets for pretraining, where the texts are not necessarily in English. Although we can utilize machine translation (MT) tools to translate non-English text to English, the performance still largely relies on MT's quality and may suffer from high latency problems in real-world applications. This paper proposes a new approach to learn cross-lingual cross-modal representations for matching images and their relevant captions in multiple languages. We seamlessly combine cross-lingual pretraining objectives and cross-modal pretraining objectives in a unified framework to learn image and text in a joint embedding space from available English image-caption data, monolingual and parallel corpus. We show that our approach achieves SOTA performance in retrieval tasks on two multimodal multilingual image caption benchmarks: Multi30k with German captions and MSCOCO with Japanese captions",
    "checked": true,
    "id": "13f76ffdd096b037ef1ef39852ecd87c48dc6f00",
    "semantic_title": "cross-lingual cross-modal pretraining for multimodal retrieval",
    "citation_count": 29,
    "authors": [
      "Hongliang Fei",
      "Tan Yu",
      "Ping Li"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.286": {
    "title": "Wikipedia Entities as Rendezvous across Languages: Grounding Multilingual Language Models by Predicting Wikipedia Hyperlinks",
    "volume": "main",
    "abstract": "Masked language models have quickly become the de facto standard when processing text. Recently, several approaches have been proposed to further enrich word representations with external knowledge sources such as knowledge graphs. However, these models are devised and evaluated in a monolingual setting only. In this work, we propose a language-independent entity prediction task as an intermediate training procedure to ground word representations on entity semantics and bridge the gap across different languages by means of a shared vocabulary of entities. We show that our approach effectively injects new lexical-semantic knowledge into neural models, improving their performance on different semantic tasks in the zero-shot crosslingual setting. As an additional advantage, our intermediate training does not require any supplementary input, allowing our models to be applied to new datasets right away. In our experiments, we use Wikipedia articles in up to 100 languages and already observe consistent gains compared to strong baselines when predicting entities using only the English Wikipedia. Further adding extra languages lead to improvements in most tasks up to a certain point, but overall we found it non-trivial to scale improvements in model transferability by training on ever increasing amounts of Wikipedia languages",
    "checked": true,
    "id": "e9749062ddc90ec551c692b8a919d59c19e46225",
    "semantic_title": "wikipedia entities as rendezvous across languages: grounding multilingual language models by predicting wikipedia hyperlinks",
    "citation_count": 16,
    "authors": [
      "Iacer Calixto",
      "Alessandro Raganato",
      "Tommaso Pasini"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.287": {
    "title": "multiPRover: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning",
    "volume": "main",
    "abstract": "We focus on a type of linguistic formal reasoning where the goal is to reason over explicit knowledge in the form of natural language facts and rules (Clark et al., 2020). A recent work, named PRover (Saha et al., 2020), performs such reasoning by answering a question and also generating a proof graph that explains the answer. However, compositional reasoning is not always unique and there may be multiple ways of reaching the correct answer. Thus, in our work, we address a new and challenging problem of generating multiple proof graphs for reasoning over natural language rule-bases. Each proof provides a different rationale for the answer, thereby improving the interpretability of such reasoning systems. In order to jointly learn from all proof graphs and exploit the correlations between multiple proofs for a question, we pose this task as a set generation problem over structured output spaces where each proof is represented as a directed graph. We propose two variants of a proof-set generation model, multiPRover. Our first model, Multilabel-multiPRover, generates a set of proofs via multi-label classification and implicit conditioning between the proofs; while the second model, Iterative-multiPRover, generates proofs iteratively by explicitly conditioning on the previously generated proofs. Experiments on multiple synthetic, zero-shot, and human-paraphrased datasets reveal that both multiPRover models significantly outperform PRover on datasets containing multiple gold proofs. Iterative-multiPRover obtains state-of-the-art proof F1 in zero-shot scenarios where all examples have single correct proofs. It also generalizes better to questions requiring higher depths of reasoning where multiple proofs are more frequent",
    "checked": true,
    "id": "ae14dd88917b732a41475a6cb5f37d3af30d82e1",
    "semantic_title": "multiprover: generating multiple proofs for improved interpretability in rule reasoning",
    "citation_count": 25,
    "authors": [
      "Swarnadeep Saha",
      "Prateek Yadav",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.288": {
    "title": "Adaptable and Interpretable Neural MemoryOver Symbolic Knowledge",
    "volume": "main",
    "abstract": "Past research has demonstrated that large neural language models (LMs) encode surprising amounts of factual information: however, augmenting or modifying this information requires modifying a corpus and retraining, which is computationally expensive. To address this problem, we develop a neural LM that includes an interpretable neuro-symbolic KB in the form of a \"fact memory\". Each element of the fact memory is formed from a triple of vectors, where each vector corresponds to a KB entity or relation. Our LM improves performance on knowledge-intensive question-answering tasks, sometimes dramatically, including a 27 point increase in one setting of WebQuestionsSP over a state-of-the-art open-book model, despite using 5% of the parameters. Most interestingly, we demonstrate that the model can be modified, without any re-training, by updating the fact memory",
    "checked": true,
    "id": "95bc842fde382c5d6784e2f87521e35f9dd38606",
    "semantic_title": "adaptable and interpretable neural memoryover symbolic knowledge",
    "citation_count": 52,
    "authors": [
      "Pat Verga",
      "Haitian Sun",
      "Livio Baldini Soares",
      "William Cohen"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.289": {
    "title": "CLEVR_HYP: A Challenge Dataset and Baselines for Visual Question Answering with Hypothetical Actions over Images",
    "volume": "main",
    "abstract": "Most existing research on visual question answering (VQA) is limited to information explicitly present in an image or a video. In this paper, we take visual understanding to a higher level where systems are challenged to answer questions that involve mentally simulating the hypothetical consequences of performing specific actions in a given scenario. Towards that end, we formulate a vision-language question answering task based on the CLEVR (Johnson et. al., 2017) dataset. We then modify the best existing VQA methods and propose baseline solvers for this task. Finally, we motivate the development of better vision-language models by providing insights about the capability of diverse architectures to perform joint reasoning over image-text modality. Our dataset setup scripts and codes will be made publicly available at https://github.com/shailaja183/clevr_hyp",
    "checked": true,
    "id": "0247ce2ac7ecc13d0a80491c7635eb7946bbddb1",
    "semantic_title": "clevr_hyp: a challenge dataset and baselines for visual question answering with hypothetical actions over images",
    "citation_count": 23,
    "authors": [
      "Shailaja Keyur Sampat",
      "Akshay Kumar",
      "Yezhou Yang",
      "Chitta Baral"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.290": {
    "title": "Refining Targeted Syntactic Evaluation of Language Models",
    "volume": "main",
    "abstract": "Targeted syntactic evaluation of subject-verb number agreement in English (TSE) evaluates language models' syntactic knowledge using hand-crafted minimal pairs of sentences that differ only in the main verb's conjugation. The method evaluates whether language models rate each grammatical sentence as more likely than its ungrammatical counterpart. We identify two distinct goals for TSE. First, evaluating the systematicity of a language model's syntactic knowledge: given a sentence, can it conjugate arbitrary verbs correctly? Second, evaluating a model's likely behavior: given a sentence, does the model concentrate its probability mass on correctly conjugated verbs, even if only on a subset of the possible verbs? We argue that current implementations of TSE do not directly capture either of these goals, and propose new metrics to capture each goal separately. Under our metrics, we find that TSE overestimates systematicity of language models, but that models score up to 40% better on verbs that they predict are likely in context",
    "checked": true,
    "id": "59c0adcddc2b0252411351ea92252f39ef69540f",
    "semantic_title": "refining targeted syntactic evaluation of language models",
    "citation_count": 36,
    "authors": [
      "Benjamin Newman",
      "Kai-Siang Ang",
      "Julia Gong",
      "John Hewitt"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.291": {
    "title": "Universal Adversarial Attacks with Natural Triggers for Text Classification",
    "volume": "main",
    "abstract": "Recent work has demonstrated the vulnerability of modern text classifiers to universal adversarial attacks, which are input-agnostic sequences of words added to text processed by classifiers. Despite being successful, the word sequences produced in such attacks are often ungrammatical and can be easily distinguished from natural text. We develop adversarial attacks that appear closer to natural English phrases and yet confuse classification systems when added to benign inputs. We leverage an adversarially regularized autoencoder (ARAE) to generate triggers and propose a gradient-based search that aims to maximize the downstream classifier's prediction loss. Our attacks effectively reduce model accuracy on classification tasks while being less identifiable than prior models as per automatic detection metrics and human-subject studies. Our aim is to demonstrate that adversarial attacks can be made harder to detect than previously thought and to enable the development of appropriate defenses",
    "checked": true,
    "id": "64041a45b2db0cbd9d234028195779697158e44b",
    "semantic_title": "universal adversarial attacks with natural triggers for text classification",
    "citation_count": 69,
    "authors": [
      "Liwei Song",
      "Xinwei Yu",
      "Hsuan-Tung Peng",
      "Karthik Narasimhan"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.292": {
    "title": "QuadrupletBERT: An Efficient Model For Embedding-Based Large-Scale Retrieval",
    "volume": "main",
    "abstract": "The embedding-based large-scale query-document retrieval problem is a hot topic in the information retrieval (IR) field. Considering that pre-trained language models like BERT have achieved great success in a wide variety of NLP tasks, we present a QuadrupletBERT model for effective and efficient retrieval in this paper. Unlike most existing BERT-style retrieval models, which only focus on the ranking phase in retrieval systems, our model makes considerable improvements to the retrieval phase and leverages the distances between simple negative and hard negative instances to obtaining better embeddings. Experimental results demonstrate that our QuadrupletBERT achieves state-of-the-art results in embedding-based large-scale retrieval tasks",
    "checked": true,
    "id": "8c649514a05a199b547005f89c924570ae299d9b",
    "semantic_title": "quadrupletbert: an efficient model for embedding-based large-scale retrieval",
    "citation_count": 6,
    "authors": [
      "Peiyang Liu",
      "Sen Wang",
      "Xi Wang",
      "Wei Ye",
      "Shikun Zhang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.293": {
    "title": "Dynamically Disentangling Social Bias from Task-Oriented Representations with Adversarial Attack",
    "volume": "main",
    "abstract": "Representation learning is widely used in NLP for a vast range of tasks. However, representations derived from text corpora often reflect social biases. This phenomenon is pervasive and consistent across different neural models, causing serious concern. Previous methods mostly rely on a pre-specified, user-provided direction or suffer from unstable training. In this paper, we propose an adversarial disentangled debiasing model to dynamically decouple social bias attributes from the intermediate representations trained on the main task. We aim to denoise bias information while training on the downstream task, rather than completely remove social bias and pursue static unbiased representations. Experiments show the effectiveness of our method, both on the effect of debiasing and the main task performance",
    "checked": true,
    "id": "7d6255d2173b6709dd8b13e096097dfd02cf83d9",
    "semantic_title": "dynamically disentangling social bias from task-oriented representations with adversarial attack",
    "citation_count": 17,
    "authors": [
      "Liwen Wang",
      "Yuanmeng Yan",
      "Keqing He",
      "Yanan Wu",
      "Weiran Xu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.294": {
    "title": "An Empirical Investigation of Bias in the Multimodal Analysis of Financial Earnings Calls",
    "volume": "main",
    "abstract": "Volatility prediction is complex due to the stock market's stochastic nature. Existing research focuses on the textual elements of financial disclosures like earnings calls transcripts to forecast stock volatility and risk, but ignores the rich acoustic features in the company executives' speech. Recently, new multimodal approaches that leverage the verbal and vocal cues of speakers in financial disclosures significantly outperform previous state-of-the-art approaches demonstrating the benefits of multimodality and speech. However, the financial realm is still plagued with a severe underrepresentation of various communities spanning diverse demographics, gender, and native speech. While multimodal models are better risk forecasters, it is imperative to also investigate the potential bias that these models may learn from the speech signals of company executives. In this work, we present the first study to discover the gender bias in multimodal volatility prediction due to gender-sensitive audio features and fewer female executives in earnings calls of one of the world's biggest stock indexes, the S&P 500 index. We quantitatively analyze bias as error disparity and investigate the sources of this bias. Our results suggest that multimodal neural financial models accentuate gender-based stereotypes",
    "checked": true,
    "id": "cbc2a99eb3fb9ba236a8dcf7b723a8ecf60ee2ae",
    "semantic_title": "an empirical investigation of bias in the multimodal analysis of financial earnings calls",
    "citation_count": 15,
    "authors": [
      "Ramit Sawhney",
      "Arshiya Aggarwal",
      "Rajiv Ratn Shah"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.295": {
    "title": "Beyond Fair Pay: Ethical Implications of NLP Crowdsourcing",
    "volume": "main",
    "abstract": "The use of crowdworkers in NLP research is growing rapidly, in tandem with the exponential increase in research production in machine learning and AI. Ethical discussion regarding the use of crowdworkers within the NLP research community is typically confined in scope to issues related to labor conditions such as fair pay. We draw attention to the lack of ethical considerations related to the various tasks performed by workers, including labeling, evaluation, and production. We find that the Final Rule, the common ethical framework used by researchers, did not anticipate the use of online crowdsourcing platforms for data collection, resulting in gaps between the spirit and practice of human-subjects ethics in NLP research. We enumerate common scenarios where crowdworkers performing NLP tasks are at risk of harm. We thus recommend that researchers evaluate these risks by considering the three ethical principles set up by the Belmont Report. We also clarify some common misconceptions regarding the Institutional Review Board (IRB) application. We hope this paper will serve to reopen the discussion within our community regarding the ethical use of crowdworkers",
    "checked": true,
    "id": "e5d720767b7a539bb2edaa98eaf572a4506a79c6",
    "semantic_title": "beyond fair pay: ethical implications of nlp crowdsourcing",
    "citation_count": 63,
    "authors": [
      "Boaz Shmueli",
      "Jan Fell",
      "Soumya Ray",
      "Lun-Wei Ku"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.296": {
    "title": "On Transferability of Bias Mitigation Effects in Language Model Fine-Tuning",
    "volume": "main",
    "abstract": "Fine-tuned language models have been shown to exhibit biases against protected groups in a host of modeling tasks such as text classification and coreference resolution. Previous works focus on detecting these biases, reducing bias in data representations, and using auxiliary training objectives to mitigate bias during fine-tuning. Although these techniques achieve bias reduction for the task and domain at hand, the effects of bias mitigation may not directly transfer to new tasks, requiring additional data collection and customized annotation of sensitive attributes, and re-evaluation of appropriate fairness metrics. We explore the feasibility and benefits of upstream bias mitigation (UBM) for reducing bias on downstream tasks, by first applying bias mitigation to an upstream model through fine-tuning and subsequently using it for downstream fine-tuning. We find, in extensive experiments across hate speech detection, toxicity detection and coreference resolution tasks over various bias factors, that the effects of UBM are indeed transferable to new downstream tasks or domains via fine-tuning, creating less biased downstream models than directly fine-tuning on the downstream task or transferring from a vanilla upstream model. Though challenges remain, we show that UBM promises more efficient and accessible bias mitigation in LM fine-tuning",
    "checked": true,
    "id": "65b08b8d490899d1e92a82040a0e374d5677f4f7",
    "semantic_title": "on transferability of bias mitigation effects in language model fine-tuning",
    "citation_count": 62,
    "authors": [
      "Xisen Jin",
      "Francesco Barbieri",
      "Brendan Kennedy",
      "Aida Mostafazadeh Davani",
      "Leonardo Neves",
      "Xiang Ren"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.297": {
    "title": "Case Study: Deontological Ethics in NLP",
    "volume": "main",
    "abstract": "Recent work in natural language processing (NLP) has focused on ethical challenges such as understanding and mitigating bias in data and algorithms; identifying objectionable content like hate speech, stereotypes and offensive language; and building frameworks for better system design and data handling practices. However, there has been little discussion about the ethical foundations that underlie these efforts. In this work, we study one ethical theory, namely deontological ethics, from the perspective of NLP. In particular, we focus on the generalization principle and the respect for autonomy through informed consent. We provide four case studies to demonstrate how these principles can be used with NLP systems. We also recommend directions to avoid the ethical issues in these systems",
    "checked": true,
    "id": "c4dba2263bb636c3a0a55e87a445094217c024b4",
    "semantic_title": "case study: deontological ethics in nlp",
    "citation_count": 23,
    "authors": [
      "Shrimai Prabhumoye",
      "Brendon Boldt",
      "Ruslan Salakhutdinov",
      "Alan W Black"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.298": {
    "title": "Privacy Regularization: Joint Privacy-Utility Optimization in LanguageModels",
    "volume": "main",
    "abstract": "Neural language models are known to have a high capacity for memorization of training samples. This may have serious privacy im- plications when training models on user content such as email correspondence. Differential privacy (DP), a popular choice to train models with privacy guarantees, comes with significant costs in terms of utility degradation and disparate impact on subgroups of users. In this work, we introduce two privacy-preserving regularization methods for training language models that enable joint optimization of utility and privacy through (1) the use of a discriminator and (2) the inclusion of a novel triplet-loss term. We compare our methods with DP through extensive evaluation. We show the advantages of our regularizers with favorable utility-privacy trade-off, faster training with the ability to tap into existing optimization approaches, and ensuring uniform treatment of under-represented subgroups",
    "checked": true,
    "id": "425a4a9c0598e4101ca2f2b930f5c6986ce40a99",
    "semantic_title": "privacy regularization: joint privacy-utility optimization in languagemodels",
    "citation_count": 33,
    "authors": [
      "Fatemehsadat Mireshghallah",
      "Huseyin Inan",
      "Marcello Hasegawa",
      "Victor Rühle",
      "Taylor Berg-Kirkpatrick",
      "Robert Sim"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.299": {
    "title": "On the Impact of Random Seeds on the Fairness of Clinical Classifiers",
    "volume": "main",
    "abstract": "Recent work has shown that fine-tuning large networks is surprisingly sensitive to changes in random seed(s). We explore the implications of this phenomenon for model fairness across demographic groups in clinical prediction tasks over electronic health records (EHR) in MIMIC-III —— the standard dataset in clinical NLP research. Apparent subgroup performance varies substantially for seeds that yield similar overall performance, although there is no evidence of a trade-off between overall and subgroup performance. However, we also find that the small sample sizes inherent to looking at intersections of minority groups and somewhat rare conditions limit our ability to accurately estimate disparities. Further, we find that jointly optimizing for high overall performance and low disparities does not yield statistically significant improvements. Our results suggest that fairness work using MIMIC-III should carefully account for variations in apparent differences that may arise from stochasticity and small sample sizes",
    "checked": true,
    "id": "1d1b05c039ed5c8609723fb46a0ffe2a5ff60414",
    "semantic_title": "on the impact of random seeds on the fairness of clinical classifiers",
    "citation_count": 12,
    "authors": [
      "Silvio Amir",
      "Jan-Willem van de Meent",
      "Byron Wallace"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.300": {
    "title": "Topic Model or Topic Twaddle? Re-evaluating Semantic Interpretability Measures",
    "volume": "main",
    "abstract": "When developing topic models, a critical question that should be asked is: How well will this model work in an applied setting? Because standard performance evaluation of topic interpretability uses automated measures modeled on human evaluation tests that are dissimilar to applied usage, these models' generalizability remains in question. In this paper, we probe the issue of validity in topic model evaluation and assess how informative coherence measures are for specialized collections used in an applied setting. Informed by the literature, we propose four understandings of interpretability. We evaluate these using a novel experimental framework reflective of varied applied settings, including human evaluations using open labeling, typical of applied research. These evaluations show that for some specialized collections, standard coherence measures may not inform the most appropriate topic model or the optimal number of topics, and current interpretability performance validation methods are challenged as a means to confirm model quality in the absence of ground truth data",
    "checked": true,
    "id": "6a1e38c8da6e1e1a65b029b3e409049c90f6dd73",
    "semantic_title": "topic model or topic twaddle? re-evaluating semantic interpretability measures",
    "citation_count": 46,
    "authors": [
      "Caitlin Doogan",
      "Wray Buntine"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.301": {
    "title": "Discourse Probing of Pretrained Language Models",
    "volume": "main",
    "abstract": "Existing work on probing of pretrained language models (LMs) has predominantly focused on sentence-level syntactic tasks. In this paper, we introduce document-level discourse probing to evaluate the ability of pretrained LMs to capture document-level relations. We experiment with 7 pretrained LMs, 4 languages, and 7 discourse probing tasks, and find BART to be overall the best model at capturing discourse — but only in its encoder, with BERT performing surprisingly well as the baseline model. Across the different models, there are substantial differences in which layers best capture discourse information, and large disparities between models",
    "checked": true,
    "id": "a978fcb10817abe8bc91ab2ba0c0bb4605add1d9",
    "semantic_title": "discourse probing of pretrained language models",
    "citation_count": 44,
    "authors": [
      "Fajri Koto",
      "Jey Han Lau",
      "Timothy Baldwin"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.302": {
    "title": "UniDrop: A Simple yet Effective Technique to Improve Transformer without Extra Cost",
    "volume": "main",
    "abstract": "Transformer architecture achieves great success in abundant natural language processing tasks. The over-parameterization of the Transformer model has motivated plenty of works to alleviate its overfitting for superior performances. With some explorations, we find simple techniques such as dropout, can greatly boost model performance with a careful design. Therefore, in this paper, we integrate different dropout techniques into the training of Transformer models. Specifically, we propose an approach named UniDrop to unites three different dropout techniques from fine-grain to coarse-grain, i.e., feature dropout, structure dropout, and data dropout. Theoretically, we demonstrate that these three dropouts play different roles from regularization perspectives. Empirically, we conduct experiments on both neural machine translation and text classification benchmark datasets. Extensive results indicate that Transformer with UniDrop can achieve around 1.5 BLEU improvement on IWSLT14 translation tasks, and better accuracy for the classification even using strong pre-trained RoBERTa as backbone",
    "checked": true,
    "id": "0343875960a81feb75293a747d29ccb013285d23",
    "semantic_title": "unidrop: a simple yet effective technique to improve transformer without extra cost",
    "citation_count": 14,
    "authors": [
      "Zhen Wu",
      "Lijun Wu",
      "Qi Meng",
      "Yingce Xia",
      "Shufang Xie",
      "Tao Qin",
      "Xinyu Dai",
      "Tie-Yan Liu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.303": {
    "title": "tWT–WT: A Dataset to Assert the Role of Target Entities for Detecting Stance of Tweets",
    "volume": "main",
    "abstract": "The stance detection task aims at detecting the stance of a tweet or a text for a target. These targets can be named entities or free-form sentences (claims). Though the task involves reasoning of the tweet with respect to a target, we find that it is possible to achieve high accuracy on several publicly available Twitter stance detection datasets without looking at the target sentence. Specifically, a simple tweet classification model achieved human-level performance on the WT–WT dataset and more than two-third accuracy on various other datasets. We investigate the existence of biases in such datasets to find the potential spurious correlations of sentiment-stance relations and lexical choice associated with the stance category. Furthermore, we propose a new large dataset free of such biases and demonstrate its aptness on the existing stance detection systems. Our empirical findings show much scope for research on the stance detection task and proposes several considerations for creating future stance detection datasets",
    "checked": true,
    "id": "659fb3d7250efc4ce06ddab8e66b924c791b43cf",
    "semantic_title": "twt–wt: a dataset to assert the role of target entities for detecting stance of tweets",
    "citation_count": 17,
    "authors": [
      "Ayush Kaushal",
      "Avirup Saha",
      "Niloy Ganguly"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.304": {
    "title": "Learning to Learn to be Right for the Right Reasons",
    "volume": "main",
    "abstract": "Improving model generalization on held-out data is one of the core objectives in common- sense reasoning. Recent work has shown that models trained on the dataset with superficial cues tend to perform well on the easy test set with superficial cues but perform poorly on the hard test set without superficial cues. Previous approaches have resorted to manual methods of encouraging models not to overfit to superficial cues. While some of the methods have improved performance on hard instances, they also lead to degraded performance on easy in- stances. Here, we propose to explicitly learn a model that does well on both the easy test set with superficial cues and the hard test set without superficial cues. Using a meta-learning objective, we learn such a model that improves performance on both the easy test set and the hard test set. By evaluating our models on Choice of Plausible Alternatives (COPA) and Commonsense Explanation, we show that our proposed method leads to improved performance on both the easy test set and the hard test set upon which we observe up to 16.5 percentage points improvement over the baseline",
    "checked": true,
    "id": "a4aa9b406ce8d648a4f9b871d1b1b2e1356551f6",
    "semantic_title": "learning to learn to be right for the right reasons",
    "citation_count": 3,
    "authors": [
      "Pride Kavumba",
      "Benjamin Heinzerling",
      "Ana Brassard",
      "Kentaro Inui"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.305": {
    "title": "Double Perturbation: On the Robustness of Robustness and Counterfactual Bias Evaluation",
    "volume": "main",
    "abstract": "Robustness and counterfactual bias are usually evaluated on a test dataset. However, are these evaluations robust? If the test dataset is perturbed slightly, will the evaluation results keep the same? In this paper, we propose a \"double perturbation\" framework to uncover model weaknesses beyond the test dataset. The framework first perturbs the test dataset to construct abundant natural sentences similar to the test data, and then diagnoses the prediction change regarding a single-word substitution. We apply this framework to study two perturbation-based approaches that are used to analyze models' robustness and counterfactual bias in English. (1) For robustness, we focus on synonym substitutions and identify vulnerable examples where prediction can be altered. Our proposed attack attains high success rates (96.0%-99.8%) in finding vulnerable examples on both original and robustly trained CNNs and Transformers. (2) For counterfactual bias, we focus on substituting demographic tokens (e.g., gender, race) and measure the shift of the expected prediction among constructed sentences. Our method is able to reveal the hidden model biases not directly shown in the test dataset. Our code is available at https://github.com/chong-z/nlp-second-order-attack",
    "checked": true,
    "id": "01ffe738f268a0941ecbc598cb358521c4e4e714",
    "semantic_title": "double perturbation: on the robustness of robustness and counterfactual bias evaluation",
    "citation_count": 9,
    "authors": [
      "Chong Zhang",
      "Jieyu Zhao",
      "Huan Zhang",
      "Kai-Wei Chang",
      "Cho-Jui Hsieh"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.306": {
    "title": "Explaining Neural Network Predictions on Sentence Pairs via Learning Word-Group Masks",
    "volume": "main",
    "abstract": "Explaining neural network models is important for increasing their trustworthiness in real-world applications. Most existing methods generate post-hoc explanations for neural network models by identifying individual feature attributions or detecting interactions between adjacent features. However, for models with text pairs as inputs (e.g., paraphrase identification), existing methods are not sufficient to capture feature interactions between two texts and their simple extension of computing all word-pair interactions between two texts is computationally inefficient. In this work, we propose the Group Mask (GMASK) method to implicitly detect word correlations by grouping correlated words from the input text pair together and measure their contribution to the corresponding NLP tasks as a whole. The proposed method is evaluated with two different model architectures (decomposable attention model and BERT) across four datasets, including natural language inference and paraphrase identification tasks. Experiments show the effectiveness of GMASK in providing faithful explanations to these models",
    "checked": true,
    "id": "5b8db46bb3770684ced9922a49c496d467e7eec2",
    "semantic_title": "explaining neural network predictions on sentence pairs via learning word-group masks",
    "citation_count": 17,
    "authors": [
      "Hanjie Chen",
      "Song Feng",
      "Jatin Ganhotra",
      "Hui Wan",
      "Chulaka Gunasekara",
      "Sachindra Joshi",
      "Yangfeng Ji"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.307": {
    "title": "Almost Free Semantic Draft for Neural Machine Translation",
    "volume": "main",
    "abstract": "Translation quality can be improved by global information from the required target sentence because the decoder can understand both past and future information. However, the model needs additional cost to produce and consider such global information. In this work, to inject global information but also save cost, we present an efficient method to sample and consider a semantic draft as global information from semantic space for decoding with almost free of cost. Unlike other successful adaptations, we do not have to perform an EM-like process that repeatedly samples a possible semantic from the semantic space. Empirical experiments show that the presented method can achieve competitive performance in common language pairs with a clear advantage in inference efficiency. We will open all our source code on GitHub",
    "checked": true,
    "id": "4b2514f66262396e836b980bfea717ea0b309eb9",
    "semantic_title": "almost free semantic draft for neural machine translation",
    "citation_count": 5,
    "authors": [
      "Xi Ai",
      "Bin Fang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.308": {
    "title": "Pruning-then-Expanding Model for Domain Adaptation of Neural Machine Translation",
    "volume": "main",
    "abstract": "Domain Adaptation is widely used in practical applications of neural machine translation, which aims to achieve good performance on both general domain and in-domain data. However, the existing methods for domain adaptation usually suffer from catastrophic forgetting, large domain divergence, and model explosion. To address these three problems, we propose a method of \"divide and conquer\" which is based on the importance of neurons or parameters for the translation model. In this method, we first prune the model and only keep the important neurons or parameters, making them responsible for both general-domain and in-domain translation. Then we further train the pruned model supervised by the original whole model with knowledge distillation. Last we expand the model to the original size and fine-tune the added parameters for the in-domain translation. We conducted experiments on different language pairs and domains and the results show that our method can achieve significant improvements compared with several strong baselines",
    "checked": true,
    "id": "8acf06738df5ca0c65c9aa58709beaffc03c446b",
    "semantic_title": "pruning-then-expanding model for domain adaptation of neural machine translation",
    "citation_count": 24,
    "authors": [
      "Shuhao Gu",
      "Yang Feng",
      "Wanying Xie"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.309": {
    "title": "Multi-Hop Transformer for Document-Level Machine Translation",
    "volume": "main",
    "abstract": "Document-level neural machine translation (NMT) has proven to be of profound value for its effectiveness on capturing contextual information. Nevertheless, existing approaches 1) simply introduce the representations of context sentences without explicitly characterizing the inter-sentence reasoning process; and 2) feed ground-truth target contexts as extra inputs at the training time, thus facing the problem of exposure bias. We approach these problems with an inspiration from human behavior – human translators ordinarily emerge a translation draft in their mind and progressively revise it according to the reasoning in discourse. To this end, we propose a novel Multi-Hop Transformer (MHT) which offers NMT abilities to explicitly model the human-like draft-editing and reasoning process. Specifically, our model serves the sentence-level translation as a draft and properly refines its representations by attending to multiple antecedent sentences iteratively. Experiments on four widely used document translation tasks demonstrate that our method can significantly improve document-level translation performance and can tackle discourse phenomena, such as coreference error and the problem of polysemy",
    "checked": true,
    "id": "6ce32a253c1eabf8b608ea1110b2e07afbeab993",
    "semantic_title": "multi-hop transformer for document-level machine translation",
    "citation_count": 8,
    "authors": [
      "Long Zhang",
      "Tong Zhang",
      "Haibo Zhang",
      "Baosong Yang",
      "Wei Ye",
      "Shikun Zhang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.310": {
    "title": "Continual Learning for Neural Machine Translation",
    "volume": "main",
    "abstract": "Neural machine translation (NMT) models are data-driven and require large-scale training corpus. In practical applications, NMT models are usually trained on a general domain corpus and then fine-tuned by continuing training on the in-domain corpus. However, this bears the risk of catastrophic forgetting that the performance on the general domain is decreased drastically. In this work, we propose a new continual learning framework for NMT models. We consider a scenario where the training is comprised of multiple stages and propose a dynamic knowledge distillation technique to alleviate the problem of catastrophic forgetting systematically. We also find that the bias exists in the output linear projection when fine-tuning on the in-domain corpus, and propose a bias-correction module to eliminate the bias. We conduct experiments on three representative settings of NMT application. Experimental results show that the proposed method achieves superior performance compared to baseline models in all settings",
    "checked": true,
    "id": "6442d9609a1efc4a6bbac4bb47a6f39b985f32ee",
    "semantic_title": "continual learning for neural machine translation",
    "citation_count": 21,
    "authors": [
      "Yue Cao",
      "Hao-Ran Wei",
      "Boxing Chen",
      "Xiaojun Wan"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.311": {
    "title": "Self-Training for Unsupervised Neural Machine Translation in Unbalanced Training Data Scenarios",
    "volume": "main",
    "abstract": "Unsupervised neural machine translation (UNMT) that relies solely on massive monolingual corpora has achieved remarkable results in several translation tasks. However, in real-world scenarios, massive monolingual corpora do not exist for some extremely low-resource languages such as Estonian, and UNMT systems usually perform poorly when there is not adequate training corpus for one language. In this paper, we first define and analyze the unbalanced training data scenario for UNMT. Based on this scenario, we propose UNMT self-training mechanisms to train a robust UNMT system and improve its performance in this case. Experimental results on several language pairs show that the proposed methods substantially outperform conventional UNMT systems",
    "checked": true,
    "id": "3277e13821617580534fb25dd2d041bf75f0bceb",
    "semantic_title": "self-training for unsupervised neural machine translation in unbalanced training data scenarios",
    "citation_count": 10,
    "authors": [
      "Haipeng Sun",
      "Rui Wang",
      "Kehai Chen",
      "Masao Utiyama",
      "Eiichiro Sumita",
      "Tiejun Zhao"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.312": {
    "title": "Smart-Start Decoding for Neural Machine Translation",
    "volume": "main",
    "abstract": "Most current neural machine translation models adopt a monotonic decoding order of either left-to-right or right-to-left. In this work, we propose a novel method that breaks up the limitation of these decoding orders, called Smart-Start decoding. More specifically, our method first predicts a median word. It starts to decode the words on the right side of the median word and then generates words on the left. We evaluate the proposed Smart-Start decoding method on three datasets. Experimental results show that the proposed method can significantly outperform strong baseline models",
    "checked": true,
    "id": "f4b20bd6a7ec7c57f8afc2228bcfd00a7346197c",
    "semantic_title": "smart-start decoding for neural machine translation",
    "citation_count": 5,
    "authors": [
      "Jian Yang",
      "Shuming Ma",
      "Dongdong Zhang",
      "Juncheng Wan",
      "Zhoujun Li",
      "Ming Zhou"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.313": {
    "title": "Multi-Task Learning with Shared Encoder for Non-Autoregressive Machine Translation",
    "volume": "main",
    "abstract": "Non-Autoregressive machine Translation (NAT) models have demonstrated significant inference speedup but suffer from inferior translation accuracy. The common practice to tackle the problem is transferring the Autoregressive machine Translation (AT) knowledge to NAT models, e.g., with knowledge distillation. In this work, we hypothesize and empirically verify that AT and NAT encoders capture different linguistic properties of source sentences. Therefore, we propose to adopt multi-task learning to transfer the AT knowledge to NAT models through encoder sharing. Specifically, we take the AT model as an auxiliary task to enhance NAT model performance. Experimental results on WMT14 En-De and WMT16 En-Ro datasets show that the proposed Multi-Task NAT achieves significant improvements over the baseline NAT models. Furthermore, the performance on large-scale WMT19 and WMT20 En-De datasets confirm the consistency of our proposed method. In addition, experimental results demonstrate that our Multi-Task NAT is complementary to knowledge distillation, the standard knowledge transfer method for NAT",
    "checked": true,
    "id": "05a5e51b11587d8639d25bc94d8a9f9f17798f80",
    "semantic_title": "multi-task learning with shared encoder for non-autoregressive machine translation",
    "citation_count": 28,
    "authors": [
      "Yongchang Hao",
      "Shilin He",
      "Wenxiang Jiao",
      "Zhaopeng Tu",
      "Michael Lyu",
      "Xing Wang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.314": {
    "title": "ER-AE: Differentially Private Text Generation for Authorship Anonymization",
    "volume": "main",
    "abstract": "Most of privacy protection studies for textual data focus on removing explicit sensitive identifiers. However, personal writing style, as a strong indicator of the authorship, is often neglected. Recent studies, such as SynTF, have shown promising results on privacy-preserving text mining. However, their anonymization algorithm can only output numeric term vectors which are difficult for the recipients to interpret. We propose a novel text generation model with a two-set exponential mechanism for authorship anonymization. By augmenting the semantic information through a REINFORCE training reward function, the model can generate differentially private text that has a close semantic and similar grammatical structure to the original text while removing personal traits of the writing style. It does not assume any conditioned labels or paralleled text data for training. We evaluate the performance of the proposed model on the real-life peer reviews dataset and the Yelp review dataset. The result suggests that our model outperforms the state-of-the-art on semantic preservation, authorship obfuscation, and stylometric transformation",
    "checked": true,
    "id": "61560f8c01f5693a811fed2de14cb94a9e83218d",
    "semantic_title": "er-ae: differentially private text generation for authorship anonymization",
    "citation_count": 29,
    "authors": [
      "Haohan Bo",
      "Steven H. H. Ding",
      "Benjamin C. M. Fung",
      "Farkhund Iqbal"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.315": {
    "title": "Distantly Supervised Transformers For E-Commerce Product QA",
    "volume": "main",
    "abstract": "We propose a practical instant question answering (QA) system on product pages of e-commerce services, where for each user query, relevant community question answer (CQA) pairs are retrieved. User queries and CQA pairs differ significantly in language characteristics making relevance learning difficult. Our proposed transformer-based model learns a robust relevance function by jointly learning unified syntactic and semantic representations without the need for human labeled data. This is achieved by distantly supervising our model by distilling from predictions of a syntactic matching system on user queries and simultaneously training with CQA pairs. Training with CQA pairs helps our model learning semantic QA relevance and distant supervision enables learning of syntactic features as well as the nuances of user querying language. Additionally, our model encodes queries and candidate responses independently allowing offline candidate embedding generation thereby minimizing the need for real-time transformer model execution. Consequently, our framework is able to scale to large e-commerce QA traffic. Extensive evaluation on user queries shows that our framework significantly outperforms both syntactic and semantic baselines in offline as well as large scale online A/B setups of a popular e-commerce service",
    "checked": true,
    "id": "dbc065d9ab8d6c164aa9ec0042bc464c071f3773",
    "semantic_title": "distantly supervised transformers for e-commerce product qa",
    "citation_count": 4,
    "authors": [
      "Happy Mittal",
      "Aniket Chakrabarti",
      "Belhassen Bayar",
      "Animesh Anant Sharma",
      "Nikhil Rasiwasia"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.316": {
    "title": "Quantitative Day Trading from Natural Language using Reinforcement Learning",
    "volume": "main",
    "abstract": "It is challenging to design profitable and practical trading strategies, as stock price movements are highly stochastic, and the market is heavily influenced by chaotic data across sources like news and social media. Existing NLP approaches largely treat stock prediction as a classification or regression problem and are not optimized to make profitable investment decisions. Further, they do not model the temporal dynamics of large volumes of diversely influential text to which the market responds quickly. Building on these shortcomings, we propose a deep reinforcement learning approach that makes time-aware decisions to trade stocks while optimizing profit using textual data. Our method outperforms state-of-the-art in terms of risk-adjusted returns in trading simulations on two benchmarks: Tweets (English) and financial news (Chinese) pertaining to two major indexes and four global stock markets. Through extensive experiments and studies, we build the case for our method as a tool for quantitative trading",
    "checked": true,
    "id": "d456cb788ab3a17ed721ad09019de3012d548c92",
    "semantic_title": "quantitative day trading from natural language using reinforcement learning",
    "citation_count": 15,
    "authors": [
      "Ramit Sawhney",
      "Arnav Wadhwa",
      "Shivam Agarwal",
      "Rajiv Ratn Shah"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.317": {
    "title": "Restoring and Mining the Records of the Joseon Dynasty via Neural Language Modeling and Machine Translation",
    "volume": "main",
    "abstract": "Understanding voluminous historical records provides clues on the past in various aspects, such as social and political issues and even natural science facts. However, it is generally difficult to fully utilize the historical records, since most of the documents are not written in a modern language and part of the contents are damaged over time. As a result, restoring the damaged or unrecognizable parts as well as translating the records into modern languages are crucial tasks. In response, we present a multi-task learning approach to restore and translate historical documents based on a self-attention mechanism, specifically utilizing two Korean historical records, ones of the most voluminous historical records in the world. Experimental results show that our approach significantly improves the accuracy of the translation task than baselines without multi-task learning. In addition, we present an in-depth exploratory analysis on our translated results via topic modeling, uncovering several significant historical events",
    "checked": true,
    "id": "faf97048bd0f35ccb60ad42d3ee26e11386aad92",
    "semantic_title": "restoring and mining the records of the joseon dynasty via neural language modeling and machine translation",
    "citation_count": 14,
    "authors": [
      "Kyeongpil Kang",
      "Kyohoon Jin",
      "Soyoung Yang",
      "Soojin Jang",
      "Jaegul Choo",
      "Youngbin Kim"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.318": {
    "title": "Modeling Diagnostic Label Correlation for Automatic ICD Coding",
    "volume": "main",
    "abstract": "Given the clinical notes written in electronic health records (EHRs), it is challenging to predict the diagnostic codes which is formulated as a multi-label classification task. The large set of labels, the hierarchical dependency, and the imbalanced data make this prediction task extremely hard. Most existing work built a binary prediction for each label independently, ignoring the dependencies between labels. To address this problem, we propose a two-stage framework to improve automatic ICD coding by capturing the label correlation. Specifically, we train a label set distribution estimator to rescore the probability of each label set candidate generated by a base predictor. This paper is the first attempt at learning the label set distribution as a reranking module for ICD coding. In the experiments, our proposed framework is able to improve upon best-performing predictors for medical code prediction on the benchmark MIMIC datasets",
    "checked": true,
    "id": "baaffe9ff4c8e336940bbc31ac89986d20eee3f9",
    "semantic_title": "modeling diagnostic label correlation for automatic icd coding",
    "citation_count": 14,
    "authors": [
      "Shang-Chi Tsai",
      "Chao-Wei Huang",
      "Yun-Nung Chen"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.319": {
    "title": "Self-Supervised Contrastive Learning for Efficient User Satisfaction Prediction in Conversational Agents",
    "volume": "main",
    "abstract": "Turn-level user satisfaction is one of the most important performance metrics for conversational agents. It can be used to monitor the agent's performance and provide insights about defective user experiences. While end-to-end deep learning has shown promising results, having access to a large number of reliable annotated samples required by these methods remains challenging. In a large-scale conversational system, there is a growing number of newly developed skills, making the traditional data collection, annotation, and modeling process impractical due to the required annotation costs and the turnaround times. In this paper, we suggest a self-supervised contrastive learning approach that leverages the pool of unlabeled data to learn user-agent interactions. We show that the pre-trained models using the self-supervised objective are transferable to the user satisfaction prediction. In addition, we propose a novel few-shot transfer learning approach that ensures better transferability for very small sample sizes. The suggested few-shot method does not require any inner loop optimization process and is scalable to very large datasets and complex models. Based on our experiments using real data from a large-scale commercial system, the suggested approach is able to significantly reduce the required number of annotations, while improving the generalization on unseen skills",
    "checked": true,
    "id": "a024e0da265940abe8bac5381b18fe4eff9ddd34",
    "semantic_title": "self-supervised contrastive learning for efficient user satisfaction prediction in conversational agents",
    "citation_count": 24,
    "authors": [
      "Mohammad Kachuee",
      "Hao Yuan",
      "Young-Bum Kim",
      "Sungjin Lee"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.320": {
    "title": "A recipe for annotating grounded clarifications",
    "volume": "main",
    "abstract": "In order to interpret the communicative intents of an utterance, it needs to be grounded in something that is outside of language; that is, grounded in world modalities. In this paper, we argue that dialogue clarification mechanisms make explicit the process of interpreting the communicative intents of the speaker's utterances by grounding them in the various modalities in which the dialogue is situated. This paper frames dialogue clarification mechanisms as an understudied research problem and a key missing piece in the giant jigsaw puzzle of natural language understanding. We discuss both the theoretical background and practical challenges posed by this problem and propose a recipe for obtaining grounding annotations. We conclude by highlighting ethical issues that need to be addressed in future work",
    "checked": true,
    "id": "1f59b5358209fc9b055d6285fbb77bec781b2d7e",
    "semantic_title": "a recipe for annotating grounded clarifications",
    "citation_count": 14,
    "authors": [
      "Luciana Benotti",
      "Patrick Blackburn"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.321": {
    "title": "Grey-box Adversarial Attack And Defence For Sentiment Classification",
    "volume": "main",
    "abstract": "We introduce a grey-box adversarial attack and defence framework for sentiment classification. We address the issues of differentiability, label preservation and input reconstruction for adversarial attack and defence in one unified framework. Our results show that once trained, the attacking model is capable of generating high-quality adversarial examples substantially faster (one order of magnitude less in time) than state-of-the-art attacking methods. These examples also preserve the original sentiment according to human evaluation. Additionally, our framework produces an improved classifier that is robust in defending against multiple adversarial attacking methods. Code is available at: https://github.com/ibm-aur-nlp/adv-def-text-dist",
    "checked": true,
    "id": "e837660da43eb9637fa33aee3b58599e438d1f5f",
    "semantic_title": "grey-box adversarial attack and defence for sentiment classification",
    "citation_count": 44,
    "authors": [
      "Ying Xu",
      "Xu Zhong",
      "Antonio Jimeno Yepes",
      "Jey Han Lau"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.322": {
    "title": "How low is too low? A monolingual take on lemmatisation in Indian languages",
    "volume": "main",
    "abstract": "Lemmatization aims to reduce the sparse data problem by relating the inflected forms of a word to its dictionary form. Most prior work on ML based lemmatization has focused on high resource languages, where data sets (word forms) are readily available. For languages which have no linguistic work available, especially on morphology or in languages where the computational realization of linguistic rules is complex and cumbersome, machine learning based lemmatizers are the way togo. In this paper, we devote our attention to lemmatisation for low resource, morphologically rich scheduled Indian languages using neural methods. Here, low resource means only a small number of word forms are available. We perform tests to analyse the variance in monolingual models' performance on varying the corpus size and contextual morphological tag data for training. We show that monolingual approaches with data augmentation can give competitive accuracy even in the low resource setting, which augurs well for NLP in low resource setting",
    "checked": true,
    "id": "e8adc80a60cfd44086d1e650dbb9f8901861e3db",
    "semantic_title": "how low is too low? a monolingual take on lemmatisation in indian languages",
    "citation_count": 4,
    "authors": [
      "Kumar Saunack",
      "Kumar Saurav",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.323": {
    "title": "Causal Effects of Linguistic Properties",
    "volume": "main",
    "abstract": "We consider the problem of using observational data to estimate the causal effects of linguistic properties. For example, does writing a complaint politely lead to a faster response time? How much will a positive product review increase sales? This paper addresses two technical challenges related to the problem before developing a practical method. First, we formalize the causal quantity of interest as the effect of a writer's intent, and establish the assumptions necessary to identify this from observational data. Second, in practice, we only have access to noisy proxies for the linguistic properties of interest—e.g., predictions from classifiers and lexicons. We propose an estimator for this setting and prove that its bias is bounded when we perform an adjustment for the text. Based on these results, we introduce TextCause, an algorithm for estimating causal effects of linguistic properties. The method leverages (1) distant supervision to improve the quality of noisy proxies, and (2) a pre-trained language model (BERT) to adjust for the text. We show that the proposed method outperforms related approaches when estimating the effect of Amazon review sentiment on semi-simulated sales figures. Finally, we present an applied case study investigating the effects of complaint politeness on bureaucratic response times",
    "checked": true,
    "id": "1ce96d8dbf69199ebd043de6cfa25d7e48b8ab03",
    "semantic_title": "causal effects of linguistic properties",
    "citation_count": 35,
    "authors": [
      "Reid Pryzant",
      "Dallas Card",
      "Dan Jurafsky",
      "Victor Veitch",
      "Dhanya Sridhar"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.324": {
    "title": "Dynabench: Rethinking Benchmarking in NLP",
    "volume": "main",
    "abstract": "We introduce Dynabench, an open-source platform for dynamic dataset creation and model benchmarking. Dynabench runs in a web browser and supports human-and-model-in-the-loop dataset creation: annotators seek to create examples that a target model will misclassify, but that another person will not. In this paper, we argue that Dynabench addresses a critical need in our community: contemporary models quickly achieve outstanding performance on benchmark tasks but nonetheless fail on simple challenge examples and falter in real-world scenarios. With Dynabench, dataset creation, model development, and model assessment can directly inform each other, leading to more robust and informative benchmarks. We report on four initial NLP tasks, illustrating these concepts and highlighting the promise of the platform, and address potential objections to dynamic benchmarking as a new standard for the field",
    "checked": true,
    "id": "77a096d80eb4dd4ccd103d1660c5a5498f7d026b",
    "semantic_title": "dynabench: rethinking benchmarking in nlp",
    "citation_count": 305,
    "authors": [
      "Douwe Kiela",
      "Max Bartolo",
      "Yixin Nie",
      "Divyansh Kaushik",
      "Atticus Geiger",
      "Zhengxuan Wu",
      "Bertie Vidgen",
      "Grusha Prasad",
      "Amanpreet Singh",
      "Pratik Ringshia",
      "Zhiyi Ma",
      "Tristan Thrush",
      "Sebastian Riedel",
      "Zeerak Waseem",
      "Pontus Stenetorp",
      "Robin Jia",
      "Mohit Bansal",
      "Christopher Potts",
      "Adina Williams"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.325": {
    "title": "Translational NLP: A New Paradigm and General Principles for Natural Language Processing Research",
    "volume": "main",
    "abstract": "Natural language processing (NLP) research combines the study of universal principles, through basic science, with applied science targeting specific use cases and settings. However, the process of exchange between basic NLP and applications is often assumed to emerge naturally, resulting in many innovations going unapplied and many important questions left unstudied. We describe a new paradigm of Translational NLP, which aims to structure and facilitate the processes by which basic and applied NLP research inform one another. Translational NLP thus presents a third research paradigm, focused on understanding the challenges posed by application needs and how these challenges can drive innovation in basic science and technology design. We show that many significant advances in NLP research have emerged from the intersection of basic principles with application needs, and present a conceptual framework outlining the stakeholders and key questions in translational research. Our framework provides a roadmap for developing Translational NLP as a dedicated research area, and identifies general translational principles to facilitate exchange between basic and applied research",
    "checked": true,
    "id": "c12911574f879e8e1b816b6f57e08fb1253c8022",
    "semantic_title": "translational nlp: a new paradigm and general principles for natural language processing research",
    "citation_count": 16,
    "authors": [
      "Denis Newman-Griffis",
      "Jill Fain Lehman",
      "Carolyn Rosé",
      "Harry Hochheiser"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.326": {
    "title": "Predicting Discourse Trees from Transformer-based Neural Summarizers",
    "volume": "main",
    "abstract": "Previous work indicates that discourse information benefits summarization. In this paper, we explore whether this synergy between discourse and summarization is bidirectional, by inferring document-level discourse trees from pre-trained neural summarizers. In particular, we generate unlabeled RST-style discourse trees from the self-attention matrices of the transformer model. Experiments across models and datasets reveal that the summarizer learns both, dependency- and constituency-style discourse information, which is typically encoded in a single head, covering long- and short-distance discourse dependencies. Overall, the experimental results suggest that the learned discourse information is general and transferable inter-domain",
    "checked": true,
    "id": "90d6e2bad5c7798a2eb492888040f454da7f8652",
    "semantic_title": "predicting discourse trees from transformer-based neural summarizers",
    "citation_count": 13,
    "authors": [
      "Wen Xiao",
      "Patrick Huber",
      "Giuseppe Carenini"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.327": {
    "title": "Probing for Bridging Inference in Transformer Language Models",
    "volume": "main",
    "abstract": "We probe pre-trained transformer language models for bridging inference. We first investigate individual attention heads in BERT and observe that attention heads at higher layers prominently focus on bridging relations in-comparison with the lower and middle layers, also, few specific attention heads concentrate consistently on bridging. More importantly, we consider language models as a whole in our second approach where bridging anaphora resolution is formulated as a masked token prediction task (Of-Cloze test). Our formulation produces optimistic results without any fine-tuning, which indicates that pre-trained language models substantially capture bridging inference. Our further investigation shows that the distance between anaphor-antecedent and the context provided to language models play an important role in the inference",
    "checked": true,
    "id": "1c0f8a6b8e6f4cc6fc81c4019fc07a2e5ec17107",
    "semantic_title": "probing for bridging inference in transformer language models",
    "citation_count": 10,
    "authors": [
      "Onkar Pandit",
      "Yufang Hou"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.328": {
    "title": "Is Incoherence Surprising? Targeted Evaluation of Coherence Prediction from Language Models",
    "volume": "main",
    "abstract": "Coherent discourse is distinguished from a mere collection of utterances by the satisfaction of a diverse set of constraints, for example choice of expression, logical relation between denoted events, and implicit compatibility with world-knowledge. Do neural language models encode such constraints? We design an extendable set of test suites addressing different aspects of discourse and dialogue coherence. Unlike most previous coherence evaluation studies, we address specific linguistic devices beyond sentence order perturbations, which allow for a more fine-grained analysis of what constitutes coherence and what neural models trained on a language modelling objective are capable of encoding. Extending the targeted evaluation paradigm for neural language models (Marvin and Linzen, 2018) to phenomena beyond syntax, we show that this paradigm is equally suited to evaluate linguistic qualities that contribute to the notion of coherence",
    "checked": true,
    "id": "f6e28a79e90e517c6839d5d52a189bba9f54e13c",
    "semantic_title": "is incoherence surprising? targeted evaluation of coherence prediction from language models",
    "citation_count": 11,
    "authors": [
      "Anne Beyer",
      "Sharid Loáiciga",
      "David Schlangen"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.329": {
    "title": "Stay Together: A System for Single and Split-antecedent Anaphora Resolution",
    "volume": "main",
    "abstract": "The state-of-the-art on basic, single-antecedent anaphora has greatly improved in recent years. Researchers have therefore started to pay more attention to more complex cases of anaphora such as split-antecedent anaphora, as in \"Time-Warner is considering a legal challenge to Telecommunications Inc's plan to buy half of Showtime Networks Inc–a move that could lead to all-out war between the two powerful companies\". Split-antecedent anaphora is rarer and more complex to resolve than single-antecedent anaphora; as a result, it is not annotated in many datasets designed to test coreference, and previous work on resolving this type of anaphora was carried out in unrealistic conditions that assume gold mentions and/or gold split-antecedent anaphors are available. These systems also focus on split-antecedent anaphors only. In this work, we introduce a system that resolves both single and split-antecedent anaphors, and evaluate it in a more realistic setting that uses predicted mentions. We also start addressing the question of how to evaluate single and split-antecedent anaphors together using standard coreference evaluation metrics",
    "checked": true,
    "id": "66792fd0019c3848a69a9409b361558e84d7ccfe",
    "semantic_title": "stay together: a system for single and split-antecedent anaphora resolution",
    "citation_count": 13,
    "authors": [
      "Juntao Yu",
      "Nafise Sadat Moosavi",
      "Silviu Paun",
      "Massimo Poesio"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.330": {
    "title": "Redefining Absent Keyphrases and their Effect on Retrieval Effectiveness",
    "volume": "main",
    "abstract": "Neural keyphrase generation models have recently attracted much interest due to their ability to output absent keyphrases, that is, keyphrases that do not appear in the source text. In this paper, we discuss the usefulness of absent keyphrases from an Information Retrieval (IR) perspective, and show that the commonly drawn distinction between present and absent keyphrases is not made explicit enough. We introduce a finer-grained categorization scheme that sheds more light on the impact of absent keyphrases on scientific document retrieval. Under this scheme, we find that only a fraction (around 20%) of the words that make up keyphrases actually serves as document expansion, but that this small fraction of words is behind much of the gains observed in retrieval effectiveness. We also discuss how the proposed scheme can offer a new angle to evaluate the output of neural keyphrase generation models",
    "checked": true,
    "id": "6e907e0eaaf7fd45a75331efa4b404137207e178",
    "semantic_title": "redefining absent keyphrases and their effect on retrieval effectiveness",
    "citation_count": 12,
    "authors": [
      "Florian Boudin",
      "Ygor Gallina"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.331": {
    "title": "CoRT: Complementary Rankings from Transformers",
    "volume": "main",
    "abstract": "Many recent approaches towards neural information retrieval mitigate their computational costs by using a multi-stage ranking pipeline. In the first stage, a number of potentially relevant candidates are retrieved using an efficient retrieval model such as BM25. Although BM25 has proven decent performance as a first-stage ranker, it tends to miss relevant passages. In this context we propose CoRT, a simple neural first-stage ranking model that leverages contextual representations from pretrained language models such as BERT to complement term-based ranking functions while causing no significant delay at query time. Using the MS MARCO dataset, we show that CoRT significantly increases the candidate recall by complementing BM25 with missing candidates. Consequently, we find subsequent re-rankers achieve superior results with less candidates. We further demonstrate that passage retrieval using CoRT can be realized with surprisingly low latencies",
    "checked": true,
    "id": "85795d16aa1b1b819154054905e1b4a5a87e4f43",
    "semantic_title": "cort: complementary rankings from transformers",
    "citation_count": 9,
    "authors": [
      "Marco Wrzalik",
      "Dirk Krechel"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.332": {
    "title": "Multi-source Neural Topic Modeling in Multi-view Embedding Spaces",
    "volume": "main",
    "abstract": "Though word embeddings and topics are complementary representations, several past works have only used pretrained word embeddings in (neural) topic modeling to address data sparsity in short-text or small collection of documents. This work presents a novel neural topic modeling framework using multi-view embed ding spaces: (1) pretrained topic-embeddings, and (2) pretrained word-embeddings (context-insensitive from Glove and context-sensitive from BERT models) jointly from one or many sources to improve topic quality and better deal with polysemy. In doing so, we first build respective pools of pretrained topic (i.e., TopicPool) and word embeddings (i.e., WordPool). We then identify one or more relevant source domain(s) and transfer knowledge to guide meaningful learning in the sparse target domain. Within neural topic modeling, we quantify the quality of topics and document representations via generalization (perplexity), interpretability (topic coherence) and information retrieval (IR) using short-text, long-text, small and large document collections from news and medical domains. Introducing the multi-source multi-view embedding spaces, we have shown state-of-the-art neural topic modeling using 6 source (high-resource) and 5 target (low-resource) corpora",
    "checked": true,
    "id": "5e922b5bee1cead0c259ba373410cdedbce6b2da",
    "semantic_title": "multi-source neural topic modeling in multi-view embedding spaces",
    "citation_count": 8,
    "authors": [
      "Pankaj Gupta",
      "Yatin Chaudhary",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.333": {
    "title": "Inductive Topic Variational Graph Auto-Encoder for Text Classification",
    "volume": "main",
    "abstract": "Graph convolutional networks (GCNs) have been applied recently to text classification and produced an excellent performance. However, existing GCN-based methods do not assume an explicit latent semantic structure of documents, making learned representations less effective and difficult to interpret. They are also transductive in nature, thus cannot handle out-of-graph documents. To address these issues, we propose a novel model named inductive Topic Variational Graph Auto-Encoder (T-VGAE), which incorporates a topic model into variational graph-auto-encoder (VGAE) to capture the hidden semantic information between documents and words. T-VGAE inherits the interpretability of the topic model and the efficient information propagation mechanism of VGAE. It learns probabilistic representations of words and documents by jointly encoding and reconstructing the global word-level graph and bipartite graphs of documents, where each document is considered individually and decoupled from the global correlation graph so as to enable inductive learning. Our experiments on several benchmark datasets show that our method outperforms the existing competitive models on supervised and semi-supervised text classification, as well as unsupervised text representation learning. In addition, it has higher interpretability and is able to deal with unseen documents",
    "checked": true,
    "id": "c61caf41d56eba4f9245525e56f3bd52aa634f8e",
    "semantic_title": "inductive topic variational graph auto-encoder for text classification",
    "citation_count": 21,
    "authors": [
      "Qianqian Xie",
      "Jimin Huang",
      "Pan Du",
      "Min Peng",
      "Jian-Yun Nie"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.334": {
    "title": "Self-Alignment Pretraining for Biomedical Entity Representations",
    "volume": "main",
    "abstract": "Despite the widespread success of self-supervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as entity linking where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SapBERT, a pretraining scheme that self-aligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipeline-based hybrid systems, SapBERT offers an elegant one-model-for-all solution to the problem of medical entity linking (MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking datasets. In the scientific domain, we achieve SOTA even without task-specific supervision. With substantial improvement over various domain-specific pretrained MLMs such as BioBERT, SciBERTand and PubMedBERT, our pretraining scheme proves to be both effective and robust",
    "checked": true,
    "id": "615204452304331004532c5800399ef55d58b4c7",
    "semantic_title": "self-alignment pretraining for biomedical entity representations",
    "citation_count": 250,
    "authors": [
      "Fangyu Liu",
      "Ehsan Shareghi",
      "Zaiqiao Meng",
      "Marco Basaldella",
      "Nigel Collier"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.335": {
    "title": "TaxoClass: Hierarchical Multi-Label Text Classification Using Only Class Names",
    "volume": "main",
    "abstract": "Hierarchical multi-label text classification (HMTC) aims to tag each document with a set of classes from a taxonomic class hierarchy. Most existing HMTC methods train classifiers using massive human-labeled documents, which are often too costly to obtain in real-world applications. In this paper, we explore to conduct HMTC based on only class surface names as supervision signals. We observe that to perform HMTC, human experts typically first pinpoint a few most essential classes for the document as its \"core classes\", and then check core classes' ancestor classes to ensure the coverage. To mimic human experts, we propose a novel HMTC framework, named TaxoClass. Specifically, TaxoClass (1) calculates document-class similarities using a textual entailment model, (2) identifies a document's core classes and utilizes confident core classes to train a taxonomy-enhanced classifier, and (3) generalizes the classifier via multi-label self-training. Our experiments on two challenging datasets show TaxoClass can achieve around 0.71 Example-F1 using only class names, outperforming the best previous method by 25%",
    "checked": true,
    "id": "15e100120f080b9ef4230b4cbb8e107b76e2b839",
    "semantic_title": "taxoclass: hierarchical multi-label text classification using only class names",
    "citation_count": 56,
    "authors": [
      "Jiaming Shen",
      "Wenda Qiu",
      "Yu Meng",
      "Jingbo Shang",
      "Xiang Ren",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.336": {
    "title": "MERMAID: Metaphor Generation with Symbolism and Discriminative Decoding",
    "volume": "main",
    "abstract": "Generating metaphors is a challenging task as it requires a proper understanding of abstract concepts, making connections between unrelated concepts, and deviating from the literal meaning. In this paper, we aim to generate a metaphoric sentence given a literal expression by replacing relevant verbs. Based on a theoretically-grounded connection between metaphors and symbols, we propose a method to automatically construct a parallel corpus by transforming a large number of metaphorical sentences from the Gutenberg Poetry corpus (CITATION) to their literal counterpart using recent advances in masked language modeling coupled with commonsense inference. For the generation task, we incorporate a metaphor discriminator to guide the decoding of a sequence to sequence model fine-tuned on our parallel data to generate high-quality metaphors. Human evaluation on an independent test set of literal statements shows that our best model generates metaphors better than three well-crafted baselines 66% of the time on average. A task-based evaluation shows that human-written poems enhanced with metaphors proposed by our model are preferred 68% of the time compared to poems without metaphors",
    "checked": true,
    "id": "5678a6e9bd9b4c56f45219e9ccf1a5b6356963d4",
    "semantic_title": "mermaid: metaphor generation with symbolism and discriminative decoding",
    "citation_count": 60,
    "authors": [
      "Tuhin Chakrabarty",
      "Xurui Zhang",
      "Smaranda Muresan",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.337": {
    "title": "On Learning Text Style Transfer with Direct Rewards",
    "volume": "main",
    "abstract": "In most cases, the lack of parallel corpora makes it impossible to directly train supervised models for the text style transfer task. In this paper, we explore training algorithms that instead optimize reward functions that explicitly consider different aspects of the style-transferred outputs. In particular, we leverage semantic similarity metrics originally used for fine-tuning neural machine translation models to explicitly assess the preservation of content between system outputs and input texts. We also investigate the potential weaknesses of the existing automatic metrics and propose efficient strategies of using these metrics for training. The experimental results show that our model provides significant gains in both automatic and human evaluation over strong baselines, indicating the effectiveness of our proposed methods and training strategies",
    "checked": true,
    "id": "40d49345c2cb4c1e8862a939af5f0cc090404bf0",
    "semantic_title": "on learning text style transfer with direct rewards",
    "citation_count": 38,
    "authors": [
      "Yixin Liu",
      "Graham Neubig",
      "John Wieting"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.338": {
    "title": "Focused Attention Improves Document-Grounded Generation",
    "volume": "main",
    "abstract": "Document grounded generation is the task of using the information provided in a document to improve text generation. This work focuses on two different document grounded generation tasks: Wikipedia Update Generation task and Dialogue response generation. Our work introduces two novel adaptations of large scale pre-trained encoder-decoder models focusing on building context driven representation of the document and enabling specific attention to the information in the document. Additionally, we provide a stronger BART baseline for these tasks. Our proposed techniques outperform existing methods on both automated (at least 48% increase in BLEU-4 points) and human evaluation for closeness to reference and relevance to the document. Furthermore, we perform comprehensive manual inspection of the generated output and categorize errors to provide insights into future directions in modeling these tasks",
    "checked": true,
    "id": "c5307613033500ee2061f286ba76ded2033d1a2e",
    "semantic_title": "focused attention improves document-grounded generation",
    "citation_count": 37,
    "authors": [
      "Shrimai Prabhumoye",
      "Kazuma Hashimoto",
      "Yingbo Zhou",
      "Alan W Black",
      "Ruslan Salakhutdinov"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.339": {
    "title": "NeuroLogic Decoding: (Un)supervised Neural Text Generation with Predicate Logic Constraints",
    "volume": "main",
    "abstract": "Conditional text generation often requires lexical constraints, i.e., which words should or shouldn't be included in the output text. While the dominant recipe for conditional text generation has been large-scale pretrained language models that are finetuned on the task-specific training data, such models do not learn to follow the underlying constraints reliably, even when supervised with large amounts of task-specific examples. We propose NeuroLogic Decoding, a simple yet effective algorithm that enables neural language models – supervised or not – to generate fluent text while satisfying complex lexical constraints. Our approach is powerful yet efficient. It handles any set of lexical constraints that is expressible under predicate logic, while its asymptotic runtime is equivalent to conventional beam search. Empirical results on four benchmarks show that NeuroLogic Decoding outperforms previous approaches, including algorithms that handle a subset of our constraints. Moreover, we find that unsupervised models with NeuroLogic Decoding often outperform supervised models with conventional decoding, even when the latter is based on considerably larger networks. Our results suggest the limit of large-scale neural networks for fine-grained controllable generation and the promise of inference-time algorithms",
    "checked": true,
    "id": "2c5bf29079cd958a2bef150077a02a1deb300652",
    "semantic_title": "neurologic decoding: (un)supervised neural text generation with predicate logic constraints",
    "citation_count": 105,
    "authors": [
      "Ximing Lu",
      "Peter West",
      "Rowan Zellers",
      "Ronan Le Bras",
      "Chandra Bhagavatula",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.340": {
    "title": "Ask what's missing and what's useful: Improving Clarification Question Generation using Global Knowledge",
    "volume": "main",
    "abstract": "The ability to generate clarification questions i.e., questions that identify useful missing information in a given context, is important in reducing ambiguity. Humans use previous experience with similar contexts to form a global view and compare it to the given context to ascertain what is missing and what is useful in the context. Inspired by this, we propose a model for clarification question generation where we first identify what is missing by taking a difference between the global and the local view and then train a model to identify what is useful and generate a question about it. Our model outperforms several baselines as judged by both automatic metrics and humans",
    "checked": true,
    "id": "b255a1aeed73c236fc1ae209743e32e805795168",
    "semantic_title": "ask what's missing and what's useful: improving clarification question generation using global knowledge",
    "citation_count": 42,
    "authors": [
      "Bodhisattwa Prasad Majumder",
      "Sudha Rao",
      "Michel Galley",
      "Julian McAuley"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.341": {
    "title": "Progressive Generation of Long Text with Pretrained Language Models",
    "volume": "main",
    "abstract": "Large-scale language models (LMs) pretrained on massive corpora of text, such as GPT-2, are powerful open-domain text generators. However, as our systematic examination reveals, it is still challenging for such models to generate coherent long passages of text (e.g., 1000 tokens), especially when the models are fine-tuned to the target domain on a small corpus. Previous planning-then-generation methods also fall short of producing such long text in various domains. To overcome the limitations, we propose a simple but effective method of generating text in a progressive manner, inspired by generating images from low to high resolution. Our method first produces domain-specific content keywords and then progressively refines them into complete passages in multiple stages. The simple design allows our approach to take advantage of pretrained LMs at each stage and effectively adapt to any target domain given only a small set of examples. We conduct a comprehensive empirical study with a broad set of evaluation metrics, and show that our approach significantly improves upon the fine-tuned large LMs and various planning-then-generation methods in terms of quality and sample efficiency. Human evaluation also validates that our model generations are more coherent",
    "checked": true,
    "id": "b5850a86c886baef1c536c34c33b23da2e728492",
    "semantic_title": "progressive generation of long text with pretrained language models",
    "citation_count": 76,
    "authors": [
      "Bowen Tan",
      "Zichao Yang",
      "Maruan Al-Shedivat",
      "Eric Xing",
      "Zhiting Hu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.342": {
    "title": "SOCCER: An Information-Sparse Discourse State Tracking Collection in the Sports Commentary Domain",
    "volume": "main",
    "abstract": "In the pursuit of natural language understanding, there has been a long standing interest in tracking state changes throughout narratives. Impressive progress has been made in modeling the state of transaction-centric dialogues and procedural texts. However, this problem has been less intensively studied in the realm of general discourse where ground truth descriptions of states may be loosely defined and state changes are less densely distributed over utterances. This paper proposes to turn to simplified, fully observable systems that show some of these properties: Sports events. We curated 2,263 soccer matches including time-stamped natural language commentary accompanied by discrete events such as a team scoring goals, switching players or being penalized with cards. We propose a new task formulation where, given paragraphs of commentary of a game at different timestamps, the system is asked to recognize the occurrence of in-game events. This domain allows for rich descriptions of state while avoiding the complexities of many other real-world settings. As an initial point of performance measurement, we include two baseline methods from the perspectives of sentence classification with temporal dependence and current state-of-the-art generative model, respectively, and demonstrate that even sophisticated existing methods struggle on the state tracking task when the definition of state broadens or non-event chatter becomes prevalent",
    "checked": true,
    "id": "cd7e04a375761428a91c45f49b2cd3c7f3e9effd",
    "semantic_title": "soccer: an information-sparse discourse state tracking collection in the sports commentary domain",
    "citation_count": 5,
    "authors": [
      "Ruochen Zhang",
      "Carsten Eickhoff"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.343": {
    "title": "Plot-guided Adversarial Example Construction for Evaluating Open-domain Story Generation",
    "volume": "main",
    "abstract": "With the recent advances of open-domain story generation, the lack of reliable automatic evaluation metrics becomes an increasingly imperative issue that hinders the fast development of story generation. According to conducted researches in this regard, learnable evaluation metrics have promised more accurate assessments by having higher correlations with human judgments. A critical bottleneck of obtaining a reliable learnable evaluation metric is the lack of high-quality training data for classifiers to efficiently distinguish plausible and implausible machine-generated stories. Previous works relied on heuristically manipulated plausible examples to mimic possible system drawbacks such as repetition, contradiction, or irrelevant content in the text level, which can be unnatural and oversimplify the characteristics of implausible machine-generated stories. We propose to tackle these issues by generating a more comprehensive set of implausible stories using plots, which are structured representations of controllable factors used to generate stories. Since these plots are compact and structured, it is easier to manipulate them to generate text with targeted undesirable properties, while at the same time maintain the grammatical correctness and naturalness of the generated sentences. To improve the quality of generated implausible stories, we further apply the adversarial filtering procedure presented by (CITATION) to select a more nuanced set of implausible texts. Experiments show that the evaluation metrics trained on our generated data result in more reliable automatic assessments that correlate remarkably better with human judgments compared to the baselines",
    "checked": true,
    "id": "a23f58609e7caecb68ec16d7a175b24e482b515d",
    "semantic_title": "plot-guided adversarial example construction for evaluating open-domain story generation",
    "citation_count": 13,
    "authors": [
      "Sarik Ghazarian",
      "Zixi Liu",
      "Akash S M",
      "Ralph Weischedel",
      "Aram Galstyan",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.344": {
    "title": "MultiOpEd: A Corpus of Multi-Perspective News Editorials",
    "volume": "main",
    "abstract": "We propose MultiOpEd, an open-domain news editorial corpus that supports various tasks pertaining to the argumentation structure in news editorials, focusing on automatic perspective discovery. News editorial is a genre of persuasive text, where the argumentation structure is usually implicit. However, the arguments presented in an editorial typically center around a concise, focused thesis, which we refer to as their perspective. MultiOpEd aims at supporting the study of multiple tasks relevant to automatic perspective discovery, where a system is expected to produce a single-sentence thesis statement summarizing the arguments presented. We argue that identifying and abstracting such natural language perspectives from editorials is a crucial step toward studying the implicit argumentation structure in news editorials. We first discuss the challenges and define a few conceptual tasks towards our goal. To demonstrate the utility of MultiOpEd and the induced tasks, we study the problem of perspective summarization in a multi-task learning setting, as a case study. We show that, with the induced tasks as auxiliary tasks, we can improve the quality of the perspective summary generated. We hope that MultiOpEd will be a useful resource for future studies on argumentation in the news editorial domain",
    "checked": true,
    "id": "565f5c0203f28d3f6ff534283ceac1a96d2c45a3",
    "semantic_title": "multioped: a corpus of multi-perspective news editorials",
    "citation_count": 8,
    "authors": [
      "Siyi Liu",
      "Sihao Chen",
      "Xander Uyttendaele",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.345": {
    "title": "Swords: A Benchmark for Lexical Substitution with Improved Data Coverage and Quality",
    "volume": "main",
    "abstract": "We release a new benchmark for lexical substitution, the task of finding appropriate substitutes for a target word in a context. For writing, lexical substitution systems can assist humans by suggesting words that humans cannot easily think of. However, existing benchmarks depend on human recall as the only source of data, and therefore lack coverage of the substitutes that would be most helpful to humans. Furthermore, annotators often provide substitutes of low quality, which are not actually appropriate in the given context. We collect higher-coverage and higher-quality data by framing lexical substitution as a classification problem, guided by the intuition that it is easier for humans to judge the appropriateness of candidate substitutes than conjure them from memory. To this end, we use a context-free thesaurus to produce candidates and rely on human judgement to determine contextual appropriateness. Compared to the previous largest benchmark, our Swords benchmark has 3x as many substitutes per target word for the same level of quality, and its substitutes are 1.4x more appropriate (based on human judgement) for the same number of substitutes",
    "checked": true,
    "id": "91252e0e6fa12b7204719b05d85dab0923e0fe84",
    "semantic_title": "swords: a benchmark for lexical substitution with improved data coverage and quality",
    "citation_count": 14,
    "authors": [
      "Mina Lee",
      "Chris Donahue",
      "Robin Jia",
      "Alexander Iyabor",
      "Percy Liang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.346": {
    "title": "I'm Not Mad\": Commonsense Implications of Negation and Contradiction",
    "volume": "main",
    "abstract": "Natural language inference requires reasoning about contradictions, negations, and their commonsense implications. Given a simple premise (e.g., \"I'm mad at you\"), humans can reason about the varying shades of contradictory statements ranging from straightforward negations (\"I'm not mad at you\") to commonsense contradictions (\"I'm happy\"). Moreover, these negated or contradictory statements shift the commonsense implications of the original premise in interesting and nontrivial ways. For example, while \"I'm mad\" implies \"I'm unhappy about something,\" negating the premise does not necessarily negate the corresponding commonsense implications. In this paper, we present the first comprehensive study focusing on commonsense implications of negated statements and contradictions. We introduce ANION, a new commonsense knowledge graph with 624K if-then rules focusing on negated and contradictory events. We then present joint generative and discriminative inference models for this new resource, providing novel empirical insights on how logical negations and commonsense contradictions reshape the commonsense implications of their original premises",
    "checked": true,
    "id": "ba40d5fa06a0b14aa50c681c0a38746e348a4491",
    "semantic_title": "i'm not mad\": commonsense implications of negation and contradiction",
    "citation_count": 35,
    "authors": [
      "Liwei Jiang",
      "Antoine Bosselut",
      "Chandra Bhagavatula",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.347": {
    "title": "Identifying Medical Self-Disclosure in Online Communities",
    "volume": "main",
    "abstract": "Self-disclosure in online health conversations may offer a host of benefits, including earlier detection and treatment of medical issues that may have otherwise gone unaddressed. However, research analyzing medical self-disclosure in online communities is limited. We address this shortcoming by introducing a new dataset of health-related posts collected from online social platforms, categorized into three groups (No Self-Disclosure, Possible Self-Disclosure, and Clear Self-Disclosure) with high inter-annotator agreement (_k_=0.88). We make this data available to the research community. We also release a predictive model trained on this dataset that achieves an accuracy of 81.02%, establishing a strong performance benchmark for this task",
    "checked": true,
    "id": "5b57be0227dc0f09aa9c46f04362104f824f8220",
    "semantic_title": "identifying medical self-disclosure in online communities",
    "citation_count": 20,
    "authors": [
      "Mina Valizadeh",
      "Pardis Ranjbar-Noiey",
      "Cornelia Caragea",
      "Natalie Parde"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.348": {
    "title": "Language in a (Search) Box: Grounding Language Learning in Real-World Human-Machine Interaction",
    "volume": "main",
    "abstract": "We investigate grounded language learning through real-world data, by modelling a teacher-learner dynamics through the natural interactions occurring between users and search engines; in particular, we explore the emergence of semantic generalization from unsupervised dense representations outside of synthetic environments. A grounding domain, a denotation function and a composition function are learned from user data only. We show how the resulting semantics for noun phrases exhibits compositional properties while being fully learnable without any explicit labelling. We benchmark our grounded semantics on compositionality and zero-shot inference tasks, and we show that it provides better results and better generalizations than SOTA non-grounded models, such as word2vec and BERT",
    "checked": true,
    "id": "ffd0b74c9760a25806912cdbfb14cebac69a34c6",
    "semantic_title": "language in a (search) box: grounding language learning in real-world human-machine interaction",
    "citation_count": 9,
    "authors": [
      "Federico Bianchi",
      "Ciro Greco",
      "Jacopo Tagliabue"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.349": {
    "title": "Finding Concept-specific Biases in Form–Meaning Associations",
    "volume": "main",
    "abstract": "This work presents an information-theoretic operationalisation of cross-linguistic non-arbitrariness. It is not a new idea that there are small, cross-linguistic associations between the forms and meanings of words. For instance, it has been claimed (Blasi et al., 2016) that the word for \"tongue\" is more likely than chance to contain the phone [l]. By controlling for the influence of language family and geographic proximity within a very large concept-aligned, cross-lingual lexicon, we extend methods previously used to detect within language non-arbitrariness (Pimentel et al., 2019) to measure cross-linguistic associations. We find that there is a significant effect of non-arbitrariness, but it is unsurprisingly small (less than 0.5% on average according to our information-theoretic estimate). We also provide a concept-level analysis which shows that a quarter of the concepts considered in our work exhibit a significant level of cross-linguistic non-arbitrariness. In sum, the paper provides new methods to detect cross-linguistic associations at scale, and confirms their effects are minor",
    "checked": true,
    "id": "886d79423b7e5a5e20c4d13b0aa3a45851fd9487",
    "semantic_title": "finding concept-specific biases in form–meaning associations",
    "citation_count": 5,
    "authors": [
      "Tiago Pimentel",
      "Brian Roark",
      "Søren Wichmann",
      "Ryan Cotterell",
      "Damián Blasi"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.350": {
    "title": "How (Non-)Optimal is the Lexicon?",
    "volume": "main",
    "abstract": "The mapping of lexical meanings to wordforms is a major feature of natural languages. While usage pressures might assign short words to frequent meanings (Zipf's law of abbreviation), the need for a productive and open-ended vocabulary, local constraints on sequences of symbols, and various other factors all shape the lexicons of the world's languages. Despite their importance in shaping lexical structure, the relative contributions of these factors have not been fully quantified. Taking a coding-theoretic view of the lexicon and making use of a novel generative statistical model, we define upper bounds for the compressibility of the lexicon under various constraints. Examining corpora from 7 typologically diverse languages, we use those upper bounds to quantify the lexicon's optimality and to explore the relative costs of major constraints on natural codes. We find that (compositional) morphology and graphotactics can sufficiently account for most of the complexity of natural codes—as measured by code length",
    "checked": true,
    "id": "0d74e4f02e74e4d81e6e8a47d7112e5182c4afa8",
    "semantic_title": "how (non-)optimal is the lexicon?",
    "citation_count": 20,
    "authors": [
      "Tiago Pimentel",
      "Irene Nikkarinen",
      "Kyle Mahowald",
      "Ryan Cotterell",
      "Damián Blasi"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.351": {
    "title": "Word Complexity is in the Eye of the Beholder",
    "volume": "main",
    "abstract": "Lexical complexity is a highly subjective notion, yet this factor is often neglected in lexical simplification and readability systems which use a \"one-size-fits-all\" approach. In this paper, we investigate which aspects contribute to the notion of lexical complexity in various groups of readers, focusing on native and non-native speakers of English, and how the notion of complexity changes depending on the proficiency level of a non-native reader. To facilitate reproducibility of our approach and foster further research into these aspects, we release a dataset of complex words annotated by readers with different backgrounds",
    "checked": true,
    "id": "17d136087612e32b94077b8040685c3751f887c6",
    "semantic_title": "word complexity is in the eye of the beholder",
    "citation_count": 12,
    "authors": [
      "Sian Gooding",
      "Ekaterina Kochmar",
      "Seid Muhie Yimam",
      "Chris Biemann"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.352": {
    "title": "Linguistic Complexity Loss in Text-Based Therapy",
    "volume": "main",
    "abstract": "The complexity loss paradox, which posits that individuals suffering from disease exhibit surprisingly predictable behavioral dynamics, has been observed in a variety of both human and animal physiological systems. The recent advent of online text-based therapy presents a new opportunity to analyze the complexity loss paradox in a novel operationalization: linguistic complexity loss in text-based therapy conversations. In this paper, we analyze linguistic complexity correlates of mental health in the online therapy messages sent between therapists and 7,170 clients who provided 30,437 corresponding survey responses on their anxiety. We found that when clients reported more anxiety, they showed reduced lexical diversity as estimated by the moving average type-token ratio. Therapists, on the other hand, used language of higher reading difficulty, syntactic complexity, and age of acquisition when clients were more anxious. Finally, we found that clients, and to an even greater extent, therapists, exhibited consistent levels of many linguistic complexity measures. These results demonstrate how linguistic analysis of text-based communication can be leveraged as a marker for anxiety, an exciting prospect in a time of both increased online communication and increased mental health issues",
    "checked": true,
    "id": "8af66c8149fc4afc57aa7e053dc88f983059b34c",
    "semantic_title": "linguistic complexity loss in text-based therapy",
    "citation_count": 6,
    "authors": [
      "Jason Wei",
      "Kelly Finn",
      "Emma Templeton",
      "Thalia Wheatley",
      "Soroush Vosoughi"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.353": {
    "title": "Ab Antiquo: Neural Proto-language Reconstruction",
    "volume": "main",
    "abstract": "Historical linguists have identified regularities in the process of historic sound change. The comparative method utilizes those regularities to reconstruct proto-words based on observed forms in daughter languages. Can this process be efficiently automated? We address the task of proto-word reconstruction, in which the model is exposed to cognates in contemporary daughter languages, and has to predict the proto word in the ancestor language. We provide a novel dataset for this task, encompassing over 8,000 comparative entries, and show that neural sequence models outperform conventional methods applied to this task so far. Error analysis reveals a variability in the ability of neural model to capture different phonological changes, correlating with the complexity of the changes. Analysis of learned embeddings reveals the models learn phonologically meaningful generalizations, corresponding to well-attested phonological shifts documented by historical linguistics",
    "checked": true,
    "id": "a394fbd8576cc263cb39ab37b25c609f83f1fdf1",
    "semantic_title": "ab antiquo: neural proto-language reconstruction",
    "citation_count": 28,
    "authors": [
      "Carlo Meloni",
      "Shauli Ravfogel",
      "Yoav Goldberg"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.354": {
    "title": "On Biasing Transformer Attention Towards Monotonicity",
    "volume": "main",
    "abstract": "Many sequence-to-sequence tasks in natural language processing are roughly monotonic in the alignment between source and target sequence, and previous work has facilitated or enforced learning of monotonic attention behavior via specialized attention functions or pretraining. In this work, we introduce a monotonicity loss function that is compatible with standard attention mechanisms and test it on several sequence-to-sequence tasks: grapheme-to-phoneme conversion, morphological inflection, transliteration, and dialect normalization. Experiments show that we can achieve largely monotonic behavior. Performance is mixed, with larger gains on top of RNN baselines. General monotonicity does not benefit transformer multihead attention, however, we see isolated improvements when only a subset of heads is biased towards monotonic behavior",
    "checked": true,
    "id": "109b5c0bade0dc285153d4c7f90d42a8af06126b",
    "semantic_title": "on biasing transformer attention towards monotonicity",
    "citation_count": 6,
    "authors": [
      "Annette Rios",
      "Chantal Amrhein",
      "Noëmi Aepli",
      "Rico Sennrich"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.355": {
    "title": "Extracting a Knowledge Base of Mechanisms from COVID-19 Papers",
    "volume": "main",
    "abstract": "The COVID-19 pandemic has spawned a diverse body of scientific literature that is challenging to navigate, stimulating interest in automated tools to help find useful knowledge. We pursue the construction of a knowledge base (KB) of mechanisms—a fundamental concept across the sciences, which encompasses activities, functions and causal relations, ranging from cellular processes to economic impacts. We extract this information from the natural language of scientific papers by developing a broad, unified schema that strikes a balance between relevance and breadth. We annotate a dataset of mechanisms with our schema and train a model to extract mechanism relations from papers. Our experiments demonstrate the utility of our KB in supporting interdisciplinary scientific search over COVID-19 literature, outperforming the prominent PubMed search in a study with clinical experts. Our search engine, dataset and code are publicly available",
    "checked": true,
    "id": "c4ce6aca9aed41d57d588674484932e0c2cd3547",
    "semantic_title": "extracting a knowledge base of mechanisms from covid-19 papers",
    "citation_count": 26,
    "authors": [
      "Tom Hope",
      "Aida Amini",
      "David Wadden",
      "Madeleine van Zuylen",
      "Sravanthi Parasa",
      "Eric Horvitz",
      "Daniel Weld",
      "Roy Schwartz",
      "Hannaneh Hajishirzi"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.356": {
    "title": "Constrained Multi-Task Learning for Event Coreference Resolution",
    "volume": "main",
    "abstract": "We propose a neural event coreference model in which event coreference is jointly trained with five tasks: trigger detection, entity coreference, anaphoricity determination, realis detection, and argument extraction. To guide the learning of this complex model, we incorporate cross-task consistency constraints into the learning process as soft constraints via designing penalty functions. In addition, we propose the novel idea of viewing entity coreference and event coreference as a single coreference task, which we believe is a step towards a unified model of coreference resolution. The resulting model achieves state-of-the-art results on the KBP 2017 event coreference dataset",
    "checked": true,
    "id": "88becacec14e4a7961b77fe79d2255aab886bf1d",
    "semantic_title": "constrained multi-task learning for event coreference resolution",
    "citation_count": 14,
    "authors": [
      "Jing Lu",
      "Vincent Ng"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.357": {
    "title": "Empirical Evaluation of Pre-trained Transformers for Human-Level NLP: The Role of Sample Size and Dimensionality",
    "volume": "main",
    "abstract": "In human-level NLP tasks, such as predicting mental health, personality, or demographics, the number of observations is often smaller than the standard 768+ hidden state sizes of each layer within modern transformer-based language models, limiting the ability to effectively leverage transformers. Here, we provide a systematic study on the role of dimension reduction methods (principal components analysis, factorization techniques, or multi-layer auto-encoders) as well as the dimensionality of embedding vectors and sample sizes as a function of predictive performance. We first find that fine-tuning large models with a limited amount of data pose a significant difficulty which can be overcome with a pre-trained dimension reduction regime. RoBERTa consistently achieves top performance in human-level tasks, with PCA giving benefit over other reduction methods in better handling users that write longer texts. Finally, we observe that a majority of the tasks achieve results comparable to the best performance with just 1/12 of the embedding dimensions",
    "checked": true,
    "id": "a5adcdf4be1463c3ad2a20b4ca0d78be37b28532",
    "semantic_title": "empirical evaluation of pre-trained transformers for human-level nlp: the role of sample size and dimensionality",
    "citation_count": 28,
    "authors": [
      "Adithya V Ganesan",
      "Matthew Matero",
      "Aravind Reddy Ravula",
      "Huy Vu",
      "H. Andrew Schwartz"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.358": {
    "title": "Leveraging Deep Representations of Radiology Reports in Survival Analysis for Predicting Heart Failure Patient Mortality",
    "volume": "main",
    "abstract": "Utilizing clinical texts in survival analysis is difficult because they are largely unstructured. Current automatic extraction models fail to capture textual information comprehensively since their labels are limited in scope. Furthermore, they typically require a large amount of data and high-quality expert annotations for training. In this work, we present a novel method of using BERT-based hidden layer representations of clinical texts as covariates for proportional hazards models to predict patient survival outcomes. We show that hidden layers yield notably more accurate predictions than predefined features, outperforming the previous baseline model by 5.7% on average across C-index and time-dependent AUC. We make our work publicly available at https://github.com/bionlplab/heart_failure_mortality",
    "checked": true,
    "id": "32e3dfdcded5c598085a6ee0f58c21b96bc4596d",
    "semantic_title": "leveraging deep representations of radiology reports in survival analysis for predicting heart failure patient mortality",
    "citation_count": 6,
    "authors": [
      "Hyun Gi Lee",
      "Evan Sholle",
      "Ashley Beecy",
      "Subhi Al’Aref",
      "Yifan Peng"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.359": {
    "title": "On the Use of Context for Predicting Citation Worthiness of Sentences in Scholarly Articles",
    "volume": "main",
    "abstract": "In this paper, we study the importance of context in predicting the citation worthiness of sentences in scholarly articles. We formulate this problem as a sequence labeling task solved using a hierarchical BiLSTM model. We contribute a new benchmark dataset containing over two million sentences and their corresponding labels. We preserve the sentence order in this dataset and perform document-level train/test splits, which importantly allows incorporating contextual information in the modeling process. We evaluate the proposed approach on three benchmark datasets. Our results quantify the benefits of using context and contextual embeddings for citation worthiness. Lastly, through error analysis, we provide insights into cases where context plays an essential role in predicting citation worthiness",
    "checked": true,
    "id": "5bdf8bb03ab5690fc8c1a4b303225af786361c91",
    "semantic_title": "on the use of context for predicting citation worthiness of sentences in scholarly articles",
    "citation_count": 9,
    "authors": [
      "Rakesh Gosangi",
      "Ravneet Arora",
      "Mohsen Gheisarieha",
      "Debanjan Mahata",
      "Haimin Zhang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.360": {
    "title": "Data and Model Distillation as a Solution for Domain-transferable Fact Verification",
    "volume": "main",
    "abstract": "While neural networks produce state-of-the-art performance in several NLP tasks, they generally depend heavily on lexicalized information, which transfer poorly between domains. We present a combination of two strategies to mitigate this dependence on lexicalized information in fact verification tasks. We present a data distillation technique for delexicalization, which we then combine with a model distillation method to prevent aggressive data distillation. We show that by using our solution, not only does the performance of an existing state-of-the-art model remain at par with that of the model trained on a fully lexicalized data, but it also performs better than it when tested out of domain. We show that the technique we present encourages models to extract transferable facts from a given fact verification dataset",
    "checked": true,
    "id": "59775b6439fd8f87b61e71347b079fc5eb127476",
    "semantic_title": "data and model distillation as a solution for domain-transferable fact verification",
    "citation_count": 4,
    "authors": [
      "Mitch Paul Mithun",
      "Sandeep Suntwal",
      "Mihai Surdeanu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.361": {
    "title": "Adapting Coreference Resolution for Processing Violent Death Narratives",
    "volume": "main",
    "abstract": "Coreference resolution is an important compo-nent in analyzing narrative text from admin-istrative data (e.g., clinical or police sources).However, existing coreference models trainedon general language corpora suffer from poortransferability due to domain gaps, especiallywhen they are applied to gender-inclusive datawith lesbian, gay, bisexual, and transgender(LGBT) individuals. In this paper, we an-alyzed the challenges of coreference resolu-tion in an exemplary form of administrativetext written in English: violent death nar-ratives from the USA's Centers for DiseaseControl's (CDC) National Violent Death Re-porting System. We developed a set of dataaugmentation rules to improve model perfor-mance using a probabilistic data programmingframework. Experiments on narratives froman administrative database, as well as existinggender-inclusive coreference datasets, demon-strate the effectiveness of data augmentationin training coreference models that can betterhandle text data about LGBT individuals",
    "checked": true,
    "id": "cb3b7387b7c465c5aa1d54c207ef4a11f28248d4",
    "semantic_title": "adapting coreference resolution for processing violent death narratives",
    "citation_count": 8,
    "authors": [
      "Ankith Uppunda",
      "Susan Cochran",
      "Jacob Foster",
      "Alina Arseniev-Koehler",
      "Vickie Mays",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.362": {
    "title": "Time-Stamped Language Model: Teaching Language Models to Understand The Flow of Events",
    "volume": "main",
    "abstract": "Tracking entities throughout a procedure described in a text is challenging due to the dynamic nature of the world described in the process. Firstly, we propose to formulate this task as a question answering problem. This enables us to use pre-trained transformer-based language models on other QA benchmarks by adapting those to the procedural text understanding. Secondly, since the transformer-based language models cannot encode the flow of events by themselves, we propose a Time-Stamped Language Model (TSLM) to encode event information in LMs architecture by introducing the timestamp encoding. Our model evaluated on the Propara dataset shows improvements on the published state-of-the-art results with a 3.1% increase in F1 score. Moreover, our model yields better results on the location prediction task on the NPN-Cooking dataset. This result indicates that our approach is effective for procedural text understanding in general",
    "checked": true,
    "id": "16d730799318f7e621b835c075d75034a7fc6b8e",
    "semantic_title": "time-stamped language model: teaching language models to understand the flow of events",
    "citation_count": 22,
    "authors": [
      "Hossein Rajaby Faghihi",
      "Parisa Kordjamshidi"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.363": {
    "title": "If You Want to Go Far Go Together: Unsupervised Joint Candidate Evidence Retrieval for Multi-hop Question Answering",
    "volume": "main",
    "abstract": "Multi-hop reasoning requires aggregation and inference from multiple facts. To retrieve such facts, we propose a simple approach that retrieves and reranks set of evidence facts jointly. Our approach first generates unsupervised clusters of sentences as candidate evidence by accounting links between sentences and coverage with the given query. Then, a RoBERTa-based reranker is trained to bring the most representative evidence cluster to the top. We specifically emphasize on the importance of retrieving evidence jointly by showing several comparative analyses to other methods that retrieve and rerank evidence sentences individually. First, we introduce several attention- and embedding-based analyses, which indicate that jointly retrieving and reranking approaches can learn compositional knowledge required for multi-hop reasoning. Second, our experiments show that jointly retrieving candidate evidence leads to substantially higher evidence retrieval performance when fed to the same supervised reranker. In particular, our joint retrieval and then reranking approach achieves new state-of-the-art evidence retrieval performance on two multi-hop question answering (QA) datasets: 30.5 Recall@2 on QASC, and 67.6% F1 on MultiRC. When the evidence text from our joint retrieval approach is fed to a RoBERTa-based answer selection classifier, we achieve new state-of-the-art QA performance on MultiRC and second best result on QASC",
    "checked": true,
    "id": "02cc9b6ef7db3ceb21a09661534429336ceba259",
    "semantic_title": "if you want to go far go together: unsupervised joint candidate evidence retrieval for multi-hop question answering",
    "citation_count": 10,
    "authors": [
      "Vikas Yadav",
      "Steven Bethard",
      "Mihai Surdeanu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.364": {
    "title": "SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning",
    "volume": "main",
    "abstract": "This paper proposes a question-answering (QA) benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior work and is challenging for state-of-the-art language models (LM). We propose a distant supervision method to improve on this task. Specifically, we design grammar and reasoning rules to automatically generate a spatial description of visual scenes and corresponding QA pairs. Experiments show that further pretraining LMs on these automatically generated data significantly improves LMs' capability on spatial understanding, which in turn helps to better solve two external datasets, bAbI, and boolQ. We hope that this work can foster investigations into more sophisticated models for spatial reasoning over text",
    "checked": true,
    "id": "de74100ac5a16f9169b8a7fe98dd05647b0e8777",
    "semantic_title": "spartqa: a textual question answering benchmark for spatial reasoning",
    "citation_count": 40,
    "authors": [
      "Roshanak Mirzaee",
      "Hossein Rajaby Faghihi",
      "Qiang Ning",
      "Parisa Kordjamshidi"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.365": {
    "title": "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers",
    "volume": "main",
    "abstract": "Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present Qasper, a dataset of 5049 questions over 1585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate",
    "checked": true,
    "id": "4e3935ef7da6bcbb202ec7f8b285c313cadcd044",
    "semantic_title": "a dataset of information-seeking questions and answers anchored in research papers",
    "citation_count": 182,
    "authors": [
      "Pradeep Dasigi",
      "Kyle Lo",
      "Iz Beltagy",
      "Arman Cohan",
      "Noah A. Smith",
      "Matt Gardner"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.366": {
    "title": "Differentiable Open-Ended Commonsense Reasoning",
    "volume": "main",
    "abstract": "Current commonsense reasoning research focuses on developing models that use commonsense knowledge to answer multiple-choice questions. However, systems designed to answer multiple-choice questions may not be useful in applications that do not provide a small list of candidate answers to choose from. As a step towards making commonsense reasoning research more realistic, we propose to study open-ended commonsense reasoning (OpenCSR) — the task of answering a commonsense question without any pre-defined choices — using as a resource only a corpus of commonsense facts written in natural language. OpenCSR is challenging due to a large decision space, and because many questions require implicit multi-hop reasoning. As an approach to OpenCSR, we propose DrFact, an efficient Differentiable model for multi-hop Reasoning over knowledge Facts. To evaluate OpenCSR methods, we adapt several popular commonsense reasoning benchmarks, and collect multiple new answers for each test question via crowd-sourcing. Experiments show that DrFact outperforms strong baseline methods by a large margin",
    "checked": true,
    "id": "6027ef3b4e5585b45db0b9d333956425d3972351",
    "semantic_title": "differentiable open-ended commonsense reasoning",
    "citation_count": 38,
    "authors": [
      "Bill Yuchen Lin",
      "Haitian Sun",
      "Bhuwan Dhingra",
      "Manzil Zaheer",
      "Xiang Ren",
      "William Cohen"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.367": {
    "title": "Does Structure Matter? Encoding Documents for Machine Reading Comprehension",
    "volume": "main",
    "abstract": "Machine reading comprehension is a challenging task especially for querying documents with deep and interconnected contexts. Transformer-based methods have shown advanced performances on this task; however, most of them still treat documents as a flat sequence of tokens. This work proposes a new Transformer-based method that reads a document as tree slices. It contains two modules for identifying more relevant text passage and the best answer span respectively, which are not only jointly trained but also jointly consulted at inference time. Our evaluation results show that our proposed method outperforms several competitive baseline approaches on two datasets from varied domains",
    "checked": true,
    "id": "7af513e8a395b82628dbf627fe9269a44cea14ae",
    "semantic_title": "does structure matter? encoding documents for machine reading comprehension",
    "citation_count": 6,
    "authors": [
      "Hui Wan",
      "Song Feng",
      "Chulaka Gunasekara",
      "Siva Sankalp Patel",
      "Sachindra Joshi",
      "Luis Lastras"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.368": {
    "title": "Multi-Step Reasoning Over Unstructured Text with Beam Dense Retrieval",
    "volume": "main",
    "abstract": "Complex question answering often requires finding a reasoning chain that consists of multiple evidence pieces. Current approaches incorporate the strengths of structured knowledge and unstructured text, assuming text corpora is semi-structured. Building on dense retrieval methods, we propose a new multi-step retrieval approach (BeamDR) that iteratively forms an evidence chain through beam search in dense representations. When evaluated on multi-hop question answering, BeamDR is competitive to state-of-the-art systems, without using any semi-structured information. Through query composition in dense space, BeamDR captures the implicit relationships between evidence in the reasoning chain. The code is available at https://github.com/henryzhao5852/BeamDR",
    "checked": true,
    "id": "54e82e10962452cfc158832590a78ed9133efc36",
    "semantic_title": "multi-step reasoning over unstructured text with beam dense retrieval",
    "citation_count": 27,
    "authors": [
      "Chen Zhao",
      "Chenyan Xiong",
      "Jordan Boyd-Graber",
      "Hal Daumé III"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.369": {
    "title": "Scalable and Interpretable Semantic Change Detection",
    "volume": "main",
    "abstract": "Several cluster-based methods for semantic change detection with contextual embeddings emerged recently. They allow a fine-grained analysis of word use change by aggregating embeddings into clusters that reflect the different usages of the word. However, these methods are unscalable in terms of memory consumption and computation time. Therefore, they require a limited set of target words to be picked in advance. This drastically limits the usability of these methods in open exploratory tasks, where each word from the vocabulary can be considered as a potential target. We propose a novel scalable method for word usage-change detection that offers large gains in processing time and significant memory savings while offering the same interpretability and better performance than unscalable methods. We demonstrate the applicability of the proposed method by analysing a large corpus of news articles about COVID-19",
    "checked": true,
    "id": "0c4442693f2403d5f4c44c06663200f0b82324d5",
    "semantic_title": "scalable and interpretable semantic change detection",
    "citation_count": 53,
    "authors": [
      "Syrielle Montariol",
      "Matej Martinc",
      "Lidia Pivovarova"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.370": {
    "title": "Scalar Adjective Identification and Multilingual Ranking",
    "volume": "main",
    "abstract": "The intensity relationship that holds between scalar adjectives (e.g., nice < great < wonderful) is highly relevant for natural language inference and common-sense reasoning. Previous research on scalar adjective ranking has focused on English, mainly due to the availability of datasets for evaluation. We introduce a new multilingual dataset in order to promote research on scalar adjectives in new languages. We perform a series of experiments and set performance baselines on this dataset, using monolingual and multilingual contextual language models. Additionally, we introduce a new binary classification task for English scalar adjective identification which examines the models' ability to distinguish scalar from relational adjectives. We probe contextualised representations and report baseline results for future comparison on this task",
    "checked": true,
    "id": "6498f3a9101bc8728c9013e75cfbbefbf5b24e5e",
    "semantic_title": "scalar adjective identification and multilingual ranking",
    "citation_count": 6,
    "authors": [
      "Aina Garí Soler",
      "Marianna Apidianaki"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.371": {
    "title": "ESC: Redesigning WSD with Extractive Sense Comprehension",
    "volume": "main",
    "abstract": "Word Sense Disambiguation (WSD) is a historical NLP task aimed at linking words in contexts to discrete sense inventories and it is usually cast as a multi-label classification task. Recently, several neural approaches have employed sense definitions to better represent word meanings. Yet, these approaches do not observe the input sentence and the sense definition candidates all at once, thus potentially reducing the model performance and generalization power. We cope with this issue by reframing WSD as a span extraction problem — which we called Extractive Sense Comprehension (ESC) — and propose ESCHER, a transformer-based neural architecture for this new formulation. By means of an extensive array of experiments, we show that ESC unleashes the full potential of our model, leading it to outdo all of its competitors and to set a new state of the art on the English WSD task. In the few-shot scenario, ESCHER proves to exploit training data efficiently, attaining the same performance as its closest competitor while relying on almost three times fewer annotations. Furthermore, ESCHER can nimbly combine data annotated with senses from different lexical resources, achieving performances that were previously out of everyone's reach. The model along with data is available at https://github.com/SapienzaNLP/esc",
    "checked": true,
    "id": "486df033e3689be451a5e6137b88493d01c8de43",
    "semantic_title": "esc: redesigning wsd with extractive sense comprehension",
    "citation_count": 68,
    "authors": [
      "Edoardo Barba",
      "Tommaso Pasini",
      "Roberto Navigli"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.372": {
    "title": "Recent advances in neural metaphor processing: A linguistic, cognitive and social perspective",
    "volume": "main",
    "abstract": "Metaphor is an indispensable part of human cognition and everyday communication. Much research has been conducted elucidating metaphor processing in the mind/brain and the role it plays in communication. in recent years, metaphor processing systems have benefited greatly from these studies, as well as the rapid advances in deep learning for natural language processing (NLP). This paper provides a comprehensive review and discussion of recent developments in automated metaphor processing, in light of the findings about metaphor in the mind, language, and communication, and from the perspective of downstream NLP tasks",
    "checked": true,
    "id": "adc6837c96ee82d7409727a51701406a3be84d50",
    "semantic_title": "recent advances in neural metaphor processing: a linguistic, cognitive and social perspective",
    "citation_count": 33,
    "authors": [
      "Xiaoyu Tong",
      "Ekaterina Shutova",
      "Martha Lewis"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.373": {
    "title": "Constructing Taxonomies from Pretrained Language Models",
    "volume": "main",
    "abstract": "We present a method for constructing taxonomic trees (e.g., WordNet) using pretrained language models. Our approach is composed of two modules, one that predicts parenthood relations and another that reconciles those pairwise predictions into trees. The parenthood prediction module produces likelihood scores for each potential parent-child pair, creating a graph of parent-child relation scores. The tree reconciliation module treats the task as a graph optimization problem and outputs the maximum spanning tree of this graph. We train our model on subtrees sampled from WordNet, and test on nonoverlapping WordNet subtrees. We show that incorporating web-retrieved glosses can further improve performance. On the task of constructing subtrees of English WordNet, the model achieves 66.7 ancestor F1, a 20.0% relative increase over the previous best published result on this task. In addition, we convert the original English dataset into nine other languages using Open Multilingual WordNet and extend our results across these languages",
    "checked": true,
    "id": "d877020a6808f0d4baded6d6cd92d09bbc46bde2",
    "semantic_title": "constructing taxonomies from pretrained language models",
    "citation_count": 21,
    "authors": [
      "Catherine Chen",
      "Kevin Lin",
      "Dan Klein"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.374": {
    "title": "Event Representation with Sequential, Semi-Supervised Discrete Variables",
    "volume": "main",
    "abstract": "Within the context of event modeling and understanding, we propose a new method for neural sequence modeling that takes partially-observed sequences of discrete, external knowledge into account. We construct a sequential neural variational autoencoder, which uses Gumbel-Softmax reparametrization within a carefully defined encoder, to allow for successful backpropagation during training. The core idea is to allow semi-supervised external discrete knowledge to guide, but not restrict, the variational latent parameters during training. Our experiments indicate that our approach not only outperforms multiple baselines and the state-of-the-art in narrative script induction, but also converges more quickly",
    "checked": true,
    "id": "a9d4b04e606481a4a2d97cac03fa0352756d71f1",
    "semantic_title": "event representation with sequential, semi-supervised discrete variables",
    "citation_count": 12,
    "authors": [
      "Mehdi Rezaee",
      "Francis Ferraro"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.375": {
    "title": "Seq2Emo: A Sequence to Multi-Label Emotion Classification Model",
    "volume": "main",
    "abstract": "Multi-label emotion classification is an important task in NLP and is essential to many applications. In this work, we propose a sequence-to-emotion (Seq2Emo) approach, which implicitly models emotion correlations in a bi-directional decoder. Experiments on SemEval'18 and GoEmotions datasets show that our approach outperforms state-of-the-art methods (without using external data). In particular, Seq2Emo outperforms the binary relevance (BR) and classifier chain (CC) approaches in a fair setting",
    "checked": true,
    "id": "3a7af14915e8d1a3de470ba6fe3b4d620becb85c",
    "semantic_title": "seq2emo: a sequence to multi-label emotion classification model",
    "citation_count": 24,
    "authors": [
      "Chenyang Huang",
      "Amine Trabelsi",
      "Xuebin Qin",
      "Nawshad Farruque",
      "Lili Mou",
      "Osmar Zaïane"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.376": {
    "title": "Knowledge Enhanced Masked Language Model for Stance Detection",
    "volume": "main",
    "abstract": "Detecting stance on Twitter is especially challenging because of the short length of each tweet, the continuous coinage of new terminology and hashtags, and the deviation of sentence structure from standard prose. Fine-tuned language models using large-scale in-domain data have been shown to be the new state-of-the-art for many NLP tasks, including stance detection. In this paper, we propose a novel BERT-based fine-tuning method that enhances the masked language model for stance detection. Instead of random token masking, we propose using a weighted log-odds-ratio to identify words with high stance distinguishability and then model an attention mechanism that focuses on these words. We show that our proposed approach outperforms the state of the art for stance detection on Twitter data about the 2020 US Presidential election",
    "checked": true,
    "id": "c80f6f1cc4d05068e5f564c61450c9f192ea7a52",
    "semantic_title": "knowledge enhanced masked language model for stance detection",
    "citation_count": 80,
    "authors": [
      "Kornraphop Kawintiranon",
      "Lisa Singh"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.377": {
    "title": "Learning Paralinguistic Features from Audiobooks through Style Voice Conversion",
    "volume": "main",
    "abstract": "Paralinguistics, the non-lexical components of speech, play a crucial role in human-human interaction. Models designed to recognize paralinguistic information, particularly speech emotion and style, are difficult to train because of the limited labeled datasets available. In this work, we present a new framework that enables a neural network to learn to extract paralinguistic attributes from speech using data that are not annotated for emotion. We assess the utility of the learned embeddings on the downstream tasks of emotion recognition and speaking style detection, demonstrating significant improvements over surface acoustic features as well as over embeddings extracted from other unsupervised approaches. Our work enables future systems to leverage the learned embedding extractor as a separate component capable of highlighting the paralinguistic components of speech",
    "checked": true,
    "id": "013c2778cd6853a61a80caed6b49e2767609ce76",
    "semantic_title": "learning paralinguistic features from audiobooks through style voice conversion",
    "citation_count": 1,
    "authors": [
      "Zakaria Aldeneh",
      "Matthew Perez",
      "Emily Mower Provost"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.378": {
    "title": "Adapting BERT for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks",
    "volume": "main",
    "abstract": "This paper studies continual learning (CL) of a sequence of aspect sentiment classification (ASC) tasks. Although some CL techniques have been proposed for document sentiment classification, we are not aware of any CL work on ASC. A CL system that incrementally learns a sequence of ASC tasks should address the following two issues: (1) transfer knowledge learned from previous tasks to the new task to help it learn a better model, and (2) maintain the performance of the models for previous tasks so that they are not forgotten. This paper proposes a novel capsule network based model called B-CL to address these issues. B-CL markedly improves the ASC performance on both the new task and the old tasks via forward and backward knowledge transfer. The effectiveness of B-CL is demonstrated through extensive experiments",
    "checked": true,
    "id": "916f8ae4dad6ee8a23e50c3fb0729a31aed17f2d",
    "semantic_title": "adapting bert for continual learning of a sequence of aspect sentiment classification tasks",
    "citation_count": 74,
    "authors": [
      "Zixuan Ke",
      "Hu Xu",
      "Bing Liu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.379": {
    "title": "Adversarial Learning for Zero-Shot Stance Detection on Social Media",
    "volume": "main",
    "abstract": "Stance detection on social media can help to identify and understand slanted news or commentary in everyday life. In this work, we propose a new model for zero-shot stance detection on Twitter that uses adversarial learning to generalize across topics. Our model achieves state-of-the-art performance on a number of unseen test topics with minimal computational costs. In addition, we extend zero-shot stance detection to topics not previously considered, highlighting future directions for zero-shot transfer",
    "checked": true,
    "id": "4e9d4b3d89b4dec3d313c3295c70d3afff6ae50f",
    "semantic_title": "adversarial learning for zero-shot stance detection on social media",
    "citation_count": 77,
    "authors": [
      "Emily Allaway",
      "Malavika Srikanth",
      "Kathleen McKeown"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.380": {
    "title": "Efficiently Summarizing Text and Graph Encodings of Multi-Document Clusters",
    "volume": "main",
    "abstract": "This paper presents an efficient graph-enhanced approach to multi-document summarization (MDS) with an encoder-decoder Transformer model. This model is based on recent advances in pre-training both encoder and decoder on very large text data (Lewis et al., 2019), and it incorporates an efficient encoding mechanism (Beltagy et al., 2020) that avoids the quadratic memory growth typical for traditional Transformers. We show that this powerful combination not only scales to large input documents commonly found when summarizing news clusters; it also enables us to process additional input in the form of auxiliary graph representations, which we derive from the multi-document clusters. We present a mechanism to incorporate such graph information into the encoder-decoder model that was pre-trained on text only. Our approach leads to significant improvements on the Multi-News dataset, overall leading to an average 1.8 ROUGE score improvement over previous work (Li et al., 2020). We also show improvements in a transfer-only setup on the DUC-2004 dataset. The graph encodings lead to summaries that are more abstractive. Human evaluation shows that they are also more informative and factually more consistent with their input documents",
    "checked": true,
    "id": "42e41ab2211b8ba78e36326ea21e05bd25d92c42",
    "semantic_title": "efficiently summarizing text and graph encodings of multi-document clusters",
    "citation_count": 63,
    "authors": [
      "Ramakanth Pasunuru",
      "Mengwen Liu",
      "Mohit Bansal",
      "Sujith Ravi",
      "Markus Dreyer"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.381": {
    "title": "Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization",
    "volume": "main",
    "abstract": "Abstractive summarization, the task of generating a concise summary of input documents, requires: (1) reasoning over the source document to determine the salient pieces of information scattered across the long document, and (2) composing a cohesive text by reconstructing these salient facts into a shorter summary that faithfully reflects the complex relations connecting these facts. In this paper, we adapt TP-Transformer (Schlag et al., 2019), an architecture that enriches the original Transformer (Vaswani et al., 2017) with the explicitly compositional Tensor Product Representation (TPR), for the task of abstractive summarization. The key feature of our model is a structural bias that we introduce by encoding two separate representations for each token to represent the syntactic structure (with role vectors) and semantic content (with filler vectors) separately. The model then binds the role and filler vectors into the TPR as the layer output. We argue that the structured intermediate representations enable the model to take better control of the contents (salient facts) and structures (the syntax that connects the facts) when generating the summary. Empirically, we show that our TP-Transformer outperforms the Transformer and the original TP-Transformer significantly on several abstractive summarization datasets based on both automatic and human evaluations. On several syntactic and semantic probing tasks, we demonstrate the emergent structural information in the role vectors and the performance gain by information specificity of the role vectors and improved syntactic interpretability in the TPR layer outputs.(Code and models are available at https://github.com/jiangycTarheel/TPT-Summ)",
    "checked": true,
    "id": "45f7c0caf0f1f88a568279de52d7a8c29aa8e28f",
    "semantic_title": "enriching transformers with structured tensor-product representations for abstractive summarization",
    "citation_count": 16,
    "authors": [
      "Yichen Jiang",
      "Asli Celikyilmaz",
      "Paul Smolensky",
      "Paul Soulos",
      "Sudha Rao",
      "Hamid Palangi",
      "Roland Fernandez",
      "Caitlin Smith",
      "Mohit Bansal",
      "Jianfeng Gao"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.382": {
    "title": "What's in a Summary? Laying the Groundwork for Advances in Hospital-Course Summarization",
    "volume": "main",
    "abstract": "Summarization of clinical narratives is a long-standing research problem. Here, we introduce the task of hospital-course summarization. Given the documentation authored throughout a patient's hospitalization, generate a paragraph that tells the story of the patient admission. We construct an English, text-to-text dataset of 109,000 hospitalizations (2M source notes) and their corresponding summary proxy: the clinician-authored \"Brief Hospital Course\" paragraph written as part of a discharge note. Exploratory analyses reveal that the BHC paragraphs are highly abstractive with some long extracted fragments; are concise yet comprehensive; differ in style and content organization from the source notes; exhibit minimal lexical cohesion; and represent silver-standard references. Our analysis identifies multiple implications for modeling this complex, multi-document summarization task",
    "checked": true,
    "id": "30370906b75eb04792b3f19763292996671517fd",
    "semantic_title": "what's in a summary? laying the groundwork for advances in hospital-course summarization",
    "citation_count": 43,
    "authors": [
      "Griffin Adams",
      "Emily Alsentzer",
      "Mert Ketenci",
      "Jason Zucker",
      "Noémie Elhadad"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.383": {
    "title": "Understanding Factuality in Abstractive Summarization with FRANK: A Benchmark for Factuality Metrics",
    "volume": "main",
    "abstract": "Modern summarization models generate highly fluent but often factually unreliable outputs. This motivated a surge of metrics attempting to measure the factuality of automatically generated summaries. Due to the lack of common benchmarks, these metrics cannot be compared. Moreover, all these methods treat factuality as a binary concept and fail to provide deeper insights on the kinds of inconsistencies made by different systems. To address these limitations, we devise a typology of factual errors and use it to collect human annotations of generated summaries from state-of-the-art summarization systems for the CNN/DM and XSum datasets. Through these annotations we identify the proportion of different categories of factual errors and benchmark factuality metrics, showing their correlation with human judgement as well as their specific strengths and weaknesses",
    "checked": true,
    "id": "667bdd2a8dc997d40c106ff6761babebe4050762",
    "semantic_title": "understanding factuality in abstractive summarization with frank: a benchmark for factuality metrics",
    "citation_count": 257,
    "authors": [
      "Artidoro Pagnoni",
      "Vidhisha Balachandran",
      "Yulia Tsvetkov"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.384": {
    "title": "GSum: A General Framework for Guided Neural Abstractive Summarization",
    "volume": "main",
    "abstract": "Neural abstractive summarization models are flexible and can produce coherent summaries, but they are sometimes unfaithful and can be difficult to control. While previous studies attempt to provide different types of guidance to control the output and increase faithfulness, it is not clear how these strategies compare and contrast to each other. In this paper, we propose a general and extensible guided summarization framework (GSum) that can effectively take different kinds of external guidance as input, and we perform experiments across several different varieties. Experiments demonstrate that this model is effective, achieving state-of-the-art performance according to ROUGE on 4 popular summarization datasets when using highlighted sentences as guidance. In addition, we show that our guided model can generate more faithful summaries and demonstrate how different types of guidance generate qualitatively different summaries, lending a degree of controllability to the learned models",
    "checked": true,
    "id": "d38b686b8b68d0b91b294fd8a55ac7dea191706f",
    "semantic_title": "gsum: a general framework for guided neural abstractive summarization",
    "citation_count": 229,
    "authors": [
      "Zi-Yi Dou",
      "Pengfei Liu",
      "Hiroaki Hayashi",
      "Zhengbao Jiang",
      "Graham Neubig"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.385": {
    "title": "What Will it Take to Fix Benchmarking in Natural Language Understanding?",
    "volume": "main",
    "abstract": "Evaluation for many natural language understanding (NLU) tasks is broken: Unreliable and biased systems score so highly on standard benchmarks that there is little room for researchers who develop better systems to demonstrate their improvements. The recent trend to abandon IID benchmarks in favor of adversarially-constructed, out-of-distribution test sets ensures that current models will perform poorly, but ultimately only obscures the abilities that we want our benchmarks to measure. In this position paper, we lay out four criteria that we argue NLU benchmarks should meet. We argue most current benchmarks fail at these criteria, and that adversarial data collection does not meaningfully address the causes of these failures. Instead, restoring a healthy evaluation ecosystem will require significant progress in the design of benchmark datasets, the reliability with which they are annotated, their size, and the ways they handle social bias",
    "checked": true,
    "id": "a62bbc8ec3de4108a31b3fcc3403b659e7de5116",
    "semantic_title": "what will it take to fix benchmarking in natural language understanding?",
    "citation_count": 120,
    "authors": [
      "Samuel R. Bowman",
      "George Dahl"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.386": {
    "title": "TuringAdvice: A Generative and Dynamic Evaluation of Language Use",
    "volume": "main",
    "abstract": "We propose TuringAdvice, a new challenge task and dataset for language understanding models. Given a written situation that a real person is currently facing, a model must generate helpful advice in natural language. Our evaluation framework tests a fundamental aspect of human language understanding: our ability to use language to resolve open-ended situations by communicating with each other. Empirical results show that today's models struggle at TuringAdvice, even multibillion parameter models finetuned on 600k in-domain training examples. The best model, T5, writes advice that is at least as helpful as human-written advice in only 14% of cases; a much larger non-finetunable GPT3 model does even worse at 4%. This low performance reveals language understanding errors that are hard to spot outside of a generative setting, showing much room for progress",
    "checked": true,
    "id": "9983dd0b93d268c31cc821b9f4071372d191000e",
    "semantic_title": "turingadvice: a generative and dynamic evaluation of language use",
    "citation_count": 23,
    "authors": [
      "Rowan Zellers",
      "Ari Holtzman",
      "Elizabeth Clark",
      "Lianhui Qin",
      "Ali Farhadi",
      "Yejin Choi"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.387": {
    "title": "Multitask Learning for Emotionally Analyzing Sexual Abuse Disclosures",
    "volume": "main",
    "abstract": "The #MeToo movement on social media platforms initiated discussions over several facets of sexual harassment in our society. Prior work by the NLP community for automated identification of the narratives related to sexual abuse disclosures barely explored this social phenomenon as an independent task. However, emotional attributes associated with textual conversations related to the #MeToo social movement are complexly intertwined with such narratives. We formulate the task of identifying narratives related to the sexual abuse disclosures in online posts as a joint modeling task that leverages their emotional attributes through multitask learning. Our results demonstrate that positive knowledge transfer via context-specific shared representations of a flexible cross-stitched parameter sharing model helps establish the inherent benefit of jointly modeling tasks related to sexual abuse disclosures with emotion classification from the text in homogeneous and heterogeneous settings. We show how for more domain-specific tasks related to sexual abuse disclosures such as sarcasm identification and dialogue act (refutation, justification, allegation) classification, homogeneous multitask learning is helpful, whereas for more general tasks such as stance and hate speech detection, heterogeneous multitask learning with emotion classification works better",
    "checked": true,
    "id": "99d2aafb08d7e3f2f3d1ff5c82ea38f1ee3148f2",
    "semantic_title": "multitask learning for emotionally analyzing sexual abuse disclosures",
    "citation_count": 7,
    "authors": [
      "Ramit Sawhney",
      "Puneet Mathur",
      "Taru Jain",
      "Akash Kumar Gautam",
      "Rajiv Ratn Shah"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.388": {
    "title": "Self Promotion in US Congressional Tweets",
    "volume": "main",
    "abstract": "Prior studies have found that women self-promote less than men due to gender stereotypes. In this study we built a BERT-based NLP model to predict whether a Congressional tweet shows self-promotion or not and then used this model to examine whether a gender gap in self-promotion exists among Congressional tweets. After analyzing 2 million Congressional tweets from July 2017 to March 2021, controlling for a number of factors that include political party, chamber, age, number of terms in Congress, number of daily tweets, and number of followers, we found that women in Congress actually perform more self-promotion on Twitter, indicating a reversal of traditional gender norms where women self-promote less than men",
    "checked": true,
    "id": "48e8046500f442ca42c75ff0f58e923f42ade9ac",
    "semantic_title": "self promotion in us congressional tweets",
    "citation_count": 3,
    "authors": [
      "Jun Wang",
      "Kelly Cui",
      "Bei Yu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.389": {
    "title": "Profiling of Intertextuality in Latin Literature Using Word Embeddings",
    "volume": "main",
    "abstract": "Identifying intertextual relationships between authors is of central importance to the study of literature. We report an empirical analysis of intertextuality in classical Latin literature using word embedding models. To enable quantitative evaluation of intertextual search methods, we curate a new dataset of 945 known parallels drawn from traditional scholarship on Latin epic poetry. We train an optimized word2vec model on a large corpus of lemmatized Latin, which achieves state-of-the-art performance for synonym detection and outperforms a widely used lexical method for intertextual search. We then demonstrate that training embeddings on very small corpora can capture salient aspects of literary style and apply this approach to replicate a previous intertextual study of the Roman historian Livy, which relied on hand-crafted stylometric features. Our results advance the development of core computational resources for a major premodern language and highlight a productive avenue for cross-disciplinary collaboration between the study of literature and NLP",
    "checked": true,
    "id": "7a962e74251015cbad6956dde4782404fa37da47",
    "semantic_title": "profiling of intertextuality in latin literature using word embeddings",
    "citation_count": 9,
    "authors": [
      "Patrick J. Burns",
      "James A. Brofos",
      "Kyle Li",
      "Pramit Chaudhuri",
      "Joseph P. Dexter"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.390": {
    "title": "Identifying inherent disagreement in natural language inference",
    "volume": "main",
    "abstract": "Natural language inference (NLI) is the task of determining whether a piece of text is entailed, contradicted by or unrelated to another piece of text. In this paper, we investigate how to tease systematic inferences (i.e., items for which people agree on the NLI label) apart from disagreement items (i.e., items which lead to different annotations), which most prior work has overlooked. To distinguish systematic inferences from disagreement items, we propose Artificial Annotators (AAs) to simulate the uncertainty in the annotation process by capturing the modes in annotations. Results on the CommitmentBank, a corpus of naturally occurring discourses in English, confirm that our approach performs statistically significantly better than all baselines. We further show that AAs learn linguistic patterns and context-dependent reasoning",
    "checked": true,
    "id": "f4dda4524bba281141829aaf6967431f59257e74",
    "semantic_title": "identifying inherent disagreement in natural language inference",
    "citation_count": 19,
    "authors": [
      "Xinliang Frederick Zhang",
      "Marie-Catherine de Marneffe"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.391": {
    "title": "Modeling Human Mental States with an Entity-based Narrative Graph",
    "volume": "main",
    "abstract": "Understanding narrative text requires capturing characters' motivations, goals, and mental states. This paper proposes an Entity-based Narrative Graph (ENG) to model the internal- states of characters in a story. We explicitly model entities, their interactions and the context in which they appear, and learn rich representations for them. We experiment with different task-adaptive pre-training objectives, in-domain training, and symbolic inference to capture dependencies between different decisions in the output space. We evaluate our model on two narrative understanding tasks: predicting character mental states, and desire fulfillment, and conduct a qualitative analysis",
    "checked": true,
    "id": "1dc4f4545011918ee11a9bb87970f66581bed368",
    "semantic_title": "modeling human mental states with an entity-based narrative graph",
    "citation_count": 4,
    "authors": [
      "I-Ta Lee",
      "Maria Leonor Pacheco",
      "Dan Goldwasser"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.392": {
    "title": "A Simple and Efficient Multi-Task Learning Approach for Conditioned Dialogue Generation",
    "volume": "main",
    "abstract": "Conditioned dialogue generation suffers from the scarcity of labeled responses. In this work, we exploit labeled non-dialogue text data related to the condition, which are much easier to collect. We propose a multi-task learning approach to leverage both labeled dialogue and text data. The 3 tasks jointly optimize the same pre-trained Transformer – conditioned dialogue generation task on the labeled dialogue data, conditioned language encoding task and conditioned language generation task on the labeled text data. Experimental results show that our approach outperforms the state-of-the-art models by leveraging the labeled texts, and it also obtains larger improvement in performance comparing to the previous methods to leverage text data",
    "checked": true,
    "id": "87102d1054611d8cf8bc4e743516ef3a613766ac",
    "semantic_title": "a simple and efficient multi-task learning approach for conditioned dialogue generation",
    "citation_count": 13,
    "authors": [
      "Yan Zeng",
      "Jian-Yun Nie"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.393": {
    "title": "Hurdles to Progress in Long-form Question Answering",
    "volume": "main",
    "abstract": "The task of long-form question answering (LFQA) involves retrieving documents relevant to a given question and using them to generate a paragraph-length answer. While many models have recently been proposed for LFQA, we show in this paper that the task formulation raises fundamental challenges regarding evaluation and dataset creation that currently preclude meaningful modeling progress. To demonstrate these challenges, we first design a new system that relies on sparse attention and contrastive retriever learning to achieve state-of-the-art performance on the ELI5 LFQA dataset. While our system tops the public leaderboard, a detailed analysis reveals several troubling trends: (1) our system's generated answers are not actually grounded in the documents that it retrieves; (2) ELI5 contains significant train / validation overlap, as at least 81% of ELI5 validation questions occur in paraphrased form in the training set; (3) ROUGE-L is not an informative metric of generated answer quality and can be easily gamed; and (4) human evaluations used for other text generation tasks are unreliable for LFQA. We offer suggestions to mitigate each of these issues, which we hope will lead to more rigorous LFQA research and meaningful progress in the future",
    "checked": true,
    "id": "3122a2d7799ba585b993e432b3deb47659b3f3c1",
    "semantic_title": "hurdles to progress in long-form question answering",
    "citation_count": 153,
    "authors": [
      "Kalpesh Krishna",
      "Aurko Roy",
      "Mohit Iyyer"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.394": {
    "title": "ENTRUST: Argument Reframing with Language Models and Entailment",
    "volume": "main",
    "abstract": "Framing involves the positive or negative presentation of an argument or issue depending on the audience and goal of the speaker. Differences in lexical framing, the focus of our work, can have large effects on peoples' opinions and beliefs. To make progress towards reframing arguments for positive effects, we create a dataset and method for this task. We use a lexical resource for \"connotations\" to create a parallel corpus and propose a method for argument reframing that combines controllable text generation (positive connotation) with a post-decoding entailment component (same denotation). Our results show that our method is effective compared to strong baselines along the dimensions of fluency, meaning, and trustworthiness/reduction of fear",
    "checked": true,
    "id": "0e345e074641abcd032a3bf3d1b3e8d42030d281",
    "semantic_title": "entrust: argument reframing with language models and entailment",
    "citation_count": 12,
    "authors": [
      "Tuhin Chakrabarty",
      "Christopher Hidey",
      "Smaranda Muresan"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.395": {
    "title": "Paragraph-level Simplification of Medical Texts",
    "volume": "main",
    "abstract": "We consider the problem of learning to simplify medical texts. This is important because most reliable, up-to-date information in biomedicine is dense with jargon and thus practically inaccessible to the lay audience. Furthermore, manual simplification does not scale to the rapidly growing body of biomedical literature, motivating the need for automated approaches. Unfortunately, there are no large-scale resources available for this task. In this work we introduce a new corpus of parallel texts in English comprising technical and lay summaries of all published evidence pertaining to different clinical topics. We then propose a new metric based on likelihood scores from a masked language model pretrained on scientific texts. We show that this automated measure better differentiates between technical and lay summaries than existing heuristics. We introduce and evaluate baseline encoder-decoder Transformer models for simplification and propose a novel augmentation to these in which we explicitly penalize the decoder for producing \"jargon\" terms; we find that this yields improvements over baselines in terms of readability",
    "checked": true,
    "id": "88d3229660f8f67e0bfd9689b0cf9ef08ad92106",
    "semantic_title": "paragraph-level simplification of medical texts",
    "citation_count": 70,
    "authors": [
      "Ashwin Devaraj",
      "Iain Marshall",
      "Byron Wallace",
      "Junyi Jessy Li"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.396": {
    "title": "An Empirical Study on Neural Keyphrase Generation",
    "volume": "main",
    "abstract": "Recent years have seen a flourishing of neural keyphrase generation (KPG) works, including the release of several large-scale datasets and a host of new models to tackle them. Model performance on KPG tasks has increased significantly with evolving deep learning research. However, there lacks a comprehensive comparison among different model designs, and a thorough investigation on related factors that may affect a KPG system's generalization performance. In this empirical study, we aim to fill this gap by providing extensive experimental results and analyzing the most crucial factors impacting the generalizability of KPG models. We hope this study can help clarify some of the uncertainties surrounding the KPG task and facilitate future research on this topic",
    "checked": true,
    "id": "166d2087d68b353489447af3db128e9565012bcb",
    "semantic_title": "an empirical study on neural keyphrase generation",
    "citation_count": 37,
    "authors": [
      "Rui Meng",
      "Xingdi Yuan",
      "Tong Wang",
      "Sanqiang Zhao",
      "Adam Trischler",
      "Daqing He"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.397": {
    "title": "Attention Head Masking for Inference Time Content Selection in Abstractive Summarization",
    "volume": "main",
    "abstract": "How can we effectively inform content selection in Transformer-based abstractive summarization models? In this work, we present a simple-yet-effective attention head masking technique, which is applied on encoder-decoder attentions to pinpoint salient content at inference time. Using attention head masking, we are able to reveal the relation between encoder-decoder attentions and content selection behaviors of summarization models. We then demonstrate its effectiveness on three document summarization datasets based on both in-domain and cross-domain settings. Importantly, our models outperform prior state-of-the-art models on CNN/Daily Mail and New York Times datasets. Moreover, our inference-time masking technique is also data-efficient, requiring only 20% of the training samples to outperform BART fine-tuned on the full CNN/DailyMail dataset",
    "checked": true,
    "id": "1a2b389f6661827d744fbf2a3bba0aa344b33daf",
    "semantic_title": "attention head masking for inference time content selection in abstractive summarization",
    "citation_count": 10,
    "authors": [
      "Shuyang Cao",
      "Lu Wang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.398": {
    "title": "Factual Probing Is [MASK]: Learning vs. Learning to Recall",
    "volume": "main",
    "abstract": "Petroni et al. (2019) demonstrated that it is possible to retrieve world facts from a pre-trained language model by expressing them as cloze-style prompts and interpret the model's prediction accuracy as a lower bound on the amount of factual information it encodes. Subsequent work has attempted to tighten the estimate by searching for better prompts, using a disjoint set of facts as training data. In this work, we make two complementary contributions to better understand these factual probing techniques. First, we propose OptiPrompt, a novel and efficient method which directly optimizes in continuous embedding space. We find this simple method is able to predict an additional 6.4% of facts in the LAMA benchmark. Second, we raise a more important question: Can we really interpret these probing results as a lower bound? Is it possible that these prompt-search methods learn from the training data too? We find, somewhat surprisingly, that the training data used by these methods contains certain regularities of the underlying fact distribution, and all the existing prompt methods, including ours, are able to exploit them for better fact prediction. We conduct a set of control experiments to disentangle \"learning\" from \"learning to recall\", providing a more detailed picture of what different prompts can reveal about pre-trained language models",
    "checked": true,
    "id": "f2885c6a25756cf81aa23b41bc62696a5be5c94d",
    "semantic_title": "factual probing is [mask]: learning vs. learning to recall",
    "citation_count": 352,
    "authors": [
      "Zexuan Zhong",
      "Dan Friedman",
      "Danqi Chen"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.399": {
    "title": "Evaluating Saliency Methods for Neural Language Models",
    "volume": "main",
    "abstract": "Saliency methods are widely used to interpret neural network predictions, but different variants of saliency methods often disagree even on the interpretations of the same prediction made by the same model. In these cases, how do we identify when are these interpretations trustworthy enough to be used in analyses? To address this question, we conduct a comprehensive and quantitative evaluation of saliency methods on a fundamental category of NLP models: neural language models. We evaluate the quality of prediction interpretations from two perspectives that each represents a desirable property of these interpretations: plausibility and faithfulness. Our evaluation is conducted on four different datasets constructed from the existing human annotation of syntactic and semantic agreements, on both sentence-level and document-level. Through our evaluation, we identified various ways saliency methods could yield interpretations of low quality. We recommend that future work deploying such methods to neural language models should carefully validate their interpretations before drawing insights",
    "checked": true,
    "id": "61c4f280e24d80d09e4f76ed2e31672b970bfbd1",
    "semantic_title": "evaluating saliency methods for neural language models",
    "citation_count": 47,
    "authors": [
      "Shuoyang Ding",
      "Philipp Koehn"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.400": {
    "title": "Contextualized Perturbation for Textual Adversarial Attack",
    "volume": "main",
    "abstract": "Adversarial examples expose the vulnerabilities of natural language processing (NLP) models, and can be used to evaluate and improve their robustness. Existing techniques of generating such examples are typically driven by local heuristic rules that are agnostic to the context, often resulting in unnatural and ungrammatical outputs. This paper presents CLARE, a ContextuaLized AdversaRial Example generation model that produces fluent and grammatical outputs through a mask-then-infill procedure. CLARE builds on a pre-trained masked language model and modifies the inputs in a context-aware manner. We propose three contextualized perturbations, Replace, Insert and Merge, that allow for generating outputs of varied lengths. CLARE can flexibly combine these perturbations and apply them at any position in the inputs, and is thus able to attack the victim model more effectively with fewer edits. Extensive experiments and human evaluation demonstrate that CLARE outperforms the baselines in terms of attack success rate, textual similarity, fluency and grammaticality",
    "checked": true,
    "id": "472cd41fa2ba2e520706f232cae12db4a7b5e60a",
    "semantic_title": "contextualized perturbation for textual adversarial attack",
    "citation_count": 211,
    "authors": [
      "Dianqi Li",
      "Yizhe Zhang",
      "Hao Peng",
      "Liqun Chen",
      "Chris Brockett",
      "Ming-Ting Sun",
      "Bill Dolan"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.401": {
    "title": "DirectProbe: Studying Representations without Classifiers",
    "volume": "main",
    "abstract": "Understanding how linguistic structure is encoded in contextualized embedding could help explain their impressive performance across NLP. Existing approaches for probing them usually call for training classifiers and use the accuracy, mutual information, or complexity as a proxy for the representation's goodness. In this work, we argue that doing so can be unreliable because different representations may need different classifiers. We develop a heuristic, DirectProbe, that directly studies the geometry of a representation by building upon the notion of a version space for a task. Experiments with several linguistic tasks and contextualized embeddings show that, even without training classifiers, DirectProbe can shine lights on how an embedding space represents labels and also anticipate the classifier performance for the representation",
    "checked": true,
    "id": "00c209ea764f709a5ce7d7fdc16c2352551bfa83",
    "semantic_title": "directprobe: studying representations without classifiers",
    "citation_count": 26,
    "authors": [
      "Yichu Zhou",
      "Vivek Srikumar"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.402": {
    "title": "Evaluating the Values of Sources in Transfer Learning",
    "volume": "main",
    "abstract": "Transfer learning that adapts a model trained on data-rich sources to low-resource targets has been widely applied in natural language processing (NLP). However, when training a transfer model over multiple sources, not every source is equally useful for the target. To better transfer a model, it is essential to understand the values of the sources. In this paper, we develop , an efficient source valuation framework for quantifying the usefulness of the sources (e.g., ) in transfer learning based on the Shapley value method. Experiments and comprehensive analyses on both cross-domain and cross-lingual transfers demonstrate that our framework is not only effective in choosing useful transfer sources but also the source values match the intuitive source-target similarity",
    "checked": true,
    "id": "142d60c73140c82ce382525fcac6358999d71627",
    "semantic_title": "evaluating the values of sources in transfer learning",
    "citation_count": 15,
    "authors": [
      "Md Rizwan Parvez",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.403": {
    "title": "Too Much in Common: Shifting of Embeddings in Transformer Language Models and its Implications",
    "volume": "main",
    "abstract": "The success of language models based on the Transformer architecture appears to be inconsistent with observed anisotropic properties of representations learned by such models. We resolve this by showing, contrary to previous studies, that the representations do not occupy a narrow cone, but rather drift in common directions. At any training step, all of the embeddings except for the ground-truth target embedding are updated with gradient in the same direction. Compounded over the training set, the embeddings drift and share common components, manifested in their shape in all the models we have empirically tested. Our experiments show that isotropy can be restored using a simple transformation",
    "checked": true,
    "id": "9a432edd418361d9ca2fbab263633012906b8142",
    "semantic_title": "too much in common: shifting of embeddings in transformer language models and its implications",
    "citation_count": 27,
    "authors": [
      "Daniel Biś",
      "Maksim Podkorytov",
      "Xiuwen Liu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.404": {
    "title": "On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic Dependencies",
    "volume": "main",
    "abstract": "We study how masking and predicting tokens in an unsupervised fashion can give rise to linguistic structures and downstream performance gains. Recent theories have suggested that pretrained language models acquire useful inductive biases through masks that implicitly act as cloze reductions for downstream tasks. While appealing, we show that the success of the random masking strategy used in practice cannot be explained by such cloze-like masks alone. We construct cloze-like masks using task-specific lexicons for three different classification datasets and show that the majority of pretrained performance gains come from generic masks that are not associated with the lexicon. To explain the empirical success of these generic masks, we demonstrate a correspondence between the Masked Language Model (MLM) objective and existing methods for learning statistical dependencies in graphical models. Using this, we derive a method for extracting these learned statistical dependencies in MLMs and show that these dependencies encode useful inductive biases in the form of syntactic structures. In an unsupervised parsing evaluation, simply forming a minimum spanning tree on the implied statistical dependence structure outperforms a classic method for unsupervised parsing (58.74 vs. 55.91 UUAS)",
    "checked": true,
    "id": "5f11dfa7cd146b456dceed19923f5ed6214fcc44",
    "semantic_title": "on the inductive bias of masked language modeling: from statistical to syntactic dependencies",
    "citation_count": 25,
    "authors": [
      "Tianyi Zhang",
      "Tatsunori B. Hashimoto"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.405": {
    "title": "Limitations of Autoregressive Models and Their Alternatives",
    "volume": "main",
    "abstract": "Standard autoregressive language models perform only polynomial-time computation to compute the probability of the next symbol. While this is attractive, it means they cannot model distributions whose next-symbol probability is hard to compute. Indeed, they cannot even model them well enough to solve associated easy decision problems for which an engineer might want to consult a language model. These limitations apply no matter how much computation and data are used to train the model, unless the model is given access to oracle parameters that grow superpolynomially in sequence length. Thus, simply training larger autoregressive language models is not a panacea for NLP. Alternatives include energy-based models (which give up efficient sampling) and latent-variable autoregressive models (which give up efficient scoring of a given string). Both are powerful enough to escape the above limitations",
    "checked": true,
    "id": "9e6763597414d865237fdd065eed50bbc5ff14f5",
    "semantic_title": "limitations of autoregressive models and their alternatives",
    "citation_count": 35,
    "authors": [
      "Chu-Cheng Lin",
      "Aaron Jaech",
      "Xin Li",
      "Matthew R. Gormley",
      "Jason Eisner"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.406": {
    "title": "On the Transformer Growth for Progressive BERT Training",
    "volume": "main",
    "abstract": "As the excessive pre-training cost arouses the need to improve efficiency, considerable efforts have been made to train BERT progressively–start from an inferior but low-cost model and gradually increase the computational complexity. Our objective is to help advance the understanding of such Transformer growth and discover principles that guide progressive training. First, we find that similar to network architecture selection, Transformer growth also favors compound scaling. Specifically, while existing methods only conduct network growth in a single dimension, we observe that it is beneficial to use compound growth operators and balance multiple dimensions (e.g., depth, width, and input length of the model). Moreover, we explore alternative growth operators in each dimension via controlled comparison to give practical guidance for operator selection. In light of our analyses, the proposed method CompoundGrow speeds up BERT pre-training by 73.6% and 82.2% for the base and large models respectively while achieving comparable performances",
    "checked": true,
    "id": "a5d6b9ed787b558e20d61bd8f5816317ef1b9a39",
    "semantic_title": "on the transformer growth for progressive bert training",
    "citation_count": 43,
    "authors": [
      "Xiaotao Gu",
      "Liyuan Liu",
      "Hongkun Yu",
      "Jing Li",
      "Chen Chen",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.407": {
    "title": "Revisiting Simple Neural Probabilistic Language Models",
    "volume": "main",
    "abstract": "Recent progress in language modeling has been driven not only by advances in neural architectures, but also through hardware and optimization improvements. In this paper, we revisit the neural probabilistic language model (NPLM) of Bengio et al. (2003), which simply concatenates word embeddings within a fixed window and passes the result through a feed-forward network to predict the next word. When scaled up to modern hardware, this model (despite its many limitations) performs much better than expected on word-level language model benchmarks. Our analysis reveals that the NPLM achieves lower perplexity than a baseline Transformer with short input contexts but struggles to handle long-term dependencies. Inspired by this result, we modify the Transformer by replacing its first self-attention layer with the NPLM's local concatenation layer, which results in small but consistent perplexity decreases across three word-level language modeling datasets",
    "checked": true,
    "id": "981dbdf6f87f13f3f3047a925c519fc39a35202b",
    "semantic_title": "revisiting simple neural probabilistic language models",
    "citation_count": 9,
    "authors": [
      "Simeng Sun",
      "Mohit Iyyer"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.408": {
    "title": "ReadTwice: Reading Very Large Documents with Memories",
    "volume": "main",
    "abstract": "Knowledge-intensive tasks such as question answering often require assimilating information from different sections of large inputs such as books or article collections. We propose ReadTwice, a simple and effective technique that combines several strengths of prior approaches to model long-range dependencies with Transformers. The main idea is to read text in small segments, in parallel, summarizing each segment into a memory table to be used in a second read of the text. We show that the method outperforms models of comparable size on several question answering (QA) datasets and sets a new state of the art on the challenging NarrativeQA task, with questions about entire books",
    "checked": true,
    "id": "6a3e13d7926a4aaa0ddcd3acc7c08e8d24c330e5",
    "semantic_title": "readtwice: reading very large documents with memories",
    "citation_count": 15,
    "authors": [
      "Yury Zemlyanskiy",
      "Joshua Ainslie",
      "Michiel de Jong",
      "Philip Pham",
      "Ilya Eckstein",
      "Fei Sha"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.409": {
    "title": "SCRIPT: Self-Critic PreTraining of Transformers",
    "volume": "main",
    "abstract": "We introduce Self-CRItic Pretraining Transformers (SCRIPT) for representation learning of text. The popular masked language modeling (MLM) pretraining methods like BERT replace some tokens with [MASK] and an encoder is trained to recover them, while ELECTRA trains a discriminator to detect replaced tokens proposed by a generator. In contrast, we train a language model as in MLM and further derive a discriminator or critic on top of the encoder without using any additional parameters. That is, the model itself is a critic. SCRIPT combines MLM training and discriminative training for learning rich representations and compute- and sample-efficiency. We demonstrate improved sample-efficiency in pretraining and enhanced representations evidenced by improved downstream task performance on GLUE and SQuAD over strong baselines. Also, the self-critic scores can be directly used as pseudo-log-likelihood for efficient scoring",
    "checked": true,
    "id": "ea330d8b504ce194c8c67cb8884a7a22d92b5a87",
    "semantic_title": "script: self-critic pretraining of transformers",
    "citation_count": 4,
    "authors": [
      "Erik Nijkamp",
      "Bo Pang",
      "Ying Nian Wu",
      "Caiming Xiong"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.410": {
    "title": "Learning How to Ask: Querying LMs with Mixtures of Soft Prompts",
    "volume": "main",
    "abstract": "Natural-language prompts have recently been used to coax pretrained language models into performing other AI tasks, using a fill-in-the-blank paradigm (Petroni et al., 2019) or a few-shot extrapolation paradigm (Brown et al., 2020). For example, language models retain factual knowledge from their training corpora that can be extracted by asking them to \"fill in the blank\" in a sentential prompt. However, where does this prompt come from? We explore the idea of learning prompts by gradient descent—either fine-tuning prompts taken from previous work, or starting from random initialization. Our prompts consist of \"soft words,\" i.e., continuous vectors that are not necessarily word type embeddings from the language model. Furthermore, for each task, we optimize a mixture of prompts, learning which prompts are most effective and how to ensemble them. Across multiple English LMs and tasks, our approach hugely outperforms previous methods, showing that the implicit factual knowledge in language models was previously underestimated. Moreover, this knowledge is cheap to elicit: random initialization is nearly as good as informed initialization",
    "checked": true,
    "id": "209f9bde2dee7cf1677801586562ffe56d435d38",
    "semantic_title": "learning how to ask: querying lms with mixtures of soft prompts",
    "citation_count": 457,
    "authors": [
      "Guanghui Qin",
      "Jason Eisner"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.411": {
    "title": "Nutri-bullets Hybrid: Consensual Multi-document Summarization",
    "volume": "main",
    "abstract": "We present a method for generating comparative summaries that highlight similarities and contradictions in input documents. The key challenge in creating such summaries is the lack of large parallel training data required for training typical summarization systems. To this end, we introduce a hybrid generation approach inspired by traditional concept-to-text systems. To enable accurate comparison between different sources, the model first learns to extract pertinent relations from input documents. The content planning component uses deterministic operators to aggregate these relations after identifying a subset for inclusion into a summary. The surface realization component lexicalizes this information using a text-infilling language model. By separately modeling content selection and realization, we can effectively train them with limited annotations. We implemented and tested the model in the domain of nutrition and health – rife with inconsistencies. Compared to conventional methods, our framework leads to more faithful, relevant and aggregation-sensitive summarization – while being equally fluent",
    "checked": true,
    "id": "4a470df590d90494696fce37aa1eed2d2659bc0a",
    "semantic_title": "nutri-bullets hybrid: consensual multi-document summarization",
    "citation_count": 7,
    "authors": [
      "Darsh Shah",
      "Lili Yu",
      "Tao Lei",
      "Regina Barzilay"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.412": {
    "title": "AVA: an Automatic eValuation Approach for Question Answering Systems",
    "volume": "main",
    "abstract": "We introduce AVA, an automatic evaluation approach for Question Answering, which given a set of questions associated with Gold Standard answers (references), can estimate system Accuracy. AVA uses Transformer-based language models to encode question, answer, and reference texts. This allows for effectively assessing answer correctness using similarity between the reference and an automatic answer, biased towards the question semantics. To design, train, and test AVA, we built multiple large training, development, and test sets on public and industrial benchmarks. Our innovative solutions achieve up to 74.7% F1 score in predicting human judgment for single answers. Additionally, AVA can be used to evaluate the overall system Accuracy with an error lower than 7% at 95% of confidence when measured on several QA systems",
    "checked": true,
    "id": "e12efe9fffbaec3c52cb8546b90c9b6e5fb51d6a",
    "semantic_title": "ava: an automatic evaluation approach to question answering systems",
    "citation_count": 10,
    "authors": [
      "Thuy Vu",
      "Alessandro Moschitti"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.413": {
    "title": "SpanPredict: Extraction of Predictive Document Spans with Neural Attention",
    "volume": "main",
    "abstract": "In many natural language processing applications, identifying predictive text can be as important as the predictions themselves. When predicting medical diagnoses, for example, identifying predictive content in clinical notes not only enhances interpretability, but also allows unknown, descriptive (i.e., text-based) risk factors to be identified. We here formalize this problem as predictive extraction and address it using a simple mechanism based on linear attention. Our method preserves differentiability, allowing scalable inference via stochastic gradient descent. Further, the model decomposes predictions into a sum of contributions of distinct text spans. Importantly, we require only document labels, not ground-truth spans. Results show that our model identifies semantically-cohesive spans and assigns them scores that agree with human ratings, while preserving classification performance",
    "checked": true,
    "id": "a1c6fa477686cf6db3811efe6bc60cbb3d945189",
    "semantic_title": "spanpredict: extraction of predictive document spans with neural attention",
    "citation_count": 5,
    "authors": [
      "Vivek Subramanian",
      "Matthew Engelhard",
      "Sam Berchuck",
      "Liqun Chen",
      "Ricardo Henao",
      "Lawrence Carin"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.414": {
    "title": "Text Editing by Command",
    "volume": "main",
    "abstract": "A prevailing paradigm in neural text generation is one-shot generation, where text is produced in a single step. The one-shot setting is inadequate, however, when the constraints the user wishes to impose on the generated text are dynamic, especially when authoring longer documents. We address this limitation with an interactive text generation setting in which the user interacts with the system by issuing commands to edit existing text. To this end, we propose a novel text editing task, and introduce WikiDocEdits, a dataset of single-sentence edits crawled from Wikipedia. We show that our Interactive Editor, a transformer-based model trained on this dataset, outperforms baselines and obtains positive results in both automatic and human evaluations. We present empirical and qualitative analyses of this model's performance",
    "checked": true,
    "id": "483336e50c566c5505012d8777b97800a6386113",
    "semantic_title": "text editing by command",
    "citation_count": 33,
    "authors": [
      "Felix Faltings",
      "Michel Galley",
      "Gerold Hintz",
      "Chris Brockett",
      "Chris Quirk",
      "Jianfeng Gao",
      "Bill Dolan"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.415": {
    "title": "A Deep Metric Learning Approach to Account Linking",
    "volume": "main",
    "abstract": "We consider the task of linking social media accounts that belong to the same author in an automated fashion on the basis of the content and meta-data of the corresponding document streams. We focus on learning an embedding that maps variable-sized samples of user activity–ranging from single posts to entire months of activity–to a vector space, where samples by the same author map to nearby points. Our approach does not require human-annotated data for training purposes, which allows us to leverage large amounts of social media content. The proposed model outperforms several competitive baselines under a novel evaluation framework modeled after established recognition benchmarks in other domains. Our method achieves high linking accuracy, even with small samples from accounts not seen at training time, a prerequisite for practical applications of the proposed linking framework",
    "checked": true,
    "id": "0a53be8d3af7b3e774e7a3b1208f2c9351758f8a",
    "semantic_title": "a deep metric learning approach to account linking",
    "citation_count": 14,
    "authors": [
      "Aleem Khan",
      "Elizabeth Fleming",
      "Noah Schofield",
      "Marcus Bishop",
      "Nicholas Andrews"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.416": {
    "title": "Improving Factual Completeness and Consistency of Image-to-Text Radiology Report Generation",
    "volume": "main",
    "abstract": "Neural image-to-text radiology report generation systems offer the potential to improve radiology reporting by reducing the repetitive process of report drafting and identifying possible medical errors. However, existing report generation systems, despite achieving high performances on natural language generation metrics such as CIDEr or BLEU, still suffer from incomplete and inconsistent generations. Here we introduce two new simple rewards to encourage the generation of factually complete and consistent radiology reports: one that encourages the system to generate radiology domain entities consistent with the reference, and one that uses natural language inference to encourage these entities to be described in inferentially consistent ways. We combine these with the novel use of an existing semantic equivalence metric (BERTScore). We further propose a report generation system that optimizes these rewards via reinforcement learning. On two open radiology report datasets, our system substantially improved the F1 score of a clinical information extraction performance by +22.1 (Delta +63.9%). We further show via a human evaluation and a qualitative analysis that our system leads to generations that are more factually complete and consistent compared to the baselines",
    "checked": true,
    "id": "9e90c17ef40404b79ad0f12d9b9c94656f12dfcd",
    "semantic_title": "improving factual completeness and consistency of image-to-text radiology report generation",
    "citation_count": 123,
    "authors": [
      "Yasuhide Miura",
      "Yuhao Zhang",
      "Emily Tsai",
      "Curtis Langlotz",
      "Dan Jurafsky"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.417": {
    "title": "Multimodal End-to-End Sparse Model for Emotion Recognition",
    "volume": "main",
    "abstract": "Existing works in multimodal affective computing tasks, such as emotion recognition and personality recognition, generally adopt a two-phase pipeline by first extracting feature representations for each single modality with hand crafted algorithms, and then performing end-to-end learning with extracted features. However, the extracted features are fixed and cannot be further fine-tuned on different target tasks, and manually finding feature extracting algorithms does not generalize or scale well to different tasks, which can lead to sub-optimal performance. In this paper, we develop a fully end-to-end model that connects the two phases and optimizes them jointly. In addition, we restructure the current datasets to enable the fully end-to-end training. Furthermore, to reduce the computational overhead brought by the end-to-end model, we introduce a sparse cross-modal attention mechanism for the feature extraction. Experimental results show that our fully end-to-end model significantly surpasses the current state-of-the-art models based on the two-phase pipeline. Moreover, by adding the sparse cross-modal attention, our model can maintain the performance with around half less computation in the feature extraction part of the model",
    "checked": true,
    "id": "4d59c17aa86bbbfafd07fe04f86b1b4ec9c942e3",
    "semantic_title": "multimodal end-to-end sparse model for emotion recognition",
    "citation_count": 61,
    "authors": [
      "Wenliang Dai",
      "Samuel Cahyawijaya",
      "Zihan Liu",
      "Pascale Fung"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.418": {
    "title": "MIMOQA: Multimodal Input Multimodal Output Question Answering",
    "volume": "main",
    "abstract": "Multimodal research has picked up significantly in the space of question answering with the task being extended to visual question answering, charts question answering as well as multimodal input question answering. However, all these explorations produce a unimodal textual output as the answer. In this paper, we propose a novel task - MIMOQA - Multimodal Input Multimodal Output Question Answering in which the output is also multimodal. Through human experiments, we empirically show that such multimodal outputs provide better cognitive understanding of the answers. We also propose a novel multimodal question-answering framework, MExBERT, that incorporates a joint textual and visual attention towards producing such a multimodal output. Our method relies on a novel multimodal dataset curated for this problem from publicly available unimodal datasets. We show the superior performance of MExBERT against strong baselines on both the automatic as well as human metrics",
    "checked": true,
    "id": "83e81a4a4f9329d727cf519491da3d30ec8f8d4b",
    "semantic_title": "mimoqa: multimodal input multimodal output question answering",
    "citation_count": 33,
    "authors": [
      "Hrituraj Singh",
      "Anshul Nasery",
      "Denil Mehta",
      "Aishwarya Agarwal",
      "Jatin Lamba",
      "Balaji Vasan Srinivasan"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.419": {
    "title": "OCID-Ref: A 3D Robotic Dataset With Embodied Language For Clutter Scene Grounding",
    "volume": "main",
    "abstract": "To effectively apply robots in working environments and assist humans, it is essential to develop and evaluate how visual grounding (VG) can affect machine performance on occluded objects. However, current VG works are limited in working environments, such as offices and warehouses, where objects are usually occluded due to space utilization issues. In our work, we propose a novel OCID-Ref dataset featuring a referring expression segmentation task with referring expressions of occluded objects. OCID-Ref consists of 305,694 referring expressions from 2,300 scenes with providing RGB image and point cloud inputs. To resolve challenging occlusion issues, we argue that it's crucial to take advantage of both 2D and 3D signals to resolve challenging occlusion issues. Our experimental results demonstrate the effectiveness of aggregating 2D and 3D signals but referring to occluded objects still remains challenging for the modern visual grounding systems. OCID-Ref is publicly available at https://github.com/lluma/OCID-Ref",
    "checked": true,
    "id": "4ae70ed20691908996c6b16bb74f47923c938780",
    "semantic_title": "ocid-ref: a 3d robotic dataset with embodied language for clutter scene grounding",
    "citation_count": 14,
    "authors": [
      "Ke-Jyun Wang",
      "Yun-Hsuan Liu",
      "Hung-Ting Su",
      "Jen-Wei Wang",
      "Yu-Siang Wang",
      "Winston Hsu",
      "Wen-Chin Chen"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.420": {
    "title": "Unsupervised Vision-and-Language Pre-training Without Parallel Images and Captions",
    "volume": "main",
    "abstract": "Pre-trained contextual vision-and-language (V&L) models have achieved impressive performance on various benchmarks. However, existing models require a large amount of parallel image-caption data for pre-training. Such data are costly to collect and require cumbersome curation. Inspired by unsupervised machine translation, we investigate if a strong V&L representation model can be learned through unsupervised pre-training without image-caption corpora. In particular, we propose to conduct \"mask-and-predict\" pre-training on text-only and image-only corpora and introduce the object tags detected by an object recognition model as anchor points to bridge two modalities. We find that such a simple approach achieves performance close to a model pre-trained with aligned data, on four English V&L benchmarks. Our work challenges the widely held notion that aligned data is necessary for V&L pre-training, while significantly reducing the amount of supervision needed for V&L models",
    "checked": true,
    "id": "fc123daef8ab12025c2e5e6162a0c94370faa4c5",
    "semantic_title": "unsupervised vision-and-language pre-training without parallel images and captions",
    "citation_count": 45,
    "authors": [
      "Liunian Harold Li",
      "Haoxuan You",
      "Zhecan Wang",
      "Alireza Zareian",
      "Shih-Fu Chang",
      "Kai-Wei Chang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.421": {
    "title": "Multitasking Inhibits Semantic Drift",
    "volume": "main",
    "abstract": "When intelligent agents communicate to accomplish shared goals, how do these goals shape the agents' language? We study the dynamics of learning in latent language policies (LLPs), in which instructor agents generate natural-language subgoal descriptions and executor agents map these descriptions to low-level actions. LLPs can solve challenging long-horizon reinforcement learning problems and provide a rich model for studying task-oriented language use. But previous work has found that LLP training is prone to semantic drift (use of messages in ways inconsistent with their original natural language meanings). Here, we demonstrate theoretically and empirically that multitask training is an effective counter to this problem: we prove that multitask training eliminates semantic drift in a well-studied family of signaling games, and show that multitask training of neural LLPs in a complex strategy game reduces drift and while improving sample efficiency",
    "checked": true,
    "id": "66d5863d31b165a493c93c88b257ea06485f7f12",
    "semantic_title": "multitasking inhibits semantic drift",
    "citation_count": 11,
    "authors": [
      "Athul Paul Jacob",
      "Mike Lewis",
      "Jacob Andreas"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.422": {
    "title": "Probing Contextual Language Models for Common Ground with Visual Representations",
    "volume": "main",
    "abstract": "The success of large-scale contextual language models has attracted great interest in probing what is encoded in their representations. In this work, we consider a new question: to what extent contextual representations of concrete nouns are aligned with corresponding visual representations? We design a probing model that evaluates how effective are text-only representations in distinguishing between matching and non-matching visual representations. Our findings show that language representations alone provide a strong signal for retrieving image patches from the correct object categories. Moreover, they are effective in retrieving specific instances of image patches; textual context plays an important role in this process. Visually grounded language models slightly outperform text-only language models in instance retrieval, but greatly under-perform humans. We hope our analyses inspire future research in understanding and improving the visual capabilities of language models",
    "checked": true,
    "id": "567469dc08fbb702df9f525637e9a3fc43bb1fbb",
    "semantic_title": "probing contextual language models for common ground with visual representations",
    "citation_count": 22,
    "authors": [
      "Gabriel Ilharco",
      "Rowan Zellers",
      "Ali Farhadi",
      "Hannaneh Hajishirzi"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.423": {
    "title": "BBAEG: Towards BERT-based Biomedical Adversarial Example Generation for Text Classification",
    "volume": "main",
    "abstract": "Healthcare predictive analytics aids medical decision-making, diagnosis prediction and drug review analysis. Therefore, prediction accuracy is an important criteria which also necessitates robust predictive language models. However, the models using deep learning have been proven vulnerable towards insignificantly perturbed input instances which are less likely to be misclassified by humans. Recent efforts of generating adversaries using rule-based synonyms and BERT-MLMs have been witnessed in general domain, but the ever-increasing biomedical literature poses unique challenges. We propose BBAEG (Biomedical BERT-based Adversarial Example Generation), a black-box attack algorithm for biomedical text classification, leveraging the strengths of both domain-specific synonym replacement for biomedical named entities and BERT-MLM predictions, spelling variation and number replacement. Through automatic and human evaluation on two datasets, we demonstrate that BBAEG performs stronger attack with better language fluency, semantic coherence as compared to prior work",
    "checked": true,
    "id": "7cbd2ba7b66d6dbb68ca7ae665e400fae08f3d62",
    "semantic_title": "bbaeg: towards bert-based biomedical adversarial example generation for text classification",
    "citation_count": 10,
    "authors": [
      "Ishani Mondal"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.424": {
    "title": "Targeted Adversarial Training for Natural Language Understanding",
    "volume": "main",
    "abstract": "We present a simple yet effective Targeted Adversarial Training (TAT) algorithm to improve adversarial training for natural language understanding. The key idea is to introspect current mistakes and prioritize adversarial training steps to where the model errs the most. Experiments show that TAT can significantly improve accuracy over standard adversarial training on GLUE and attain new state-of-the-art zero-shot results on XNLI. Our code will be released upon acceptance of the paper",
    "checked": true,
    "id": "a9e8c7544b41ff133b04a321179c22f7da3c3cdc",
    "semantic_title": "targeted adversarial training for natural language understanding",
    "citation_count": 12,
    "authors": [
      "Lis Pereira",
      "Xiaodong Liu",
      "Hao Cheng",
      "Hoifung Poon",
      "Jianfeng Gao",
      "Ichiro Kobayashi"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.425": {
    "title": "Latent-Optimized Adversarial Neural Transfer for Sarcasm Detection",
    "volume": "main",
    "abstract": "The existence of multiple datasets for sarcasm detection prompts us to apply transfer learning to exploit their commonality. The adversarial neural transfer (ANT) framework utilizes multiple loss terms that encourage the source-domain and the target-domain feature distributions to be similar while optimizing for domain-specific performance. However, these objectives may be in conflict, which can lead to optimization difficulties and sometimes diminished transfer. We propose a generalized latent optimization strategy that allows different losses to accommodate each other and improves training dynamics. The proposed method outperforms transfer learning and meta-learning baselines. In particular, we achieve 10.02% absolute performance gain over the previous state of the art on the iSarcasm dataset",
    "checked": true,
    "id": "cab2fb723903d268ab911dd1d04c6e4626e90007",
    "semantic_title": "latent-optimized adversarial neural transfer for sarcasm detection",
    "citation_count": 15,
    "authors": [
      "Xu Guo",
      "Boyang Li",
      "Han Yu",
      "Chunyan Miao"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.426": {
    "title": "Self-training Improves Pre-training for Natural Language Understanding",
    "volume": "main",
    "abstract": "Unsupervised pre-training has led to much recent progress in natural language understanding. In this paper, we study self-training as another way to leverage unlabeled data through semi-supervised learning. To obtain additional data for a specific task, we introduce SentAugment, a data augmentation method which computes task-specific query embeddings from labeled data to retrieve sentences from a bank of billions of unlabeled sentences crawled from the web. Unlike previous semi-supervised methods, our approach does not require in-domain unlabeled data and is therefore more generally applicable. Experiments show that self-training is complementary to strong RoBERTa baselines on a variety of tasks. Our augmentation approach leads to scalable and effective self-training with improvements of up to 2.6% on standard text classification benchmarks. Finally, we also show strong gains on knowledge-distillation and few-shot learning",
    "checked": true,
    "id": "a321d1ec561a23512a5aa687c0d89c971bb5687b",
    "semantic_title": "self-training improves pre-training for natural language understanding",
    "citation_count": 149,
    "authors": [
      "Jingfei Du",
      "Edouard Grave",
      "Beliz Gunel",
      "Vishrav Chaudhary",
      "Onur Celebi",
      "Michael Auli",
      "Veselin Stoyanov",
      "Alexis Conneau"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.427": {
    "title": "Supporting Clustering with Contrastive Learning",
    "volume": "main",
    "abstract": "Unsupervised clustering aims at discovering the semantic categories of data according to some distance measured in the representation space. However, different categories often overlap with each other in the representation space at the beginning of the learning process, which poses a significant challenge for distance-based clustering in achieving good separation between different categories. To this end, we propose Supporting Clustering with Contrastive Learning (SCCL) – a novel framework to leverage contrastive learning to promote better separation. We assess the performance of SCCL on short text clustering and show that SCCL significantly advances the state-of-the-art results on most benchmark datasets with 3%-11% improvement on Accuracy and 4%-15% improvement on Normalized Mutual Information. Furthermore, our quantitative analysis demonstrates the effectiveness of SCCL in leveraging the strengths of both bottom-up instance discrimination and top-down clustering to achieve better intra-cluster and inter-cluster distances when evaluated with the ground truth cluster labels",
    "checked": true,
    "id": "4d1ab60f309629241a00b71190d338ab27c40170",
    "semantic_title": "supporting clustering with contrastive learning",
    "citation_count": 158,
    "authors": [
      "Dejiao Zhang",
      "Feng Nan",
      "Xiaokai Wei",
      "Shang-Wen Li",
      "Henghui Zhu",
      "Kathleen McKeown",
      "Ramesh Nallapati",
      "Andrew O. Arnold",
      "Bing Xiang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.428": {
    "title": "TITA: A Two-stage Interaction and Topic-Aware Text Matching Model",
    "volume": "main",
    "abstract": "In this paper, we focus on the problem of keyword and document matching by considering different relevance levels. In our recommendation system, different people follow different hot keywords with interest. We need to attach documents to each keyword and then distribute the documents to people who follow these keywords. The ideal documents should have the same topic with the keyword, which we call topic-aware relevance. In other words, topic-aware relevance documents are better than partially-relevance ones in this application. However, previous tasks never define topic-aware relevance clearly. To tackle this problem, we define a three-level relevance in keyword-document matching task: topic-aware relevance, partially-relevance and irrelevance. To capture the relevance between the short keyword and the document at above-mentioned three levels, we should not only combine the latent topic of the document with its deep neural representation, but also model complex interactions between the keyword and the document. To this end, we propose a Two-stage Interaction and Topic-Aware text matching model (TITA). In terms of \"topic-aware\", we introduce neural topic model to analyze the topic of the document and then use it to further encode the document. In terms of \"two-stage interaction\", we propose two successive stages to model complex interactions between the keyword and the document. Extensive experiments reveal that TITA outperforms other well-designed baselines and shows excellent performance in our recommendation system",
    "checked": true,
    "id": "609d31ae4683959fc1c38ec037b393a3274e2d6c",
    "semantic_title": "tita: a two-stage interaction and topic-aware text matching model",
    "citation_count": 4,
    "authors": [
      "Xingwu Sun",
      "Yanling Cui",
      "Hongyin Tang",
      "Qiuyu Zhu",
      "Fuzheng Zhang",
      "Beihong Jin"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.429": {
    "title": "Neural Quality Estimation with Multiple Hypotheses for Grammatical Error Correction",
    "volume": "main",
    "abstract": "Grammatical Error Correction (GEC) aims to correct writing errors and help language learners improve their writing skills. However, existing GEC models tend to produce spurious corrections or fail to detect lots of errors. The quality estimation model is necessary to ensure learners get accurate GEC results and avoid misleading from poorly corrected sentences. Well-trained GEC models can generate several high-quality hypotheses through decoding, such as beam search, which provide valuable GEC evidence and can be used to evaluate GEC quality. However, existing models neglect the possible GEC evidence from different hypotheses. This paper presents the Neural Verification Network (VERNet) for GEC quality estimation with multiple hypotheses. VERNet establishes interactions among hypotheses with a reasoning graph and conducts two kinds of attention mechanisms to propagate GEC evidence to verify the quality of generated hypotheses. Our experiments on four GEC datasets show that VERNet achieves state-of-the-art grammatical error detection performance, achieves the best quality estimation results, and significantly improves GEC performance by reranking hypotheses. All data and source codes are available at https://github.com/thunlp/VERNet",
    "checked": true,
    "id": "209923f17bb2fccc1a92dd5aea818171846496ec",
    "semantic_title": "neural quality estimation with multiple hypotheses for grammatical error correction",
    "citation_count": 24,
    "authors": [
      "Zhenghao Liu",
      "Xiaoyuan Yi",
      "Maosong Sun",
      "Liner Yang",
      "Tat-Seng Chua"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.430": {
    "title": "Neural Network Surgery: Injecting Data Patterns into Pre-trained Models with Minimal Instance-wise Side Effects",
    "volume": "main",
    "abstract": "Side effects during neural network tuning are typically measured by overall accuracy changes. However, we find that even with similar overall accuracy, existing tuning methods result in non-negligible instance-wise side effects. Motivated by neuroscientific evidence and theoretical results, we demonstrate that side effects can be controlled by the number of changed parameters and thus, we propose to conduct neural network surgery by only modifying a limited number of parameters. Neural network surgery can be realized using diverse techniques and we investigate three lines of methods. Experimental results on representative tuning problems validate the effectiveness of the surgery approach. The dynamic selecting method achieves the best overall performance that not only satisfies the tuning goal but also induces fewer instance-wise side effects by changing only 10-5 of the parameters",
    "checked": true,
    "id": "e6987c47cbe890d9d6384bdb27fbd003bb2b8568",
    "semantic_title": "neural network surgery: injecting data patterns into pre-trained models with minimal instance-wise side effects",
    "citation_count": 13,
    "authors": [
      "Zhiyuan Zhang",
      "Xuancheng Ren",
      "Qi Su",
      "Xu Sun",
      "Bin He"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.431": {
    "title": "Discrete Argument Representation Learning for Interactive Argument Pair Identification",
    "volume": "main",
    "abstract": "In this paper, we focus on identifying interactive argument pairs from two posts with opposite stances to a certain topic. Considering opinions are exchanged from different perspectives of the discussing topic, we study the discrete representations for arguments to capture varying aspects in argumentation languages (e.g., the debate focus and the participant behavior). Moreover, we utilize hierarchical structure to model post-wise information incorporating contextual knowledge. Experimental results on the large-scale dataset collected from CMV show that our proposed framework can significantly outperform the competitive baselines. Further analyses reveal why our model yields superior performance and prove the usefulness of our learned representations",
    "checked": true,
    "id": "36e4e4099f08a6e7edd9b40224ffcdfecd804459",
    "semantic_title": "discrete argument representation learning for interactive argument pair identification",
    "citation_count": 14,
    "authors": [
      "Lu Ji",
      "Zhongyu Wei",
      "Jing Li",
      "Qi Zhang",
      "Xuanjing Huang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.432": {
    "title": "On Unifying Misinformation Detection",
    "volume": "main",
    "abstract": "In this paper, we introduce UnifiedM2, a general-purpose misinformation model that jointly models multiple domains of misinformation with a single, unified setup. The model is trained to handle four tasks: detecting news bias, clickbait, fake news, and verifying rumors. By grouping these tasks together, UnifiedM2 learns a richer representation of misinformation, which leads to state-of-the-art or comparable performance across all tasks. Furthermore, we demonstrate that UnifiedM2's learned representation is helpful for few-shot learning of unseen misinformation tasks/datasets and the model's generalizability to unseen events",
    "checked": true,
    "id": "8de8010a09edced1866a3aa82bec2e01581b76a3",
    "semantic_title": "on unifying misinformation detection",
    "citation_count": 19,
    "authors": [
      "Nayeon Lee",
      "Belinda Z. Li",
      "Sinong Wang",
      "Pascale Fung",
      "Hao Ma",
      "Wen-tau Yih",
      "Madian Khabsa"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.433": {
    "title": "Frustratingly Easy Edit-based Linguistic Steganography with a Masked Language Model",
    "volume": "main",
    "abstract": "With advances in neural language models, the focus of linguistic steganography has shifted from edit-based approaches to generation-based ones. While the latter's payload capacity is impressive, generating genuine-looking texts remains challenging. In this paper, we revisit edit-based linguistic steganography, with the idea that a masked language model offers an off-the-shelf solution. The proposed method eliminates painstaking rule construction and has a high payload capacity for an edit-based model. It is also shown to be more secure against automatic detection than a generation-based method while offering better control of the security/payload capacity trade-off",
    "checked": true,
    "id": "d75503985a0c3a0b867a1805de5db01450dc2992",
    "semantic_title": "frustratingly easy edit-based linguistic steganography with a masked language model",
    "citation_count": 32,
    "authors": [
      "Honai Ueoka",
      "Yugo Murawaki",
      "Sadao Kurohashi"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.434": {
    "title": "Few-Shot Text Classification with Triplet Networks, Data Augmentation, and Curriculum Learning",
    "volume": "main",
    "abstract": "Few-shot text classification is a fundamental NLP task in which a model aims to classify text into a large number of categories, given only a few training examples per category. This paper explores data augmentation—a technique particularly suitable for training with limited data—for this few-shot, highly-multiclass text classification setting. On four diverse text classification tasks, we find that common data augmentation techniques can improve the performance of triplet networks by up to 3.0% on average. To further boost performance, we present a simple training strategy called curriculum data augmentation, which leverages curriculum learning by first training on only original examples and then introducing augmented data as training progresses. We explore a two-stage and a gradual schedule, and find that, compared with standard single-stage training, curriculum data augmentation trains faster, improves performance, and remains robust to high amounts of noising from augmentation",
    "checked": true,
    "id": "8d26d4f850acb8cf4f17345ad1dee67ac318a3a1",
    "semantic_title": "few-shot text classification with triplet networks, data augmentation, and curriculum learning",
    "citation_count": 52,
    "authors": [
      "Jason Wei",
      "Chengyu Huang",
      "Soroush Vosoughi",
      "Yu Cheng",
      "Shiqi Xu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.435": {
    "title": "Do RNN States Encode Abstract Phonological Alternations?",
    "volume": "main",
    "abstract": "Sequence-to-sequence models have delivered impressive results in word formation tasks such as morphological inflection, often learning to model subtle morphophonological details with limited training data. Despite the performance, the opacity of neural models makes it difficult to determine whether complex generalizations are learned, or whether a kind of separate rote memorization of each morphophonological process takes place. To investigate whether complex alternations are simply memorized or whether there is some level of generalization across related sound changes in a sequence-to-sequence model, we perform several experiments on Finnish consonant gradation—a complex set of sound changes triggered in some words by certain suffixes. We find that our models often—though not always—encode 17 different consonant gradation processes in a handful of dimensions in the RNN. We also show that by scaling the activations in these dimensions we can control whether consonant gradation occurs and the direction of the gradation",
    "checked": true,
    "id": "d4c9fa8854d9d3ee8865656d67a26b51841d8ee3",
    "semantic_title": "do rnn states encode abstract phonological alternations?",
    "citation_count": 5,
    "authors": [
      "Miikka Silfverberg",
      "Francis Tyers",
      "Garrett Nicolai",
      "Mans Hulden"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.436": {
    "title": "Pre-training with Meta Learning for Chinese Word Segmentation",
    "volume": "main",
    "abstract": "Recent researches show that pre-trained models (PTMs) are beneficial to Chinese Word Segmentation (CWS). However, PTMs used in previous works usually adopt language modeling as pre-training tasks, lacking task-specific prior segmentation knowledge and ignoring the discrepancy between pre-training tasks and downstream CWS tasks. In this paper, we propose a CWS-specific pre-trained model MetaSeg, which employs a unified architecture and incorporates meta learning algorithm into a multi-criteria pre-training task. Empirical results show that MetaSeg could utilize common prior segmentation knowledge from different existing criteria and alleviate the discrepancy between pre-trained models and downstream CWS tasks. Besides, MetaSeg can achieve new state-of-the-art performance on twelve widely-used CWS datasets and significantly improve model performance in low-resource settings",
    "checked": true,
    "id": "fd7585592eb48f82585abdb44c9c30a59dedf9a1",
    "semantic_title": "pre-training with meta learning for chinese word segmentation",
    "citation_count": 16,
    "authors": [
      "Zhen Ke",
      "Liang Shi",
      "Songtao Sun",
      "Erli Meng",
      "Bin Wang",
      "Xipeng Qiu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.437": {
    "title": "Decompose, Fuse and Generate: A Formation-Informed Method for Chinese Definition Generation",
    "volume": "main",
    "abstract": "In this paper, we tackle the task of Definition Generation (DG) in Chinese, which aims at automatically generating a definition for a word. Most existing methods take the source word as an indecomposable semantic unit. However, in parataxis languages like Chinese, word meanings can be composed using the word formation process, where a word (\"桃花\", peach-blossom) is formed by formation components (\"桃\", peach; \"花\", flower) using a formation rule (Modifier-Head). Inspired by this process, we propose to enhance DG with word formation features. We build a formation-informed dataset, and propose a model DeFT, which Decomposes words into formation features, dynamically Fuses different features through a gating mechanism, and generaTes word definitions. Experimental results show that our method is both effective and robust",
    "checked": true,
    "id": "d2478ffb8368f6f82f4acd2637a61fc87931a8a3",
    "semantic_title": "decompose, fuse and generate: a formation-informed method for chinese definition generation",
    "citation_count": 9,
    "authors": [
      "Hua Zheng",
      "Damai Dai",
      "Lei Li",
      "Tianyu Liu",
      "Zhifang Sui",
      "Baobao Chang",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.438": {
    "title": "User-Generated Text Corpus for Evaluating Japanese Morphological Analysis and Lexical Normalization",
    "volume": "main",
    "abstract": "Morphological analysis (MA) and lexical normalization (LN) are both important tasks for Japanese user-generated text (UGT). To evaluate and compare different MA/LN systems, we have constructed a publicly available Japanese UGT corpus. Our corpus comprises 929 sentences annotated with morphological and normalization information, along with category information we classified for frequent UGT-specific phenomena. Experiments on the corpus demonstrated the low performance of existing MA/LN methods for non-general words and non-standard forms, indicating that the corpus would be a challenging benchmark for further research on UGT",
    "checked": true,
    "id": "5878a1c25871a20e55ce07bc00620337c6601342",
    "semantic_title": "user-generated text corpus for evaluating japanese morphological analysis and lexical normalization",
    "citation_count": 4,
    "authors": [
      "Shohei Higashiyama",
      "Masao Utiyama",
      "Taro Watanabe",
      "Eiichiro Sumita"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.439": {
    "title": "GPT Perdetry Test: Generating new meanings for new words",
    "volume": "main",
    "abstract": "Human innovation in language, such as inventing new words, is a challenge for pretrained language models. We assess the ability of one large model, GPT-3, to process new words and decide on their meaning. We create a set of nonce words and prompt GPT-3 to generate their dictionary definitions. We find GPT-3 produces plausible definitions that align with human judgments. Moreover, GPT-3's definitions are sometimes preferred to those invented by humans, signaling its intriguing ability not just to adapt, but to add to the evolving vocabulary of the English language",
    "checked": true,
    "id": "ea55ed2108e8ef144c2eec38cf28a263b7a73239",
    "semantic_title": "gpt perdetry test: generating new meanings for new words",
    "citation_count": 13,
    "authors": [
      "Nikolay Malkin",
      "Sameera Lanka",
      "Pranav Goel",
      "Sudha Rao",
      "Nebojsa Jojic"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.440": {
    "title": "Universal Semantic Tagging for English and Mandarin Chinese",
    "volume": "main",
    "abstract": "Universal Semantic Tagging aims to provide lightweight unified analysis for all languages at the word level. Though the proposed annotation scheme is conceptually promising, the feasibility is only examined in four Indo–European languages. This paper is concerned with extending the annotation scheme to handle Mandarin Chinese and empirically study the plausibility of unifying meaning representations for multiple languages. We discuss a set of language-specific semantic phenomena, propose new annotation specifications and build a richly annotated corpus. The corpus consists of 1100 English–Chinese parallel sentences, where compositional semantic analysis is available for English, and another 1000 Chinese sentences which has enriched syntactic analysis. By means of the new annotations, we also evaluate a series of neural tagging models to gauge how successful semantic tagging can be: accuracies of 92.7% and 94.6% are obtained for Chinese and English respectively. The English tagging performance is remarkably better than the state-of-the-art by 7.7%",
    "checked": true,
    "id": "0c367fb5884fe8d59a0bc7e8dfeacf357a8a0535",
    "semantic_title": "universal semantic tagging for english and mandarin chinese",
    "citation_count": 0,
    "authors": [
      "Wenxi Li",
      "Yiyang Hou",
      "Yajie Ye",
      "Li Liang",
      "Weiwei Sun"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.441": {
    "title": "ShadowGNN: Graph Projection Neural Network for Text-to-SQL Parser",
    "volume": "main",
    "abstract": "Given a database schema, Text-to-SQL aims to translate a natural language question into the corresponding SQL query. Under the setup of cross-domain, traditional semantic parsing models struggle to adapt to unseen database schemas. To improve the model generalization capability for rare and unseen schemas, we propose a new architecture, ShadowGNN, which processes schemas at abstract and semantic levels. By ignoring names of semantic items in databases, abstract schemas are exploited in a well-designed graph projection neural network to obtain delexicalized representation of question and schema. Based on the domain-independent representations, a relation-aware transformer is utilized to further extract logical linking between question and schema. Finally, a SQL decoder with context-free grammar is applied. On the challenging Text-to-SQL benchmark Spider, empirical results show that ShadowGNN outperforms state-of-the-art models. When the annotated data is extremely limited (only 10% training set), ShadowGNN gets over absolute 5% performance gain, which shows its powerful generalization ability. Our implementation will be open-sourced at https://github.com/WowCZ/shadowgnn",
    "checked": true,
    "id": "c114db5f1c38cbe6797bc74ef98072cac71f6cc6",
    "semantic_title": "shadowgnn: graph projection neural network for text-to-sql parser",
    "citation_count": 43,
    "authors": [
      "Zhi Chen",
      "Lu Chen",
      "Yanbin Zhao",
      "Ruisheng Cao",
      "Zihan Xu",
      "Su Zhu",
      "Kai Yu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.442": {
    "title": "Contextualized and Generalized Sentence Representations by Contrastive Self-Supervised Learning: A Case Study on Discourse Relation Analysis",
    "volume": "main",
    "abstract": "We propose a method to learn contextualized and generalized sentence representations using contrastive self-supervised learning. In the proposed method, a model is given a text consisting of multiple sentences. One sentence is randomly selected as a target sentence. The model is trained to maximize the similarity between the representation of the target sentence with its context and that of the masked target sentence with the same context. Simultaneously, the model minimizes the similarity between the latter representation and the representation of a random sentence with the same context. We apply our method to discourse relation analysis in English and Japanese and show that it outperforms strong baseline methods based on BERT, XLNet, and RoBERTa",
    "checked": true,
    "id": "f90795818b3135c9a0fa2ee6c92219a6a980d7f6",
    "semantic_title": "contextualized and generalized sentence representations by contrastive self-supervised learning: a case study on discourse relation analysis",
    "citation_count": 9,
    "authors": [
      "Hirokazu Kiyomaru",
      "Sadao Kurohashi"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.443": {
    "title": "AMR Parsing with Action-Pointer Transformer",
    "volume": "main",
    "abstract": "Abstract Meaning Representation parsing is a sentence-to-graph prediction task where target nodes are not explicitly aligned to sentence tokens. However, since graph nodes are semantically based on one or more sentence tokens, implicit alignments can be derived. Transition-based parsers operate over the sentence from left to right, capturing this inductive bias via alignments at the cost of limited expressiveness. In this work, we propose a transition-based system that combines hard-attention over sentences with a target-side action pointer mechanism to decouple source tokens from node representations and address alignments. We model the transitions as well as the pointer mechanism through straightforward modifications within a single Transformer architecture. Parser state and graph structure information are efficiently encoded using attention heads. We show that our action-pointer approach leads to increased expressiveness and attains large gains (+1.6 points) against the best transition-based AMR parser in very similar conditions. While using no graph re-categorization, our single model yields the second best Smatch score on AMR 2.0 (81.8), which is further improved to 83.4 with silver data and ensemble decoding",
    "checked": true,
    "id": "44e02e57b186dfef713b24beb44727d4b1048fe8",
    "semantic_title": "amr parsing with action-pointer transformer",
    "citation_count": 42,
    "authors": [
      "Jiawei Zhou",
      "Tahira Naseem",
      "Ramón Fernandez Astudillo",
      "Radu Florian"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.444": {
    "title": "NL-EDIT: Correcting Semantic Parse Errors through Natural Language Interaction",
    "volume": "main",
    "abstract": "We study semantic parsing in an interactive setting in which users correct errors with natural language feedback. We present NL-EDIT, a model for interpreting natural language feedback in the interaction context to generate a sequence of edits that can be applied to the initial parse to correct its errors. We show that NL-EDIT can boost the accuracy of existing text-to-SQL parsers by up to 20% with only one turn of correction. We analyze the limitations of the model and discuss directions for improvement and evaluation. The code and datasets used in this paper are publicly available at http://aka.ms/NLEdit",
    "checked": true,
    "id": "6b9564d94aa4415b90561b430ac85b0ee11a5833",
    "semantic_title": "nl-edit: correcting semantic parse errors through natural language interaction",
    "citation_count": 39,
    "authors": [
      "Ahmed Elgohary",
      "Christopher Meek",
      "Matthew Richardson",
      "Adam Fourney",
      "Gonzalo Ramos",
      "Ahmed Hassan Awadallah"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.445": {
    "title": "Unsupervised Concept Representation Learning for Length-Varying Text Similarity",
    "volume": "main",
    "abstract": "Measuring document similarity plays an important role in natural language processing tasks. Most existing document similarity approaches suffer from the information gap caused by context and vocabulary mismatches when comparing varying-length texts. In this paper, we propose an unsupervised concept representation learning approach to address the above issues. Specifically, we propose a novel Concept Generation Network (CGNet) to learn concept representations from the perspective of the entire text corpus. Moreover, a concept-based document matching method is proposed to leverage advances in the recognition of local phrase features and corpus-level concept features. Extensive experiments on real-world data sets demonstrate that new method can achieve a considerable improvement in comparing length-varying texts. In particular, our model achieved 6.5% better F1 Score compared to the best of the baseline models for a concept-project benchmark dataset",
    "checked": true,
    "id": "26380f0bf00ccffa32170838dc22f53ae096dfc4",
    "semantic_title": "unsupervised concept representation learning for length-varying text similarity",
    "citation_count": 3,
    "authors": [
      "Xuchao Zhang",
      "Bo Zong",
      "Wei Cheng",
      "Jingchao Ni",
      "Yanchi Liu",
      "Haifeng Chen"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.446": {
    "title": "Augmenting Knowledge-grounded Conversations with Sequential Knowledge Transition",
    "volume": "main",
    "abstract": "Knowledge data are massive and widespread in the real-world, which can serve as good external sources to enrich conversations. However, in knowledge-grounded conversations, current models still lack the fine-grained control over knowledge selection and integration with dialogues, which finally leads to the knowledge-irrelevant response generation problems: 1) knowledge selection merely relies on the dialogue context, ignoring the inherent knowledge transitions along with conversation flows; 2) the models often over-fit during training, resulting with incoherent response by referring to unrelated tokens from specific knowledge content in the testing phase; 3) although response is generated upon the dialogue history and knowledge, the models often tend to overlook the selected knowledge, and hence generates knowledge-irrelevant response. To address these problems, we proposed to explicitly model the knowledge transition in sequential multi-turn conversations by abstracting knowledge into topic tags. Besides, to fully utilizing the selected knowledge in generative process, we propose pre-training a knowledge-aware response generator to pay more attention on the selected knowledge. In particular, a sequential knowledge transition model equipped with a pre-trained knowledge-aware response generator (SKT-KG) formulates the high-level knowledge transition and fully utilizes the limited knowledge data. Experimental results on both structured and unstructured knowledge-grounded dialogue benchmarks indicate that our model achieves better performance over baseline models",
    "checked": true,
    "id": "0f88bb2df2c7c2e0ee5ec95cf7459f07b9395fb4",
    "semantic_title": "augmenting knowledge-grounded conversations with sequential knowledge transition",
    "citation_count": 28,
    "authors": [
      "Haolan Zhan",
      "Hainan Zhang",
      "Hongshen Chen",
      "Zhuoye Ding",
      "Yongjun Bao",
      "Yanyan Lan"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.447": {
    "title": "Adversarial Self-Supervised Learning for Out-of-Domain Detection",
    "volume": "main",
    "abstract": "Detecting out-of-domain (OOD) intents is crucial for the deployed task-oriented dialogue system. Previous unsupervised OOD detection methods only extract discriminative features of different in-domain intents while supervised counterparts can directly distinguish OOD and in-domain intents but require extensive labeled OOD data. To combine the benefits of both types, we propose a self-supervised contrastive learning framework to model discriminative semantic features of both in-domain intents and OOD intents from unlabeled data. Besides, we introduce an adversarial augmentation neural module to improve the efficiency and robustness of contrastive learning. Experiments on two public benchmark datasets show that our method can consistently outperform the baselines with a statistically significant margin",
    "checked": true,
    "id": "7d3342983a2c76c65318008a30e5389bc9dcc082",
    "semantic_title": "adversarial self-supervised learning for out-of-domain detection",
    "citation_count": 29,
    "authors": [
      "Zhiyuan Zeng",
      "Keqing He",
      "Yuanmeng Yan",
      "Hong Xu",
      "Weiran Xu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.448": {
    "title": "Leveraging Slot Descriptions for Zero-Shot Cross-Domain Dialogue StateTracking",
    "volume": "main",
    "abstract": "Zero-shot cross-domain dialogue state tracking (DST) enables us to handle unseen domains without the expense of collecting in-domain data. In this paper, we propose a slot descriptions enhanced generative approach for zero-shot cross-domain DST. Specifically, our model first encodes a dialogue context and a slot with a pre-trained self-attentive encoder, and generates slot value in auto-regressive manner. In addition, we incorporate Slot Type Informed Descriptions that capture the shared information of different slots to facilitates the cross-domain knowledge transfer. Experimental results on MultiWOZ shows that our model significantly improve existing state-of-the-art results in zero-shot cross-domain setting",
    "checked": true,
    "id": "aecc84aa9ff29a99f44556689592cd0fff84cb87",
    "semantic_title": "leveraging slot descriptions for zero-shot cross-domain dialogue statetracking",
    "citation_count": 77,
    "authors": [
      "Zhaojiang Lin",
      "Bing Liu",
      "Seungwhan Moon",
      "Paul Crook",
      "Zhenpeng Zhou",
      "Zhiguang Wang",
      "Zhou Yu",
      "Andrea Madotto",
      "Eunjoon Cho",
      "Rajen Subba"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.449": {
    "title": "Hierarchical Transformer for Task Oriented Dialog Systems",
    "volume": "main",
    "abstract": "Generative models for dialog systems have gained much interest because of the recent success of RNN and Transformer based models in tasks like question answering and summarization. Although the task of dialog response generation is generally seen as a sequence to sequence (Seq2Seq) problem, researchers in the past have found it challenging to train dialog systems using the standard Seq2Seq models. Therefore, to help the model learn meaningful utterance and conversation level features, Sordoni et al. (2015b), Serban et al. (2016) proposed Hierarchical RNN architecture, which was later adopted by several other RNN based dialog systems. With the transformer-based models dominating the seq2seq problems lately, the natural question to ask is the applicability of the notion of hierarchy in transformer-based dialog systems. In this paper, we propose a generalized framework for Hierarchical Transformer Encoders and show how a standard transformer can be morphed into any hierarchical encoder, including HRED and HIBERT like models, by using specially designed attention masks and positional encodings. We demonstrate that Hierarchical Encoding helps achieve better natural language understanding of the contexts in transformer-based models for task-oriented dialog systems through a wide range of experiments",
    "checked": true,
    "id": "60a31c41a85d5f58eba6ddf37408b45a83353063",
    "semantic_title": "hierarchical transformer for task oriented dialog systems",
    "citation_count": 26,
    "authors": [
      "Bishal Santra",
      "Potnuru Anusha",
      "Pawan Goyal"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.450": {
    "title": "Measuring the ‘I don't know' Problem through the Lens of Gricean Quantity",
    "volume": "main",
    "abstract": "We consider the intrinsic evaluation of neural generative dialog models through the lens of Grice's Maxims of Conversation (1975). Based on the maxim of Quantity (be informative), we propose Relative Utterance Quantity (RUQ) to diagnose the ‘I don't know' problem, in which a dialog system produces generic responses. The linguistically motivated RUQ diagnostic compares the model score of a generic response to that of the reference response. We find that for reasonable baseline models, ‘I don't know' is preferred over the reference the majority of the time, but this can be reduced to less than 5% with hyperparameter tuning. RUQ allows for the direct analysis of the ‘I don't know' problem, which has been addressed but not analyzed by prior work",
    "checked": true,
    "id": "5e9e088513369d6d003e76d422701783e852078b",
    "semantic_title": "measuring the ‘i don't know' problem through the lens of gricean quantity",
    "citation_count": 2,
    "authors": [
      "Huda Khayrallah",
      "João Sedoc"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.451": {
    "title": "RTFE: A Recursive Temporal Fact Embedding Framework for Temporal Knowledge Graph Completion",
    "volume": "main",
    "abstract": "Static knowledge graph (SKG) embedding (SKGE) has been studied intensively in the past years. Recently, temporal knowledge graph (TKG) embedding (TKGE) has emerged. In this paper, we propose a Recursive Temporal Fact Embedding (RTFE) framework to transplant SKGE models to TKGs and to enhance the performance of existing TKGE models for TKG completion. Different from previous work which ignores the continuity of states of TKG in time evolution, we treat the sequence of graphs as a Markov chain, which transitions from the previous state to the next state. RTFE takes the SKGE to initialize the embeddings of TKG. Then it recursively tracks the state transition of TKG by passing updated parameters/features between timestamps. Specifically, at each timestamp, we approximate the state transition as the gradient update process. Since RTFE learns each timestamp recursively, it can naturally transit to future timestamps. Experiments on five TKG datasets show the effectiveness of RTFE",
    "checked": true,
    "id": "57f3b5e3b41fac84661d49299bd255dbf1c225a3",
    "semantic_title": "rtfe: a recursive temporal fact embedding framework for temporal knowledge graph completion",
    "citation_count": 23,
    "authors": [
      "Youri Xu",
      "Haihong E",
      "Meina Song",
      "Wenyu Song",
      "Xiaodong Lv",
      "Wang Haotian",
      "Yang Jinrui"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.452": {
    "title": "Open Hierarchical Relation Extraction",
    "volume": "main",
    "abstract": "Open relation extraction (OpenRE) aims to extract novel relation types from open-domain corpora, which plays an important role in completing the relation schemes of knowledge bases (KBs). Most OpenRE methods cast different relation types in isolation without considering their hierarchical dependency. We argue that OpenRE is inherently in close connection with relation hierarchies. To establish the bidirectional connections between OpenRE and relation hierarchy, we propose the task of open hierarchical relation extraction and present a novel OHRE framework for the task. We propose a dynamic hierarchical triplet objective and hierarchical curriculum training paradigm, to effectively integrate hierarchy information into relation representations for better novel relation extraction. We also present a top-down hierarchy expansion algorithm to add the extracted relations into existing hierarchies with reasonable interpretability. Comprehensive experiments show that OHRE outperforms state-of-the-art models by a large margin on both relation clustering and hierarchy expansion",
    "checked": true,
    "id": "b66d9b7326bd794c4c00224769db0eb2552a55b2",
    "semantic_title": "open hierarchical relation extraction",
    "citation_count": 30,
    "authors": [
      "Kai Zhang",
      "Yuan Yao",
      "Ruobing Xie",
      "Xu Han",
      "Zhiyuan Liu",
      "Fen Lin",
      "Leyu Lin",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.453": {
    "title": "Jointly Extracting Explicit and Implicit Relational Triples with Reasoning Pattern Enhanced Binary Pointer Network",
    "volume": "main",
    "abstract": "Relational triple extraction is a crucial task for knowledge graph construction. Existing methods mainly focused on explicit relational triples that are directly expressed, but usually suffer from ignoring implicit triples that lack explicit expressions. This will lead to serious incompleteness of the constructed knowledge graphs. Fortunately, other triples in the sentence provide supplementary information for discovering entity pairs that may have implicit relations. Also, the relation types between the implicitly connected entity pairs can be identified with relational reasoning patterns in the real world. In this paper, we propose a unified framework to jointly extract explicit and implicit relational triples. To explore entity pairs that may be implicitly connected by relations, we propose a binary pointer network to extract overlapping relational triples relevant to each word sequentially and retain the information of previously extracted triples in an external memory. To infer the relation types of implicit relational triples, we propose to introduce real-world relational reasoning patterns in our model and capture these patterns with a relation network. We conduct experiments on several benchmark datasets, and the results prove the validity of our method",
    "checked": true,
    "id": "ad419277f74965a080ab60a4366e736fe62931ab",
    "semantic_title": "jointly extracting explicit and implicit relational triples with reasoning pattern enhanced binary pointer network",
    "citation_count": 22,
    "authors": [
      "Yubo Chen",
      "Yunqi Zhang",
      "Changran Hu",
      "Yongfeng Huang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.454": {
    "title": "Multi-Grained Knowledge Distillation for Named Entity Recognition",
    "volume": "main",
    "abstract": "Although pre-trained big models (e.g., BERT, ERNIE, XLNet, GPT3 etc.) have delivered top performance in Seq2seq modeling, their deployments in real-world applications are often hindered by the excessive computations and memory demand involved. For many applications, including named entity recognition (NER), matching the state-of-the-art result under budget has attracted considerable attention. Drawing power from the recent advance in knowledge distillation (KD), this work presents a novel distillation scheme to efficiently transfer the knowledge learned from big models to their more affordable counterpart. Our solution highlights the construction of surrogate labels through the k-best Viterbi algorithm to distill knowledge from the teacher model. To maximally assimilate knowledge into the student model, we propose a multi-grained distillation scheme, which integrates cross entropy involved in conditional random field (CRF) and fuzzy learning. To validate the effectiveness of our proposal, we conducted a comprehensive evaluation on five NER benchmarks, reporting cross-the-board performance gains relative to competing prior-arts. We further discuss ablation results to dissect our gains",
    "checked": true,
    "id": "f47e0c58583943ed7b6e9800ab30ac5fae69a99d",
    "semantic_title": "multi-grained knowledge distillation for named entity recognition",
    "citation_count": 12,
    "authors": [
      "Xuan Zhou",
      "Xiao Zhang",
      "Chenyang Tao",
      "Junya Chen",
      "Bing Xu",
      "Wei Wang",
      "Jing Xiao"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.455": {
    "title": "SGG: Learning to Select, Guide, and Generate for Keyphrase Generation",
    "volume": "main",
    "abstract": "Keyphrases, that concisely summarize the high-level topics discussed in a document, can be categorized into present keyphrase which explicitly appears in the source text and absent keyphrase which does not match any contiguous subsequence but is highly semantically related to the source. Most existing keyphrase generation approaches synchronously generate present and absent keyphrases without explicitly distinguishing these two categories. In this paper, a Select-Guide-Generate (SGG) approach is proposed to deal with present and absent keyphrases generation separately with different mechanisms. Specifically, SGG is a hierarchical neural network which consists of a pointing-based selector at low layer concentrated on present keyphrase generation, a selection-guided generator at high layer dedicated to absent keyphrase generation, and a guider in the middle to transfer information from selector to generator. Experimental results on four keyphrase generation benchmarks demonstrate the effectiveness of our model, which significantly outperforms the strong baselines for both present and absent keyphrases generation. Furthermore, we extend SGG to a title generation task which indicates its extensibility in natural language generation tasks",
    "checked": true,
    "id": "ea804abd23fa29e2c6183da847c5889e06ccbf42",
    "semantic_title": "sgg: learning to select, guide, and generate for keyphrase generation",
    "citation_count": 10,
    "authors": [
      "Jing Zhao",
      "Junwei Bao",
      "Yifan Wang",
      "Youzheng Wu",
      "Xiaodong He",
      "Bowen Zhou"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.456": {
    "title": "Towards Sentiment and Emotion aided Multi-modal Speech Act Classification in Twitter",
    "volume": "main",
    "abstract": "Speech Act Classification determining the communicative intent of an utterance has been investigated widely over the years as a standalone task. This holds true for discussion in any fora including social media platform such as Twitter. But the emotional state of the tweeter which has a considerable effect on the communication has not received the attention it deserves. Closely related to emotion is sentiment, and understanding of one helps understand the other. In this work, we firstly create a new multi-modal, emotion-TA (‘TA' means tweet act, i.e., speech act in Twitter) dataset called EmoTA collected from open-source Twitter dataset. We propose a Dyadic Attention Mechanism (DAM) based multi-modal, adversarial multi-tasking framework. DAM incorporates intra-modal and inter-modal attention to fuse multiple modalities and learns generalized features across all the tasks. Experimental results indicate that the proposed framework boosts the performance of the primary task, i.e., TA classification (TAC) by benefitting from the two secondary tasks, i.e., Sentiment and Emotion Analysis compared to its uni-modal and single task TAC (tweet act classification) variants",
    "checked": true,
    "id": "0a74c41c70fb801f66d375b1ac4d019e5347c174",
    "semantic_title": "towards sentiment and emotion aided multi-modal speech act classification in twitter",
    "citation_count": 26,
    "authors": [
      "Tulika Saha",
      "Apoorva Upadhyaya",
      "Sriparna Saha",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.457": {
    "title": "Generative Imagination Elevates Machine Translation",
    "volume": "main",
    "abstract": "There are common semantics shared across text and images. Given a sentence in a source language, whether depicting the visual scene helps translation into a target language? Existing multimodal neural machine translation methods (MNMT) require triplets of bilingual sentence - image for training and tuples of source sentence - image for inference. In this paper, we propose ImagiT, a novel machine translation method via visual imagination. ImagiT first learns to generate visual representation from the source sentence, and then utilizes both source sentence and the \"imagined representation\" to produce a target translation. Unlike previous methods, it only needs the source sentence at the inference time. Experiments demonstrate that ImagiT benefits from visual imagination and significantly outperforms the text-only neural machine translation baselines. Further analysis reveals that the imagination process in ImagiT helps fill in missing information when performing the degradation strategy",
    "checked": true,
    "id": "07a394c6d95c646fb4c50393046db58f7d6fddee",
    "semantic_title": "generative imagination elevates machine translation",
    "citation_count": 30,
    "authors": [
      "Quanyu Long",
      "Mingxuan Wang",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.458": {
    "title": "Non-Autoregressive Translation by Learning Target Categorical Codes",
    "volume": "main",
    "abstract": "Non-autoregressive Transformer is a promising text generation model. However, current non-autoregressive models still fall behind their autoregressive counterparts in translation quality. We attribute this accuracy gap to the lack of dependency modeling among decoder inputs. In this paper, we propose CNAT, which learns implicitly categorical codes as latent variables into the non-autoregressive decoding. The interaction among these categorical codes remedies the missing dependencies and improves the model capacity. Experiment results show that our model achieves comparable or better performance in machine translation tasks than several strong baselines",
    "checked": true,
    "id": "8e23606793d4ce455302b110f50033c6e241d9aa",
    "semantic_title": "non-autoregressive translation by learning target categorical codes",
    "citation_count": 32,
    "authors": [
      "Yu Bao",
      "Shujian Huang",
      "Tong Xiao",
      "Dongqi Wang",
      "Xinyu Dai",
      "Jiajun Chen"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.459": {
    "title": "Training Data Augmentation for Code-Mixed Translation",
    "volume": "main",
    "abstract": "Machine translation of user-generated code-mixed inputs to English is of crucial importance in applications like web search and targeted advertising. We address the scarcity of parallel training data for training such models by designing a strategy of converting existing non-code-mixed parallel data sources to code-mixed parallel data. We present an m-BERT based procedure whose core learnable component is a ternary sequence labeling model, that can be trained with a limited code-mixed corpus alone. We show a 5.8 point increase in BLEU on heavily code-mixed sentences by training a translation model using our data augmentation strategy on an Hindi-English code-mixed translation task",
    "checked": true,
    "id": "fcf24ec00524fd4b74ad1b5615eeab53377a43c9",
    "semantic_title": "training data augmentation for code-mixed translation",
    "citation_count": 22,
    "authors": [
      "Abhirut Gupta",
      "Aditya Vavre",
      "Sunita Sarawagi"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.460": {
    "title": "Rethinking Perturbations in Encoder-Decoders for Fast Training",
    "volume": "main",
    "abstract": "We often use perturbations to regularize neural models. For neural encoder-decoders, previous studies applied the scheduled sampling (Bengio et al., 2015) and adversarial perturbations (Sato et al., 2019) as perturbations but these methods require considerable computational time. Thus, this study addresses the question of whether these approaches are efficient enough for training time. We compare several perturbations in sequence-to-sequence problems with respect to computational time. Experimental results show that the simple techniques such as word dropout (Gal and Ghahramani, 2016) and random replacement of input tokens achieve comparable (or better) scores to the recently proposed perturbations, even though these simple methods are faster",
    "checked": true,
    "id": "4f90ef30a35db06b4e953cfce8b8fc4179d4bff7",
    "semantic_title": "rethinking perturbations in encoder-decoders for fast training",
    "citation_count": 41,
    "authors": [
      "Sho Takase",
      "Shun Kiyono"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.461": {
    "title": "Context-aware Decoder for Neural Machine Translation using a Target-side Document-Level Language Model",
    "volume": "main",
    "abstract": "Although many end-to-end context-aware neural machine translation models have been proposed to incorporate inter-sentential contexts in translation, these models can be trained only in domains where parallel documents with sentential alignments exist. We therefore present a simple method to perform context-aware decoding with any pre-trained sentence-level translation model by using a document-level language model. Our context-aware decoder is built upon sentence-level parallel data and target-side document-level monolingual data. From a theoretical viewpoint, our core contribution is the novel representation of contextual information using point-wise mutual information between context and the current sentence. We demonstrate the effectiveness of our method on English to Russian translation, by evaluating with BLEU and contrastive tests for context-aware translation",
    "checked": true,
    "id": "d2405a1cb8b41701fa8177e1c2438c730051ac01",
    "semantic_title": "context-aware decoder for neural machine translation using a target-side document-level language model",
    "citation_count": 8,
    "authors": [
      "Amane Sugiyama",
      "Naoki Yoshinaga"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.462": {
    "title": "Machine Translated Text Detection Through Text Similarity with Round-Trip Translation",
    "volume": "main",
    "abstract": "Translated texts have been used for malicious purposes, i.e., plagiarism or fake reviews. Existing detectors have been built around a specific translator (e.g., Google) but fail to detect a translated text from a strange translator. If we use the same translator, the translated text is similar to its round-trip translation, which is when text is translated into another language and translated back into the original language. However, a round-trip translated text is significantly different from the original text or a translated text using a strange translator. Hence, we propose a detector using text similarity with round-trip translation (TSRT). TSRT achieves 86.9% accuracy in detecting a translated text from a strange translator. It outperforms existing detectors (77.9%) and human recognition (53.3%)",
    "checked": true,
    "id": "48e83d3a69b2f7e9554de0bbcc7635ab96c3a46f",
    "semantic_title": "machine translated text detection through text similarity with round-trip translation",
    "citation_count": 7,
    "authors": [
      "Hoang-Quoc Nguyen-Son",
      "Tran Thao",
      "Seira Hidano",
      "Ishita Gupta",
      "Shinsaku Kiyomoto"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.463": {
    "title": "TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference",
    "volume": "main",
    "abstract": "Existing pre-trained language models (PLMs) are often computationally expensive in inference, making them impractical in various resource-limited real-world applications. To address this issue, we propose a dynamic token reduction approach to accelerate PLMs' inference, named TR-BERT, which could flexibly adapt the layer number of each token in inference to avoid redundant calculation. Specially, TR-BERT formulates the token reduction process as a multi-step token selection problem and automatically learns the selection strategy via reinforcement learning. The experimental results on several downstream NLP tasks show that TR-BERT is able to speed up BERT by 2-5 times to satisfy various performance demands. Moreover, TR-BERT can also achieve better performance with less computation in a suite of long-text tasks since its token-level layer number adaption greatly accelerates the self-attention operation in PLMs. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/TR-BERT",
    "checked": true,
    "id": "6da4b231148bf26677233d1f778d08a5d26f4313",
    "semantic_title": "tr-bert: dynamic token reduction for accelerating bert inference",
    "citation_count": 51,
    "authors": [
      "Deming Ye",
      "Yankai Lin",
      "Yufei Huang",
      "Maosong Sun"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.464": {
    "title": "Breadth First Reasoning Graph for Multi-hop Question Answering",
    "volume": "main",
    "abstract": "Recently Graph Neural Network (GNN) has been used as a promising tool in multi-hop question answering task. However, the unnecessary updations and simple edge constructions prevent an accurate answer span extraction in a more direct and interpretable way. In this paper, we propose a novel model of Breadth First Reasoning Graph (BFR-Graph), which presents a new message passing way that better conforms to the reasoning process. In BFR-Graph, the reasoning message is required to start from the question node and pass to the next sentences node hop by hop until all the edges have been passed, which can effectively prevent each node from over-smoothing or being updated multiple times unnecessarily. To introduce more semantics, we also define the reasoning graph as a weighted graph with considering the number of co-occurrence entities and the distance between sentences. Then we present a more direct and interpretable way to aggregate scores from different levels of granularity based on the GNN. On HotpotQA leaderboard, the proposed BFR-Graph achieves state-of-the-art on answer span prediction",
    "checked": true,
    "id": "549c682b49fcd29f7404ecfb11fb36467131d3ec",
    "semantic_title": "breadth first reasoning graph for multi-hop question answering",
    "citation_count": 16,
    "authors": [
      "Yongjie Huang",
      "Meng Yang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.465": {
    "title": "Improving Zero-Shot Cross-lingual Transfer for Multilingual Question Answering over Knowledge Graph",
    "volume": "main",
    "abstract": "Multilingual question answering over knowledge graph (KGQA) aims to derive answers from a knowledge graph (KG) for questions in multiple languages. To be widely applicable, we focus on its zero-shot transfer setting. That is, we can only access training data in a high-resource language, while need to answer multilingual questions without any labeled data in target languages. A straightforward approach is resorting to pre-trained multilingual models (e.g., mBERT) for cross-lingual transfer, but there is a still significant gap of KGQA performance between source and target languages. In this paper, we exploit unsupervised bilingual lexicon induction (BLI) to map training questions in source language into those in target language as augmented training data, which circumvents language inconsistency between training and inference. Furthermore, we propose an adversarial learning strategy to alleviate syntax-disorder of the augmented data, making the model incline to both language- and syntax-independence. Consequently, our model narrows the gap in zero-shot cross-lingual transfer. Experiments on two multilingual KGQA datasets with 11 zero-resource languages verify its effectiveness",
    "checked": true,
    "id": "8745ca0ffc4c06650cd795a532680cf6c1e61f89",
    "semantic_title": "improving zero-shot cross-lingual transfer for multilingual question answering over knowledge graph",
    "citation_count": 38,
    "authors": [
      "Yucheng Zhou",
      "Xiubo Geng",
      "Tao Shen",
      "Wenqiang Zhang",
      "Daxin Jiang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.466": {
    "title": "RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering",
    "volume": "main",
    "abstract": "In open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for finding answers. Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for semantic matching. However, it is difficult to effectively train a dual-encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and limited training data. To address these challenges, we propose an optimized training approach, called RocketQA, to improving dense passage retrieval. We make three major technical contributions in RocketQA, namely cross-batch negatives, denoised hard negatives and data augmentation. The experiment results show that RocketQA significantly outperforms previous state-of-the-art models on both MSMARCO and Natural Questions. We also conduct extensive experiments to examine the effectiveness of the three strategies in RocketQA. Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever",
    "checked": true,
    "id": "f2c1ab4e850ad89bd6b6b9908e12a4151d152070",
    "semantic_title": "rocketqa: an optimized training approach to dense passage retrieval for open-domain question answering",
    "citation_count": 511,
    "authors": [
      "Yingqi Qu",
      "Yuchen Ding",
      "Jing Liu",
      "Kai Liu",
      "Ruiyang Ren",
      "Wayne Xin Zhao",
      "Daxiang Dong",
      "Hua Wu",
      "Haifeng Wang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.467": {
    "title": "DAGN: Discourse-Aware Graph Network for Logical Reasoning",
    "volume": "main",
    "abstract": "Recent QA with logical reasoning questions requires passage-level relations among the sentences. However, current approaches still focus on sentence-level relations interacting among tokens. In this work, we explore aggregating passage-level clues for solving logical reasoning QA by using discourse-based information. We propose a discourse-aware graph network (DAGN) that reasons relying on the discourse structure of the texts. The model encodes discourse information as a graph with elementary discourse units (EDUs) and discourse relations, and learns the discourse-aware features via a graph network for downstream QA tasks. Experiments are conducted on two logical reasoning QA datasets, ReClor and LogiQA, and our proposed DAGN achieves competitive results. The source code is available at https://github.com/Eleanor-H/DAGN",
    "checked": true,
    "id": "7a9ef45ec4a6fe04c373715f455924b6c1c51ddd",
    "semantic_title": "dagn: discourse-aware graph network for logical reasoning",
    "citation_count": 52,
    "authors": [
      "Yinya Huang",
      "Meng Fang",
      "Yu Cao",
      "Liwei Wang",
      "Xiaodan Liang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.468": {
    "title": "Designing a Minimal Retrieve-and-Read System for Open-Domain Question Answering",
    "volume": "main",
    "abstract": "In open-domain question answering (QA), retrieve-and-read mechanism has the inherent benefit of interpretability and the easiness of adding, removing, or editing knowledge compared to the parametric approaches of closed-book QA models. However, it is also known to suffer from its large storage footprint due to its document corpus and index. Here, we discuss several orthogonal strategies to drastically reduce the footprint of a retrieve-and-read open-domain QA system by up to 160x. Our results indicate that retrieve-and-read can be a viable option even in a highly constrained serving environment such as edge devices, as we show that it can achieve better accuracy than a purely parametric model with comparable docker-level system size",
    "checked": true,
    "id": "681b2120803ca621620341ef3da2c041f8e0ea02",
    "semantic_title": "designing a minimal retrieve-and-read system for open-domain question answering",
    "citation_count": 6,
    "authors": [
      "Sohee Yang",
      "Minjoon Seo"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.469": {
    "title": "Unsupervised Multi-hop Question Answering by Question Generation",
    "volume": "main",
    "abstract": "Obtaining training data for multi-hop question answering (QA) is time-consuming and resource-intensive. We explore the possibility to train a well-performed multi-hop QA model without referencing any human-labeled multi-hop question-answer pairs, i.e., unsupervised multi-hop QA. We propose MQA-QG, an unsupervised framework that can generate human-like multi-hop training data from both homogeneous and heterogeneous data sources. MQA-QG generates questions by first selecting/generating relevant information from each data source and then integrating the multiple information to form a multi-hop question. Using only generated training data, we can train a competent multi-hop QA which achieves 61% and 83% of the supervised learning performance for the HybridQA and the HotpotQA dataset, respectively. We also show that pretraining the QA system with the generated data would greatly reduce the demand for human-annotated training data. Our codes are publicly available at https://github.com/teacherpeterpan/Unsupervised-Multi-hop-QA",
    "checked": true,
    "id": "405c0d9b7cf5482d2e1197167f690e7b7801b9bd",
    "semantic_title": "unsupervised multi-hop question answering by question generation",
    "citation_count": 52,
    "authors": [
      "Liangming Pan",
      "Wenhu Chen",
      "Wenhan Xiong",
      "Min-Yen Kan",
      "William Yang Wang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.470": {
    "title": "Sliding Selector Network with Dynamic Memory for Extractive Summarization of Long Documents",
    "volume": "main",
    "abstract": "Neural-based summarization models suffer from the length limitation of text encoder. Long documents have to been truncated before they are sent to the model, which results in huge loss of summary-relevant contents. To address this issue, we propose the sliding selector network with dynamic memory for extractive summarization of long-form documents, which employs a sliding window to extract summary sentences segment by segment. Moreover, we adopt memory mechanism to preserve and update the history information dynamically, allowing the semantic flow across different windows. Experimental results on two large-scale datasets that consist of scientific papers demonstrate that our model substantially outperforms previous state-of-the-art models. Besides, we perform qualitative and quantitative investigations on how our model works and where the performance gain comes from",
    "checked": true,
    "id": "592dd524b78ff141ef41ed8b784cfe3de32ec974",
    "semantic_title": "sliding selector network with dynamic memory for extractive summarization of long documents",
    "citation_count": 28,
    "authors": [
      "Peng Cui",
      "Le Hu"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.471": {
    "title": "AdaptSum: Towards Low-Resource Domain Adaptation for Abstractive Summarization",
    "volume": "main",
    "abstract": "State-of-the-art abstractive summarization models generally rely on extensive labeled data, which lowers their generalization ability on domains where such data are not available. In this paper, we present a study of domain adaptation for the abstractive summarization task across six diverse target domains in a low-resource setting. Specifically, we investigate the second phase of pre-training on large-scale generative models under three different settings: 1) source domain pre-training; 2) domain-adaptive pre-training; and 3) task-adaptive pre-training. Experiments show that the effectiveness of pre-training is correlated with the similarity between the pre-training data and the target domain task. Moreover, we find that continuing pre-training could lead to the pre-trained model's catastrophic forgetting, and a learning method with less forgetting can alleviate this issue. Furthermore, results illustrate that a huge gap still exists between the low-resource and high-resource settings, which highlights the need for more advanced domain adaptation methods for the abstractive summarization task",
    "checked": true,
    "id": "32128a25b1ebf868f9b02590e361a98524bd371b",
    "semantic_title": "adaptsum: towards low-resource domain adaptation for abstractive summarization",
    "citation_count": 73,
    "authors": [
      "Tiezheng Yu",
      "Zihan Liu",
      "Pascale Fung"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.472": {
    "title": "QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization",
    "volume": "main",
    "abstract": "Meetings are a key component of human collaboration. As increasing numbers of meetings are recorded and transcribed, meeting summaries have become essential to remind those who may or may not have attended the meetings about the key decisions made and the tasks to be completed. However, it is hard to create a single short summary that covers all the content of a long meeting involving multiple people and topics. In order to satisfy the needs of different types of users, we define a new query-based multi-domain meeting summarization task, where models have to select and summarize relevant spans of meetings in response to a query, and we introduce QMSum, a new benchmark for this task. QMSum consists of 1,808 query-summary pairs over 232 meetings in multiple domains. Besides, we investigate a locate-then-summarize method and evaluate a set of strong summarization baselines on the task. Experimental results and manual analysis reveal that QMSum presents significant challenges in long meeting summarization for future research. Dataset is available at https://github.com/Yale-LILY/QMSum",
    "checked": true,
    "id": "aa28873534c24e4a8c5deb7bff723cd5fc69a6f0",
    "semantic_title": "qmsum: a new benchmark for query-based multi-domain meeting summarization",
    "citation_count": 248,
    "authors": [
      "Ming Zhong",
      "Da Yin",
      "Tao Yu",
      "Ahmad Zaidi",
      "Mutethia Mutuma",
      "Rahul Jha",
      "Ahmed Hassan Awadallah",
      "Asli Celikyilmaz",
      "Yang Liu",
      "Xipeng Qiu",
      "Dragomir Radev"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.473": {
    "title": "MM-AVS: A Full-Scale Dataset for Multi-modal Summarization",
    "volume": "main",
    "abstract": "Multimodal summarization becomes increasingly significant as it is the basis for question answering, Web search, and many other downstream tasks. However, its learning materials have been lacking a holistic organization by integrating resources from various modalities, thereby lagging behind the research progress of this field. In this study, we release a full-scale multimodal dataset comprehensively gathering documents, summaries, images, captions, videos, audios, transcripts, and titles in English from CNN and Daily Mail. To our best knowledge, this is the first collection that spans all modalities and nearly comprises all types of materials available in this community. In addition, we devise a baseline model based on the novel dataset, which employs a newly proposed Jump-Attention mechanism based on transcripts. The experimental results validate the important assistance role of the external information for multimodal summarization",
    "checked": true,
    "id": "bdba49fa211a42b3814bcc10548361fa41ceeeb4",
    "semantic_title": "mm-avs: a full-scale dataset for multi-modal summarization",
    "citation_count": 18,
    "authors": [
      "Xiyan Fu",
      "Jun Wang",
      "Zhenglu Yang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.474": {
    "title": "MediaSum: A Large-scale Media Interview Dataset for Dialogue Summarization",
    "volume": "main",
    "abstract": "This paper introduces MediaSum, a large-scale media interview dataset consisting of 463.6K transcripts with abstractive summaries. To create this dataset, we collect interview transcripts from NPR and CNN and employ the overview and topic descriptions as summaries. Compared with existing public corpora for dialogue summarization, our dataset is an order of magnitude larger and contains complex multi-party conversations from multiple domains. We conduct statistical analysis to demonstrate the unique positional bias exhibited in the transcripts of televised and radioed interviews. We also show that MediaSum can be used in transfer learning to improve a model's performance on other dialogue summarization tasks",
    "checked": true,
    "id": "9623e9e461647a10a8f14419a9abe40482e9eb47",
    "semantic_title": "mediasum: a large-scale media interview dataset for dialogue summarization",
    "citation_count": 100,
    "authors": [
      "Chenguang Zhu",
      "Yang Liu",
      "Jie Mei",
      "Michael Zeng"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.475": {
    "title": "Improving Faithfulness in Abstractive Summarization with Contrast Candidate Generation and Selection",
    "volume": "main",
    "abstract": "Despite significant progress in neural abstractive summarization, recent studies have shown that the current models are prone to generating summaries that are unfaithful to the original context. To address the issue, we study contrast candidate generation and selection as a model-agnostic post-processing technique to correct the extrinsic hallucinations (i.e. information not present in the source text) in unfaithful summaries. We learn a discriminative correction model by generating alternative candidate summaries where named entities and quantities in the generated summary are replaced with ones with compatible semantic types from the source document. This model is then used to select the best candidate as the final output summary. Our experiments and analysis across a number of neural summarization systems show that our proposed method is effective in identifying and correcting extrinsic hallucinations. We analyze the typical hallucination phenomenon by different types of neural summarization systems, in hope to provide insights for future work on the direction",
    "checked": true,
    "id": "6399471944fdf64ad2d43de5a83e02035c138788",
    "semantic_title": "improving faithfulness in abstractive summarization with contrast candidate generation and selection",
    "citation_count": 81,
    "authors": [
      "Sihao Chen",
      "Fan Zhang",
      "Kazoo Sone",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.476": {
    "title": "Inference Time Style Control for Summarization",
    "volume": "main",
    "abstract": "How to generate summaries of different styles without requiring corpora in the target styles, or training separate models? We present two novel methods that can be deployed during summary decoding on any pre-trained Transformer-based summarization model. (1) Decoder state adjustment instantly modifies decoder final states with externally trained style scorers, to iteratively refine the output against a target style. (2) Word unit prediction constrains the word usage to impose strong lexical control during generation. In experiments of summarizing with simplicity control, automatic evaluation and human judges both find our models producing outputs in simpler languages while still informative. We also generate news headlines with various ideological leanings, which can be distinguished by humans with a reasonable probability",
    "checked": true,
    "id": "40b8ac8cf4df9fcc4a678a71809530f2b54610d8",
    "semantic_title": "inference time style control for summarization",
    "citation_count": 12,
    "authors": [
      "Shuyang Cao",
      "Lu Wang"
    ]
  },
  "https://aclanthology.org/2021.naacl-main.477": {
    "title": "ReinforceBug: A Framework to Generate Adversarial Textual Examples",
    "volume": "main",
    "abstract": "Adversarial Examples (AEs) generated by perturbingining examples are useful in improving the robustness of Deep Learning (DL) based models. Most prior works generate AEs that are either unconscionable due to lexical errors or semantically and functionally deviant from original examples. In this paper, we present ReinforceBug, a reinforcement learning framework, that learns a policy that is transferable on unseen datasets and generates utility-preserving and transferable (on other models) AEs. Our experiments show that ReinforceBug is on average 10% more successful as compared to the state-of the-art attack TextFooler. Moreover, the target models have on average 73.64% confidence in wrong prediction, the generated AEs preserve the functional equivalence and semantic similarity (83.38%) to their original counterparts, and are transferable on other models with an average success rate of 46%",
    "checked": true,
    "id": "39f8cac9c1810d6eb3c3990fc51c8917e3495312",
    "semantic_title": "reinforcebug: a framework to generate adversarial textual examples",
    "citation_count": 2,
    "authors": [
      "Bushra Sabir",
      "Muhammad Ali Babar",
      "Raj Gaire"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.1": {
    "title": "When does text prediction benefit from additional context? An exploration of contextual signals for chat and email messages",
    "volume": "industry",
    "abstract": "Email and chat communication tools are increasingly important for completing daily tasks. Accurate real-time phrase completion can save time and bolster productivity. Modern text prediction algorithms are based on large language models which typically rely on the prior words in a message to predict a completion. We examine how additional contextual signals (from previous messages, time, and subject) affect the performance of a commercial text prediction model. We compare contextual text prediction in chat and email messages from two of the largest commercial platforms Microsoft Teams and Outlook, finding that contextual signals contribute to performance differently between these scenarios. On emails, time context is most beneficial with small relative gains of 2% over baseline. Whereas, in chat scenarios, using a tailored set of previous messages as context yields relative improvements over the baseline between 9.3% and 18.6% across various critical service-oriented text prediction metrics",
    "checked": true,
    "id": "426989945b8feb5a3882371104af0b5c737bd4f9",
    "semantic_title": "when does text prediction benefit from additional context? an exploration of contextual signals for chat and email messages",
    "citation_count": 6,
    "authors": [
      "Stojan Trajanovski",
      "Chad Atalla",
      "Kunho Kim",
      "Vipul Agarwal",
      "Milad Shokouhi",
      "Chris Quirk"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.2": {
    "title": "Identifying and Resolving Annotation Changes for Natural Language Understanding",
    "volume": "industry",
    "abstract": "Annotation conflict resolution is crucial towards building machine learning models with acceptable performance. Past work on annotation conflict resolution had assumed that data is collected at once, with a fixed set of annotators and fixed annotation guidelines. Moreover, previous work dealt with atomic labeling tasks. In this paper, we address annotation conflict resolution for Natural Language Understanding (NLU), a structured prediction task, in a real-world setting of commercial voice-controlled personal assistants, where (1) regular data collections are needed to support new and existing functionalities, (2) annotation guidelines evolve over time, and (3) the pool of annotators change across data collections. We devise an approach combining information-theoretic measures and a supervised neural model to resolve conflicts in data annotation. We evaluate our approach both intrinsically and extrinsically on a real-world dataset with 3.5M utterances of a commercial dialog system in German. Our approach leads to dramatic improvements over a majority baseline especially in contentious cases. On the NLU task, our approach achieves 2.75% error reduction over a no-resolution baseline",
    "checked": true,
    "id": "1e82dcaa8c695d6dee4a5f8d945294dd64055a5b",
    "semantic_title": "identifying and resolving annotation changes for natural language understanding",
    "citation_count": 1,
    "authors": [
      "Jose Garrido Ramas",
      "Giorgio Pessot",
      "Abdalghani Abujabal",
      "Martin Rajman"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.3": {
    "title": "Optimizing NLU Reranking Using Entity Resolution Signals in Multi-domain Dialog Systems",
    "volume": "industry",
    "abstract": "In dialog systems, the Natural Language Understanding (NLU) component typically makes the interpretation decision (including domain, intent and slots) for an utterance before the mentioned entities are resolved. This may result in intent classification and slot tagging errors. In this work, we propose to leverage Entity Resolution (ER) features in NLU reranking and introduce a novel loss term based on ER signals to better learn model weights in the reranking framework. In addition, for a multi-domain dialog scenario, we propose a score distribution matching method to ensure scores generated by the NLU reranking models for different domains are properly calibrated. In offline experiments, we demonstrate our proposed approach significantly outperforms the baseline model on both single-domain and cross-domain evaluations",
    "checked": true,
    "id": "32e501a0cd9a4ebcaa5989657690be38b8340340",
    "semantic_title": "optimizing nlu reranking using entity resolution signals in multi-domain dialog systems",
    "citation_count": 2,
    "authors": [
      "Tong Wang",
      "Jiangning Chen",
      "Mohsen Malmir",
      "Shuyan Dong",
      "Xin He",
      "Han Wang",
      "Chengwei Su",
      "Yue Liu",
      "Yang Liu"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.4": {
    "title": "Entity Resolution in Open-domain Conversations",
    "volume": "industry",
    "abstract": "In recent years, incorporating external knowledge for response generation in open-domain conversation systems has attracted great interest. To improve the relevancy of retrieved knowledge, we propose a neural entity linking (NEL) approach. Different from formal documents, such as news, conversational utterances are informal and multi-turn, which makes it more challenging to disambiguate the entities. Therefore, we present a context-aware named entity recognition model (NER) and entity resolution (ER) model to utilize dialogue context information. We conduct NEL experiments on three open-domain conversation datasets and validate that incorporating context information improves the performance of NER and ER models. The end-to-end NEL approach outperforms the baseline by 62.8% relatively in F1 metric. Furthermore, we verify that using external knowledge based on NEL benefits the neural response generation model",
    "checked": true,
    "id": "dfa530fff84bcd21517b8d98ac002ffc7046b036",
    "semantic_title": "entity resolution in open-domain conversations",
    "citation_count": 9,
    "authors": [
      "Mingyue Shang",
      "Tong Wang",
      "Mihail Eric",
      "Jiangning Chen",
      "Jiyang Wang",
      "Matthew Welch",
      "Tiantong Deng",
      "Akshay Grewal",
      "Han Wang",
      "Yue Liu",
      "Yang Liu",
      "Dilek Hakkani-Tur"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.5": {
    "title": "Pretrain-Finetune Based Training of Task-Oriented Dialogue Systems in a Real-World Setting",
    "volume": "industry",
    "abstract": "One main challenge in building task-oriented dialogue systems is the limited amount of supervised training data available. In this work, we present a method for training retrieval-based dialogue systems using a small amount of high-quality, annotated data and a larger, unlabeled dataset. We show that pretraining using unlabeled data can bring better model performance with a 31% boost in Recall@1 compared with no pretraining. The proposed finetuning technique based on a small amount of high-quality, annotated data resulted in 26% offline and 33% online performance improvement in Recall@1 over the pretrained model. The model is deployed in an agent-support application and evaluated on live customer service contacts, providing additional insights into the real-world implications compared with most other publications in the domain often using asynchronous transcripts (e.g. Reddit data). The high performance of 74% Recall@1 shown in the customer service example demonstrates the effectiveness of this pretrain-finetune approach in dealing with the limited supervised data challenge",
    "checked": true,
    "id": "fbb67bb3e8d5d51fbc688187f5c5748af882ac22",
    "semantic_title": "pretrain-finetune based training of task-oriented dialogue systems in a real-world setting",
    "citation_count": 2,
    "authors": [
      "Manisha Srivastava",
      "Yichao Lu",
      "Riley Peschon",
      "Chenyang Li"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.6": {
    "title": "Contextual Domain Classification with Temporal Representations",
    "volume": "industry",
    "abstract": "In commercial dialogue systems, the Spoken Language Understanding (SLU) component tends to have numerous domains thus context is needed to help resolve ambiguities. Previous works that incorporate context for SLU have mostly focused on domains where context is limited to a few minutes. However, there are domains that have related context that could span up to hours and days. In this paper, we propose temporal representations that combine wall-clock second difference and turn order offset information to utilize both recent and distant context in a novel large-scale setup. Experiments on the Contextual Domain Classification (CDC) task with various encoder architectures show that temporal representations combining both information outperforms only one of the two. We further demonstrate that our contextual Transformer is able to reduce 13.04% of classification errors compared to a non-contextual baseline. We also conduct empirical analyses to study recent versus distant context and opportunities to lower deployment costs",
    "checked": true,
    "id": "d27c88170b9e692c8ed26c68d15859bc35d764f6",
    "semantic_title": "contextual domain classification with temporal representations",
    "citation_count": 2,
    "authors": [
      "Tzu-Hsiang Lin",
      "Yipeng Shi",
      "Chentao Ye",
      "Yang Fan",
      "Weitong Ruan",
      "Emre Barut",
      "Wael Hamza",
      "Chengwei Su"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.7": {
    "title": "Bootstrapping a Music Voice Assistant with Weak Supervision",
    "volume": "industry",
    "abstract": "One of the first building blocks to create a voice assistant relates to the task of tagging entities or attributes in user queries. This can be particularly challenging when entities are in the tenth of millions, as is the case of e.g. music catalogs. Training slot tagging models at an industrial scale requires large quantities of accurately labeled user queries, which are often hard and costly to gather. On the other hand, voice assistants typically collect plenty of unlabeled queries that often remain unexploited. This paper presents a weakly-supervised methodology to label large amounts of voice query logs, enhanced with a manual filtering step. Our experimental evaluations show that slot tagging models trained on weakly-supervised data outperform models trained on hand-annotated or synthetic data, at a lower cost. Further, manual filtering of weakly-supervised data leads to a very significant reduction in Sentence Error Rate, while allowing us to drastically reduce human curation efforts from weeks to hours, with respect to hand-annotation of queries. The method is applied to successfully bootstrap a slot tagging system for a major music streaming service that currently serves several tens of thousands of daily voice queries",
    "checked": true,
    "id": "3a5f35b9d73ad3afa5208eea0920b9759dd08730",
    "semantic_title": "bootstrapping a music voice assistant with weak supervision",
    "citation_count": 1,
    "authors": [
      "Sergio Oramas",
      "Massimo Quadrana",
      "Fabien Gouyon"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.8": {
    "title": "Continuous Model Improvement for Language Understanding with Machine Translation",
    "volume": "industry",
    "abstract": "Scaling conversational personal assistants to a multitude of languages puts high demands on collecting and labelling data, a setting in which cross-lingual learning techniques can help to reconcile the need for well-performing Natural Language Understanding (NLU) with a desideratum to support many languages without incurring unacceptable cost. In this work, we show that automatically annotating unlabeled utterances using Machine Translation in an offline fashion and adding them to the training data can improve performance for existing NLU features for low-resource languages, where a straightforward translate-test approach as considered in existing literature would fail the latency requirements of a live environment. We demonstrate the effectiveness of our method with intrinsic and extrinsic evaluation using a real-world commercial dialog system in German. Beyond an intrinsic evaluation, where 56% of the resulting automatically labeled utterances had a perfect match with ground-truth labels, we see significant performance improvements in an extrinsic evaluation settings when manual labeled data is available in small quantities",
    "checked": true,
    "id": "b7d975a9fa2fdda54f16adc7a00315792de5bb85",
    "semantic_title": "continuous model improvement for language understanding with machine translation",
    "citation_count": 2,
    "authors": [
      "Abdalghani Abujabal",
      "Claudio Delli Bovi",
      "Sungho Ryu",
      "Turan Gojayev",
      "Fabian Triefenbach",
      "Yannick Versley"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.9": {
    "title": "A Hybrid Approach to Scalable and Robust Spoken Language Understanding in Enterprise Virtual Agents",
    "volume": "industry",
    "abstract": "Spoken language understanding (SLU) extracts the intended mean- ing from a user utterance and is a critical component of conversational virtual agents. In enterprise virtual agents (EVAs), language understanding is substantially challenging. First, the users are infrequent callers who are unfamiliar with the expectations of a pre-designed conversation flow. Second, the users are paying customers of an enterprise who demand a reliable, consistent and efficient user experience when resolving their issues. In this work, we describe a general and robust framework for intent and entity extraction utilizing a hybrid of statistical and rule-based approaches. Our framework includes confidence modeling that incorporates information from all components in the SLU pipeline, a critical addition for EVAs to en- sure accuracy. Our focus is on creating accurate and scalable SLU that can be deployed rapidly for a large class of EVA applications with little need for human intervention",
    "checked": true,
    "id": "3e24307227e73ccf2f8fb369ed56f2390c147cf0",
    "semantic_title": "a hybrid approach to scalable and robust spoken language understanding in enterprise virtual agents",
    "citation_count": 3,
    "authors": [
      "Ryan Price",
      "Mahnoosh Mehrabani",
      "Narendra Gupta",
      "Yeon-Jun Kim",
      "Shahab Jalalvand",
      "Minhua Chen",
      "Yanjie Zhao",
      "Srinivas Bangalore"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.10": {
    "title": "Proteno: Text Normalization with Limited Data for Fast Deployment in Text to Speech Systems",
    "volume": "industry",
    "abstract": "Developing Text Normalization (TN) systems for Text-to-Speech (TTS) on new languages is hard. We propose a novel architecture to facilitate it for multiple languages while using data less than 3% of the size of the data used by the state of the art results on English. We treat TN as a sequence classification problem and propose a granular tokenization mechanism that enables the system to learn majority of the classes and their normalizations from the training data itself. This is further combined with minimal precoded linguistic knowledge for other classes. We publish the first results on TN for TTS in Spanish and Tamil and also demonstrate that the performance of the approach is comparable with the previous work done on English. All annotated datasets used for experimentation will be released",
    "checked": true,
    "id": "91ddac716722af8e17efc691600590919bb8b563",
    "semantic_title": "proteno: text normalization with limited data for fast deployment in text to speech systems",
    "citation_count": 10,
    "authors": [
      "Shubhi Tyagi",
      "Antonio Bonafonte",
      "Jaime Lorenzo-Trueba",
      "Javier Latorre"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.11": {
    "title": "Addressing the Vulnerability of NMT in Input Perturbations",
    "volume": "industry",
    "abstract": "Neural Machine Translation (NMT) has achieved significant breakthrough in performance but is known to suffer vulnerability to input perturbations. As real input noise is difficult to predict during training, robustness is a big issue for system deployment. In this paper, we improve the robustness of NMT models by reducing the effect of noisy words through a Context-Enhanced Reconstruction (CER) approach. CER trains the model to resist noise in two steps: (1) perturbation step that breaks the naturalness of input sequence with made-up words; (2) reconstruction step that defends the noise propagation by generating better and more robust contextual representation. Experimental results on Chinese-English (ZH-EN) and French-English (FR-EN) translation tasks demonstrate robustness improvement on both news and social media text. Further fine-tuning experiments on social media text show our approach can converge at a higher position and provide a better adaptation",
    "checked": true,
    "id": "f12494f036f8d9204e23f46fe3053f67a940be7e",
    "semantic_title": "addressing the vulnerability of nmt in input perturbations",
    "citation_count": 9,
    "authors": [
      "Weiwen Xu",
      "Ai Ti Aw",
      "Yang Ding",
      "Kui Wu",
      "Shafiq Joty"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.12": {
    "title": "Cross-lingual Supervision Improves Unsupervised Neural Machine Translation",
    "volume": "industry",
    "abstract": "We propose to improve unsupervised neural machine translation with cross-lingual supervision (), which utilizes supervision signals from high resource language pairs to improve the translation of zero-source languages. Specifically, for training En-Ro system without parallel corpus, we can leverage the corpus from En-Fr and En-De to collectively train the translation from one language into many languages under one model. % is based on multilingual models which require no changes to the standard unsupervised NMT. Simple and effective, significantly improves the translation quality with a big margin in the benchmark unsupervised translation tasks, and even achieves comparable performance to supervised NMT. In particular, on WMT'14 -tasks achieves 37.6 and 35.18 BLEU score, which is very close to the large scale supervised setting and on WMT'16 -tasks achieves 35.09 BLEU score which is even better than the supervised Transformer baseline",
    "checked": true,
    "id": "0a416efba20dc8d1fdc99f8bd79f9f32001acaf9",
    "semantic_title": "cross-lingual supervision improves unsupervised neural machine translation",
    "citation_count": 7,
    "authors": [
      "Mingxuan Wang",
      "Hongxiao Bai",
      "Hai Zhao",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.13": {
    "title": "Should we find another model?: Improving Neural Machine Translation Performance with ONE-Piece Tokenization Method without Model Modification",
    "volume": "industry",
    "abstract": "Most of the recent Natural Language Processing(NLP) studies are based on the Pretrain-Finetuning Approach (PFA), but in small and medium-sized enterprises or companies with insufficient hardware there are many limitations to servicing NLP application software using such technology due to slow speed and insufficient memory. The latest PFA technologies require large amounts of data, especially for low-resource languages, making them much more difficult to work with. We propose a new tokenization method, ONE-Piece, to address this limitation that combines the morphology-considered subword tokenization method and the vocabulary method used after probing for an existing method that has not been carefully considered before. Our proposed method can also be used without modifying the model structure. We experiment by applying ONE-Piece to Korean, a morphologically-rich and low-resource language. We derive an optimal subword tokenization result for Korean-English machine translation by conducting a case study that combines the subword tokenization method, morphological segmentation, and vocabulary method. Through comparative experiments with all the tokenization methods currently used in NLP research, ONE-Piece achieves performance comparable to the current Korean-English machine translation state-of-the-art model",
    "checked": true,
    "id": "3b6bab26e32a297b15999160c5cf5727e2635210",
    "semantic_title": "should we find another model?: improving neural machine translation performance with one-piece tokenization method without model modification",
    "citation_count": 25,
    "authors": [
      "Chanjun Park",
      "Sugyeong Eo",
      "Hyeonseok Moon",
      "Heuiseok Lim"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.14": {
    "title": "Autocorrect in the Process of Translation — Multi-task Learning Improves Dialogue Machine Translation",
    "volume": "industry",
    "abstract": "Automatic translation of dialogue texts is a much needed demand in many real life scenarios. However, the currently existing neural machine translation delivers unsatisfying results. In this paper, we conduct a deep analysis of a dialogue corpus and summarize three major issues on dialogue translation, including pronoun dropping (), punctuation dropping (), and typos (). In response to these challenges, we propose a joint learning method to identify omission and typo, and utilize context to translate dialogue utterances. To properly evaluate the performance, we propose a manually annotated dataset with 1,931 Chinese-English parallel utterances from 300 dialogues as a benchmark testbed for dialogue translation. Our experiments show that the proposed method improves translation quality by 3.2 BLEU over the baselines. It also elevates the recovery rate of omitted pronouns from 26.09% to 47.16%. We will publish the code and dataset publicly at https://xxx.xx",
    "checked": true,
    "id": "5cd746622c2e3bd9344090dc675ad0852e7af534",
    "semantic_title": "autocorrect in the process of translation — multi-task learning improves dialogue machine translation",
    "citation_count": 11,
    "authors": [
      "Tao Wang",
      "Chengqi Zhao",
      "Mingxuan Wang",
      "Lei Li",
      "Deyi Xiong"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.15": {
    "title": "LightSeq: A High Performance Inference Library for Transformers",
    "volume": "industry",
    "abstract": "Transformer and its variants have achieved great success in natural language processing. Since Transformer models are huge in size, serving these models is a challenge for real industrial applications. In this paper, we propose , a highly efficient inference library for models in the Transformer family. includes a series of GPU optimization techniques to both streamline the computation of Transformer layers and reduce memory footprint. supports models trained using PyTorch and Tensorflow. Experimental results on standard machine translation benchmarks show that achieves up to 14x speedup compared with TensorFlow and 1.4x speedup compared with , a concurrent CUDA implementation. The code will be released publicly after the review",
    "checked": true,
    "id": "1554887c6bd76c443a477b27dbcab35877787b27",
    "semantic_title": "lightseq: a high performance inference library for transformers",
    "citation_count": 43,
    "authors": [
      "Xiaohui Wang",
      "Ying Xiong",
      "Yang Wei",
      "Mingxuan Wang",
      "Lei Li"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.16": {
    "title": "Practical Transformer-based Multilingual Text Classification",
    "volume": "industry",
    "abstract": "Transformer-based methods are appealing for multilingual text classification, but common research benchmarks like XNLI (Conneau et al., 2018) do not reflect the data availability and task variety of industry applications. We present an empirical comparison of transformer-based text classification models in a variety of practical monolingual and multilingual pretraining and fine-tuning settings. We evaluate these methods on two distinct tasks in five different languages. Departing from prior work, our results show that multilingual language models can outperform monolingual ones in some downstream tasks and target languages. We additionally show that practical modifications such as task- and domain-adaptive pretraining and data augmentation can improve classification performance without the need for additional labeled data",
    "checked": true,
    "id": "91ea7176cc41f7b97867d483075df7886aa3dc33",
    "semantic_title": "practical transformer-based multilingual text classification",
    "citation_count": 19,
    "authors": [
      "Cindy Wang",
      "Michele Banko"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.17": {
    "title": "An Emotional Comfort Framework for Improving User Satisfaction in E-Commerce Customer Service Chatbots",
    "volume": "industry",
    "abstract": "E-commerce has grown substantially over the last several years, and chatbots for intelligent customer service are concurrently drawing attention. We presented AliMe Assist, a Chinese intelligent assistant designed for creating an innovative online shopping experience in E-commerce. Based on question answering (QA), AliMe Assist offers assistance service, customer service, and chatting service. According to the survey of user studies and the real online testing, emotional comfort of customers' negative emotions, which make up more than 5% of whole number of customer visits on AliMe, is a key point for providing considerate service. In this paper, we propose a framework to obtain proper answer to customers' emotional questions. The framework takes emotion classification model as a core, and final answer selection is based on topic classification and text matching. Our experiments on real online systems show that the framework is very promising",
    "checked": true,
    "id": "2fbd94ed96f4458a41b8c499a5b314cc813a96e1",
    "semantic_title": "an emotional comfort framework for improving user satisfaction in e-commerce customer service chatbots",
    "citation_count": 4,
    "authors": [
      "Shuangyong Song",
      "Chao Wang",
      "Haiqing Chen",
      "Huan Chen"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.18": {
    "title": "Language Scaling for Universal Suggested Replies Model",
    "volume": "industry",
    "abstract": "We consider the problem of scaling automated suggested replies for a commercial email application to multiple languages. Faced with increased compute requirements and low language resources for language expansion, we build a single universal model for improving the quality and reducing run-time costs of our production system. However, restricted data movement across regional centers prevents joint training across languages. To this end, we propose a multi-lingual multi-task continual learning framework, with auxiliary tasks and language adapters to train universal language representation across regions. The experimental results show positive cross-lingual transfer across languages while reducing catastrophic forgetting across regions. Our online results on real user traffic show significant CTR and Char-saved gain as well as 65% training cost reduction compared with per-language models. As a consequence, we have scaled the feature in multiple languages including low-resource markets",
    "checked": true,
    "id": "20e7f3b52be431724efa953d1d65a7dbd171a817",
    "semantic_title": "language scaling for universal suggested replies model",
    "citation_count": 2,
    "authors": [
      "Qianlan Ying",
      "Payal Bajaj",
      "Budhaditya Deb",
      "Yu Yang",
      "Wei Wang",
      "Bojia Lin",
      "Milad Shokouhi",
      "Xia Song",
      "Yang Yang",
      "Daxin Jiang"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.19": {
    "title": "Graph-based Multilingual Product Retrieval in E-Commerce Search",
    "volume": "industry",
    "abstract": "Nowadays, with many e-commerce platforms conducting global business, e-commerce search systems are required to handle product retrieval under multilingual scenarios. Moreover, comparing with maintaining per-country specific e-commerce search systems, having an universal system across countries can further reduce the operational and computational costs, and facilitate business expansion to new countries. In this paper, we introduce an universal end-to-end multilingual retrieval system, and discuss our learnings and technical details when training and deploying the system to serve billion-scale product retrieval for e-commerce search. In particular, we propose a multilingual graph attention based retrieval network by leveraging recent advances in transformer-based multilingual language models and graph neural network architectures to capture the interactions between search queries and items in e-commerce search. Offline experiments on five countries data show that our algorithm outperforms the state-of-the-art baselines by 35% recall and 25% mAP on average. Moreover, the proposed model shows significant increase of conversion/revenue in online A/B experiments and has been deployed in production for multiple countries",
    "checked": true,
    "id": "64ef5ac25aee30e36ea90f8a01d234d20ec48781",
    "semantic_title": "graph-based multilingual product retrieval in e-commerce search",
    "citation_count": 19,
    "authors": [
      "Hanqing Lu",
      "Youna Hu",
      "Tong Zhao",
      "Tony Wu",
      "Yiwei Song",
      "Bing Yin"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.20": {
    "title": "Query2Prod2Vec: Grounded Word Embeddings for eCommerce",
    "volume": "industry",
    "abstract": "We present Query2Prod2Vec, a model that grounds lexical representations for product search in product embeddings: in our model, meaning is a mapping between words and a latent space of products in a digital shop. We leverage shopping sessions to learn the underlying space and use merchandising annotations to build lexical analogies for evaluation: our experiments show that our model is more accurate than known techniques from the NLP and IR literature. Finally, we stress the importance of data efficiency for product search outside of retail giants, and highlight how Query2Prod2Vec fits with practical constraints faced by most practitioners",
    "checked": true,
    "id": "c327018533712833b3f86e5ad70288744a00c5ac",
    "semantic_title": "query2prod2vec grounded word embeddings for ecommerce",
    "citation_count": 18,
    "authors": [
      "Federico Bianchi",
      "Jacopo Tagliabue",
      "Bingqing Yu"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.21": {
    "title": "An Architecture for Accelerated Large-Scale Inference of Transformer-Based Language Models",
    "volume": "industry",
    "abstract": "This work demonstrates the development process of a machine learning architecture for inference that can scale to a large volume of requests. We used a BERT model that was fine-tuned for emotion analysis, returning a probability distribution of emotions given a paragraph. The model was deployed as a gRPC service on Kubernetes. Apache Spark was used to perform inference in batches by calling the service. We encountered some performance and concurrency challenges and created solutions to achieve faster running time. Starting with 200 successful inference requests per minute, we were able to achieve as high as 18 thousand successful requests per minute with the same batch job resource allocation. As a result, we successfully stored emotion probabilities for 95 million paragraphs within 96 hours",
    "checked": true,
    "id": "7cf52f5dbb45025b70301574089465e6d1a14eae",
    "semantic_title": "an architecture for accelerated large-scale inference of transformer-based language models",
    "citation_count": 0,
    "authors": [
      "Amir Ganiev",
      "Colton Chapin",
      "Anderson De Andrade",
      "Chen Liu"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.22": {
    "title": "When and Why a Model Fails? A Human-in-the-loop Error Detection Framework for Sentiment Analysis",
    "volume": "industry",
    "abstract": "Although deep neural networks have been widely employed and proven effective in sentiment analysis tasks, it remains challenging for model developers to assess their models for erroneous predictions that might exist prior to deployment. Once deployed, emergent errors can be hard to identify in prediction run-time and impossible to trace back to their sources. To address such gaps, in this paper we propose an error detection framework for sentiment analysis based on explainable features. We perform global-level feature validation with human-in-the-loop assessment, followed by an integration of global and local-level feature contribution analysis. Experimental results show that, given limited human-in-the-loop intervention, our method is able to identify erroneous model predictions on unseen data with high precision",
    "checked": true,
    "id": "a9cf4bd80bf7b529c0a4182de1b35e430bcd081d",
    "semantic_title": "when and why a model fails? a human-in-the-loop error detection framework for sentiment analysis",
    "citation_count": 6,
    "authors": [
      "Zhe Liu",
      "Yufan Guo",
      "Jalal Mahmud"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.23": {
    "title": "Technical Question Answering across Tasks and Domains",
    "volume": "industry",
    "abstract": "Building automatic technical support system is an important yet challenge task. Conceptually, to answer a user question on a technical forum, a human expert has to first retrieve relevant documents, and then read them carefully to identify the answer snippet. Despite huge success the researchers have achieved in coping with general domain question answering (QA), much less attentions have been paid for investigating technical QA. Specifically, existing methods suffer from several unique challenges (i) the question and answer rarely overlaps substantially and (ii) very limited data size. In this paper, we propose a novel framework of deep transfer learning to effectively address technical QA across tasks and domains. To this end, we present an adjustable joint learning approach for document retrieval and reading comprehension tasks. Our experiments on the TechQA demonstrates superior performance compared with state-of-the-art methods",
    "checked": true,
    "id": "4a86aa392358bafc5fe52f8932a1a78d9d0cadc0",
    "semantic_title": "technical question answering across tasks and domains",
    "citation_count": 6,
    "authors": [
      "Wenhao Yu",
      "Lingfei Wu",
      "Yu Deng",
      "Qingkai Zeng",
      "Ruchi Mahindru",
      "Sinem Guven",
      "Meng Jiang"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.24": {
    "title": "Cost-effective Deployment of BERT Models in Serverless Environment",
    "volume": "industry",
    "abstract": "In this study, we demonstrate the viability of deploying BERT-style models to AWS Lambda in a production environment. Since the freely available pre-trained models are too large to be deployed in this environment, we utilize knowledge distillation and fine-tune the models on proprietary datasets for two real-world tasks: sentiment analysis and semantic textual similarity. As a result, we obtain models that are tuned for a specific domain and deployable in the serverless environment. The subsequent performance analysis shows that this solution does not only report latency levels acceptable for production use but that it is also a cost-effective alternative to small-to-medium size deployments of BERT models, all without any infrastructure overhead",
    "checked": true,
    "id": "2101193a3ec4d8fe260c7505614e760a7235ecf5",
    "semantic_title": "cost-effective deployment of bert models in serverless environment",
    "citation_count": 2,
    "authors": [
      "Marek Suppa",
      "Katarína Benešová",
      "Andrej Švec"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.25": {
    "title": "Noise Robust Named Entity Understanding for Voice Assistants",
    "volume": "industry",
    "abstract": "Named Entity Recognition (NER) and Entity Linking (EL) play an essential role in voice assistant interaction, but are challenging due to the special difficulties associated with spoken user queries. In this paper, we propose a novel architecture that jointly solves the NER and EL tasks by combining them in a joint reranking module. We show that our proposed framework improves NER accuracy by up to 3.13% and EL accuracy by up to 3.6% in F1 score. The features used also lead to better accuracies in other natural language understanding tasks, such as domain classification and semantic parsing",
    "checked": true,
    "id": "baae0355fb31103e5bf9d9b562bacfc05463a88e",
    "semantic_title": "noise robust named entity understanding for voice assistants",
    "citation_count": 7,
    "authors": [
      "Deepak Muralidharan",
      "Joel Ruben Antony Moniz",
      "Sida Gao",
      "Xiao Yang",
      "Justine Kao",
      "Stephen Pulman",
      "Atish Kothari",
      "Ray Shen",
      "Yinying Pan",
      "Vivek Kaul",
      "Mubarak Seyed Ibrahim",
      "Gang Xiang",
      "Nan Dun",
      "Yidan Zhou",
      "Andy O",
      "Yuan Zhang",
      "Pooja Chitkara",
      "Xuan Wang",
      "Alkesh Patel",
      "Kushal Tayal",
      "Roger Zheng",
      "Peter Grasch",
      "Jason D Williams",
      "Lin Li"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.26": {
    "title": "Goodwill Hunting: Analyzing and Repurposing Off-the-Shelf Named Entity Linking Systems",
    "volume": "industry",
    "abstract": "Named entity linking (NEL) or mapping \"strings\" to \"things\" in a knowledge base is a fundamental preprocessing step in systems that require knowledge of entities such as information extraction and question answering. In this work, we lay out and investigate two challenges faced by individuals or organizations building NEL systems. Can they directly use an off-the-shelf system? If not, how easily can such a system be repurposed for their use case? First, we conduct a study of off-the-shelf commercial and academic NEL systems. We find that most systems struggle to link rare entities, with commercial solutions lagging their academic counterparts by 10%+. Second, for a use case where the NEL model is used in a sports question-answering (QA) system, we investigate how to close the loop in our analysis by repurposing the best off-the-shelf model (Bootleg) to correct sport-related errors. We show how tailoring a simple technique for patching models using weak labeling can provide a 25% absolute improvement in accuracy of sport-related errors",
    "checked": true,
    "id": "a2df8df0a0c46ed290dbcb683496ed018371c33b",
    "semantic_title": "goodwill hunting: analyzing and repurposing off-the-shelf named entity linking systems",
    "citation_count": 5,
    "authors": [
      "Karan Goel",
      "Laurel Orr",
      "Nazneen Fatema Rajani",
      "Jesse Vig",
      "Christopher Ré"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.27": {
    "title": "Intent Features for Rich Natural Language Understanding",
    "volume": "industry",
    "abstract": "Complex natural language understanding modules in dialog systems have a richer understanding of user utterances, and thus are critical in providing a better user experience. However, these models are often created from scratch, for specific clients and use cases and require the annotation of large datasets. This encourages the sharing of annotated data across multiple clients. To facilitate this we introduce the idea of intent features: domain and topic agnostic properties of intents that can be learnt from the syntactic cues only, and hence can be shared. We introduce a new neural network architecture, the Global-Local model, that shows significant improvement over strong baselines for identifying these features in a deployed, multi-intent natural language understanding module, and more generally in a classification setting where a part of an utterance has to be classified utilizing the whole context",
    "checked": true,
    "id": "7f6f7e56da14711b2645007492aece0af38ddd06",
    "semantic_title": "intent features for rich natural language understanding",
    "citation_count": 0,
    "authors": [
      "Brian Lester",
      "Sagnik Ray Choudhury",
      "Rashmi Prasad",
      "Srinivas Bangalore"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.28": {
    "title": "Development of an Enterprise-Grade Contract Understanding System",
    "volume": "industry",
    "abstract": "Contracts are arguably the most important type of business documents. Despite their significance in business, legal contract review largely remains an arduous, expensive and manual process. In this paper, we describe TECUS: a commercial system designed and deployed for contract understanding and used by a wide range of enterprise users for the past few years. We reflect on the challenges and design decisions when building TECUS. We also summarize the data science life cycle of TECUS and share lessons learned",
    "checked": true,
    "id": "0ce581e6ce34c31d342f56d1614e01d0a6dd3d4b",
    "semantic_title": "development of an enterprise-grade contract understanding system",
    "citation_count": 3,
    "authors": [
      "Arvind Agarwal",
      "Laura Chiticariu",
      "Poornima Chozhiyath Raman",
      "Marina Danilevsky",
      "Diman Ghazi",
      "Ankush Gupta",
      "Shanmukha Guttula",
      "Yannis Katsis",
      "Rajasekar Krishnamurthy",
      "Yunyao Li",
      "Shubham Mudgal",
      "Vitobha Munigala",
      "Nicholas Phan",
      "Dhaval Sonawane",
      "Sneha Srinivasan",
      "Sudarshan R. Thitte",
      "Mitesh Vasa",
      "Ramiya Venkatachalam",
      "Vinitha Yaski",
      "Huaiyu Zhu"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.29": {
    "title": "Discovering Better Model Architectures for Medical Query Understanding",
    "volume": "industry",
    "abstract": "In developing an online question-answering system for the medical domains, natural language inference (NLI) models play a central role in question matching and intention detection. However, which models are best for our datasets? Manually selecting or tuning a model is time-consuming. Thus we experiment with automatically optimizing the model architectures on the task at hand via neural architecture search (NAS). First, we formulate a novel architecture search space based on the previous NAS literature, supporting cross-sentence attention (cross-attn) modeling. Second, we propose to modify the ENAS method to accelerate and stabilize the search results. We conduct extensive experiments on our two medical NLI tasks. Results show that our system can easily outperform the classical baseline models. We compare different NAS methods and demonstrate our approach provides the best results",
    "checked": true,
    "id": "50920f95cee87913dea19656e83ae06b51f9ba4a",
    "semantic_title": "discovering better model architectures for medical query understanding",
    "citation_count": 12,
    "authors": [
      "Wei Zhu",
      "Yuan Ni",
      "Xiaoling Wang",
      "Guotong Xie"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.30": {
    "title": "OodGAN: Generative Adversarial Network for Out-of-Domain Data Generation",
    "volume": "industry",
    "abstract": "Detecting an Out-of-Domain (OOD) utterance is crucial for a robust dialog system. Most dialog systems are trained on a pool of annotated OOD data to achieve this goal. However, collecting the annotated OOD data for a given domain is an expensive process. To mitigate this issue, previous works have proposed generative adversarial networks (GAN) based models to generate OOD data for a given domain automatically. However, these proposed models do not work directly with the text. They work with the text's latent space instead, enforcing these models to include components responsible for encoding text into latent space and decoding it back, such as auto-encoder. These components increase the model complexity, making it difficult to train. We propose OodGAN, a sequential generative adversarial network (SeqGAN) based model for OOD data generation. Our proposed model works directly on the text and hence eliminates the need to include an auto-encoder. OOD data generated using OodGAN model outperforms state-of-the-art in OOD detection metrics for ROSTD (67% relative improvement in FPR 0.95) and OSQ datasets (28% relative improvement in FPR 0.95)",
    "checked": true,
    "id": "043bfc87afb16d80808dded02b22270fc4665c07",
    "semantic_title": "oodgan: generative adversarial network for out-of-domain data generation",
    "citation_count": 26,
    "authors": [
      "Petr Marek",
      "Vishal Ishwar Naik",
      "Anuj Goyal",
      "Vincent Auvray"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.31": {
    "title": "Coherent and Concise Radiology Report Generation via Context Specific Image Representations and Orthogonal Sentence States",
    "volume": "industry",
    "abstract": "Neural models for text generation are often designed in an end-to-end fashion, typically with zero control over intermediate computations, limiting their practical usability in downstream applications. In this work, we incorporate explicit means into neural models to ensure topical continuity, informativeness and content diversity of generated radiology reports. For the purpose we propose a method to compute image representations specific to each sentential context and eliminate redundant content by exploiting diverse sentence states. We conduct experiments to generate radiology reports from medical images of chest x-rays using MIMIC-CXR. Our model outperforms baselines by up to 18% and 29% respective in the evaluation for informativeness and content ordering respectively, relative on objective metrics and 16% on human evaluation",
    "checked": true,
    "id": "f2c8e7e553a5548d96525405297c08f0b2c8d36a",
    "semantic_title": "coherent and concise radiology report generation via context specific image representations and orthogonal sentence states",
    "citation_count": 4,
    "authors": [
      "Litton J Kurisinkel",
      "Ai Ti Aw",
      "Nancy F Chen"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.32": {
    "title": "An Empirical Study of Generating Texts for Search Engine Advertising",
    "volume": "industry",
    "abstract": "Although there are many studies on neural language generation (NLG), few trials are put into the real world, especially in the advertising domain. Generating ads with NLG models can help copywriters in their creation. However, few studies have adequately evaluated the effect of generated ads with actual serving included because it requires a large amount of training data and a particular environment. In this paper, we demonstrate a practical use case of generating ad-text with an NLG model. Specially, we show how to improve the ads' impact, deploy models to a product, and evaluate the generated ads",
    "checked": true,
    "id": "bd0b8a5e7590815c20fe237a956412d0a50e60ea",
    "semantic_title": "an empirical study of generating texts for search engine advertising",
    "citation_count": 9,
    "authors": [
      "Hidetaka Kamigaito",
      "Peinan Zhang",
      "Hiroya Takamura",
      "Manabu Okumura"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.33": {
    "title": "Ad Headline Generation using Self-Critical Masked Language Model",
    "volume": "industry",
    "abstract": "For any E-commerce website it is a nontrivial problem to build enduring advertisements that attract shoppers. It is hard to pass the creative quality bar of the website, especially at a large scale. We thus propose a programmatic solution to generate product advertising headlines using retail content. We propose a state of the art application of Reinforcement Learning (RL) Policy gradient methods on Transformer (Vaswani et al., 2017) based Masked Language Models (Devlin et al., 2019). Our method creates the advertising headline by jointly conditioning on multiple products that a seller wishes to advertise. We demonstrate that our method outperforms existing Transformer and LSTM + RL methods in overlap metrics and quality audits. We also show that our model generated headlines outperform human submitted headlines in terms of both grammar and creative quality as determined by audits",
    "checked": true,
    "id": "689203cbf139b01b18592c14c487ca3df880dee1",
    "semantic_title": "ad headline generation using self-critical masked language model",
    "citation_count": 14,
    "authors": [
      "Yashal Shakti Kanungo",
      "Sumit Negi",
      "Aruna Rajan"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.34": {
    "title": "LATEX-Numeric: Language Agnostic Text Attribute Extraction for Numeric Attributes",
    "volume": "industry",
    "abstract": "In this paper, we present LATEX-Numeric - a high-precision fully-automated scalable framework for extracting E-commerce numeric attributes from unstructured product text like product description. Most of the past work on attribute extraction is not scalable as they rely on manually curated training data, either with or without use of active learning. We rely on distant supervision for training data generation, removing dependency on manual labels. One issue with distant supervision is that it leads to incomplete training annotation due to missing attribute values while matching. We propose a multi-task learning architecture to deal with missing labels in the training data, leading to F1 improvement of 9.2% for numeric attributes over state-of-the-art single-task architecture. While multi-task architecture benefits both numeric and non-numeric attributes, we present automated techniques to further improve the numeric attributes extraction models. Numeric attributes require a list of units (or aliases) for better matching with distant supervision. We propose an automated algorithm for alias creation using unstructured text and attribute values, leading to a 20.2% F1 improvement. Extensive experiments on real world datasets for 20 numeric attributes across 5 product categories and 3 English marketplaces show that LATEX-numeric achieves a high F1-score, without any manual intervention, making it suitable for practical applications. Finally we show that the improvements are language-agnostic and LATEX-Numeric achieves 13.9% F1 improvement for 3 non-English languages",
    "checked": true,
    "id": "2781182a38b6303dc5b4cb224579e9a365156bc3",
    "semantic_title": "latex-numeric: language agnostic text attribute extraction for numeric attributes",
    "citation_count": 5,
    "authors": [
      "Kartik Mehta",
      "Ioana Oprea",
      "Nikhil Rasiwasia"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.35": {
    "title": "Training Language Models under Resource Constraints for Adversarial Advertisement Detection",
    "volume": "industry",
    "abstract": "Advertising on e-commerce and social media sites deliver ad impressions at web scale on a daily basis driving value to both shoppers and advertisers. This scale necessitates programmatic ways of detecting unsuitable content in ads to safeguard customer experience and trust. This paper focusses on techniques for training text classification models under resource constraints, built as part of automated solutions for advertising content moderation. We show how weak supervision, curriculum learning and multi-lingual training can be applied effectively to fine-tune BERT and its variants for text classification tasks in conjunction with different data augmentation strategies. Our extensive experiments on multiple languages show that these techniques detect adversarial ad categories with a substantial gain in precision at high recall threshold over the baseline",
    "checked": true,
    "id": "c73c95876b94fcad8d869c3d5b0c4b2a981dd395",
    "semantic_title": "training language models under resource constraints for adversarial advertisement detection",
    "citation_count": 1,
    "authors": [
      "Eshwar Shamanna Girishekar",
      "Shiv Surya",
      "Nishant Nikhil",
      "Dyut Kumar Sil",
      "Sumit Negi",
      "Aruna Rajan"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.36": {
    "title": "Combining Weakly Supervised ML Techniques for Low-Resource NLU",
    "volume": "industry",
    "abstract": "Recent advances in transfer learning have improved the performance of virtual assistants considerably. Nevertheless, creating sophisticated voice-enabled applications for new domains remains a challenge, and meager training data is often a key bottleneck. Accordingly, unsupervised learning and SSL (semi-supervised learning) techniques continue to be of vital importance. While a number of such methods have been explored previously in isolation, in this paper we investigate the synergistic use of a number of weakly supervised techniques with a view to improving NLU (Natural Language Understanding) accuracy in low-resource settings. We explore three different approaches incorporating anonymized, unlabeled and automatically transcribed user utterances into the training process, two focused on data augmentation via SSL and another one focused on unsupervised and transfer learning. We show promising results, obtaining gains that range from 4.73% to 7.65% relative improvements on semantic error rate for each individual approach. Moreover, the combination of all three methods together yields a relative improvement of 11.77% over our current baseline model. Our methods are applicable to any new domain with minimal training data, and can be deployed over time into a cycle of continual learning",
    "checked": true,
    "id": "ea173ebaf9f3725fab8f931b4a2531dc2507a503",
    "semantic_title": "combining weakly supervised ml techniques for low-resource nlu",
    "citation_count": 1,
    "authors": [
      "Victor Soto",
      "Konstantine Arkoudas"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.37": {
    "title": "Label-Guided Learning for Item Categorization in e-Commerce",
    "volume": "industry",
    "abstract": "Item categorization is an important application of text classification in e-commerce due to its impact on the online shopping experience of users. One class of text classification techniques that has gained attention recently is using the semantic information of the labels to guide the classification task. We have conducted a systematic investigation of the potential benefits of these methods on a real data set from a major e-commerce company in Japan. Furthermore, using a hyperbolic space to embed product labels that are organized in a hierarchical structure led to better performance compared to using a conventional Euclidean space embedding. These findings demonstrate how label-guided learning can improve item categorization systems in the e-commerce domain",
    "checked": true,
    "id": "71cac5ef629cebdbaecad10f6ef522d58787b682",
    "semantic_title": "label-guided learning for item categorization in e-commerce",
    "citation_count": 4,
    "authors": [
      "Lei Chen",
      "Hirokazu Miyake"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.38": {
    "title": "Benchmarking Commercial Intent Detection Services with Practice-Driven Evaluations",
    "volume": "industry",
    "abstract": "Intent detection is a key component of modern goal-oriented dialog systems that accomplish a user task by predicting the intent of users' text input. There are three primary challenges in designing robust and accurate intent detection models. First, typical intent detection models require a large amount of labeled data to achieve high accuracy. Unfortunately, in practical scenarios it is more common to find small, unbalanced, and noisy datasets. Secondly, even with large training data, the intent detection models can see a different distribution of test data when being deployed in the real world, leading to poor accuracy. Finally, a practical intent detection model must be computationally efficient in both training and single query inference so that it can be used continuously and re-trained frequently. We benchmark intent detection methods on a variety of datasets. Our results show that Watson Assistant's intent detection model outperforms other commercial solutions and is comparable to large pretrained language models while requiring only a fraction of computational resources and training data. Watson Assistant demonstrates a higher degree of robustness when the training and test distributions differ",
    "checked": true,
    "id": "76b813ad2c1d62e56503eb8df32f15d767f8426c",
    "semantic_title": "benchmarking commercial intent detection services with practice-driven evaluations",
    "citation_count": 9,
    "authors": [
      "Haode Qi",
      "Lin Pan",
      "Atin Sood",
      "Abhishek Shah",
      "Ladislav Kunc",
      "Mo Yu",
      "Saloni Potdar"
    ]
  },
  "https://aclanthology.org/2021.naacl-industry.39": {
    "title": "Industry Scale Semi-Supervised Learning for Natural Language Understanding",
    "volume": "industry",
    "abstract": "This paper presents a production Semi-Supervised Learning (SSL) pipeline based on the student-teacher framework, which leverages millions of unlabeled examples to improve Natural Language Understanding (NLU) tasks. We investigate two questions related to the use of unlabeled data in production SSL context: 1) how to select samples from a huge unlabeled data pool that are beneficial for SSL training, and 2) how does the selected data affect the performance of different state-of-the-art SSL techniques. We compare four widely used SSL techniques, Pseudo-label (PL), Knowledge Distillation (KD), Virtual Adversarial Training (VAT) and Cross-View Training (CVT) in conjunction with two data selection methods including committee-based selection and submodular optimization based selection. We further examine the benefits and drawbacks of these techniques when applied to intent classification (IC) and named entity recognition (NER) tasks, and provide guidelines specifying when each of these methods might be beneficial to improve large scale NLU systems",
    "checked": true,
    "id": "9c5986215267ffff94d7351f615cf575cd9cc61f",
    "semantic_title": "industry scale semi-supervised learning for natural language understanding",
    "citation_count": 5,
    "authors": [
      "Luoxin Chen",
      "Francisco Garcia",
      "Varun Kumar",
      "He Xie",
      "Jianhua Lu"
    ]
  },
  "https://aclanthology.org/2021.naacl-srw.1": {
    "title": "Sampling and Filtering of Neural Machine Translation Distillation Data",
    "volume": "student",
    "abstract": "In most of neural machine translation distillation or stealing scenarios, the highest-scoring hypothesis of the target model (teacher) is used to train a new model (student). If reference translations are also available, then better hypotheses (with respect to the references) can be oversampled and poor hypotheses either removed or undersampled. This paper explores the sampling method landscape (pruning, hypothesis oversampling and undersampling, deduplication and their combination) with English to Czech and English to German MT models using standard MT evaluation metrics. We show that careful oversampling and combination with the original data leads to better performance when compared to training only on the original or synthesized data or their direct combination",
    "checked": true,
    "id": "9db4e74bd70af5f4bfe9d51c138eee92e3b60642",
    "semantic_title": "sampling and filtering of neural machine translation distillation data",
    "citation_count": 2,
    "authors": [
      "Vilém Zouhar"
    ]
  },
  "https://aclanthology.org/2021.naacl-srw.2": {
    "title": "IceSum: An Icelandic Text Summarization Corpus",
    "volume": "student",
    "abstract": "Automatic Text Summarization (ATS) is the task of generating concise and fluent summaries from one or more documents. In this paper, we present IceSum, the first Icelandic corpus annotated with human-generated summaries. IceSum consists of 1,000 online news articles and their extractive summaries. We train and evaluate several neural network-based models on this dataset, comparing them against a selection of baseline methods. We find that an encoder-decoder model with a sequence-to-sequence based extractor obtains the best results, outperforming all baseline methods. Furthermore, we evaluate how the size of the training corpus affects the quality of the generated summaries. We release the corpus and the models with an open license",
    "checked": true,
    "id": "132a3badaed398ec3298b11ead5f216e677cfbae",
    "semantic_title": "icesum: an icelandic text summarization corpus",
    "citation_count": 3,
    "authors": [
      "Jón Daðason",
      "Hrafn Loftsson",
      "Salome Sigurðardóttir",
      "Þorsteinn Björnsson"
    ]
  },
  "https://aclanthology.org/2021.naacl-srw.3": {
    "title": "Negation typology and general representation models for cross-lingual zero-shot negation scope resolution in Russian, French, and Spanish",
    "volume": "student",
    "abstract": "Negation is a linguistic universal that poses difficulties for cognitive and computational processing. Despite many advances in text analytics, negation resolution remains an acute and continuously researched question in Natural Language Processing. Reliable negation parsing affects results in biomedical text mining, sentiment analysis, machine translation, and many other fields. The availability of multilingual pre-trained general representation models makes it possible to experiment with negation detection in languages that lack annotated data. In this work we test the performance of two state-of-the-art contextual representation models, Multilingual BERT and XLM-RoBERTa. We resolve negation scope by conducting zero-shot transfer between English, Spanish, French, and Russian. Our best result amounts to a token-level F1-score of 86.86% between Spanish and Russian. We correlate these results with a linguistic negation typology and lexical capacity of the models",
    "checked": true,
    "id": "076ffe70a443cf9f25ccf0ff86350e57c1e49c7e",
    "semantic_title": "negation typology and general representation models for cross-lingual zero-shot negation scope resolution in russian, french, and spanish",
    "citation_count": 6,
    "authors": [
      "Anastassia Shaitarova",
      "Fabio Rinaldi"
    ]
  },
  "https://aclanthology.org/2021.naacl-srw.4": {
    "title": "Representations of Meaning in Neural Networks for NLP: a Thesis Proposal",
    "volume": "student",
    "abstract": "Neural networks are the state-of-the-art method of machine learning for many problems in NLP. Their success in machine translation and other NLP tasks is phenomenal, but their interpretability is challenging. We want to find out how neural networks represent meaning. In order to do this, we propose to examine the distribution of meaning in the vector space representation of words in neural networks trained for NLP tasks. Furthermore, we propose to consider various theories of meaning in the philosophy of language and to find a methodology that would enable us to connect these areas",
    "checked": true,
    "id": "cd8e65d8feb00c54aa0c9e045fdfe51b65ac9bbe",
    "semantic_title": "representations of meaning in neural networks for nlp: a thesis proposal",
    "citation_count": 2,
    "authors": [
      "Tomáš Musil"
    ]
  },
  "https://aclanthology.org/2021.naacl-srw.5": {
    "title": "Towards Layered Events and Schema Representations in Long Documents",
    "volume": "student",
    "abstract": "In this thesis proposal, we explore the application of event extraction to literary texts. Considering the lengths of literary documents modeling events in different granularities may be more adequate to extract meaningful information, as individual elements contribute little to the overall semantics. We adapt the concept of schemas as sequences of events all describing a single process, connected through shared participants extending it to for multiple schemas in a document. Segmentation of event sequences into schemas is approached by modeling event sequences, on such task as the narrative cloze task, the prediction of missing events in sequences. We propose building on sequences of event embeddings to form schema embeddings, thereby summarizing sections of documents using a single representation. This approach will allow for the comparisons of different sections of documents and entire literary works. Literature is a challenging domain based on its variety of genres, yet the representation of literary content has received relatively little attention",
    "checked": true,
    "id": "bebc5712159073f363f09db40f059fcca1507dba",
    "semantic_title": "towards layered events and schema representations in long documents",
    "citation_count": 2,
    "authors": [
      "Hans Ole Hatzel",
      "Chris Biemann"
    ]
  },
  "https://aclanthology.org/2021.naacl-srw.6": {
    "title": "Parallel Text Alignment and Monolingual Parallel Corpus Creation from Philosophical Texts for Text Simplification",
    "volume": "student",
    "abstract": "Text simplification is a growing field with many potential useful applications. Training text simplification algorithms generally requires a lot of annotated data, however there are not many corpora suitable for this task. We propose a new unsupervised method for aligning text based on Doc2Vec embeddings and a new alignment algorithm, capable of aligning texts at different levels. Initial evaluation shows promising results for the new approach. We used the newly developed approach to create a new monolingual parallel corpus composed of the works of English early modern philosophers and their corresponding simplified versions",
    "checked": true,
    "id": "8445c6df6788fe7703b1c8a17051d869c89ef737",
    "semantic_title": "parallel text alignment and monolingual parallel corpus creation from philosophical texts for text simplification",
    "citation_count": 3,
    "authors": [
      "Stefan Paun"
    ]
  },
  "https://aclanthology.org/2021.naacl-srw.7": {
    "title": "Syntax-Based Attention Masking for Neural Machine Translation",
    "volume": "student",
    "abstract": "We present a simple method for extending transformers to source-side trees. We define a number of masks that limit self-attention based on relationships among tree nodes, and we allow each attention head to learn which mask or masks to use. On translation from English to various low-resource languages, and translation in both directions between English and German, our method always improves over simple linearization of the source-side parse tree and almost always improves over a sequence-to-sequence baseline, by up to +2.1 BLEU",
    "checked": true,
    "id": "7a7fa90f9bed03b7e3070445547c66137c38f9f5",
    "semantic_title": "syntax-based attention masking for neural machine translation",
    "citation_count": 5,
    "authors": [
      "Colin McDonald",
      "David Chiang"
    ]
  },
  "https://aclanthology.org/2021.naacl-srw.8": {
    "title": "Multi-Modal Image Captioning for the Visually Impaired",
    "volume": "student",
    "abstract": "One of the ways blind people understand their surroundings is by clicking images and relying on descriptions generated by image-captioning systems. Current work on captioning images for the visually impaired do not use the textual data present in the image when generating captions. This problem is critical as many visual scenes contain text, and 21% of the questions asked by blind people about the images they click pertain to the text present in them. In this work, we propose altering AoANet, a state-of-the-art image-captioning system, to leverage text detected in the image as an input feature. In addition, we use a pointer-generator network to copy detected text to the caption when tokens need to be reproduced accurately. Our model outperforms AoANet on the benchmark dataset VizWiz, giving a 35% and 16.2% performance improvement on CIDEr and SPICE scores, respectively",
    "checked": true,
    "id": "9fa2903b98c9dc5597bc1c05fa6c2733d03ae5bc",
    "semantic_title": "multi-modal image captioning for the visually impaired",
    "citation_count": 9,
    "authors": [
      "Hiba Ahsan",
      "Daivat Bhatt",
      "Kaivan Shah",
      "Nikita Bhalla"
    ]
  },
  "https://aclanthology.org/2021.naacl-srw.9": {
    "title": "Open-Domain Question Answering with Pre-Constructed Question Spaces",
    "volume": "student",
    "abstract": "Open-domain question answering aims at locating the answers to user-generated questions in massive collections of documents. Retriever-readers and knowledge graph approaches are two big families of solutions to this task. A retriever-reader first applies information retrieval techniques to locate a few passages that are likely to be relevant, and then feeds the retrieved text to a neural network reader to extract the answer. Alternatively, knowledge graphs can be constructed and queried to answer users' questions. We propose an algorithm with a novel reader-retriever design that differs from both families. Our reader-retriever first uses an offline reader to read the corpus and generate collections of all answerable questions associated with their answers, and then uses an online retriever to respond to user queries by searching the pre-constructed question spaces for answers that are most likely to be asked in the given way. We further combine one retriever-reader and two reader-retrievers into a hybrid model called R6 for the best performance. Experiments with two large-scale public datasets show that R6 achieves state-of-the-art accuracy",
    "checked": true,
    "id": "131d30cdaa978e113e2a5f5113fe7590dc1e8c5e",
    "semantic_title": "open-domain question answering with pre-constructed question spaces",
    "citation_count": 6,
    "authors": [
      "Jinfeng Xiao",
      "Lidan Wang",
      "Franck Dernoncourt",
      "Trung Bui",
      "Tong Sun",
      "Jiawei Han"
    ]
  },
  "https://aclanthology.org/2021.naacl-srw.10": {
    "title": "A Sliding-Window Approach to Automatic Creation of Meeting Minutes",
    "volume": "student",
    "abstract": "Meeting minutes record any subject matter discussed, decisions reached and actions taken at the meeting. The importance of automatic minuting cannot be overstated. In this paper, we present a sliding window approach to automatic generation of meeting minutes. It aims at addressing issues pertaining to the nature of spoken text, including the lengthy transcript and lack of document structure, which make it difficult to identify salient content to be included in meeting minutes. Our approach combines a sliding-window approach and a neural abstractive summarizer to navigate through the raw transcript to find salient content. The approach is evaluated on transcripts of natural meeting conversations, where we compare results obtained for human transcripts and two versions of automatic transcripts and discuss how and to what extent the summarizer succeeds at capturing salient content",
    "checked": true,
    "id": "3cf95d513429e3eca9846d0ebbc846a693960b0a",
    "semantic_title": "a sliding-window approach to automatic creation of meeting minutes",
    "citation_count": 17,
    "authors": [
      "Jia Jin Koay",
      "Alexander Roustai",
      "Xiaojin Dai",
      "Fei Liu"
    ]
  },
  "https://aclanthology.org/2021.naacl-srw.11": {
    "title": "Exploration and Discovery of the COVID-19 Literature through Semantic Visualization",
    "volume": "student",
    "abstract": "We propose semantic visualization as a linguistic visual analytic method. It can enable exploration and discovery over large datasets of complex networks by exploiting the semantics of the relations in them. This involves extracting information, applying parameter reduction operations, building hierarchical data representation and designing visualization. We also present the accompanying COVID-SemViz a searchable and interactive visualization system for knowledge exploration of COVID-19 data to demonstrate the application of our proposed method. In the user studies, users found that semantic visualization-powered COVID-SemViz is helpful in terms of finding relevant information and discovering unknown associations",
    "checked": true,
    "id": "ffaaab9163e6d9fccd83bb596f643824ce9c043f",
    "semantic_title": "exploration and discovery of the covid-19 literature through semantic visualization",
    "citation_count": 18,
    "authors": [
      "Jingxuan Tu",
      "Marc Verhagen",
      "Brent Cochran",
      "James Pustejovsky"
    ]
  },
  "https://aclanthology.org/2021.naacl-srw.12": {
    "title": "Shuffled-token Detection for Refining Pre-trained RoBERTa",
    "volume": "student",
    "abstract": "State-of-the-art transformer models have achieved robust performance on a variety of NLP tasks. Many of these approaches have employed domain agnostic pre-training tasks to train models that yield highly generalized sentence representations that can be fine-tuned for specific downstream tasks. We propose refining a pre-trained NLP model using the objective of detecting shuffled tokens. We use a sequential approach by starting with the pre-trained RoBERTa model and training it using our approach. Applying random shuffling strategy on the word-level, we found that our approach enables the RoBERTa model achieve better performance on 4 out of 7 GLUE tasks. Our results indicate that learning to detect shuffled tokens is a promising approach to learn more coherent sentence representations",
    "checked": true,
    "id": "a9693b7b57f203940889de6d3f979c70c09202ed",
    "semantic_title": "shuffled-token detection for refining pre-trained roberta",
    "citation_count": 10,
    "authors": [
      "Subhadarshi Panda",
      "Anjali Agrawal",
      "Jeewon Ha",
      "Benjamin Bloch"
    ]
  },
  "https://aclanthology.org/2021.naacl-srw.13": {
    "title": "Morphology-Aware Meta-Embeddings for Tamil",
    "volume": "student",
    "abstract": "In this work, we explore generating morphologically enhanced word embeddings for Tamil, a highly agglutinative South Indian language with rich morphology that remains low-resource with regards to NLP tasks. We present here the first-ever word analogy dataset for Tamil, consisting of 4499 hand-curated word tetrads across 10 semantic and 13 morphological relation types. Using a rules-based segmenter to capture morphology as well as meta-embedding techniques, we train meta-embeddings that outperform existing baselines by 16% on our analogy task and appear to mitigate a previously observed trade-off between semantic and morphological accuracy",
    "checked": true,
    "id": "5e62c3c7f614855f0bcdc3c0d91ca020761e2f27",
    "semantic_title": "morphology-aware meta-embeddings for tamil",
    "citation_count": 0,
    "authors": [
      "Arjun Sai Krishnan",
      "Seyoon Ragavan"
    ]
  },
  "https://aclanthology.org/2021.naacl-srw.14": {
    "title": "Seed Word Selection for Weakly-Supervised Text Classification with Unsupervised Error Estimation",
    "volume": "student",
    "abstract": "Weakly-supervised text classification aims to induce text classifiers from only a few user-provided seed words. The vast majority of previous work assumes high-quality seed words are given. However, the expert-annotated seed words are sometimes non-trivial to come up with. Furthermore, in the weakly-supervised learning setting, we do not have any labeled document to measure the seed words' efficacy, making the seed word selection process \"a walk in the dark\". In this work, we remove the need for expert-curated seed words by first mining (noisy) candidate seed words associated with the category names. We then train interim models with individual candidate seed words. Lastly, we estimate the interim models' error rate in an unsupervised manner. The seed words that yield the lowest estimated error rates are added to the final seed word set. A comprehensive evaluation of six binary classification tasks on four popular datasets demonstrates that the proposed method outperforms a baseline using only category name seed words and obtained comparable performance as a counterpart using expert-annotated seed words",
    "checked": true,
    "id": "0e440ad6ae61e8988b9269b19bcbaf45ddb64875",
    "semantic_title": "seed word selection for weakly-supervised text classification with unsupervised error estimation",
    "citation_count": 9,
    "authors": [
      "Yiping Jin",
      "Akshay Bhatia",
      "Dittaya Wanvarie"
    ]
  },
  "https://aclanthology.org/2021.naacl-srw.15": {
    "title": "Multi-Task Learning of Generation and Classification for Emotion-Aware Dialogue Response Generation",
    "volume": "student",
    "abstract": "For a computer to naturally interact with a human, it needs to be human-like. In this paper, we propose a neural response generation model with multi-task learning of generation and classification, focusing on emotion. Our model based on BART (Lewis et al., 2020), a pre-trained transformer encoder-decoder model, is trained to generate responses and recognize emotions simultaneously. Furthermore, we weight the losses for the tasks to control the update of parameters. Automatic evaluations and crowdsourced manual evaluations show that the proposed model makes generated responses more emotionally aware",
    "checked": true,
    "id": "f4bef31094420c572e6c4159c45234c741d9e5bf",
    "semantic_title": "multi-task learning of generation and classification for emotion-aware dialogue response generation",
    "citation_count": 20,
    "authors": [
      "Tatsuya Ide",
      "Daisuke Kawahara"
    ]
  },
  "https://aclanthology.org/2021.naacl-srw.16": {
    "title": "Comparison of Grammatical Error Correction Using Back-Translation Models",
    "volume": "student",
    "abstract": "Grammatical error correction (GEC) suffers from a lack of sufficient parallel data. Studies on GEC have proposed several methods to generate pseudo data, which comprise pairs of grammatical and artificially produced ungrammatical sentences. Currently, a mainstream approach to generate pseudo data is back-translation (BT). Most previous studies using BT have employed the same architecture for both the GEC and BT models. However, GEC models have different correction tendencies depending on the architecture of their models. Thus, in this study, we compare the correction tendencies of GEC models trained on pseudo data generated by three BT models with different architectures, namely, Transformer, CNN, and LSTM. The results confirm that the correction tendencies for each error type are different for every BT model. In addition, we investigate the correction tendencies when using a combination of pseudo data generated by different BT models. As a result, we find that the combination of different BT models improves or interpolates the performance of each error type compared with using a single BT model with different seeds",
    "checked": true,
    "id": "3f2bb1e7ed87b06ac82b08bd42e75896c2628ac1",
    "semantic_title": "comparison of grammatical error correction using back-translation models",
    "citation_count": 9,
    "authors": [
      "Aomi Koyama",
      "Kengo Hotate",
      "Masahiro Kaneko",
      "Mamoru Komachi"
    ]
  },
  "https://aclanthology.org/2021.naacl-srw.17": {
    "title": "Parallel sentences mining with transfer learning in an unsupervised setting",
    "volume": "student",
    "abstract": "The quality and quantity of parallel sentences are known as very important training data for constructing neural machine translation (NMT) systems. However, these resources are not available for many low-resource language pairs. Many existing methods need strong supervision are not suitable. Although several attempts at developing unsupervised models, they ignore the language-invariant between languages. In this paper, we propose an approach based on transfer learning to mine parallel sentences in the unsupervised setting. With the help of bilingual corpora of rich-resource language pairs, we can mine parallel sentences without bilingual supervision of low-resource language pairs. Experiments show that our approach improves the performance of mined parallel sentences compared with previous methods. In particular, we achieve excellent results at two real-world low-resource language pairs",
    "checked": true,
    "id": "9b4358a82b5740b53a12a64071ab9f90aafb2567",
    "semantic_title": "parallel sentences mining with transfer learning in an unsupervised setting",
    "citation_count": 7,
    "authors": [
      "Yu Sun",
      "Shaolin Zhu",
      "Feng Yifan",
      "Chenggang Mi"
    ]
  },
  "https://aclanthology.org/2021.naacl-srw.18": {
    "title": "Sentence Concatenation Approach to Data Augmentation for Neural Machine Translation",
    "volume": "student",
    "abstract": "Recently, neural machine translation is widely used for its high translation accuracy, but it is also known to show poor performance at long sentence translation. Besides, this tendency appears prominently for low resource languages. We assume that these problems are caused by long sentences being few in the train data. Therefore, we propose a data augmentation method for handling long sentences. Our method is simple; we only use given parallel corpora as train data and generate long sentences by concatenating two sentences. Based on our experiments, we confirm improvements in long sentence translation by proposed data augmentation despite the simplicity. Moreover, the proposed method improves translation quality more when combined with back-translation",
    "checked": true,
    "id": "2c1736f11602088d8a1a0199d03f70d630091a3c",
    "semantic_title": "sentence concatenation approach to data augmentation for neural machine translation",
    "citation_count": 14,
    "authors": [
      "Seiichiro Kondo",
      "Kengo Hotate",
      "Tosho Hirasawa",
      "Masahiro Kaneko",
      "Mamoru Komachi"
    ]
  },
  "https://aclanthology.org/2021.naacl-srw.19": {
    "title": "Emotion Classification in a Resource Constrained Language Using Transformer-based Approach",
    "volume": "student",
    "abstract": "Although research on emotion classification has significantly progressed in high-resource languages, it is still infancy for resource-constrained languages like Bengali. However, unavailability of necessary language processing tools and deficiency of benchmark corpora makes the emotion classification task in Bengali more challenging and complicated. This work proposes a transformer-based technique to classify the Bengali text into one of the six basic emotions: anger, fear, disgust, sadness, joy, and surprise. A Bengali emotion corpus consists of 6243 texts is developed for the classification task. Experimentation carried out using various machine learning (LR, RF, MNB, SVM), deep neural networks (CNN, BiLSTM, CNN+BiLSTM) and transformer (Bangla-BERT, m-BERT, XLM-R) based approaches. Experimental outcomes indicate that XLM-R outdoes all other techniques by achieving the highest weighted f_1-score of 69.73% on the test data",
    "checked": true,
    "id": "aa2e964f6a8bba6b467b4719cdeff4b975c3e5c6",
    "semantic_title": "emotion classification in a resource constrained language using transformer-based approach",
    "citation_count": 29,
    "authors": [
      "Avishek Das",
      "Omar Sharif",
      "Mohammed Moshiul Hoque",
      "Iqbal H. Sarker"
    ]
  },
  "https://aclanthology.org/2021.naacl-srw.20": {
    "title": "Hie-BART: Document Summarization with Hierarchical BART",
    "volume": "student",
    "abstract": "This paper proposes a new abstractive document summarization model, hierarchical BART (Hie-BART), which captures hierarchical structures of a document (i.e., sentence-word structures) in the BART model. Although the existing BART model has achieved a state-of-the-art performance on document summarization tasks, the model does not have the interactions between sentence-level information and word-level information. In machine translation tasks, the performance of neural machine translation models has been improved by incorporating multi-granularity self-attention (MG-SA), which captures the relationships between words and phrases. Inspired by the previous work, the proposed Hie-BART model incorporates MG-SA into the encoder of the BART model for capturing sentence-word structures. Evaluations on the CNN/Daily Mail dataset show that the proposed Hie-BART model outperforms some strong baselines and improves the performance of a non-hierarchical BART model (+0.23 ROUGE-L)",
    "checked": true,
    "id": "9d8b7f6710559bc4280a4eda605c94aa7a71267a",
    "semantic_title": "hie-bart: document summarization with hierarchical bart",
    "citation_count": 11,
    "authors": [
      "Kazuki Akiyama",
      "Akihiro Tamura",
      "Takashi Ninomiya"
    ]
  },
  "https://aclanthology.org/2021.naacl-srw.21": {
    "title": "Towards Multi-Modal Text-Image Retrieval to improve Human Reading",
    "volume": "student",
    "abstract": "In primary school, children's books, as well as in modern language learning apps, multi-modal learning strategies like illustrations of terms and phrases are used to support reading comprehension. Also, several studies in educational psychology suggest that integrating cross-modal information will improve reading comprehension. We claim that state-of- he-art multi-modal transformers, which could be used in a language learner context to improve human reading, will perform poorly because of the short and relatively simple textual data those models are trained with. To prove our hypotheses, we collected a new multi-modal image-retrieval dataset based on data from Wikipedia. In an in-depth data analysis, we highlight the differences between our dataset and other popular datasets. Additionally, we evaluate several state-of-the-art multi-modal transformers on text-image retrieval on our dataset and analyze their meager results, which verify our claims",
    "checked": true,
    "id": "325a10363725f0c24084a8cd12229c91204c60b7",
    "semantic_title": "towards multi-modal text-image retrieval to improve human reading",
    "citation_count": 5,
    "authors": [
      "Florian Schneider",
      "Özge Alaçam",
      "Xintong Wang",
      "Chris Biemann"
    ]
  },
  "https://aclanthology.org/2021.naacl-demos.1": {
    "title": "PhoNLP: A joint multi-task learning model for Vietnamese part-of-speech tagging, named entity recognition and dependency parsing",
    "volume": "demo",
    "abstract": "We present the first multi-task learning model – named PhoNLP – for joint Vietnamese part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing. Experiments on Vietnamese benchmark datasets show that PhoNLP produces state-of-the-art results, outperforming a single-task learning approach that fine-tunes the pre-trained Vietnamese language model PhoBERT (Nguyen and Nguyen, 2020) for each task independently. We publicly release PhoNLP as an open-source toolkit under the Apache License 2.0. Although we specify PhoNLP for Vietnamese, our PhoNLP training and evaluation command scripts in fact can directly work for other languages that have a pre-trained BERT-based language model and gold annotated corpora available for the three tasks of POS tagging, NER and dependency parsing. We hope that PhoNLP can serve as a strong baseline and useful toolkit for future NLP research and applications to not only Vietnamese but also the other languages. Our PhoNLP is available at https://github.com/VinAIResearch/PhoNLP",
    "checked": true,
    "id": "d851e3d18a86ca819fbf226ec5d25b27b400992b",
    "semantic_title": "phonlp: a joint multi-task learning model for vietnamese part-of-speech tagging, named entity recognition and dependency parsing",
    "citation_count": 26,
    "authors": [
      "Linh The Nguyen",
      "Dat Quoc Nguyen"
    ]
  },
  "https://aclanthology.org/2021.naacl-demos.2": {
    "title": "Machine-Assisted Script Curation",
    "volume": "demo",
    "abstract": "We describe Machine-Aided Script Curator (MASC), a system for human-machine collaborative script authoring. Scripts produced with MASC include (1) English descriptions of sub-events that comprise a larger, complex event; (2) event types for each of those events; (3) a record of entities expected to participate in multiple sub-events; and (4) temporal sequencing between the sub-events. MASC automates portions of the script creation process with suggestions for event types, links to Wikidata, and sub-events that may have been forgotten. We illustrate how these automations are useful to the script writer with a few case-study scripts",
    "checked": true,
    "id": "4563a3a0fa8a1bad759c63ae6427ba7cad9f8d16",
    "semantic_title": "machine-assisted script curation",
    "citation_count": 4,
    "authors": [
      "Manuel Ciosici",
      "Joseph Cummings",
      "Mitchell DeHaven",
      "Alex Hedges",
      "Yash Kankanampati",
      "Dong-Ho Lee",
      "Ralph Weischedel",
      "Marjorie Freedman"
    ]
  },
  "https://aclanthology.org/2021.naacl-demos.3": {
    "title": "NAMER: A Node-Based Multitasking Framework for Multi-Hop Knowledge Base Question Answering",
    "volume": "demo",
    "abstract": "We present NAMER, an open-domain Chinese knowledge base question answering system based on a novel node-based framework that better grasps the structural mapping between questions and KB queries by aligning the nodes in a query with their corresponding mentions in question. Equipped with techniques including data augmentation and multitasking, we show that the proposed framework outperforms the previous SoTA on CCKS CKBQA dataset. Moreover, we develop a novel data annotation strategy that facilitates the node-to-mention alignment, a dataset (https://github.com/ridiculouz/CKBQA) with such strategy is also published to promote further research. An online demo of NAMER (http://kbqademo.gstore.cn) is provided to visualize our framework and supply extra information for users, a video illustration (https://youtu.be/yetnVye_hg4) of NAMER is also available",
    "checked": true,
    "id": "6378193a8eec1aca00a18b3826c220ec0e4ec0b0",
    "semantic_title": "namer: a node-based multitasking framework for multi-hop knowledge base question answering",
    "citation_count": 8,
    "authors": [
      "Minhao Zhang",
      "Ruoyu Zhang",
      "Lei Zou",
      "Yinnian Lin",
      "Sen Hu"
    ]
  },
  "https://aclanthology.org/2021.naacl-demos.4": {
    "title": "DiSCoL: Toward Engaging Dialogue Systems through Conversational Line Guided Response Generation",
    "volume": "demo",
    "abstract": "Having engaging and informative conversations with users is the utmost goal for open-domain conversational systems. Recent advances in transformer-based language models and their applications to dialogue systems have succeeded to generate fluent and human-like responses. However, they still lack control over the generation process towards producing contentful responses and achieving engaging conversations. To achieve this goal, we present DiSCoL (Dialogue Systems through Coversational Line guided response generation). DiSCoL is an open-domain dialogue system that leverages conversational lines (briefly convlines) as controllable and informative content-planning elements to guide the generation model produce engaging and informative responses. Two primary modules in DiSCoL's pipeline are conditional generators trained for 1) predicting relevant and informative convlines for dialogue contexts and 2) generating high-quality responses conditioned on the predicted convlines. Users can also change the returned convlines to control the direction of the conversations towards topics that are more interesting for them. Through automatic and human evaluations, we demonstrate the efficiency of the convlines in producing engaging conversations",
    "checked": true,
    "id": "a6ef66a05c04464597b3f2029210b6ce8a51d304",
    "semantic_title": "discol: toward engaging dialogue systems through conversational line guided response generation",
    "citation_count": 10,
    "authors": [
      "Sarik Ghazarian",
      "Zixi Liu",
      "Tuhin Chakrabarty",
      "Xuezhe Ma",
      "Aram Galstyan",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2021.naacl-demos.5": {
    "title": "FITAnnotator: A Flexible and Intelligent Text Annotation System",
    "volume": "demo",
    "abstract": "In this paper, we introduce FITAnnotator, a generic web-based tool for efficient text annotation. Benefiting from the fully modular architecture design, FITAnnotator provides a systematic solution for the annotation of a variety of natural language processing tasks, including classification, sequence tagging and semantic role annotation, regardless of the language. Three kinds of interfaces are developed to annotate instances, evaluate annotation quality and manage the annotation task for annotators, reviewers and managers, respectively. FITAnnotator also gives intelligent annotations by introducing task-specific assistant to support and guide the annotators based on active learning and incremental learning strategies. This assistant is able to effectively update from the annotator feedbacks and easily handle the incremental labeling scenarios",
    "checked": true,
    "id": "3110f314b969695c2969eb44d0e2ac8acf63d8bf",
    "semantic_title": "fitannotator: a flexible and intelligent text annotation system",
    "citation_count": 7,
    "authors": [
      "Yanzeng Li",
      "Bowen Yu",
      "Li Quangang",
      "Tingwen Liu"
    ]
  },
  "https://aclanthology.org/2021.naacl-demos.6": {
    "title": "Robustness Gym: Unifying the NLP Evaluation Landscape",
    "volume": "demo",
    "abstract": "Despite impressive performance on standard benchmarks, natural language processing (NLP) models are often brittle when deployed in real-world systems. In this work, we identify challenges with evaluating NLP systems and propose a solution in the form of Robustness Gym (RG), a simple and extensible evaluation toolkit that unifies 4 standard evaluation paradigms: subpopulations, transformations, evaluation sets, and adversarial attacks. By providing a common platform for evaluation, RG enables practitioners to compare results from disparate evaluation paradigms with a single click, and to easily develop and share novel evaluation methods using a built-in set of abstractions. RG is under active development and we welcome feedback & contributions from the community",
    "checked": true,
    "id": "9b54941de1e21826ecc28b32730ac3f69991ede4",
    "semantic_title": "robustness gym: unifying the nlp evaluation landscape",
    "citation_count": 117,
    "authors": [
      "Karan Goel",
      "Nazneen Fatema Rajani",
      "Jesse Vig",
      "Zachary Taschdjian",
      "Mohit Bansal",
      "Christopher Ré"
    ]
  },
  "https://aclanthology.org/2021.naacl-demos.7": {
    "title": "EventPlus: A Temporal Event Understanding Pipeline",
    "volume": "demo",
    "abstract": "We present EventPlus, a temporal event understanding pipeline that integrates various state-of-the-art event understanding components including event trigger and type detection, event argument detection, event duration and temporal relation extraction. Event information, especially event temporal knowledge, is a type of common sense knowledge that helps people understand how stories evolve and provides predictive hints for future events. EventPlus as the first comprehensive temporal event understanding pipeline provides a convenient tool for users to quickly obtain annotations about events and their temporal information for any user-provided document. Furthermore, we show EventPlus can be easily adapted to other domains (e.g., biomedical domain). We make EventPlus publicly available to facilitate event-related information extraction and downstream applications",
    "checked": true,
    "id": "1120daebc2b344e19bc685c1b1783fed4efce047",
    "semantic_title": "eventplus: a temporal event understanding pipeline",
    "citation_count": 25,
    "authors": [
      "Mingyu Derek Ma",
      "Jiao Sun",
      "Mu Yang",
      "Kung-Hsiang Huang",
      "Nuan Wen",
      "Shikhar Singh",
      "Rujun Han",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2021.naacl-demos.8": {
    "title": "COVID-19 Literature Knowledge Graph Construction and Drug Repurposing Report Generation",
    "volume": "demo",
    "abstract": "To combat COVID-19, both clinicians and scientists need to digest the vast amount of relevant biomedical knowledge in literature to understand the disease mechanism and the related biological functions. We have developed a novel and comprehensive knowledge discovery framework, COVID-KG to extract fine-grained multimedia knowledge elements (entities, relations and events) from scientific literature. We then exploit the constructed multimedia knowledge graphs (KGs) for question answering and report generation, using drug repurposing as a case study. Our framework also provides detailed contextual sentences, subfigures, and knowledge subgraphs as evidence. All of the data, KGs, reports",
    "checked": true,
    "id": "328d027a999efc9d9e315367f5e01096ef4e7255",
    "semantic_title": "covid-19 literature knowledge graph construction and drug repurposing report generation",
    "citation_count": 104,
    "authors": [
      "Qingyun Wang",
      "Manling Li",
      "Xuan Wang",
      "Nikolaus Parulian",
      "Guangxing Han",
      "Jiawei Ma",
      "Jingxuan Tu",
      "Ying Lin",
      "Ranran Haoran Zhang",
      "Weili Liu",
      "Aabhas Chauhan",
      "Yingjun Guan",
      "Bangzheng Li",
      "Ruisong Li",
      "Xiangchen Song",
      "Yi Fung",
      "Heng Ji",
      "Jiawei Han",
      "Shih-Fu Chang",
      "James Pustejovsky",
      "Jasmine Rah",
      "David Liem",
      "Ahmed ELsayed",
      "Martha Palmer",
      "Clare Voss",
      "Cynthia Schneider",
      "Boyan Onyshkevych"
    ]
  },
  "https://aclanthology.org/2021.naacl-demos.9": {
    "title": "Multifaceted Domain-Specific Document Embeddings",
    "volume": "demo",
    "abstract": "Current document embeddings require large training corpora but fail to learn high-quality representations when confronted with a small number of domain-specific documents and rare terms. Further, they transform each document into a single embedding vector, making it hard to capture different notions of document similarity or explain why two documents are considered similar. In this work, we propose our Faceted Domain Encoder, a novel approach to learn multifaceted embeddings for domain-specific documents. It is based on a Siamese neural network architecture and leverages knowledge graphs to further enhance the embeddings even if only a few training samples are available. The model identifies different types of domain knowledge and encodes them into separate dimensions of the embedding, thereby enabling multiple ways of finding and comparing related documents in the vector space. We evaluate our approach on two benchmark datasets and find that it achieves the same embedding quality as state-of-the-art models while requiring only a tiny fraction of their training data. An interactive demo, our source code, and the evaluation datasets are available online: https://hpi.de/naumann/s/multifaceted-embeddings and a screencast is available on YouTube: https://youtu.be/HHcsX2clEwg",
    "checked": true,
    "id": "d83190286a7d7f6f4e99f82d71a137ba18655c76",
    "semantic_title": "multifaceted domain-specific document embeddings",
    "citation_count": 2,
    "authors": [
      "Julian Risch",
      "Philipp Hager",
      "Ralf Krestel"
    ]
  },
  "https://aclanthology.org/2021.naacl-demos.10": {
    "title": "Improving Evidence Retrieval for Automated Explainable Fact-Checking",
    "volume": "demo",
    "abstract": "Automated fact-checking on a large-scale is a challenging task that has not been studied systematically until recently. Large noisy document collections like the web or news articles make the task more difficult. We describe a three-stage automated fact-checking system, named Quin+, using evidence retrieval and selection methods. We demonstrate that using dense passage representations leads to much higher evidence recall in a noisy setting. We also propose two sentence selection approaches, an embedding-based selection using a dense retrieval model, and a sequence labeling approach for context-aware selection. Quin+ is able to verify open-domain claims using results from web search engines",
    "checked": true,
    "id": "633d572d96c894c8b39f86f792c1a6b890661401",
    "semantic_title": "improving evidence retrieval for automated explainable fact-checking",
    "citation_count": 24,
    "authors": [
      "Chris Samarinas",
      "Wynne Hsu",
      "Mong Li Lee"
    ]
  },
  "https://aclanthology.org/2021.naacl-demos.11": {
    "title": "Interactive Plot Manipulation using Natural Language",
    "volume": "demo",
    "abstract": "We present an interactive Plotting Agent, a system that enables users to directly manipulate plots using natural language instructions within an interactive programming environment. The Plotting Agent maps language to plot updates. We formulate this problem as a slot-based task-oriented dialog problem, which we tackle with a sequence-to-sequence model. This plotting model while accurate in most cases, still makes errors, therefore, the system allows a feedback mode, wherein the user is presented with a top-k list of plots, among which the user can pick the desired one. From this kind of feedback, we can then, in principle, continuously learn and improve the system. Given that plotting is widely used across data-driven fields, we believe our demonstration will be of interest to both practitioners such as data scientists broadly defined, and researchers interested in natural language interfaces",
    "checked": true,
    "id": "321b3f1d6c17712d7344c526ac553fcf72486893",
    "semantic_title": "interactive plot manipulation using natural language",
    "citation_count": 3,
    "authors": [
      "Yihan Wang",
      "Yutong Shao",
      "Ndapa Nakashole"
    ]
  },
  "https://aclanthology.org/2021.naacl-demos.12": {
    "title": "ActiveAnno: General-Purpose Document-Level Annotation Tool with Active Learning Integration",
    "volume": "demo",
    "abstract": "ActiveAnno is an annotation tool focused on document-level annotation tasks developed both for industry and research settings. It is designed to be a general-purpose tool with a wide variety of use cases. It features a modern and responsive web UI for creating annotation projects, conducting annotations, adjudicating disagreements, and analyzing annotation results. ActiveAnno embeds a highly configurable and interactive user interface. The tool also integrates a RESTful API that enables integration into other software systems, including an API for machine learning integration. ActiveAnno is built with extensible design and easy deployment in mind, all to enable users to perform annotation tasks with high efficiency and high-quality annotation results",
    "checked": true,
    "id": "460a171901341431d90eac9d229ab57ee3c419fb",
    "semantic_title": "activeanno: general-purpose document-level annotation tool with active learning integration",
    "citation_count": 9,
    "authors": [
      "Max Wiechmann",
      "Seid Muhie Yimam",
      "Chris Biemann"
    ]
  },
  "https://aclanthology.org/2021.naacl-demos.13": {
    "title": "TextEssence: A Tool for Interactive Analysis of Semantic Shifts Between Corpora",
    "volume": "demo",
    "abstract": "Embeddings of words and concepts capture syntactic and semantic regularities of language; however, they have seen limited use as tools to study characteristics of different corpora and how they relate to one another. We introduce TextEssence, an interactive system designed to enable comparative analysis of corpora using embeddings. TextEssence includes visual, neighbor-based, and similarity-based modes of embedding analysis in a lightweight, web-based interface. We further propose a new measure of embedding confidence based on nearest neighborhood overlap, to assist in identifying high-quality embeddings for corpus analysis. A case study on COVID-19 scientific literature illustrates the utility of the system. TextEssence can be found at https://textessence.github.io",
    "checked": true,
    "id": "50b6334af2919e4e10e65042e4cc7b7b09817778",
    "semantic_title": "textessence: a tool for interactive analysis of semantic shifts between corpora",
    "citation_count": 1,
    "authors": [
      "Denis Newman-Griffis",
      "Venkatesh Sivaraman",
      "Adam Perer",
      "Eric Fosler-Lussier",
      "Harry Hochheiser"
    ]
  },
  "https://aclanthology.org/2021.naacl-demos.14": {
    "title": "Supporting Spanish Writers using Automated Feedback",
    "volume": "demo",
    "abstract": "We present a tool that provides automated feedback to students studying Spanish writing. The feedback is given for four categories: topic development, coherence, writing conventions, and essay organization. The tool is made freely available via a Google Docs add-on. A small user study with third-level students in Mexico shows that students found the tool generally helpful and that most of them plan to continue using it as they work to improve their writing skills",
    "checked": true,
    "id": "0bf013043b749ece1a246cac550f886e293385a0",
    "semantic_title": "supporting spanish writers using automated feedback",
    "citation_count": 2,
    "authors": [
      "Aoife Cahill",
      "James Bruno",
      "James Ramey",
      "Gilmar Ayala Meneses",
      "Ian Blood",
      "Florencia Tolentino",
      "Tamar Lavee",
      "Slava Andreyev"
    ]
  },
  "https://aclanthology.org/2021.naacl-demos.15": {
    "title": "Alexa Conversations: An Extensible Data-driven Approach for Building Task-oriented Dialogue Systems",
    "volume": "demo",
    "abstract": "Traditional goal-oriented dialogue systems rely on various components such as natural language understanding, dialogue state tracking, policy learning and response generation. Training each component requires annotations which are hard to obtain for every new domain, limiting scalability of such systems. Similarly, rule-based dialogue systems require extensive writing and maintenance of rules and do not scale either. End-to-End dialogue systems, on the other hand, do not require module-specific annotations but need a large amount of data for training. To overcome these problems, in this demo, we present Alexa Conversations, a new approach for building goal-oriented dialogue systems that is scalable, extensible as well as data efficient. The components of this system are trained in a data-driven manner, but instead of collecting annotated conversations for training, we generate them using a novel dialogue simulator based on a few seed dialogues and specifications of APIs and entities provided by the developer. Our approach provides out-of-the-box support for natural conversational phenomenon like entity sharing across turns or users changing their mind during conversation without requiring developers to provide any such dialogue flows. We exemplify our approach using a simple pizza ordering task and showcase its value in reducing the developer burden for creating a robust experience. Finally, we evaluate our system using a typical movie ticket booking task integrated with live APIs and show that the dialogue simulator is an essential component of the system that leads to over 50% improvement in turn-level action signature prediction accuracy",
    "checked": true,
    "id": "2c498b6504ca7f5fb156a994aa2e68a51156237f",
    "semantic_title": "alexa conversations: an extensible data-driven approach for building task-oriented dialogue systems",
    "citation_count": 17,
    "authors": [
      "Anish Acharya",
      "Suranjit Adhikari",
      "Sanchit Agarwal",
      "Vincent Auvray",
      "Nehal Belgamwar",
      "Arijit Biswas",
      "Shubhra Chandra",
      "Tagyoung Chung",
      "Maryam Fazel-Zarandi",
      "Raefer Gabriel",
      "Shuyang Gao",
      "Rahul Goel",
      "Dilek Hakkani-Tur",
      "Jan Jezabek",
      "Abhay Jha",
      "Jiun-Yu Kao",
      "Prakash Krishnan",
      "Peter Ku",
      "Anuj Goyal",
      "Chien-Wei Lin",
      "Qing Liu",
      "Arindam Mandal",
      "Angeliki Metallinou",
      "Vishal Naik",
      "Yi Pan",
      "Shachi Paul",
      "Vittorio Perera",
      "Abhishek Sethi",
      "Minmin Shen",
      "Nikko Strom",
      "Eddie Wang"
    ]
  },
  "https://aclanthology.org/2021.naacl-demos.16": {
    "title": "RESIN: A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System",
    "volume": "demo",
    "abstract": "We present a new information extraction system that can automatically construct temporal event graphs from a collection of news documents from multiple sources, multiple languages (English and Spanish for our experiment), and multiple data modalities (speech, text, image and video). The system advances state-of-the-art from two aspects: (1) extending from sentence-level event extraction to cross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub, with a demo video",
    "checked": true,
    "id": "039ce73659332c12168de439e3f79e7039b636af",
    "semantic_title": "resin: a dockerized schema-guided cross-document cross-lingual cross-media information extraction and event tracking system",
    "citation_count": 46,
    "authors": [
      "Haoyang Wen",
      "Ying Lin",
      "Tuan Lai",
      "Xiaoman Pan",
      "Sha Li",
      "Xudong Lin",
      "Ben Zhou",
      "Manling Li",
      "Haoyu Wang",
      "Hongming Zhang",
      "Xiaodong Yu",
      "Alexander Dong",
      "Zhenhailong Wang",
      "Yi Fung",
      "Piyush Mishra",
      "Qing Lyu",
      "Dídac Surís",
      "Brian Chen",
      "Susan Windisch Brown",
      "Martha Palmer",
      "Chris Callison-Burch",
      "Carl Vondrick",
      "Jiawei Han",
      "Dan Roth",
      "Shih-Fu Chang",
      "Heng Ji"
    ]
  },
  "https://aclanthology.org/2021.naacl-demos.17": {
    "title": "MUDES: Multilingual Detection of Offensive Spans",
    "volume": "demo",
    "abstract": "The interest in offensive content identification in social media has grown substantially in recent years. Previous work has dealt mostly with post level annotations. However, identifying offensive spans is useful in many ways. To help coping with this important challenge, we present MUDES, a multilingual system to detect offensive spans in texts. MUDES features pre-trained models, a Python API for developers, and a user-friendly web-based interface. A detailed description of MUDES' components is presented in this paper",
    "checked": true,
    "id": "b659c762823f641f44ec09bf9367070508c7f982",
    "semantic_title": "mudes: multilingual detection of offensive spans",
    "citation_count": 33,
    "authors": [
      "Tharindu Ranasinghe",
      "Marcos Zampieri"
    ]
  },
  "https://aclanthology.org/2021.naacl-tutorials.1": {
    "title": "Pretrained Transformers for Text Ranking: BERT and Beyond",
    "volume": "tutorial",
    "abstract": "The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query for a particular task. Although the most common formulation of text ranking is search, instances of the task can also be found in many text processing applications. This tutorial provides an overview of text ranking with neural network architectures known as transformers, of which BERT (Bidirectional Encoder Representations from Transformers) is the best-known example. These models produce high quality results across many domains, tasks, and settings. This tutorial, which is based on the preprint of a forthcoming book to be published by Morgan and & Claypool under the Synthesis Lectures on Human Language Technologies series, provides an overview of existing work as a single point of entry for practitioners who wish to deploy transformers for text ranking in real-world applications and researchers who wish to pursue work in this area. We cover a wide range of techniques, grouped into two categories: transformer models that perform reranking in multi-stage ranking architectures and learned dense representations that perform ranking directly",
    "checked": true,
    "id": "2c953a3c378b40dadf2e3fb486713c8608b8e282",
    "semantic_title": "pretrained transformers for text ranking: bert and beyond",
    "citation_count": 469,
    "authors": [
      "Andrew Yates",
      "Rodrigo Nogueira",
      "Jimmy Lin"
    ]
  },
  "https://aclanthology.org/2021.naacl-tutorials.2": {
    "title": "Fine-grained Interpretation and Causation Analysis in Deep NLP Models",
    "volume": "tutorial",
    "abstract": "Deep neural networks have constantly pushed the state-of-the-art performance in natural language processing and are considered as the de-facto modeling approach in solving complex NLP tasks such as machine translation, summarization and question-answering. Despite the proven efficacy of deep neural networks at-large, their opaqueness is a major cause of concern. In this tutorial, we will present research work on interpreting fine-grained components of a neural network model from two perspectives, i) fine-grained interpretation, and ii) causation analysis. The former is a class of methods to analyze neurons with respect to a desired language concept or a task. The latter studies the role of neurons and input features in explaining the decisions made by the model. We will also discuss how interpretation methods and causation analysis can connect towards better interpretability of model prediction. Finally, we will walk you through various toolkits that facilitate fine-grained interpretation and causation analysis of neural models",
    "checked": true,
    "id": "4ce4190ec40955f5274e4cfa538f0cad212aac45",
    "semantic_title": "fine-grained interpretation and causation analysis in deep nlp models",
    "citation_count": 7,
    "authors": [
      "Hassan Sajjad",
      "Narine Kokhlikyan",
      "Fahim Dalvi",
      "Nadir Durrani"
    ]
  },
  "https://aclanthology.org/2021.naacl-tutorials.3": {
    "title": "Deep Learning on Graphs for Natural Language Processing",
    "volume": "tutorial",
    "abstract": "Due to its great power in modeling non-Euclidean data like graphs or manifolds, deep learning on graph techniques (i.e., Graph Neural Networks (GNNs)) have opened a new door to solving challenging graph-related NLP problems. There has seen a surge of interests in applying deep learning on graph techniques to NLP, and has achieved considerable success in many NLP tasks, ranging from classification tasks like sentence classification, semantic role labeling and relation extraction, to generation tasks like machine translation, question generation and summarization. Despite these successes, deep learning on graphs for NLP still face many challenges, including automatically transforming original text sequence data into highly graph-structured data, and effectively modeling complex data that involves mapping between graph-based inputs and other highly structured output data such as sequences, trees, and graph data with multi-types in both nodes and edges. This tutorial will cover relevant and interesting topics on applying deep learning on graph techniques to NLP, including automatic graph construction for NLP, graph representation learning for NLP, advanced GNN based models (e.g., graph2seq, graph2tree, and graph2graph) for NLP, and the applications of GNNs in various NLP tasks (e.g., machine translation, natural language generation, information extraction and semantic parsing). In addition, hands-on demonstration sessions will be included to help the audience gain practical experience on applying GNNs to solve challenging NLP problems using our recently developed open source library – Graph4NLP, the first library for researchers and practitioners for easy use of GNNs for various NLP tasks",
    "checked": true,
    "id": "0ddd6dcf82032e0c5e10427502941b467167a518",
    "semantic_title": "deep learning on graphs for natural language processing",
    "citation_count": 25,
    "authors": [
      "Lingfei Wu",
      "Yu Chen",
      "Heng Ji",
      "Yunyao Li"
    ]
  },
  "https://aclanthology.org/2021.naacl-tutorials.4": {
    "title": "A Tutorial on Evaluation Metrics used in Natural Language Generation",
    "volume": "tutorial",
    "abstract": "The advent of Deep Learning and the availability of large scale datasets has accelerated research on Natural Language Generation with a focus on newer tasks and better models. With such rapid progress, it is vital to assess the extent of scientific progress made and identify the areas/components that need improvement. To accomplish this in an automatic and reliable manner, the NLP community has actively pursued the development of automatic evaluation metrics. Especially in the last few years, there has been an increasing focus on evaluation metrics, with several criticisms of existing metrics and proposals for several new metrics. This tutorial presents the evolution of automatic evaluation metrics to their current state along with the emerging trends in this field by specifically addressing the following questions: (i) What makes NLG evaluation challenging? (ii) Why do we need automatic evaluation metrics? (iii) What are the existing automatic evaluation metrics and how can they be organised in a coherent taxonomy? (iv) What are the criticisms and shortcomings of existing metrics? (v) What are the possible future directions of research?",
    "checked": true,
    "id": "c6f4cd0efcef2712bced1738eca8ec0bb40d4c6d",
    "semantic_title": "a tutorial on evaluation metrics used in natural language generation",
    "citation_count": 4,
    "authors": [
      "Mitesh M. Khapra",
      "Ananya B. Sai"
    ]
  },
  "https://aclanthology.org/2021.naacl-tutorials.5": {
    "title": "Beyond Paragraphs: NLP for Long Sequences",
    "volume": "tutorial",
    "abstract": "In this tutorial, we aim at bringing interested NLP researchers up to speed about the recent and ongoing techniques for document-level representation learning. Additionally, our goal is to reveal new research opportunities to the audience, which will hopefully bring us closer to address existing challenges in this domain",
    "checked": true,
    "id": "69285c9040c1356272752499b7e1e53ef25ac008",
    "semantic_title": "beyond paragraphs: nlp for long sequences",
    "citation_count": 6,
    "authors": [
      "Iz Beltagy",
      "Arman Cohan",
      "Hannaneh Hajishirzi",
      "Sewon Min",
      "Matthew E. Peters"
    ]
  },
  "https://aclanthology.org/2021.naacl-tutorials.6": {
    "title": "Crowdsourcing Natural Language Data at Scale: A Hands-On Tutorial",
    "volume": "tutorial",
    "abstract": "In this tutorial, we present a portion of unique industry experience in efficient natural language data annotation via crowdsourcing shared by both leading researchers and engineers from Yandex. We will make an introduction to data labeling via public crowdsourcing marketplaces and will present the key components of efficient label collection. This will be followed by a practical session, where participants address a real-world language resource production task, experiment with selecting settings for the labeling process, and launch their label collection project on one of the largest crowdsourcing marketplaces. The projects will be run on real crowds within the tutorial session and we will present useful quality control techniques and provide the attendees with an opportunity to discuss their own annotation ideas",
    "checked": true,
    "id": "61a07d1e4eaa831152e253b96b91808ef3a184b4",
    "semantic_title": "crowdsourcing natural language data at scale: a hands-on tutorial",
    "citation_count": 7,
    "authors": [
      "Alexey Drutsa",
      "Dmitry Ustalov",
      "Valentina Fedorova",
      "Olga Megorskaya",
      "Daria Baidakova"
    ]
  },
  "https://aclanthology.org/2021.alvr-1.1": {
    "title": "Feature-level Incongruence Reduction for Multimodal Translation",
    "volume": "workshop",
    "abstract": "Caption translation aims to translate image annotations (captions for short). Recently, Multimodal Neural Machine Translation (MNMT) has been explored as the essential solution. Besides of linguistic features in captions, MNMT allows visual(image) features to be used. The integration of multimodal features reinforces the semantic representation and considerably improves translation performance. However, MNMT suffers from the incongruence between visual and linguistic features. To overcome the problem, we propose to extend MNMT architecture with a harmonization network, which harmonizes multimodal features(linguistic and visual features)by unidirectional modal space conversion. It enables multimodal translation to be carried out in a seemingly monomodal translation pipeline. We experiment on the golden Multi30k-16 and 17. Experimental results show that, compared to the baseline,the proposed method yields the improvements of 2.2% BLEU for the scenario of translating English captions into German (En→De) at best,7.6% for the case of English-to-French translation(En→Fr) and 1.5% for English-to-Czech(En→Cz). The utilization of harmonization network leads to the competitive performance to the-state-of-the-art",
    "checked": true,
    "id": "ef1529c0219424283bbe4d09c79ca051c19acb90",
    "semantic_title": "feature-level incongruence reduction for multimodal translation",
    "citation_count": 3,
    "authors": [
      "Zhifeng Li",
      "Yu Hong",
      "Yuchen Pan",
      "Jian Tang",
      "Jianmin Yao",
      "Guodong Zhou"
    ]
  },
  "https://aclanthology.org/2021.alvr-1.2": {
    "title": "Error Causal inference for Multi-Fusion models",
    "volume": "workshop",
    "abstract": "In this paper, we propose an error causal inference method that could be used for finding dominant features for a faulty instance under a well-trained multi-modality input model, which could apply to any testing instance. We evaluate our method using a well-trained multi-modalities stylish caption generation model and find those causal inferences that could provide us the insights for next step optimization",
    "checked": true,
    "id": "9b418ad4120fd13fe5f894bfca20bab93b5cdcb3",
    "semantic_title": "error causal inference for multi-fusion models",
    "citation_count": 1,
    "authors": [
      "Chengxi Li",
      "Brent Harrison"
    ]
  },
  "https://aclanthology.org/2021.alvr-1.3": {
    "title": "Leveraging Partial Dependency Trees to Control Image Captions",
    "volume": "workshop",
    "abstract": "Controlling the generation of image captions attracts lots of attention recently. In this paper, we propose a framework leveraging partial syntactic dependency trees as control signals to make image captions include specified words and their syntactic structures. To achieve this purpose, we propose a Syntactic Dependency Structure Aware Model (SDSAM), which explicitly learns to generate the syntactic structures of image captions to include given partial dependency trees. In addition, we come up with a metric to evaluate how many specified words and their syntactic dependencies are included in generated captions. We carry out experiments on two standard datasets: Microsoft COCO and Flickr30k. Empirical results show that image captions generated by our model are effectively controlled in terms of specified words and their syntactic structures. The code is available on GitHub",
    "checked": true,
    "id": "e736814d45b6721bfc204b6ee86bd29c15e51f46",
    "semantic_title": "leveraging partial dependency trees to control image captions",
    "citation_count": 1,
    "authors": [
      "Wenjie Zhong",
      "Yusuke Miyao"
    ]
  },
  "https://aclanthology.org/2021.alvr-1.4": {
    "title": "Grounding Plural Phrases: Countering Evaluation Biases by Individuation",
    "volume": "workshop",
    "abstract": "Phrase grounding (PG) is a multimodal task that grounds language in images. PG systems are evaluated on well-known benchmarks, using Intersection over Union (IoU) as evaluation metric. This work highlights a disconcerting bias in the evaluation of grounded plural phrases, which arises from representing sets of objects as a union box covering all component bounding boxes, in conjunction with the IoU metric. We detect, analyze and quantify an evaluation bias in the grounding of plural phrases and define a novel metric, c-IoU, based on a union box's component boxes. We experimentally show that our new metric greatly alleviates this bias and recommend using it for fairer evaluation of plural phrases in PG tasks",
    "checked": true,
    "id": "35b67abd153fc7743a7c635313273b5577e8ee00",
    "semantic_title": "grounding plural phrases: countering evaluation biases by individuation",
    "citation_count": 0,
    "authors": [
      "Julia Suter",
      "Letitia Parcalabescu",
      "Anette Frank"
    ]
  },
  "https://aclanthology.org/2021.alvr-1.5": {
    "title": "PanGEA: The Panoramic Graph Environment Annotation Toolkit",
    "volume": "workshop",
    "abstract": "PanGEA, the Panoramic Graph Environment Annotation toolkit, is a lightweight toolkit for collecting speech and text annotations in photo-realistic 3D environments. PanGEA immerses annotators in a web-based simulation and allows them to move around easily as they speak and/or listen. It includes database and cloud storage integration, plus utilities for automatically aligning recorded speech with manual transcriptions and the virtual pose of the annotators. Out of the box, PanGEA supports two tasks – collecting navigation instructions and navigation instruction following – and it could be easily adapted for annotating walking tours, finding and labeling landmarks or objects, and similar tasks. We share best practices learned from using PanGEA in a 20,000 hour annotation effort to collect the Room-Across-Room dataset. We hope that our open-source annotation toolkit and insights will both expedite future data collection efforts and spur innovation on the kinds of grounded language tasks such environments can support",
    "checked": true,
    "id": "11c2a4706f8993f40475bf8ff79058c88117ed26",
    "semantic_title": "pangea: the panoramic graph environment annotation toolkit",
    "citation_count": 2,
    "authors": [
      "Alexander Ku",
      "Peter Anderson",
      "Jordi Pont Tuset",
      "Jason Baldridge"
    ]
  },
  "https://aclanthology.org/2021.alvr-1.6": {
    "title": "Learning to Learn Semantic Factors in Heterogeneous Image Classification",
    "volume": "workshop",
    "abstract": "Few-shot learning is to recognize novel classes with a few labeled samples per class. Although numerous meta-learning methods have made significant progress, they struggle to directly address the heterogeneity of training and evaluating task distributions, resulting in the domain shift problem when transitioning to new tasks with disjoint spaces. In this paper, we propose a novel method to deal with the heterogeneity. Specifically, by simulating class-difference domain shift during the meta-train phase, a bilevel optimization procedure is applied to learn a transferable representation space that can rapidly adapt to heterogeneous tasks. Experiments demonstrate the effectiveness of our proposed method",
    "checked": true,
    "id": "15b9b28feb24433ffd212e8a8263a26c35cfe9c8",
    "semantic_title": "learning to learn semantic factors in heterogeneous image classification",
    "citation_count": 0,
    "authors": [
      "Boyue Fan",
      "Zhenting Liu"
    ]
  },
  "https://aclanthology.org/2021.alvr-1.7": {
    "title": "Reference and coreference in situated dialogue",
    "volume": "workshop",
    "abstract": "In recent years several corpora have been developed for vision and language tasks. We argue that there is still significant room for corpora that increase the complexity of both visual and linguistic domains and which capture different varieties of perceptual and conversational contexts. Working with two corpora approaching this goal, we present a linguistic perspective on some of the challenges in creating and extending resources combining language and vision while preserving continuity with the existing best practices in the area of coreference annotation",
    "checked": true,
    "id": "acae36d76c5e563914895758d5691b4ab119194f",
    "semantic_title": "reference and coreference in situated dialogue",
    "citation_count": 4,
    "authors": [
      "Sharid Loáiciga",
      "Simon Dobnik",
      "David Schlangen"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.1": {
    "title": "qxoRef 1.0: A coreference corpus and mention-pair baseline for coreference resolution in Conchucos Quechua",
    "volume": "workshop",
    "abstract": "This paper introduces qxoRef 1.0, the first coreference corpus to be developed for a Quechuan language, and describes a baseline mention-pair coreference resolution system developed for this corpus. The evaluation of this system will illustrate that earlier steps in the NLP pipeline, in particular syntactic parsing, should be in place before a complex task like coreference resolution can truly succeed. qxoRef 1.0 is freely available under a CC-BY-NC-SA 4.0 license",
    "checked": true,
    "id": "57197216d0a7b4c276ba0f06ece24f2a5639b7da",
    "semantic_title": "qxoref 1.0: a coreference corpus and mention-pair baseline for coreference resolution in conchucos quechua",
    "citation_count": 1,
    "authors": [
      "Elizabeth Pankratz"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.2": {
    "title": "A corpus of K'iche' annotated for morphosyntactic structure",
    "volume": "workshop",
    "abstract": "This article describes a collection of sentences in K'iche' annotated for morphology and syntax. K'iche' is a language in the Mayan language family, spoken in Guatemala. The annotation is done according to the guidelines of the Universal Dependencies project. The corpus consists of a total of 1,433 sentences containing approximately 10,000 tokens and is released under a free/open-source licence. We present a comparison of parsing systems for K'iche' using this corpus and describe how it can be used for mining linguistic examples",
    "checked": true,
    "id": "036e80b3450242701a2df4a0f96d0129e383d3b8",
    "semantic_title": "a corpus of k'iche' annotated for morphosyntactic structure",
    "citation_count": 12,
    "authors": [
      "Francis Tyers",
      "Robert Henderson"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.3": {
    "title": "Investigating variation in written forms of Nahuatl using character-based language models",
    "volume": "workshop",
    "abstract": "We describe experiments with character-based language modeling for written variants of Nahuatl. Using a standard LSTM model and publicly available Bible translations, we explore how character language models can be applied to the tasks of estimating mutual intelligibility, identifying genetic similarity, and distinguishing written variants. We demonstrate that these simple language models are able to capture similarities and differences that have been described in the linguistic literature",
    "checked": true,
    "id": "7ad73c6e3e8f17eda05d69385007424ad4107194",
    "semantic_title": "investigating variation in written forms of nahuatl using character-based language models",
    "citation_count": 1,
    "authors": [
      "Robert Pugh",
      "Francis Tyers"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.4": {
    "title": "Apurinã Universal Dependencies Treebank",
    "volume": "workshop",
    "abstract": "This paper presents and discusses the first Universal Dependencies treebank for the Apurinã language. The treebank contains 76 fully annotated sentences, applies 14 parts-of-speech, as well as seven augmented or new features — some of which are unique to Apurinã. The construction of the treebank has also served as an opportunity to develop finite-state description of the language and facilitate the transfer of open-source infrastructure possibilities to an endangered language of the Amazon. The source materials used in the initial treebank represent fieldwork practices where not all tokens of all sentences are equally annotated. For this reason, establishing regular annotation practices for the entire Apurinã treebank is an ongoing project",
    "checked": true,
    "id": "94df6f6387eca930d29b0c34c26848e5129dcea8",
    "semantic_title": "apurinã universal dependencies treebank",
    "citation_count": 6,
    "authors": [
      "Jack Rueter",
      "Marília Fernanda Pereira de Freitas",
      "Sidney Da Silva Facundes",
      "Mika Hämäläinen",
      "Niko Partanen"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.5": {
    "title": "Automatic Interlinear Glossing for Otomi language",
    "volume": "workshop",
    "abstract": "In linguistics, interlinear glossing is an essential procedure for analyzing the morphology of languages. This type of annotation is useful for language documentation, and it can also provide valuable data for NLP applications. We perform automatic glossing for Otomi, an under-resourced language. Our work also comprises the pre-processing and annotation of the corpus. We implement different sequential labelers. CRF models represented an efficient and good solution for our task. Two main observations emerged from our work: 1) models with a higher number of parameters (RNNs) performed worse in our low-resource scenario; and 2) the information encoded in the CRF feature function plays an important role in the prediction of labels; however, even in cases where POS tags are not available it is still possible to achieve competitive results",
    "checked": true,
    "id": "67916759dcfae91d2abf5d2a1a3f36686d0cdec4",
    "semantic_title": "automatic interlinear glossing for otomi language",
    "citation_count": 8,
    "authors": [
      "Diego Barriga Martínez",
      "Victor Mijangos",
      "Ximena Gutierrez-Vasques"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.6": {
    "title": "A survey of part-of-speech tagging approaches applied to K'iche",
    "volume": "workshop",
    "abstract": "We study the performance of several popular neural part-of-speech taggers from the Universal Dependencies ecosystem on Mayan languages using a small corpus of 1435 annotated K'iche' sentences consisting of approximately 10,000 tokens, with encouraging results: F1 scores 93%+ on lemmatisation, part-of-speech and morphological feature assignment. The high performance motivates a cross-language part-of-speech tagging study, where K'iche'-trained models are evaluated on two other Mayan languages, Kaqchikel and Uspanteko: performance on Kaqchikel is good, 63-85%, and on Uspanteko modest, 60-71%. Supporting experiments lead us to conclude the relative diversity of morphological features as a plausible explanation for the limiting factors in cross-language tagging performance, providing some direction for future sentence annotation and collection work to support these and other Mayan languages",
    "checked": true,
    "id": "5b7a4b98018e6ae5b98fbce2f66bee8e42b1fa98",
    "semantic_title": "a survey of part-of-speech tagging approaches applied to k'iche",
    "citation_count": 2,
    "authors": [
      "Francis Tyers",
      "Nick Howell"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.7": {
    "title": "Highland Puebla Nahuatl Speech Translation Corpus for Endangered Language Documentation",
    "volume": "workshop",
    "abstract": "Documentation of endangered languages (ELs) has become increasingly urgent as thousands of languages are on the verge of disappearing by the end of the 21st century. One challenging aspect of documentation is to develop machine learning tools to automate the processing of EL audio via automatic speech recognition (ASR), machine translation (MT), or speech translation (ST). This paper presents an open-access speech translation corpus of Highland Puebla Nahuatl (glottocode high1278), an EL spoken in central Mexico. It then addresses machine learning contributions to endangered language documentation and argues for the importance of speech translation as a key element in the documentation process. In our experiments, we observed that state-of-the-art end-to-end ST models could outperform a cascaded ST (ASR > MT) pipeline when translating endangered language documentation materials",
    "checked": true,
    "id": "59f3e3cad309eb4965d67773d68bc2f91b2e376f",
    "semantic_title": "highland puebla nahuatl speech translation corpus for endangered language documentation",
    "citation_count": 9,
    "authors": [
      "Jiatong Shi",
      "Jonathan D. Amith",
      "Xuankai Chang",
      "Siddharth Dalmia",
      "Brian Yan",
      "Shinji Watanabe"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.8": {
    "title": "End-to-End Automatic Speech Recognition: Its Impact on the Workflowin Documenting Yoloxóchitl Mixtec",
    "volume": "workshop",
    "abstract": "This paper describes three open access Yoloxóchitl Mixtec corpora and presents the results and implications of end-to-end automatic speech recognition for endangered language documentation. Two issues are addressed. First, the advantage for ASR accuracy of targeting informational (BPE) units in addition to, or in substitution of, linguistic units (word, morpheme, morae) and then using ROVER for system combination. BPE units consistently outperform linguistic units although the best results are obtained by system combination of different BPE targets. Second, a case is made that for endangered language documentation, ASR contributions should be evaluated according to extrinsic criteria (e.g., positive impact on downstream tasks) and not simply intrinsic metrics (e.g., CER and WER). The extrinsic metric chosen is the level of reduction in the human effort needed to produce high-quality transcriptions for permanent archiving",
    "checked": true,
    "id": "ccbc599ff00c575cb1334e386b4a6fd09aa68b7e",
    "semantic_title": "end-to-end automatic speech recognition: its impact on the workflowin documenting yoloxóchitl mixtec",
    "citation_count": 6,
    "authors": [
      "Jonathan D. Amith",
      "Jiatong Shi",
      "Rey Castillo García"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.9": {
    "title": "A finite-state morphological analyser for Paraguayan Guaraní",
    "volume": "workshop",
    "abstract": "This article describes the development of morphological analyser for Paraguayan Guaraní, agglutinative indigenous language spoken by nearly 6 million people in South America. The implementation of our analyser uses HFST (Helsiki Finite State Technology) and two-level transducer that covers morphotactics and phonological processes occurring in Guaraní. We assess the efficacy of the approach on publicly available Wikipedia and Bible corpora and the naive coverage of analyser reaches 86% on Wikipedia and 91% on Bible corpora",
    "checked": true,
    "id": "ea0a8f987516b7d2cb836d0a5b71a44907371e70",
    "semantic_title": "a finite-state morphological analyser for paraguayan guaraní",
    "citation_count": 6,
    "authors": [
      "Anastasia Kuznetsova",
      "Francis Tyers"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.10": {
    "title": "Morphological Segmentation for Seneca",
    "volume": "workshop",
    "abstract": "This study takes up the task of low-resource morphological segmentation for Seneca, a critically endangered and morphologically complex Native American language primarily spoken in what is now New York State and Ontario. The labeled data in our experiments comes from two sources: one digitized from a publicly available grammar book and the other collected from informal sources. We treat these two sources as distinct domains and investigate different evaluation designs for model selection. The first design abides by standard practices and evaluate models with the in-domain development set, while the second one carries out evaluation using a development domain, or the out-of-domain development set. Across a series of monolingual and crosslinguistic training settings, our results demonstrate the utility of neural encoder-decoder architecture when coupled with multi-task learning",
    "checked": true,
    "id": "767d79df92bb34b3e27e0443c4a1405186c35a2c",
    "semantic_title": "morphological segmentation for seneca",
    "citation_count": 9,
    "authors": [
      "Zoey Liu",
      "Robert Jimerson",
      "Emily Prud’hommeaux"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.11": {
    "title": "Representation of Yine [Arawak] Morphology by Finite State Transducer Formalism",
    "volume": "workshop",
    "abstract": "We represent the complexity of Yine (Arawak) morphology with a finite state transducer (FST) based morphological analyzer. Yine is a low-resource indigenous polysynthetic Peruvian language spoken by approximately 3,000 people and is classified as ‘definitely endangered' by UNESCO. We review Yine morphology focusing on morphophonology, possessive constructions and verbal predicates. Then we develop FSTs to model these components proposing techniques to solve challenging problems such as complex patterns of incorporating open and closed category arguments. This is a work in progress and we still have more to do in the development and verification of our analyzer. Our analyzer will serve both as a tool to better document the Yine language and as a component of natural language processing (NLP) applications such as spell checking and correction",
    "checked": true,
    "id": "dfd9d14c0ec8799b1dc057d1d87d6fb0c0d79e7e",
    "semantic_title": "representation of yine [arawak] morphology by finite state transducer formalism",
    "citation_count": 2,
    "authors": [
      "Adriano Ingunza Torres",
      "John Miller",
      "Arturo Oncevay",
      "Roberto Zariquiey Biondi"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.12": {
    "title": "Leveraging English Word Embeddings for Semi-Automatic Semantic Classification in Nêhiyawêwin (Plains Cree)",
    "volume": "workshop",
    "abstract": "This paper details a semi-automatic method of word clustering for the Algonquian language, Nêhiyawêwin (Plains Cree). Although this method worked well, particularly for nouns, it required some amount of manual postprocessing. The main benefit of this approach over implementing an existing classification ontology is that this method approaches the language from an endogenous point of view, while performing classification quicker than in a fully manual context",
    "checked": true,
    "id": "3b9a5d035b5b712786787de2de002bf7f6f30e25",
    "semantic_title": "leveraging english word embeddings for semi-automatic semantic classification in nêhiyawêwin (plains cree)",
    "citation_count": 4,
    "authors": [
      "Atticus Harrigan",
      "Antti Arppe"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.13": {
    "title": "Restoring the Sister: Reconstructing a Lexicon from Sister Languages using Neural Machine Translation",
    "volume": "workshop",
    "abstract": "The historical comparative method has a long history in historical linguists. It describes a process by which historical linguists aim to reverse-engineer the historical developments of language families in order to reconstruct proto-forms and familial relations between languages. In recent years, there have been multiple attempts to replicate this process through machine learning, especially in the realm of cognate detection (List et al., 2016; Ciobanu and Dinu, 2014; Rama et al., 2018). So far, most of these experiments aimed at actual reconstruction have attempted the prediction of a proto-form from the forms of the daughter languages (Ciobanu and Dinu, 2018; Meloni et al., 2019).. Here, we propose a reimplementation that uses modern related languages, or sisters, instead, to reconstruct the vocabulary of a target language. In particular, we show that we can reconstruct vocabulary of a target language by using a fairly small data set of parallel cognates from different sister languages, using a neural machine translation (NMT) architecture with a standard encoder-decoder setup. This effort is directly in furtherance of the goal to use machine learning tools to help under-served language communities in their efforts at reclaiming, preserving, or reconstructing their own languages",
    "checked": true,
    "id": "991e723f515037305d695ad55e6af8c4f4fcbe3d",
    "semantic_title": "restoring the sister: reconstructing a lexicon from sister languages using neural machine translation",
    "citation_count": 1,
    "authors": [
      "Remo Nitschke"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.14": {
    "title": "Expanding Universal Dependencies for Polysynthetic Languages: A Case of St. Lawrence Island Yupik",
    "volume": "workshop",
    "abstract": "This paper describes the development of the first Universal Dependencies (UD) treebank for St. Lawrence Island Yupik, an endangered language spoken in the Bering Strait region. While the UD guidelines provided a general framework for our annotations, language-specific decisions were made necessary by the rich morphology of the polysynthetic language. Most notably, we annotated a corpus at the morpheme level as well as the word level. The morpheme level annotation was conducted using an existing morphological analyzer and manual disambiguation. By comparing the two resulting annotation schemes, we argue that morpheme-level annotation is essential for polysynthetic languages like St. Lawrence Island Yupik. Word-level annotation results in degenerate trees for some Yupik sentences and often fails to capture syntactic relations that can be manifested at the morpheme level. Dependency parsing experiments provide further support for morpheme-level annotation. Implications for UD annotation of other polysynthetic languages are discussed",
    "checked": true,
    "id": "9ca57d294ef286e2f20a99f6e2db34640350faa0",
    "semantic_title": "expanding universal dependencies for polysynthetic languages: a case of st. lawrence island yupik",
    "citation_count": 6,
    "authors": [
      "Hyunji Hayley Park",
      "Lane Schwartz",
      "Francis Tyers"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.15": {
    "title": "The More Detail, the Better? – Investigating the Effects of Semantic Ontology Specificity on Vector Semantic Classification with a Plains Cree / nêhiyawêwin Dictionary",
    "volume": "workshop",
    "abstract": "One problem in the task of automatic semantic classification is the problem of determining the level on which to group lexical items. This is often accomplished using pre-made, hierarchical semantic ontologies. The following investigation explores the computational assignment of semantic classifications on the contents of a dictionary of nêhiyawêwin / Plains Cree (ISO: crk, Algonquian, Western Canada and United States), using a semantic vector space model, and following two semantic ontologies, WordNet and SIL's Rapid Words, and compares how these computational results compare to manual classifications with the same two ontologies",
    "checked": true,
    "id": "df169dbcd3d4440ca47d332729d98bfa0bb823c6",
    "semantic_title": "the more detail, the better? – investigating the effects of semantic ontology specificity on vector semantic classification with a plains cree / nêhiyawêwin dictionary",
    "citation_count": 3,
    "authors": [
      "Daniel Dacanay",
      "Atticus Harrigan",
      "Arok Wolvengrey",
      "Antti Arppe"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.16": {
    "title": "Experiments on a Guarani Corpus of News and Social Media",
    "volume": "workshop",
    "abstract": "While Guarani is widely spoken in South America, obtaining a large amount of Guarani text from the web is hard. We present the building process of a Guarani corpus composed of a parallel Guarani-Spanish set of news articles, and a monolingual set of tweets. We perform some word embeddings experiments aiming at evaluating the quality of the Guarani split of the corpus, finding encouraging results but noticing that more diversity in text domains might be needed for further improvements",
    "checked": true,
    "id": "906eae0519d880682795637cebc79f3482a64095",
    "semantic_title": "experiments on a guarani corpus of news and social media",
    "citation_count": 8,
    "authors": [
      "Santiago Góngora",
      "Nicolás Giossa",
      "Luis Chiruzzo"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.17": {
    "title": "Towards a First Automatic Unsupervised Morphological Segmentation for Inuinnaqtun",
    "volume": "workshop",
    "abstract": "Low-resource polysynthetic languages pose many challenges in NLP tasks, such as morphological analysis and Machine Translation, due to available resources and tools, and the morphologically complex languages. This research focuses on the morphological segmentation while adapting an unsupervised approach based on Adaptor Grammars in low-resource setting. Experiments and evaluations on Inuinnaqtun, one of Inuit language family in Northern Canada, considered a language that will be extinct in less than two generations, have shown promising results",
    "checked": true,
    "id": "2096313f0a16c10b086319e8a1c4ea065f48bc95",
    "semantic_title": "towards a first automatic unsupervised morphological segmentation for inuinnaqtun",
    "citation_count": 2,
    "authors": [
      "Ngoc Tan Le",
      "Fatiha Sadat"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.18": {
    "title": "Toward Creation of Ancash Lexical Resources from OCR",
    "volume": "workshop",
    "abstract": "The Quechua linguistic family has a limited number of NLP resources, most of them being dedicated to Southern Quechua, whereas the varieties of Central Quechua have, to the best of our knowledge, no specific resources (software, lexicon or corpus). Our work addresses this issue by producing two resources for the Ancash Quechua: a full digital version of a dictionary, and an OCR model adapted to the considered variety. In this paper, we describe the steps towards this goal: we first measure performances of existing models for the task of digitising a Quechua dictionary, then adapt a model for the Ancash variety, and finally create a reliable resource for NLP in XML-TEI format. We hope that this work will be a basis for initiating NLP projects for Central Quechua, and that it will encourage digitisation initiatives for under-resourced languages",
    "checked": true,
    "id": "ecd22f5c0b5965842791df1ee4632200617f01f5",
    "semantic_title": "toward creation of ancash lexical resources from ocr",
    "citation_count": 1,
    "authors": [
      "Johanna Cordova",
      "Damien Nouvel"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.19": {
    "title": "Ayuuk-Spanish Neural Machine Translator",
    "volume": "workshop",
    "abstract": "This paper presents the first neural machine translator system for the Ayuuk language. In our experiments we translate from Ayuuk to Spanish, and fromSpanish to Ayuuk. Ayuuk is a language spoken in the Oaxaca state of Mexico by the Ayuukjä'äy people (in Spanish commonly known as Mixes. We use different sources to create a low-resource parallel corpus, more than 6,000 phrases. For some of these resources we rely on automatic alignment. The proposed system is based on the Transformer neural architecture and it uses sub-word level tokenization as the input. We show the current performance given the resources we have collected for the San Juan Güichicovi variant, they are promising, up to 5 BLEU. We based our development on the Masakhane project for African languages",
    "checked": true,
    "id": "e79c05adb321cba5bc86b1d6d177dbeb79158454",
    "semantic_title": "ayuuk-spanish neural machine translator",
    "citation_count": 3,
    "authors": [
      "Delfino Zacarías Márquez",
      "Ivan Vladimir Meza Ruiz"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.20": {
    "title": "Explicit Tone Transcription Improves ASR Performance in Extremely Low-Resource Languages: A Case Study in Bribri",
    "volume": "workshop",
    "abstract": "Linguistic tone is transcribed for input into ASR systems in numerous ways. This paper shows a systematic test of several transcription styles, using as an example the Chibchan language Bribri, an extremely low-resource language from Costa Rica. The most successful models separate the tone from the vowel, so that the ASR algorithms learn tone patterns independently. These models showed improvements ranging from 4% to 25% in character error rate (CER), and between 3% and 23% in word error rate (WER). This is true for both traditional GMM/HMM and end-to-end CTC algorithms. This paper also presents the first attempt to train ASR models for Bribri. The best performing models had a CER of 33% and a WER of 50%. Despite the disadvantage of using hand-engineered representations, these models were trained on only 68 minutes of data, and therefore show the potential of ASR to generate further training materials and aid in the documentation and revitalization of the language",
    "checked": true,
    "id": "f26a801e946d6e7f72a3b92986bf01e88b780508",
    "semantic_title": "explicit tone transcription improves asr performance in extremely low-resource languages: a case study in bribri",
    "citation_count": 6,
    "authors": [
      "Rolando Coto-Solano"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.21": {
    "title": "Towards a morphological transducer and orthography converter for Western Tlacolula Valley Zapotec",
    "volume": "workshop",
    "abstract": "This paper presents work towards a morphological transducer and orthography converter for Dizhsa, or San Lucas Quiaviní Zapotec, an endangered Western Tlacolula Valley Zapotec language. The implementation of various aspects of the language's morphology is presented, as well as the transducer's ability to perform analysis in two orthographies and convert between them. Potential uses of the transducer for language maintenance and issues of licensing are also discussed. Evaluation of the transducer shows that it is fairly robust although incomplete, and evaluation of orthographic conversion shows that this method is strongly affected by the coverage of the transducer",
    "checked": true,
    "id": "9369dd77ac91cc381380dcf1f92714ce519b09ce",
    "semantic_title": "towards a morphological transducer and orthography converter for western tlacolula valley zapotec",
    "citation_count": 3,
    "authors": [
      "Jonathan Washington",
      "Felipe Lopez",
      "Brook Lillehaugen"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.22": {
    "title": "Peru is Multilingual, Its Machine Translation Should Be Too?",
    "volume": "workshop",
    "abstract": "Peru is a multilingual country with a long history of contact between the indigenous languages and Spanish. Taking advantage of this context for machine translation is possible with multilingual approaches for learning both unsupervised subword segmentation and neural machine translation models. The study proposes the first multilingual translation models for four languages spoken in Peru: Aymara, Ashaninka, Quechua and Shipibo-Konibo, providing both many-to-Spanish and Spanish-to-many models and outperforming pairwise baselines in most of them. The task exploited a large English-Spanish dataset for pre-training, monolingual texts with tagged back-translation, and parallel corpora aligned with English. Finally, by fine-tuning the best models, we also assessed the out-of-domain capabilities in two evaluation datasets for Quechua and a new one for Shipibo-Konibo",
    "checked": true,
    "id": "18d9f575e44618802576fc240af0efd9aa67c7c0",
    "semantic_title": "peru is multilingual, its machine translation should be too?",
    "citation_count": 5,
    "authors": [
      "Arturo Oncevay"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.23": {
    "title": "Findings of the AmericasNLP 2021 Shared Task on Open Machine Translation for Indigenous Languages of the Americas",
    "volume": "workshop",
    "abstract": "This paper presents the results of the 2021 Shared Task on Open Machine Translation for Indigenous Languages of the Americas. The shared task featured two independent tracks, and participants submitted machine translation systems for up to 10 indigenous languages. Overall, 8 teams participated with a total of 214 submissions. We provided training sets consisting of data collected from various sources, as well as manually translated sentences for the development and test sets. An official baseline trained on this data was also provided. Team submissions featured a variety of architectures, including both statistical and neural models, and for the majority of languages, many teams were able to considerably improve over the baseline. The best performing systems achieved 12.97 ChrF higher than baseline, when averaged across languages",
    "checked": true,
    "id": "e073049578b989958607130d21420d5632efc973",
    "semantic_title": "findings of the americasnlp 2021 shared task on open machine translation for indigenous languages of the americas",
    "citation_count": 52,
    "authors": [
      "Manuel Mager",
      "Arturo Oncevay",
      "Abteen Ebrahimi",
      "John Ortega",
      "Annette Rios",
      "Angela Fan",
      "Ximena Gutierrez-Vasques",
      "Luis Chiruzzo",
      "Gustavo Giménez-Lugo",
      "Ricardo Ramos",
      "Ivan Vladimir Meza Ruiz",
      "Rolando Coto-Solano",
      "Alexis Palmer",
      "Elisabeth Mager-Hois",
      "Vishrav Chaudhary",
      "Graham Neubig",
      "Ngoc Thang Vu",
      "Katharina Kann"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.24": {
    "title": "Open Machine Translation for Low Resource South American Languages (AmericasNLP 2021 Shared Task Contribution)",
    "volume": "workshop",
    "abstract": "This paper describes the team (\"Tamalli\")'s submission to AmericasNLP2021 shared task on Open Machine Translation for low resource South American languages. Our goal was to evaluate different Machine Translation (MT) techniques, statistical and neural-based, under several configuration settings. We obtained the second-best results for the language pairs \"Spanish-Bribri\", \"Spanish-Asháninka\", and \"Spanish-Rarámuri\" in the category \"Development set not used for training\". Our performed experiments will serve as a point of reference for researchers working on MT with low-resource languages",
    "checked": true,
    "id": "c13068f16e5b4f9db0c54def0025e619bf49b09e",
    "semantic_title": "open machine translation for low resource south american languages (americasnlp 2021 shared task contribution)",
    "citation_count": 5,
    "authors": [
      "Shantipriya Parida",
      "Subhadarshi Panda",
      "Amulya Dash",
      "Esau Villatoro-Tello",
      "A. Seza Doğruöz",
      "Rosa M. Ortega-Mendoza",
      "Amadeo Hernández",
      "Yashvardhan Sharma",
      "Petr Motlicek"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.25": {
    "title": "NRC-CNRC Machine Translation Systems for the 2021 AmericasNLP Shared Task",
    "volume": "workshop",
    "abstract": "We describe the NRC-CNRC systems submitted to the AmericasNLP shared task on machine translation. We submitted systems translating from Spanish into Wixárika, Nahuatl, Rarámuri, and Guaraní. Our best neural machine translation systems used multilingual pretraining, ensembling, finetuning, training on parts of the development data, and subword regularization. We also submitted translation memory systems as a strong baseline",
    "checked": true,
    "id": "2179c387d5cfef7388770acb83ca2a625d0ce7c0",
    "semantic_title": "nrc-cnrc machine translation systems for the 2021 americasnlp shared task",
    "citation_count": 5,
    "authors": [
      "Rebecca Knowles",
      "Darlene Stewart",
      "Samuel Larkin",
      "Patrick Littell"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.26": {
    "title": "Low-Resource Machine Translation Using Cross-Lingual Language Model Pretraining",
    "volume": "workshop",
    "abstract": "This paper describes UTokyo's submission to the AmericasNLP 2021 Shared Task on machine translation systems for indigenous languages of the Americas. We present a low-resource machine translation system that improves translation accuracy using cross-lingual language model pretraining. Our system uses an mBART implementation of fairseq to pretrain on a large set of monolingual data from a diverse set of high-resource languages before finetuning on 10 low-resource indigenous American languages: Aymara, Bribri, Asháninka, Guaraní, Wixarika, Náhuatl, Hñähñu, Quechua, Shipibo-Konibo, and Rarámuri. On average, our system achieved BLEU scores that were 1.64 higher and chrF scores that were 0.0749 higher than the baseline",
    "checked": true,
    "id": "fe2b0b013f11306edc55181a0e3cc4e0a69fe663",
    "semantic_title": "low-resource machine translation using cross-lingual language model pretraining",
    "citation_count": 14,
    "authors": [
      "Francis Zheng",
      "Machel Reid",
      "Edison Marrese-Taylor",
      "Yutaka Matsuo"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.27": {
    "title": "The REPU CS' Spanish–Quechua Submission to the AmericasNLP 2021 Shared Task on Open Machine Translation",
    "volume": "workshop",
    "abstract": "We present the submission of REPUcs to the AmericasNLP machine translation shared task for the low resource language pair Spanish–Quechua. Our neural machine translation system ranked first in Track two (development set not used for training) and third in Track one (training includes development data). Our contribution is focused on: (i) the collection of new parallel data from different web sources (poems, lyrics, lexicons, handbooks), and (ii) using large Spanish–English data for pre-training and then fine-tuning the Spanish–Quechua system. This paper describes the new parallel corpora and our approach in detail",
    "checked": true,
    "id": "b03f81e4782b9c92c42bba0deb88c2e90dd4c35e",
    "semantic_title": "the repu cs' spanish–quechua submission to the americasnlp 2021 shared task on open machine translation",
    "citation_count": 9,
    "authors": [
      "Oscar Moreno"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.28": {
    "title": "Moses and the Character-Based Random Babbling Baseline: CoAStaL at AmericasNLP 2021 Shared Task",
    "volume": "workshop",
    "abstract": "We evaluated a range of neural machine translation techniques developed specifically for low-resource scenarios. Unsuccessfully. In the end, we submitted two runs: (i) a standard phrase-based model, and (ii) a random babbling baseline using character trigrams. We found that it was surprisingly hard to beat (i), in spite of this model being, in theory, a bad fit for polysynthetic languages; and more interestingly, that (ii) was better than several of the submitted systems, highlighting how difficult low-resource machine translation for polysynthetic languages is",
    "checked": true,
    "id": "464b47a6a395fa1338e230254965cf5f669e715c",
    "semantic_title": "moses and the character-based random babbling baseline: coastal at americasnlp 2021 shared task",
    "citation_count": 5,
    "authors": [
      "Marcel Bollmann",
      "Rahul Aralikatte",
      "Héctor Murrieta Bello",
      "Daniel Hershcovich",
      "Miryam de Lhoneux",
      "Anders Søgaard"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.29": {
    "title": "The Helsinki submission to the AmericasNLP shared task",
    "volume": "workshop",
    "abstract": "The University of Helsinki participated in the AmericasNLP shared task for all ten language pairs. Our multilingual NMT models reached the first rank on all language pairs in track 1, and first rank on nine out of ten language pairs in track 2. We focused our efforts on three aspects: (1) the collection of additional data from various sources such as Bibles and political constitutions, (2) the cleaning and filtering of training data with the OpusFilter toolkit, and (3) different multilingual training techniques enabled by the latest version of the OpenNMT-py toolkit to make the most efficient use of the scarce data. This paper describes our efforts in detail",
    "checked": true,
    "id": "6e835edd25bf78e76b78f330fbdbb1c70bfbc9ab",
    "semantic_title": "the helsinki submission to the americasnlp shared task",
    "citation_count": 17,
    "authors": [
      "Raúl Vázquez",
      "Yves Scherrer",
      "Sami Virpioja",
      "Jörg Tiedemann"
    ]
  },
  "https://aclanthology.org/2021.americasnlp-1.30": {
    "title": "IndT5: A Text-to-Text Transformer for 10 Indigenous Languages",
    "volume": "workshop",
    "abstract": "Transformer language models have become fundamental components of NLP based pipelines. Although several Transformer have been introduced to serve many languages, there is a shortage of models pre-trained for low-resource and Indigenous languages in particular. In this work, we introduce IndT5, the first Transformer language model for Indigenous languages. To train IndT5, we build IndCorpus, a new corpus for 10 Indigenous languages and Spanish. We also present the application of IndT5 to machine translation by investigating different approaches to translate between Spanish and the Indigenous languages as part of our contribution to the AmericasNLP 2021 Shared Task on Open Machine Translation. IndT5 and IndCorpus are publicly available for research",
    "checked": true,
    "id": "6fd41ee12ae828bb7de965bf7e06bb1bd5259fe7",
    "semantic_title": "indt5: a text-to-text transformer for 10 indigenous languages",
    "citation_count": 17,
    "authors": [
      "El Moatez Billah Nagoudi",
      "Wei-Rui Chen",
      "Muhammad Abdul-Mageed",
      "Hasan Cavusoglu"
    ]
  },
  "https://aclanthology.org/2021.autosimtrans-1.1": {
    "title": "ICT's System for AutoSimTrans 2021: Robust Char-Level Simultaneous Translation",
    "volume": "workshop",
    "abstract": "Simultaneous translation (ST) outputs the translation simultaneously while reading the input sentence, which is an important component of simultaneous interpretation. In this paper, we describe our submitted ST system, which won the first place in the streaming transcription input track of the Chinese-English translation task of AutoSimTrans 2021. Aiming at the robustness of ST, we first propose char-level simultaneous translation and applied wait-k policy on it. Meanwhile, we apply two data processing methods and combine two training methods for domain adaptation. Our method enhance the ST model with stronger robustness and domain adaptability. Experiments on streaming transcription show that our method outperforms the baseline at all latency, especially at low latency, the proposed method improves about 6 BLEU. Besides, ablation studies we conduct verify the effectiveness of each module in the proposed method",
    "checked": true,
    "id": "4e7d076a10cae942b4787699238e748f8ab5c0ba",
    "semantic_title": "ict's system for autosimtrans 2021: robust char-level simultaneous translation",
    "citation_count": 13,
    "authors": [
      "Shaolei Zhang",
      "Yang Feng"
    ]
  },
  "https://aclanthology.org/2021.autosimtrans-1.2": {
    "title": "BIT's system for AutoSimulTrans2021",
    "volume": "workshop",
    "abstract": "In this paper we introduce our Chinese-English simultaneous translation system participating in AutoSimulTrans2021. In simultaneous translation, translation quality and delay are both important. In order to reduce the translation delay, we cut the streaming-input source sentence into segments and translate the segments before the full sentence is received. In order to obtain high-quality translations, we pre-train a translation model with adequate corpus and fine-tune the model with domain adaptation and sentence length adaptation. The experimental results on the evaluation data show that our system performs better than the baseline system",
    "checked": true,
    "id": "e0e6f3418042fe19842e595223d066d463d5b74e",
    "semantic_title": "bit's system for autosimultrans2021",
    "citation_count": 0,
    "authors": [
      "Mengge Liu",
      "Shuoying Chen",
      "Minqin Li",
      "Zhipeng Wang",
      "Yuhang Guo"
    ]
  },
  "https://aclanthology.org/2021.autosimtrans-1.3": {
    "title": "XMU's Simultaneous Translation System at NAACL 2021",
    "volume": "workshop",
    "abstract": "This paper describes our two systems submitted to the simultaneous translation evaluation at the 2nd automatic simultaneous translation workshop",
    "checked": true,
    "id": "41cace0fd3e0222cfc741279560e9e6d91464a3c",
    "semantic_title": "xmu's simultaneous translation system at naacl 2021",
    "citation_count": 0,
    "authors": [
      "Shuangtao Li",
      "Jinming Hu",
      "Boli Wang",
      "Xiaodong Shi",
      "Yidong Chen"
    ]
  },
  "https://aclanthology.org/2021.autosimtrans-1.4": {
    "title": "System Description on Automatic Simultaneous Translation Workshop",
    "volume": "workshop",
    "abstract": "This paper shows our submission on the second automatic simultaneous translation workshop at NAACL2021. We participate in all the two directions of Chinese-to-English translation, Chinese audio→English text and Chinese text→English text. We do data filtering and model training techniques to get the best BLEU score and reduce the average lagging. We propose a two-stage simultaneous translation pipeline system which is composed of Quartznet and BPE-based transformer. We propose a competitive simultaneous translation system and achieves a BLEU score of 24.39 in the audio input track",
    "checked": true,
    "id": "3f9a989533ea98ccb350b945f75179b2cfbe0400",
    "semantic_title": "system description on automatic simultaneous translation workshop",
    "citation_count": 1,
    "authors": [
      "Linjie Chen",
      "Jianzong Wang",
      "Zhangcheng Huang",
      "Xiongbin Ding",
      "Jing Xiao"
    ]
  },
  "https://aclanthology.org/2021.autosimtrans-1.5": {
    "title": "BSTC: A Large-Scale Chinese-English Speech Translation Dataset",
    "volume": "workshop",
    "abstract": "This paper presents BSTC (Baidu Speech Translation Corpus), a large-scale Chinese-English speech translation dataset. This dataset is constructed based on a collection of licensed videos of talks or lectures, including about 68 hours of Mandarin data, their manual transcripts and translations into English, as well as automated transcripts by an automatic speech recognition (ASR) model. We have further asked three experienced interpreters to simultaneously interpret the testing talks in a mock conference setting. This corpus is expected to promote the research of automatic simultaneous translation as well as the development of practical systems. We have organized simultaneous translation tasks and used this corpus to evaluate automatic simultaneous translation systems",
    "checked": true,
    "id": "24cffb95674ca6e74d84ca340c26687b898a02d3",
    "semantic_title": "bstc: a large-scale chinese-english speech translation dataset",
    "citation_count": 29,
    "authors": [
      "Ruiqing Zhang",
      "Xiyang Wang",
      "Chuanqiang Zhang",
      "Zhongjun He",
      "Hua Wu",
      "Zhi Li",
      "Haifeng Wang",
      "Ying Chen",
      "Qinfei Li"
    ]
  },
  "https://aclanthology.org/2021.autosimtrans-1.6": {
    "title": "Findings of the Second Workshop on Automatic Simultaneous Translation",
    "volume": "workshop",
    "abstract": "This paper presents the results of the shared task of the 2nd Workshop on Automatic Simultaneous Translation (AutoSimTrans). The task includes two tracks, one for text-to-text translation and one for speech-to-text, requiring participants to build systems to translate from either the source text or speech into the target text. Different from traditional machine translation, the AutoSimTrans shared task evaluates not only translation quality but also latency. We propose a metric \"Monotonic Optimal Sequence\" (MOS) considering both quality and latency to rank the submissions. We also discuss some important open issues in simultaneous translation",
    "checked": true,
    "id": "dde31e7b8f7a1798939e4b907637aaab88b3c79b",
    "semantic_title": "findings of the second workshop on automatic simultaneous translation",
    "citation_count": 2,
    "authors": [
      "Ruiqing Zhang",
      "Chuanqiang Zhang",
      "Zhongjun He",
      "Hua Wu",
      "Haifeng Wang"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.1": {
    "title": "Improving BERT Model Using Contrastive Learning for Biomedical Relation Extraction",
    "volume": "workshop",
    "abstract": "Contrastive learning has been used to learn a high-quality representation of the image in computer vision. However, contrastive learning is not widely utilized in natural language processing due to the lack of a general method of data augmentation for text data. In this work, we explore the method of employing contrastive learning to improve the text representation from the BERT model for relation extraction. The key knob of our framework is a unique contrastive pre-training step tailored for the relation extraction tasks by seamlessly integrating linguistic knowledge into the data augmentation. Furthermore, we investigate how large-scale data constructed from the external knowledge bases can enhance the generality of contrastive pre-training of BERT. The experimental results on three relation extraction benchmark datasets demonstrate that our method can improve the BERT model representation and achieve state-of-the-art performance. In addition, we explore the interpretability of models by showing that BERT with contrastive pre-training relies more on rationales for prediction. Our code and data are publicly available at: https://github.com/AnonymousForNow",
    "checked": true,
    "id": "1dd525b5af40e613ae1665cf15a193b5ef23431b",
    "semantic_title": "improving bert model using contrastive learning for biomedical relation extraction",
    "citation_count": 25,
    "authors": [
      "Peng Su",
      "Yifan Peng",
      "K. Vijay-Shanker"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.2": {
    "title": "Triplet-Trained Vector Space and Sieve-Based Search Improve Biomedical Concept Normalization",
    "volume": "workshop",
    "abstract": "Concept normalization, the task of linking textual mentions of concepts to concepts in an ontology, is critical for mining and analyzing biomedical texts. We propose a vector-space model for concept normalization, where mentions and concepts are encoded via transformer networks that are trained via a triplet objective with online hard triplet mining. The transformer networks refine existing pre-trained models, and the online triplet mining makes training efficient even with hundreds of thousands of concepts by sampling training triples within each mini-batch. We introduce a variety of strategies for searching with the trained vector-space model, including approaches that incorporate domain-specific synonyms at search time with no model retraining. Across five datasets, our models that are trained only once on their corresponding ontologies are within 3 points of state-of-the-art models that are retrained for each new domain. Our models can also be trained for each domain, achieving new state-of-the-art on multiple datasets",
    "checked": true,
    "id": "29612eef06cad0cb0ace846860377bfce271090f",
    "semantic_title": "triplet-trained vector space and sieve-based search improve biomedical concept normalization",
    "citation_count": 6,
    "authors": [
      "Dongfang Xu",
      "Steven Bethard"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.3": {
    "title": "Scalable Few-Shot Learning of Robust Biomedical Name Representations",
    "volume": "workshop",
    "abstract": "Recent research on robust representations of biomedical names has focused on modeling large amounts of fine-grained conceptual distinctions using complex neural encoders. In this paper, we explore the opposite paradigm: training a simple encoder architecture using only small sets of names sampled from high-level biomedical concepts. Our encoder post-processes pretrained representations of biomedical names, and is effective for various types of input representations, both domain-specific or unsupervised. We validate our proposed few-shot learning approach on multiple biomedical relatedness benchmarks, and show that it allows for continual learning, where we accumulate information from various conceptual hierarchies to consistently improve encoder performance. Given these findings, we propose our approach as a low-cost alternative for exploring the impact of conceptual distinctions on robust biomedical name representations",
    "checked": true,
    "id": "757b361ebf05af28fa584d3939147a2c15299407",
    "semantic_title": "scalable few-shot learning of robust biomedical name representations",
    "citation_count": 3,
    "authors": [
      "Pieter Fivez",
      "Simon Suster",
      "Walter Daelemans"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.4": {
    "title": "SAFFRON: tranSfer leArning For Food-disease RelatiOn extractioN",
    "volume": "workshop",
    "abstract": "The accelerating growth of big data in the biomedical domain, with an endless amount of electronic health records and more than 30 million citations and abstracts in PubMed, introduces the need for automatic structuring of textual biomedical data. In this paper, we develop a method for detecting relations between food and disease entities from raw text. Due to the lack of annotated data on food with respect to health, we explore the feasibility of transfer learning by training BERT-based models on existing datasets annotated for the presence of cause and treat relations among different types of biomedical entities, and using them to recognize the same relations between food and disease entities in a dataset created for the purposes of this study. The best models achieve macro averaged F1 scores of 0.847 and 0.900 for the cause and treat relations, respectively",
    "checked": true,
    "id": "7e7d9494689291b89284f90d05c81654221d2faa",
    "semantic_title": "saffron: transfer learning for food-disease relation extraction",
    "citation_count": 7,
    "authors": [
      "Gjorgjina Cenikj",
      "Tome Eftimov",
      "Barbara Koroušić Seljak"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.5": {
    "title": "Are we there yet? Exploring clinical domain knowledge of BERT models",
    "volume": "workshop",
    "abstract": "We explore whether state-of-the-art BERT models encode sufficient domain knowledge to correctly perform domain-specific inference. Although BERT implementations such as BioBERT are better at domain-based reasoning than those trained on general-domain corpora, there is still a wide margin compared to human performance on these tasks. To bridge this gap, we explore whether supplementing textual domain knowledge in the medical NLI task: a) by further language model pretraining on the medical domain corpora, b) by means of lexical match algorithms such as the BM25 algorithm, c) by supplementing lexical retrieval with dependency relations, or d) by using a trained retriever module, can push this performance closer to that of humans. We do not find any significant difference between knowledge supplemented classification as opposed to the baseline BERT models, however. This is contrary to the results for evidence retrieval on other tasks such as open domain question answering (QA). By examining the retrieval output, we show that the methods fail due to unreliable knowledge retrieval for complex domain-specific reasoning. We conclude that the task of unsupervised text retrieval to bridge the gap in existing information to facilitate inference is more complex than what the state-of-the-art methods can solve, and warrants extensive research in the future",
    "checked": true,
    "id": "1b806d23d6e1daee1a7fa8df12b009e5c64bee59",
    "semantic_title": "are we there yet? exploring clinical domain knowledge of bert models",
    "citation_count": 16,
    "authors": [
      "Madhumita Sushil",
      "Simon Suster",
      "Walter Daelemans"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.6": {
    "title": "Towards BERT-based Automatic ICD Coding: Limitations and Opportunities",
    "volume": "workshop",
    "abstract": "Automatic ICD coding is the task of assigning codes from the International Classification of Diseases (ICD) to medical notes. These codes describe the state of the patient and have multiple applications, e.g., computer-assisted diagnosis or epidemiological studies. ICD coding is a challenging task due to the complexity and length of medical notes. Unlike the general trend in language processing, no transformer model has been reported to reach high performance on this task. Here, we investigate in detail ICD coding using PubMedBERT, a state-of-the-art transformer model for biomedical language understanding. We find that the difficulty of fine-tuning the model on long pieces of text is the main limitation for BERT-based models on ICD coding. We run extensive experiments and show that despite the gap with current state-of-the-art, pretrained transformers can reach competitive performance using relatively small portions of text. We point at better methods to aggregate information from long texts as the main need for improving BERT-based ICD coding",
    "checked": true,
    "id": "d4d37ad448e845552583658cc2c3928c82b7045b",
    "semantic_title": "towards bert-based automatic icd coding: limitations and opportunities",
    "citation_count": 41,
    "authors": [
      "Damian Pascual",
      "Sandro Luck",
      "Roger Wattenhofer"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.7": {
    "title": "emrKBQA: A Clinical Knowledge-Base Question Answering Dataset",
    "volume": "workshop",
    "abstract": "We present emrKBQA, a dataset for answering physician questions from a structured patient record. It consists of questions, logical forms and answers. The questions and logical forms are generated based on real-world physician questions and are slot-filled and answered from patients in the MIMIC-III KB through a semi-automated process. This community-shared release consists of over 940000 question, logical form and answer triplets with 389 types of questions and ~7.5 paraphrases per question type. We perform experiments to validate the quality of the dataset and set benchmarks for question to logical form learning that helps answer questions on this dataset",
    "checked": true,
    "id": "c1aa45a570f1d445a70988c6ace89e1b19405cd2",
    "semantic_title": "emrkbqa: a clinical knowledge-base question answering dataset",
    "citation_count": 19,
    "authors": [
      "Preethi Raghavan",
      "Jennifer J Liang",
      "Diwakar Mahajan",
      "Rachita Chandra",
      "Peter Szolovits"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.8": {
    "title": "Overview of the MEDIQA 2021 Shared Task on Summarization in the Medical Domain",
    "volume": "workshop",
    "abstract": "The MEDIQA 2021 shared tasks at the BioNLP 2021 workshop addressed three tasks on summarization for medical text: (i) a question summarization task aimed at exploring new approaches to understanding complex real-world consumer health queries, (ii) a multi-answer summarization task that targeted aggregation of multiple relevant answers to a biomedical question into one concise and relevant answer, and (iii) a radiology report summarization task addressing the development of clinically relevant impressions from radiology report findings. Thirty-five teams participated in these shared tasks with sixteen working notes submitted (fifteen accepted) describing a wide variety of models developed and tested on the shared and external datasets. In this paper, we describe the tasks, the datasets, the models and techniques developed by various teams, the results of the evaluation, and a study of correlations among various summarization evaluation measures. We hope that these shared tasks will bring new research and insights in biomedical text summarization and evaluation",
    "checked": true,
    "id": "19164116fed967ca4dfd0905221f8c5f192a6999",
    "semantic_title": "overview of the mediqa 2021 shared task on summarization in the medical domain",
    "citation_count": 61,
    "authors": [
      "Asma Ben Abacha",
      "Yassine Mrabet",
      "Yuhao Zhang",
      "Chaitanya Shivade",
      "Curtis Langlotz",
      "Dina Demner-Fushman"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.9": {
    "title": "WBI at MEDIQA 2021: Summarizing Consumer Health Questions with Generative Transformers",
    "volume": "workshop",
    "abstract": "This paper describes our contribution for the MEDIQA-2021 Task 1 question summarization competition. We model the task as conditional generation problem. Our concrete pipeline performs a finetuning of the large pretrained generative transformers PEGASUS (Zhang et al.,2020a) and BART (Lewis et al.,2020). We used the resulting models as strong baselines and experimented with (i) integrating structured knowledge via entity embeddings, (ii) ensembling multiple generative models with the generator-discriminator framework and (iii) disentangling summarization and interrogative prediction to achieve further improvements. Our best performing model, a fine-tuned vanilla PEGASUS, reached the second place in the competition with an ROUGE-2-F1 score of 15.99. We observed that all of our additional measures hurt performance (up to 5.2 pp) on the official test set. In course of a post-hoc experimental analysis which uses a larger validation set results indicate slight performance improvements through the proposed extensions. However, further analysis is need to provide stronger evidence",
    "checked": true,
    "id": "6bf2f849838a6f53e2b878540e47add64ab4556c",
    "semantic_title": "wbi at mediqa 2021: summarizing consumer health questions with generative transformers",
    "citation_count": 9,
    "authors": [
      "Mario Sänger",
      "Leon Weber",
      "Ulf Leser"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.10": {
    "title": "paht_nlp @ MEDIQA 2021: Multi-grained Query Focused Multi-Answer Summarization",
    "volume": "workshop",
    "abstract": "In this article, we describe our systems for the MEDIQA 2021 Shared Tasks. First, we will describe our method for the second task, Multi-Answer Summarization (MAS). For extractive summarization, two series of methods are applied. The first one follows (CITATION). First a RoBERTa model is first applied to give a local ranking of the candidate sentences. Then a Markov Chain model is applied to evaluate the sentences globally. The second method applies cross-sentence contextualization to improve the local ranking and discard the global ranking step. Our methods achieve the 1st Place in the MAS task. For the question summarization (QS) and radiology report summarization (RRS) tasks, we explore how end-to-end pre-trained seq2seq model perform. A series of tricks for improving the fine-tuning performances are validated",
    "checked": true,
    "id": "fc5e25efe0a30230b553c817e38bc8308e127a08",
    "semantic_title": "paht_nlp @ mediqa 2021: multi-grained query focused multi-answer summarization",
    "citation_count": 10,
    "authors": [
      "Wei Zhu",
      "Yilong He",
      "Ling Chai",
      "Yunxiao Fan",
      "Yuan Ni",
      "Guotong Xie",
      "Xiaoling Wang"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.11": {
    "title": "BDKG at MEDIQA 2021: System Report for the Radiology Report Summarization Task",
    "volume": "workshop",
    "abstract": "This paper presents our winning system at the Radiology Report Summarization track of the MEDIQA 2021 shared task. Radiology report summarization automatically summarizes radiology findings into free-text impressions. This year's task emphasizes the generalization and transfer ability of participating systems. Our system is built upon a pre-trained Transformer encoder-decoder architecture, i.e., PEGASUS, deployed with an additional domain adaptation module to particularly handle the transfer and generalization issue. Heuristics like ensemble and text normalization are also used. Our system is conceptually simple yet highly effective, achieving a ROUGE-2 score of 0.436 on test set and ranked the 1st place among all participating systems",
    "checked": true,
    "id": "960a0dabd013c307072a550c0c3ff703927dca88",
    "semantic_title": "bdkg at mediqa 2021: system report for the radiology report summarization task",
    "citation_count": 18,
    "authors": [
      "Songtai Dai",
      "Quan Wang",
      "Yajuan Lyu",
      "Yong Zhu"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.12": {
    "title": "damo_nlp at MEDIQA 2021: Knowledge-based Preprocessing and Coverage-oriented Reranking for Medical Question Summarization",
    "volume": "workshop",
    "abstract": "Medical question summarization is an important but difficult task, where the input is often complex and erroneous while annotated data is expensive to acquire. We report our participation in the MEDIQA 2021 question summarization task in which we are required to address these challenges. We start from pre-trained conditional generative language models, use knowledge bases to help correct input errors, and rerank single system outputs to boost coverage. Experimental results show significant improvement in string-based metrics",
    "checked": true,
    "id": "18d1cd7da1ea12eb9bcc7b438e04fd117d5aa370",
    "semantic_title": "damo_nlp at mediqa 2021: knowledge-based preprocessing and coverage-oriented reranking for medical question summarization",
    "citation_count": 12,
    "authors": [
      "Yifan He",
      "Mosha Chen",
      "Songfang Huang"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.13": {
    "title": "Stress Test Evaluation of Biomedical Word Embeddings",
    "volume": "workshop",
    "abstract": "The success of pretrained word embeddings has motivated their use in the biomedical domain, with contextualized embeddings yielding remarkable results in several biomedical NLP tasks. However, there is a lack of research on quantifying their behavior under severe \"stress\" scenarios. In this work, we systematically evaluate three language models with adversarial examples – automatically constructed tests that allow us to examine how robust the models are. We propose two types of stress scenarios focused on the biomedical named entity recognition (NER) task, one inspired by spelling errors and another based on the use of synonyms for medical terms. Our experiments with three benchmarks show that the performance of the original models decreases considerably, in addition to revealing their weaknesses and strengths. Finally, we show that adversarial training causes the models to improve their robustness and even to exceed the original performance in some cases",
    "checked": true,
    "id": "be9cd619a0367e2790ef69df068347b951f137ba",
    "semantic_title": "stress test evaluation of biomedical word embeddings",
    "citation_count": 8,
    "authors": [
      "Vladimir Araujo",
      "Andrés Carvallo",
      "Carlos Aspillaga",
      "Camilo Thorne",
      "Denis Parra"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.14": {
    "title": "BLAR: Biomedical Local Acronym Resolver",
    "volume": "workshop",
    "abstract": "NLP has emerged as an essential tool to extract knowledge from the exponentially increasing volumes of biomedical texts. Many NLP tasks, such as named entity recognition and named entity normalization, are especially challenging in the biomedical domain partly because of the prolific use of acronyms. Long names for diseases, bacteria, and chemicals are often replaced by acronyms. We propose Biomedical Local Acronym Resolver (BLAR), a high-performing acronym resolver that leverages state-of-the-art (SOTA) pre-trained language models to accurately resolve local acronyms in biomedical texts. We test BLAR on the Ab3P corpus and achieve state-of-the-art results compared to the current best-performing local acronym resolution algorithms and models",
    "checked": true,
    "id": "081034b0f2cf52cd98025a232ea69bcd40ffa971",
    "semantic_title": "blar: biomedical local acronym resolver",
    "citation_count": 0,
    "authors": [
      "William Hogan",
      "Yoshiki Vazquez Baeza",
      "Yannis Katsis",
      "Tyler Baldwin",
      "Ho-Cheol Kim",
      "Chun-Nan Hsu"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.15": {
    "title": "Claim Detection in Biomedical Twitter Posts",
    "volume": "workshop",
    "abstract": "Social media contains unfiltered and unique information, which is potentially of great value, but, in the case of misinformation, can also do great harm. With regards to biomedical topics, false information can be particularly dangerous. Methods of automatic fact-checking and fake news detection address this problem, but have not been applied to the biomedical domain in social media yet. We aim to fill this research gap and annotate a corpus of 1200 tweets for implicit and explicit biomedical claims (the latter also with span annotations for the claim phrase). With this corpus, which we sample to be related to COVID-19, measles, cystic fibrosis, and depression, we develop baseline models which detect tweets that contain a claim automatically. Our analyses reveal that biomedical tweets are densely populated with claims (45 % in a corpus sampled to contain 1200 tweets focused on the domains mentioned above). Baseline classification experiments with embedding-based classifiers and BERT-based transfer learning demonstrate that the detection is challenging, however, shows acceptable performance for the identification of explicit expressions of claims. Implicit claim tweets are more challenging to detect",
    "checked": true,
    "id": "cc2fd4bf21b1fe38acb8d4398621d9264835937f",
    "semantic_title": "claim detection in biomedical twitter posts",
    "citation_count": 22,
    "authors": [
      "Amelie Wührl",
      "Roman Klinger"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.16": {
    "title": "BioELECTRA:Pretrained Biomedical text Encoder using Discriminators",
    "volume": "workshop",
    "abstract": "Recent advancements in pretraining strategies in NLP have shown a significant improvement in the performance of models on various text mining tasks. We apply ‘replaced token detection' pretraining technique proposed by ELECTRA and pretrain a biomedical language model from scratch using biomedical text and vocabulary. We introduce BioELECTRA, a biomedical domain-specific language encoder model that adapts ELECTRA for the Biomedical domain. WE evaluate our model on the BLURB and BLUE biomedical NLP benchmarks. BioELECTRA outperforms the previous models and achieves state of the art (SOTA) on all the 13 datasets in BLURB benchmark and on all the 4 Clinical datasets from BLUE Benchmark across 7 different NLP tasks. BioELECTRA pretrained on PubMed and PMC full text articles performs very well on Clinical datasets as well. BioELECTRA achieves new SOTA 86.34%(1.39% accuracy improvement) on MedNLI and 64% (2.98% accuracy improvement) on PubMedQA dataset",
    "checked": true,
    "id": "ee6564e2e0f47c6ac131b093f057ca75907958c9",
    "semantic_title": "bioelectra:pretrained biomedical text encoder using discriminators",
    "citation_count": 94,
    "authors": [
      "Kamal raj Kanakarajan",
      "Bhuvana Kundumani",
      "Malaikannan Sankarasubbu"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.17": {
    "title": "Word centrality constrained representation for keyphrase extraction",
    "volume": "workshop",
    "abstract": "To keep pace with the increased generation and digitization of documents, automated methods that can improve search, discovery and mining of the vast body of literature are essential. Keyphrases provide a concise representation by identifying salient concepts in a document. Various supervised approaches model keyphrase extraction using local context to predict the label for each token and perform much better than the unsupervised counterparts. Unfortunately, this method fails for short documents where the context is unclear. Moreover, keyphrases, which are usually the gist of a document, need to be the central theme. We propose a new extraction model that introduces a centrality constraint to enrich the word representation of a Bidirectional long short-term memory. Performance evaluation on 2 publicly available datasets demonstrate our model outperforms existing state-of-the art approaches",
    "checked": true,
    "id": "81d0b36f8d9009b2a0f06a4545701cdc2c1f1c7e",
    "semantic_title": "word centrality constrained representation for keyphrase extraction",
    "citation_count": 5,
    "authors": [
      "Zelalem Gero",
      "Joyce Ho"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.18": {
    "title": "End-to-end Biomedical Entity Linking with Span-based Dictionary Matching",
    "volume": "workshop",
    "abstract": "Disease name recognition and normalization is a fundamental process in biomedical text mining. Recently, neural joint learning of both tasks has been proposed to utilize the mutual benefits. While this approach achieves high performance, disease concepts that do not appear in the training dataset cannot be accurately predicted. This study introduces a novel end-to-end approach that combines span representations with dictionary-matching features to address this problem. Our model handles unseen concepts by referring to a dictionary while maintaining the performance of neural network-based models. Experiments using two major datasaets demonstrate that our model achieved competitive results with strong baselines, especially for unseen concepts during training",
    "checked": true,
    "id": "04d2471de6bf10bce4ff37e13e68e31b38bf33bf",
    "semantic_title": "end-to-end biomedical entity linking with span-based dictionary matching",
    "citation_count": 8,
    "authors": [
      "Shogo Ujiie",
      "Hayate Iso",
      "Shuntaro Yada",
      "Shoko Wakamiya",
      "Eiji Aramaki"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.19": {
    "title": "Word-Level Alignment of Paper Documents with their Electronic Full-Text Counterparts",
    "volume": "workshop",
    "abstract": "We describe a simple procedure for the automatic creation of word-level alignments between printed documents and their respective full-text versions. The procedure is unsupervised, uses standard, off-the-shelf components only, and reaches an F-score of 85.01 in the basic setup and up to 86.63 when using pre- and post-processing. Potential areas of application are manual database curation (incl. document triage) and biomedical expression OCR",
    "checked": true,
    "id": "98aad36ac0c57b6d5646292203163e8ee4e6716b",
    "semantic_title": "word-level alignment of paper documents with their electronic full-text counterparts",
    "citation_count": 1,
    "authors": [
      "Mark-Christoph Müller",
      "Sucheta Ghosh",
      "Ulrike Wittig",
      "Maja Rey"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.20": {
    "title": "Improving Biomedical Pretrained Language Models with Knowledge",
    "volume": "workshop",
    "abstract": "Pretrained language models have shown success in many natural language processing tasks. Many works explore to incorporate the knowledge into the language models. In the biomedical domain, experts have taken decades of effort on building large-scale knowledge bases. For example, UMLS contains millions of entities with their synonyms and defines hundreds of relations among entities. Leveraging this knowledge can benefit a variety of downstream tasks such as named entity recognition and relation extraction. To this end, we propose KeBioLM, a biomedical pretrained language model that explicitly leverages knowledge from the UMLS knowledge bases. Specifically, we extract entities from PubMed abstracts and link them to UMLS. We then train a knowledge-aware language model that firstly applies a text-only encoding layer to learn entity representation and then applies a text-entity fusion encoding to aggregate entity representation. In addition, we add two training objectives as entity detection and entity linking. Experiments on the named entity recognition and relation extraction tasks from the BLURB benchmark demonstrate the effectiveness of our approach. Further analysis on a collected probing dataset shows that our model has better ability to model medical knowledge",
    "checked": true,
    "id": "58fe64beb45b18f63cbc001849a0dee3e4e60482",
    "semantic_title": "improving biomedical pretrained language models with knowledge",
    "citation_count": 72,
    "authors": [
      "Zheng Yuan",
      "Yijia Liu",
      "Chuanqi Tan",
      "Songfang Huang",
      "Fei Huang"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.21": {
    "title": "EntityBERT: Entity-centric Masking Strategy for Model Pretraining for the Clinical Domain",
    "volume": "workshop",
    "abstract": "Transformer-based neural language models have led to breakthroughs for a variety of natural language processing (NLP) tasks. However, most models are pretrained on general domain data. We propose a methodology to produce a model focused on the clinical domain: continued pretraining of a model with a broad representation of biomedical terminology (PubMedBERT) on a clinical corpus along with a novel entity-centric masking strategy to infuse domain knowledge in the learning process. We show that such a model achieves superior results on clinical extraction tasks by comparing our entity-centric masking strategy with classic random masking on three clinical NLP tasks: cross-domain negation detection, document time relation (DocTimeRel) classification, and temporal relation extraction. We also evaluate our models on the PubMedQA dataset to measure the models' performance on a non-entity-centric task in the biomedical domain. The language addressed in this work is English",
    "checked": true,
    "id": "f60684c39f180093a26cbe568be4c8801be141ae",
    "semantic_title": "entitybert: entity-centric masking strategy for model pretraining for the clinical domain",
    "citation_count": 23,
    "authors": [
      "Chen Lin",
      "Timothy Miller",
      "Dmitriy Dligach",
      "Steven Bethard",
      "Guergana Savova"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.22": {
    "title": "Contextual explanation rules for neural clinical classifiers",
    "volume": "workshop",
    "abstract": "Several previous studies on explanation for recurrent neural networks focus on approaches that find the most important input segments for a network as its explanations. In that case, the manner in which these input segments combine with each other to form an explanatory pattern remains unknown. To overcome this, some previous work tries to find patterns (called rules) in the data that explain neural outputs. However, their explanations are often insensitive to model parameters, which limits the scalability of text explanations. To overcome these limitations, we propose a pipeline to explain RNNs by means of decision lists (also called rules) over skipgrams. For evaluation of explanations, we create a synthetic sepsis-identification dataset, as well as apply our technique on additional clinical and sentiment analysis datasets. We find that our technique persistently achieves high explanation fidelity and qualitatively interpretable rules",
    "checked": true,
    "id": "fd7cc4f3da7aaeed29c9cf220bf46934d4988c67",
    "semantic_title": "contextual explanation rules for neural clinical classifiers",
    "citation_count": 1,
    "authors": [
      "Madhumita Sushil",
      "Simon Suster",
      "Walter Daelemans"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.23": {
    "title": "Exploring Word Segmentation and Medical Concept Recognition for Chinese Medical Texts",
    "volume": "workshop",
    "abstract": "Chinese word segmentation (CWS) and medical concept recognition are two fundamental tasks to process Chinese electronic medical records (EMRs) and play important roles in downstream tasks for understanding Chinese EMRs. One challenge to these tasks is the lack of medical domain datasets with high-quality annotations, especially medical-related tags that reveal the characteristics of Chinese EMRs. In this paper, we collected a Chinese EMR corpus, namely, ACEMR, with human annotations for Chinese word segmentation and EMR-related tags. On the ACEMR corpus, we run well-known models (i.e., BiLSTM, BERT, and ZEN) and existing state-of-the-art systems (e.g., WMSeg and TwASP) for CWS and medical concept recognition. Experimental results demonstrate the necessity of building a dedicated medical dataset and show that models that leverage extra resources achieve the best performance for both tasks, which provides certain guidance for future studies on model selection in the medical domain",
    "checked": true,
    "id": "59589ea891af73fdc9c08c76b1159249a7677d7c",
    "semantic_title": "exploring word segmentation and medical concept recognition for chinese medical texts",
    "citation_count": 5,
    "authors": [
      "Yang Liu",
      "Yuanhe Tian",
      "Tsung-Hui Chang",
      "Song Wu",
      "Xiang Wan",
      "Yan Song"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.24": {
    "title": "BioM-Transformers: Building Large Biomedical Language Models with BERT, ALBERT and ELECTRA",
    "volume": "workshop",
    "abstract": "The impact of design choices on the performance of biomedical language models recently has been a subject for investigation. In this paper, we empirically study biomedical domain adaptation with large transformer models using different design choices. We evaluate the performance of our pretrained models against other existing biomedical language models in the literature. Our results show that we achieve state-of-the-art results on several biomedical domain tasks despite using similar or less computational cost compared to other models in the literature. Our findings highlight the significant effect of design choices on improving the performance of biomedical language models",
    "checked": true,
    "id": "9d2a0337979c62cb6ce337d9977dd9b2020da98d",
    "semantic_title": "biom-transformers: building large biomedical language models with bert, albert and electra",
    "citation_count": 64,
    "authors": [
      "Sultan Alrowili",
      "Vijay Shanker"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.25": {
    "title": "Semi-Supervised Language Models for Identification of Personal Health Experiential from Twitter Data: A Case for Medication Effects",
    "volume": "workshop",
    "abstract": "First-hand experience related to any changes of one's health condition and understanding such experience can play an important role in advancing medical science and healthcare. Monitoring the safe use of medication drugs is an important task of pharmacovigilance, and first-hand experience of effects about consumers' medication intake can be valuable to gain insight into how our human body reacts to medications. Social media have been considered as a possible alternative data source for gathering personal experience with medications posted by users. Identifying personal experience tweets is a challenging classification task, and efforts have made to tackle the challenges using supervised approaches requiring annotated data. There exists abundance of unlabeled Twitter data, and being able to use such data for training without suffering in classification performance is of great value, which can reduce the cost of laborious annotation process. We investigated two semi-supervised learning methods, with different mixes of labeled and unlabeled data in the training set, to understand the impact on classification performance. Our results from both pseudo-label and consistency regularization methods show that both methods generated a noticeable improvement in F1 score when the labeled set was small, and consistency regularization could still provide a small gain even a larger labeled set was used",
    "checked": true,
    "id": "a9ed3e22335597b37f94f9db6ecbc67ed61ff7c4",
    "semantic_title": "semi-supervised language models for identification of personal health experiential from twitter data: a case for medication effects",
    "citation_count": 2,
    "authors": [
      "Minghao Zhu",
      "Keyuan Jiang"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.26": {
    "title": "Context-aware query design combines knowledge and data for efficient reading and reasoning",
    "volume": "workshop",
    "abstract": "The amount of biomedical literature has vastly increased over the past few decades. As a result, the sheer quantity of accessible information is overwhelming, and complicates manual information retrieval. Automated methods seek to speed up information retrieval from biomedical literature. However, such automated methods are still too time-intensive to survey all existing biomedical literature. We present a methodology for automatically generating literature queries that select relevant papers based on biological data. By using differentially expressed genes to inform our literature searches, we focus information extraction on mechanistic signaling details that are crucial for the disease or context of interest",
    "checked": true,
    "id": "b56b081f3147280f72387ac401f2aec6a43cd4e0",
    "semantic_title": "context-aware query design combines knowledge and data for efficient reading and reasoning",
    "citation_count": 0,
    "authors": [
      "Emilee Holtzapple",
      "Brent Cochran",
      "Natasa Miskov-Zivanov"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.27": {
    "title": "Measuring the relative importance of full text sections for information retrieval from scientific literature",
    "volume": "workshop",
    "abstract": "With the growing availability of full-text articles, integrating abstracts and full texts of documents into a unified representation is essential for comprehensive search of scientific literature. However, previous studies have shown that naïvely merging abstracts with full texts of articles does not consistently yield better performance. Balancing the contribution of query terms appearing in the abstract and in sections of different importance in full text articles remains a challenge both with traditional bag-of-words IR approaches and for neural retrieval methods. In this work we establish the connection between the BM25 score of a query term appearing in a section of a full text document and the probability of that document being clicked or identified as relevant. Probability is computed using Pool Adjacent Violators (PAV), an isotonic regression algorithm, providing a maximum likelihood estimate based on the observed data. Using this probabilistic transformation of BM25 scores we show an improved performance on the PubMed Click dataset developed and presented in this study, as well as the 2007 TREC Genomics collection",
    "checked": true,
    "id": "27c12b8d9cfe4e88e513a53e620094e3a87a6ab2",
    "semantic_title": "measuring the relative importance of full text sections for information retrieval from scientific literature",
    "citation_count": 3,
    "authors": [
      "Lana Yeganova",
      "Won Gyu Kim",
      "Donald Comeau",
      "W John Wilbur",
      "Zhiyong Lu"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.28": {
    "title": "UCSD-Adobe at MEDIQA 2021: Transfer Learning and Answer Sentence Selection for Medical Summarization",
    "volume": "workshop",
    "abstract": "In this paper, we describe our approach to question summarization and multi-answer summarization in the context of the 2021 MEDIQA shared task (Ben Abacha et al., 2021). We propose two kinds of transfer learning for the abstractive summarization of medical questions. First, we train on HealthCareMagic, a large question summarization dataset collected from an online healthcare service platform. Second, we leverage the ability of the BART encoder-decoder architecture to model both generation and classification tasks to train on the task of Recognizing Question Entailment (RQE) in the medical domain. We show that both transfer learning methods combined achieve the highest ROUGE scores. Finally, we cast the question-driven extractive summarization of multiple relevant answer documents as an Answer Sentence Selection (AS2) problem. We show how we can preprocess the MEDIQA-AnS dataset such that it can be trained in an AS2 setting. Our AS2 model is able to generate extractive summaries achieving high ROUGE scores",
    "checked": true,
    "id": "4061f3c5e32589c0a9a6569708b48ec5378ed12a",
    "semantic_title": "ucsd-adobe at mediqa 2021: transfer learning and answer sentence selection for medical summarization",
    "citation_count": 12,
    "authors": [
      "Khalil Mrini",
      "Franck Dernoncourt",
      "Seunghyun Yoon",
      "Trung Bui",
      "Walter Chang",
      "Emilia Farcas",
      "Ndapa Nakashole"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.29": {
    "title": "ChicHealth @ MEDIQA 2021: Exploring the limits of pre-trained seq2seq models for medical summarization",
    "volume": "workshop",
    "abstract": "In this article, we will describe our system for MEDIQA2021 shared tasks. First, we will describe the method of the second task, multiple answer summary (MAS). For extracting abstracts, we follow the rules of (CITATION). First, the candidate sentences are roughly estimated by using the Roberta model. Then the Markov chain model is used to evaluate the sentences in a fine-grained manner. Our team won the first place in overall performance, with the fourth place in MAS task, the seventh place in RRS task and the eleventh place in QS task. For the QS and RRS tasks, we investigate the performanceS of the end-to-end pre-trained seq2seq model. Experiments show that the methods of adversarial training and reverse translation are beneficial to improve the fine tuning performance",
    "checked": true,
    "id": "1a2208420c24e39c3ea3f97bb19820c65d9e1ce3",
    "semantic_title": "chichealth @ mediqa 2021: exploring the limits of pre-trained seq2seq models for medical summarization",
    "citation_count": 9,
    "authors": [
      "Liwen Xu",
      "Yan Zhang",
      "Lei Hong",
      "Yi Cai",
      "Szui Sung"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.30": {
    "title": "NCUEE-NLP at MEDIQA 2021: Health Question Summarization Using PEGASUS Transformers",
    "volume": "workshop",
    "abstract": "This study describes the model design of the NCUEE-NLP system for the MEDIQA challenge at the BioNLP 2021 workshop. We use the PEGASUS transformers and fine-tune the downstream summarization task using our collected and processed datasets. A total of 22 teams participated in the consumer health question summarization task of MEDIQA 2021. Each participating team was allowed to submit a maximum of ten runs. Our best submission, achieving a ROUGE2-F1 score of 0.1597, ranked third among all 128 submissions",
    "checked": true,
    "id": "78abeca3cc4fbb9b022506328f0f2ebf14035804",
    "semantic_title": "ncuee-nlp at mediqa 2021: health question summarization using pegasus transformers",
    "citation_count": 4,
    "authors": [
      "Lung-Hao Lee",
      "Po-Han Chen",
      "Yu-Xiang Zeng",
      "Po-Lei Lee",
      "Kuo-Kai Shyu"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.31": {
    "title": "SB_NITK at MEDIQA 2021: Leveraging Transfer Learning for Question Summarization in Medical Domain",
    "volume": "workshop",
    "abstract": "Recent strides in the healthcare domain, have resulted in vast quantities of streaming data available for use for building intelligent knowledge-based applications. However, the challenges introduced to the huge volume, velocity of generation, variety and variability of this medical data have to be adequately addressed. In this paper, we describe the model and results for our submission at MEDIQA 2021 Question Summarization shared task. In order to improve the performance of summarization of consumer health questions, our method explores the use of transfer learning to utilize the knowledge of NLP transformers like BART, T5 and PEGASUS. The proposed models utilize the knowledge of pre-trained NLP transformers to achieve improved results when compared to conventional deep learning models such as LSTM, RNN etc. Our team SB_NITK ranked 12th among the total 22 submissions in the official final rankings. Our BART based model achieved a ROUGE-2 F1 score of 0.139",
    "checked": true,
    "id": "766803b7ed95c564b04165fd9c175ff23c85d0a2",
    "semantic_title": "sb_nitk at mediqa 2021: leveraging transfer learning for question summarization in medical domain",
    "citation_count": 6,
    "authors": [
      "Spandana Balumuri",
      "Sony Bachina",
      "Sowmya Kamath S"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.32": {
    "title": "Optum at MEDIQA 2021: Abstractive Summarization of Radiology Reports using simple BART Finetuning",
    "volume": "workshop",
    "abstract": "This paper describes experiments undertaken and their results as part of the BioNLP MEDIQA 2021 challenge. We participated in Task 3: Radiology Report Summarization. Multiple runs were submitted for evaluation, from solutions leveraging transfer learning from pre-trained transformer models, which were then fine tuned on a subset of MIMIC-CXR, for abstractive report summarization. The task was evaluated using ROUGE and our best performing system obtained a ROUGE-2 score of 0.392",
    "checked": true,
    "id": "e8f74c4ce0d230b016c49a1d473618650c126208",
    "semantic_title": "optum at mediqa 2021: abstractive summarization of radiology reports using simple bart finetuning",
    "citation_count": 10,
    "authors": [
      "Ravi Kondadadi",
      "Sahil Manchanda",
      "Jason Ngo",
      "Ronan McCormack"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.33": {
    "title": "QIAI at MEDIQA 2021: Multimodal Radiology Report Summarization",
    "volume": "workshop",
    "abstract": "This paper describes the solution of the QIAI lab sent to the Radiology Report Summarization (RRS) challenge at MEDIQA 2021. This paper aims to investigate whether using multimodality during training improves the summarizing performances of the model at test-time. Our preliminary results shows that taking advantage of the visual features from the x-rays associated to the radiology reports leads to higher evaluation metrics compared to a text-only baseline system. These improvements are reported according to the automatic evaluation metrics METEOR, BLEU and ROUGE scores. Our experiments can be fully replicated at the following address: https://github.com/jbdel/vilmedic",
    "checked": true,
    "id": "e6dea138484a59f7b0b42464f5f48c587c6894f3",
    "semantic_title": "qiai at mediqa 2021: multimodal radiology report summarization",
    "citation_count": 19,
    "authors": [
      "Jean-Benoit Delbrouck",
      "Cassie Zhang",
      "Daniel Rubin"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.34": {
    "title": "NLM at MEDIQA 2021: Transfer Learning-based Approaches for Consumer Question and Multi-Answer Summarization",
    "volume": "workshop",
    "abstract": "The quest for seeking health information has swamped the web with consumers' healthrelated questions, which makes the need for efficient and reliable question answering systems more pressing. The consumers' questions, however, are very descriptive and contain several peripheral information (like patient's medical history, demographic information, etc.), that are often not required for answering the question. Furthermore, it contributes to the challenges of understanding natural language questions for automatic answer retrieval. Also, it is crucial to provide the consumers with the exact and relevant answers, rather than the entire pool of answer documents to their question. One of the cardinal tasks in achieving robust consumer health question answering systems is the question summarization and multi-document answer summarization. This paper describes the participation of the U.S. National Library of Medicine (NLM) in Consumer Question and Multi-Answer Summarization tasks of the MEDIQA 2021 challenge at NAACL-BioNLP workshop. In this work, we exploited the capabilities of pre-trained transformer models and introduced a transfer learning approach for the abstractive Question Summarization and extractive Multi-Answer Summarization tasks by first pre-training our model on a task-specific summarization dataset followed by fine-tuning it for both the tasks via incorporating medical entities. We achieved the second, sixth and the fourth position for the Question Summarization task in terms ROUGE-1, ROUGE-2 and ROUGE-L scores respectively",
    "checked": true,
    "id": "99a2a9e956b24c62222a20d63644d1c1edb838a5",
    "semantic_title": "nlm at mediqa 2021: transfer learning-based approaches for consumer question and multi-answer summarization",
    "citation_count": 15,
    "authors": [
      "Shweta Yadav",
      "Mourad Sarrouti",
      "Deepak Gupta"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.35": {
    "title": "IBMResearch at MEDIQA 2021: Toward Improving Factual Correctness of Radiology Report Abstractive Summarization",
    "volume": "workshop",
    "abstract": "Although recent advances in abstractive summarization systems have achieved high scores on standard natural language metrics like ROUGE, their lack of factual consistency remains an open challenge for their use in sensitive real-world settings such as clinical practice. In this work, we propose a novel approach to improve factual correctness of a summarization system by re-ranking the candidate summaries based on a factual vector of the summary. We applied this process during our participation in MEDIQA 2021 Task 3: Radiology Report Summarization, where the task is to generate an impression summary of a radiology report, given findings and background as inputs. In our system, we first used a transformer-based encoder-decoder model to generate top N candidate impression summaries for a report, then trained another transformer-based model to predict a 14-observations-vector of the impression based on the findings and background of the report, and finally, utilized this vector to re-rank the candidate summaries. We also employed a source-specific ensembling technique to accommodate for distinct writing styles from different radiology report sources. Our approach yielded 2nd place in the challenge",
    "checked": true,
    "id": "f056922cb62d242a1da2e045ef50048111a3e82a",
    "semantic_title": "ibmresearch at mediqa 2021: toward improving factual correctness of radiology report abstractive summarization",
    "citation_count": 9,
    "authors": [
      "Diwakar Mahajan",
      "Ching-Huei Tsou",
      "Jennifer J Liang"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.36": {
    "title": "UETrice at MEDIQA 2021: A Prosper-thy-neighbour Extractive Multi-document Summarization Model",
    "volume": "workshop",
    "abstract": "This paper describes a system developed to summarize multiple answers challenge in the MEDIQA 2021 shared task collocated with the BioNLP 2021 Workshop. We propose an extractive summarization architecture based on several scores and state-of-the-art techniques. We also present our novel prosper-thy-neighbour strategies to improve performance. Our model has been proven to be effective with the best ROUGE-1/ROUGE-L scores, being the shared task runner up by ROUGE-2 F1 score (over 13 participated teams)",
    "checked": true,
    "id": "d992f6d95546fb4c91c35a3177fa5d311599edf4",
    "semantic_title": "uetrice at mediqa 2021: a prosper-thy-neighbour extractive multi-document summarization model",
    "citation_count": 4,
    "authors": [
      "Duy-Cat Can",
      "Quoc-An Nguyen",
      "Quoc-Hung Duong",
      "Minh-Quang Nguyen",
      "Huy-Son Nguyen",
      "Linh Nguyen Tran Ngoc",
      "Quang-Thuy Ha",
      "Mai-Vu Tran"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.37": {
    "title": "MNLP at MEDIQA 2021: Fine-Tuning PEGASUS for Consumer Health Question Summarization",
    "volume": "workshop",
    "abstract": "This paper details a Consumer Health Question (CHQ) summarization model submitted to MEDIQA 2021 for shared task 1: Question Summarization. Many CHQs are composed of multiple sentences with typos or unnecessary information, which can interfere with automated question answering systems. Question summarization mitigates this issue by removing this unnecessary information, aiding automated systems in generating a more accurate summary. Our summarization approach focuses on applying multiple pre-processing techniques, including question focus identification on the input and the development of an ensemble method to combine question focus with an abstractive summarization method. We use the state-of-art abstractive summarization model, PEGASUS (Pre-training with Extracted Gap-sentences for Abstractive Summarization), to generate abstractive summaries. Our experiments show that using our ensemble method, which combines abstractive summarization with question focus identification, improves performance over using summarization alone. Our model shows a ROUGE-2 F-measure of 11.14% against the official test dataset",
    "checked": true,
    "id": "277469e34c7d946a73d0548f649e0829d73149c9",
    "semantic_title": "mnlp at mediqa 2021: fine-tuning pegasus for consumer health question summarization",
    "citation_count": 4,
    "authors": [
      "Jooyeon Lee",
      "Huong Dang",
      "Ozlem Uzuner",
      "Sam Henry"
    ]
  },
  "https://aclanthology.org/2021.bionlp-1.38": {
    "title": "UETfishes at MEDIQA 2021: Standing-on-the-Shoulders-of-Giants Model for Abstractive Multi-answer Summarization",
    "volume": "workshop",
    "abstract": "This paper describes a system developed to summarize multiple answers challenge in the MEDIQA 2021 shared task collocated with the BioNLP 2021 Workshop. We present an abstractive summarization model based on BART, a denoising auto-encoder for pre-training sequence-to-sequence models. As focusing on the summarization of answers to consumer health questions, we propose a query-driven filtering phase to choose useful information from the input document automatically. Our approach achieves potential results, rank no.2 (evaluated on extractive references) and no.3 (evaluated on abstractive references) in the final evaluation",
    "checked": true,
    "id": "802c4f034e9f353c0e9800c63ac36f0ee167bfcc",
    "semantic_title": "uetfishes at mediqa 2021: standing-on-the-shoulders-of-giants model for abstractive multi-answer summarization",
    "citation_count": 2,
    "authors": [
      "Hoang-Quynh Le",
      "Quoc-An Nguyen",
      "Quoc-Hung Duong",
      "Minh-Quang Nguyen",
      "Huy-Son Nguyen",
      "Tam Doan Thanh",
      "Hai-Yen Thi Vuong",
      "Trang M. Nguyen"
    ]
  },
  "https://aclanthology.org/2021.calcs-1.1": {
    "title": "Political Discourse Analysis: A Case Study of Code Mixing and Code Switching in Political Speeches",
    "volume": "workshop",
    "abstract": "Political discourse is one of the most interesting data to study power relations in the framework of Critical Discourse Analysis. With the increase in the modes of textual and spoken forms of communication, politicians use language and linguistic mechanisms that contribute significantly in building their relationship with people, especially in a multilingual country like India with many political parties with different ideologies. This paper analyses code-mixing and code-switching in Telugu political speeches to determine the factors responsible for their usage levels in various social settings and communicative contexts. We also compile a detailed set of rules capturing dialectal variations between Standard and Telangana dialects of Telugu",
    "checked": true,
    "id": "7a989d1fbbe7185ab235bfe8ad437989f2efd455",
    "semantic_title": "political discourse analysis: a case study of code mixing and code switching in political speeches",
    "citation_count": 6,
    "authors": [
      "Dama Sravani",
      "Lalitha Kameswari",
      "Radhika Mamidi"
    ]
  },
  "https://aclanthology.org/2021.calcs-1.2": {
    "title": "Challenges and Limitations with the Metrics Measuring the Complexity of Code-Mixed Text",
    "volume": "workshop",
    "abstract": "Code-mixing is a frequent communication style among multilingual speakers where they mix words and phrases from two different languages in the same utterance of text or speech. Identifying and filtering code-mixed text is a challenging task due to its co-existence with monolingual and noisy text. Over the years, several code-mixing metrics have been extensively used to identify and validate code-mixed text quality. This paper demonstrates several inherent limitations of code-mixing metrics with examples from the already existing datasets that are popularly used across various experiments",
    "checked": true,
    "id": "63d1d0995ed04ceb08f58294813de12ec156ca37",
    "semantic_title": "challenges and limitations with the metrics measuring the complexity of code-mixed text",
    "citation_count": 14,
    "authors": [
      "Vivek Srivastava",
      "Mayank Singh"
    ]
  },
  "https://aclanthology.org/2021.calcs-1.3": {
    "title": "Translate and Classify: Improving Sequence Level Classification for English-Hindi Code-Mixed Data",
    "volume": "workshop",
    "abstract": "Code-mixing is a common phenomenon in multilingual societies around the world and is especially common in social media texts. Traditional NLP systems, usually trained on monolingual corpora, do not perform well on code-mixed texts. Training specialized models for code-switched texts is difficult due to the lack of large-scale datasets. Translating code-mixed data into standard languages like English could improve performance on various code-mixed tasks since we can use transfer learning from state-of-the-art English models for processing the translated data. This paper focuses on two sequence-level classification tasks for English-Hindi code mixed texts, which are part of the GLUECoS benchmark - Natural Language Inference and Sentiment Analysis. We propose using various pre-trained models that have been fine-tuned for similar English-only tasks and have shown state-of-the-art performance. We further fine-tune these models on the translated code-mixed datasets and achieve state-of-the-art performance in both tasks. To translate English-Hindi code-mixed data to English, we use mBART, a pre-trained multilingual sequence-to-sequence model that has shown competitive performance on various low-resource machine translation pairs and has also shown performance gains in languages that were not in its pre-training corpus",
    "checked": true,
    "id": "717337fd461d3f32894d2a1e87a2b3b3bdc22cb7",
    "semantic_title": "translate and classify: improving sequence level classification for english-hindi code-mixed data",
    "citation_count": 4,
    "authors": [
      "Devansh Gautam",
      "Kshitij Gupta",
      "Manish Shrivastava"
    ]
  },
  "https://aclanthology.org/2021.calcs-1.4": {
    "title": "Gated Convolutional Sequence to Sequence Based Learning for English-Hingilsh Code-Switched Machine Translation",
    "volume": "workshop",
    "abstract": "Code-Switching is the embedding of linguistic units or phrases from two or more languages in a single sentence. This phenomenon is practiced in all multilingual communities and is prominent in social media. Consequently, there is a growing need to understand code-switched translations by translating the code-switched text into one of the standard languages or vice versa. Neural Machine translation is a well-studied research problem in the monolingual text. In this paper, we have used the gated convolutional sequences to sequence networks for English-Hinglish translation. The convolutions in the model help to identify the compositional structure in the sequences more easily. The model relies on gating and performs multiple attention steps at encoder and decoder layers",
    "checked": true,
    "id": "0bbbd92605b17e669832577784f72f2b900d647a",
    "semantic_title": "gated convolutional sequence to sequence based learning for english-hingilsh code-switched machine translation",
    "citation_count": 5,
    "authors": [
      "Suman Dowlagar",
      "Radhika Mamidi"
    ]
  },
  "https://aclanthology.org/2021.calcs-1.5": {
    "title": "IITP-MT at CALCS2021: English to Hinglish Neural Machine Translation using Unsupervised Synthetic Code-Mixed Parallel Corpus",
    "volume": "workshop",
    "abstract": "This paper describes the system submitted by IITP-MT team to Computational Approaches to Linguistic Code-Switching (CALCS 2021) shared task on MT for English→Hinglish. We submit a neural machine translation (NMT) system which is trained on the synthetic code-mixed (cm) English-Hinglish parallel corpus. We propose an approach to create code-mixed parallel corpus from a clean parallel corpus in an unsupervised manner. It is an alignment based approach and we do not use any linguistic resources for explicitly marking any token for code-switching. We also train NMT model on the gold corpus provided by the workshop organizers augmented with the generated synthetic code-mixed parallel corpus. The model trained over the generated synthetic cm data achieves 10.09 BLEU points over the given test set",
    "checked": true,
    "id": "0d049df1120337024af28c05a247f6c6534e9bff",
    "semantic_title": "iitp-mt at calcs2021: english to hinglish neural machine translation using unsupervised synthetic code-mixed parallel corpus",
    "citation_count": 8,
    "authors": [
      "Ramakrishna Appicharla",
      "Kamal Kumar Gupta",
      "Asif Ekbal",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2021.calcs-1.6": {
    "title": "Exploring Text-to-Text Transformers for English to Hinglish Machine Translation with Synthetic Code-Mixing",
    "volume": "workshop",
    "abstract": "We describe models focused at the understudied problem of translating between monolingual and code-mixed language pairs. More specifically, we offer a wide range of models that convert monolingual English text into Hinglish (code-mixed Hindi and English). Given the recent success of pretrained language models, we also test the utility of two recent Transformer-based encoder-decoder models (i.e., mT5 and mBART) on the task finding both to work well. Given the paucity of training data for code-mixing, we also propose a dependency-free method for generating code-mixed texts from bilingual distributed representations that we exploit for improving language model performance. In particular, armed with this additional data, we adopt a curriculum learning approach where we first finetune the language models on synthetic data then on gold code-mixed data. We find that, although simple, our synthetic code-mixing method is competitive with (and in some cases is even superior to) several standard methods (backtranslation, method based on equivalence constraint theory) under a diverse set of conditions. Our work shows that the mT5 model, finetuned following the curriculum learning procedure, achieves best translation performance (12.67 BLEU). Our models place first in the overall ranking of the English-Hinglish official shared task",
    "checked": true,
    "id": "3fdd66a0eed6ee259c1c91321f866ed1f737f340",
    "semantic_title": "exploring text-to-text transformers for english to hinglish machine translation with synthetic code-mixing",
    "citation_count": 25,
    "authors": [
      "Ganesh Jawahar",
      "El Moatez Billah Nagoudi",
      "Muhammad Abdul-Mageed",
      "Laks Lakshmanan, V.S."
    ]
  },
  "https://aclanthology.org/2021.calcs-1.7": {
    "title": "CoMeT: Towards Code-Mixed Translation Using Parallel Monolingual Sentences",
    "volume": "workshop",
    "abstract": "Code-mixed languages are very popular in multilingual societies around the world, yet the resources lag behind to enable robust systems on such languages. A major contributing factor is the informal nature of these languages which makes it difficult to collect code-mixed data. In this paper, we propose our system for Task 1 of CACLS 2021 to generate a machine translation system for English to Hinglish in a supervised setting. Translating in the given direction can help expand the set of resources for several tasks by translating valuable datasets from high resource languages. We propose to use mBART, a pre-trained multilingual sequence-to-sequence model, and fully utilize the pre-training of the model by transliterating the roman Hindi words in the code-mixed sentences to Devanagri script. We evaluate how expanding the input by concatenating Hindi translations of the English sentences improves mBART's performance. Our system gives a BLEU score of 12.22 on test set. Further, we perform a detailed error analysis of our proposed systems and explore the limitations of the provided dataset and metrics",
    "checked": true,
    "id": "43fc602a2193a4808dc42eddcf4ae1893187e5c3",
    "semantic_title": "comet: towards code-mixed translation using parallel monolingual sentences",
    "citation_count": 31,
    "authors": [
      "Devansh Gautam",
      "Prashant Kodali",
      "Kshitij Gupta",
      "Anmol Goel",
      "Manish Shrivastava",
      "Ponnurangam Kumaraguru"
    ]
  },
  "https://aclanthology.org/2021.calcs-1.8": {
    "title": "Investigating Code-Mixed Modern Standard Arabic-Egyptian to English Machine Translation",
    "volume": "workshop",
    "abstract": "Recent progress in neural machine translation (NMT) has made it possible to translate successfully between monolingual language pairs where large parallel data exist, with pre-trained models improving performance even further. Although there exists work on translating in code-mixed settings (where one of the pairs includes text from two or more languages), it is still unclear what recent success in NMT and language modeling exactly means for translating code-mixed text. We investigate one such context, namely MT from code-mixed Modern Standard Arabic and Egyptian Arabic (MSAEA) into English. We develop models under different conditions, employing both (i) standard end-to-end sequence-to-sequence (S2S) Transformers trained from scratch and (ii) pre-trained S2S language models (LMs). We are able to acquire reasonable performance using only MSA-EN parallel data with S2S models trained from scratch. We also find LMs fine-tuned on data from various Arabic dialects to help the MSAEA-EN task. Our work is in the context of the Shared Task on Machine Translation in Code-Switching. Our best model achieves 25.72 BLEU, placing us first on the official shared task evaluation for MSAEA-EN",
    "checked": true,
    "id": "d511dc0284e5e6e5c6577d36635c97ca680f6909",
    "semantic_title": "investigating code-mixed modern standard arabic-egyptian to english machine translation",
    "citation_count": 9,
    "authors": [
      "El Moatez Billah Nagoudi",
      "AbdelRahim Elmadany",
      "Muhammad Abdul-Mageed"
    ]
  },
  "https://aclanthology.org/2021.calcs-1.9": {
    "title": "Much Gracias: Semi-supervised Code-switch Detection for Spanish-English: How far can we get?",
    "volume": "workshop",
    "abstract": "Because of globalization, it is becoming more and more common to use multiple languages in a single utterance, also called code-switching. This results in special linguistic structures and, therefore, poses many challenges for Natural Language Processing. Existing models for language identification in code-switched data are all supervised, requiring annotated training data which is only available for a limited number of language pairs. In this paper, we explore semi-supervised approaches, that exploit out-of-domain mono-lingual training data. We experiment with word uni-grams, word n-grams, character n-grams, Viterbi Decoding, Latent Dirichlet Allocation, Support Vector Machine and Logistic Regression. The Viterbi model was the best semi-supervised model, scoring a weighted F1 score of 92.23%, whereas a fully supervised state-of-the-art BERT-based model scored 98.43%",
    "checked": true,
    "id": "ada29cb31f820c5958d194b43eee298b5ab4c32b",
    "semantic_title": "much gracias: semi-supervised code-switch detection for spanish-english: how far can we get?",
    "citation_count": 2,
    "authors": [
      "Dana-Maria Iliescu",
      "Rasmus Grand",
      "Sara Qirko",
      "Rob van der Goot"
    ]
  },
  "https://aclanthology.org/2021.calcs-1.10": {
    "title": "A Language-aware Approach to Code-switched Morphological Tagging",
    "volume": "workshop",
    "abstract": "Morphological tagging of code-switching (CS) data becomes more challenging especially when language pairs composing the CS data have different morphological representations. In this paper, we explore a number of ways of implementing a language-aware morphological tagging method and present our approach for integrating language IDs into a transformer-based framework for CS morphological tagging. We perform our set of experiments on the Turkish-German SAGT Treebank. Experimental results show that including language IDs to the learning model significantly improves accuracy over other approaches",
    "checked": true,
    "id": "b2ec3857bde1f8ecd317047aa9c86107f87d9c67",
    "semantic_title": "a language-aware approach to code-switched morphological tagging",
    "citation_count": 5,
    "authors": [
      "Şaziye Betül Özateş",
      "Özlem Çetinoğlu"
    ]
  },
  "https://aclanthology.org/2021.calcs-1.11": {
    "title": "Can You Traducir This? Machine Translation for Code-Switched Input",
    "volume": "workshop",
    "abstract": "Code-Switching (CSW) is a common phenomenon that occurs in multilingual geographic or social contexts, which raises challenging problems for natural language processing tools. We focus here on Machine Translation (MT) of CSW texts, where we aim to simultaneously disentangle and translate the two mixed languages. Due to the lack of actual translated CSW data, we generate artificial training data from regular parallel texts. Experiments show this training strategy yields MT systems that surpass multilingual systems for code-switched texts. These results are confirmed in an alternative task aimed at providing contextual translations for a L2 writing assistant",
    "checked": true,
    "id": "a321432a9d672ad2df42b762f1dcdbd48a08417f",
    "semantic_title": "can you traducir this? machine translation for code-switched input",
    "citation_count": 21,
    "authors": [
      "Jitao Xu",
      "François Yvon"
    ]
  },
  "https://aclanthology.org/2021.calcs-1.12": {
    "title": "On the logistical difficulties and findings of Jopara Sentiment Analysis",
    "volume": "workshop",
    "abstract": "This paper addresses the problem of sentiment analysis for Jopara, a code-switching language between Guarani and Spanish. We first collect a corpus of Guarani-dominant tweets and discuss on the difficulties of finding quality data for even relatively easy-to-annotate tasks, such as sentiment analysis. Then, we train a set of neural models, including pre-trained language models, and explore whether they perform better than traditional machine learning ones in this low-resource setup. Transformer architectures obtain the best results, despite not considering Guarani during pre-training, but traditional machine learning models perform close due to the low-resource nature of the problem",
    "checked": true,
    "id": "00f2265db674002abef5a623bee5decb31c89378",
    "semantic_title": "on the logistical difficulties and findings of jopara sentiment analysis",
    "citation_count": 7,
    "authors": [
      "Marvin Agüero-Torales",
      "David Vilares",
      "Antonio López-Herrera"
    ]
  },
  "https://aclanthology.org/2021.calcs-1.13": {
    "title": "Unsupervised Self-Training for Sentiment Analysis of Code-Switched Data",
    "volume": "workshop",
    "abstract": "Sentiment analysis is an important task in understanding social media content like customer reviews, Twitter and Facebook feeds etc. In multilingual communities around the world, a large amount of social media text is characterized by the presence of Code-Switching. Thus, it has become important to build models that can handle code-switched data. However, annotated code-switched data is scarce and there is a need for unsupervised models and algorithms. We propose a general framework called Unsupervised Self-Training and show its applications for the specific use case of sentiment analysis of code-switched data. We use the power of pre-trained BERT models for initialization and fine-tune them in an unsupervised manner, only using pseudo labels produced by zero-shot transfer. We test our algorithm on multiple code-switched languages and provide a detailed analysis of the learning dynamics of the algorithm with the aim of answering the question - ‘Does our unsupervised model understand the Code-Switched languages or does it just learn its representations?'. Our unsupervised models compete well with their supervised counterparts, with their performance reaching within 1-7% (weighted F1 scores) when compared to supervised models trained for a two class problem",
    "checked": true,
    "id": "9bfeafe564780639c800be4ebb617795b588ef61",
    "semantic_title": "unsupervised self-training for sentiment analysis of code-switched data",
    "citation_count": 11,
    "authors": [
      "Akshat Gupta",
      "Sargam Menghani",
      "Sai Krishna Rallabandi",
      "Alan W Black"
    ]
  },
  "https://aclanthology.org/2021.calcs-1.14": {
    "title": "CodemixedNLP: An Extensible and Open NLP Toolkit for Code-Mixing",
    "volume": "workshop",
    "abstract": "The NLP community has witnessed steep progress in a variety of tasks across the realms of monolingual and multilingual language processing recently. These successes, in conjunction with the proliferating mixed language interactions on social media, have boosted interest in modeling code-mixed texts. In this work, we present CodemixedNLP, an open-source library with the goals of bringing together the advances in code-mixed NLP and opening it up to a wider machine learning community. The library consists of tools to develop and benchmark versatile model architectures that are tailored for mixed texts, methods to expand training sets, techniques to quantify mixing styles, and fine-tuned state-of-the-art models for 7 tasks in Hinglish. We believe this work has the potential to foster a distributed yet collaborative and sustainable ecosystem in an otherwise dispersed space of code-mixing research. The toolkit is designed to be simple, easily extensible, and resourceful to both researchers as well as practitioners. Demo: http://k-ikkees.pc.cs.cmu.edu:5000 and Library: https://github.com/murali1996/CodemixedNLP",
    "checked": true,
    "id": "87bca71f182bdb942adbe3f6898894cadef6776e",
    "semantic_title": "codemixednlp: an extensible and open nlp toolkit for code-mixing",
    "citation_count": 3,
    "authors": [
      "Sai Muralidhar Jayanthi",
      "Kavya Nerella",
      "Khyathi Raghavi Chandu",
      "Alan W Black"
    ]
  },
  "https://aclanthology.org/2021.calcs-1.15": {
    "title": "Normalization and Back-Transliteration for Code-Switched Data",
    "volume": "workshop",
    "abstract": "Code-switching is an omnipresent phenomenon in multilingual communities all around the world but remains a challenge for NLP systems due to the lack of proper data and processing techniques. Hindi-English code-switched text on social media is often transliterated to the Roman script which prevents from utilizing monolingual resources available in the native Devanagari script. In this paper, we propose a method to normalize and back-transliterate code-switched Hindi-English text. In addition, we present a grapheme-to-phoneme (G2P) conversion technique for romanized Hindi data. We also release a dataset of script-corrected Hindi-English code-switched sentences labeled for the named entity recognition and part-of-speech tagging tasks to facilitate further research",
    "checked": true,
    "id": "abfa0e92c7f463bf92fff326a132271afc57de3b",
    "semantic_title": "normalization and back-transliteration for code-switched data",
    "citation_count": 7,
    "authors": [
      "Dwija Parikh",
      "Thamar Solorio"
    ]
  },
  "https://aclanthology.org/2021.calcs-1.16": {
    "title": "Abusive content detection in transliterated Bengali-English social media corpus",
    "volume": "workshop",
    "abstract": "Abusive text detection in low-resource languages such as Bengali is a challenging task due to the inadequacy of resources and tools. The ubiquity of transliterated Bengali comments in social media makes the task even more involved as monolingual approaches cannot capture them. Unfortunately, no transliterated Bengali corpus is publicly available yet for abusive content analysis. Therefore, in this paper, we introduce an annotated Bengali corpus of 3000 transliterated Bengali comments categorized into two classes, abusive and non-abusive, 1500 comments for each. For baseline evaluations, we employ several supervised machine learning (ML) and deep learning-based classifiers. We find support vector machine (SVM) shows the highest efficacy for identifying abusive content. We make the annotated corpus freely available for the researcher to aid abusive content detection in Bengali social media data",
    "checked": true,
    "id": "d0ae835a30f2b0fc0b470ab9ee9e0e08860eb31c",
    "semantic_title": "abusive content detection in transliterated bengali-english social media corpus",
    "citation_count": 25,
    "authors": [
      "Salim Sazzed"
    ]
  },
  "https://aclanthology.org/2021.calcs-1.17": {
    "title": "Developing ASR for Indonesian-English Bilingual Language Teaching",
    "volume": "workshop",
    "abstract": "Usage-based analyses of teacher corpora and code-switching (Boztepe, 2003) are an important next stage in understanding language acquisition. Multilingual corpora are difficult to compile and a classroom setting adds pedagogy to the mix of factors which make this data so rich and problematic to classify. Using quantitative methods to understand language learning and teaching is difficult work as the ‘transcription bottleneck' constrains the size of datasets. We found that using an automatic speech recognition (ASR) toolkit with a small set of training data is likely to speed data collection in this context (Maxwelll-Smith et al., 2020)",
    "checked": true,
    "id": "234fa23b9e1b109b0fe9036da229f60eb9769a68",
    "semantic_title": "developing asr for indonesian-english bilingual language teaching",
    "citation_count": 2,
    "authors": [
      "Zara Maxwell-Smith",
      "Ben Foley"
    ]
  },
  "https://aclanthology.org/2021.calcs-1.18": {
    "title": "Transliteration for Low-Resource Code-Switching Texts: Building an Automatic Cyrillic-to-Latin Converter for Tatar",
    "volume": "workshop",
    "abstract": "We introduce a Cyrillic-to-Latin transliterator for the Tatar language based on subword-level language identification. The transliteration is a challenging task due to the following two reasons. First, because modern Tatar texts often contain intra-word code-switching to Russian, a different transliteration set of rules needs to be applied to each morpheme depending on the language, which necessitates morpheme-level language identification. Second, the fact that Tatar is a low-resource language, with most of the texts in Cyrillic, makes it difficult to prepare a sufficient dataset. Given this situation, we proposed a transliteration method based on subword-level language identification. We trained a language classifier with monolingual Tatar and Russian texts, and applied different transliteration rules in accord with the identified language. The results demonstrate that our proposed method outscores other Tatar transliteration tools, and imply that it correctly transcribes Russian loanwords to some extent",
    "checked": true,
    "id": "196ca5ac53546ae45b89986e92e2b1ef810153cc",
    "semantic_title": "transliteration for low-resource code-switching texts: building an automatic cyrillic-to-latin converter for tatar",
    "citation_count": 2,
    "authors": [
      "Chihiro Taguchi",
      "Yusuke Sakai",
      "Taro Watanabe"
    ]
  },
  "https://aclanthology.org/2021.calcs-1.19": {
    "title": "Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots",
    "volume": "workshop",
    "abstract": "Multilingual models have demonstrated impressive cross-lingual transfer performance. However, test sets like XNLI are monolingual at the example level. In multilingual communities, it is common for polyglots to code-mix when conversing with each other. Inspired by this phenomenon, we present two strong black-box adversarial attacks (one word-level, one phrase-level) for multilingual models that push their ability to handle code-mixed sentences to the limit. The former (PolyGloss) uses bilingual dictionaries to propose perturbations and translations of the clean example for sense disambiguation. The latter (Bumblebee) directly aligns the clean example with its translations before extracting phrases as perturbations. Bumblebee has a success rate of 89.75% against XLM-R-large, bringing its average accuracy of 79.85 down to 8.18 on XNLI. Finally, we propose an efficient adversarial training scheme, Code-mixed Adversarial Training (CAT), that trains in the same number of steps as the original model. Even after controlling for the extra training data introduced, CAT improves model accuracy when the model is prevented from relying on lexical overlaps (+3.45), with a negligible drop (-0.15 points) in performance on the original XNLI test set. t-SNE visualizations reveal that CAT improves a model's language agnosticity. This paper will be published in the proceedings of NAACL-HLT 2021",
    "checked": true,
    "id": "042bf849f28f97e03e1e2a807704063dbb93a4d1",
    "semantic_title": "code-mixing on sesame street: dawn of the adversarial polyglots",
    "citation_count": 0,
    "authors": [
      "Samson Tan",
      "Shafiq Joty"
    ]
  },
  "https://aclanthology.org/2021.calcs-1.20": {
    "title": "Are Multilingual Models Effective in Code-Switching?",
    "volume": "workshop",
    "abstract": "Multilingual language models have shown decent performance in multilingual and cross-lingual natural language understanding tasks. However, the power of these multilingual models in code-switching tasks has not been fully explored. In this paper, we study the effectiveness of multilingual language models to understand their capability and adaptability to the mixed-language setting by considering the inference speed, performance, and number of parameters to measure their practicality. We conduct experiments in three language pairs on named entity recognition and part-of-speech tagging and compare them with existing methods, such as using bilingual embeddings and multilingual meta-embeddings. Our findings suggest that pre-trained multilingual models do not necessarily guarantee high-quality representations on code-switching, while using meta-embeddings achieves similar results with significantly fewer parameters",
    "checked": true,
    "id": "00aa232912d84fe1890761ccc543a5deb6128ff6",
    "semantic_title": "are multilingual models effective in code-switching?",
    "citation_count": 49,
    "authors": [
      "Genta Indra Winata",
      "Samuel Cahyawijaya",
      "Zihan Liu",
      "Zhaojiang Lin",
      "Andrea Madotto",
      "Pascale Fung"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.1": {
    "title": "Understanding who uses Reddit: Profiling individuals with a self-reported bipolar disorder diagnosis",
    "volume": "workshop",
    "abstract": "Recently, research on mental health conditions using public online data, including Reddit, has surged in NLP and health research but has not reported user characteristics, which are important to judge generalisability of findings. This paper shows how existing NLP methods can yield information on clinical, demographic, and identity characteristics of almost 20K Reddit users who self-report a bipolar disorder diagnosis. This population consists of slightly more feminine- than masculine-gendered mainly young or middle-aged US-based adults who often report additional mental health diagnoses, which is compared with general Reddit statistics and epidemiological studies. Additionally, this paper carefully evaluates all methods and discusses ethical issues",
    "checked": true,
    "id": "efd6540b6c3631127ae9d414a2dcfa6fe22e930e",
    "semantic_title": "understanding who uses reddit: profiling individuals with a self-reported bipolar disorder diagnosis",
    "citation_count": 9,
    "authors": [
      "Glorianna Jagfeld",
      "Fiona Lobban",
      "Paul Rayson",
      "Steven Jones"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.2": {
    "title": "On the State of Social Media Data for Mental Health Research",
    "volume": "workshop",
    "abstract": "Data-driven methods for mental health treatment and surveillance have become a major focus in computational science research in the last decade. However, progress in the domain remains bounded by the availability of adequate data. Prior systematic reviews have not necessarily made it possible to measure the degree to which data-related challenges have affected research progress. In this paper, we offer an analysis specifically on the state of social media data that exists for conducting mental health research. We do so by introducing an open-source directory of mental health datasets, annotated using a standardized schema to facilitate meta-analysis",
    "checked": true,
    "id": "952306c6c683e275ed1abfeb1f2407e187cd7c8c",
    "semantic_title": "on the state of social media data for mental health research",
    "citation_count": 38,
    "authors": [
      "Keith Harrigian",
      "Carlos Aguirre",
      "Mark Dredze"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.3": {
    "title": "Individual Differences in the Movement-Mood Relationship in Digital Life Data",
    "volume": "workshop",
    "abstract": "Our increasingly digitized lives generate troves of data that reflect our behavior, beliefs, mood, and wellbeing. Such \"digital life data\" provides crucial insight into the lives of patients outside the healthcare setting that has long been lacking, from a better understanding of mundane patterns of exercise and sleep routines to harbingers of emotional crisis. Moreover, information about individual differences and personalities is encoded in digital life data. In this paper we examine the relationship between mood and movement using linguistic and biometric data, respectively. Does increased physical activity (movement) have an effect on a person's mood (or vice-versa)? We find that weak group-level relationships between movement and mood mask interesting and often strong relationships between the two for individuals within the group. We describe these individual differences, and argue that individual variability in the relationship between movement and mood is one of many such factors that ought be taken into account in wellbeing-focused apps and AI systems",
    "checked": true,
    "id": "96f5edc93845cb60d634606849dc07d3c1944889",
    "semantic_title": "individual differences in the movement-mood relationship in digital life data",
    "citation_count": 2,
    "authors": [
      "Glen Coppersmith",
      "Alex Fine",
      "Patrick Crutchley",
      "Joshua Carroll"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.4": {
    "title": "Dissociating Semantic and Phonemic Search Strategies in the Phonemic Verbal Fluency Task in early Dementia",
    "volume": "workshop",
    "abstract": "Effective management of dementia hinges on timely detection and precise diagnosis of the underlying cause of the syndrome at an early mild cognitive impairment (MCI) stage. Verbal fluency tasks are among the most often applied tests for early dementia detection due to their efficiency and ease of use. In these tasks, participants are asked to produce as many words as possible belonging to either a semantic category (SVF task) or a phonemic category (PVF task). Even though both SVF and PVF share neurocognitive function profiles, the PVF is typically believed to be less sensitive to measure MCI-related cognitive impairment and recent research on fine-grained automatic evaluation of VF tasks has mainly focused on the SVF. Contrary to this belief, we show that by applying state-of-the-art semantic and phonemic distance metrics in automatic analysis of PVF word productions, in-depth conclusions about production strategy of MCI patients are possible. Our results reveal a dissociation between semantically- and phonemically-guided search processes in the PVF. Specifically, we show that subjects with MCI rely less on semantic- and more on phonemic processes to guide their word production as compared to healthy controls (HC). We further show that semantic similarity-based features improve automatic MCI versus HC classification by 29% over previous approaches for the PVF. As such, these results point towards the yet underexplored utility of the PVF for in-depth assessment of cognition in MCI",
    "checked": true,
    "id": "e8b362384aa13317bc9e52a6e871044aa69bbd5a",
    "semantic_title": "dissociating semantic and phonemic search strategies in the phonemic verbal fluency task in early dementia",
    "citation_count": 5,
    "authors": [
      "Hali Lindsay",
      "Philipp Müller",
      "Nicklas Linz",
      "Radia Zeghari",
      "Mario Magued Mina",
      "Alexandra Konig",
      "Johannes Tröger"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.5": {
    "title": "Demonstrating the Reliability of Self-Annotated Emotion Data",
    "volume": "workshop",
    "abstract": "Vent is a specialised iOS/Android social media platform with the stated goal to encourage people to post about their feelings and explicitly label them. In this paper, we study a snapshot of more than 100 million messages obtained from the developers of Vent, together with the labels assigned by the authors of the messages. We establish the quality of the self-annotated data by conducting a qualitative analysis, a vocabulary based analysis, and by training and testing an emotion classifier. We conclude that the self-annotated labels of our corpus are indeed indicative of the emotional contents expressed in the text and thus can support more detailed analyses of emotion expression on social media, such as emotion trajectories and factors influencing them",
    "checked": true,
    "id": "3f75d4dcd3bbed926b8eeb093ed58026185ed396",
    "semantic_title": "demonstrating the reliability of self-annotated emotion data",
    "citation_count": 6,
    "authors": [
      "Anton Malko",
      "Cecile Paris",
      "Andreas Duenser",
      "Maria Kangas",
      "Diego Molla",
      "Ross Sparks",
      "Stephen Wan"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.6": {
    "title": "Hebrew Psychological Lexicons",
    "volume": "workshop",
    "abstract": "We introduce a large set of Hebrew lexicons pertaining to psychological aspects. These lexicons are useful for various psychology applications such as detecting emotional state, well being, relationship quality in conversation, identifying topics (e.g., family, work) and many more. We discuss the challenges in creating and validating lexicons in a new language, and highlight our methodological considerations in the data-driven lexicon construction process. Most of the lexicons are publicly available, which will facilitate further research on Hebrew clinical psychology text analysis. The lexicons were developed through data driven means, and verified by domain experts, clinical psychologists and psychology students, in a process of reconciliation with three judges. Development and verification relied on a dataset of a total of 872 psychotherapy session transcripts. We describe the construction process of each collection, the final resource and initial results of research studies employing this resource",
    "checked": true,
    "id": "899fa470eb47afc75347484c1feedc1947cb52c5",
    "semantic_title": "hebrew psychological lexicons",
    "citation_count": 2,
    "authors": [
      "Natalie Shapira",
      "Dana Atzil-Slonim",
      "Daniel Juravski",
      "Moran Baruch",
      "Dana Stolowicz-Melman",
      "Adar Paz",
      "Tal Alfi-Yogev",
      "Roy Azoulay",
      "Adi Singer",
      "Maayan Revivo",
      "Chen Dahbash",
      "Limor Dayan",
      "Tamar Naim",
      "Lidar Gez",
      "Boaz Yanai",
      "Adva Maman",
      "Adam Nadaf",
      "Elinor Sarfati",
      "Amna Baloum",
      "Tal Naor",
      "Ephraim Mosenkis",
      "Badreya Sarsour",
      "Jany Gelfand Morgenshteyn",
      "Yarden Elias",
      "Liat Braun",
      "Moria Rubin",
      "Matan Kenigsbuch",
      "Noa Bergwerk",
      "Noam Yosef",
      "Sivan Peled",
      "Coral Avigdor",
      "Rahav Obercyger",
      "Rachel Mann",
      "Tomer Alper",
      "Inbal Beka",
      "Ori Shapira",
      "Yoav Goldberg"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.7": {
    "title": "Community-level Research on Suicidality Prediction in a Secure Environment: Overview of the CLPsych 2021 Shared Task",
    "volume": "workshop",
    "abstract": "Progress on NLP for mental health — indeed, for healthcare in general — is hampered by obstacles to shared, community-level access to relevant data. We report on what is, to our knowledge, the first attempt to address this problem in mental health by conducting a shared task using sensitive data in a secure data enclave. Participating teams received access to Twitter posts donated for research, including data from users with and without suicide attempts, and did all work with the dataset entirely within a secure computational environment. We discuss the task, team results, and lessons learned to set the stage for future tasks on sensitive or confidential data",
    "checked": true,
    "id": "7235a223c759171df423b328f8b26d238c294b22",
    "semantic_title": "community-level research on suicidality prediction in a secure environment: overview of the clpsych 2021 shared task",
    "citation_count": 26,
    "authors": [
      "Sean MacAvaney",
      "Anjali Mittu",
      "Glen Coppersmith",
      "Jeff Leintz",
      "Philip Resnik"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.8": {
    "title": "Determining a Person's Suicide Risk by Voting on the Short-Term History of Tweets for the CLPsych 2021 Shared Task",
    "volume": "workshop",
    "abstract": "In this shared task, we accept the challenge of constructing models to identify Twitter users who attempted suicide based on their tweets 30 and 182 days before the adverse event's occurrence. We explore multiple machine learning and deep learning methods to identify a person's suicide risk based on the short-term history of their tweets. Taking the real-life applicability of the model into account, we make the design choice of classifying on the tweet level. By voting the tweet-level suicide risk scores through an ensemble of classifiers, we predict the suicidal users 30-days before the event with an 81.8% true-positives rate. Meanwhile, the tweet-level voting falls short on the six-month-long data as the number of tweets with weak suicidal ideation levels weakens the overall suicidal signals in the long term",
    "checked": true,
    "id": "3f0c2d5154bec49d62f4e63be308fa501cf1b88c",
    "semantic_title": "determining a person's suicide risk by voting on the short-term history of tweets for the clpsych 2021 shared task",
    "citation_count": 9,
    "authors": [
      "Ulya Bayram",
      "Lamia Benhiba"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.9": {
    "title": "Learning Models for Suicide Prediction from Social Media Posts",
    "volume": "workshop",
    "abstract": "We propose a deep learning architecture and test three other machine learning models to automatically detect individuals that will attempt suicide within (1) 30 days and (2) six months, using their social media post data provided in the CL-Psych-Challenge. Additionally, we create and extract three sets of handcrafted features for suicide detection based on the three-stage theory of suicide and prior work on emotions and the use of pronouns among persons exhibiting suicidal ideations. Extensive experimentations show that some of the traditional machine learning methods outperform the baseline with an F1 score of 0.741 and F2 score of 0.833 on subtask 1 (prediction of a suicide attempt 30 days prior). However, the proposed deep learning method outperforms the baseline with F1 score of 0.737 and F2 score of 0.843 on subtask2 (prediction of suicide 6 months prior)",
    "checked": true,
    "id": "0b0311e47b6d71fa1590a3bcd8544d5570c2e679",
    "semantic_title": "learning models for suicide prediction from social media posts",
    "citation_count": 24,
    "authors": [
      "Ning Wang",
      "Luo Fan",
      "Yuvraj Shivtare",
      "Varsha Badal",
      "Koduvayur Subbalakshmi",
      "Rajarathnam Chandramouli",
      "Ellen Lee"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.10": {
    "title": "Suicide Risk Prediction by Tracking Self-Harm Aspects in Tweets: NUS-IDS at the CLPsych 2021 Shared Task",
    "volume": "workshop",
    "abstract": "We describe our system for identifying users at-risk for suicide based on their tweets developed for the CLPsych 2021 Shared Task. Based on research in mental health studies linking self-harm tendencies with suicide, in our system, we attempt to characterize self-harm aspects expressed in user tweets over a period of time. To this end, we design SHTM, a Self-Harm Topic Model that combines Latent Dirichlet Allocation with a self-harm dictionary for modeling daily tweets of users. Next, differences in moods and topics over time are captured as features to train a deep learning model for suicide prediction",
    "checked": true,
    "id": "0f2c5facd68c8bbea8be6cef48266e87d035ea33",
    "semantic_title": "suicide risk prediction by tracking self-harm aspects in tweets: nus-ids at the clpsych 2021 shared task",
    "citation_count": 4,
    "authors": [
      "Sujatha Das Gollapalli",
      "Guilherme Augusto Zagatti",
      "See-Kiong Ng"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.11": {
    "title": "Team 9: A Comparison of Simple vs. Complex Models for Suicide Risk Assessment",
    "volume": "workshop",
    "abstract": "This work presents the systems explored as part of the CLPsych 2021 Shared Task. More specifically, this work explores the relative performance of models trained on social me- dia data for suicide risk assessment. For this task, we aim to investigate whether or not simple traditional models can outperform more complex fine-tuned deep learning mod- els. Specifically, we build and compare a range of models including simple baseline models, feature-engineered machine learning models, and lastly, fine-tuned deep learning models. We find that simple more traditional machine learning models are more suited for this task and highlight the challenges faced when trying to leverage more sophisticated deep learning models",
    "checked": true,
    "id": "0a60cc279a9c887c63af8fddd1cc4f3cdc6af589",
    "semantic_title": "team 9: a comparison of simple vs. complex models for suicide risk assessment",
    "citation_count": 5,
    "authors": [
      "Michelle Morales",
      "Prajjalita Dey",
      "Kriti Kohli"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.12": {
    "title": "Using Psychologically-Informed Priors for Suicide Prediction in the CLPsych 2021 Shared Task",
    "volume": "workshop",
    "abstract": "This paper describes our approach to the CLPsych 2021 Shared Task, in which we aimed to predict suicide attempts based on Twitter feed data. We addressed this challenge by emphasizing reliance on prior domain knowledge. We engineered novel theory-driven features, and integrated prior knowledge with empirical evidence in a principled manner using Bayesian modeling. While this theory-guided approach increases bias and lowers accuracy on the training set, it was successful in preventing over-fitting. The models provided reasonable classification accuracy on unseen test data (0.68<=AUC<= 0.84). Our approach may be particularly useful in prediction tasks trained on a relatively small data set",
    "checked": true,
    "id": "fef24292981f7a01eee62892bc985e1109622df8",
    "semantic_title": "using psychologically-informed priors for suicide prediction in the clpsych 2021 shared task",
    "citation_count": 5,
    "authors": [
      "Avi Gamoran",
      "Yonatan Kaplan",
      "Almog Simchon",
      "Michael Gilead"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.13": {
    "title": "Analysis of Behavior Classification in Motivational Interviewing",
    "volume": "workshop",
    "abstract": "Analysis of client and therapist behavior in counseling sessions can provide helpful insights for assessing the quality of the session and consequently, the client's behavioral outcome. In this paper, we study the automatic classification of standardized behavior codes (annotations) used for assessment of psychotherapy sessions in Motivational Interviewing (MI). We develop models and examine the classification of client behaviors throughout MI sessions, comparing the performance by models trained on large pretrained embeddings (RoBERTa) versus interpretable and expert-selected features (LIWC). Our best performing model using the pretrained RoBERTa embeddings beats the baseline model, achieving an F1 score of 0.66 in the subject-independent 3-class classification. Through statistical analysis on the classification results, we identify prominent LIWC features that may not have been captured by the model using pretrained embeddings. Although classification using LIWC features underperforms RoBERTa, our findings motivate the future direction of incorporating auxiliary tasks in the classification of MI codes",
    "checked": true,
    "id": "54ab7794bf24a6d60e4c19969d4f2ab93e813171",
    "semantic_title": "analysis of behavior classification in motivational interviewing",
    "citation_count": 7,
    "authors": [
      "Leili Tavabi",
      "Trang Tran",
      "Kalin Stefanov",
      "Brian Borsari",
      "Joshua Woolley",
      "Stefan Scherer",
      "Mohammad Soleymani"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.14": {
    "title": "Automatic Detection and Prediction of Psychiatric Hospitalizations From Social Media Posts",
    "volume": "workshop",
    "abstract": "We address the problem of predicting psychiatric hospitalizations using linguistic features drawn from social media posts. We formulate this novel task and develop an approach to automatically extract time spans of self-reported psychiatric hospitalizations. Using this dataset, we build predictive models of psychiatric hospitalization, comparing feature sets, user vs. post classification, and comparing model performance using a varying time window of posts. Our best model achieves an F1 of .718 using 7 days of posts. Our results suggest that this is a useful framework for collecting hospitalization data, and that social media data can be leveraged to predict acute psychiatric crises before they occur, potentially saving lives and improving outcomes for individuals with mental illness",
    "checked": true,
    "id": "0e84f35162f71aa96683d0e2bbcfb7862f468cc9",
    "semantic_title": "automatic detection and prediction of psychiatric hospitalizations from social media posts",
    "citation_count": 1,
    "authors": [
      "Zhengping Jiang",
      "Jonathan Zomick",
      "Sarah Ita Levitan",
      "Mark Serper",
      "Julia Hirschberg"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.15": {
    "title": "Automatic Identification of Ruptures in Transcribed Psychotherapy Sessions",
    "volume": "workshop",
    "abstract": "We present the first work on automatically capturing alliance rupture in transcribed therapy sessions, trained on the text and self-reported rupture scores from both therapists and clients. Our NLP baseline outperforms a strong majority baseline by a large margin and captures client reported ruptures unidentified by therapists in 40% of such cases",
    "checked": true,
    "id": "9a58d14ca41038455295ed245d8c1d27fd378c9b",
    "semantic_title": "automatic identification of ruptures in transcribed psychotherapy sessions",
    "citation_count": 4,
    "authors": [
      "Adam Tsakalidis",
      "Dana Atzil-Slonim",
      "Asaf Polakovski",
      "Natalie Shapira",
      "Rivka Tuval-Mashiach",
      "Maria Liakata"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.16": {
    "title": "Automated coherence measures fail to index thought disorder in individuals at risk for psychosis",
    "volume": "workshop",
    "abstract": "Thought disorder – linguistic disturbances including incoherence and derailment of topic – is seen in individuals both with and at risk for psychosis. Methods from computational linguistics have increasingly sought to quantify thought disorder to detect group differences between clinical populations and healthy controls. While previous work has been quite successful at these classification tasks, the lack of interpretability of the computational metrics has made it unclear whether they are in fact measuring thought disorder. In this paper, we dive into these measures to try to better understand what they reflect. While we find group differences between at-risk and healthy control populations, we also find that the measures mostly do not correlate with existing measures of thought disorder symptoms (what they are intended to measure), but rather correlate with surface properties of the speech (e.g., sentence length) and sociodemographic properties of the speaker (e.g., race). These results highlight the importance of considering interpretability and front and center as the field continues to grow. Ethical use of computational measures like those studied here – especially in the high-stakes context of clinical care – requires us to devote substantial attention to potential biases in our measures",
    "checked": true,
    "id": "fcf23a5be6516b9e5ee87d9a6ebd80d65ce4274b",
    "semantic_title": "automated coherence measures fail to index thought disorder in individuals at risk for psychosis",
    "citation_count": 11,
    "authors": [
      "Kasia Hitczenko",
      "Henry Cowan",
      "Vijay Mittal",
      "Matthew Goldrick"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.17": {
    "title": "Detecting Cognitive Distortions from Patient-Therapist Interactions",
    "volume": "workshop",
    "abstract": "An important part of Cognitive Behavioral Therapy (CBT) is to recognize and restructure certain negative thinking patterns that are also known as cognitive distortions. The aim of this project is to detect these distortions using natural language processing. We compare and contrast different types of linguistic features as well as different classification algorithms and explore the limitations of applying these techniques on a small dataset. We find that pre-trained Sentence-BERT embeddings to train an SVM classifier yields the best results with an F1-score of 0.79. Lastly, we discuss how this work provides insights into the types of linguistic features that are inherent in cognitive distortions",
    "checked": true,
    "id": "6e3bfca9c23ef76b0bc66151009b4f0ebb0730a2",
    "semantic_title": "detecting cognitive distortions from patient-therapist interactions",
    "citation_count": 12,
    "authors": [
      "Sagarika Shreevastava",
      "Peter Foltz"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.18": {
    "title": "Evaluating Automatic Speech Recognition Quality and Its Impact on Counselor Utterance Coding",
    "volume": "workshop",
    "abstract": "Automatic speech recognition (ASR) is a crucial step in many natural language processing (NLP) applications, as often available data consists mainly of raw speech. Since the result of the ASR step is considered as a meaningful, informative input to later steps in the NLP pipeline, it is important to understand the behavior and failure mode of this step. In this work, we analyze the quality of ASR in the psychotherapy domain, using motivational interviewing conversations between therapists and clients. We conduct domain agnostic and domain-relevant evaluations using standard evaluation metrics and also identify domain-relevant keywords in the ASR output. Moreover, we empirically study the effect of mixing ASR and manual data during the training of a downstream NLP model, and also demonstrate how additional local context can help alleviate the error introduced by noisy ASR transcripts",
    "checked": true,
    "id": "8b751fede16b4850ada6d2f45f1c72f2f01c5d34",
    "semantic_title": "evaluating automatic speech recognition quality and its impact on counselor utterance coding",
    "citation_count": 4,
    "authors": [
      "Do June Min",
      "Verónica Pérez-Rosas",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.19": {
    "title": "Qualitative Analysis of Depression Models by Demographics",
    "volume": "workshop",
    "abstract": "Models for identifying depression using social media text exhibit biases towards different gender and racial/ethnic groups. Factors like representation and balance of groups within the dataset are contributory factors, but difference in content and social media use may further explain these biases. We present an analysis of the content of social media posts from different demographic groups. Our analysis shows that there are content differences between depression and control subgroups across demographic groups, and that temporal topics and demographic-specific topics are correlated with downstream depression model error. We discuss the implications of our work on creating future datasets, as well as designing and training models for mental health",
    "checked": true,
    "id": "fd646528f25cb275c1e53bd631d90ed23c72e550",
    "semantic_title": "qualitative analysis of depression models by demographics",
    "citation_count": 7,
    "authors": [
      "Carlos Aguirre",
      "Mark Dredze"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.20": {
    "title": "Safeguarding against spurious AI-based predictions: The case of automated verbal memory assessment",
    "volume": "workshop",
    "abstract": "A growing amount of psychiatric research incorporates machine learning and natural language processing methods, however findings have yet to be translated into actual clinical decision support systems. Many of these studies are based on relatively small datasets in homogeneous populations, which has the associated risk that the models may not perform adequately on new data in real clinical practice. The nature of serious mental illness is that it is hard to define, hard to capture, and requires frequent monitoring, which leads to imperfect data where attribute and class noise are common. With the goal of an effective AI-mediated clinical decision support system, there must be computational safeguards placed on the models used in order to avoid spurious predictions and thus allow humans to review data in the settings where models are unstable or bound not to generalize. This paper describes two approaches to implementing safeguards: (1) the determination of cases in which models are unstable by means of attribute and class based outlier detection and (2) finding the extent to which models show inductive bias. These safeguards are illustrated in the automated scoring of a story recall task via natural language processing methods. With the integration of human-in-the-loop machine learning in the clinical implementation process, incorporating safeguards such as these into the models will offer patients increased protection from spurious predictions",
    "checked": true,
    "id": "2925ab22ed585337cdfb78067c0547fadcf0cc80",
    "semantic_title": "safeguarding against spurious ai-based predictions: the case of automated verbal memory assessment",
    "citation_count": 1,
    "authors": [
      "Chelsea Chandler",
      "Peter Foltz",
      "Alex Cohen",
      "Terje Holmlund",
      "Brita Elvevåg"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.21": {
    "title": "Towards the Development of Speech-Based Measures of Stress Response in Individuals",
    "volume": "workshop",
    "abstract": "Psychological and physiological stress in the environment can induce a different stress response in different individuals. Given the causal relationship between stress, mental health, and psychopathologies, as well as its impact on individuals' executive functioning and performance, identifying the extent of stress response in individuals can be useful for providing targeted support to those who are in need. In this paper, we identify and validate features in speech that can be used as indicators of stress response in individuals to develop speech-based measures of stress response. We evaluate effectiveness of two types of tasks used for collecting speech samples in developing stress response measures, namely Read Speech Task and Open-Ended Question Task. Participants completed these tasks, along with the verbal fluency task (an established measure of executive functioning) before and after clinically validated stress induction to see if the changes in the speech-based features are associated with the stress-induced decline in executive functioning. Further, we supplement our analyses with an extensive, external assessment of the individuals' stress tolerance in the real life to validate the usefulness of the speech-based measures in predicting meaningful outcomes outside of the experimental setting",
    "checked": true,
    "id": "cf1704af7a28103f352864d39c658a4d2fbf28a4",
    "semantic_title": "towards the development of speech-based measures of stress response in individuals",
    "citation_count": 1,
    "authors": [
      "Archna Bhatia",
      "Toshiya Miyatsu",
      "Peter Pirolli"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.22": {
    "title": "Towards Low-Resource Real-Time Assessment of Empathy in Counselling",
    "volume": "workshop",
    "abstract": "Gauging therapist empathy in counselling is an important component of understanding counselling quality. While session-level empathy assessment based on machine learning has been investigated extensively, it relies on relatively large amounts of well-annotated dialogue data, and real-time evaluation has been overlooked in the past. In this paper, we focus on the task of low-resource utterance-level binary empathy assessment. We train deep learning models on heuristically constructed empathy vs. non-empathy contrast in general conversations, and apply the models directly to therapeutic dialogues, assuming correlation between empathy manifested in those two domains. We show that such training yields poor performance in general, probe its causes, and examine the actual effect of learning from empathy contrast in general conversation",
    "checked": true,
    "id": "811d9b60dce7dbed9d21b3f0a22791d2c754669c",
    "semantic_title": "towards low-resource real-time assessment of empathy in counselling",
    "citation_count": 8,
    "authors": [
      "Zixiu Wu",
      "Rim Helaoui",
      "Diego Reforgiato Recupero",
      "Daniele Riboni"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.23": {
    "title": "Towards Understanding the Role of Gender in Deploying Social Media-Based Mental Health Surveillance Models",
    "volume": "workshop",
    "abstract": "Spurred by advances in machine learning and natural language processing, developing social media-based mental health surveillance models has received substantial recent attention. For these models to be maximally useful, it is necessary to understand how they perform on various subgroups, especially those defined in terms of protected characteristics. In this paper we study the relationship between user demographics – focusing on gender – and depression. Considering a population of Reddit users with known genders and depression statuses, we analyze the degree to which depression predictions are subject to biases along gender lines using domain-informed classifiers. We then study our models' parameters to gain qualitative insight into the differences in posting behavior across genders",
    "checked": true,
    "id": "e46d58866eb02a30bb129e8d4284856822a1cc2a",
    "semantic_title": "towards understanding the role of gender in deploying social media-based mental health surveillance models",
    "citation_count": 5,
    "authors": [
      "Eli Sherman",
      "Keith Harrigian",
      "Carlos Aguirre",
      "Mark Dredze"
    ]
  },
  "https://aclanthology.org/2021.clpsych-1.24": {
    "title": "Understanding Patterns of Anorexia Manifestations in Social Media Data with Deep Learning",
    "volume": "workshop",
    "abstract": "Eating disorders are a growing problem especially among young people, yet they have been under-studied in computational research compared to other mental health disorders such as depression. Computational methods have a great potential to aid with the automatic detection of mental health problems, but state-of-the-art machine learning methods based on neural networks are notoriously difficult to interpret, which is a crucial problem for applications in the mental health domain. We propose leveraging the power of deep learning models for automatically detecting signs of anorexia based on social media data, while at the same time focusing on interpreting their behavior. We train a hierarchical attention network to detect people with anorexia and use its internal encodings to discover different clusters of anorexia symptoms. We interpret the identified patterns from multiple perspectives, including emotion expression, psycho-linguistic features and personality traits, and we offer novel hypotheses to interpret our findings from a psycho-social perspective. Some interesting findings are patterns of word usage in some users with anorexia which show that they feel less as being part of a group compared to control cases, as well as that they have abandoned explanatory activity as a result of a greater feeling of helplessness and fear",
    "checked": true,
    "id": "609724e1ace384632b9bb23b9ca598eb8d4b2c24",
    "semantic_title": "understanding patterns of anorexia manifestations in social media data with deep learning",
    "citation_count": 8,
    "authors": [
      "Ana Sabina Uban",
      "Berta Chulvi",
      "Paolo Rosso"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.1": {
    "title": "Non-Complementarity of Information in Word-Embedding and Brain Representations in Distinguishing between Concrete and Abstract Words",
    "volume": "workshop",
    "abstract": "Word concreteness and imageability have proven crucial in understanding how humans process and represent language in the brain. While word-embeddings do not explicitly incorporate the concreteness of words into their computations, they have been shown to accurately predict human judgments of concreteness and imageability. Inspired by the recent interest in using neural activity patterns to analyze distributed meaning representations, we first show that brain responses acquired while human subjects passively comprehend natural stories can significantly distinguish the concreteness levels of the words encountered. We then examine for the same task whether the additional perceptual information in the brain representations can complement the contextual information in the word-embeddings. However, the results of our predictive models and residual analyses indicate the contrary. We find that the relevant information in the brain representations is a subset of the relevant information in the contextualized word-embeddings, providing new insight into the existing state of natural language processing models",
    "checked": true,
    "id": "2d535629ca98d22ea6b8a33a7a600f8f926eee7e",
    "semantic_title": "non-complementarity of information in word-embedding and brain representations in distinguishing between concrete and abstract words",
    "citation_count": 4,
    "authors": [
      "Kalyan Ramakrishnan",
      "Fatma Deniz"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.2": {
    "title": "Human Sentence Processing: Recurrence or Attention?",
    "volume": "workshop",
    "abstract": "Recurrent neural networks (RNNs) have long been an architecture of interest for computational models of human sentence processing. The recently introduced Transformer architecture outperforms RNNs on many natural language processing tasks but little is known about its ability to model human language processing. We compare Transformer- and RNN-based language models' ability to account for measures of human reading effort. Our analysis shows Transformers to outperform RNNs in explaining self-paced reading times and neural activity during reading English sentences, challenging the widely held idea that human sentence processing involves recurrent and immediate processing and provides evidence for cue-based retrieval",
    "checked": true,
    "id": "6add8d6f1ba7e78924dad8ba61df9808ed4d9ca6",
    "semantic_title": "human sentence processing: recurrence or attention?",
    "citation_count": 64,
    "authors": [
      "Danny Merkx",
      "Stefan L. Frank"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.3": {
    "title": "Modeling Incremental Language Comprehension in the Brain with Combinatory Categorial Grammar",
    "volume": "workshop",
    "abstract": "Hierarchical sentence structure plays a role in word-by-word human sentence comprehension, but it remains unclear how best to characterize this structure and unknown how exactly it would be recognized in a step-by-step process model. With a view towards sharpening this picture, we model the time course of hemodynamic activity within the brain during an extended episode of naturalistic language comprehension using Combinatory Categorial Grammar (CCG). CCG has well-defined incremental parsing algorithms, surface compositional semantics, and can explain long-range dependencies as well as complicated cases of coordination. We find that CCG-derived predictors improve a regression model of fMRI time course in six language-relevant brain regions, over and above predictors derived from context-free phrase structure. Adding a special Revealing operator to CCG parsing, one designed to handle right-adjunction, improves the fit in three of these regions. This evidence for CCG from neuroimaging bolsters the more general case for mildly context-sensitive grammars in the cognitive science of language",
    "checked": true,
    "id": "6b3a9063689293ba849852a9dab42d32967985f5",
    "semantic_title": "modeling incremental language comprehension in the brain with combinatory categorial grammar",
    "citation_count": 13,
    "authors": [
      "Miloš Stanojević",
      "Shohini Bhattasali",
      "Donald Dunagan",
      "Luca Campanelli",
      "Mark Steedman",
      "Jonathan Brennan",
      "John Hale"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.4": {
    "title": "A Multinomial Processing Tree Model of RC Attachment",
    "volume": "workshop",
    "abstract": "In the field of sentence processing, speakers' preferred interpretation of ambiguous sentences are often determined using a variant of a discrete choice task, in which participants are asked to indicate their preferred meaning of an ambiguous sentence. We discuss participants' degree of attentiveness as a potential source of bias and variability in such tasks. We show that it may distort the estimates of the preference of a particular interpretation obtained in such experiments and may thus complicate the interpretation of the results as well as the comparison of the results of several experiments. We propose an analysis method based on multinomial processing tree models (Batchelder and Riefer, 1999) which can correct for this bias and allows for a separation of parameters of theoretical importance from nuisance parameters. We test two variants of the MPT-based model on experimental data from English and Turkish and demonstrate that our method can provide deeper insight into the processes underlying participants' answering behavior and their interpretation preferences than an analysis based on raw percentages",
    "checked": true,
    "id": "cce2906aafdfe82e0e1405d387fe47d4c76613c5",
    "semantic_title": "a multinomial processing tree model of rc attachment",
    "citation_count": 1,
    "authors": [
      "Pavel Logacev",
      "Noyan Dokudan"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.5": {
    "title": "That Looks Hard: Characterizing Linguistic Complexity in Humans and Language Models",
    "volume": "workshop",
    "abstract": "This paper investigates the relationship between two complementary perspectives in the human assessment of sentence complexity and how they are modeled in a neural language model (NLM). The first perspective takes into account multiple online behavioral metrics obtained from eye-tracking recordings. The second one concerns the offline perception of complexity measured by explicit human judgments. Using a broad spectrum of linguistic features modeling lexical, morpho-syntactic, and syntactic properties of sentences, we perform a comprehensive analysis of linguistic phenomena associated with the two complexity viewpoints and report similarities and differences. We then show the effectiveness of linguistic features when explicitly leveraged by a regression model for predicting sentence complexity and compare its results with the ones obtained by a fine-tuned neural language model. We finally probe the NLM's linguistic competence before and after fine-tuning, highlighting how linguistic information encoded in representations changes when the model learns to predict complexity",
    "checked": true,
    "id": "71628915fcbe41ffecb3b70c00db6f2ff84674f3",
    "semantic_title": "that looks hard: characterizing linguistic complexity in humans and language models",
    "citation_count": 11,
    "authors": [
      "Gabriele Sarti",
      "Dominique Brunato",
      "Felice Dell’Orletta"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.6": {
    "title": "Accounting for Agreement Phenomena in Sentence Comprehension with Transformer Language Models: Effects of Similarity-based Interference on Surprisal and Attention",
    "volume": "workshop",
    "abstract": "We advance a novel explanation of similarity-based interference effects in subject-verb and reflexive pronoun agreement processing, grounded in surprisal values computed from a pretrained large-scale Transformer model, GPT-2. Specifically, we show that surprisal of the verb or reflexive pronoun predicts facilitatory interference effects in ungrammatical sentences, where a distractor noun that matches in number with the verb or pronouns leads to faster reading times, despite the distractor not participating in the agreement relation. We review the human empirical evidence for such effects, including recent meta-analyses and large-scale studies. We also show that attention patterns (indexed by entropy and other measures) in the Transformer show patterns of diffuse attention in the presence of similar distractors, consistent with cue-based retrieval models of parsing. But in contrast to these models, the attentional cues and memory representations are learned entirely from the simple self-supervised task of predicting the next word",
    "checked": true,
    "id": "da238bb002e0cca26e6827e6914e0d4ecb099442",
    "semantic_title": "accounting for agreement phenomena in sentence comprehension with transformer language models: effects of similarity-based interference on surprisal and attention",
    "citation_count": 13,
    "authors": [
      "Soo Hyun Ryu",
      "Richard Lewis"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.7": {
    "title": "CMCL 2021 Shared Task on Eye-Tracking Prediction",
    "volume": "workshop",
    "abstract": "Eye-tracking data from reading represent an important resource for both linguistics and natural language processing. The ability to accurately model gaze features is crucial to advance our understanding of language processing. This paper describes the Shared Task on Eye-Tracking Data Prediction, jointly organized with the eleventh edition of the Work- shop on Cognitive Modeling and Computational Linguistics (CMCL 2021). The goal of the task is to predict 5 different token- level eye-tracking metrics of the Zurich Cognitive Language Processing Corpus (ZuCo). Eye-tracking data were recorded during natural reading of English sentences. In total, we received submissions from 13 registered teams, whose systems include boosting algorithms with handcrafted features, neural models leveraging transformer language models, or hybrid approaches. The winning system used a range of linguistic and psychometric features in a gradient boosting framework",
    "checked": true,
    "id": "9193cfcda57691c942eeba17965d95c14ec6e371",
    "semantic_title": "cmcl 2021 shared task on eye-tracking prediction",
    "citation_count": 31,
    "authors": [
      "Nora Hollenstein",
      "Emmanuele Chersoni",
      "Cassandra L. Jacobs",
      "Yohei Oseki",
      "Laurent Prévot",
      "Enrico Santus"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.8": {
    "title": "LangResearchLab_NC at CMCL2021 Shared Task: Predicting Gaze Behaviour Using Linguistic Features and Tree Regressors",
    "volume": "workshop",
    "abstract": "Analysis of gaze data behaviour has gained momentum in recent years for different NLP applications. The present paper aims at modelling gaze data behaviour of tokens in the context of a sentence. We have experimented with various Machine Learning Regression Algorithms on a feature space comprising the linguistic features of the target tokens for prediction of five Eye-Tracking features. CatBoost Regressor performed the best and achieved fourth position in terms of MAE based accuracy measurement for the ZuCo Dataset",
    "checked": true,
    "id": "c59e867541d80e0754ecf3c44065be237bd7fb7c",
    "semantic_title": "langresearchlab_nc at cmcl2021 shared task: predicting gaze behaviour using linguistic features and tree regressors",
    "citation_count": 2,
    "authors": [
      "Raksha Agarwal",
      "Niladri Chatterjee"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.9": {
    "title": "TorontoCL at CMCL 2021 Shared Task: RoBERTa with Multi-Stage Fine-Tuning for Eye-Tracking Prediction",
    "volume": "workshop",
    "abstract": "Eye movement data during reading is a useful source of information for understanding language comprehension processes. In this paper, we describe our submission to the CMCL 2021 shared task on predicting human reading patterns. Our model uses RoBERTa with a regression layer to predict 5 eye-tracking features. We train the model in two stages: we first fine-tune on the Provo corpus (another eye-tracking dataset), then fine-tune on the task data. We compare different Transformer models and apply ensembling methods to improve the performance. Our final submission achieves a MAE score of 3.929, ranking 3rd place out of 13 teams that participated in this shared task",
    "checked": true,
    "id": "310213842f3646d5fc0a5dbbd4e0ef3d0130b91c",
    "semantic_title": "torontocl at cmcl 2021 shared task: roberta with multi-stage fine-tuning for eye-tracking prediction",
    "citation_count": 6,
    "authors": [
      "Bai Li",
      "Frank Rudzicz"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.10": {
    "title": "LAST at CMCL 2021 Shared Task: Predicting Gaze Data During Reading with a Gradient Boosting Decision Tree Approach",
    "volume": "workshop",
    "abstract": "A LightGBM model fed with target word lexical characteristics and features obtained from word frequency lists, psychometric data and bigram association measures has been optimized for the 2021 CMCL Shared Task on Eye-Tracking Data Prediction. It obtained the best performance of all teams on two of the five eye-tracking measures to predict, allowing it to rank first on the official challenge criterion and to outperform all deep-learning based systems participating in the challenge",
    "checked": true,
    "id": "ab8717108efbcb3245bfa57d39eea349f70f5577",
    "semantic_title": "last at cmcl 2021 shared task: predicting gaze data during reading with a gradient boosting decision tree approach",
    "citation_count": 10,
    "authors": [
      "Yves Bestgen"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.11": {
    "title": "Team Ohio State at CMCL 2021 Shared Task: Fine-Tuned RoBERTa for Eye-Tracking Data Prediction",
    "volume": "workshop",
    "abstract": "This paper describes Team Ohio State's approach to the CMCL 2021 Shared Task, the goal of which is to predict five eye-tracking features from naturalistic self-paced reading corpora. For this task, we fine-tune a pre-trained neural language model (RoBERTa; Liu et al., 2019) to predict each feature based on the contextualized representations. Moreover, motivated by previous eye-tracking studies, we include word length in characters and proportion of sentence processed as two additional input features. Our best model strongly outperforms the baseline and is also competitive with other systems submitted to the shared task. An ablation study shows that the word length feature contributes to making more accurate predictions, indicating the usefulness of features that are specific to the eye-tracking paradigm",
    "checked": true,
    "id": "4e739adec62a894187a4bf80101eaae6841d6001",
    "semantic_title": "team ohio state at cmcl 2021 shared task: fine-tuned roberta for eye-tracking data prediction",
    "citation_count": 3,
    "authors": [
      "Byung-Doh Oh"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.12": {
    "title": "PIHKers at CMCL 2021 Shared Task: Cosine Similarity and Surprisal to Predict Human Reading Patterns",
    "volume": "workshop",
    "abstract": "Eye-tracking psycholinguistic studies have revealed that context-word semantic coherence and predictability influence language processing. In this paper we show our approach to predict eye-tracking features from the ZuCo dataset for the shared task of the Cognitive Modeling and Computational Linguistics (CMCL2021) workshop. Using both cosine similarity and surprisal within a regression model, we significantly improved the baseline Mean Absolute Error computed among five eye-tracking features",
    "checked": true,
    "id": "b4c380b6728417134138928273829f80703c6b1a",
    "semantic_title": "pihkers at cmcl 2021 shared task: cosine similarity and surprisal to predict human reading patterns",
    "citation_count": 6,
    "authors": [
      "Lavinia Salicchi",
      "Alessandro Lenci"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.13": {
    "title": "TALEP at CMCL 2021 Shared Task: Non Linear Combination of Low and High-Level Features for Predicting Eye-Tracking Data",
    "volume": "workshop",
    "abstract": "In this paper we describe our contribution to the CMCL 2021 Shared Task, which consists in predicting 5 different eye tracking variables from English tokenized text. Our approach is based on a neural network that combines both raw textual features we extracted from the text and parser-based features that include linguistic predictions (e.g. part of speech) and complexity metrics (e.g., entropy of parsing). We found that both the features we considered as well as the architecture of the neural model that combined these features played a role in the overall performance. Our system achieved relatively high accuracy on the test data of the challenge and was ranked 2nd out of 13 competing teams and a total of 30 submissions",
    "checked": true,
    "id": "d4bcdf4262494472c6431fe01f78991006656acb",
    "semantic_title": "talep at cmcl 2021 shared task: non linear combination of low and high-level features for predicting eye-tracking data",
    "citation_count": 2,
    "authors": [
      "Franck Dary",
      "Alexis Nasr",
      "Abdellah Fourtassi"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.14": {
    "title": "MTL782_IITD at CMCL 2021 Shared Task: Prediction of Eye-Tracking Features Using BERT Embeddings and Linguistic Features",
    "volume": "workshop",
    "abstract": "Reading and comprehension are quintessentially cognitive tasks. Eye movement acts as a surrogate to understand which part of a sentence is critical to the process of comprehension. The aim of the shared task is to predict five eye-tracking features for a given word of the input sentence. We experimented with several models based on LGBM (Light Gradient Boosting Machine) Regression, ANN (Artificial Neural Network), and CNN (Convolutional Neural Network), using BERT embeddings and some combination of linguistic features. Our submission using CNN achieved an average MAE of 4.0639 and ranked 7th in the shared task. The average MAE was further lowered to 3.994 in post-task evaluation",
    "checked": true,
    "id": "cd8674b67456ae3fa2b16049f3c86cbe44cb2237",
    "semantic_title": "mtl782_iitd at cmcl 2021 shared task: prediction of eye-tracking features using bert embeddings and linguistic features",
    "citation_count": 5,
    "authors": [
      "Shivani Choudhary",
      "Kushagri Tandon",
      "Raksha Agarwal",
      "Niladri Chatterjee"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.15": {
    "title": "KonTra at CMCL 2021 Shared Task: Predicting Eye Movements by Combining BERT with Surface, Linguistic and Behavioral Information",
    "volume": "workshop",
    "abstract": "This paper describes the submission of the team KonTra to the CMCL 2021 Shared Task on eye-tracking prediction. Our system combines the embeddings extracted from a fine-tuned BERT model with surface, linguistic and behavioral features, resulting in an average mean absolute error of 4.22 across all 5 eye-tracking measures. We show that word length and features representing the expectedness of a word are consistently the strongest predictors across all 5 eye-tracking measures",
    "checked": true,
    "id": "3ae3f952161bd05168fa3dff8f96208aeb774c7b",
    "semantic_title": "kontra at cmcl 2021 shared task: predicting eye movements by combining bert with surface, linguistic and behavioral information",
    "citation_count": 3,
    "authors": [
      "Qi Yu",
      "Aikaterini-Lida Kalouli",
      "Diego Frassinelli"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.16": {
    "title": "CogNLP-Sheffield at CMCL 2021 Shared Task: Blending Cognitively Inspired Features with Transformer-based Language Models for Predicting Eye Tracking Patterns",
    "volume": "workshop",
    "abstract": "The CogNLP-Sheffield submissions to the CMCL 2021 Shared Task examine the value of a variety of cognitively and linguistically inspired features for predicting eye tracking patterns, as both standalone model inputs and as supplements to contextual word embeddings (XLNet). Surprisingly, the smaller pre-trained model (XLNet-base) outperforms the larger (XLNet-large), and despite evidence that multi-word expressions (MWEs) provide cognitive processing advantages, MWE features provide little benefit to either model",
    "checked": true,
    "id": "ba70a468b3ee1f900e7d14543174419c704c70a1",
    "semantic_title": "cognlp-sheffield at cmcl 2021 shared task: blending cognitively inspired features with transformer-based language models for predicting eye tracking patterns",
    "citation_count": 4,
    "authors": [
      "Peter Vickers",
      "Rosa Wainwright",
      "Harish Tayyar Madabushi",
      "Aline Villavicencio"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.17": {
    "title": "Team ReadMe at CMCL 2021 Shared Task: Predicting Human Reading Patterns by Traditional Oculomotor Control Models and Machine Learning",
    "volume": "workshop",
    "abstract": "This system description paper describes our participation in CMCL 2021 shared task on predicting human reading patterns. Our focus in this study is making use of well-known,traditional oculomotor control models and machine learning systems. We present experiments with a traditional oculomotor control model (the EZ Reader) and two machine learning models (a linear regression model and a re-current network model), as well as combining the two different models. In all experiments we test effects of features well-known in the literature for predicting reading patterns, such as frequency, word length and predictability. Our experiments support the earlier findings that such features are useful when combined. Furthermore, we show that although machine learning models perform better in comparison to traditional models, combination of both gives a consistent improvement for predicting multiple eye tracking variables during reading",
    "checked": true,
    "id": "4103bbcb0a110a5d4598595f32fef391474d20b0",
    "semantic_title": "team readme at cmcl 2021 shared task: predicting human reading patterns by traditional oculomotor control models and machine learning",
    "citation_count": 1,
    "authors": [
      "Alisan Balkoca",
      "Abdullah Algan",
      "Cengiz Acarturk",
      "Çağrı Çöltekin"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.18": {
    "title": "Enhancing Cognitive Models of Emotions with Representation Learning",
    "volume": "workshop",
    "abstract": "We present a novel deep learning-based framework to generate embedding representations of fine-grained emotions that can be used to computationally describe psychological models of emotions. Our framework integrates a contextualized embedding encoder with a multi-head probing model that enables to interpret dynamically learned representations optimized for an emotion classification task. Our model is evaluated on the Empathetic Dialogue dataset and shows the state-of-the-art result for classifying 32 emotions. Our layer analysis can derive an emotion graph to depict hierarchical relations among the emotions. Our emotion representations can be used to generate an emotion wheel directly comparable to the one from Plutchik's model, and also augment the values of missing emotions in the PAD emotional state model",
    "checked": true,
    "id": "5b41b3c31edb20f7b08ebb8fa49b9d0887f01a3c",
    "semantic_title": "enhancing cognitive models of emotions with representation learning",
    "citation_count": 3,
    "authors": [
      "Yuting Guo",
      "Jinho D. Choi"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.19": {
    "title": "Production vs Perception: The Role of Individuality in Usage-Based Grammar Induction",
    "volume": "workshop",
    "abstract": "This paper asks whether a distinction between production-based and perception-based grammar induction influences either (i) the growth curve of grammars and lexicons or (ii) the similarity between representations learned from independent sub-sets of a corpus. A production-based model is trained on the usage of a single individual, thus simulating the grammatical knowledge of a single speaker. A perception-based model is trained on an aggregation of many individuals, thus simulating grammatical generalizations learned from exposure to many different speakers. To ensure robustness, the experiments are replicated across two registers of written English, with four additional registers reserved as a control. A set of three computational experiments shows that production-based grammars are significantly different from perception-based grammars across all conditions, with a steeper growth curve that can be explained by substantial inter-individual grammatical differences",
    "checked": true,
    "id": "ec27896fd21be2ff6b51608e8c111f6e185d08ef",
    "semantic_title": "production vs perception: the role of individuality in usage-based grammar induction",
    "citation_count": 6,
    "authors": [
      "Jonathan Dunn",
      "Andrea Nini"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.20": {
    "title": "Clause Final Verb Prediction in Hindi: Evidence for Noisy Channel Model of Communication",
    "volume": "workshop",
    "abstract": "Verbal prediction has been shown to be critical during online comprehension of Subject-Object-Verb (SOV) languages. In this work we present three computational models to predict clause final verbs in Hindi given its prior arguments. The models differ in their use of prior context during the prediction process – the context is either noisy or noise-free. Model predictions are compared with the sentence completion data obtained from Hindi native speakers. Results show that models that assume noisy context outperform the noise-free model. In particular, a lossy context model that assumes prior context to be affected by predictability and recency captures the distribution of the predicted verb class and error sources best. The success of the predictability-recency lossy context model is consistent with the noisy channel hypothesis for sentence comprehension and supports the idea that the reconstruction of the context during prediction is driven by prior linguistic exposure. These results also shed light on the nature of the noise that affects the reconstruction process. Overall the results pose a challenge to the adaptability hypothesis that assumes use of noise-free preverbal context for robust verbal prediction",
    "checked": true,
    "id": "2c3108bcbf4817e62e3457c5ae8caf9c1d6954a7",
    "semantic_title": "clause final verb prediction in hindi: evidence for noisy channel model of communication",
    "citation_count": 1,
    "authors": [
      "Kartik Sharma",
      "Niyati Bafna",
      "Samar Husain"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.21": {
    "title": "Dependency Locality and Neural Surprisal as Predictors of Processing Difficulty: Evidence from Reading Times",
    "volume": "workshop",
    "abstract": "This paper compares two influential theories of processing difficulty: Gibson (2000)'s Dependency Locality Theory (DLT) and Hale (2001)'s Surprisal Theory. While prior work has aimed to compare DLT and Surprisal Theory (see Demberg and Keller, 2008), they have not yet been compared using more modern and powerful methods for estimating surprisal and DLT integration cost. I compare estimated surprisal values from two models, an RNN and a Transformer neural network, as well as DLT integration cost from a hand-parsed treebank, to reading times from the Dundee Corpus. Our results for integration cost corroborate those of Demberg and Keller (2008), finding that it is a negative predictor of reading times overall and a strong positive predictor for nouns, but contrast with their observations for surprisal, finding strong evidence for lexicalized surprisal as a predictor of reading times. Ultimately, I conclude that a broad-coverage model must integrate both theories in order to most accurately predict processing difficulty",
    "checked": true,
    "id": "c27bf3ac510f1c384daac6d30b3e574b4b4ad1f6",
    "semantic_title": "dependency locality and neural surprisal as predictors of processing difficulty: evidence from reading times",
    "citation_count": 1,
    "authors": [
      "Neil Rathi"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.22": {
    "title": "Modeling Sentence Comprehension Deficits in Aphasia: A Computational Evaluation of the Direct-access Model of Retrieval",
    "volume": "workshop",
    "abstract": "Several researchers have argued that sentence comprehension is mediated via a content-addressable retrieval mechanism that allows fast and direct access to memory items. Initially failed retrievals can result in backtracking, which leads to correct retrieval. We present an augmented version of the direct-access model that allows backtracking to fail. Based on self-paced listening data from individuals with aphasia, we compare the augmented model to the base model without backtracking failures. The augmented model shows quantitatively similar performance to the base model, but only the augmented model can account for slow incorrect responses. We argue that the modified direct-access model is theoretically better suited to fit data from impaired populations",
    "checked": true,
    "id": "de0a3f0541f11a6922c3a7329407a039bb27a306",
    "semantic_title": "modeling sentence comprehension deficits in aphasia: a computational evaluation of the direct-access model of retrieval",
    "citation_count": 1,
    "authors": [
      "Paula Lissón",
      "Dorothea Pregla",
      "Dario Paape",
      "Frank Burchert",
      "Nicole Stadie",
      "Shravan Vasishth"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.23": {
    "title": "Sentence Complexity in Context",
    "volume": "workshop",
    "abstract": "We study the influence of context on how humans evaluate the complexity of a sentence in English. We collect a new dataset of sentences, where each sentence is rated for perceived complexity within different contextual windows. We carry out an in-depth analysis to detect which linguistic features correlate more with complexity judgments and with the degree of agreement among annotators. We train several regression models, using either explicit linguistic features or contextualized word embeddings, to predict the mean complexity values assigned to sentences in the different contextual windows, as well as their standard deviation. Results show that models leveraging explicit features capturing morphosyntactic and syntactic phenomena perform always better, especially when they have access to features extracted from all contextual sentences",
    "checked": true,
    "id": "47d12aad6baa0415f496c9920478e281faaf6b37",
    "semantic_title": "sentence complexity in context",
    "citation_count": 11,
    "authors": [
      "Benedetta Iavarone",
      "Dominique Brunato",
      "Felice Dell’Orletta"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.24": {
    "title": "Evaluating the Acquisition of Semantic Knowledge from Cross-situational Learning in Artificial Neural Networks",
    "volume": "workshop",
    "abstract": "When learning their native language, children acquire the meanings of words and sentences from highly ambiguous input without much explicit supervision. One possible learning mechanism is cross-situational learning, which has been successfully tested in laboratory experiments with children. Here we use Artificial Neural Networks to test if this mechanism scales up to more natural language and visual scenes using a large dataset of crowd-sourced images with corresponding descriptions. We evaluate learning using a series of tasks inspired by methods commonly used in laboratory studies of language acquisition. We show that the model acquires rich semantic knowledge both at the word- and sentence-level, mirroring the patterns and trajectory of learning in early childhood. Our work highlights the usefulness of low-level co-occurrence statistics across modalities in facilitating the early acquisition of higher-level semantic knowledge",
    "checked": true,
    "id": "9c52ea438bac33ae9230acf9236dbe836aba8346",
    "semantic_title": "evaluating the acquisition of semantic knowledge from cross-situational learning in artificial neural networks",
    "citation_count": 9,
    "authors": [
      "Mitja Nikolaus",
      "Abdellah Fourtassi"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.25": {
    "title": "Representation and Pre-Activation of Lexical-Semantic Knowledge in Neural Language Models",
    "volume": "workshop",
    "abstract": "In this paper, we perform a systematic analysis of how closely the intermediate layers from LSTM and trans former language models correspond to human semantic knowledge. Furthermore, in order to make more meaningful comparisons with theories of human language comprehension in psycholinguistics, we focus on two key stages where the meaning of a particular target word may arise: immediately before the word's presentation to the model (comparable to forward inferencing), and immediately after the word token has been input into the network. Our results indicate that the transformer models are better at capturing semantic knowledge relating to lexical concepts, both during word prediction and when retention is required",
    "checked": true,
    "id": "e05976bd4809f75ea7b22255db66d9d26b312f6d",
    "semantic_title": "representation and pre-activation of lexical-semantic knowledge in neural language models",
    "citation_count": 4,
    "authors": [
      "Steven Derby",
      "Paul Miller",
      "Barry Devereux"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.26": {
    "title": "Relation Classification with Cognitive Attention Supervision",
    "volume": "workshop",
    "abstract": "Many current language models such as BERT utilize attention mechanisms to transform sequence representations. We ask whether we can influence BERT's attention with human reading patterns by using eye-tracking and brain imaging data. We fine-tune BERT for relation extraction with auxiliary attention supervision in which BERT's attention weights are supervised by cognitive data. Through a variety of metrics we find that this attention supervision can be used to increase similarity between model attention distributions over sequences and the cognitive data without significantly affecting classification performance while making unique errors from the baseline. In particular, models with cognitive attention supervision more often correctly classified samples misclassified by the baseline",
    "checked": true,
    "id": "8c0d8027037a71e63cc12e060066ed417b82b429",
    "semantic_title": "relation classification with cognitive attention supervision",
    "citation_count": 5,
    "authors": [
      "Erik McGuire",
      "Noriko Tomuro"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.27": {
    "title": "Graph-theoretic Properties of the Class of Phonological Neighbourhood Networks",
    "volume": "workshop",
    "abstract": "This paper concerns the structure of phonological neighbourhood networks, which are a graph-theoretic representation of the phonological lexicon. These networks represent each word as a node and links are placed between words which are phonological neighbours, usually defined as a string edit distance of one. Phonological neighbourhood networks have been used to study many aspects of the mental lexicon and psycholinguistic theories of speech production and perception. This paper offers preliminary graph-theoretic observations about phonological neighbourhood networks considered as a class. To aid this exploration, this paper introduces the concept of the hyperlexicon, the network consisting of all possible words for a given symbol set and their neighbourhood relations. The construction of the hyperlexicon is discussed, and basic properties are derived. This work is among the first to directly address the nature of phonological neighbourhood networks from an analytic perspective",
    "checked": true,
    "id": "9042e56c4dcb56e4a1bf3e4d68770ac9dd8e5915",
    "semantic_title": "graph-theoretic properties of the class of phonological neighbourhood networks",
    "citation_count": 1,
    "authors": [
      "Rory Turnbull"
    ]
  },
  "https://aclanthology.org/2021.cmcl-1.28": {
    "title": "Contributions of Propositional Content and Syntactic Category Information in Sentence Processing",
    "volume": "workshop",
    "abstract": "Expectation-based theories of sentence processing posit that processing difficulty is determined by predictability in context. While predictability quantified via surprisal has gained empirical support, this representation-agnostic measure leaves open the question of how to best approximate the human comprehender's latent probability model. This work presents an incremental left-corner parser that incorporates information about both propositional content and syntactic categories into a single probability model. This parser can be trained to make parsing decisions conditioning on only one source of information, thus allowing a clean ablation of the relative contribution of propositional content and syntactic category information. Regression analyses show that surprisal estimates calculated from the full parser make a significant contribution to predicting self-paced reading times over those from the parser without syntactic category information, as well as a significant contribution to predicting eye-gaze durations over those from the parser without propositional content information. Taken together, these results suggest a role for propositional content and syntactic category information in incremental sentence processing",
    "checked": true,
    "id": "949a4e16a3da721e6e40b3b9f0072ca4743bcf3d",
    "semantic_title": "contributions of propositional content and syntactic category information in sentence processing",
    "citation_count": 3,
    "authors": [
      "Byung-Doh Oh",
      "William Schuler"
    ]
  },
  "https://aclanthology.org/2021.dash-1.1": {
    "title": "Leveraging Wikipedia Navigational Templates for Curating Domain-Specific Fuzzy Conceptual Bases",
    "volume": "workshop",
    "abstract": "Domain-specific conceptual bases use key concepts to capture domain scope and relevant information. Conceptual bases serve as a foundation for various downstream tasks, including ontology construction, information mapping, and analysis. However, building conceptual bases necessitates domain awareness and takes time. Wikipedia navigational templates offer multiple articles on the same/similar domain. It is possible to use the templates to recognize fundamental concepts that shape the domain. Earlier work in this domain used Wikipedia's structured and unstructured data to construct open-domain ontologies, domain terminologies, and knowledge bases. We present a novel method for leveraging navigational templates to create domain-specific fuzzy conceptual bases in this work. Our system generates knowledge graphs from the articles mentioned in the template, which we then process using Wikidata and machine learning algorithms. We filter important concepts using fuzzy logic on network metrics to create a crude conceptual base. Finally, the expert helps by refining the conceptual base. We demonstrate our system using an example of RNA virus antiviral drugs",
    "checked": true,
    "id": "3bef4a3fc642271bedbdc0ea3cb8a536cc938b75",
    "semantic_title": "leveraging wikipedia navigational templates for curating domain-specific fuzzy conceptual bases",
    "citation_count": 5,
    "authors": [
      "Krati Saxena",
      "Tushita Singh",
      "Ashwini Patil",
      "Sagar Sunkle",
      "Vinay Kulkarni"
    ]
  },
  "https://aclanthology.org/2021.dash-1.2": {
    "title": "It is better to Verify: Semi-Supervised Learning with a human in the loop for large-scale NLU models",
    "volume": "workshop",
    "abstract": "When a NLU model is updated, new utter- ances must be annotated to be included for training. However, manual annotation is very costly. We evaluate a semi-supervised learning workflow with a human in the loop in a produc- tion environment. The previous NLU model predicts the annotation of the new utterances, a human then reviews the predicted annotation. Only when the NLU prediction is assessed as incorrect the utterance is sent for human anno- tation. Experimental results show that the pro- posed workflow boosts the performance of the NLU model while significantly reducing the annotation volume. Specifically, in our setup, we see improvements of up to 14.16% for a recall-based metric and up to 9.57% for a F1- score based metric, while reducing the annota- tion volume by 97% and overall cost by 60% for each iteration",
    "checked": true,
    "id": "6e5b3e010cb1f342e700fe2ab29c5abe6c9b4061",
    "semantic_title": "it is better to verify: semi-supervised learning with a human in the loop for large-scale nlu models",
    "citation_count": 3,
    "authors": [
      "Verena Weber",
      "Enrico Piovano",
      "Melanie Bradford"
    ]
  },
  "https://aclanthology.org/2021.dash-1.3": {
    "title": "ViziTex: Interactive Visual Sense-Making of Text Corpora",
    "volume": "workshop",
    "abstract": "Information visualization is critical to analytical reasoning and knowledge discovery. We present an interactive studio that integrates perceptive visualization techniques with powerful text analytics algorithms to assist humans in sense-making of large complex text corpora. The novel visual representations introduced here encode the features delivered by modern text mining models using advanced metaphors such as hypergraphs, nested topologies and tessellated planes. They enhance human-computer interaction experience for various tasks such as summarization, exploration, organization and labeling of documents. We demonstrate the ability of the visuals to surface the structure, relations and concepts from documents across different domains",
    "checked": true,
    "id": "dce54248f202e6a1ba98c66859cd58bd24007f98",
    "semantic_title": "vizitex: interactive visual sense-making of text corpora",
    "citation_count": 1,
    "authors": [
      "Natraj Raman",
      "Sameena Shah",
      "Tucker Balch",
      "Manuela Veloso"
    ]
  },
  "https://aclanthology.org/2021.dash-1.4": {
    "title": "A Visualization Approach for Rapid Labeling of Clinical Notes for Smoking Status Extraction",
    "volume": "workshop",
    "abstract": "Labeling is typically the most human-intensive step during the development of supervised learning models. In this paper, we propose a simple and easy-to-implement visualization approach that reduces cognitive load and increases the speed of text labeling. The approach is fine-tuned for task of extraction of patient smoking status from clinical notes. The proposed approach consists of the ordering of sentences that mention smoking, centering them at smoking tokens, and annotating to enhance informative parts of the text. Our experiments on clinical notes from the MIMIC-III clinical database demonstrate that our visualization approach enables human annotators to label sentences up to 3 times faster than with a baseline approach",
    "checked": true,
    "id": "776d7934d80e0395782cfa14ed9b839066e1c7d2",
    "semantic_title": "a visualization approach for rapid labeling of clinical notes for smoking status extraction",
    "citation_count": 10,
    "authors": [
      "Saman Enayati",
      "Ziyu Yang",
      "Benjamin Lu",
      "Slobodan Vucetic"
    ]
  },
  "https://aclanthology.org/2021.dash-1.5": {
    "title": "Semi-supervised Interactive Intent Labeling",
    "volume": "workshop",
    "abstract": "Building the Natural Language Understanding (NLU) modules of task-oriented Spoken Dialogue Systems (SDS) involves a definition of intents and entities, collection of task-relevant data, annotating the data with intents and entities, and then repeating the same process over and over again for adding any functionality/enhancement to the SDS. In this work, we showcase an Intent Bulk Labeling system where SDS developers can interactively label and augment training data from unlabeled utterance corpora using advanced clustering and visual labeling methods. We extend the Deep Aligned Clustering work with a better backbone BERT model, explore techniques to select the seed data for labeling, and develop a data balancing method using an oversampling technique that utilizes paraphrasing models. We also look at the effect of data augmentation on the clustering process. Our results show that we can achieve over 10% gain in clustering accuracy on some datasets using the combination of the above techniques. Finally, we extract utterance embeddings from the clustering model and plot the data to interactively bulk label the samples, reducing the time and effort for data labeling of the whole dataset significantly",
    "checked": true,
    "id": "677daf92e768e65900988352fa7cbbdef5e37b91",
    "semantic_title": "semi-supervised interactive intent labeling",
    "citation_count": 6,
    "authors": [
      "Saurav Sahay",
      "Eda Okur",
      "Nagib Hakim",
      "Lama Nachman"
    ]
  },
  "https://aclanthology.org/2021.dash-1.6": {
    "title": "Human-In-The-LoopEntity Linking for Low Resource Domains",
    "volume": "workshop",
    "abstract": "Entity linking (EL) is concerned with disambiguating entity mentions in a text against knowledge bases (KB). To quickly annotate texts with EL even in low-resource domains and noisy text, we present a novel Human-In-The-Loop EL approach. We show that it greatly outperforms a strong baseline in simulation. In a user study, annotation time is reduced by 35 % compared to annotating without interactive support; users report that they strongly prefer our system over ones without. An open-source and ready-to-use implementation based on the text annotation platform is made available",
    "checked": true,
    "id": "c196e85fc2c057c47f42d977f8dc6569b578e59d",
    "semantic_title": "human-in-the-loopentity linking for low resource domains",
    "citation_count": 0,
    "authors": [
      "Jan-Christoph Klie",
      "Richard Eckart de Castilho",
      "Iryna Gurevych"
    ]
  },
  "https://aclanthology.org/2021.dash-1.7": {
    "title": "Bridging Multi-disciplinary Collaboration Challenges in ML Development via Domain Knowledge Elicitation",
    "volume": "workshop",
    "abstract": "Building a machine learning model in a sophisticated domain is a time-consuming process, partially due to the steep learning curve of domain knowledge for data scientists. We introduce Ziva, an interface for supporting domain knowledge from domain experts to data scientists in two ways: (1) a concept creation interface where domain experts extract important concept of the domain and (2) five kinds of justification elicitation interfaces that solicit elicitation how the domain concept are expressed in data instances",
    "checked": true,
    "id": "10b1c4a88041ed0c7510224f5c52bb2026b2f7bd",
    "semantic_title": "bridging multi-disciplinary collaboration challenges in ml development via domain knowledge elicitation",
    "citation_count": 0,
    "authors": [
      "Soya Park"
    ]
  },
  "https://aclanthology.org/2021.dash-1.8": {
    "title": "Active learning and negative evidence for language identification",
    "volume": "workshop",
    "abstract": "Language identification (LID), the task of determining the natural language of a given text, is an essential first step in most NLP pipelines. While generally a solved problem for documents of sufficient length and languages with ample training data, the proliferation of microblogs and other social media has made it increasingly common to encounter use-cases that *don't* satisfy these conditions. In these situations, the fundamental difficulty is the lack of, and cost of gathering, labeled data: unlike some annotation tasks, no single \"expert\" can quickly and reliably identify more than a handful of languages. This leads to a natural question: can we gain useful information when annotators are only able to *rule out* languages for a given document, rather than supply a positive label? What are the optimal choices for gathering and representing such *negative evidence* as a model is trained? In this paper, we demonstrate that using negative evidence can improve the performance of a simple neural LID model. This improvement is sensitive to policies of how the evidence is represented in the loss function, and for deciding which annotators to employ given the instance and model state. We consider simple policies and report experimental results that indicate the optimal choices for this task. We conclude with a discussion of future work to determine if and how the results generalize to other classification tasks",
    "checked": true,
    "id": "2a262baf8b7e86490fe6c34b5de1d81d5546f1d7",
    "semantic_title": "active learning and negative evidence for language identification",
    "citation_count": 2,
    "authors": [
      "Thomas Lippincott",
      "Ben Van Durme"
    ]
  },
  "https://aclanthology.org/2021.dash-1.9": {
    "title": "Towards integrated, interactive, and extensible text data analytics with Leam",
    "volume": "workshop",
    "abstract": "From tweets to product reviews, text is ubiquitous on the web and often contains valuable information for both enterprises and consumers. However, the online text is generally noisy and incomplete, requiring users to process and analyze the data to extract insights. While there are systems effective for different stages of text analysis, users lack extensible platforms to support interactive text analysis workflows end-to-end. To facilitate integrated text analytics, we introduce LEAM, which aims at combining the strengths of spreadsheets, computational notebooks, and interactive visualizations. LEAM supports interactive analysis via GUI-based interactions and provides a declarative specification language, implemented based on a visual text algebra, to enable user-guided analysis. We evaluate LEAM through two case studies using two popular Kaggle text analytics workflows to understand the strengths and weaknesses of the system",
    "checked": true,
    "id": "d0706c90bf29ef5ffb7c68d6af51624af7702ab4",
    "semantic_title": "towards integrated, interactive, and extensible text data analytics with leam",
    "citation_count": 2,
    "authors": [
      "Peter Griggs",
      "Cagatay Demiralp",
      "Sajjadur Rahman"
    ]
  },
  "https://aclanthology.org/2021.dash-1.10": {
    "title": "Data Cleaning Tools for Token Classification Tasks",
    "volume": "workshop",
    "abstract": "Human-in-the-loop systems for cleaning NLP training data rely on automated sieves to isolate potentially-incorrect labels for manual review. We have developed a novel technique for flagging potentially-incorrect labels with high sensitivity in named entity recognition corpora. We incorporated our sieve into an end-to-end system for cleaning NLP corpora, implemented as a modular collection of Jupyter notebooks built on extensions to the Pandas DataFrame library. We used this system to identify incorrect labels in the CoNLL-2003 corpus for English-language named entity recognition (NER), one of the most influential corpora for NER model research. Unlike previous work that only looked at a subset of the corpus's validation fold, our automated sieve enabled us to examine the entire corpus in depth. Across the entire CoNLL-2003 corpus, we identified over 1300 incorrect labels (out of 35089 in the corpus). We have published our corrections, along with the code we used in our experiments. We are developing a repeatable version of the process we used on the CoNLL-2003 corpus as an open-source library",
    "checked": true,
    "id": "a6b85381b2763d6afb134482f62317f7b69dcc53",
    "semantic_title": "data cleaning tools for token classification tasks",
    "citation_count": 5,
    "authors": [
      "Karthik Muthuraman",
      "Frederick Reiss",
      "Hong Xu",
      "Bryan Cutler",
      "Zachary Eichenberger"
    ]
  },
  "https://aclanthology.org/2021.dash-1.11": {
    "title": "Building Low-Resource NER Models Using Non-Speaker Annotations",
    "volume": "workshop",
    "abstract": "In low-resource natural language processing (NLP), the key problems are a lack of target language training data, and a lack of native speakers to create it. Cross-lingual methods have had notable success in addressing these concerns, but in certain common circumstances, such as insufficient pre-training corpora or languages far from the source language, their performance suffers. In this work we propose a complementary approach to building low-resource Named Entity Recognition (NER) models using \"non-speaker\" (NS) annotations, provided by annotators with no prior experience in the target language. We recruit 30 participants in a carefully controlled annotation experiment with Indonesian, Russian, and Hindi. We show that use of NS annotators produces results that are consistently on par or better than cross-lingual methods built on modern contextual representations, and have the potential to outperform with additional effort. We conclude with observations of common annotation patterns and recommended implementation practices, and motivate how NS annotations can be used in addition to prior methods for improved performance",
    "checked": true,
    "id": "7fd10513cc34c56653200ce31484298309168b11",
    "semantic_title": "building low-resource ner models using non-speaker annotations",
    "citation_count": 6,
    "authors": [
      "Tatiana Tsygankova",
      "Francesca Marini",
      "Stephen Mayhew",
      "Dan Roth"
    ]
  },
  "https://aclanthology.org/2021.dash-1.12": {
    "title": "Evaluating and Explaining Natural Language Generation with GenX",
    "volume": "workshop",
    "abstract": "Current methods for evaluation of natural language generation models focus on measuring text quality but fail to probe the model creativity, i.e., its ability to generate novel but coherent text sequences not seen in the training corpus. We present the GenX tool which is designed to enable interactive exploration and explanation of natural language generation outputs with a focus on the detection of memorization. We demonstrate the utility of the tool on two domain-conditioned generation use cases - phishing emails and ACL abstracts",
    "checked": true,
    "id": "eb860daa62e3db436fe698f7409fe3c1efd2ffd5",
    "semantic_title": "evaluating and explaining natural language generation with genx",
    "citation_count": 1,
    "authors": [
      "Kayla Duskin",
      "Shivam Sharma",
      "Ji Young Yun",
      "Emily Saldanha",
      "Dustin Arendt"
    ]
  },
  "https://aclanthology.org/2021.dash-1.13": {
    "title": "CrossCheck: Rapid, Reproducible, and Interpretable Model Evaluation",
    "volume": "workshop",
    "abstract": "Evaluation beyond aggregate performance metrics, e.g. F1-score, is crucial to both establish an appropriate level of trust in machine learning models and identify avenues for future model improvements. In this paper we demonstrate CrossCheck, an interactive capability for rapid cross-model comparison and reproducible error analysis. We describe the tool, discuss design and implementation details, and present three NLP use cases – named entity recognition, reading comprehension, and clickbait detection that show the benefits of using the tool for model evaluation. CrossCheck enables users to make informed decisions when choosing between multiple models, identify when the models are correct and for which examples, investigate whether the models are making the same mistakes as humans, evaluate models' generalizability and highlight models' limitations, strengths and weaknesses. Furthermore, CrossCheck is implemented as a Jupyter widget, which allows for rapid and convenient integration into existing model development workflows",
    "checked": true,
    "id": "8b2f270a5ef28b0dea0bc7700dbe34515e8b2473",
    "semantic_title": "crosscheck: rapid, reproducible, and interpretable model evaluation",
    "citation_count": 8,
    "authors": [
      "Dustin Arendt",
      "Zhuanyi Shaw",
      "Prasha Shrestha",
      "Ellyn Ayton",
      "Maria Glenski",
      "Svitlana Volkova"
    ]
  },
  "https://aclanthology.org/2021.dash-1.14": {
    "title": "TopGuNN: Fast NLP Training Data Augmentation using Large Corpora",
    "volume": "workshop",
    "abstract": "Acquiring training data for natural language processing systems can be expensive and time-consuming. Given a few training examples crafted by experts, large corpora can be mined for thousands of semantically similar examples that provide useful variability to improve model generalization. We present TopGuNN, a fast contextualized k-NN retrieval system that can efficiently index and search over contextual embeddings generated from large corpora. TopGuNN is demonstrated for a training data augmentation use case over the Gigaword corpus. Using approximate k-NN and an efficient architecture, TopGuNN performs queries over an embedding space of 4.63TB (approximately 1.5B embeddings) in less than a day",
    "checked": true,
    "id": "603893589d298633c8617c00d9010a9753d682fa",
    "semantic_title": "topgunn: fast nlp training data augmentation using large corpora",
    "citation_count": 2,
    "authors": [
      "Rebecca Iglesias-Flores",
      "Megha Mishra",
      "Ajay Patel",
      "Akanksha Malhotra",
      "Reno Kriz",
      "Martha Palmer",
      "Chris Callison-Burch"
    ]
  },
  "https://aclanthology.org/2021.dash-1.15": {
    "title": "Everyday Living Artificial Intelligence Hub",
    "volume": "workshop",
    "abstract": "We present the Everyday Living Artificial Intelligence (AI) Hub, a novel proof-of-concept framework for enhancing human health and wellbeing via a combination of tailored wear-able and Conversational Agent (CA) solutions for non-invasive monitoring of physiological signals, assessment of behaviors through unobtrusive wearable devices, and the provision of personalized interventions to reduce stress and anxiety. We utilize recent advancements and industry standards in the Internet of Things (IoT)and AI technologies to develop this proof-of-concept framework",
    "checked": true,
    "id": "a79089aba4a88d716b91ecc4e2e11f1283a77b24",
    "semantic_title": "everyday living artificial intelligence hub",
    "citation_count": 0,
    "authors": [
      "Raymond Finzel",
      "Esha Singh",
      "Martin Michalowski",
      "Maria Gini",
      "Serguei Pakhomov"
    ]
  },
  "https://aclanthology.org/2021.dash-1.16": {
    "title": "A Computational Model for Interactive Transcription",
    "volume": "workshop",
    "abstract": "Transcribing low resource languages can be challenging in the absence of a good lexicon and trained transcribers. Accordingly, we seek a way to enable interactive transcription whereby the machine amplifies human efforts. This paper presents a data model and a system architecture for interactive transcription, supporting multiple modes of interactivity, increasing the likelihood of finding tasks that engage local participation in language work. The approach also supports other applications which are useful in our context, including spoken document retrieval and language learning",
    "checked": true,
    "id": "705d75eb6c687d303f84f48e0f8794a405713767",
    "semantic_title": "a computational model for interactive transcription",
    "citation_count": 5,
    "authors": [
      "William Lane",
      "Mat Bettinson",
      "Steven Bird"
    ]
  },
  "https://aclanthology.org/2021.deelio-1.1": {
    "title": "Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors",
    "volume": "workshop",
    "abstract": "Transformer networks have revolutionized NLP representation learning since they were introduced. Though a great effort has been made to explain the representation in transformers, it is widely recognized that our understanding is not sufficient. One important reason is that there lack enough visualization tools for detailed analysis. In this paper, we propose to use dictionary learning to open up these ‘black boxes' as linear superpositions of transformer factors. Through visualization, we demonstrate the hierarchical semantic structures captured by the transformer factors, e.g., word-level polysemy disambiguation, sentence-level pattern formation, and long-range dependency. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights. We hope this visualization tool can bring further knowledge and a better understanding of how transformer networks work. The code is available at: https://github.com/zeyuyun1/TransformerVis",
    "checked": true,
    "id": "7cc88a1a904e8bb6edc1123c0800d1c5a0ea435d",
    "semantic_title": "transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors",
    "citation_count": 24,
    "authors": [
      "Zeyu Yun",
      "Yubei Chen",
      "Bruno Olshausen",
      "Yann LeCun"
    ]
  },
  "https://aclanthology.org/2021.deelio-1.2": {
    "title": "Reconstructing Implicit Knowledge with Language Models",
    "volume": "workshop",
    "abstract": "In this work we propose an approach for generating statements that explicate implicit knowledge connecting sentences in text. We make use of pre-trained language models which we refine by fine-tuning them on specifically prepared corpora that we enriched with implicit information, and by constraining them with relevant concepts and connecting commonsense knowledge paths. Manual and automatic evaluation of the generations shows that by refining language models as proposed, we can generate coherent and grammatically sound sentences that explicate implicit knowledge which connects sentence pairs in texts – on both in-domain and out-of-domain test data",
    "checked": true,
    "id": "07803a577293540f405547a2eb3c0c41b6ed8bb2",
    "semantic_title": "reconstructing implicit knowledge with language models",
    "citation_count": 15,
    "authors": [
      "Maria Becker",
      "Siting Liang",
      "Anette Frank"
    ]
  },
  "https://aclanthology.org/2021.deelio-1.3": {
    "title": "Investigating the Effect of Background Knowledge on Natural Questions",
    "volume": "workshop",
    "abstract": "Existing work shows the benefits of integrating KBs with textual evidence for QA only on questions that are answerable by KBs alone (Sun et al., 2019). In contrast, real world QA systems often have to deal with questions that might not be directly answerable by KBs. Here, we investigate the effect of integrating background knowledge from KBs for the Natural Questions (NQ) task. We create a subset of the NQ data, Factual Questions (FQ), where the questions have evidence in the KB in the form of paths that link question entities to answer entities but still must be answered using text, to facilitate further research into KB integration methods. We propose and analyze a simple, model-agnostic approach for incorporating KB paths into text-based QA systems and establish a strong upper bound on FQ for our method using an oracle retriever. We show that several variants of Personalized PageRank based fact retrievers lead to a low recall of answer entities and consequently fail to improve QA performance. Our results suggest that fact retrieval is a bottleneck for integrating KBs into real world QA datasets",
    "checked": true,
    "id": "60163eab80666401862f16c01a799368ddd8be8d",
    "semantic_title": "investigating the effect of background knowledge on natural questions",
    "citation_count": 4,
    "authors": [
      "Vidhisha Balachandran",
      "Bhuwan Dhingra",
      "Haitian Sun",
      "Michael Collins",
      "William Cohen"
    ]
  },
  "https://aclanthology.org/2021.deelio-1.4": {
    "title": "Augmenting Topic Aware Knowledge-Grounded Conversations with Dynamic Built Knowledge Graphs",
    "volume": "workshop",
    "abstract": "Dialog topic management and background knowledge selection are essential factors for the success of knowledge-grounded open-domain conversations. However, existing models are primarily performed with symmetric knowledge bases or stylized with pre-defined roles between conversational partners, while people usually have their own knowledge before a real chit-chat. To address this problem, we propose a dynamic knowledge graph-based topical conversation model (DKGT). Given a dialog history context, our model first builds knowledge graphs from the context as an imitation of human's ability to form logical relationships between known and unknown topics during a conversation. This logical information will be fed into a topic predictor to promote topic management, then facilitate background knowledge selection and response generation. To the best of our knowledge, this is the first attempt to dynamically form knowledge graphs between chatting topics to assist dialog topic management during a conversation. Experimental results manifest that our model can properly schedule conversational topics and pick suitable knowledge to generate informative responses comparing to several strong baselines",
    "checked": true,
    "id": "36b461d478640081dcac28bc25ff4244d0a13d03",
    "semantic_title": "augmenting topic aware knowledge-grounded conversations with dynamic built knowledge graphs",
    "citation_count": 2,
    "authors": [
      "Junjie Wu",
      "Hao Zhou"
    ]
  },
  "https://aclanthology.org/2021.deelio-1.5": {
    "title": "What Makes My Model Perplexed? A Linguistic Investigation on Neural Language Models Perplexity",
    "volume": "workshop",
    "abstract": "This paper presents an investigation aimed at studying how the linguistic structure of a sentence affects the perplexity of two of the most popular Neural Language Models (NLMs), BERT and GPT-2. We first compare the sentence-level likelihood computed with BERT and the GPT-2's perplexity showing that the two metrics are correlated. In addition, we exploit linguistic features capturing a wide set of morpho-syntactic and syntactic phenomena showing how they contribute to predict the perplexity of the two NLMs",
    "checked": true,
    "id": "96a05ab61538547efe2c81ab00318688a52d3a62",
    "semantic_title": "what makes my model perplexed? a linguistic investigation on neural language models perplexity",
    "citation_count": 5,
    "authors": [
      "Alessio Miaschi",
      "Dominique Brunato",
      "Felice Dell’Orletta",
      "Giulia Venturi"
    ]
  },
  "https://aclanthology.org/2021.deelio-1.6": {
    "title": "How Do BERT Embeddings Organize Linguistic Knowledge?",
    "volume": "workshop",
    "abstract": "Several studies investigated the linguistic information implicitly encoded in Neural Language Models. Most of these works focused on quantifying the amount and type of information available within their internal representations and across their layers. In line with this scenario, we proposed a different study, based on Lasso regression, aimed at understanding how the information encoded by BERT sentence-level representations is arrange within its hidden units. Using a suite of several probing tasks, we showed the existence of a relationship between the implicit knowledge learned by the model and the number of individual units involved in the encodings of this competence. Moreover, we found that it is possible to identify groups of hidden units more relevant for specific linguistic properties",
    "checked": true,
    "id": "773472a423d176fedfc46ba03287ed41995c99f7",
    "semantic_title": "how do bert embeddings organize linguistic knowledge?",
    "citation_count": 13,
    "authors": [
      "Giovanni Puccetti",
      "Alessio Miaschi",
      "Felice Dell’Orletta"
    ]
  },
  "https://aclanthology.org/2021.deelio-1.7": {
    "title": "ERNIE-NLI: Analyzing the Impact of Domain-Specific External Knowledge on Enhanced Representations for NLI",
    "volume": "workshop",
    "abstract": "We examine the effect of domain-specific external knowledge variations on deep large scale language model performance. Recent work in enhancing BERT with external knowledge has been very popular, resulting in models such as ERNIE (Zhang et al., 2019a). Using the ERNIE architecture, we provide a detailed analysis on the types of knowledge that result in a performance increase on the Natural Language Inference (NLI) task, specifically on the Multi-Genre Natural Language Inference Corpus (MNLI). While ERNIE uses general TransE embeddings, we instead train domain-specific knowledge embeddings and insert this knowledge via an information fusion layer in the ERNIE architecture, allowing us to directly control and analyze knowledge input. Using several different knowledge training objectives, sources of knowledge, and knowledge ablations, we find a strong correlation between knowledge and classification labels within the same polarity, illustrating that knowledge polarity is an important feature in predicting entailment. We also perform classification change analysis across different knowledge variations to illustrate the importance of selecting appropriate knowledge input regarding content and polarity, and show representative examples of these changes",
    "checked": true,
    "id": "84c8d939c765dd30574e6c7e6d0a3eb82db1c8fb",
    "semantic_title": "ernie-nli: analyzing the impact of domain-specific external knowledge on enhanced representations for nli",
    "citation_count": 8,
    "authors": [
      "Lisa Bauer",
      "Lingjia Deng",
      "Mohit Bansal"
    ]
  },
  "https://aclanthology.org/2021.deelio-1.8": {
    "title": "Enhancing Multiple-Choice Question Answering with Causal Knowledge",
    "volume": "workshop",
    "abstract": "The task of causal question answering aims to reason about causes and effects over a provided real or hypothetical premise. Recent approaches have converged on using transformer-based language models to solve question answering tasks. However, pretrained language models often struggle when external knowledge is not present in the premise or when additional context is required to answer the question. To the best of our knowledge, no prior work has explored the efficacy of augmenting pretrained language models with external causal knowledge for multiple-choice causal question answering. In this paper, we present novel strategies for the representation of causal knowledge. Our empirical results demonstrate the efficacy of augmenting pretrained models with external causal knowledge. We show improved performance on the COPA (Choice of Plausible Alternatives) and WIQA (What If Reasoning Over Procedural Text) benchmark tasks. On the WIQA benchmark, our approach is competitive with the state-of-the-art and exceeds it within the evaluation subcategories of In-Paragraph and Out-of-Paragraph perturbations",
    "checked": true,
    "id": "4e9c745c52eab2e0d99ce9334cb96e9211941e5c",
    "semantic_title": "enhancing multiple-choice question answering with causal knowledge",
    "citation_count": 13,
    "authors": [
      "Dhairya Dalal",
      "Mihael Arcan",
      "Paul Buitelaar"
    ]
  },
  "https://aclanthology.org/2021.deelio-1.9": {
    "title": "Low Anisotropy Sense Retrofitting (LASeR) : Towards Isotropic and Sense Enriched Representations",
    "volume": "workshop",
    "abstract": "Contextual word representation models have shown massive improvements on a multitude of NLP tasks, yet their word sense disambiguation capabilities remain poorly explained. To address this gap, we assess whether contextual word representations extracted from deep pretrained language models create distinguishable representations for different senses of a given word. We analyze the representation geometry and find that most layers of deep pretrained language models create highly anisotropic representations, pointing towards the existence of representation degeneration problem in contextual word representations. After accounting for anisotropy, our study further reveals that there is variability in sense learning capabilities across different language models. Finally, we propose LASeR, a ‘Low Anisotropy Sense Retrofitting' approach that renders off-the-shelf representations isotropic and semantically more meaningful, resolving the representation degeneration problem as a post-processing step, and conducting sense-enrichment of contextualized representations extracted from deep neural language models",
    "checked": true,
    "id": "67cf77e563571ff05fcbd6b15845c6fd3a75fa78",
    "semantic_title": "low anisotropy sense retrofitting (laser) : towards isotropic and sense enriched representations",
    "citation_count": 10,
    "authors": [
      "Geetanjali Bihani",
      "Julia Rayz"
    ]
  },
  "https://aclanthology.org/2021.deelio-1.10": {
    "title": "KW-ATTN: Knowledge Infused Attention for Accurate and Interpretable Text Classification",
    "volume": "workshop",
    "abstract": "Text classification has wide-ranging applications in various domains. While neural network approaches have drastically advanced performance in text classification, they tend to be powered by a large amount of training data, and interpretability is often an issue. As a step towards better accuracy and interpretability especially on small data, in this paper we present a new knowledge-infused attention mechanism, called KW-ATTN (KnoWledge-infused ATTentioN) to incorporate high-level concepts from external knowledge bases into Neural Network models. We show that KW-ATTN outperforms baseline models using only words as well as other approaches using concepts by classification accuracy, which indicates that high-level concepts help model prediction. Furthermore, crowdsourced human evaluation suggests that additional concept information helps interpretability of the model",
    "checked": true,
    "id": "1e6e2ec7cdb37962a9a1e6dd48262951e2f25ff5",
    "semantic_title": "kw-attn: knowledge infused attention for accurate and interpretable text classification",
    "citation_count": 6,
    "authors": [
      "Hyeju Jang",
      "Seojin Bang",
      "Wen Xiao",
      "Giuseppe Carenini",
      "Raymond Ng",
      "Young ji Lee"
    ]
  },
  "https://aclanthology.org/2021.deelio-1.11": {
    "title": "Multi-input Recurrent Independent Mechanisms for leveraging knowledge sources: Case studies on sentiment analysis and health text mining",
    "volume": "workshop",
    "abstract": "This paper presents a way to inject and leverage existing knowledge from external sources in a Deep Learning environment, extending the recently proposed Recurrent Independent Mechnisms (RIMs) architecture, which comprises a set of interacting yet independent modules. We show that this extension of the RIMs architecture is an effective framework with lower parameter implications compared to purely fine-tuned systems",
    "checked": true,
    "id": "6d3dff5242f731b43b48130e51f16e4795167095",
    "semantic_title": "multi-input recurrent independent mechanisms for leveraging knowledge sources: case studies on sentiment analysis and health text mining",
    "citation_count": 8,
    "authors": [
      "Parsa Bagherzadeh",
      "Sabine Bergler"
    ]
  },
  "https://aclanthology.org/2021.deelio-1.12": {
    "title": "What BERTs and GPTs know about your brand? Probing contextual language models for affect associations",
    "volume": "workshop",
    "abstract": "Investigating brand perception is fundamental to marketing strategies. In this regard, brand image, defined by a set of attributes (Aaker, 1997), is recognized as a key element in indicating how a brand is perceived by various stakeholders such as consumers and competitors. Traditional approaches (e.g., surveys) to monitor brand perceptions are time-consuming and inefficient. In the era of digital marketing, both brand managers and consumers engage with a vast amount of digital marketing content. The exponential growth of digital content has propelled the emergence of pre-trained language models such as BERT and GPT as essential tools in solving myriads of challenges with textual data. This paper seeks to investigate the extent of brand perceptions (i.e., brand and image attribute associations) these language models encode. We believe that any kind of bias for a brand and attribute pair may influence customer-centric downstream tasks such as recommender systems, sentiment analysis, and question-answering, e.g., suggesting a specific brand consistently when queried for innovative products. We use synthetic data and real-life data and report comparison results for five contextual LMs, viz. BERT, RoBERTa, DistilBERT, ALBERT and BART",
    "checked": true,
    "id": "bc5f9a01a0b9e3015c094ff92f5ff1863612a2b7",
    "semantic_title": "what berts and gpts know about your brand? probing contextual language models for affect associations",
    "citation_count": 0,
    "authors": [
      "Vivek Srivastava",
      "Stephen Pilli",
      "Savita Bhat",
      "Niranjan Pedanekar",
      "Shirish Karande"
    ]
  },
  "https://aclanthology.org/2021.deelio-1.13": {
    "title": "Attention vs non-attention for a Shapley-based explanation method",
    "volume": "workshop",
    "abstract": "The field of explainable AI has recently seen an explosion in the number of explanation methods for highly non-linear deep neural networks. The extent to which such methods – that are often proposed and tested in the domain of computer vision – are appropriate to address the explainability challenges in NLP is yet relatively unexplored. In this work, we consider Contextual Decomposition (CD) – a Shapley-based input feature attribution method that has been shown to work well for recurrent NLP models – and we test the extent to which it is useful for models that contain attention operations. To this end, we extend CD to cover the operations necessary for attention-based models. We then compare how long distance subject-verb relationships are processed by models with and without attention, considering a number of different syntactic structures in two different languages: English and Dutch. Our experiments confirm that CD can successfully be applied for attention-based models as well, providing an alternative Shapley-based attribution method for modern neural networks. In particular, using CD, we show that the English and Dutch models demonstrate similar processing behaviour, but that under the hood there are consistent differences between our attention and non-attention models",
    "checked": true,
    "id": "d5de10333fb28b3987df399273cb6aa47ce02a7b",
    "semantic_title": "attention vs non-attention for a shapley-based explanation method",
    "citation_count": 3,
    "authors": [
      "Tom Kersten",
      "Hugh Mee Wong",
      "Jaap Jumelet",
      "Dieuwke Hupkes"
    ]
  },
  "https://aclanthology.org/2021.deelio-1.14": {
    "title": "Predicting Numerals in Natural Language Text Using a Language Model Considering the Quantitative Aspects of Numerals",
    "volume": "workshop",
    "abstract": "Numerical common sense (NCS) is necessary to fully understand natural language text that includes numerals. NCS is knowledge about the numerical features of objects in text, such as size, weight, or color. Existing neural language models treat numerals in a text as string tokens in the same way as other words. Therefore, they cannot reflect the quantitative aspects of numerals in the training process, making it difficult to learn NCS. In this paper, we measure the NCS acquired by existing neural language models using a masked numeral prediction task as an evaluation task. In this task, we use two evaluation metrics to evaluate the language models in terms of the symbolic and quantitative aspects of the numerals, respectively. We also propose methods to reflect not only the symbolic aspect but also the quantitative aspect of numerals in the training of language models, using a loss function that depends on the magnitudes of the numerals and a regression model for the masked numeral prediction task. Finally, we quantitatively evaluate our proposed approaches on four datasets with different properties using the two metrics. Compared with methods that use existing language models, the proposed methods reduce numerical absolute errors, although exact match accuracy was reduced. This result confirms that the proposed methods, which use the magnitudes of the numerals for model training, are an effective way for models to capture NCS",
    "checked": true,
    "id": "c9343c26a0e604f7afd94b7290bbdf8d96cd65b6",
    "semantic_title": "predicting numerals in natural language text using a language model considering the quantitative aspects of numerals",
    "citation_count": 2,
    "authors": [
      "Taku Sakamoto",
      "Akiko Aizawa"
    ]
  },
  "https://aclanthology.org/2021.maiworkshop-1.1": {
    "title": "Multimodal Weighted Fusion of Transformers for Movie Genre Classification",
    "volume": "workshop",
    "abstract": "The Multimodal Transformer showed to be a competitive model for multimodal tasks involving textual, visual and audio signals. However, as more modalities are involved, its late fusion by concatenation starts to have a negative impact on the model's performance. Besides, interpreting model's predictions becomes difficult, as one would have to look at the different attention activation matrices. In order to overcome these shortcomings, we propose to perform late fusion by adding a GMU module, which effectively allows the model to weight modalities at instance level, improving its performance while providing a better interpretabilty mechanism. In the experiments, we compare our proposed model (MulT-GMU) against the original implementation (MulT-Concat) and a SOTA model tested in a movie genre classification dataset. Our approach, MulT-GMU, outperforms both, MulT-Concat and previous SOTA model",
    "checked": true,
    "id": "eee3189699bce4a9315ff30a790e28b8a3326b1b",
    "semantic_title": "multimodal weighted fusion of transformers for movie genre classification",
    "citation_count": 5,
    "authors": [
      "Isaac Rodríguez Bribiesca",
      "Adrián Pastor López Monroy",
      "Manuel Montes-y-Gómez"
    ]
  },
  "https://aclanthology.org/2021.maiworkshop-1.2": {
    "title": "On Randomized Classification Layers and Their Implications in Natural Language Generation",
    "volume": "workshop",
    "abstract": "In natural language generation tasks, a neural language model is used for generating a sequence of words forming a sentence. The topmost weight matrix of the language model, known as the classification layer, can be viewed as a set of vectors, each representing a target word from the target dictionary. The target word vectors, along with the rest of the model parameters, are learned and updated during training. In this paper, we analyze the properties encoded in the target vectors and question the necessity of learning these vectors. We suggest to randomly draw the target vectors and set them as fixed so that no weights updates are being made during training. We show that by excluding the vectors from the optimization, the number of parameters drastically decreases with a marginal effect on the performance. We demonstrate the effectiveness of our method in image-captioning and machine-translation",
    "checked": true,
    "id": "e40339c217548679b77d9fa194963e0be7d371a8",
    "semantic_title": "on randomized classification layers and their implications in natural language generation",
    "citation_count": 2,
    "authors": [
      "Gal-Lev Shalev",
      "Gabi Shalev",
      "Joseph Keshet"
    ]
  },
  "https://aclanthology.org/2021.maiworkshop-1.3": {
    "title": "COIN: Conversational Interactive Networks for Emotion Recognition in Conversation",
    "volume": "workshop",
    "abstract": "Emotion recognition in conversation has received considerable attention recently because of its practical industrial applications. Existing methods tend to overlook the immediate mutual interaction between different speakers in the speaker-utterance level, or apply single speaker-agnostic RNN for utterances from different speakers. We propose COIN, a conversational interactive model to mitigate this problem by applying state mutual interaction within history contexts. In addition, we introduce a stacked global interaction module to capture the contextual and inter-dependency representation in a hierarchical manner. To improve the robustness and generalization during training, we generate adversarial examples by applying the minor perturbations on multimodal feature inputs, unveiling the benefits of adversarial examples for emotion detection. The proposed model empirically achieves the current state-of-the-art results on the IEMOCAP benchmark dataset",
    "checked": true,
    "id": "0072bb736d83bac426fd4db75153b533df993ecc",
    "semantic_title": "coin: conversational interactive networks for emotion recognition in conversation",
    "citation_count": 9,
    "authors": [
      "Haidong Zhang",
      "Yekun Chai"
    ]
  },
  "https://aclanthology.org/2021.maiworkshop-1.4": {
    "title": "A First Look: Towards Explainable TextVQA Models via Visual and Textual Explanations",
    "volume": "workshop",
    "abstract": "Explainable deep learning models are advantageous in many situations. Prior work mostly provide unimodal explanations through post-hoc approaches not part of the original system design. Explanation mechanisms also ignore useful textual information present in images. In this paper, we propose MTXNet, an end-to-end trainable multimodal architecture to generate multimodal explanations, which focuses on the text in the image. We curate a novel dataset TextVQA-X, containing ground truth visual and multi-reference textual explanations that can be leveraged during both training and evaluation. We then quantitatively show that training with multimodal explanations complements model performance and surpasses unimodal baselines by up to 7% in CIDEr scores and 2% in IoU. More importantly, we demonstrate that the multimodal explanations are consistent with human interpretations, help justify the models' decision, and provide useful insights to help diagnose an incorrect prediction. Finally, we describe a real-world e-commerce application for using the generated multimodal explanations",
    "checked": true,
    "id": "d9135f69875aaf0efea38c796aadceabdcb2c1cc",
    "semantic_title": "a first look: towards explainable textvqa models via visual and textual explanations",
    "citation_count": 10,
    "authors": [
      "Varun Nagaraj Rao",
      "Xingjian Zhen",
      "Karen Hovsepian",
      "Mingwei Shen"
    ]
  },
  "https://aclanthology.org/2021.maiworkshop-1.5": {
    "title": "Multi Task Learning based Framework for Multimodal Classification",
    "volume": "workshop",
    "abstract": "Large-scale multi-modal classification aim to distinguish between different multi-modal data, and it has drawn dramatically attentions since last decade. In this paper, we propose a multi-task learning-based framework for the multimodal classification task, which consists of two branches: multi-modal autoencoder branch and attention-based multi-modal modeling branch. Multi-modal autoencoder can receive multi-modal features and obtain the interactive information which called multi-modal encoder feature, and use this feature to reconstitute all the input data. Besides, multi-modal encoder feature can be used to enrich the raw dataset, and improve the performance of downstream tasks (such as classification task). As for attention-based multimodal modeling branch, we first employ attention mechanism to make the model focused on important features, then we use the multi-modal encoder feature to enrich the input information, achieve a better performance. We conduct extensive experiments on different dataset, the results demonstrate the effectiveness of proposed framework",
    "checked": true,
    "id": "bfaeb9a0396ef384d814c599340be0d3e8051258",
    "semantic_title": "multi task learning based framework for multimodal classification",
    "citation_count": 1,
    "authors": [
      "Danting Zeng"
    ]
  },
  "https://aclanthology.org/2021.maiworkshop-1.6": {
    "title": "Validity-Based Sampling and Smoothing Methods for Multiple Reference Image Captioning",
    "volume": "workshop",
    "abstract": "In image captioning, multiple captions are often provided as ground truths, since a valid caption is not always uniquely determined. Conventional methods randomly select a single caption and treat it as correct, but there have been few effective training methods that utilize multiple given captions. In this paper, we proposed two training technique for making effective use of multiple reference captions: 1) validity-based caption sampling (VBCS), which prioritizes the use of captions that are estimated to be highly valid during training, and 2) weighted caption smoothing (WCS), which applies smoothing only to the relevant words the reference caption to reflect multiple reference captions simultaneously. Experiments show that our proposed methods improve CIDEr by 2.6 points and BLEU4 by 0.9 points from baseline on the MSCOCO dataset",
    "checked": true,
    "id": "a5878295f400df1cba04398bdaf4f0b2e5c71af8",
    "semantic_title": "validity-based sampling and smoothing methods for multiple reference image captioning",
    "citation_count": 0,
    "authors": [
      "Shunta Nagasawa",
      "Yotaro Watanabe",
      "Hitoshi Iyatomi"
    ]
  },
  "https://aclanthology.org/2021.maiworkshop-1.7": {
    "title": "Modality-specific Distillation",
    "volume": "workshop",
    "abstract": "Large neural networks are impractical to deploy on mobile devices due to their heavy computational cost and slow inference. Knowledge distillation (KD) is a technique to reduce the model size while retaining performance by transferring knowledge from a large \"teacher\" model to a smaller \"student\" model. However, KD on multimodal datasets such as vision-language datasets is relatively unexplored and digesting such multimodal information is challenging since different modalities present different types of information. In this paper, we propose modality-specific distillation (MSD) to effectively transfer knowledge from a teacher on multimodal datasets. Existing KD approaches can be applied to multimodal setup, but a student doesn't have access to modality-specific predictions. Our idea aims at mimicking a teacher's modality-specific predictions by introducing an auxiliary loss term for each modality. Because each modality has different importance for predictions, we also propose weighting approaches for the auxiliary losses; a meta-learning approach to learn the optimal weights on these loss terms. In our experiments, we demonstrate the effectiveness of our MSD and the weighting scheme and show that it achieves better performance than KD",
    "checked": true,
    "id": "8f6770bf003f0c4ada6fd468b5084c5522ef8828",
    "semantic_title": "modality-specific distillation",
    "citation_count": 4,
    "authors": [
      "Woojeong Jin",
      "Maziar Sanjabi",
      "Shaoliang Nie",
      "Liang Tan",
      "Xiang Ren",
      "Hamed Firooz"
    ]
  },
  "https://aclanthology.org/2021.maiworkshop-1.8": {
    "title": "Cold Start Problem For Automated Live Video Comments",
    "volume": "workshop",
    "abstract": "Live video comments, or \"danmu\", are an emerging feature on Asian online video platforms. Danmu are time-synchronous comments that are overlaid on a video playback. These comments uniquely enrich the experience and engagement of their users. These comments have become a determining factor in the popularity of the videos. Similar to the \"cold start problem\" in recommender systems, a video will only start to attract attention when sufficient danmu comments have been posted on it. We study this video cold start problem and examine how new comments can be generated automatically on less-commented videos. We propose to predict the danmu comments by exploiting a multi-modal combination of the video visual content, subtitles, audio signals, and any surrounding comments (when they exist). Our method fuses these multi-modalities in a transformer network which is then trained for different comment density scenarios. We evaluate our proposed system through both a retrieval based evaluation method, as well as human judgement. Results show that our proposed system improves significantly over state-of-the-art methods",
    "checked": true,
    "id": "af0aa14e280f81aa87eeca63889e07219797e083",
    "semantic_title": "cold start problem for automated live video comments",
    "citation_count": 4,
    "authors": [
      "Hao Wu",
      "François Pitie",
      "Gareth Jones"
    ]
  },
  "https://aclanthology.org/2021.maiworkshop-1.9": {
    "title": "¡Qué maravilla! Multimodal Sarcasm Detection in Spanish: a Dataset and a Baseline",
    "volume": "workshop",
    "abstract": "We construct the first ever multimodal sarcasm dataset for Spanish. The audiovisual dataset consists of sarcasm annotated text that is aligned with video and audio. The dataset represents two varieties of Spanish, a Latin American variety and a Peninsular Spanish variety, which ensures a wider dialectal coverage for this global language. We present several models for sarcasm detection that will serve as baselines in the future research. Our results show that results with text only (89%) are worse than when combining text with audio (91.9%). Finally, the best results are obtained when combining all the modalities: text, audio and video (93.1%). Our dataset will be published on Zenodo with access granted by request",
    "checked": true,
    "id": "f4ea380812ed5f376f1134d5f95994a5b46bb1c1",
    "semantic_title": "¡qué maravilla! multimodal sarcasm detection in spanish: a dataset and a baseline",
    "citation_count": 5,
    "authors": [
      "Khalid Alnajjar",
      "Mika Hämäläinen"
    ]
  },
  "https://aclanthology.org/2021.maiworkshop-1.10": {
    "title": "A Package for Learning on Tabular and Text Data with Transformers",
    "volume": "workshop",
    "abstract": "Recent progress in natural language processing has led to Transformer architectures becoming the predominant model used for natural language tasks. However, in many real- world datasets, additional modalities are included which the Transformer does not directly leverage. We present Multimodal- Toolkit, an open-source Python package to incorporate text and tabular (categorical and numerical) data with Transformers for downstream applications. Our toolkit integrates well with Hugging Face's existing API such as tokenization and the model hub which allows easy download of different pre-trained models",
    "checked": true,
    "id": "d93b5079c9d668d89948ab2b8e14e88f63ff806b",
    "semantic_title": "a package for learning on tabular and text data with transformers",
    "citation_count": 31,
    "authors": [
      "Ken Gu",
      "Akshay Budhkar"
    ]
  },
  "https://aclanthology.org/2021.maiworkshop-1.11": {
    "title": "Semantic Aligned Multi-modal Transformer for Vision-LanguageUnderstanding: A Preliminary Study on Visual QA",
    "volume": "workshop",
    "abstract": "Recent vision-language understanding approaches adopt a multi-modal transformer pre-training and finetuning paradigm. Prior work learns representations of text tokens and visual features with cross-attention mechanisms and captures the alignment solely based on indirect signals. In this work, we propose to enhance the alignment mechanism by incorporating image scene graph structures as the bridge between the two modalities, and learning with new contrastive objectives. In our preliminary study on the challenging compositional visual question answering task, we show the proposed approach achieves improved results, demonstrating potentials to enhance vision-language understanding",
    "checked": true,
    "id": "38f684d6af7ffde97c7a92da2caae51c5d44ec87",
    "semantic_title": "semantic aligned multi-modal transformer for vision-languageunderstanding: a preliminary study on visual qa",
    "citation_count": 1,
    "authors": [
      "Han Ding",
      "Li Erran Li",
      "Zhiting Hu",
      "Yi Xu",
      "Dilek Hakkani-Tur",
      "Zheng Du",
      "Belinda Zeng"
    ]
  },
  "https://aclanthology.org/2021.maiworkshop-1.12": {
    "title": "GraghVQA: Language-Guided Graph Neural Networks for Graph-based Visual Question Answering",
    "volume": "workshop",
    "abstract": "Images are more than a collection of objects or attributes — they represent a web of relationships among interconnected objects. Scene Graph has emerged as a new modality as a structured graphical representation of images. Scene Graph encodes objects as nodes connected via pairwise relations as edges. To support question answering on scene graphs, we propose GraphVQA, a language-guided graph neural network framework that translates and executes a natural language question as multiple iterations of message passing among graph nodes. We explore the design space of GraphVQA framework, and discuss the trade-off of different design choices. Our experiments on GQA dataset show that GraphVQA outperforms the state-of-the-art accuracy by a large margin (88.43% vs. 94.78%)",
    "checked": true,
    "id": "6badb4a7395b26511ef5c873fcb9c42bb651addc",
    "semantic_title": "graghvqa: language-guided graph neural networks for graph-based visual question answering",
    "citation_count": 21,
    "authors": [
      "Weixin Liang",
      "Yanhao Jiang",
      "Zixuan Liu"
    ]
  },
  "https://aclanthology.org/2021.maiworkshop-1.13": {
    "title": "Learning to Select Question-Relevant Relations for Visual Question Answering",
    "volume": "workshop",
    "abstract": "Previous existing visual question answering (VQA) systems commonly use graph neural networks(GNNs) to extract visual relationships such as semantic relations or spatial relations. However, studies that use GNNs typically ignore the importance of each relation and simply concatenate outputs from multiple relation encoders. In this paper, we propose a novel layer architecture that fuses multiple visual relations through an attention mechanism to address this issue. Specifically, we develop a model that uses question embedding and joint embedding of the encoders to obtain dynamic attention weights with regard to the type of questions. Using the learnable attention weights, the proposed model can efficiently use the necessary visual relation features for a given question. Experimental results on the VQA 2.0 dataset demonstrate that the proposed model outperforms existing graph attention network-based architectures. Additionally, we visualize the attention weight and show that the proposed model assigns a higher weight to relations that are more relevant to the question",
    "checked": true,
    "id": "e298a0983f61f1ca39b51c1249bfe93017afdafe",
    "semantic_title": "learning to select question-relevant relations for visual question answering",
    "citation_count": 3,
    "authors": [
      "Jaewoong Lee",
      "Heejoon Lee",
      "Hwanhee Lee",
      "Kyomin Jung"
    ]
  },
  "https://aclanthology.org/2021.nlp4if-1.1": {
    "title": "Identifying Automatically Generated Headlines using Transformers",
    "volume": "workshop",
    "abstract": "False information spread via the internet and social media influences public opinion and user activity, while generative models enable fake content to be generated faster and more cheaply than had previously been possible. In the not so distant future, identifying fake content generated by deep learning models will play a key role in protecting users from misinformation. To this end, a dataset containing human and computer-generated headlines was created and a user study indicated that humans were only able to identify the fake headlines in 47.8% of the cases. However, the most accurate automatic approach, transformers, achieved an overall accuracy of 85.7%, indicating that content generated from language models can be filtered out accurately",
    "checked": true,
    "id": "c375dbef75ff5c0ff6923d77d55645118f392168",
    "semantic_title": "identifying automatically generated headlines using transformers",
    "citation_count": 6,
    "authors": [
      "Antonis Maronikolakis",
      "Hinrich Schütze",
      "Mark Stevenson"
    ]
  },
  "https://aclanthology.org/2021.nlp4if-1.2": {
    "title": "Improving Hate Speech Type and Target Detection with Hateful Metaphor Features",
    "volume": "workshop",
    "abstract": "We study the usefulness of hateful metaphorsas features for the identification of the type and target of hate speech in Dutch Facebook comments. For this purpose, all hateful metaphors in the Dutch LiLaH corpus were annotated and interpreted in line with Conceptual Metaphor Theory and Critical Metaphor Analysis. We provide SVM and BERT/RoBERTa results, and investigate the effect of different metaphor information encoding methods on hate speech type and target detection accuracy. The results of the conducted experiments show that hateful metaphor features improve model performance for the both tasks. To our knowledge, it is the first time that the effectiveness of hateful metaphors as an information source for hatespeech classification is investigated",
    "checked": true,
    "id": "3337534d9a9100d959a8bb9521a80b9eb8c15f80",
    "semantic_title": "improving hate speech type and target detection with hateful metaphor features",
    "citation_count": 17,
    "authors": [
      "Jens Lemmens",
      "Ilia Markov",
      "Walter Daelemans"
    ]
  },
  "https://aclanthology.org/2021.nlp4if-1.3": {
    "title": "Improving Cross-Domain Hate Speech Detection by Reducing the False Positive Rate",
    "volume": "workshop",
    "abstract": "Hate speech detection is an actively growing field of research with a variety of recently proposed approaches that allowed to push the state-of-the-art results. One of the challenges of such automated approaches – namely recent deep learning models – is a risk of false positives (i.e., false accusations), which may lead to over-blocking or removal of harmless social media content in applications with little moderator intervention. We evaluate deep learning models both under in-domain and cross-domain hate speech detection conditions, and introduce an SVM approach that allows to significantly improve the state-of-the-art results when combined with the deep learning models through a simple majority-voting ensemble. The improvement is mainly due to a reduction of the false positive rate",
    "checked": true,
    "id": "83bac727650f8c830e90b0a174d0b100bbc6988d",
    "semantic_title": "improving cross-domain hate speech detection by reducing the false positive rate",
    "citation_count": 13,
    "authors": [
      "Ilia Markov",
      "Walter Daelemans"
    ]
  },
  "https://aclanthology.org/2021.nlp4if-1.4": {
    "title": "Understanding the Impact of Evidence-Aware Sentence Selection for Fact Checking",
    "volume": "workshop",
    "abstract": "Fact Extraction and VERification (FEVER) is a recently introduced task that consists of the following subtasks (i) document retrieval, (ii) sentence retrieval, and (iii) claim verification. In this work, we focus on the subtask of sentence retrieval. Specifically, we propose an evidence-aware transformer-based model that outperforms all other models in terms of FEVER score by using a subset of training instances. In addition, we conduct a large experimental study to get a better understanding of the problem, while we summarize our findings by presenting future research challenges",
    "checked": true,
    "id": "f344875f205baf6ec74ebd088ff57e6231ef42ff",
    "semantic_title": "understanding the impact of evidence-aware sentence selection for fact checking",
    "citation_count": 5,
    "authors": [
      "Giannis Bekoulis",
      "Christina Papagiannopoulou",
      "Nikos Deligiannis"
    ]
  },
  "https://aclanthology.org/2021.nlp4if-1.5": {
    "title": "Leveraging Community and Author Context to Explain the Performance and Bias of Text-Based Deception Detection Models",
    "volume": "workshop",
    "abstract": "Deceptive news posts shared in online communities can be detected with NLP models, and much recent research has focused on the development of such models. In this work, we use characteristics of online communities and authors — the context of how and where content is posted — to explain the performance of a neural network deception detection model and identify sub-populations who are disproportionately affected by model accuracy or failure. We examine who is posting the content, and where the content is posted to. We find that while author characteristics are better predictors of deceptive content than community characteristics, both characteristics are strongly correlated with model performance. Traditional performance metrics such as F1 score may fail to capture poor model performance on isolated sub-populations such as specific authors, and as such, more nuanced evaluation of deception detection models is critical",
    "checked": true,
    "id": "3ef4e04bb2ebbca191244812217ae309a382d693",
    "semantic_title": "leveraging community and author context to explain the performance and bias of text-based deception detection models",
    "citation_count": 0,
    "authors": [
      "Galen Weld",
      "Ellyn Ayton",
      "Tim Althoff",
      "Maria Glenski"
    ]
  },
  "https://aclanthology.org/2021.nlp4if-1.6": {
    "title": "Never guess what I heard... Rumor Detection in Finnish News: a Dataset and a Baseline",
    "volume": "workshop",
    "abstract": "This study presents a new dataset on rumor detection in Finnish language news headlines. We have evaluated two different LSTM based models and two different BERT models, and have found very significant differences in the results. A fine-tuned FinBERT reaches the best overall accuracy of 94.3% and rumor label accuracy of 96.0% of the time. However, a model fine-tuned on Multilingual BERT reaches the best factual label accuracy of 97.2%. Our results suggest that the performance difference is due to a difference in the original training data. Furthermore, we find that a regular LSTM model works better than one trained with a pretrained word2vec model. These findings suggest that more work needs to be done for pretrained models in Finnish language as they have been trained on small and biased corpora",
    "checked": true,
    "id": "a19df9455a7fa62c32f08f88946d8ac3aa7bf09d",
    "semantic_title": "never guess what i heard... rumor detection in finnish news: a dataset and a baseline",
    "citation_count": 6,
    "authors": [
      "Mika Hämäläinen",
      "Khalid Alnajjar",
      "Niko Partanen",
      "Jack Rueter"
    ]
  },
  "https://aclanthology.org/2021.nlp4if-1.7": {
    "title": "Extractive and Abstractive Explanations for Fact-Checking and Evaluation of News",
    "volume": "workshop",
    "abstract": "In this paper, we explore the construction of natural language explanations for news claims, with the goal of assisting fact-checking and news evaluation applications. We experiment with two methods: (1) an extractive method based on Biased TextRank – a resource-effective unsupervised graph-based algorithm for content extraction; and (2) an abstractive method based on the GPT-2 language model. We perform comparative evaluations on two misinformation datasets in the political and health news domains, and find that the extractive method shows the most promise",
    "checked": true,
    "id": "69116800a8a8195531d29c8e14cefb1c92cbb8a7",
    "semantic_title": "extractive and abstractive explanations for fact-checking and evaluation of news",
    "citation_count": 11,
    "authors": [
      "Ashkan Kazemi",
      "Zehua Li",
      "Verónica Pérez-Rosas",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2021.nlp4if-1.8": {
    "title": "Generalisability of Topic Models in Cross-corpora Abusive Language Detection",
    "volume": "workshop",
    "abstract": "Rapidly changing social media content calls for robust and generalisable abuse detection models. However, the state-of-the-art supervised models display degraded performance when they are evaluated on abusive comments that differ from the training corpus. We investigate if the performance of supervised models for cross-corpora abuse detection can be improved by incorporating additional information from topic models, as the latter can infer the latent topic mixtures from unseen samples. In particular, we combine topical information with representations from a model tuned for classifying abusive comments. Our performance analysis reveals that topic models are able to capture abuse-related topics that can transfer across corpora, and result in improved generalisability",
    "checked": true,
    "id": "530c81d88eb9ee299dd17462ec8f0c4f560bd885",
    "semantic_title": "generalisability of topic models in cross-corpora abusive language detection",
    "citation_count": 3,
    "authors": [
      "Tulika Bose",
      "Irina Illina",
      "Dominique Fohr"
    ]
  },
  "https://aclanthology.org/2021.nlp4if-1.9": {
    "title": "AraStance: A Multi-Country and Multi-Domain Dataset of Arabic Stance Detection for Fact Checking",
    "volume": "workshop",
    "abstract": "With the continuing spread of misinformation and disinformation online, it is of increasing importance to develop combating mechanisms at scale in the form of automated systems that support multiple languages. One task of interest is claim veracity prediction, which can be addressed using stance detection with respect to relevant documents retrieved online. To this end, we present our new Arabic Stance Detection dataset (AraStance) of 4,063 claim–article pairs from a diverse set of sources comprising three fact-checking websites and one news website. AraStance covers false and true claims from multiple domains (e.g., politics, sports, health) and several Arab countries, and it is well-balanced between related and unrelated documents with respect to the claims. We benchmark AraStance, along with two other stance detection datasets, using a number of BERT-based models. Our best model achieves an accuracy of 85% and a macro F1 score of 78%, which leaves room for improvement and reflects the challenging nature of AraStance and the task of stance detection in general",
    "checked": true,
    "id": "bd063b5fac129a122642e9dce42b0f68ba84c8ff",
    "semantic_title": "arastance: a multi-country and multi-domain dataset of arabic stance detection for fact checking",
    "citation_count": 17,
    "authors": [
      "Tariq Alhindi",
      "Amal Alabdulkarim",
      "Ali Alshehri",
      "Muhammad Abdul-Mageed",
      "Preslav Nakov"
    ]
  },
  "https://aclanthology.org/2021.nlp4if-1.10": {
    "title": "MEAN: Multi-head Entity Aware Attention Networkfor Political Perspective Detection in News Media",
    "volume": "workshop",
    "abstract": "The way information is generated and disseminated has changed dramatically over the last decade. Identifying the political perspective shaping the way events are discussed in the media becomes more important due to the sharp increase in the number of news outlets and articles. Previous approaches usually only leverage linguistic information. However, news articles attempt to maintain credibility and seem impartial. Therefore, bias is introduced in subtle ways, usually by emphasizing different aspects of the story. In this paper, we propose a novel framework that considers entities mentioned in news articles and external knowledge about them, capturing the bias with respect to those entities. We explore different ways to inject entity information into the text model. Experiments show that our proposed framework achieves significant improvements over the standard text models, and is capable of identifying the difference in news narratives with different perspectives",
    "checked": true,
    "id": "aa284ddcf885d4b0f3b0692d91d7286d29952c38",
    "semantic_title": "mean: multi-head entity aware attention networkfor political perspective detection in news media",
    "citation_count": 13,
    "authors": [
      "Chang Li",
      "Dan Goldwasser"
    ]
  },
  "https://aclanthology.org/2021.nlp4if-1.11": {
    "title": "An Empirical Assessment of the Qualitative Aspects of Misinformation in Health News",
    "volume": "workshop",
    "abstract": "The explosion of online health news articles runs the risk of the proliferation of low-quality information. Within the existing work on fact-checking, however, relatively little attention has been paid to medical news. We present a health news classification task to determine whether medical news articles satisfy a set of review criteria deemed important by medical experts and health care journalists. We present a dataset of 1,119 health news paired with systematic reviews. The review criteria consist of six elements that are essential to the accuracy of medical news. We then present experiments comparing the classical token-based approach with the more recent transformer-based models. Our results show that detecting qualitative lapses is a challenging task with direct ramifications in misinformation, but is an important direction to pursue beyond assigning True or False labels to short claims",
    "checked": true,
    "id": "54352318d9e429a299f3b36dfe05bec9d845f87a",
    "semantic_title": "an empirical assessment of the qualitative aspects of misinformation in health news",
    "citation_count": 5,
    "authors": [
      "Chaoyuan Zuo",
      "Qi Zhang",
      "Ritwik Banerjee"
    ]
  },
  "https://aclanthology.org/2021.nlp4if-1.12": {
    "title": "Findings of the NLP4IF-2021 Shared Tasks on Fighting the COVID-19 Infodemic and Censorship Detection",
    "volume": "workshop",
    "abstract": "We present the results and the main findings of the NLP4IF-2021 shared tasks. Task 1 focused on fighting the COVID-19 infodemic in social media, and it was offered in Arabic, Bulgarian, and English. Given a tweet, it asked to predict whether that tweet contains a verifiable claim, and if so, whether it is likely to be false, is of general interest, is likely to be harmful, and is worthy of manual fact-checking; also, whether it is harmful to society, and whether it requires the attention of policy makers. Task 2 focused on censorship detection, and was offered in Chinese. A total of ten teams submitted systems for task 1, and one team participated in task 2; nine teams also submitted a system description paper. Here, we present the tasks, analyze the results, and discuss the system submissions and the methods they used. Most submissions achieved sizable improvements over several baselines, and the best systems used pre-trained Transformers and ensembles. The data, the scorers and the leaderboards for the tasks are available at http://gitlab.com/NLP4IF/nlp4if-2021",
    "checked": true,
    "id": "f44f2214e7c6f2c209006c32f889bf905489e306",
    "semantic_title": "findings of the nlp4if-2021 shared tasks on fighting the covid-19 infodemic and censorship detection",
    "citation_count": 33,
    "authors": [
      "Shaden Shaar",
      "Firoj Alam",
      "Giovanni Da San Martino",
      "Alex Nikolov",
      "Wajdi Zaghouani",
      "Preslav Nakov",
      "Anna Feldman"
    ]
  },
  "https://aclanthology.org/2021.nlp4if-1.13": {
    "title": "DamascusTeam at NLP4IF2021: Fighting the Arabic COVID-19 Infodemic on Twitter Using AraBERT",
    "volume": "workshop",
    "abstract": "The objective of this work was the introduction of an effective approach based on the AraBERT language model for fighting Tweets COVID-19 Infodemic. It was arranged in the form of a two-step pipeline, where the first step involved a series of pre-processing procedures to transform Twitter jargon, including emojis and emoticons, into plain text, and the second step exploited a version of AraBERT, which was pre-trained on plain text, to fine-tune and classify the tweets with respect to their Label. The use of language models pre-trained on plain texts rather than on tweets was motivated by the necessity to address two critical issues shown by the scientific literature, namely (1) pre-trained language models are widely available in many languages, avoiding the time-consuming and resource-intensive model training directly on tweets from scratch, allowing to focus only on their fine-tuning; (2) available plain text corpora are larger than tweet-only ones, allowing for better performance",
    "checked": true,
    "id": "bf9ebf48038ead66888bb23ca55a9153c7ffcca7",
    "semantic_title": "damascusteam at nlp4if2021: fighting the arabic covid-19 infodemic on twitter using arabert",
    "citation_count": 8,
    "authors": [
      "Ahmad Hussein",
      "Nada Ghneim",
      "Ammar Joukhadar"
    ]
  },
  "https://aclanthology.org/2021.nlp4if-1.14": {
    "title": "NARNIA at NLP4IF-2021: Identification of Misinformation in COVID-19 Tweets Using BERTweet",
    "volume": "workshop",
    "abstract": "The spread of COVID-19 has been accompanied with widespread misinformation on social media. In particular, Twitterverse has seen a huge increase in dissemination of distorted facts and figures. The present work aims at identifying tweets regarding COVID-19 which contains harmful and false information. We have experimented with a number of Deep Learning-based models, including different word embeddings, such as Glove, ELMo, among others. BERTweet model achieved the best overall F1-score of 0.881 and secured the third rank on the above task",
    "checked": true,
    "id": "c2274a0ea168da854092cafb6b1b81eb288f463b",
    "semantic_title": "narnia at nlp4if-2021: identification of misinformation in covid-19 tweets using bertweet",
    "citation_count": 3,
    "authors": [
      "Ankit Kumar",
      "Naman Jhunjhunwala",
      "Raksha Agarwal",
      "Niladri Chatterjee"
    ]
  },
  "https://aclanthology.org/2021.nlp4if-1.15": {
    "title": "R00 at NLP4IF-2021 Fighting COVID-19 Infodemic with Transformers and More Transformers",
    "volume": "workshop",
    "abstract": "This paper describes the winning model in the Arabic NLP4IF shared task for fighting the COVID-19 infodemic. The goal of the shared task is to check disinformation about COVID-19 in Arabic tweets. Our proposed model has been ranked 1st with an F1-Score of 0.780 and an Accuracy score of 0.762. A variety of transformer-based pre-trained language models have been experimented with through this study. The best-scored model is an ensemble of AraBERT-Base, Asafya-BERT, and ARBERT models. One of the study's key findings is showing the effect the pre-processing can have on every model's score. In addition to describing the winning model, the current study shows the error analysis",
    "checked": true,
    "id": "223e3c3b1660c3d6772323579a6f7012d4732034",
    "semantic_title": "r00 at nlp4if-2021 fighting covid-19 infodemic with transformers and more transformers",
    "citation_count": 3,
    "authors": [
      "Ahmed Qarqaz",
      "Dia Abujaber",
      "Malak Abdullah"
    ]
  },
  "https://aclanthology.org/2021.nlp4if-1.16": {
    "title": "Multi Output Learning using Task Wise Attention for Predicting Binary Properties of Tweets : Shared-Task-On-Fighting the COVID-19 Infodemic",
    "volume": "workshop",
    "abstract": "In this paper, we describe our system for the shared task on Fighting the COVID-19 Infodemic in the English Language. Our proposed architecture consists of a multi-output classification model for the seven tasks, with a task-wise multi-head attention layer for inter-task information aggregation. This was built on top of the Bidirectional Encoder Representations obtained from the RoBERTa Transformer. We were able to achieve a mean F1 score of 0.891 on the test data, leading us to the second position on the test-set leaderboard",
    "checked": true,
    "id": "637b0db85dcd196073865a55fccfa89d16916f5d",
    "semantic_title": "multi output learning using task wise attention for predicting binary properties of tweets : shared-task-on-fighting the covid-19 infodemic",
    "citation_count": 2,
    "authors": [
      "Ayush Suhane",
      "Shreyas Kowshik"
    ]
  },
  "https://aclanthology.org/2021.nlp4if-1.17": {
    "title": "iCompass at NLP4IF-2021–Fighting the COVID-19 Infodemic",
    "volume": "workshop",
    "abstract": "This paper provides a detailed overview of the system and its outcomes, which were produced as part of the NLP4IF Shared Task on Fighting the COVID-19 Infodemic at NAACL 2021. This task is accomplished using a variety of techniques. We used state-of-the-art contextualized text representation models that were fine-tuned for the downstream task in hand. ARBERT, MARBERT,AraBERT, Arabic ALBERT and BERT-base-arabic were used. According to the results, BERT-base-arabic had the highest 0.784 F1 score on the test set",
    "checked": true,
    "id": "0ff115da6d2f09953f4b65c90ceef940df5046fe",
    "semantic_title": "icompass at nlp4if-2021–fighting the covid-19 infodemic",
    "citation_count": 4,
    "authors": [
      "Wassim Henia",
      "Oumayma Rjab",
      "Hatem Haddad",
      "Chayma Fourati"
    ]
  },
  "https://aclanthology.org/2021.nlp4if-1.18": {
    "title": "Fighting the COVID-19 Infodemic with a Holistic BERT Ensemble",
    "volume": "workshop",
    "abstract": "This paper describes the TOKOFOU system, an ensemble model for misinformation detection tasks based on six different transformer-based pre-trained encoders, implemented in the context of the COVID-19 Infodemic Shared Task for English. We fine tune each model on each of the task's questions and aggregate their prediction scores using a majority voting approach. TOKOFOU obtains an overall F1 score of 89.7%, ranking first",
    "checked": true,
    "id": "085b449b7aae74e0efc252bd52ba5cdec4318ad0",
    "semantic_title": "fighting the covid-19 infodemic with a holistic bert ensemble",
    "citation_count": 6,
    "authors": [
      "Georgios Tziafas",
      "Konstantinos Kogkalidis",
      "Tommaso Caselli"
    ]
  },
  "https://aclanthology.org/2021.nlp4if-1.19": {
    "title": "Detecting Multilingual COVID-19 Misinformation on Social Media via Contextualized Embeddings",
    "volume": "workshop",
    "abstract": "We present machine learning classifiers to automatically identify COVID-19 misinformation on social media in three languages: English, Bulgarian, and Arabic. We compared 4 multitask learning models for this task and found that a model trained with English BERT achieves the best results for English, and multilingual BERT achieves the best results for Bulgarian and Arabic. We experimented with zero shot, few shot, and target-only conditions to evaluate the impact of target-language training data on classifier performance, and to understand the capabilities of different models to generalize across languages in detecting misinformation online. This work was performed as a submission to the shared task, NLP4IF 2021: Fighting the COVID-19 Infodemic. Our best models achieved the second best evaluation test results for Bulgarian and Arabic among all the participating teams and obtained competitive scores for English",
    "checked": true,
    "id": "26da4d36dc5c404298cb9e0069a89b5964b0ffe7",
    "semantic_title": "detecting multilingual covid-19 misinformation on social media via contextualized embeddings",
    "citation_count": 12,
    "authors": [
      "Subhadarshi Panda",
      "Sarah Ita Levitan"
    ]
  },
  "https://aclanthology.org/2021.nlp4if-1.20": {
    "title": "Transformers to Fight the COVID-19 Infodemic",
    "volume": "workshop",
    "abstract": "The massive spread of false information on social media has become a global risk especially in a global pandemic situation like COVID-19. False information detection has thus become a surging research topic in recent months. NLP4IF-2021 shared task on fighting the COVID-19 infodemic has been organised to strengthen the research in false information detection where the participants are asked to predict seven different binary labels regarding false information in a tweet. The shared task has been organised in three languages; Arabic, Bulgarian and English. In this paper, we present our approach to tackle the task objective using transformers. Overall, our approach achieves a 0.707 mean F1 score in Arabic, 0.578 mean F1 score in Bulgarian and 0.864 mean F1 score in English ranking 4th place in all the languages",
    "checked": true,
    "id": "32c956be991114d2e15bf758baeb544db80943cf",
    "semantic_title": "transformers to fight the covid-19 infodemic",
    "citation_count": 11,
    "authors": [
      "Lasitha Uyangodage",
      "Tharindu Ranasinghe",
      "Hansi Hettiarachchi"
    ]
  },
  "https://aclanthology.org/2021.nlp4if-1.21": {
    "title": "Classification of Censored Tweets in Chinese Language using XLNet",
    "volume": "workshop",
    "abstract": "In the growth of today's world and advanced technology, social media networks play a significant role in impacting human lives. Censorship is the overthrowing of speech, public transmission, or other details that play a vast role in social media. The content may be considered harmful, sensitive, or inconvenient. Authorities like institutes, governments, and other organizations conduct Censorship. This paper has implemented a model that helps classify censored and uncensored tweets as a binary classification. The paper describes submission to the Censorship shared task of the NLP4IF 2021 workshop. We used various transformer-based pre-trained models, and XLNet outputs a better accuracy among all. We fine-tuned the model for better performance and achieved a reasonable accuracy, and calculated other performance metrics",
    "checked": true,
    "id": "2b83bdf7f548ceb9c6df86fe6a6d6d64fd715929",
    "semantic_title": "classification of censored tweets in chinese language using xlnet",
    "citation_count": 3,
    "authors": [
      "Shaikh Sahil Ahmed",
      "Anand Kumar M."
    ]
  },
  "https://aclanthology.org/2021.nlpmc-1.1": {
    "title": "Would you like to tell me more? Generating a corpus of psychotherapy dialogues",
    "volume": "workshop",
    "abstract": "The acquisition of a dialogue corpus is a key step in the process of training a dialogue model. In this context, corpora acquisitions have been designed either for open-domain information retrieval or slot-filling (e.g. restaurant booking) tasks. However, there has been scarce research in the problem of collecting personal conversations with users over a long period of time. In this paper we focus on the types of dialogues that are required for mental health applications. One of these types is the follow-up dialogue that a psychotherapist would initiate in reviewing the progress of a Cognitive Behavioral Therapy (CBT) intervention. The elicitation of the dialogues is achieved through textual stimuli presented to dialogue writers. We propose an automatic algorithm that generates textual stimuli from personal narratives collected during psychotherapy interventions. The automatically generated stimuli are presented as a seed to dialogue writers following principled guidelines. We analyze the linguistic quality of the collected corpus and compare the performances of psychotherapists and non-expert dialogue writers. Moreover, we report the human evaluation of a corpus-based response-selection model",
    "checked": true,
    "id": "fd61ed225ce6fb87d24f4a2b5d90939daabf05ff",
    "semantic_title": "would you like to tell me more? generating a corpus of psychotherapy dialogues",
    "citation_count": 11,
    "authors": [
      "Seyed Mahed Mousavi",
      "Alessandra Cervone",
      "Morena Danieli",
      "Giuseppe Riccardi"
    ]
  },
  "https://aclanthology.org/2021.nlpmc-1.2": {
    "title": "Towards Automating Medical Scribing : Clinic Visit Dialogue2Note Sentence Alignment and Snippet Summarization",
    "volume": "workshop",
    "abstract": "Medical conversations from patient visits are routinely summarized into clinical notes for documentation of clinical care. The automatic creation of clinical note is particularly challenging given that it requires summarization over spoken language and multiple speaker turns; as well, clinical notes include highly technical semi-structured text. In this paper, we describe our corpus creation method and baseline systems for two NLP tasks, clinical dialogue2note sentence alignment and clinical dialogue2note snippet summarization. These two systems, as well as other models created from such a corpus, may be incorporated as parts of an overall end-to-end clinical note generation system",
    "checked": true,
    "id": "781be521705dcd753a4092edc8de40aeaee9a37f",
    "semantic_title": "towards automating medical scribing : clinic visit dialogue2note sentence alignment and snippet summarization",
    "citation_count": 25,
    "authors": [
      "Wen-wai Yim",
      "Meliha Yetisgen"
    ]
  },
  "https://aclanthology.org/2021.nlpmc-1.3": {
    "title": "Gathering Information and Engaging the User ComBot: A Task-Based, Serendipitous Dialog Model for Patient-Doctor Interactions",
    "volume": "workshop",
    "abstract": "We focus on dialog models in the context of clinical studies where the goal is to help gather, in addition to the close information collected based on a questionnaire, serendipitous information that is medically relevant. To promote user engagement and address this dual goal (collecting both a predefined set of data points and more informal information about the state of the patients), we introduce an ensemble model made of three bots: a task-based, a follow-up and a social bot. We introduce a generic method for developing follow-up bots. We compare different ensemble configurations and we show that the combination of the three bots (i) provides a better basis for collecting information than just the information seeking bot and (ii) collects information in a more user-friendly, more efficient manner that an ensemble model combining the information seeking and the social bot",
    "checked": true,
    "id": "892c37529053e560db6fb32ee6c4f8730405f374",
    "semantic_title": "gathering information and engaging the user combot: a task-based, serendipitous dialog model for patient-doctor interactions",
    "citation_count": 5,
    "authors": [
      "Anna Liednikova",
      "Philippe Jolivet",
      "Alexandre Durand-Salmon",
      "Claire Gardent"
    ]
  },
  "https://aclanthology.org/2021.nlpmc-1.4": {
    "title": "Automatic Speech-Based Checklist for Medical Simulations",
    "volume": "workshop",
    "abstract": "Medical simulators provide a controlled environment for training and assessing clinical skills. However, as an assessment platform, it requires the presence of an experienced examiner to provide performance feedback, commonly preformed using a task specific checklist. This makes the assessment process inefficient and expensive. Furthermore, this evaluation method does not provide medical practitioners the opportunity for independent training. Ideally, the process of filling the checklist should be done by a fully-aware objective system, capable of recognizing and monitoring the clinical performance. To this end, we have developed an autonomous and a fully automatic speech-based checklist system, capable of objectively identifying and validating anesthesia residents' actions in a simulation environment. Based on the analyzed results, our system is capable of recognizing most of the tasks in the checklist: F1 score of 0.77 for all of the tasks, and F1 score of 0.79 for the verbal tasks. Developing an audio-based system will improve the experience of a wide range of simulation platforms. Furthermore, in the future, this approach may be implemented in the operation room and emergency room. This could facilitate the development of automatic assistive technologies for these domains",
    "checked": true,
    "id": "1d9036b4798ef380de92d138988ff10e3542a416",
    "semantic_title": "automatic speech-based checklist for medical simulations",
    "citation_count": 2,
    "authors": [
      "Sapir Gershov",
      "Yaniv Ringel",
      "Erez Dvir",
      "Tzvia Tsirilman",
      "Elad Ben Zvi",
      "Sandra Braun",
      "Aeyal Raz",
      "Shlomi Laufer"
    ]
  },
  "https://aclanthology.org/2021.nlpmc-1.5": {
    "title": "Assertion Detection in Clinical Notes: Medical Language Models to the Rescue?",
    "volume": "workshop",
    "abstract": "In order to provide high-quality care, health professionals must efficiently identify the presence, possibility, or absence of symptoms, treatments and other relevant entities in free-text clinical notes. Such is the task of assertion detection - to identify the assertion class (present, possible, absent) of an entity based on textual cues in unstructured text. We evaluate state-of-the-art medical language models on the task and show that they outperform the baselines in all three classes. As transferability is especially important in the medical domain we further study how the best performing model behaves on unseen data from two other medical datasets. For this purpose we introduce a newly annotated set of 5,000 assertions for the publicly available MIMIC-III dataset. We conclude with an error analysis that reveals situations in which the models still go wrong and points towards future research directions",
    "checked": true,
    "id": "c08f92553221af2384eac1a7319db0a0d36114d4",
    "semantic_title": "assertion detection in clinical notes: medical language models to the rescue?",
    "citation_count": 15,
    "authors": [
      "Betty van Aken",
      "Ivana Trajanovska",
      "Amy Siu",
      "Manuel Mayrdorfer",
      "Klemens Budde",
      "Alexander Loeser"
    ]
  },
  "https://aclanthology.org/2021.nlpmc-1.6": {
    "title": "Extracting Appointment Spans from Medical Conversations",
    "volume": "workshop",
    "abstract": "Extracting structured information from medical conversations can reduce the documentation burden for doctors and help patients follow through with their care plan. In this paper, we introduce a novel task of extracting appointment spans from medical conversations. We frame this task as a sequence tagging problem and focus on extracting spans for appointment reason and time. However, annotating medical conversations is expensive, time-consuming, and requires considerable domain expertise. Hence, we propose to leverage weak supervision approaches, namely incomplete supervision, inaccurate supervision, and a hybrid supervision approach and evaluate both generic and domain-specific, ELMo, and BERT embeddings using sequence tagging models. The best performing model is the domain-specific BERT variant using weak hybrid supervision and obtains an F1 score of 79.32",
    "checked": true,
    "id": "2725f90a86a46042c42c15b45a01143658da0325",
    "semantic_title": "extracting appointment spans from medical conversations",
    "citation_count": 3,
    "authors": [
      "Nimshi Venkat Meripo",
      "Sandeep Konam"
    ]
  },
  "https://aclanthology.org/2021.nlpmc-1.7": {
    "title": "Building blocks of a task-oriented dialogue system in the healthcare domain",
    "volume": "workshop",
    "abstract": "There has been significant progress in dialogue systems research. However, dialogue systems research in the healthcare domain is still in its infancy. In this paper, we analyse recent studies and outline three building blocks of a task-oriented dialogue system in the healthcare domain: i) privacy-preserving data collection; ii) medical knowledge-grounded dialogue management; and iii) human-centric evaluations. To this end, we propose a framework for developing a dialogue system and show preliminary results of simulated dialogue data generation by utilising expert knowledge and crowd-sourcing",
    "checked": true,
    "id": "d9abe36ac62c93f839fa3e63b726b996db08c3fe",
    "semantic_title": "building blocks of a task-oriented dialogue system in the healthcare domain",
    "citation_count": 5,
    "authors": [
      "Heereen Shim",
      "Dietwig Lowet",
      "Stijn Luca",
      "Bart Vanrumste"
    ]
  },
  "https://aclanthology.org/2021.nlpmc-1.8": {
    "title": "Joint Summarization-Entailment Optimization for Consumer Health Question Understanding",
    "volume": "workshop",
    "abstract": "Understanding the intent of medical questions asked by patients, or Consumer Health Questions, is an essential skill for medical Conversational AI systems. We propose a novel data-augmented and simple joint learning approach combining question summarization and Recognizing Question Entailment (RQE) in the medical domain. Our data augmentation approach enables to use just one dataset for joint learning. We show improvements on both tasks across four biomedical datasets in accuracy (+8%), ROUGE-1 (+2.5%) and human evaluation scores. Human evaluation shows joint learning generates faithful and informative summaries. Finally, we release our code, the two question summarization datasets extracted from a large-scale medical dialogue dataset, as well as our augmented datasets",
    "checked": true,
    "id": "1b60e235fb995d97a25f940f396a484b0ac7b8da",
    "semantic_title": "joint summarization-entailment optimization for consumer health question understanding",
    "citation_count": 14,
    "authors": [
      "Khalil Mrini",
      "Franck Dernoncourt",
      "Walter Chang",
      "Emilia Farcas",
      "Ndapa Nakashole"
    ]
  },
  "https://aclanthology.org/2021.nlpmc-1.9": {
    "title": "Medically Aware GPT-3 as a Data Generator for Medical Dialogue Summarization",
    "volume": "workshop",
    "abstract": "In medical dialogue summarization, summaries must be coherent and must capture all the medically relevant information in the dialogue. However, learning effective models for summarization require large amounts of labeled data which is especially hard to obtain. We present an algorithm to create synthetic training data with an explicit focus on capturing medically relevant information. We utilize GPT-3 as the backbone of our algorithm and scale 210 human labeled examples to yield results comparable to using 6400 human labeled examples (~30x) leveraging low-shot learning and an ensemble method. In detailed experiments, we show that this approach produces high quality training data that can further be combined with human labeled data to get summaries that are strongly preferable to those produced by models trained on human data alone both in terms of medical accuracy and coherency",
    "checked": true,
    "id": "5a08cd9cca3f208283f5e913037df875ca24ef03",
    "semantic_title": "medically aware gpt-3 as a data generator for medical dialogue summarization",
    "citation_count": 98,
    "authors": [
      "Bharath Chintagunta",
      "Namit Katariya",
      "Xavier Amatriain",
      "Anitha Kannan"
    ]
  },
  "https://aclanthology.org/2021.nuse-1.1": {
    "title": "Hierarchical Encoders for Modeling and Interpreting Screenplays",
    "volume": "workshop",
    "abstract": "While natural language understanding of long-form documents remains an open challenge, such documents often contain structural information that can inform the design of models encoding them. Movie scripts are an example of such richly structured text – scripts are segmented into scenes, which decompose into dialogue and descriptive components. In this work, we propose a neural architecture to encode this structure, which performs robustly on two multi-label tag classification tasks without using handcrafted features. We add a layer of insight by augmenting the encoder with an unsupervised ‘interpretability' module, which can be used to extract and visualize narrative trajectories. Though this work specifically tackles screenplays, we discuss how the underlying approach can be generalized to a range of structured documents",
    "checked": true,
    "id": "ede216560d0aa452a4c93c1d20e033019703ff16",
    "semantic_title": "hierarchical encoders for modeling and interpreting screenplays",
    "citation_count": 5,
    "authors": [
      "Gayatri Bhat",
      "Avneesh Saluja",
      "Melody Dye",
      "Jan Florjanczyk"
    ]
  },
  "https://aclanthology.org/2021.nuse-1.2": {
    "title": "FanfictionNLP: A Text Processing Pipeline for Fanfiction",
    "volume": "workshop",
    "abstract": "Fanfiction presents an opportunity as a data source for research in NLP, education, and social science. However, answering specific research questions with this data is difficult, since fanfiction contains more diverse writing styles than formal fiction. We present a text processing pipeline for fanfiction, with a focus on identifying text associated with characters. The pipeline includes modules for character identification and coreference, as well as the attribution of quotes and narration to those characters. Additionally, the pipeline contains a novel approach to character coreference that uses knowledge from quote attribution to resolve pronouns within quotes. For each module, we evaluate the effectiveness of various approaches on 10 annotated fanfiction stories. This pipeline outperforms tools developed for formal fiction on the tasks of character coreference and quote attribution",
    "checked": true,
    "id": "b8613c086387a59549732a5a126b40f871cbb5e6",
    "semantic_title": "fanfictionnlp: a text processing pipeline for fanfiction",
    "citation_count": 12,
    "authors": [
      "Michael Yoder",
      "Sopan Khosla",
      "Qinlan Shen",
      "Aakanksha Naik",
      "Huiming Jin",
      "Hariharan Muralidharan",
      "Carolyn Rosé"
    ]
  },
  "https://aclanthology.org/2021.nuse-1.3": {
    "title": "Learning Similarity between Movie Characters and Its Potential Implications on Understanding Human Experiences",
    "volume": "workshop",
    "abstract": "While many different aspects of human experiences have been studied by the NLP community, none has captured its full richness. We propose a new task to capture this richness based on an unlikely setting: movie characters. We sought to capture theme-level similarities between movie characters that were community-curated into 20,000 themes. By introducing a two-step approach that balances performance and efficiency, we managed to achieve 9-27% improvement over recent paragraph-embedding based methods. Finally, we demonstrate how the thematic information learnt from movie characters can potentially be used to understand themes in the experience of people, as indicated on Reddit posts",
    "checked": true,
    "id": "a4f3fa2d3cbf07458a7d82c076b50fc38d607182",
    "semantic_title": "learning similarity between movie characters and its potential implications on understanding human experiences",
    "citation_count": 0,
    "authors": [
      "Zhilin Wang",
      "Weizhe Lin",
      "Xiaodong Wu"
    ]
  },
  "https://aclanthology.org/2021.nuse-1.4": {
    "title": "Document-level Event Extraction with Efficient End-to-end Learning of Cross-event Dependencies",
    "volume": "workshop",
    "abstract": "Fully understanding narratives often requires identifying events in the context of whole documents and modeling the event relations. However, document-level event extraction is a challenging task as it requires the extraction of event and entity coreference, and capturing arguments that span across different sentences. Existing works on event extraction usually confine on extracting events from single sentences, which fail to capture the relationships between the event mentions at the scale of a document, as well as the event arguments that appear in a different sentence than the event trigger. In this paper, we propose an end-to-end model leveraging Deep Value Networks (DVN), a structured prediction algorithm, to efficiently capture cross-event dependencies for document-level event extraction. Experimental results show that our approach achieves comparable performance to CRF-based models on ACE05, while enjoys significantly higher computational efficiency",
    "checked": true,
    "id": "1a80df343b76dbd1cba92ff94b48d60446ae6118",
    "semantic_title": "document-level event extraction with efficient end-to-end learning of cross-event dependencies",
    "citation_count": 29,
    "authors": [
      "Kung-Hsiang Huang",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2021.nuse-1.5": {
    "title": "Gender and Representation Bias in GPT-3 Generated Stories",
    "volume": "workshop",
    "abstract": "Using topic modeling and lexicon-based word similarity, we find that stories generated by GPT-3 exhibit many known gender stereotypes. Generated stories depict different topics and descriptions depending on GPT-3's perceived gender of the character in a prompt, with feminine characters more likely to be associated with family and appearance, and described as less powerful than masculine characters, even when associated with high power verbs in a prompt. Our study raises questions on how one can avoid unintended social biases when using large language models for storytelling",
    "checked": true,
    "id": "998577ea3c393330e4ecc563d5e4026cf9e49a17",
    "semantic_title": "gender and representation bias in gpt-3 generated stories",
    "citation_count": 187,
    "authors": [
      "Li Lucy",
      "David Bamman"
    ]
  },
  "https://aclanthology.org/2021.nuse-1.6": {
    "title": "Transformer-based Screenplay Summarization Using Augmented Learning Representation with Dialogue Information",
    "volume": "workshop",
    "abstract": "Screenplay summarization is the task of extracting informative scenes from a screenplay. The screenplay contains turning point (TP) events that change the story direction and thus define the story structure decisively. Accordingly, this task can be defined as the TP identification task. We suggest using dialogue information, one attribute of screenplays, motivated by previous work that discovered that TPs have a relation with dialogues appearing in screenplays. To teach a model this characteristic, we add a dialogue feature to the input embedding. Moreover, in an attempt to improve the model architecture of previous studies, we replace LSTM with Transformer. We observed that the model can better identify TPs in a screenplay by using dialogue information and that a model adopting Transformer outperforms LSTM-based models",
    "checked": true,
    "id": "fc228aabff4c8bb79ddbc1b1b392d81479c061b3",
    "semantic_title": "transformer-based screenplay summarization using augmented learning representation with dialogue information",
    "citation_count": 4,
    "authors": [
      "Myungji Lee",
      "Hongseok Kwon",
      "Jaehun Shin",
      "WonKee Lee",
      "Baikjin Jung",
      "Jong-Hyeok Lee"
    ]
  },
  "https://aclanthology.org/2021.nuse-1.7": {
    "title": "Plug-and-Blend: A Framework for Controllable Story Generation with Blended Control Codes",
    "volume": "workshop",
    "abstract": "We describe a Plug-and-Play controllable language generation framework, Plug-and-Blend, that allows a human user to input multiple control codes (topics). In the context of automated story generation, this allows a human user lose or fine grained control of the topics that will appear in the generated story, and can even allow for overlapping, blended topics. We show that our framework, working with different generation models, controls the generation towards given continuous-weighted control codes while keeping the generated sentences fluent, demonstrating strong blending capability",
    "checked": true,
    "id": "89ffb2586b23561676e821187aa8fb3bab73bb7f",
    "semantic_title": "plug-and-blend: a framework for controllable story generation with blended control codes",
    "citation_count": 18,
    "authors": [
      "Zhiyu Lin",
      "Mark Riedl"
    ]
  },
  "https://aclanthology.org/2021.nuse-1.8": {
    "title": "Automatic Story Generation: Challenges and Attempts",
    "volume": "workshop",
    "abstract": "Automated storytelling has long captured the attention of researchers for the ubiquity of narratives in everyday life. The best human-crafted stories exhibit coherent plot, strong characters, and adherence to genres, attributes that current states-of-the-art still struggle to produce, even using transformer architectures. In this paper, we analyze works in story generation that utilize machine learning approaches to (1) address story generation controllability, (2) incorporate commonsense knowledge, (3) infer reasonable character actions, and (4) generate creative language",
    "checked": true,
    "id": "df7bf316e7bad359e87b10544155be09336720ca",
    "semantic_title": "automatic story generation: challenges and attempts",
    "citation_count": 32,
    "authors": [
      "Amal Alabdulkarim",
      "Siyan Li",
      "Xiangyu Peng"
    ]
  },
  "https://aclanthology.org/2021.nuse-1.9": {
    "title": "Fabula Entropy Indexing: Objective Measures of Story Coherence",
    "volume": "workshop",
    "abstract": "Automated story generation remains a difficult area of research because it lacks strong objective measures. Generated stories may be linguistically sound, but in many cases suffer poor narrative coherence required for a compelling, logically-sound story. To address this, we present Fabula Entropy Indexing (FEI), an evaluation method to assess story coherence by measuring the degree to which human participants agree with each other when answering true/false questions about stories. We devise two theoretically grounded measures of reader question-answering entropy, the entropy of world coherence (EWC), and the entropy of transitional coherence (ETC), focusing on global and local coherence, respectively. We evaluate these metrics by testing them on human-written stories and comparing against the same stories that have been corrupted to introduce incoherencies. We show that in these controlled studies, our entropy indices provide a reliable objective measure of story coherence",
    "checked": true,
    "id": "a89769cbb2bad2fa723d1c2848c836ef18b50900",
    "semantic_title": "fabula entropy indexing: objective measures of story coherence",
    "citation_count": 6,
    "authors": [
      "Louis Castricato",
      "Spencer Frazier",
      "Jonathan Balloch",
      "Mark Riedl"
    ]
  },
  "https://aclanthology.org/2021.nuse-1.10": {
    "title": "Towards a Model-Theoretic View of Narratives",
    "volume": "workshop",
    "abstract": "In this paper, we propose the beginnings of a formal framework for modeling narrative qua narrative. Our framework affords the ability to discuss key qualities of stories and their communication, including the flow of information from a Narrator to a Reader, the evolution of a Reader's story model over time, and Reader uncertainty. We demonstrate its applicability to computational narratology by giving explicit algorithms for measuring the accuracy with which information was conveyed to the Reader, along with two novel measurements of story coherence",
    "checked": true,
    "id": "1d31576c3bde15aa64efe443ca68a28edc8a8daa",
    "semantic_title": "towards a model-theoretic view of narratives",
    "citation_count": 3,
    "authors": [
      "Louis Castricato",
      "Stella Biderman",
      "David Thue",
      "Rogelio Cardona-Rivera"
    ]
  },
  "https://aclanthology.org/2021.privatenlp-1.1": {
    "title": "Understanding Unintended Memorization in Language Models Under Federated Learning",
    "volume": "workshop",
    "abstract": "Recent works have shown that language models (LMs), e.g., for next word prediction (NWP), have a tendency to memorize rare or unique sequences in the training data. Since useful LMs are often trained on sensitive data, it is critical to identify and mitigate such unintended memorization. Federated Learning (FL) has emerged as a novel framework for large-scale distributed learning tasks. It differs in many aspects from the well-studied central learning setting where all the data is stored at the central server, and minibatch stochastic gradient descent is used to conduct training. This work is motivated by our observation that NWP models trained under FL exhibited remarkably less propensity to such memorization compared to the central learning setting. Thus, we initiate a formal study to understand the effect of different components of FL on unintended memorization in trained NWP models. Our results show that several differing components of FL play an important role in reducing unintended memorization. First, we discover that the clustering of data according to users—which happens by design in FL—has the most significant effect in reducing such memorization. Using the Federated Averaging optimizer with larger effective minibatch sizes for training causes a further reduction. We also demonstrate that training in FL with a user-level differential privacy guarantee results in models that can provide high utility while being resilient to memorizing out-of-distribution phrases with thousands of insertions across over a hundred users in the training set",
    "checked": true,
    "id": "b856e46e23a9dd055392cafe372d73d673fe8e35",
    "semantic_title": "understanding unintended memorization in language models under federated learning",
    "citation_count": 26,
    "authors": [
      "Om Dipakbhai Thakkar",
      "Swaroop Ramaswamy",
      "Rajiv Mathews",
      "Francoise Beaufays"
    ]
  },
  "https://aclanthology.org/2021.privatenlp-1.2": {
    "title": "On a Utilitarian Approach to Privacy Preserving Text Generation",
    "volume": "workshop",
    "abstract": "Differentially-private mechanisms for text generation typically add carefully calibrated noise to input words and use the nearest neighbor to the noised input as the output word. When the noise is small in magnitude, these mechanisms are susceptible to reconstruction of the original sensitive text. This is because the nearest neighbor to the noised input is likely to be the original input. To mitigate this empirical privacy risk, we propose a novel class of differentially private mechanisms that parameterizes the nearest neighbor selection criterion in traditional mechanisms. Motivated by Vickrey auction, where only the second highest price is revealed and the highest price is kept private, we balance the choice between the first and the second nearest neighbors in the proposed class of mechanisms using a tuning parameter. This parameter is selected by empirically solving a constrained optimization problem for maximizing utility, while maintaining the desired privacy guarantees. We argue that this empirical measurement framework can be used to align different mechanisms along a common benchmark for their privacy-utility tradeoff, particularly when different distance metrics are used to calibrate the amount of noise added. Our experiments on real text classification datasets show up to 50% improvement in utility compared to the existing state-of-the-art with the same empirical privacy guarantee",
    "checked": true,
    "id": "dfd8fc9966ca8ec5c8bdc2dfc94099285f0e07a9",
    "semantic_title": "on a utilitarian approach to privacy preserving text generation",
    "citation_count": 11,
    "authors": [
      "Zekun Xu",
      "Abhinav Aggarwal",
      "Oluwaseyi Feyisetan",
      "Nathanael Teissier"
    ]
  },
  "https://aclanthology.org/2021.privatenlp-1.3": {
    "title": "Learning and Evaluating a Differentially Private Pre-trained Language Model",
    "volume": "workshop",
    "abstract": "Contextual language models have led to significantly better results on a plethora of language understanding tasks, especially when pre-trained on the same data as the downstream task. While this additional pre-training usually improves performance, it can lead to information leakage and therefore risks the privacy of individuals mentioned in the training data. One method to guarantee the privacy of such individuals is to train a differentially-private model, but this usually comes at the expense of model performance. Moreover, it is hard to tell given a privacy parameter 𝜖 what was the effect on the trained representation. In this work we aim to guide future practitioners and researchers on how to improve privacy while maintaining good model performance. We demonstrate how to train a differentially-private pre-trained language model (i.e., BERT) with a privacy guarantee of 𝜖=1 and with only a small degradation in performance. We experiment on a dataset of clinical notes with a model trained on a target entity extraction task, and compare it to a similar model trained without differential privacy. Finally, we present experiments showing how to interpret the differentially-private representation and understand the information lost and maintained in this process",
    "checked": true,
    "id": "c3b597011f64e7c5459bbe4502163e463fb13f5a",
    "semantic_title": "learning and evaluating a differentially private pre-trained language model",
    "citation_count": 50,
    "authors": [
      "Shlomo Hoory",
      "Amir Feder",
      "Avichai Tendler",
      "Alon Cohen",
      "Sofia Erell",
      "Itay Laish",
      "Hootan Nakhost",
      "Uri Stemmer",
      "Ayelet Benjamini",
      "Avinatan Hassidim",
      "Yossi Matias"
    ]
  },
  "https://aclanthology.org/2021.privatenlp-1.4": {
    "title": "An Investigation towards Differentially Private Sequence Tagging in a Federated Framework",
    "volume": "workshop",
    "abstract": "To build machine learning-based applications for sensitive domains like medical, legal, etc. where the digitized text contains private information, anonymization of text is required for preserving privacy. Sequence tagging, e.g. as done in Named Entity Recognition (NER) can help to detect private information. However, to train sequence tagging models, a sufficient amount of labeled data are required but for privacy-sensitive domains, such labeled data also can not be shared directly. In this paper, we investigate the applicability of a privacy-preserving framework for sequence tagging tasks, specifically NER. Hence, we analyze a framework for the NER task, which incorporates two levels of privacy protection. Firstly, we deploy a federated learning (FL) framework where the labeled data are not shared with the centralized server as well as the peer clients. Secondly, we apply differential privacy (DP) while the models are being trained in each client instance. While both privacy measures are suitable for privacy-aware models, their combination results in unstable models. To our knowledge, this is the first study of its kind on privacy-aware sequence tagging models",
    "checked": true,
    "id": "e60feb9fdbc20740d046bc88bfeb9f93d5c0145d",
    "semantic_title": "an investigation towards differentially private sequence tagging in a federated framework",
    "citation_count": 10,
    "authors": [
      "Abhik Jana",
      "Chris Biemann"
    ]
  },
  "https://aclanthology.org/2021.privatenlp-1.5": {
    "title": "A Privacy-Preserving Approach to Extraction of Personal Information through Automatic Annotation and Federated Learning",
    "volume": "workshop",
    "abstract": "We curated WikiPII, an automatically labeled dataset composed of Wikipedia biography pages, annotated for personal information extraction. Although automatic annotation can lead to a high degree of label noise, it is an inexpensive process and can generate large volumes of annotated documents. We trained a BERT-based NER model with WikiPII and showed that with an adequately large training dataset, the model can significantly decrease the cost of manual information extraction, despite the high level of label noise. In a similar approach, organizations can leverage text mining techniques to create customized annotated datasets from their historical data without sharing the raw data for human annotation. Also, we explore collaborative training of NER models through federated learning when the annotation is noisy. Our results suggest that depending on the level of trust to the ML operator and the volume of the available data, distributed training can be an effective way of training a personal information identifier in a privacy-preserved manner. Research material is available at https://github.com/ratmcu/wikipiifed",
    "checked": true,
    "id": "ec408373db723333046e3f1fa287fcae9108fe33",
    "semantic_title": "a privacy-preserving approach to extraction of personal information through automatic annotation and federated learning",
    "citation_count": 13,
    "authors": [
      "Rajitha Hathurusinghe",
      "Isar Nejadgholi",
      "Miodrag Bolic"
    ]
  },
  "https://aclanthology.org/2021.privatenlp-1.6": {
    "title": "Using Confidential Data for Domain Adaptation of Neural Machine Translation",
    "volume": "workshop",
    "abstract": "We study the problem of domain adaptation in Neural Machine Translation (NMT) when domain-specific data cannot be shared due to confidentiality or copyright issues. As a first step, we propose to fragment data into phrase pairs and use a random sample to fine-tune a generic NMT model instead of the full sentences. Despite the loss of long segments for the sake of confidentiality protection, we find that NMT quality can considerably benefit from this adaptation, and that further gains can be obtained with a simple tagging technique",
    "checked": true,
    "id": "269cec6230a443881fc42320ae93d22123071564",
    "semantic_title": "using confidential data for domain adaptation of neural machine translation",
    "citation_count": 3,
    "authors": [
      "Sohyung Kim",
      "Arianna Bisazza",
      "Fatih Turkmen"
    ]
  },
  "https://aclanthology.org/2021.privatenlp-1.7": {
    "title": "Private Text Classification with Convolutional Neural Networks",
    "volume": "workshop",
    "abstract": "Text classifiers are regularly applied to personal texts, leaving users of these classifiers vulnerable to privacy breaches. We propose a solution for privacy-preserving text classification that is based on Convolutional Neural Networks (CNNs) and Secure Multiparty Computation (MPC). Our method enables the inference of a class label for a personal text in such a way that (1) the owner of the personal text does not have to disclose their text to anyone in an unencrypted manner, and (2) the owner of the text classifier does not have to reveal the trained model parameters to the text owner or to anyone else. To demonstrate the feasibility of our protocol for practical private text classification, we implemented it in the PyTorch-based MPC framework CrypTen, using a well-known additive secret sharing scheme in the honest-but-curious setting. We test the runtime of our privacy-preserving text classifier, which is fast enough to be used in practice",
    "checked": true,
    "id": "e7bf5e861e9b2bd9881cc52f3a83bd0df9b91667",
    "semantic_title": "private text classification with convolutional neural networks",
    "citation_count": 2,
    "authors": [
      "Samuel Adams",
      "David Melanson",
      "Martine De Cock"
    ]
  },
  "https://aclanthology.org/2021.sdp-1.1": {
    "title": "Determining the Credibility of Science Communication",
    "volume": "workshop",
    "abstract": "Most work on scholarly document processing assumes that the information processed is trust-worthy and factually correct. However, this is not always the case. There are two core challenges, which should be addressed: 1) ensuring that scientific publications are credible – e.g. that claims are not made without supporting evidence, and that all relevant supporting evidence is provided; and 2) that scientific findings are not misrepresented, distorted or outright misreported when communicated by journalists or the general public. I will present some first steps towards addressing these problems and outline remaining challenges",
    "checked": true,
    "id": "8c19883bc33282985af977fd3f3cc4b6ac5b15d3",
    "semantic_title": "determining the credibility of science communication",
    "citation_count": 3,
    "authors": [
      "Isabelle Augenstein"
    ]
  },
  "https://aclanthology.org/2021.sdp-1.2": {
    "title": "Unsupervised Document Expansion for Information Retrieval with Stochastic Text Generation",
    "volume": "workshop",
    "abstract": "One of the challenges in information retrieval (IR) is the vocabulary mismatch problem, which happens when the terms between queries and documents are lexically different but semantically similar. While recent work has proposed to expand the queries or documents by enriching their representations with additional relevant terms to address this challenge, they usually require a large volume of query-document pairs to train an expansion model. In this paper, we propose an Unsupervised Document Expansion with Generation (UDEG) framework with a pre-trained language model, which generates diverse supplementary sentences for the original document without using labels on query-document pairs for training. For generating sentences, we further stochastically perturb their embeddings to generate more diverse sentences for document expansion. We validate our framework on two standard IR benchmark datasets. The results show that our framework significantly outperforms relevant expansion baselines for IR",
    "checked": true,
    "id": "68fb8e0aca6978f16dc9b219d79cc72306cc26c6",
    "semantic_title": "unsupervised document expansion for information retrieval with stochastic text generation",
    "citation_count": 10,
    "authors": [
      "Soyeong Jeong",
      "Jinheon Baek",
      "ChaeHun Park",
      "Jong Park"
    ]
  },
  "https://aclanthology.org/2021.sdp-1.3": {
    "title": "Task Definition and Integration For Scientific-Document Writing Support",
    "volume": "workshop",
    "abstract": "With the increase in the number of published academic papers, growing expectations have been placed on research related to supporting the writing process of scientific papers. Recently, research has been conducted on various tasks such as citation worthiness (judging whether a sentence requires citation), citation recommendation, and citation-text generation. However, since each task has been studied and evaluated using data that has been independently developed, it is currently impossible to verify whether such tasks can be successfully pipelined to effective use in scientific-document writing. In this paper, we first define a series of tasks related to scientific-document writing that can be pipelined. Then, we create a dataset of academic papers that can be used for the evaluation of each task as well as a series of these tasks. Finally, using the dataset, we evaluate the tasks of citation worthiness and citation recommendation as well as both of these tasks integrated. The results of our evaluations show that the proposed approach is promising",
    "checked": true,
    "id": "0dac38a2dfd27caaf2fe8fc74e7206e649137660",
    "semantic_title": "task definition and integration for scientific-document writing support",
    "citation_count": 1,
    "authors": [
      "Hiromi Narimatsu",
      "Kohei Koyama",
      "Kohji Dohsaka",
      "Ryuichiro Higashinaka",
      "Yasuhiro Minami",
      "Hirotoshi Taira"
    ]
  },
  "https://aclanthology.org/2021.sdp-1.4": {
    "title": "Detecting Anatomical and Functional Connectivity Relations in Biomedical Literature via Language Representation Models",
    "volume": "workshop",
    "abstract": "Understanding of nerve-organ interactions is crucial to facilitate the development of effective bioelectronic treatments. Towards the end of developing a systematized and computable wiring diagram of the autonomic nervous system (ANS), we introduce a curated ANS connectivity corpus together with several neural language representation model based connectivity relation extraction systems. We also show that active learning guided curation for labeled corpus expansion significantly outperforms randomly selecting connectivity relation candidates minimizing curation effort. Our final relation extraction system achieves F1 = 72.8% on anatomical connectivity and F1 = 74.6% on functional connectivity relation extraction",
    "checked": true,
    "id": "e9d010ad209198eeca67f7f1ded5fbef2b1bc152",
    "semantic_title": "detecting anatomical and functional connectivity relations in biomedical literature via language representation models",
    "citation_count": 0,
    "authors": [
      "Ibrahim Burak Ozyurt",
      "Joseph Menke",
      "Anita Bandrowski",
      "Maryann Martone"
    ]
  },
  "https://aclanthology.org/2021.sdp-1.5": {
    "title": "The Biomaterials Annotator: a system for ontology-based concept annotation of biomaterials text",
    "volume": "workshop",
    "abstract": "Biomaterials are synthetic or natural materials used for constructing artificial organs, fabricating prostheses, or replacing tissues. The last century saw the development of thousands of novel biomaterials and, as a result, an exponential increase in scientific publications in the field. Large-scale analysis of biomaterials and their performance could enable data-driven material selection and implant design. However, such analysis requires identification and organization of concepts, such as materials and structures, from published texts. To facilitate future information extraction and the application of machine-learning techniques, we developed a semantic annotator specifically tailored for the biomaterials literature. The Biomaterials Annotator has been implemented following a modular organization using software containers for the different components and orchestrated using Nextflow as workflow manager. Natural language processing (NLP) components are mainly developed in Java. This set-up has allowed named entity recognition of seventeen classes relevant to the biomaterials domain. Here we detail the development, evaluation and performance of the system, as well as the release of the first collection of annotated biomaterials abstracts. We make both the corpus and system available to the community to promote future efforts in the field and contribute towards its sustainability",
    "checked": true,
    "id": "3b601248b795552a654836e77fceac3341b46742",
    "semantic_title": "the biomaterials annotator: a system for ontology-based concept annotation of biomaterials text",
    "citation_count": 3,
    "authors": [
      "Javier Corvi",
      "Carla Fuenteslópez",
      "José Fernández",
      "Josep Gelpi",
      "Maria-Pau Ginebra",
      "Salvador Capella-Guitierrez",
      "Osnat Hakimi"
    ]
  },
  "https://aclanthology.org/2021.sdp-1.6": {
    "title": "Keyphrase Extraction from Scientific Articles via Extractive Summarization",
    "volume": "workshop",
    "abstract": "Automatically extracting keyphrases from scholarly documents leads to a valuable concise representation that humans can understand and machines can process for tasks, such as information retrieval, article clustering and article classification. This paper is concerned with the parts of a scientific article that should be given as input to keyphrase extraction methods. Recent deep learning methods take titles and abstracts as input due to the increased computational complexity in processing long sequences, whereas traditional approaches can also work with full-texts. Titles and abstracts are dense in keyphrases, but often miss important aspects of the articles, while full-texts on the other hand are richer in keyphrases but much noisier. To address this trade-off, we propose the use of extractive summarization models on the full-texts of scholarly documents. Our empirical study on 3 article collections using 3 keyphrase extraction methods shows promising results",
    "checked": true,
    "id": "c43b3c70d68f27ed687eee46eac26c9b8f6a1d7e",
    "semantic_title": "keyphrase extraction from scientific articles via extractive summarization",
    "citation_count": 4,
    "authors": [
      "Chrysovalantis Giorgos Kontoulis",
      "Eirini Papagiannopoulou",
      "Grigorios Tsoumakas"
    ]
  },
  "https://aclanthology.org/2021.sdp-1.7": {
    "title": "Argument Mining for Scholarly Document Processing: Taking Stock and Looking Ahead",
    "volume": "workshop",
    "abstract": "Argument mining targets structures in natural language related to interpretation and persuasion which are central to scientific communication. Most scholarly discourse involves interpreting experimental evidence and attempting to persuade other scientists to adopt the same conclusions. While various argument mining studies have addressed student essays and news articles, those that target scientific discourse are still scarce. This paper surveys existing work in argument mining of scholarly discourse, and provides an overview of current models, data, tasks, and applications. We identify a number of key challenges confronting argument mining in the scientific domain, and suggest some possible solutions and future directions",
    "checked": true,
    "id": "7d87e18c4ac1029c92cc72cb81461542481cb683",
    "semantic_title": "argument mining for scholarly document processing: taking stock and looking ahead",
    "citation_count": 10,
    "authors": [
      "Khalid Al Khatib",
      "Tirthankar Ghosal",
      "Yufang Hou",
      "Anita de Waard",
      "Dayne Freitag"
    ]
  },
  "https://aclanthology.org/2021.sdp-1.8": {
    "title": "Bootstrapping Multilingual Metadata Extraction: A Showcase in Cyrillic",
    "volume": "workshop",
    "abstract": "Applications based on scholarly data are of ever increasing importance. This results in disadvantages for areas where high-quality data and compatible systems are not available, such as non-English publications. To advance the mitigation of this imbalance, we use Cyrillic script publications from the CORE collection to create a high-quality data set for metadata extraction. We utilize our data for training and evaluating sequence labeling models to extract title and author information. Retraining GROBID on our data, we observe significant improvements in terms of precision and recall and achieve even better results with a self developed model. We make our data set covering over 15,000 publications as well as our source code freely available",
    "checked": true,
    "id": "2c08e950614113dd53485f0d5b2b91b1a0b987df",
    "semantic_title": "bootstrapping multilingual metadata extraction: a showcase in cyrillic",
    "citation_count": 1,
    "authors": [
      "Johan Krause",
      "Igor Shapiro",
      "Tarek Saier",
      "Michael Färber"
    ]
  },
  "https://aclanthology.org/2021.sdp-1.9": {
    "title": "The Effect of Pretraining on Extractive Summarization for Scientific Documents",
    "volume": "workshop",
    "abstract": "Large pretrained models have seen enormous success in extractive summarization tasks. In this work, we investigate the influence of pretraining on a BERT-based extractive summarization system for scientific documents. We derive significant performance improvements using an intermediate pretraining step that leverages existing summarization datasets and report state-of-the-art results on a recently released scientific summarization dataset, SciTLDR. We systematically analyze the intermediate pretraining step by varying the size and domain of the pretraining corpus, changing the length of the input sequence in the target task and varying target tasks. We also investigate how intermediate pretraining interacts with contextualized word embeddings trained on different domains",
    "checked": true,
    "id": "cfbe9183f2fe2847f7a3c811f6309a2cab3f85cf",
    "semantic_title": "the effect of pretraining on extractive summarization for scientific documents",
    "citation_count": 7,
    "authors": [
      "Yash Gupta",
      "Pawan Sasanka Ammanamanchi",
      "Shikha Bordia",
      "Arjun Manoharan",
      "Deepak Mittal",
      "Ramakanth Pasunuru",
      "Manish Shrivastava",
      "Maneesh Singh",
      "Mohit Bansal",
      "Preethi Jyothi"
    ]
  },
  "https://aclanthology.org/2021.sdp-1.10": {
    "title": "Finding Pragmatic Differences Between Disciplines",
    "volume": "workshop",
    "abstract": "Scholarly documents have a great degree of variation, both in terms of content (semantics) and structure (pragmatics). Prior work in scholarly document understanding emphasizes semantics through document summarization and corpus topic modeling but tends to omit pragmatics such as document organization and flow. Using a corpus of scholarly documents across 19 disciplines and state-of-the-art language modeling techniques, we learn a fixed set of domain-agnostic descriptors for document sections and \"retrofit\" the corpus to these descriptors (also referred to as \"normalization\"). Then, we analyze the position and ordering of these descriptors across documents to understand the relationship between discipline and structure. We report within-discipline structural archetypes, variability, and between-discipline comparisons, supporting the hypothesis that scholarly communities, despite their size, diversity, and breadth, share similar avenues for expressing their work. Our findings lay the foundation for future work in assessing research quality, domain style transfer, and further pragmatic analysis",
    "checked": true,
    "id": "4c0497c1191a4430a981336776c97b2b018245c7",
    "semantic_title": "finding pragmatic differences between disciplines",
    "citation_count": 0,
    "authors": [
      "Lee Kezar",
      "Jay Pujara"
    ]
  },
  "https://aclanthology.org/2021.sdp-1.11": {
    "title": "Extractive Research Slide Generation Using Windowed Labeling Ranking",
    "volume": "workshop",
    "abstract": "Presentation slides generated from original research papers provide an efficient form to present research innovations. Manually generating presentation slides is labor-intensive. We propose a method to automatically generates slides for scientific articles based on a corpus of 5000 paper-slide pairs compiled from conference proceedings websites. The sentence labeling module of our method is based on SummaRuNNer, a neural sequence model for extractive summarization. Instead of ranking sentences based on semantic similarities in the whole document, our algorithm measures the importance and novelty of sentences by combining semantic and lexical features within a sentence window. Our method outperforms several baseline methods including SummaRuNNer by a significant margin in terms of ROUGE score",
    "checked": true,
    "id": "adf3e9bd9b71dd2f30cc1257ffd4cf8e7083cde6",
    "semantic_title": "extractive research slide generation using windowed labeling ranking",
    "citation_count": 4,
    "authors": [
      "Athar Sefid",
      "Prasenjit Mitra",
      "Jian Wu",
      "C Lee Giles"
    ]
  },
  "https://aclanthology.org/2021.sdp-1.12": {
    "title": "LongSumm 2021: Session based automatic summarization model for scientific document",
    "volume": "workshop",
    "abstract": "Most summarization task focuses on generating relatively short summaries. Such a length constraint might not be appropriate when summarizing scientific work. The LongSumm task needs participants generate long summary for scientific document. This task usual can be solved by language model. But an important problem is that model like BERT is limit to memory, and can not deal with a long input like a document. Also generate a long output is hard. In this paper, we propose a session based automatic summarization model(SBAS) which using a session and ensemble mechanism to generate long summary. And our model achieves the best performance in the LongSumm task",
    "checked": true,
    "id": "ec58e77e9198d1004b3ec871b6d9bfbf259aaceb",
    "semantic_title": "longsumm 2021: session based automatic summarization model for scientific document",
    "citation_count": 3,
    "authors": [
      "Senci Ying",
      "Zheng Yan Zhao",
      "Wuhe Zou"
    ]
  },
  "https://aclanthology.org/2021.sdp-1.13": {
    "title": "CNLP-NITS @ LongSumm 2021: TextRank Variant for Generating Long Summaries",
    "volume": "workshop",
    "abstract": "The huge influx of published papers in the field of machine learning makes the task of summarization of scholarly documents vital, not just to eliminate the redundancy but also to provide a complete and satisfying crux of the content. We participated in LongSumm 2021: The 2nd Shared Task on Generating Long Summaries for scientific documents, where the task is to generate long summaries for scientific papers provided by the organizers. This paper discusses our extractive summarization approach to solve the task. We used TextRank algorithm with the BM25 score as a similarity function. Even after being a graph-based ranking algorithm that does not require any learning, TextRank produced pretty decent results with minimal compute power and time. We attained 3rd rank according to ROUGE-1 scores (0.5131 for F-measure and 0.5271 for recall) and performed decently as shown by the ROUGE-2 scores",
    "checked": true,
    "id": "716d4ecb67a34d19ffb9cc097124e00213e495ef",
    "semantic_title": "cnlp-nits @ longsumm 2021: textrank variant for generating long summaries",
    "citation_count": 1,
    "authors": [
      "Darsh Kaushik",
      "Abdullah Faiz Ur Rahman Khilji",
      "Utkarsh Sinha",
      "Partha Pakray"
    ]
  },
  "https://aclanthology.org/2021.sdp-1.14": {
    "title": "Unsupervised document summarization using pre-trained sentence embeddings and graph centrality",
    "volume": "workshop",
    "abstract": "This paper describes our submission for the LongSumm task in SDP 2021. We propose a method for incorporating sentence embeddings produced by deep language models into extractive summarization techniques based on graph centrality in an unsupervised manner. The proposed method is simple, fast, can summarize any kind of document of any size and can satisfy any length constraints for the summaries produced. The method offers competitive performance to more sophisticated supervised methods and can serve as a proxy for abstractive summarization techniques",
    "checked": true,
    "id": "38addbf4bcfeae595972b54370920d9beff68c9c",
    "semantic_title": "unsupervised document summarization using pre-trained sentence embeddings and graph centrality",
    "citation_count": 4,
    "authors": [
      "Juan Ramirez-Orta",
      "Evangelos Milios"
    ]
  },
  "https://aclanthology.org/2021.sdp-1.15": {
    "title": "QMUL-SDS at SCIVER: Step-by-Step Binary Classification for Scientific Claim Verification",
    "volume": "workshop",
    "abstract": "Scientific claim verification is a unique challenge that is attracting increasing interest. The SCIVER shared task offers a benchmark scenario to test and compare claim verification approaches by participating teams and consists in three steps: relevant abstract selection, rationale selection and label prediction. In this paper, we present team QMUL-SDS's participation in the shared task. We propose an approach that performs scientific claim verification by doing binary classifications step-by-step. We trained a BioBERT-large classifier to select abstracts based on pairwise relevance assessments for each <claim, title of the abstract> and continued to train it to select rationales out of each retrieved abstract based on <claim, sentence>. We then propose a two-step setting for label prediction, i.e. first predicting \"NOT_ENOUGH_INFO\" or \"ENOUGH_INFO\", then label those marked as \"ENOUGH_INFO\" as either \"SUPPORT\" or \"CONTRADICT\". Compared to the baseline system, we achieve substantial improvements on the dev set. As a result, our team is the No. 4 team on the leaderboard",
    "checked": true,
    "id": "5ac0dbe09cbbfb4ea9a8843190981d8f93444bad",
    "semantic_title": "qmul-sds at sciver: step-by-step binary classification for scientific claim verification",
    "citation_count": 8,
    "authors": [
      "Xia Zeng",
      "Arkaitz Zubiaga"
    ]
  },
  "https://aclanthology.org/2021.sdp-1.16": {
    "title": "Overview and Insights from the SCIVER shared task on Scientific Claim Verification",
    "volume": "workshop",
    "abstract": "We present an overview of the SCIVER shared task, presented at the 2nd Scholarly Document Processing (SDP) workshop at NAACL 2021. In this shared task, systems were provided a scientific claim and a corpus of research abstracts, and asked to identify which articles Support or Refute the claim as well as provide evidentiary sentences justifying those labels. 11 teams made a total of 14 submissions to the shared task leaderboard, leading to an improvement of more than +23 F1 on the primary task evaluation metric. In addition to surveying the participating systems, we provide several insights into modeling approaches to support continued progress and future research on the important and challenging task of scientific claim verification",
    "checked": true,
    "id": "ee22e2e31d68f1ed8609b926de5ce058c0ee97ef",
    "semantic_title": "overview and insights from the sciver shared task on scientific claim verification",
    "citation_count": 9,
    "authors": [
      "David Wadden",
      "Kyle Lo"
    ]
  },
  "https://aclanthology.org/2021.sdp-1.17": {
    "title": "SciBERT Sentence Representation for Citation Context Classification",
    "volume": "workshop",
    "abstract": "This paper describes our system (IREL) for 3C-Citation Context Classification shared task of the Scholarly Document Processing Workshop at NAACL 2021. We participated in both subtask A and subtask B. Our best system achieved a Macro F1 score of 0.26973 on the private leaderboard for subtask A and was ranked one. For subtask B our best system achieved a Macro F1 score of 0.59071 on the private leaderboard and was ranked two. We used similar models for both the subtasks with some minor changes, as discussed in this paper. Our best performing model for both the subtask was a finetuned SciBert model followed by a linear layer. This paper provides a detailed description of all the approaches we tried and their results",
    "checked": true,
    "id": "da2bfc05c405904b04f98ca12f68e4fbf4d1368c",
    "semantic_title": "scibert sentence representation for citation context classiﬁcation",
    "citation_count": 16,
    "authors": [
      "Himanshu Maheshwari",
      "Bhavyajeet Singh",
      "Vasudeva Varma"
    ]
  },
  "https://aclanthology.org/2021.sdp-1.18": {
    "title": "Multitask Learning for Citation Purpose Classification",
    "volume": "workshop",
    "abstract": "We present our entry into the 2021 3C Shared Task Citation Context Classification based on Purpose competition. The goal of the competition is to classify a citation in a scientific article based on its purpose. This task is important because it could potentially lead to more comprehensive ways of summarizing the purpose and uses of scientific articles, but it is also difficult, mainly due to the limited amount of available training data in which the purposes of each citation have been hand-labeled, along with the subjectivity of these labels. Our entry in the competition is a multi-task model that combines multiple modules designed to handle the problem from different perspectives, including hand-generated linguistic features, TF-IDF features, and an LSTM-with- attention model. We also provide an ablation study and feature analysis whose insights could lead to future work",
    "checked": true,
    "id": "227351021378ccc442dfa48391a8d59668a20750",
    "semantic_title": "multitask learning for citation purpose classification",
    "citation_count": 5,
    "authors": [
      "Yasa M. Baig",
      "Alex X. Oesterling",
      "Rui Xin",
      "Haoyang Yu",
      "Angikar Ghosal",
      "Lesia Semenova",
      "Cynthia Rudin"
    ]
  },
  "https://aclanthology.org/2021.sdp-1.19": {
    "title": "IITP-CUNI@3C: Supervised Approaches for Citation Classification (Task A) and Citation Significance Detection (Task B)",
    "volume": "workshop",
    "abstract": "Citations are crucial to a scientific discourse. Besides providing additional contexts to research papers, citations act as trackers of the direction of research in a field and as an important measure in understanding the impact of a research publication. With the rapid growth in research publications, automated solutions for identifying the purpose and influence of citations are becoming very important. The 3C Citation Context Classification Task organized as part of the Second Workshop on Scholarly Document Processing @ NAACL 2021 is a shared task to address the aforementioned problems. In this paper, we present our team, IITP-CUNI@3C's submission to the 3C shared tasks. For Task A, citation context purpose classification, we propose a neural multi-task learning framework that harnesses the structural information of the research papers and the relation between the citation context and the cited paper for citation classification. For Task B, citation context influence classification, we use a set of simple features to classify citations based on their perceived significance. We achieve comparable performance with respect to the best performing systems in Task A and superseded the majority baseline in Task B with very simple features",
    "checked": true,
    "id": "df619677fba5cfb485360c3e788181283c37f0b8",
    "semantic_title": "iitp-cuni@3c: supervised approaches for citation classiﬁcation (task a) and citation signiﬁcance detection (task b)",
    "citation_count": 5,
    "authors": [
      "Kamal Kaushik Varanasi",
      "Tirthankar Ghosal",
      "Piyush Tiwary",
      "Muskaan Singh"
    ]
  },
  "https://aclanthology.org/2021.sdp-1.20": {
    "title": "Amrita_CEN_NLP@SDP2021 Task A and B",
    "volume": "workshop",
    "abstract": "The purpose and influence of a citation are important in understanding the quality of a publication. The 3c citation context classification shared task at the Second Workshop on Scholarly Document Processing aims at addressing this problem. This paper is the submission of the team Amrita_CEN_NLP to the shared task. We employed Bi-directional Long Short Term Memory (LSTM) networks and a Random Forest classifier for modelling the aforementioned problems by considering the class imbalance problem in the data",
    "checked": true,
    "id": "e716e530945f95c7efcc8e0470a5e3e193a40446",
    "semantic_title": "amrita_cen_nlp@sdp2021 task a and b",
    "citation_count": 1,
    "authors": [
      "Premjith B",
      "Isha Indhu S",
      "Kavya S. Kumar",
      "Lakshaya Karthikeyan",
      "Soman Kp"
    ]
  },
  "https://aclanthology.org/2021.sdp-1.21": {
    "title": "Overview of the 2021 SDP 3C Citation Context Classification Shared Task",
    "volume": "workshop",
    "abstract": "This paper provides an overview of the 2021 3C Citation Context Classification shared task. The second edition of the shared task was organised as part of the 2nd Workshop on Scholarly Document Processing (SDP 2021). The task is composed of two subtasks: classifying citations based on their (Subtask A) purpose and (Subtask B) influence. As in the previous year, both tasks were hosted on Kaggle and used a portion of the new ACT dataset. A total of 22 teams participated in Subtask A, and 19 teams competed in Subtask B. All the participated systems were ranked based on their achieved macro f-score. The highest scores of 0.26973 and 0.60025 were reported for subtask A and B, respectively",
    "checked": false,
    "id": "c047ffb2e3597f5d5842c74f21dd9f74c2b83e87",
    "semantic_title": "overview of the 2021 sdp 3c citation context classiﬁcation shared task",
    "citation_count": 10,
    "authors": [
      "Suchetha N. Kunnath",
      "David Pride",
      "Drahomira Herrmannova",
      "Petr Knoth"
    ]
  },
  "https://aclanthology.org/2021.sdp-1.22": {
    "title": "Overview of the Second Workshop on Scholarly Document Processing",
    "volume": "workshop",
    "abstract": "With the ever-increasing pace of research and high volume of scholarly communication, scholars face a daunting task. Not only must they keep up with the growing literature in their own and related fields, scholars increasingly also need to rebut pseudo-science and disinformation. These needs have motivated an increasing focus on computational methods for enhancing search, summarization, and analysis of scholarly documents. However, the various strands of research on scholarly document processing remain fragmented. To reach out to the broader NLP and AI/ML community, pool distributed efforts in this area, and enable shared access to published research, we held the 2nd Workshop on Scholarly Document Processing (SDP) at NAACL 2021 as a virtual event (https://sdproc.org/2021/). The SDP workshop consisted of a research track, three invited talks, and three Shared Tasks (LongSumm 2021, SCIVER, and 3C). The program was geared towards the application of NLP, information retrieval, and data mining for scholarly documents, with an emphasis on identifying and providing solutions to open challenges",
    "checked": true,
    "id": "f25a978da0f34c98db85c8c6f1613570b9c8a841",
    "semantic_title": "overview of the second workshop on scholarly document processing",
    "citation_count": 5,
    "authors": [
      "Iz Beltagy",
      "Arman Cohan",
      "Guy Feigenblat",
      "Dayne Freitag",
      "Tirthankar Ghosal",
      "Keith Hall",
      "Drahomira Herrmannova",
      "Petr Knoth",
      "Kyle Lo",
      "Philipp Mayr",
      "Robert Patton",
      "Michal Shmueli-Scheuer",
      "Anita de Waard",
      "Kuansan Wang",
      "Lucy Lu Wang"
    ]
  },
  "https://aclanthology.org/2021.sigtyp-1.1": {
    "title": "OTEANN: Estimating the Transparency of Orthographies with an Artificial Neural Network",
    "volume": "workshop",
    "abstract": "To transcribe spoken language to written medium, most alphabets enable an unambiguous sound-to-letter rule. However, some writing systems have distanced themselves from this simple concept and little work exists in Natural Language Processing (NLP) on measuring such distance. In this study, we use an Artificial Neural Network (ANN) model to evaluate the transparency between written words and their pronunciation, hence its name Orthographic Transparency Estimation with an ANN (OTEANN). Based on datasets derived from Wikimedia dictionaries, we trained and tested this model to score the percentage of false predictions in phoneme-to-grapheme and grapheme-to-phoneme translation tasks. The scores obtained on 17 orthographies were in line with the estimations of other studies. Interestingly, the model also provided insight into typical mistakes made by learners who only consider the phonemic rule in reading and writing",
    "checked": true,
    "id": "4e55a883f2f88d98a74c9848cbcc110e008026ad",
    "semantic_title": "oteann: estimating the transparency of orthographies with an artificial neural network",
    "citation_count": 19,
    "authors": [
      "Xavier Marjou"
    ]
  },
  "https://aclanthology.org/2021.sigtyp-1.2": {
    "title": "Inferring Morphological Complexity from Syntactic Dependency Networks: A Test",
    "volume": "workshop",
    "abstract": "Research in linguistic typology has shown that languages do not fall into the neat morphological types (synthetic vs. analytic) postulated in the 19th century. Instead, analytic and synthetic must be viewed as two poles of a continuum and languages may show a mix analytic and synthetic strategies to different degrees. Unfortunately, empirical studies that offer a more fine-grained morphological classification of languages based on these parameters remain few. In this paper, we build upon previous research by Liu & Xu (2011) and investigate the possibility of inferring information on morphological complexity from syntactic dependency networks",
    "checked": true,
    "id": "8c2a1bb573a8b296e169e762621fed642d8b976e",
    "semantic_title": "inferring morphological complexity from syntactic dependency networks: a test",
    "citation_count": 3,
    "authors": [
      "Guglielmo Inglese",
      "Luca Brigada Villa"
    ]
  },
  "https://aclanthology.org/2021.sigtyp-1.3": {
    "title": "A Universal Dependencies Corpora Maintenance Methodology Using Downstream Application",
    "volume": "workshop",
    "abstract": "This paper investigates updates of Universal Dependencies (UD) treebanks in 23 languages and their impact on a downstream application. Numerous people are involved in updating UD's annotation guidelines and treebanks in various languages. However, it is not easy to verify whether the updated resources maintain universality with other language resources. Thus, validity and consistency of multilingual corpora should be tested through application tasks involving syntactic structures with PoS tags, dependency labels, and universal features. We apply the syntactic parsers trained on UD treebanks from multiple versions (2.0 to 2.7) to a clause-level sentiment extractor. We then analyze the relationships between attachment scores of dependency parsers and performance in application tasks. For future UD developments, we show examples of outputs that differ depending on version",
    "checked": true,
    "id": "098174d9d1033cd774869b721e37c172a296d53a",
    "semantic_title": "a universal dependencies corpora maintenance methodology using downstream application",
    "citation_count": 0,
    "authors": [
      "Ran Iwamoto",
      "Hiroshi Kanayama",
      "Alexandre Rademaker",
      "Takuya Ohko"
    ]
  },
  "https://aclanthology.org/2021.sigtyp-1.4": {
    "title": "Improving Cross-Lingual Sentiment Analysis via Conditional Language Adversarial Nets",
    "volume": "workshop",
    "abstract": "Sentiment analysis has come a long way for high-resource languages due to the availability of large annotated corpora. However, it still suffers from lack of training data for low-resource languages. To tackle this problem, we propose Conditional Language Adversarial Network (CLAN), an end-to-end neural architecture for cross-lingual sentiment analysis without cross-lingual supervision. CLAN differs from prior work in that it allows the adversarial training to be conditioned on both learned features and the sentiment prediction, to increase discriminativity for learned representation in the cross-lingual setting. Experimental results demonstrate that CLAN outperforms previous methods on the multilingual multi-domain Amazon review dataset. Our source code is released at https://github.com/hemanthkandula/clan",
    "checked": true,
    "id": "9b2f4d163616f9ec4f994a6ab6bd8a6925dbfd3a",
    "semantic_title": "improving cross-lingual sentiment analysis via conditional language adversarial nets",
    "citation_count": 2,
    "authors": [
      "Hemanth Kandula",
      "Bonan Min"
    ]
  },
  "https://aclanthology.org/2021.sigtyp-1.5": {
    "title": "Improving the Performance of UDify with Linguistic Typology Knowledge",
    "volume": "workshop",
    "abstract": "UDify is the state-of-the-art language-agnostic dependency parser which is trained on a polyglot corpus of 75 languages. This multilingual modeling enables the model to generalize over unknown/lesser-known languages, thus leading to improved performance on low-resource languages. In this work we used linguistic typology knowledge available in URIEL database, to improve the cross-lingual transferring ability of UDify even further",
    "checked": true,
    "id": "95d97288e3af9c891f172a8fc624983a3141e27a",
    "semantic_title": "improving the performance of udify with linguistic typology knowledge",
    "citation_count": 3,
    "authors": [
      "Chinmay Choudhary"
    ]
  },
  "https://aclanthology.org/2021.sigtyp-1.6": {
    "title": "FrameNet and Typology",
    "volume": "workshop",
    "abstract": "FrameNet and the Multilingual FrameNet project have produced multilingual semantic annotations of parallel texts that yield extremely fine-grained typological insights. Moreover, frame semantic annotation of a wide cross-section of languages would provide information on the limits of Frame Semantics (Fillmore 1982, Fillmore1985). Multilingual semantic annotation offers critical input for research on linguistic diversity and recurrent patterns in computational typology. Drawing on results from FrameNet annotation of parallel texts, this paper proposes frame semantic annotation as a new component to complement the state of the art in computational semantic typology",
    "checked": true,
    "id": "b8b8226f0aaf7bfdc3fe193faaeb07c211449196",
    "semantic_title": "framenet and typology",
    "citation_count": 0,
    "authors": [
      "Michael Ellsworth",
      "Collin Baker",
      "Miriam R. L. Petruck"
    ]
  },
  "https://aclanthology.org/2021.sigtyp-1.7": {
    "title": "Family of Origin and Family of Choice: Massively Parallel Lexiconized Iterative Pretraining for Severely Low Resource Text-based Translation",
    "volume": "workshop",
    "abstract": "We translate a closed text that is known in advance into a severely low resource language by leveraging massive source parallelism. In other words, given a text in 124 source languages, we translate it into a severely low resource language using only ∼1,000 lines of low resource data without any external help. Firstly, we propose a systematic method to rank and choose source languages that are close to the low resource language. We call the linguistic definition of language family Family of Origin (FAMO), and we call the empirical definition of higher-ranked languages using our metrics Family of Choice (FAMC). Secondly, we build an Iteratively Pretrained Multilingual Order-preserving Lexiconized Transformer (IPML) to train on ∼1,000 lines (∼3.5%) of low resource data. In order to translate named entities well, we build a massive lexicon table for 2,939 Bible named entities in 124 source languages, and include many that occur once and covers more than 66 severely low resource languages. Moreover, we also build a novel method of combining translations from different source languages into one. Using English as a hypothetical low resource language, we get a +23.9 BLEU increase over a multilingual baseline, and a +10.3 BLEU increase over our asymmetric baseline in the Bible dataset. We get a 42.8 BLEU score for Portuguese-English translation on the medical EMEA dataset. We also have good results for a real severely low resource Mayan language, Eastern Pokomchi",
    "checked": true,
    "id": "07dbcf2ec873c1ecbd6c7eb93e13d7618e91e76b",
    "semantic_title": "family of origin and family of choice: massively parallel lexiconized iterative pretraining for severely low resource text-based translation",
    "citation_count": 4,
    "authors": [
      "Zhong Zhou",
      "Alexander Waibel"
    ]
  },
  "https://aclanthology.org/2021.sigtyp-1.8": {
    "title": "Measuring Prefixation and Suffixation in the Languages of the World",
    "volume": "workshop",
    "abstract": "It has long been recognized that suffixing is more common than prefixing in the languages of the world. More detailed statistics on this tendency are needed to sharpen proposed explanations for this tendency. The classic approach to gathering data on the prefix/suffix preference is for a human to read grammatical descriptions (948 languages), which is time-consuming and involves discretization judgments. In this paper we explore two machine-driven approaches for prefix and suffix statistics which are crude approximations, but have advantages in terms of time and replicability. The first simply searches a large collection of grammatical descriptions for occurrences of the terms ‘prefix' and ‘suffix' (4 287 languages). The second counts substrings from raw text data in a way indirectly reflecting prefixation and suffixation (1 030 languages, using New Testament translations). The three approaches largely agree in their measurements but there are important theoretical and practical differences. In all measurements, there is an overall preference for suffixation, albeit only slightly, at ratios ranging between 0.51 and 0.68",
    "checked": true,
    "id": "262bcba4deb1412161a6e20836091cafd9bd0e05",
    "semantic_title": "measuring prefixation and suffixation in the languages of the world",
    "citation_count": 5,
    "authors": [
      "Harald Hammarström"
    ]
  },
  "https://aclanthology.org/2021.sigtyp-1.9": {
    "title": "Predicting and Explaining French Grammatical Gender",
    "volume": "workshop",
    "abstract": "Grammatical gender may be determined by semantics, orthography, phonology, or could even be arbitrary. Identifying patterns in the factors that govern noun genders can be useful for language learners, and for understanding innate linguistic sources of gender bias. Traditional manual rule-based approaches may be substituted by more accurate and scalable but harder-to-interpret computational approaches for predicting gender from typological information. In this work, we propose interpretable gender classification models for French, which obtain the best of both worlds. We present high accuracy neural approaches which are augmented by a novel global surrogate based approach for explaining predictions. We introduce ‘auxiliary attributes' to provide tunable explanation complexity",
    "checked": true,
    "id": "af7d900fddc42001a564fea5e7e1cf63bef0dd71",
    "semantic_title": "predicting and explaining french grammatical gender",
    "citation_count": 3,
    "authors": [
      "Saumya Sahai",
      "Dravyansh Sharma"
    ]
  },
  "https://aclanthology.org/2021.sigtyp-1.10": {
    "title": "Morph Call: Probing Morphosyntactic Content of Multilingual Transformers",
    "volume": "workshop",
    "abstract": "The outstanding performance of transformer-based language models on a great variety of NLP and NLU tasks has stimulated interest in exploration of their inner workings. Recent research has been primarily focused on higher-level and complex linguistic phenomena such as syntax, semantics, world knowledge and common-sense. The majority of the studies is anglocentric, and little remains known regarding other languages, specifically their morphosyntactic properties. To this end, our work presents Morph Call, a suite of 46 probing tasks for four Indo-European languages of different morphology: Russian, French, English and German. We propose a new type of probing tasks based on detection of guided sentence perturbations. We use a combination of neuron-, layer- and representation-level introspection techniques to analyze the morphosyntactic content of four multilingual transformers, including their understudied distilled versions. Besides, we examine how fine-tuning on POS-tagging task affects the probing performance",
    "checked": true,
    "id": "2f9a33496079585ed2ed0d8118ec28e3f609c100",
    "semantic_title": "morph call: probing morphosyntactic content of multilingual transformers",
    "citation_count": 6,
    "authors": [
      "Vladislav Mikhailov",
      "Oleg Serikov",
      "Ekaterina Artemova"
    ]
  },
  "https://aclanthology.org/2021.sigtyp-1.11": {
    "title": "SIGTYP 2021 Shared Task: Robust Spoken Language Identification",
    "volume": "workshop",
    "abstract": "While language identification is a fundamental speech and language processing task, for many languages and language families it remains a challenging task. For many low-resource and endangered languages this is in part due to resource availability: where larger datasets exist, they may be single-speaker or have different domains than desired application scenarios, demanding a need for domain and speaker-invariant language identification systems. This year's shared task on robust spoken language identification sought to investigate just this scenario: systems were to be trained on largely single-speaker speech from one domain, but evaluated on data in other domains recorded from speakers under different recording circumstances, mimicking realistic low-resource scenarios. We see that domain and speaker mismatch proves very challenging for current methods which can perform above 95% accuracy in-domain, which domain adaptation can address to some degree, but that these conditions merit further investigation to make spoken language identification accessible in many scenarios",
    "checked": true,
    "id": "66901b630b2d0b248fb02bf6d7a6a9d6244f9d0a",
    "semantic_title": "sigtyp 2021 shared task: robust spoken language identification",
    "citation_count": 10,
    "authors": [
      "Elizabeth Salesky",
      "Badr M. Abdullah",
      "Sabrina Mielke",
      "Elena Klyachko",
      "Oleg Serikov",
      "Edoardo Maria Ponti",
      "Ritesh Kumar",
      "Ryan Cotterell",
      "Ekaterina Vylomova"
    ]
  },
  "https://aclanthology.org/2021.sigtyp-1.12": {
    "title": "Language ID Prediction from Speech Using Self-Attentive Pooling",
    "volume": "workshop",
    "abstract": "This memo describes NTR-TSU submission for SIGTYP 2021 Shared Task on predicting language IDs from speech. Spoken Language Identification (LID) is an important step in a multilingual Automated Speech Recognition (ASR) system pipeline. For many low-resource and endangered languages, only single-speaker recordings may be available, demanding a need for domain and speaker-invariant language ID systems. In this memo, we show that a convolutional neural network with a Self-Attentive Pooling layer shows promising results for the language identification task",
    "checked": true,
    "id": "be5c495949c2b4ad3c35f23b4efc7fdbbb1dcb94",
    "semantic_title": "language id prediction from speech using self-attentive pooling",
    "citation_count": 2,
    "authors": [
      "Roman Bedyakin",
      "Nikolay Mikhaylovskiy"
    ]
  },
  "https://aclanthology.org/2021.sigtyp-1.13": {
    "title": "A ResNet-50-Based Convolutional Neural Network Model for Language ID Identification from Speech Recordings",
    "volume": "workshop",
    "abstract": "This paper describes the model built for the SIGTYP 2021 Shared Task aimed at identifying 18 typologically different languages from speech recordings. Mel-frequency cepstral coefficients derived from audio files are transformed into spectrograms, which are then fed into a ResNet-50-based CNN architecture. The final model achieved validation and test accuracies of 0.73 and 0.53, respectively",
    "checked": true,
    "id": "5c389938a67f2b056ffdc65ecc40e1787173e6b9",
    "semantic_title": "a resnet-50-based convolutional neural network model for language id identification from speech recordings",
    "citation_count": 19,
    "authors": [
      "Giuseppe G. A. Celano"
    ]
  },
  "https://aclanthology.org/2021.sigtyp-1.14": {
    "title": "Anlirika: An LSTM–CNN Flow Twister for Spoken Language Identification",
    "volume": "workshop",
    "abstract": "The paper presents Anlirika's submission to SIGTYP 2021 Shared Task on Robust Spoken Language Identification. The task aims at building a robust system that generalizes well across different domains and speakers. The training data is limited to a single domain only with predominantly single speaker per language while the validation and test data samples are derived from diverse dataset and multiple speakers. We experiment with a neural system comprising a combination of dense, convolutional, and recurrent layers that are designed to perform better generalization and obtain speaker-invariant representations. We demonstrate that the task in its constrained form (without making use of external data or augmentation the train set with samples from the validation set) is still challenging. Our best system trained on the data augmented with validation samples achieves 29.9% accuracy on the test data",
    "checked": true,
    "id": "4e3a0aa2ce77563a7ff7d440ad2388aaa6fc92d0",
    "semantic_title": "anlirika: an lstm–cnn flow twister for spoken language identification",
    "citation_count": 4,
    "authors": [
      "Andreas Scherbakov",
      "Liam Whittle",
      "Ritesh Kumar",
      "Siddharth Singh",
      "Matthew Coleman",
      "Ekaterina Vylomova"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.1": {
    "title": "Statistically Evaluating Social Media Sentiment Trends towards COVID-19 Non-Pharmaceutical Interventions with Event Studies",
    "volume": "workshop",
    "abstract": "In the midst of a global pandemic, understanding the public's opinion of their government's policy-level, non-pharmaceutical interventions (NPIs) is a crucial component of the health-policy-making process. Prior work on CoViD-19 NPI sentiment analysis by the epidemiological community has proceeded without a method for properly attributing sentiment changes to events, an ability to distinguish the influence of various events across time, a coherent model for predicting the public's opinion of future events of the same sort, nor even a means of conducting significance tests. We argue here that this urgently needed evaluation method does already exist. In the financial sector, event studies of the fluctuations in a publicly traded company's stock price are commonplace for determining the effects of earnings announcements, product placements, etc. The same method is suitable for analysing temporal sentiment variation in the light of policy-level NPIs. We provide a case study of Twitter sentiment towards policy-level NPIs in Canada. Our results confirm a generally positive connection between the announcements of NPIs and Twitter sentiment, and we document a promising correlation between the results of this study and a public-health survey of popular compliance with NPIs",
    "checked": true,
    "id": "ff07f7c77c57f0b9089bd2428080df303880953f",
    "semantic_title": "statistically evaluating social media sentiment trends towards covid-19 non-pharmaceutical interventions with event studies",
    "citation_count": 2,
    "authors": [
      "Jingcheng Niu",
      "Erin Rees",
      "Victoria Ng",
      "Gerald Penn"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.2": {
    "title": "View Distillation with Unlabeled Data for Extracting Adverse Drug Effects from User-Generated Data",
    "volume": "workshop",
    "abstract": "We present an algorithm based on multi-layer transformers for identifying Adverse Drug Reactions (ADR) in social media data. Our model relies on the properties of the problem and the characteristics of contextual word embeddings to extract two views from documents. Then a classifier is trained on each view to label a set of unlabeled documents to be used as an initializer for a new classifier in the other view. Finally, the initialized classifier in each view is further trained using the initial training examples. We evaluated our model in the largest publicly available ADR dataset. The experiments testify that our model significantly outperforms the transformer-based models pretrained on domain-specific data",
    "checked": true,
    "id": "8f11507867630c903461802e0ea0e95f3cf6ef17",
    "semantic_title": "view distillation with unlabeled data for extracting adverse drug effects from user-generated data",
    "citation_count": 2,
    "authors": [
      "Payam Karisani",
      "Jinho D. Choi",
      "Li Xiong"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.3": {
    "title": "The ProfNER shared task on automatic recognition of occupation mentions in social media: systems, evaluation, guidelines, embeddings and corpora",
    "volume": "workshop",
    "abstract": "Detection of occupations in texts is relevant for a range of important application scenarios, like competitive intelligence, sociodemographic analysis, legal NLP or health-related occupational data mining. Despite the importance and heterogeneous data types that mention occupations, text mining efforts to recognize them have been limited. This is due to the lack of clear annotation guidelines and high-quality Gold Standard corpora. Social media data can be regarded as a relevant source of information for real-time monitoring of at-risk occupational groups in the context of pandemics like the COVID-19 one, facilitating intervention strategies for occupations in direct contact with infectious agents or affected by mental health issues. To evaluate current NLP methods and to generate resources, we have organized the ProfNER track at SMM4H 2021, providing ProfNER participants with a Gold Standard corpus of manually annotated tweets (human IAA of 0.919) following annotation guidelines available in Spanish and English, an occupation gazetteer, a machine-translated version of tweets, and FastText embeddings. Out of 35 registered teams, 11 submitted a total of 27 runs. Best-performing participants built systems based on recent NLP technologies (e.g. transformers) and achieved 0.93 F-score in Text Classification and 0.839 in Named Entity Recognition. Corpus: https://doi.org/10.5281/zenodo.4309356",
    "checked": true,
    "id": "b4a05aef02446c6311fb60db90b2498e044ee037",
    "semantic_title": "the profner shared task on automatic recognition of occupation mentions in social media: systems, evaluation, guidelines, embeddings and corpora",
    "citation_count": 23,
    "authors": [
      "Antonio Miranda-Escalada",
      "Eulàlia Farré-Maduell",
      "Salvador Lima-López",
      "Luis Gascó",
      "Vicent Briva-Iglesias",
      "Marvin Agüero-Torales",
      "Martin Krallinger"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.4": {
    "title": "Overview of the Sixth Social Media Mining for Health Applications (#SMM4H) Shared Tasks at NAACL 2021",
    "volume": "workshop",
    "abstract": "The global growth of social media usage over the past decade has opened research avenues for mining health related information that can ultimately be used to improve public health. The Social Media Mining for Health Applications (#SMM4H) shared tasks in its sixth iteration sought to advance the use of social media texts such as Twitter for pharmacovigilance, disease tracking and patient centered outcomes. #SMM4H 2021 hosted a total of eight tasks that included reruns of adverse drug effect extraction in English and Russian and newer tasks such as detecting medication non-adherence from Twitter and WebMD forum, detecting self-reported adverse pregnancy outcomes, detecting cases and symptoms of COVID-19, identifying occupations mentioned in Spanish by Twitter users, and detecting self-reported breast cancer diagnosis. The eight tasks included a total of 12 individual subtasks spanning three languages requiring methods for binary classification, multi-class classification, named entity recognition and entity normalization. With a total of 97 registering teams and 40 teams submitting predictions, the interest in the shared tasks grew by 70% and participation grew by 38% compared to the previous iteration",
    "checked": true,
    "id": "26eabac11d907c45c461d6c8e82b2f90aa843b3b",
    "semantic_title": "overview of the sixth social media mining for health applications (#smm4h) shared tasks at naacl 2021",
    "citation_count": 78,
    "authors": [
      "Arjun Magge",
      "Ari Klein",
      "Antonio Miranda-Escalada",
      "Mohammed Ali Al-Garadi",
      "Ilseyar Alimova",
      "Zulfat Miftahutdinov",
      "Eulalia Farre",
      "Salvador Lima López",
      "Ivan Flores",
      "Karen O’Connor",
      "Davy Weissenbacher",
      "Elena Tutubalina",
      "Abeed Sarker",
      "Juan Banda",
      "Martin Krallinger",
      "Graciela Gonzalez-Hernandez"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.5": {
    "title": "BERT based Transformers lead the way in Extraction of Health Information from Social Media",
    "volume": "workshop",
    "abstract": "This paper describes our submissions for the Social Media Mining for Health (SMM4H) 2021 shared tasks. We participated in 2 tasks: (1) Classification, extraction and normalization of adverse drug effect (ADE) mentions in English tweets (Task-1) and (2) Classification of COVID-19 tweets containing symptoms (Task-6). Our approach for the first task uses the language representation model RoBERTa with a binary classification head. For the second task, we use BERTweet, based on RoBERTa. Fine-tuning is performed on the pre-trained models for both tasks. The models are placed on top of a custom domain-specific pre-processing pipeline. Our system ranked first among all the submissions for subtask-1(a) with an F1-score of 61%. For subtask-1(b), our system obtained an F1-score of 50% with improvements up to +8% F1 over the median score across all submissions. The BERTweet model achieved an F1 score of 94% on SMM4H 2021 Task-6",
    "checked": true,
    "id": "09df1d9fc501989d33092c020fd86d02ff47be25",
    "semantic_title": "bert based transformers lead the way in extraction of health information from social media",
    "citation_count": 13,
    "authors": [
      "Sidharth Ramesh",
      "Abhiraj Tiwari",
      "Parthivi Choubey",
      "Saisha Kashyap",
      "Sahil Khose",
      "Kumud Lakara",
      "Nishesh Singh",
      "Ujjwal Verma"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.6": {
    "title": "KFU NLP Team at SMM4H 2021 Tasks: Cross-lingual and Cross-modal BERT-based Models for Adverse Drug Effects",
    "volume": "workshop",
    "abstract": "This paper describes neural models developed for the Social Media Mining for Health (SMM4H) 2021 Shared Task. We participated in two tasks on classification of tweets that mention an adverse drug effect (ADE) (Tasks 1a & 2) and two tasks on extraction of ADE concepts (Tasks 1b & 1c). For classification, we investigate the impact of joint use of BERTbased language models and drug embeddings obtained by chemical structure BERT-based encoder. The BERT-based multimodal models ranked first and second on classification of Russian (Task 2) and English tweets (Task 1a) with the F1 scores of 57% and 61%, respectively. For Task 1b and 1c, we utilized the previous year's best solution based on the EnDR-BERT model with additional corpora. Our model achieved the best results in Task 1c, obtaining an F1 of 29%",
    "checked": true,
    "id": "dd3ede59e5bac000f3c5a7a0e856d3e4208983a4",
    "semantic_title": "kfu nlp team at smm4h 2021 tasks: cross-lingual and cross-modal bert-based models for adverse drug effects",
    "citation_count": 12,
    "authors": [
      "Andrey Sakhovskiy",
      "Zulfat Miftahutdinov",
      "Elena Tutubalina"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.7": {
    "title": "Transformer-based Multi-Task Learning for Adverse Effect Mention Analysis in Tweets",
    "volume": "workshop",
    "abstract": "This paper presents our contribution to the Social Media Mining for Health Applications Shared Task 2021. We addressed all the three subtasks of Task 1: Subtask A (classification of tweets containing adverse effects), Subtask B (extraction of text spans containing adverse effects) and Subtask C (adverse effects resolution). We explored various pre-trained transformer-based language models and we focused on a multi-task training architecture. For the first subtask, we also applied adversarial augmentation techniques and we formed model ensembles in order to improve the robustness of the prediction. Our system ranked first at Subtask B with 0.51 F1 score, 0.514 precision and 0.514 recall. For Subtask A we obtained 0.44 F1 score, 0.49 precision and 0.39 recall and for Subtask C we obtained 0.16 F1 score with 0.16 precision and 0.17 recall",
    "checked": true,
    "id": "e02753a4d6d556cb98d31c10c7a1ba57dd345c49",
    "semantic_title": "transformer-based multi-task learning for adverse effect mention analysis in tweets",
    "citation_count": 9,
    "authors": [
      "George-Andrei Dima",
      "Dumitru-Clementin Cercel",
      "Mihai Dascalu"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.8": {
    "title": "Pre-trained Transformer-based Classification and Span Detection Models for Social Media Health Applications",
    "volume": "workshop",
    "abstract": "This paper describes our approach for six classification tasks (Tasks 1a, 3a, 3b, 4 and 5) and one span detection task (Task 1b) from the Social Media Mining for Health (SMM4H) 2021 shared tasks. We developed two separate systems for classification and span detection, both based on pre-trained Transformer-based models. In addition, we applied oversampling and classifier ensembling in the classification tasks. The results of our submissions are over the median scores in all tasks except for Task 1a. Furthermore, our model achieved first place in Task 4 and obtained a 7% higher F1-score than the median in Task 1b",
    "checked": true,
    "id": "7302cb136dba812c8074a5dd0abdd574993480dd",
    "semantic_title": "pre-trained transformer-based classification and span detection models for social media health applications",
    "citation_count": 5,
    "authors": [
      "Yuting Guo",
      "Yao Ge",
      "Mohammed Ali Al-Garadi",
      "Abeed Sarker"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.9": {
    "title": "BERT Goes Brrr: A Venture Towards the Lesser Error in Classifying Medical Self-Reporters on Twitter",
    "volume": "workshop",
    "abstract": "This paper describes our team's submission for the Social Media Mining for Health (SMM4H) 2021 shared task. We participated in three subtasks: Classifying adverse drug effect, COVID-19 self-report, and COVID-19 symptoms. Our system is based on BERT model pre-trained on the domain-specific text. In addition, we perform data cleaning and augmentation, as well as hyperparameter optimization and model ensemble to further boost the BERT performance. We achieved the first rank in both classifying adverse drug effects and COVID-19 self-report tasks",
    "checked": true,
    "id": "e5fd86a21c1784e15fabd16ed036b9ceae2b0881",
    "semantic_title": "bert goes brrr: a venture towards the lesser error in classifying medical self-reporters on twitter",
    "citation_count": 5,
    "authors": [
      "Alham Fikri Aji",
      "Made Nindyatama Nityasya",
      "Haryo Akbarianto Wibowo",
      "Radityo Eko Prasojo",
      "Tirana Fatyanosa"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.10": {
    "title": "UACH-INAOE at SMM4H: a BERT based approach for classification of COVID-19 Twitter posts",
    "volume": "workshop",
    "abstract": "This work describes the participation of the Universidad Autónoma de Chihuahua - Instituto Nacional de Astrofísica, Óptica y Electrónica team at the Social Media Mining for Health Applications (SMM4H) 2021 shared task. Our team participated in task 5 and 6, both focused on the automatic classification of Twitter posts related to COVID-19. Task 5 was oriented on solving a binary classification problem, trying to identify self-reporting tweets of potential cases of COVID-19. Task 6 objective was to classify tweets containing COVID-19 symptoms. For both tasks we used models based on bidirectional encoder representations from transformers (BERT). Our objective was to determine if a model pretrained on a corpus in the domain of interest can outperform one trained on a much larger general domain corpus. Our F1 results were encouraging, 0.77 and 0.95 for task 5 and 6 respectively, having achieved the highest score among all the participants in the latter",
    "checked": true,
    "id": "ee1d1e037bc8314b02c69978b52032d8633df617",
    "semantic_title": "uach-inaoe at smm4h: a bert based approach for classification of covid-19 twitter posts",
    "citation_count": 8,
    "authors": [
      "Alberto Valdes",
      "Jesus Lopez",
      "Manuel Montes"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.11": {
    "title": "System description for ProfNER - SMMH: Optimized finetuning of a pretrained transformer and word vectors",
    "volume": "workshop",
    "abstract": "This shared task system description depicts two neural network architectures submitted to the ProfNER track, among them the winning system that scored highest in the two sub-tasks 7a and 7b. We present in detail the approach, preprocessing steps and the architectures used to achieve the submitted results, and also provide a GitHub repository to reproduce the scores. The winning system is based on a transformer-based pretrained language model and solves the two sub-tasks simultaneously",
    "checked": true,
    "id": "3990da25423d18fecf98515b2b775f514926203b",
    "semantic_title": "system description for profner - smmh: optimized finetuning of a pretrained transformer and word vectors",
    "citation_count": 4,
    "authors": [
      "David Carreto Fidalgo",
      "Daniel Vila-Suero",
      "Francisco Aranda Montes",
      "Ignacio Talavera Cepeda"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.12": {
    "title": "Word Embeddings, Cosine Similarity and Deep Learning for Identification of Professions & Occupations in Health-related Social Media",
    "volume": "workshop",
    "abstract": "ProfNER-ST focuses on the recognition of professions and occupations from Twitter using Spanish data. Our participation is based on a combination of word-level embeddings, including pre-trained Spanish BERT, as well as cosine similarity computed over a subset of entities that serve as input for an encoder-decoder architecture with attention mechanism. Finally, our best score achieved an F1-measure of 0.823 in the official test set",
    "checked": true,
    "id": "27d155226799ad34fa7a4f9df4668e90607b3629",
    "semantic_title": "word embeddings, cosine similarity and deep learning for identification of professions & occupations in health-related social media",
    "citation_count": 5,
    "authors": [
      "Sergio Santamaría Carrasco",
      "Roberto Cuervo Rosillo"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.13": {
    "title": "Classification, Extraction, and Normalization : CASIA_Unisound Team at the Social Media Mining for Health 2021 Shared Tasks",
    "volume": "workshop",
    "abstract": "This is the system description of the CASIA_Unisound team for Task 1, Task 7b, and Task 8 of the sixth Social Media Mining for Health Applications (SMM4H) shared task in 2021. Targeting on deal with two shared challenges, the colloquial text and the imbalance annotation, among those tasks, we apply a customized pre-trained language model and propose various training strategies. Experimental results show the effectiveness of our system. Moreover, we got an F1-score of 0.87 in task 8, which is the highest among all participates",
    "checked": true,
    "id": "7530701aedf6f98dad3d8b1e0b0850793d8e3f9d",
    "semantic_title": "classification, extraction, and normalization : casia_unisound team at the social media mining for health 2021 shared tasks",
    "citation_count": 5,
    "authors": [
      "Tong Zhou",
      "Zhucong Li",
      "Zhen Gan",
      "Baoli Zhang",
      "Yubo Chen",
      "Kun Niu",
      "Jing Wan",
      "Kang Liu",
      "Jun Zhao",
      "Yafei Shi",
      "Weifeng Chong",
      "Shengping Liu"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.14": {
    "title": "Neural Text Classification and Stacked Heterogeneous Embeddings for Named Entity Recognition in SMM4H 2021",
    "volume": "workshop",
    "abstract": "This paper presents our findings from participating in the SMM4H Shared Task 2021. We addressed Named Entity Recognition (NER) and Text Classification. To address NER we explored BiLSTM-CRF with Stacked Heterogeneous embeddings and linguistic features. We investigated various machine learning algorithms (logistic regression, SVM and Neural Networks) to address text classification. Our proposed approaches can be generalized to different languages and we have shown its effectiveness for English and Spanish. Our text classification submissions have achieved competitive performance with F1-score of 0.46 and 0.90 on ADE Classification (Task 1a) and Profession Classification (Task 7a) respectively. In the case of NER, our submissions scored F1-score of 0.50 and 0.82 on ADE Span Detection (Task 1b) and Profession span detection (Task 7b) respectively",
    "checked": true,
    "id": "c5c3898f684042abbb20201ac251fa887c12193a",
    "semantic_title": "neural text classification and stacked heterogeneous embeddings for named entity recognition in smm4h 2021",
    "citation_count": 12,
    "authors": [
      "Usama Yaseen",
      "Stefan Langer"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.15": {
    "title": "BERT based Adverse Drug Effect Tweet Classification",
    "volume": "workshop",
    "abstract": "This paper describes models developed for the Social Media Mining for Health (SMM4H) 2021 shared tasks. Our team participated in the first subtask that classifies tweets with Adverse Drug Effect (ADE) mentions. Our best performing model utilizes BERTweet followed by a single layer of BiLSTM. The system achieves an F-score of 0.45 on the test set without the use of any auxiliary resources such as Part-of-Speech tags, dependency tags, or knowledge from medical dictionaries",
    "checked": true,
    "id": "dc11f69c3f1b2867c699763a4333cb0724227b58",
    "semantic_title": "bert based adverse drug effect tweet classification",
    "citation_count": 7,
    "authors": [
      "Tanay Kayastha",
      "Pranjal Gupta",
      "Pushpak Bhattacharyya"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.16": {
    "title": "A Joint Training Approach to Tweet Classification and Adverse Effect Extraction and Normalization for SMM4H 2021",
    "volume": "workshop",
    "abstract": "In this work we describe our submissions to the Social Media Mining for Health (SMM4H) 2021 Shared Task. We investigated the effectiveness of a joint training approach to Task 1, specifically classification, extraction and normalization of Adverse Drug Effect (ADE) mentions in English tweets. Our approach performed well on the normalization task, achieving an above average f1 score of 24%, but less so on classification and extraction, with f1 scores of 22% and 37% respectively. Our experiments also showed that a larger dataset with more negative results led to stronger results than a smaller more balanced dataset, even when both datasets have the same positive examples. Finally we also submitted a tuned BERT model for Task 6: Classification of Covid-19 tweets containing symptoms, which achieved an above average f1 score of 96%",
    "checked": true,
    "id": "926434a54792cff939b7e25919f923cbf41cc809",
    "semantic_title": "a joint training approach to tweet classification and adverse effect extraction and normalization for smm4h 2021",
    "citation_count": 3,
    "authors": [
      "Mohab Elkaref",
      "Lamiece Hassan"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.17": {
    "title": "Text Augmentation Techniques in Drug Adverse Effect Detection Task",
    "volume": "workshop",
    "abstract": "The paper researches the problem of drug adverse effect detection in texts of social media. We describe the development of such classification system for Russian tweets. To increase the train dataset we apply a couple of augmentation techniques and analyze their effect in comparison with similar systems presented at 2021 years' SMM4H Workshop",
    "checked": true,
    "id": "31d4e4f46f12c364373d4325b7480037695e6910",
    "semantic_title": "text augmentation techniques in drug adverse effect detection task",
    "citation_count": 1,
    "authors": [
      "Pavel Blinov"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.18": {
    "title": "Classification of Tweets Self-reporting Adverse Pregnancy Outcomes and Potential COVID-19 Cases Using RoBERTa Transformers",
    "volume": "workshop",
    "abstract": "This study describes our proposed model design for SMM4H 2021 shared tasks. We fine-tune the language model of RoBERTa transformers and their connecting classifier to complete the classification tasks of tweets for adverse pregnancy outcomes (Task 4) and potential COVID-19 cases (Task 5). The evaluation metric is F1-score of the positive class for both tasks. For Task 4, our best score of 0.93 exceeded the mean score of 0.925. For Task 5, our best of 0.75 exceeded the mean score of 0.745",
    "checked": true,
    "id": "59e7925f029fd1da2741d00dcea4b11bfa36b318",
    "semantic_title": "classification of tweets self-reporting adverse pregnancy outcomes and potential covid-19 cases using roberta transformers",
    "citation_count": 4,
    "authors": [
      "Lung-Hao Lee",
      "Man-Chen Hung",
      "Chien-Huan Lu",
      "Chang-Hao Chen",
      "Po-Lei Lee",
      "Kuo-Kai Shyu"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.19": {
    "title": "NLP@NISER: Classification of COVID19 tweets containing symptoms",
    "volume": "workshop",
    "abstract": "In this paper, we describe our approaches for task six of Social Media Mining for Health Applications (SMM4H) shared task in 2021. The task is to classify twitter tweets containing COVID-19 symptoms in three classes (self-reports, non-personal reports & literature/news mentions). We implemented BERT and XLNet for this text classification task. Best result was achieved by XLNet approach, which is F1 score 0.94, precision 0.9448 and recall 0.94448. This is slightly better than the average score, i.e. F1 score 0.93, precision 0.93235 and recall 0.93235",
    "checked": true,
    "id": "7d260aa70a788ccaa0b64fe0122448a3aa34b58f",
    "semantic_title": "nlp@niser: classification of covid19 tweets containing symptoms",
    "citation_count": 3,
    "authors": [
      "Deepak Kumar",
      "Nalin Kumar",
      "Subhankar Mishra"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.20": {
    "title": "Identification of profession & occupation in Health-related Social Media using tweets in Spanish",
    "volume": "workshop",
    "abstract": "In this paper we present our approach and system description on Task 7a in ProfNer-ST: Identification of profession & occupation in Health related Social Media. Our main contribution is to show the effectiveness of using BETO-Spanish BERT as a model based on transformers pretrained with a Spanish Corpus for classification tasks. In our experiments we compared several architectures based on transformers with others based on classical machine learning algorithms. With this approach, we achieved an F1-score of 0.92 in the evaluation process",
    "checked": true,
    "id": "d4583207b9252735047aa8446720a5c528998a6c",
    "semantic_title": "identification of profession & occupation in health-related social media using tweets in spanish",
    "citation_count": 4,
    "authors": [
      "Victoria Pachón",
      "Jacinto Mata Vázquez",
      "Juan Luís Domínguez Olmedo"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.21": {
    "title": "Lasige-BioTM at ProfNER: BiLSTM-CRF and contextual Spanish embeddings for Named Entity Recognition and Tweet Binary Classification",
    "volume": "workshop",
    "abstract": "The paper describes the participation of the Lasige-BioTM team at sub-tracks A and B of ProfNER, which was based on: i) a BiLSTM-CRF model that leverages contextual and classical word embeddings to recognize and classify the mentions, and ii) on a rule-based module to classify tweets. In the Evaluation phase, our model achieved a F1-score of 0.917 (0,031 more than the median) in sub-track A and a F1-score of 0.727 (0,034 less than the median) in sub-track B",
    "checked": true,
    "id": "3b5d9cd7c1a995a6223070f5a5c748365488f518",
    "semantic_title": "lasige-biotm at profner: bilstm-crf and contextual spanish embeddings for named entity recognition and tweet binary classification",
    "citation_count": 5,
    "authors": [
      "Pedro Ruas",
      "Vitor Andrade",
      "Francisco Couto"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.22": {
    "title": "Adversities are all you need: Classification of self-reported breast cancer posts on Twitter using Adversarial Fine-tuning",
    "volume": "workshop",
    "abstract": "In this paper, we describe our system entry for Shared Task 8 at SMM4H-2021, which is on automatic classification of self-reported breast cancer posts on Twitter. In our system, we use a transformer-based language model fine-tuning approach to automatically identify tweets in the self-reports category. Furthermore, we involve a Gradient-based Adversarial fine-tuning to improve the overall model's robustness. Our system achieved an F1-score of 0.8625 on the Development set and 0.8501 on the Test set in Shared Task-8 of SMM4H-2021",
    "checked": true,
    "id": "483b6ccab60f4837ca01f5100e819fed3627f4a6",
    "semantic_title": "adversities are all you need: classification of self-reported breast cancer posts on twitter using adversarial fine-tuning",
    "citation_count": 0,
    "authors": [
      "Adarsh Kumar",
      "Ojasv Kamal",
      "Susmita Mazumdar"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.23": {
    "title": "UoB at ProfNER 2021: Data Augmentation for Classification Using Machine Translation",
    "volume": "workshop",
    "abstract": "This paper describes the participation of the UoB-NLP team in the ProfNER-ST shared subtask 7a. The task was aimed at detecting the mention of professions in social media text. Our team experimented with two methods of improving the performance of pre-trained models: Specifically, we experimented with data augmentation through translation and the merging of multiple language inputs to meet the objective of the task. While the best performing model on the test data consisted of mBERT fine-tuned on augmented data using back-translation, the improvement is minor possibly because multi-lingual pre-trained models such as mBERT already have access to the kind of information provided through back-translation and bilingual data",
    "checked": true,
    "id": "0f3e44e8b92b4612838763ef8276295bfb7da368",
    "semantic_title": "uob at profner 2021: data augmentation for classification using machine translation",
    "citation_count": 3,
    "authors": [
      "Frances Adriana Laureano De Leon",
      "Harish Tayyar Madabushi",
      "Mark Lee"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.24": {
    "title": "IIITN NLP at SMM4H 2021 Tasks: Transformer Models for Classification on Health-Related Imbalanced Twitter Datasets",
    "volume": "workshop",
    "abstract": "With increasing users sharing health-related information on social media, there has been a rise in using social media for health monitoring and surveillance. In this paper, we present a system that addresses classic health-related binary classification problems presented in Tasks 1a, 4, and 8 of the 6th edition of Social Media Mining for Health Applications (SMM4H) shared tasks. We developed a system based on RoBERTa (for Task 1a & 4) and BioBERT (for Task 8). Furthermore, we address the challenge of the imbalanced dataset and propose techniques such as undersampling, oversampling, and data augmentation to overcome the imbalanced nature of a given health-related dataset",
    "checked": true,
    "id": "aa58bb491e96c075699f227a808d57ce997e6654",
    "semantic_title": "iiitn nlp at smm4h 2021 tasks: transformer models for classification on health-related imbalanced twitter datasets",
    "citation_count": 2,
    "authors": [
      "Varad Pimpalkhute",
      "Prajwal Nakhate",
      "Tausif Diwan"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.25": {
    "title": "OCHADAI at SMM4H-2021 Task 5: Classifying self-reporting tweets on potential cases of COVID-19 by ensembling pre-trained language models",
    "volume": "workshop",
    "abstract": "Since the outbreak of coronavirus at the end of 2019, there have been numerous studies on coro- navirus in the NLP arena. Meanwhile, Twitter has been a valuable source of news and a pub- lic medium for the conveyance of information and personal expression. This paper describes the system developed by the Ochadai team for the Social Media Mining for Health Appli- cations (SMM4H) 2021 Task 5, which aims to automatically distinguish English tweets that self-report potential cases of COVID-19 from those that do not. We proposed a model ensemble that leverages pre-trained represen- tations from COVID-Twitter-BERT (Müller et al., 2020), RoBERTa (Liu et al., 2019), and Twitter-RoBERTa (Glazkova et al., 2021). Our model obtained F1-scores of 76% on the test set in the evaluation phase, and 77.5% in the post-evaluation phase",
    "checked": true,
    "id": "e67e4fb2dbed59b7a4e9e734493082e8ddb8a1b0",
    "semantic_title": "ochadai at smm4h-2021 task 5: classifying self-reporting tweets on potential cases of covid-19 by ensembling pre-trained language models",
    "citation_count": 1,
    "authors": [
      "Ying Luo",
      "Lis Pereira",
      "Kobayashi Ichiro"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.26": {
    "title": "PAII-NLP at SMM4H 2021: Joint Extraction and Normalization of Adverse Drug Effect Mentions in Tweets",
    "volume": "workshop",
    "abstract": "This paper describes our system developed for the subtask 1c of the sixth Social Media Mining for Health Applications (SMM4H) shared task in 2021. The aim of the subtask is to recognize the adverse drug effect (ADE) mentions from tweets and normalize the identified mentions to their mapping MedDRA preferred term IDs. Our system is based on a neural transition-based joint model, which is to perform recognition and normalization simultaneously. Our final two submissions outperform the average F1 score by 1-2%",
    "checked": true,
    "id": "e042b9f8877472ce220920d343b9951d9b185e47",
    "semantic_title": "paii-nlp at smm4h 2021: joint extraction and normalization of adverse drug effect mentions in tweets",
    "citation_count": 4,
    "authors": [
      "Zongcheng Ji",
      "Tian Xia",
      "Mei Han"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.27": {
    "title": "Assessing multiple word embeddings for named entity recognition of professions and occupations in health-related social media",
    "volume": "workshop",
    "abstract": "This paper presents our contribution to the ProfNER shared task. Our work focused on evaluating different pre-trained word embedding representations suitable for the task. We further explored combinations of embeddings in order to improve the overall results",
    "checked": true,
    "id": "e72ca2d2abecfdc1b97e33027ec79145dcf92825",
    "semantic_title": "assessing multiple word embeddings for named entity recognition of professions and occupations in health-related social media",
    "citation_count": 6,
    "authors": [
      "Vasile Pais",
      "Maria Mitrofan"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.28": {
    "title": "Fine-tuning Transformers for Identifying Self-Reporting Potential Cases and Symptoms of COVID-19 in Tweets",
    "volume": "workshop",
    "abstract": "We describe our straight-forward approach for Tasks 5 and 6 of 2021 Social Media Min- ing for Health Applications (SMM4H) shared tasks. Our system is based on fine-tuning Dis- tillBERT on each task, as well as first fine- tuning the model on the other task. In this paper, we additionally explore how much fine- tuning is necessary for accurately classifying tweets as containing self-reported COVID-19 symptoms (Task 5) or whether a tweet related to COVID-19 is self-reporting, non-personal reporting, or a literature/news mention of the virus (Task 6)",
    "checked": true,
    "id": "36785d4d1eebec447dad524d881be3b43173c26b",
    "semantic_title": "fine-tuning transformers for identifying self-reporting potential cases and symptoms of covid-19 in tweets",
    "citation_count": 2,
    "authors": [
      "Max Fleming",
      "Priyanka Dondeti",
      "Caitlin Dreisbach",
      "Adam Poliak"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.29": {
    "title": "Classification of COVID19 tweets using Machine Learning Approaches",
    "volume": "workshop",
    "abstract": "The reported work is a description of our participation in the \"Classification of COVID19 tweets containing symptoms\" shared task, organized by the \"Social Media Mining for Health Applications (SMM4H)\" workshop. The literature describes two machine learning approaches that were used to build a three class classification system, that categorizes tweets related to COVID19, into three classes, viz., self-reports, non-personal reports, and literature/news mentions. The steps for pre-processing tweets, feature extraction, and the development of the machine learning models, are described extensively in the documentation. Both the developed learning models, when evaluated by the organizers, garnered F1 scores of 0.93 and 0.92 respectively",
    "checked": true,
    "id": "99a70dc5ac1289781177d34eaafa47fffb642221",
    "semantic_title": "classification of covid19 tweets using machine learning approaches",
    "citation_count": 6,
    "authors": [
      "Anupam Mondal",
      "Sainik Mahata",
      "Monalisa Dey",
      "Dipankar Das"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.30": {
    "title": "Fine-tuning BERT to classify COVID19 tweets containing symptoms",
    "volume": "workshop",
    "abstract": "Twitter is a valuable source of patient-generated data that has been used in various population health studies. The first step in many of these studies is to identify and capture Twitter messages (tweets) containing medication mentions. Identifying personal mentions of COVID19 symptoms requires distinguishing personal mentions from other mentions such as symptoms reported by others and references to news articles or other sources. In this article, we describe our submission to Task 6 of the Social Media Mining for Health Applications (SMM4H) Shared Task 2021. This task challenged participants to classify tweets where the target classes are:(1) self-reports,(2) non-personal reports, and (3) literature/news mentions. Our system used a handcrafted preprocessing and word embeddings from BERT encoder model. We achieved an F1 score of 93%",
    "checked": true,
    "id": "0f951e8ada2a49025223be2157e447df6c68f7da",
    "semantic_title": "fine-tuning bert to classify covid19 tweets containing symptoms",
    "citation_count": 1,
    "authors": [
      "Rajarshi Roychoudhury",
      "Sudip Naskar"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.31": {
    "title": "Identifying professions & occupations in Health-related Social Media using Natural Language Processing",
    "volume": "workshop",
    "abstract": "This paper describes the entry of the research group SINAI at SMM4H's ProfNER task on the identification of professions and occupations in social media related with health. Specifically we have participated in Task 7a: Tweet Binary Classification to determine whether a tweet contains mentions of occupations or not, as well as in Task 7b: NER Offset Detection and Classification aimed at predicting occupations mentions and classify them discriminating by professions and working statuses",
    "checked": true,
    "id": "c8420eaccd3cd7b362fda276df06db4d262bf8e0",
    "semantic_title": "identifying professions & occupations in health-related social media using natural language processing",
    "citation_count": 4,
    "authors": [
      "Alberto Mesa Murgado",
      "Ana Parras Portillo",
      "Pilar López Úbeda",
      "Maite Martin",
      "Alfonso Ureña-López"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.32": {
    "title": "Approaching SMM4H with auto-regressive language models and back-translation",
    "volume": "workshop",
    "abstract": "We describe our submissions to the 6th edition of the Social Media Mining for Health Applications (SMM4H) shared task. Our team (OGNLP) participated in the sub-task: Classification of tweets self-reporting potential cases of COVID-19 (Task 5). For our submissions, we employed systems based on auto-regressive transformer models (XLNet) and back-translation for balancing the dataset",
    "checked": true,
    "id": "417b16de7121312cdbbf15e685793647e5207427",
    "semantic_title": "approaching smm4h with auto-regressive language models and back-translation",
    "citation_count": 1,
    "authors": [
      "Joseph Cornelius",
      "Tilia Ellendorff",
      "Fabio Rinaldi"
    ]
  },
  "https://aclanthology.org/2021.smm4h-1.33": {
    "title": "ULD-NUIG at Social Media Mining for Health Applications (#SMM4H) Shared Task 2021",
    "volume": "workshop",
    "abstract": "Social media platforms such as Twitter and Facebook have been utilised for various research studies, from the cohort-level discussion to community-driven approaches to address the challenges in utilizing social media data for health, clinical and biomedical information. Detection of medical jargon's, named entity recognition, multi-word expression becomes the primary, fundamental steps in solving those challenges. In this paper, we enumerate the ULD-NUIG team's system, designed as part of Social Media Mining for Health Applications (#SMM4H) Shared Task 2021. The team conducted a series of experiments to explore the challenges of task 6 and task 5. The submitted systems achieve F-1 0.84 and 0.53 score for task 6 and 5 respectively",
    "checked": true,
    "id": "d6f4ffb30831fa3aec3721b0506c5175acfb6027",
    "semantic_title": "uld-nuig at social media mining for health applications (#smm4h) shared task 2021",
    "citation_count": 1,
    "authors": [
      "Atul Kr. Ojha",
      "Priya Rani",
      "Koustava Goswami",
      "Bharathi Raja Chakravarthi",
      "John P. McCrae"
    ]
  },
  "https://aclanthology.org/2021.socialnlp-1.1": {
    "title": "Analysis of Nuanced Stances and Sentiment Towards Entities of US Politicians through the Lens of Moral Foundation Theory",
    "volume": "workshop",
    "abstract": "The Moral Foundation Theory suggests five moral foundations that can capture the view of a user on a particular issue. It is widely used to identify sentence-level sentiment. In this paper, we study the Moral Foundation Theory in tweets by US politicians on two politically divisive issues - Gun Control and Immigration. We define the nuanced stance of politicians on these two topics by the grades given by related organizations to the politicians. First, we identify moral foundations in tweets from a huge corpus using deep relational learning. Then, qualitative and quantitative evaluations using the corpus show that there is a strong correlation between the moral foundation usage and the politicians' nuanced stance on a particular topic. We also found substantial differences in moral foundation usage by different political parties when they address different entities. All of these results indicate the need for more intense research in this area",
    "checked": true,
    "id": "7c146ff897c0d0860173ecd9030879314ef8a224",
    "semantic_title": "analysis of nuanced stances and sentiment towards entities of us politicians through the lens of moral foundation theory",
    "citation_count": 17,
    "authors": [
      "Shamik Roy",
      "Dan Goldwasser"
    ]
  },
  "https://aclanthology.org/2021.socialnlp-1.2": {
    "title": "Content-based Stance Classification of Tweets about the 2020 Italian Constitutional Referendum",
    "volume": "workshop",
    "abstract": "On September 2020 a constitutional referendum was held in Italy. In this work we collect a dataset of 1.2M tweets related to this event, with particular interest to the textual content shared, and we design a hashtag-based semi-automatic approach to label them as Supporters or Against the referendum. We use the labelled dataset to train a classifier based on transformers, unsupervisedly pre-trained on Italian corpora. Our model generalizes well on tweets that cannot be labeled by the hashtag-based approach. We check that no length-, lexicon- and sentiment-biases are present to affect the performance of the classifier. Finally, we discuss the discrepancy between the magnitudes of tweets expressing a specific stance, obtained using both the hashtag-based approach and our trained classifier, and the real outcome of the referendum: the referendum was approved by 70% of the voters, while the number of tweets against the referendum is four times greater than the number of tweets supporting it. We conclude that the Italian referendum was an example of event where the minority was very loud on social media, highly influencing the perception of the event. Analyzing only the activity on social media is dangerous and can lead to extremely wrong forecasts",
    "checked": true,
    "id": "c8f2dbb4addb9f8c531858f2aad80676c0787e95",
    "semantic_title": "content-based stance classification of tweets about the 2020 italian constitutional referendum",
    "citation_count": 6,
    "authors": [
      "Marco Di Giovanni",
      "Marco Brambilla"
    ]
  },
  "https://aclanthology.org/2021.socialnlp-1.3": {
    "title": "A Case Study of In-House Competition for Ranking Constructive Comments in a News Service",
    "volume": "workshop",
    "abstract": "Ranking the user comments posted on a news article is important for online news services because comment visibility directly affects the user experience. Research on ranking comments with different metrics to measure the comment quality has shown \"constructiveness\" used in argument analysis is promising from a practical standpoint. In this paper, we report a case study in which this constructiveness is examined in the real world. Specifically, we examine an in-house competition to improve the performance of ranking constructive comments and demonstrate the effectiveness of the best obtained model for a commercial service",
    "checked": true,
    "id": "2f4ad113b747dc03d19ef1d29ee6c6a0e0bcc781",
    "semantic_title": "a case study of in-house competition for ranking constructive comments in a news service",
    "citation_count": 0,
    "authors": [
      "Hayato Kobayashi",
      "Hiroaki Taguchi",
      "Yoshimune Tabuchi",
      "Chahine Koleejan",
      "Ken Kobayashi",
      "Soichiro Fujita",
      "Kazuma Murao",
      "Takeshi Masuyama",
      "Taichi Yatsuka",
      "Manabu Okumura",
      "Satoshi Sekine"
    ]
  },
  "https://aclanthology.org/2021.socialnlp-1.4": {
    "title": "Quantifying the Effects of COVID-19 on Restaurant Reviews",
    "volume": "workshop",
    "abstract": "The COVID-19 pandemic has implications beyond physical health, affecting society and economies. Government efforts to slow down the spread of the virus have had a severe impact on many businesses, including restaurants. Mandatory policies such as restaurant closures, bans on social gatherings, and social distancing restrictions have affected restaurant operations as well as customer preferences (e.g., prompting a demand of stricter hygiene standards). As of now, however, it is not clear how and to what extent the pandemic has affected restaurant reviews, an analysis of which could potentially inform policies for addressing this ongoing situation. In this work, we present our efforts to understand the effects of COVID-19 on restaurant reviews, with a focus on Yelp reviews produced during the pandemic for New York City and Los Angeles County restaurants. Overall, we make the following contributions. First, we assemble a dataset of 600 reviews with manual annotations of fine-grained COVID-19 aspects related to restaurants (e.g., hygiene practices, service changes, sympathy and support for local businesses). Second, we address COVID-19 aspect detection using supervised classifiers, weakly-supervised approaches based on keywords, and unsupervised topic modeling approaches, and experimentally show that classifiers based on pre-trained BERT representations achieve the best performance (F1=0.79). Third, we analyze the number and evolution of COVID-related aspects over time and show that the resulting time series have substantial correlation (Spearman's 𝜌=0.84) with critical statistics related to the COVID-19 pandemic, including the number of new COVID-19 cases. To our knowledge, this is the first work analyzing the effects of COVID-19 on Yelp restaurant reviews and could potentially inform policies by public health departments, for example, to cover resource utilization",
    "checked": true,
    "id": "9fafc9896d2b81d1328791e4c2fd7ab096f155f3",
    "semantic_title": "quantifying the effects of covid-19 on restaurant reviews",
    "citation_count": 8,
    "authors": [
      "Ivy Cao",
      "Zizhou Liu",
      "Giannis Karamanolakis",
      "Daniel Hsu",
      "Luis Gravano"
    ]
  },
  "https://aclanthology.org/2021.socialnlp-1.5": {
    "title": "Assessing Cognitive Linguistic Influences in the Assignment of Blame",
    "volume": "workshop",
    "abstract": "Lab studies in cognition and the psychology of morality have proposed some thematic and linguistic factors that influence moral reasoning. This paper assesses how well the findings of these studies generalize to a large corpus of over 22,000 descriptions of fraught situations posted to a dedicated forum. At this social-media site, users judge whether or not an author is in the wrong with respect to the event that the author described. We find that, consistent with lab studies, there are statistically significant differences in uses of first-person passive voice, as well as first-person agents and patients, between descriptions of situations that receive different blame judgments. These features also aid performance in the task of predicting the eventual collective verdicts",
    "checked": true,
    "id": "54b4a56b219513ad8847343906e43d307f67079a",
    "semantic_title": "assessing cognitive linguistic influences in the assignment of blame",
    "citation_count": 15,
    "authors": [
      "Karen Zhou",
      "Ana Smith",
      "Lillian Lee"
    ]
  },
  "https://aclanthology.org/2021.socialnlp-1.6": {
    "title": "Evaluating Deception Detection Model Robustness To Linguistic Variation",
    "volume": "workshop",
    "abstract": "With the increasing use of machine-learning driven algorithmic judgements, it is critical to develop models that are robust to evolving or manipulated inputs. We propose an extensive analysis of model robustness against linguistic variation in the setting of deceptive news detection, an important task in the context of misinformation spread online. We consider two prediction tasks and compare three state-of-the-art embeddings to highlight consistent trends in model performance, high confidence misclassifications, and high impact failures. By measuring the effectiveness of adversarial defense strategies and evaluating model susceptibility to adversarial attacks using character- and word-perturbed text, we find that character or mixed ensemble models are the most effective defenses and that character perturbation-based attack tactics are more successful",
    "checked": true,
    "id": "da708bfc4d775828de22f2f351d68ecfce680b14",
    "semantic_title": "evaluating deception detection model robustness to linguistic variation",
    "citation_count": 0,
    "authors": [
      "Maria Glenski",
      "Ellyn Ayton",
      "Robin Cosbey",
      "Dustin Arendt",
      "Svitlana Volkova"
    ]
  },
  "https://aclanthology.org/2021.socialnlp-1.7": {
    "title": "Reconsidering Annotator Disagreement about Racist Language: Noise or Signal?",
    "volume": "workshop",
    "abstract": "An abundance of methodological work aims to detect hateful and racist language in text. However, these tools are hampered by problems like low annotator agreement and remain largely disconnected from theoretical work on race and racism in the social sciences. Using annotations of 5188 tweets from 291 annotators, we investigate how annotator perceptions of racism in tweets vary by annotator racial identity and two text features of the tweets: relevant keywords and latent topics identified through structural topic modeling. We provide a descriptive summary of our data and estimate a series of generalized linear models to determine if annotator racial identity and our 12 latent topics, alone or in combination, explain the way racial sentiment was annotated, net of relevant annotator characteristics and tweet features. Our results show that White and non-White annotators exhibit significant differences in ratings when reading tweets with high prevalence of particular, racially-charged topics. We conclude by suggesting how future methodological work can draw on our results and further incorporate social science theory into analyses",
    "checked": true,
    "id": "61e873f717f88fb0bde1c3f221575ff5456da9b1",
    "semantic_title": "reconsidering annotator disagreement about racist language: noise or signal?",
    "citation_count": 29,
    "authors": [
      "Savannah Larimore",
      "Ian Kennedy",
      "Breon Haskett",
      "Alina Arseniev-Koehler"
    ]
  },
  "https://aclanthology.org/2021.socialnlp-1.8": {
    "title": "Understanding and Interpreting the Impact of User Context in Hate Speech Detection",
    "volume": "workshop",
    "abstract": "As hate speech spreads on social media and online communities, research continues to work on its automatic detection. Recently, recognition performance has been increasing thanks to advances in deep learning and the integration of user features. This work investigates the effects that such features can have on a detection model. Unlike previous research, we show that simple performance comparison does not expose the full impact of including contextual- and user information. By leveraging explainability techniques, we show (1) that user features play a role in the model's decision and (2) how they affect the feature space learned by the model. Besides revealing that—and also illustrating why—user features are the reason for performance gains, we show how such techniques can be combined to better understand the model and to detect unintended bias",
    "checked": true,
    "id": "4529dfc6f5d224973afa278c82081ed2e7821a24",
    "semantic_title": "understanding and interpreting the impact of user context in hate speech detection",
    "citation_count": 30,
    "authors": [
      "Edoardo Mosca",
      "Maximilian Wich",
      "Georg Groh"
    ]
  },
  "https://aclanthology.org/2021.socialnlp-1.9": {
    "title": "Self-Contextualized Attention for Abusive Language Identification",
    "volume": "workshop",
    "abstract": "The use of attention mechanisms in deep learning approaches has become popular in natural language processing due to its outstanding performance. The use of these mechanisms allows one managing the importance of the elements of a sequence in accordance to their context, however, this importance has been observed independently between the pairs of elements of a sequence (self-attention) and between the application domain of a sequence (contextual attention), leading to the loss of relevant information and limiting the representation of the sequences. To tackle these particular issues we propose the self-contextualized attention mechanism, which trades off the previous limitations, by considering the internal and contextual relationships between the elements of a sequence. The proposed mechanism was evaluated in four standard collections for the abusive language identification task achieving encouraging results. It outperformed the current attention mechanisms and showed a competitive performance with respect to state-of-the-art approaches",
    "checked": true,
    "id": "80ad370f4e4b537e666f9f8acfeb9c7f628efbf3",
    "semantic_title": "self-contextualized attention for abusive language identification",
    "citation_count": 2,
    "authors": [
      "Horacio Jarquín-Vásquez",
      "Hugo Jair Escalante",
      "Manuel Montes"
    ]
  },
  "https://aclanthology.org/2021.socialnlp-1.10": {
    "title": "Unsupervised Domain Adaptation in Cross-corpora Abusive Language Detection",
    "volume": "workshop",
    "abstract": "The state-of-the-art abusive language detection models report great in-corpus performance, but underperform when evaluated on abusive comments that differ from the training scenario. As human annotation involves substantial time and effort, models that can adapt to newly collected comments can prove to be useful. In this paper, we investigate the effectiveness of several Unsupervised Domain Adaptation (UDA) approaches for the task of cross-corpora abusive language detection. In comparison, we adapt a variant of the BERT model, trained on large-scale abusive comments, using Masked Language Model (MLM) fine-tuning. Our evaluation shows that the UDA approaches result in sub-optimal performance, while the MLM fine-tuning does better in the cross-corpora setting. Detailed analysis reveals the limitations of the UDA approaches and emphasizes the need to build efficient adaptation methods for this task",
    "checked": true,
    "id": "bd6bca48d9440d49acbd26f2efff0629a2886c63",
    "semantic_title": "unsupervised domain adaptation in cross-corpora abusive language detection",
    "citation_count": 12,
    "authors": [
      "Tulika Bose",
      "Irina Illina",
      "Dominique Fohr"
    ]
  },
  "https://aclanthology.org/2021.socialnlp-1.11": {
    "title": "Using Noisy Self-Reports to Predict Twitter User Demographics",
    "volume": "workshop",
    "abstract": "Computational social science studies often contextualize content analysis within standard demographics. Since demographics are unavailable on many social media platforms (e.g. Twitter), numerous studies have inferred demographics automatically. Despite many studies presenting proof-of-concept inference of race and ethnicity, training of practical systems remains elusive since there are few annotated datasets. Existing datasets are small, inaccurate, or fail to cover the four most common racial and ethnic groups in the United States. We present a method to identify self-reports of race and ethnicity from Twitter profile descriptions. Despite the noise of automated supervision, our self-report datasets enable improvements in classification performance on gold standard self-report survey data. The result is a reproducible method for creating large-scale training resources for race and ethnicity",
    "checked": true,
    "id": "7ccc13da5ed0f2dbfde7e02b7d590c74cf4bdae2",
    "semantic_title": "using noisy self-reports to predict twitter user demographics",
    "citation_count": 18,
    "authors": [
      "Zach Wood-Doughty",
      "Paiheng Xu",
      "Xiao Liu",
      "Mark Dredze"
    ]
  },
  "https://aclanthology.org/2021.socialnlp-1.12": {
    "title": "PANDORA Talks: Personality and Demographics on Reddit",
    "volume": "workshop",
    "abstract": "Personality and demographics are important variables in social sciences and computational sociolinguistics. However, datasets with both personality and demographic labels are scarce. To address this, we present PANDORA, the first dataset of Reddit comments of 10k users partially labeled with three personality models and demographics (age, gender, and location), including 1.6k users labeled with the well-established Big 5 personality model. We showcase the usefulness of this dataset on three experiments, where we leverage the more readily available data from other personality models to predict the Big 5 traits, analyze gender classification biases arising from psycho-demographic variables, and carry out a confirmatory and exploratory analysis based on psychological theories. Finally, we present benchmark prediction models for all personality and demographic variables",
    "checked": true,
    "id": "1b297d8598b5a9688066a96e76cddcf99a98d970",
    "semantic_title": "pandora talks: personality and demographics on reddit",
    "citation_count": 52,
    "authors": [
      "Matej Gjurković",
      "Mladen Karan",
      "Iva Vukojević",
      "Mihaela Bošnjak",
      "Jan Snajder"
    ]
  },
  "https://aclanthology.org/2021.socialnlp-1.13": {
    "title": "Room to Grow: Understanding Personal Characteristics Behind Self Improvement Using Social Media",
    "volume": "workshop",
    "abstract": "Many people aim for change, but not everyone succeeds. While there are a number of social psychology theories that propose motivation-related characteristics of those who persist with change, few computational studies have explored the motivational stage of personal change. In this paper, we investigate a new dataset consisting of the writings of people who manifest intention to change, some of whom persist while others do not. Using a variety of linguistic analysis techniques, we first examine the writing patterns that distinguish the two groups of people. Persistent people tend to reference more topics related to long-term self-improvement and use a more complicated writing style. Drawing on these consistent differences, we build a classifier that can reliably identify the people more likely to persist, based on their language. Our experiments provide new insights into the motivation-related behavior of people who persist with their intention to change",
    "checked": true,
    "id": "c1744647c01255fe3373208c48808f277f8ce251",
    "semantic_title": "room to grow: understanding personal characteristics behind self improvement using social media",
    "citation_count": 2,
    "authors": [
      "MeiXing Dong",
      "Xueming Xu",
      "Yiwei Zhang",
      "Ian Stewart",
      "Rada Mihalcea"
    ]
  },
  "https://aclanthology.org/2021.socialnlp-1.14": {
    "title": "Mitigating Temporal-Drift: A Simple Approach to Keep NER Models Crisp",
    "volume": "workshop",
    "abstract": "Performance of neural models for named entity recognition degrades over time, becoming stale. This degradation is due to temporal drift, the change in our target variables' statistical properties over time. This issue is especially problematic for social media data, where topics change rapidly. In order to mitigate the problem, data annotation and retraining of models is common. Despite its usefulness, this process is expensive and time-consuming, which motivates new research on efficient model updating. In this paper, we propose an intuitive approach to measure the potential trendiness of tweets and use this metric to select the most informative instances to use for training. We conduct experiments on three state-of-the-art models on the Temporal Twitter Dataset. Our approach shows larger increases in prediction accuracy with less training data than the alternatives, making it an attractive, practical solution",
    "checked": true,
    "id": "e61a8123e1ff4353c206ad1c86717b321300dc13",
    "semantic_title": "mitigating temporal-drift: a simple approach to keep ner models crisp",
    "citation_count": 7,
    "authors": [
      "Shuguang Chen",
      "Leonardo Neves",
      "Thamar Solorio"
    ]
  },
  "https://aclanthology.org/2021.socialnlp-1.15": {
    "title": "Jujeop: Korean Puns for K-pop Stars on Social Media",
    "volume": "workshop",
    "abstract": "Jujeop is a type of pun and a unique way for fans to express their love for the K-pop stars they follow using Korean. One of the unique characteristics of Jujeop is its use of exaggerated expressions to compliment K-pop stars, which contain or lead to humor. Based on this characteristic, Jujeop can be separated into four distinct types, with their own lexical collocations: (1) Fragmenting words to create a twist, (2) Homophones and homographs, (3) Repetition, and (4) Nonsense. Thus, the current study first defines the concept of Jujeop in Korean, manually labels 8.6K comments and annotates the comments to one of the four Jujeop types. With the given annotated corpus, this study presents distinctive characteristics of Jujeop comments compared to the other comments by classification task. Moreover, with the clustering approach, we proposed a structural dependency within each Jujeop type. We have made our dataset publicly available for future research of Jujeop expressions",
    "checked": true,
    "id": "b7b8b3c35d56da6cdc35772822cb9cb25253d892",
    "semantic_title": "jujeop: korean puns for k-pop stars on social media",
    "citation_count": 4,
    "authors": [
      "Soyoung Oh",
      "Jisu Kim",
      "Seungpeel Lee",
      "Eunil Park"
    ]
  },
  "https://aclanthology.org/2021.socialnlp-1.16": {
    "title": "Identifying Distributional Perspectives from Colingual Groups",
    "volume": "workshop",
    "abstract": "Discrepancies exist among different cultures or languages. A lack of mutual understanding among different colingual groups about the perspectives on specific values or events may lead to uninformed decisions or biased opinions. Thus, automatically understanding the group perspectives can provide essential back-ground for many natural language processing tasks. In this paper, we study colingual groups and use language corpora as a proxy to identify their distributional perspectives. We present a novel computational approach to learn shared understandings, and benchmark our method by building culturally-aware models for the English, Chinese, and Japanese languages. Ona held out set of diverse topics, including marriage, corruption, democracy, etc., our model achieves high correlation with human judgements regarding intra-group values and inter-group differences",
    "checked": true,
    "id": "060ffa95caec15a972c6b88d3b4a4cd6f9eb4f80",
    "semantic_title": "identifying distributional perspectives from colingual groups",
    "citation_count": 4,
    "authors": [
      "Yufei Tian",
      "Tuhin Chakrabarty",
      "Fred Morstatter",
      "Nanyun Peng"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.1": {
    "title": "Pedagogical Principles in the Online Teaching of Text Mining: A Retrospection",
    "volume": "workshop",
    "abstract": "The ongoing COVID-19 pandemic has brought online education to the forefront of pedagogical discussions. To make this increased interest sustainable in a post-pandemic era, online courses must be built on strong pedagogical foundations. With a long history of pedagogic research, there are many principles, frameworks, and models available to help teachers in doing so. These models cover different teaching perspectives, such as constructive alignment, feedback, and the learning environment. In this paper, we discuss how we designed and implemented our online Natural Language Processing (NLP) course following constructive alignment and adhering to the pedagogical principles of LTU. By examining our course and analyzing student evaluation forms, we show that we have met our goal and successfully delivered the course. Furthermore, we discuss the additional benefits resulting from the current mode of delivery, including the increased reusability of course content and increased potential for collaboration between universities. Lastly, we also discuss where we can and will further improve the current course design",
    "checked": true,
    "id": "21c3ed3c66b1b43c413a26d74128c86f6f37e298",
    "semantic_title": "pedagogical principles in the online teaching of text mining: a retrospection",
    "citation_count": 1,
    "authors": [
      "Rajkumar Saini",
      "György Kovács",
      "Mohamadreza Faridghasemnia",
      "Hamam Mokayed",
      "Oluwatosin Adewumi",
      "Pedro Alonso",
      "Sumit Rakesh",
      "Marcus Liwicki"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.2": {
    "title": "Teaching a Massive Open Online Course on Natural Language Processing",
    "volume": "workshop",
    "abstract": "In this paper we present a new Massive Open Online Course on Natural Language Processing, targeted at non-English speaking students. The course lasts 12 weeks, every week consists of lectures, practical sessions and quiz assigments. Three weeks out of 12 are followed by Kaggle-style coding assigments. Our course intents to serve multiple purposes: (i) familirize students with the core concepts and methods in NLP, such as language modelling or word or sentence representations, (ii) show that recent advances, including pre-trained Transformer-based models, are build upon these concepts; (iii) to introduce architectures for most most demanded real-life applications, (iii) to develop practical skills to process texts in multiple languages. The course was prepared and recorded during 2020 and so far have received positive feedback",
    "checked": true,
    "id": "4799803d1ae2b2b1f19942afa2b05d4b622af7b9",
    "semantic_title": "teaching a massive open online course on natural language processing",
    "citation_count": 1,
    "authors": [
      "Ekaterina Artemova",
      "Murat Apishev",
      "Denis Kirianov",
      "Veronica Sarkisyan",
      "Sergey Aksenov",
      "Oleg Serikov"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.3": {
    "title": "Natural Language Processing 4 All (NLP4All): A New Online Platform for Teaching and Learning NLP Concepts",
    "volume": "workshop",
    "abstract": "Natural Language Processing offers new insights into language data across almost all disciplines and domains, and allows us to corroborate and/or challenge existing knowledge. The primary hurdles to widening participation in and use of these new research tools are, first, a lack of coding skills in students across K-16, and in the population at large, and second, a lack of knowledge of how NLP-methods can be used to answer questions of disciplinary interest outside of linguistics and/or computer science. To broaden participation in NLP and improve NLP-literacy, we introduced a new tool web-based tool called Natural Language Processing 4 All (NLP4All). The intended purpose of NLP4All is to help teachers facilitate learning with and about NLP, by providing easy-to-use interfaces to NLP-methods, data, and analyses, making it possible for non- and novice-programmers to learn NLP concepts interactively",
    "checked": true,
    "id": "05db3e3527e45929c61d1ed72c8e54bfbfa8988a",
    "semantic_title": "natural language processing 4 all (nlp4all): a new online platform for teaching and learning nlp concepts",
    "citation_count": 2,
    "authors": [
      "Rebekah Baglini",
      "Hermes Hjorth"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.4": {
    "title": "A New Broad NLP Training from Speech to Knowledge",
    "volume": "workshop",
    "abstract": "In 2018, the Master Sc. in NLP opened at IDMC - Institut des Sciences du Digital, du Management et de la Cognition, Université de Lorraine - Nancy, France. Far from being a creation ex-nihilo, it is the product of a history and many reflections on the field and its teaching. This article proposes epistemological and critical elements on the opening and maintainance of this so far new master's program in NLP",
    "checked": true,
    "id": "df852fd548c64d495e5383c3195e2a1ae5b794d6",
    "semantic_title": "a new broad nlp training from speech to knowledge",
    "citation_count": 0,
    "authors": [
      "Maxime Amblard",
      "Miguel Couceiro"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.5": {
    "title": "Applied Language Technology: NLP for the Humanities",
    "volume": "workshop",
    "abstract": "This contribution describes a two-course module that seeks to provide humanities majors with a basic understanding of language technology and its applications using Python. The learning materials consist of interactive Jupyter Notebooks and accompanying YouTube videos, which are openly available with a Creative Commons licence",
    "checked": true,
    "id": "4d1a643d16c67f7fe7bfb12d814a718764f357ac",
    "semantic_title": "applied language technology: nlp for the humanities",
    "citation_count": 4,
    "authors": [
      "Tuomo Hiippala"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.6": {
    "title": "A Crash Course on Ethics for Natural Language Processing",
    "volume": "workshop",
    "abstract": "It is generally agreed upon in the natural language processing (NLP) community that ethics should be integrated into any curriculum. Being aware of and understanding the relevant core concepts is a prerequisite for following and participating in the discourse on ethical NLP. We here present ready-made teaching material in the form of slides and practical exercises on ethical issues in NLP, which is primarily intended to be integrated into introductory NLP or computational linguistics courses. By making this material freely available, we aim at lowering the threshold to adding ethics to the curriculum. We hope that increased awareness will enable students to identify potentially unethical behavior",
    "checked": true,
    "id": "e914eac062e46e985352aa385a0ecdcb5cf9db6f",
    "semantic_title": "a crash course on ethics for natural language processing",
    "citation_count": 1,
    "authors": [
      "Annemarie Friedrich",
      "Torsten Zesch"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.7": {
    "title": "A dissemination workshop for introducing young Italian students to NLP",
    "volume": "workshop",
    "abstract": "We describe and make available the game-based material developed for a laboratory run at several Italian science festivals to popularize NLP among young students",
    "checked": true,
    "id": "61f7bce4bcac53f0ddbafeb89e5511a5abc039dd",
    "semantic_title": "a dissemination workshop for introducing young italian students to nlp",
    "citation_count": 2,
    "authors": [
      "Lucio Messina",
      "Lucia Busso",
      "Claudia Roberta Combei",
      "Alessio Miaschi",
      "Ludovica Pannitto",
      "Gabriele Sarti",
      "Malvina Nissim"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.8": {
    "title": "MiniVQA - A resource to build your tailored VQA competition",
    "volume": "workshop",
    "abstract": "MiniVQA is a Jupyter notebook to build a tailored VQA competition for your students. The resource creates all the needed resources to create a classroom competition that engages and inspires your students on the free, self-service Kaggle platform. \"InClass competitions make machine learning fun¡‘",
    "checked": true,
    "id": "e9881656e5ac53b63f452f0ef885b22bc117c4ca",
    "semantic_title": "minivqa - a resource to build your tailored vqa competition",
    "citation_count": 0,
    "authors": [
      "Jean-Benoit Delbrouck"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.9": {
    "title": "From back to the roots into the gated woods: Deep learning for NLP",
    "volume": "workshop",
    "abstract": "Deep neural networks have revolutionized many fields, including Natural Language Processing. This paper outlines teaching materials for an introductory lecture on deep learning in Natural Language Processing (NLP). The main submitted material covers a summer school lecture on encoder-decoder models. Complementary to this is a set of jupyter notebook slides from earlier teaching, on which parts of the lecture were based on. The main goal of this teaching material is to provide an overview of neural network approaches to natural language processing, while linking modern concepts back to the roots showing traditional essential counterparts. The lecture departs from count-based statistical methods and spans up to gated recurrent networks and attention, which is ubiquitous in today's NLP",
    "checked": true,
    "id": "ae1310abdf4601de38244f078596d71608972ab4",
    "semantic_title": "from back to the roots into the gated woods: deep learning for nlp",
    "citation_count": 0,
    "authors": [
      "Barbara Plank"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.10": {
    "title": "Learning PyTorch Through A Neural Dependency Parsing Exercise",
    "volume": "workshop",
    "abstract": "Dependency parsing is increasingly the popular parsing formalism in practice. This assignment provides a practice exercise in implementing the shift-reduce dependency parser of Chen and Manning (2014). This parser is a two-layer feed-forward neural network, which students implement in PyTorch, providing practice in developing deep learning models and exposure to developing parser models",
    "checked": true,
    "id": "5a1b874b20ae84fa56996d006f8c3798a8d1b940",
    "semantic_title": "learning pytorch through a neural dependency parsing exercise",
    "citation_count": 0,
    "authors": [
      "David Jurgens"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.11": {
    "title": "A Balanced and Broadly Targeted Computational Linguistics Curriculum",
    "volume": "workshop",
    "abstract": "This paper describes the primarily-graduate computational linguistics and NLP curriculum at Georgetown University, a U.S. university that has seen significant growth in these areas in recent years. We reflect on the principles behind our curriculum choices, including recognizing the various academic backgrounds and goals of our students; teaching a variety of skills with an emphasis on working directly with data; encouraging collaboration and interdisciplinary work; and including languages beyond English. We reflect on challenges we have encountered, such as the difficulty of teaching programming skills alongside NLP fundamentals, and discuss areas for future growth",
    "checked": true,
    "id": "b9a4cb84f30ebadfaa08cc58f20641ec04f9c196",
    "semantic_title": "a balanced and broadly targeted computational linguistics curriculum",
    "citation_count": 0,
    "authors": [
      "Emma Manning",
      "Nathan Schneider",
      "Amir Zeldes"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.12": {
    "title": "Gaining Experience with Structured Data: Using the Resources of Dialog State Tracking Challenge 2",
    "volume": "workshop",
    "abstract": "This paper describes a class project for a recently introduced undergraduate NLP course that gives computer science students the opportunity to explore the data of Dialog State Tracking Challenge 2 (DSTC 2). Student background, curriculum choices, and project details are discussed. The paper concludes with some instructor advice and final reflections",
    "checked": true,
    "id": "e02ae6f390643a07bd582b6ae39c648841f125b4",
    "semantic_title": "gaining experience with structured data: using the resources of dialog state tracking challenge 2",
    "citation_count": 0,
    "authors": [
      "Ronnie Smith"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.13": {
    "title": "The Flipped Classroom model for teaching Conditional Random Fields in an NLP course",
    "volume": "workshop",
    "abstract": "In this article, we show and discuss our experience in applying the flipped classroom method for teaching Conditional Random Fields in a Natural Language Processing course. We present the activities that we developed together with their relationship to a cognitive complexity model (Bloom's taxonomy). After this, we provide our own reflections and expectations of the model itself. Based on the evaluation got from students, it seems that students learn about the topic and also that the method is rewarding for some students. Additionally, we discuss some shortcomings and we propose possible solutions to them. We conclude the paper with some possible future work",
    "checked": true,
    "id": "748b2c11f7c8be1d23a2f0ff24e8b778728f83db",
    "semantic_title": "the flipped classroom model for teaching conditional random fields in an nlp course",
    "citation_count": 0,
    "authors": [
      "Manex Agirrezabal"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.14": {
    "title": "Flamingos and Hedgehogs in the Croquet-Ground: Teaching Evaluation of NLP Systems for Undergraduate Students",
    "volume": "workshop",
    "abstract": "This report describes the course Evaluation of NLP Systems, taught for Computational Linguistics undergraduate students during the winter semester 20/21 at the University of Potsdam, Germany. It was a discussion-based seminar that covered different aspects of evaluation in NLP, namely paradigms, common procedures, data annotation, metrics and measurements, statistical significance testing, best practices and common approaches in specific NLP tasks and applications",
    "checked": true,
    "id": "097012648ff7b91597d4865f31c5fbf1dcc239dd",
    "semantic_title": "flamingos and hedgehogs in the croquet-ground: teaching evaluation of nlp systems for undergraduate students",
    "citation_count": 2,
    "authors": [
      "Brielen Madureira"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.15": {
    "title": "An Immersive Computational Text Analysis Course for Non-Computer Science Students at Barnard College",
    "volume": "workshop",
    "abstract": "We provide an overview of a new Computational Text Analysis course that will be taught at Barnard College over a six week period in May and June 2021. The course is targeted to non Computer Science at a Liberal Arts college that wish to incorporate fundamental Natural Language Processing tools in their re- search and studies. During the course, students will complete daily programming tutorials, read and review contemporary research papers, and propose and develop independent research projects",
    "checked": true,
    "id": "63b45a12e53768ec29be8eea0640e2b7cb808424",
    "semantic_title": "an immersive computational text analysis course for non-computer science students at barnard college",
    "citation_count": 0,
    "authors": [
      "Adam Poliak",
      "Jalisha Jenifer"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.16": {
    "title": "Introducing Information Retrieval for Biomedical Informatics Students",
    "volume": "workshop",
    "abstract": "Introducing biomedical informatics (BMI) students to natural language processing (NLP) requires balancing technical depth with practical know-how to address application-focused needs. We developed a set of three activities introducing introductory BMI students to information retrieval with NLP, covering document representation strategies and language models from TF-IDF to BERT. These activities provide students with hands-on experience targeted towards common use cases, and introduce fundamental components of NLP workflows for a wide variety of applications",
    "checked": true,
    "id": "85e1424c25fe975303b2e6933a5ed4f7a549cada",
    "semantic_title": "introducing information retrieval for biomedical informatics students",
    "citation_count": 0,
    "authors": [
      "Sanya Taneja",
      "Richard Boyce",
      "William Reynolds",
      "Denis Newman-Griffis"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.17": {
    "title": "Contemporary NLP Modeling in Six Comprehensive Programming Assignments",
    "volume": "workshop",
    "abstract": "We present a series of programming assignments, adaptable to a range of experience levels from advanced undergraduate to PhD, to teach students design and implementation of modern NLP systems. These assignments build from the ground up and emphasize full-stack understanding of machine learning models: initially, students implement inference and gradient computation by hand, then use PyTorch to build nearly state-of-the-art neural networks using current best practices. Topics are chosen to cover a wide range of modeling and inference techniques that one might encounter, ranging from linear models suitable for industry applications to state-of-the-art deep learning models used in NLP research. The assignments are customizable, with constrained options to guide less experienced students or open-ended options giving advanced students freedom to explore. All of them can be deployed in a fully autogradable fashion, and have collectively been tested on over 300 students across several semesters",
    "checked": true,
    "id": "ddd213ecbc9ba61dd1907f493c050d8d5593dbc7",
    "semantic_title": "contemporary nlp modeling in six comprehensive programming assignments",
    "citation_count": 0,
    "authors": [
      "Greg Durrett",
      "Jifan Chen",
      "Shrey Desai",
      "Tanya Goyal",
      "Lucas Kabela",
      "Yasumasa Onoe",
      "Jiacheng Xu"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.18": {
    "title": "Interactive Assignments for Teaching Structured Neural NLP",
    "volume": "workshop",
    "abstract": "We present a set of assignments for a graduate-level NLP course. Assignments are designed to be interactive, easily gradable, and to give students hands-on experience with several key types of structure (sequences, tags, parse trees, and logical forms), modern neural architectures (LSTMs and Transformers), inference algorithms (dynamic programs and approximate search) and training methods (full and weak supervision). We designed assignments to build incrementally both within each assignment and across assignments, with the goal of enabling students to undertake graduate-level research in NLP by the end of the course",
    "checked": true,
    "id": "041b2510e54b2504890cb9f58b9bbc5601f35e3e",
    "semantic_title": "interactive assignments for teaching structured neural nlp",
    "citation_count": 1,
    "authors": [
      "David Gaddy",
      "Daniel Fried",
      "Nikita Kitaev",
      "Mitchell Stern",
      "Rodolfo Corona",
      "John DeNero",
      "Dan Klein"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.19": {
    "title": "Learning about Word Vector Representations and Deep Learning through Implementing Word2vec",
    "volume": "workshop",
    "abstract": "Word vector representations are an essential part of an NLP curriculum. Here, we describe a homework that has students implement a popular method for learning word vectors, word2vec. Students implement the core parts of the method, including text preprocessing, negative sampling, and gradient descent. Starter code provides guidance and handles basic operations, which allows students to focus on the conceptually challenging aspects. After generating their vectors, students evaluate them using qualitative and quantitative tests",
    "checked": true,
    "id": "3de9bb15efa2b8f20c0f6b6ee779f2b668f3e0c2",
    "semantic_title": "learning about word vector representations and deep learning through implementing word2vec",
    "citation_count": 2,
    "authors": [
      "David Jurgens"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.20": {
    "title": "Naive Bayes versus BERT: Jupyter notebook assignments for an introductory NLP course",
    "volume": "workshop",
    "abstract": "We describe two Jupyter notebooks that form the basis of two assignments in an introductory Natural Language Processing (NLP) module taught to final year undergraduate students at Dublin City University. The notebooks show the students how to train a bag-of-words polarity classifier using multinomial Naive Bayes, and how to fine-tune a polarity classifier using BERT. The students take the code as a starting point for their own experiments",
    "checked": true,
    "id": "59516317394378b60b8f3d1858bbd85ca609de64",
    "semantic_title": "naive bayes versus bert: jupyter notebook assignments for an introductory nlp course",
    "citation_count": 1,
    "authors": [
      "Jennifer Foster",
      "Joachim Wagner"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.21": {
    "title": "Natural Language Processing for Computer Scientists and Data Scientists at a Large State University",
    "volume": "workshop",
    "abstract": "The field of Natural Language Processing (NLP) changes rapidly, requiring course offerings to adjust with those changes, and NLP is not just for computer scientists; it's a field that should be accessible to anyone who has a sufficient background. In this paper, I explain how students with Computer Science and Data Science backgrounds can be well-prepared for an upper-division NLP course at a large state university. The course covers probability and information theory, elementary linguistics, machine and deep learning, with an attempt to balance theoretical ideas and concepts with practical applications. I explain the course objectives, topics and assignments, reflect on adjustments to the course over the last four years, as well as feedback from students",
    "checked": true,
    "id": "265dae041d068c902cbada43fd528046abde5402",
    "semantic_title": "natural language processing for computer scientists and data scientists at a large state university",
    "citation_count": 1,
    "authors": [
      "Casey Kennington"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.22": {
    "title": "On Writing a Textbook on Natural Language Processing",
    "volume": "workshop",
    "abstract": "There are thousands of papers about natural language processing and computational linguistics, but very few textbooks. I describe the motivation and process for writing a college textbook on natural language processing, and offer advice and encouragement for readers who may be interested in writing a textbook of their own",
    "checked": true,
    "id": "4554fa00d273dcefd76405bb380274be89cbcebf",
    "semantic_title": "on writing a textbook on natural language processing",
    "citation_count": 0,
    "authors": [
      "Jacob Eisenstein"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.23": {
    "title": "Learning How To Learn NLP: Developing Introductory Concepts Through Scaffolded Discovery",
    "volume": "workshop",
    "abstract": "We present a scaffolded discovery learning approach to introducing concepts in a Natural Language Processing course aimed at computer science students at liberal arts institutions. We describe some of the objectives of this approach, as well as presenting specific ways that four of our discovery-based assignments combine specific natural language processing concepts with broader analytic skills. We argue this approach helps prepare students for many possible future paths involving both application and innovation of NLP technology by emphasizing experimental data navigation, experiment design, and awareness of the complexities and challenges of analysis",
    "checked": true,
    "id": "fceeada35977c7ec296df7ed71d8eddb107362b0",
    "semantic_title": "learning how to learn nlp: developing introductory concepts through scaffolded discovery",
    "citation_count": 0,
    "authors": [
      "Alexandra Schofield",
      "Richard Wicentowski",
      "Julie Medero"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.24": {
    "title": "The Online Pivot: Lessons Learned from Teaching a Text and Data Mining Course in Lockdown, Enhancing online Teaching with Pair Programming and Digital Badges",
    "volume": "workshop",
    "abstract": "In this paper we provide an account of how we ported a text and data mining course online in summer 2020 as a result of the COVID-19 pandemic and how we improved it in a second pilot run. We describe the course, how we adapted it over the two pilot runs and what teaching techniques we used to improve students' learning and community building online. We also provide information on the relentless feedback collected during the course which helped us to adapt our teaching from one session to the next and one pilot to the next. We discuss the lessons learned and promote the use of innovative teaching techniques applied to the digital such as digital badges and pair programming in break-out rooms for teaching Natural Language Processing courses to beginners and students with different backgrounds",
    "checked": true,
    "id": "ce59e68320848d9358b7574a0c9918756d88db17",
    "semantic_title": "the online pivot: lessons learned from teaching a text and data mining course in lockdown, enhancing online teaching with pair programming and digital badges",
    "citation_count": 0,
    "authors": [
      "Beatrice Alex",
      "Clare Llewellyn",
      "Pawel Orzechowski",
      "Maria Boutchkova"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.25": {
    "title": "Teaching NLP outside Linguistics and Computer Science classrooms: Some challenges and some opportunities",
    "volume": "workshop",
    "abstract": "NLP's sphere of influence went much beyond computer science research and the development of software applications in the past decade. We see people using NLP methods in a range of academic disciplines from Asian Studies to Clinical Oncology. We also notice the presence of NLP as a module in most of the data science curricula within and outside of regular university setups. These courses are taken by students from very diverse backgrounds. This paper takes a closer look at some issues related to teaching NLP to these diverse audiences based on my classroom experiences, and identifies some challenges the instructors face, particularly when there is no ecosystem of related courses for the students. In this process, it also identifies a few challenge areas for both NLP researchers and tool developers",
    "checked": true,
    "id": "5ea31377327015f7f8c0f2da7e8881463a3909a7",
    "semantic_title": "teaching nlp outside linguistics and computer science classrooms: some challenges and some opportunities",
    "citation_count": 1,
    "authors": [
      "Sowmya Vajjala"
    ]
  },
  "https://aclanthology.org/2021.teachingnlp-1.26": {
    "title": "Teaching NLP with Bracelets and Restaurant Menus: An Interactive Workshop for Italian Students",
    "volume": "workshop",
    "abstract": "Although Natural Language Processing is at the core of many tools young people use in their everyday life, high school curricula (in Italy) do not include any computational linguistics education. This lack of exposure makes the use of such tools less responsible than it could be, and makes choosing computational linguistics as a university degree unlikely. To raise awareness, curiosity, and longer-term interest in young people, we have developed an interactive workshop designed to illustrate the basic principles of NLP and computational linguistics to high school Italian students aged between 13 and 18 years. The workshop takes the form of a game in which participants play the role of machines needing to solve some of the most common problems a computer faces in understanding language: from voice recognition to Markov chains to syntactic parsing. Participants are guided through the workshop with the help of instructors, who present the activities and explain core concepts from computational linguistics. The workshop was presented at numerous outlets in Italy between 2019 and 2020, both face-to-face and online",
    "checked": true,
    "id": "a48acf5b02fd8ef6fe8d0185e0128248c99d7ef0",
    "semantic_title": "teaching nlp with bracelets and restaurant menus: an interactive workshop for italian students",
    "citation_count": 3,
    "authors": [
      "Ludovica Pannitto",
      "Lucia Busso",
      "Claudia Roberta Combei",
      "Lucio Messina",
      "Alessio Miaschi",
      "Gabriele Sarti",
      "Malvina Nissim"
    ]
  },
  "https://aclanthology.org/2021.textgraphs-1.1": {
    "title": "Bootstrapping Large-Scale Fine-Grained Contextual Advertising Classifier from Wikipedia",
    "volume": "workshop",
    "abstract": "Contextual advertising provides advertisers with the opportunity to target the context which is most relevant to their ads. The large variety of potential topics makes it very challenging to collect training documents to build a supervised classification model or compose expert-written rules in a rule-based classification system. Besides, in fine-grained classification, different categories often overlap or co-occur, making it harder to classify accurately. In this work, we propose wiki2cat, a method to tackle large-scaled fine-grained text classification by tapping on the Wikipedia category graph. The categories in the IAB taxonomy are first mapped to category nodes in the graph. Then the label is propagated across the graph to obtain a list of labeled Wikipedia documents to induce text classifiers. The method is ideal for large-scale classification problems since it does not require any manually-labeled document or hand-curated rules or keywords. The proposed method is benchmarked with various learning-based and keyword-based baselines and yields competitive performance on publicly available datasets and a new dataset containing more than 300 fine-grained categories",
    "checked": true,
    "id": "4589df25ebfded4a05d3b0a697362fd1da3a4fd1",
    "semantic_title": "bootstrapping large-scale fine-grained contextual advertising classifier from wikipedia",
    "citation_count": 4,
    "authors": [
      "Yiping Jin",
      "Vishakha Kadam",
      "Dittaya Wanvarie"
    ]
  },
  "https://aclanthology.org/2021.textgraphs-1.2": {
    "title": "Modeling Graph Structure via Relative Position for Text Generation from Knowledge Graphs",
    "volume": "workshop",
    "abstract": "We present Graformer, a novel Transformer-based encoder-decoder architecture for graph-to-text generation. With our novel graph self-attention, the encoding of a node relies on all nodes in the input graph - not only direct neighbors - facilitating the detection of global patterns. We represent the relation between two nodes as the length of the shortest path between them. Graformer learns to weight these node-node relations differently for different attention heads, thus virtually learning differently connected views of the input graph. We evaluate Graformer on two popular graph-to-text generation benchmarks, AGENDA and WebNLG, where it achieves strong performance while using many fewer parameters than other approaches",
    "checked": true,
    "id": "a83902f8b3aadfda633968a840ca1738bedef837",
    "semantic_title": "modeling graph structure via relative position for text generation from knowledge graphs",
    "citation_count": 23,
    "authors": [
      "Martin Schmitt",
      "Leonardo F. R. Ribeiro",
      "Philipp Dufter",
      "Iryna Gurevych",
      "Hinrich Schütze"
    ]
  },
  "https://aclanthology.org/2021.textgraphs-1.3": {
    "title": "Entity Prediction in Knowledge Graphs with Joint Embeddings",
    "volume": "workshop",
    "abstract": "Knowledge Graphs (KGs) have become increasingly popular in the recent years. However, as knowledge constantly grows and changes, it is inevitable to extend existing KGs with entities that emerged or became relevant to the scope of the KG after its creation. Research on updating KGs typically relies on extracting named entities and relations from text. However, these approaches cannot infer entities or relations that were not explicitly stated. Alternatively, embedding models exploit implicit structural regularities to predict missing relations, but cannot predict missing entities. In this article, we introduce a novel method to enrich a KG with new entities given their textual description. Our method leverages joint embedding models, hence does not require entities or relations to be named explicitly. We show that our approach can identify new concepts in a document corpus and transfer them into the KG, and we find that the performance of our method improves substantially when extended with techniques from association rule mining, text mining, and active learning",
    "checked": true,
    "id": "84a4a2ef9b2a3e558179992dab28c89d8ac61d7e",
    "semantic_title": "entity prediction in knowledge graphs with joint embeddings",
    "citation_count": 3,
    "authors": [
      "Matthias Baumgartner",
      "Daniele Dell’Aglio",
      "Abraham Bernstein"
    ]
  },
  "https://aclanthology.org/2021.textgraphs-1.4": {
    "title": "Hierarchical Graph Convolutional Networks for Jointly Resolving Cross-document Coreference of Entity and Event Mentions",
    "volume": "workshop",
    "abstract": "This paper studies the problem of cross-document event coreference resolution (CDECR) that seeks to determine if event mentions across multiple documents refer to the same real-world events. Prior work has demonstrated the benefits of the predicate-argument information and document context for resolving the coreference of event mentions. However, such information has not been captured effectively in prior work for CDECR. To address these limitations, we propose a novel deep learning model for CDECR that introduces hierarchical graph convolutional neural networks (GCN) to jointly resolve entity and event mentions. As such, sentence-level GCNs enable the encoding of important context words for event mentions and their arguments while the document-level GCN leverages the interaction structures of event mentions and arguments to compute document representations to perform CDECR. Extensive experiments are conducted to demonstrate the effectiveness of the proposed model",
    "checked": true,
    "id": "8fe5193468ef15b6f84c705d98c4966c61a7c0a8",
    "semantic_title": "hierarchical graph convolutional networks for jointly resolving cross-document coreference of entity and event mentions",
    "citation_count": 8,
    "authors": [
      "Duy Phung",
      "Tuan Ngo Nguyen",
      "Thien Huu Nguyen"
    ]
  },
  "https://aclanthology.org/2021.textgraphs-1.5": {
    "title": "GENE: Global Event Network Embedding",
    "volume": "workshop",
    "abstract": "Current methods for event representation ignore related events in a corpus-level global context. For a deep and comprehensive understanding of complex events, we introduce a new task, Event Network Embedding, which aims to represent events by capturing the connections among events. We propose a novel framework, Global Event Network Embedding (GENE), that encodes the event network with a multi-view graph encoder while preserving the graph topology and node semantics. The graph encoder is trained by minimizing both structural and semantic losses. We develop a new series of structured probing tasks, and show that our approach effectively outperforms baseline models on node typing, argument role classification, and event coreference resolution",
    "checked": true,
    "id": "7870a1f7641c65deef097400f749d15fee5bf26f",
    "semantic_title": "gene: global event network embedding",
    "citation_count": 9,
    "authors": [
      "Qi Zeng",
      "Manling Li",
      "Tuan Lai",
      "Heng Ji",
      "Mohit Bansal",
      "Hanghang Tong"
    ]
  },
  "https://aclanthology.org/2021.textgraphs-1.6": {
    "title": "Learning Clause Representation from Dependency-Anchor Graph for Connective Prediction",
    "volume": "workshop",
    "abstract": "Semantic representation that supports the choice of an appropriate connective between pairs of clauses inherently addresses discourse coherence, which is important for tasks such as narrative understanding, argumentation, and discourse parsing. We propose a novel clause embedding method that applies graph learning to a data structure we refer to as a dependency-anchor graph. The dependency anchor graph incorporates two kinds of syntactic information, constituency structure, and dependency relations, to highlight the subject and verb phrase relation. This enhances coherence-related aspects of representation. We design a neural model to learn a semantic representation for clauses from graph convolution over latent representations of the subject and verb phrase. We evaluate our method on two new datasets: a subset of a large corpus where the source texts are published novels, and a new dataset collected from students' essays. The results demonstrate a significant improvement over tree-based models, confirming the importance of emphasizing the subject and verb phrase. The performance gap between the two datasets illustrates the challenges of analyzing student's written text, plus a potential evaluation task for coherence modeling and an application for suggesting revisions to students",
    "checked": true,
    "id": "2eff03a12c65fe799f568c98113f432b2f1a2c7d",
    "semantic_title": "learning clause representation from dependency-anchor graph for connective prediction",
    "citation_count": 1,
    "authors": [
      "Yanjun Gao",
      "Ting-Hao Huang",
      "Rebecca J. Passonneau"
    ]
  },
  "https://aclanthology.org/2021.textgraphs-1.7": {
    "title": "WikiGraphs: A Wikipedia Text - Knowledge Graph Paired Dataset",
    "volume": "workshop",
    "abstract": "We present a new dataset of Wikipedia articles each paired with a knowledge graph, to facilitate the research in conditional text generation, graph generation and graph representation learning. Existing graph-text paired datasets typically contain small graphs and short text (1 or few sentences), thus limiting the capabilities of the models that can be learned on the data. Our new dataset WikiGraphs is collected by pairing each Wikipedia article from the established WikiText-103 benchmark (Merity et al., 2016) with a subgraph from the Freebase knowledge graph (Bollacker et al., 2008). This makes it easy to benchmark against other state-of-the-art text generative models that are capable of generating long paragraphs of coherent text. Both the graphs and the text data are of significantly larger scale compared to prior graph-text paired datasets. We present baseline graph neural network and transformer model results on our dataset for 3 tasks: graph -> text generation, graph -> text retrieval and text -> graph retrieval. We show that better conditioning on the graph provides gains in generation and retrieval quality but there is still large room for improvement",
    "checked": true,
    "id": "56765a17295370c70cbc510329c082ec60ed3c62",
    "semantic_title": "wikigraphs: a wikipedia text - knowledge graph paired dataset",
    "citation_count": 16,
    "authors": [
      "Luyu Wang",
      "Yujia Li",
      "Ozlem Aslan",
      "Oriol Vinyals"
    ]
  },
  "https://aclanthology.org/2021.textgraphs-1.8": {
    "title": "Selective Attention Based Graph Convolutional Networks for Aspect-Level Sentiment Classification",
    "volume": "workshop",
    "abstract": "Recent work on aspect-level sentiment classification has employed Graph Convolutional Networks (GCN) over dependency trees to learn interactions between aspect terms and opinion words. In some cases, the corresponding opinion words for an aspect term cannot be reached within two hops on dependency trees, which requires more GCN layers to model. However, GCNs often achieve the best performance with two layers, and deeper GCNs do not bring any additional gain. Therefore, we design a novel selective attention based GCN model. On one hand, the proposed model enables the direct interaction between aspect terms and context words via the self-attention operation without the distance limitation on dependency trees. On the other hand, a top-k selection procedure is designed to locate opinion words by selecting k context words with the highest attention scores. We conduct experiments on several commonly used benchmark datasets and the results show that our proposed SA-GCN outperforms strong baseline models",
    "checked": true,
    "id": "b8e95f387548349d91693ae049750be83bc4f53d",
    "semantic_title": "selective attention based graph convolutional networks for aspect-level sentiment classification",
    "citation_count": 44,
    "authors": [
      "Xiaochen Hou",
      "Jing Huang",
      "Guangtao Wang",
      "Peng Qi",
      "Xiaodong He",
      "Bowen Zhou"
    ]
  },
  "https://aclanthology.org/2021.textgraphs-1.9": {
    "title": "Keyword Extraction Using Unsupervised Learning on the Document's Adjacency Matrix",
    "volume": "workshop",
    "abstract": "This work revisits the information given by the graph-of-words and its typical utilization through graph-based ranking approaches in the context of keyword extraction. Recent, well-known graph-based approaches typically employ the knowledge from word vector representations during the ranking process via popular centrality measures (e.g., PageRank) without giving the primary role to vectors' distribution. We consider the adjacency matrix that corresponds to the graph-of-words of a target text document as the vector representation of its vocabulary. We propose the distribution-based modeling of this adjacency matrix using unsupervised (learning) algorithms. The efficacy of the distribution-based modeling approaches compared to state-of-the-art graph-based methods is confirmed by an extensive experimental study according to the F1 score. Our code is available on GitHub",
    "checked": true,
    "id": "88b68623dd161b22af65ff52577982d791e66113",
    "semantic_title": "keyword extraction using unsupervised learning on the document's adjacency matrix",
    "citation_count": 7,
    "authors": [
      "Eirini Papagiannopoulou",
      "Grigorios Tsoumakas",
      "Apostolos Papadopoulos"
    ]
  },
  "https://aclanthology.org/2021.textgraphs-1.10": {
    "title": "Improving Human Text Simplification with Sentence Fusion",
    "volume": "workshop",
    "abstract": "The quality of fully automated text simplification systems is not good enough for use in real-world settings; instead, human simplifications are used. In this paper, we examine how to improve the cost and quality of human simplifications by leveraging crowdsourcing. We introduce a graph-based sentence fusion approach to augment human simplifications and a reranking approach to both select high quality simplifications and to allow for targeting simplifications with varying levels of simplicity. Using the Newsela dataset (Xu et al., 2015) we show consistent improvements over experts at varying simplification levels and find that the additional sentence fusion simplifications allow for simpler output than the human simplifications alone",
    "checked": true,
    "id": "9347a3a9eecfbf07828007b266a8b3e3bacd6531",
    "semantic_title": "improving human text simplification with sentence fusion",
    "citation_count": 2,
    "authors": [
      "Max Schwarzer",
      "Teerapaun Tanprasert",
      "David Kauchak"
    ]
  },
  "https://aclanthology.org/2021.textgraphs-1.11": {
    "title": "Structural Realization with GGNNs",
    "volume": "workshop",
    "abstract": "In this paper, we define an abstract task called structural realization that generates words given a prefix of words and a partial representation of a parse tree. We also present a method for solving instances of this task using a Gated Graph Neural Network (GGNN). We evaluate it with standard accuracy measures, as well as with respect to perplexity, in which its comparison to previous work on language modelling serves to quantify the information added to a lexical selection task by the presence of syntactic knowledge. That the addition of parse-tree-internal nodes to this neural model should improve the model, with respect both to accuracy and to more conventional measures such as perplexity, may seem unsurprising, but previous attempts have not met with nearly as much success. We have also learned that transverse links through the parse tree compromise the model's accuracy at generating adjectival and nominal parts of speech",
    "checked": true,
    "id": "c31b43ea3eb77138710e575760ba0dc08cd8baf9",
    "semantic_title": "structural realization with ggnns",
    "citation_count": 0,
    "authors": [
      "Jinman Zhao",
      "Gerald Penn",
      "Huan Ling"
    ]
  },
  "https://aclanthology.org/2021.textgraphs-1.12": {
    "title": "MG-BERT: Multi-Graph Augmented BERT for Masked Language Modeling",
    "volume": "workshop",
    "abstract": "Pre-trained models like Bidirectional Encoder Representations from Transformers (BERT), have recently made a big leap forward in Natural Language Processing (NLP) tasks. However, there are still some shortcomings in the Masked Language Modeling (MLM) task performed by these models. In this paper, we first introduce a multi-graph including different types of relations between words. Then, we propose Multi-Graph augmented BERT (MG-BERT) model that is based on BERT. MG-BERT embeds tokens while taking advantage of a static multi-graph containing global word co-occurrences in the text corpus beside global real-world facts about words in knowledge graphs. The proposed model also employs a dynamic sentence graph to capture local context effectively. Experimental results demonstrate that our model can considerably enhance the performance in the MLM task",
    "checked": true,
    "id": "b9464b492f6638035d25b42f32ff3d51cb6d1e30",
    "semantic_title": "mg-bert: multi-graph augmented bert for masked language modeling",
    "citation_count": 2,
    "authors": [
      "Parishad BehnamGhader",
      "Hossein Zakerinia",
      "Mahdieh Soleymani Baghshah"
    ]
  },
  "https://aclanthology.org/2021.textgraphs-1.13": {
    "title": "GTN-ED: Event Detection Using Graph Transformer Networks",
    "volume": "workshop",
    "abstract": "Recent works show that the graph structure of sentences, generated from dependency parsers, has potential for improving event detection. However, they often only leverage the edges (dependencies) between words, and discard the dependency labels (e.g., nominal-subject), treating the underlying graph edges as homogeneous. In this work, we propose a novel framework for incorporating both dependencies and their labels using a recently proposed technique called Graph Transformer Network (GTN). We integrate GTN to leverage dependency relations on two existing homogeneous-graph-based models and demonstrate an improvement in the F1 score on the ACE dataset",
    "checked": true,
    "id": "dd21645a340c1af98c3b64ae76da15b2cac51569",
    "semantic_title": "gtn-ed: event detection using graph transformer networks",
    "citation_count": 7,
    "authors": [
      "Sanghamitra Dutta",
      "Liang Ma",
      "Tanay Kumar Saha",
      "Di Liu",
      "Joel Tetreault",
      "Alejandro Jaimes"
    ]
  },
  "https://aclanthology.org/2021.textgraphs-1.14": {
    "title": "Fine-grained General Entity Typing in German using GermaNet",
    "volume": "workshop",
    "abstract": "Fine-grained entity typing is important to tasks like relation extraction and knowledge base construction. We find however, that fine-grained entity typing systems perform poorly on general entities (e.g. \"ex-president\") as compared to named entities (e.g. \"Barack Obama\"). This is due to a lack of general entities in existing training data sets. We show that this problem can be mitigated by automatically generating training data from WordNets. We use a German WordNet equivalent, GermaNet, to automatically generate training data for German general entity typing. We use this data to supplement named entity data to train a neural fine-grained entity typing system. This leads to a 10% improvement in accuracy of the prediction of level 1 FIGER types for German general entities, while decreasing named entity type prediction accuracy by only 1%",
    "checked": true,
    "id": "d5edc7b40593e4ad662a18e317a69c7c74c7c7ef",
    "semantic_title": "fine-grained general entity typing in german using germanet",
    "citation_count": 1,
    "authors": [
      "Sabine Weber",
      "Mark Steedman"
    ]
  },
  "https://aclanthology.org/2021.textgraphs-1.15": {
    "title": "On Geodesic Distances and Contextual Embedding Compression for Text Classification",
    "volume": "workshop",
    "abstract": "In some memory-constrained settings like IoT devices and over-the-network data pipelines, it can be advantageous to have smaller contextual embeddings. We investigate the efficacy of projecting contextual embedding data (BERT) onto a manifold, and using nonlinear dimensionality reduction techniques to compress these embeddings. In particular, we propose a novel post-processing approach, applying a combination of Isomap and PCA. We find that the geodesic distance estimations, estimates of the shortest path on a Riemannian manifold, from Isomap's k-Nearest Neighbors graph bolstered the performance of the compressed embeddings to be comparable to the original BERT embeddings. On one dataset, we find that despite a 12-fold dimensionality reduction, the compressed embeddings performed within 0.1% of the original BERT embeddings on a downstream classification task. In addition, we find that this approach works particularly well on tasks reliant on syntactic data, when compared with linear dimensionality reduction. These results show promise for a novel geometric approach to achieve lower dimensional text embeddings from existing transformers and pave the way for data-specific and application-specific embedding compressions",
    "checked": true,
    "id": "46f812f848ec318959e547c6771cd8ddad2495cc",
    "semantic_title": "on geodesic distances and contextual embedding compression for text classification",
    "citation_count": 2,
    "authors": [
      "Rishi Jha",
      "Kai Mihata"
    ]
  },
  "https://aclanthology.org/2021.textgraphs-1.16": {
    "title": "Semi-Supervised Joint Estimation of Word and Document Readability",
    "volume": "workshop",
    "abstract": "Readability or difficulty estimation of words and documents has been investigated independently in the literature, often assuming the existence of extensive annotated resources for the other. Motivated by our analysis showing that there is a recursive relationship between word and document difficulty, we propose to jointly estimate word and document difficulty through a graph convolutional network (GCN) in a semi-supervised fashion. Our experimental results reveal that the GCN-based method can achieve higher accuracy than strong baselines, and stays robust even with a smaller amount of labeled data",
    "checked": true,
    "id": "2a1bb3fec1b99d61caa9e81e7516f00e785125eb",
    "semantic_title": "semi-supervised joint estimation of word and document readability",
    "citation_count": 6,
    "authors": [
      "Yoshinari Fujinuma",
      "Masato Hagiwara"
    ]
  },
  "https://aclanthology.org/2021.textgraphs-1.17": {
    "title": "TextGraphs 2021 Shared Task on Multi-Hop Inference for Explanation Regeneration",
    "volume": "workshop",
    "abstract": "The Shared Task on Multi-Hop Inference for Explanation Regeneration asks participants to compose large multi-hop explanations to questions by assembling large chains of facts from a supporting knowledge base. While previous editions of this shared task aimed to evaluate explanatory completeness – finding a set of facts that form a complete inference chain, without gaps, to arrive from question to correct answer, this 2021 instantiation concentrates on the subtask of determining relevance in large multi-hop explanations. To this end, this edition of the shared task makes use of a large set of approximately 250k manual explanatory relevancy ratings that augment the 2020 shared task data. In this summary paper, we describe the details of the explanation regeneration task, the evaluation data, and the participating systems. Additionally, we perform a detailed analysis of participating systems, evaluating various aspects involved in the multi-hop inference process. The best performing system achieved an NDCG of 0.82 on this challenging task, substantially increasing performance over baseline methods by 32%, while also leaving significant room for future improvement",
    "checked": true,
    "id": "464a75c05a5ce709fc515a2577b43acc8e3d45ce",
    "semantic_title": "textgraphs 2021 shared task on multi-hop inference for explanation regeneration",
    "citation_count": 8,
    "authors": [
      "Mokanarangan Thayaparan",
      "Marco Valentino",
      "Peter Jansen",
      "Dmitry Ustalov"
    ]
  },
  "https://aclanthology.org/2021.textgraphs-1.18": {
    "title": "DeepBlueAI at TextGraphs 2021 Shared Task: Treating Multi-Hop Inference Explanation Regeneration as A Ranking Problem",
    "volume": "workshop",
    "abstract": "This paper describes the winning system for TextGraphs 2021 shared task: Multi-hop inference explanation regeneration. Given a question and its corresponding correct answer, this task aims to select the facts that can explain why the answer is correct for that question and answering (QA) from a large knowledge base. To address this problem and accelerate training as well, our strategy includes two steps. First, fine-tuning pre-trained language models (PLMs) with triplet loss to recall top-K relevant facts for each question and answer pair. Then, adopting the same architecture to train the re-ranking model to rank the top-K candidates. To further improve the performance, we average the results from models based on different PLMs (e.g., RoBERTa) and different parameter settings to make the final predictions. The official evaluation shows that, our system can outperform the second best system by 4.93 points, which proves the effectiveness of our system. Our code has been open source, address is https://github.com/DeepBlueAI/TextGraphs-15",
    "checked": true,
    "id": "c42b8a16c7680963e2ea99d07c3a19ba24201950",
    "semantic_title": "deepblueai at textgraphs 2021 shared task: treating multi-hop inference explanation regeneration as a ranking problem",
    "citation_count": 3,
    "authors": [
      "Chunguang Pan",
      "Bingyan Song",
      "Zhipeng Luo"
    ]
  },
  "https://aclanthology.org/2021.textgraphs-1.19": {
    "title": "A Three-step Method for Multi-Hop Inference Explanation Regeneration",
    "volume": "workshop",
    "abstract": "Multi-hop inference for explanation generation is to combine two or more facts to make an inference. The task focuses on generating explanations for elementary science questions. In the task, the relevance between the explanations and the QA pairs is of vital importance. To address the task, a three-step framework is proposed. Firstly, vector distance between two texts is utilized to recall the top-K relevant explanations for each question, reducing the calculation consumption. Then, a selection module is employed to choose those most relative facts in an autoregressive manner, giving a preliminary order for the retrieved facts. Thirdly, we adopt a re-ranking module to re-rank the retrieved candidate explanations with relevance between each fact and the QA pairs. Experimental results illustrate the effectiveness of the proposed framework with an improvement of 39.78% in NDCG over the official baseline",
    "checked": true,
    "id": "2c126b90ada873448069c3b95e721a1d156613b3",
    "semantic_title": "a three-step method for multi-hop inference explanation regeneration",
    "citation_count": 1,
    "authors": [
      "Yuejia Xiang",
      "Yunyan Zhang",
      "Xiaoming Shi",
      "Bo Liu",
      "Wandi Xu",
      "Xi Chen"
    ]
  },
  "https://aclanthology.org/2021.textgraphs-1.20": {
    "title": "Textgraphs-15 Shared Task System Description : Multi-Hop Inference Explanation Regeneration by Matching Expert Ratings",
    "volume": "workshop",
    "abstract": "Creating explanations for answers to science questions is a challenging task that requires multi-hop inference over a large set of fact sentences. This year, to refocus the Textgraphs Shared Task on the problem of gathering relevant statements (rather than solely finding a single ‘correct path'), the WorldTree dataset was augmented with expert ratings of ‘relevance' of statements to each overall explanation. Our system, which achieved second place on the Shared Task leaderboard, combines initial statement retrieval; language models trained to predict the relevance scores; and ensembling of a number of the resulting rankings. Our code implementation is made available at https://github.com/mdda/worldtree_corpus/tree/textgraphs_2021",
    "checked": true,
    "id": "f88f97e7b821a6c96e48b43a150455b7cba3816c",
    "semantic_title": "textgraphs-15 shared task system description : multi-hop inference explanation regeneration by matching expert ratings",
    "citation_count": 1,
    "authors": [
      "Sureshkumar Vivek Kalyan",
      "Sam Witteveen",
      "Martin Andrews"
    ]
  },
  "https://aclanthology.org/2021.trustnlp-1.1": {
    "title": "Interpretability Rules: Jointly Bootstrapping a Neural Relation Extractorwith an Explanation Decoder",
    "volume": "workshop",
    "abstract": "We introduce a method that transforms a rule-based relation extraction (RE) classifier into a neural one such that both interpretability and performance are achieved. Our approach jointly trains a RE classifier with a decoder that generates explanations for these extractions, using as sole supervision a set of rules that match these relations. Our evaluation on the TACRED dataset shows that our neural RE classifier outperforms the rule-based one we started from by 9 F1 points; our decoder generates explanations with a high BLEU score of over 90%; and, the joint learning improves the performance of both the classifier and decoder",
    "checked": true,
    "id": "c2e9738afe400a64aa832cc3bef59c5bf0783994",
    "semantic_title": "interpretability rules: jointly bootstrapping a neural relation extractorwith an explanation decoder",
    "citation_count": 3,
    "authors": [
      "Zheng Tang",
      "Mihai Surdeanu"
    ]
  },
  "https://aclanthology.org/2021.trustnlp-1.2": {
    "title": "Measuring Biases of Word Embeddings: What Similarity Measures and Descriptive Statistics to Use?",
    "volume": "workshop",
    "abstract": "Word embeddings are widely used in Natural Language Processing (NLP) for a vast range of applications. However, it has been consistently proven that these embeddings reflect the same human biases that exist in the data used to train them. Most of the introduced bias indicators to reveal word embeddings' bias are average-based indicators based on the cosine similarity measure. In this study, we examine the impacts of different similarity measures as well as other descriptive techniques than averaging in measuring the biases of contextual and non-contextual word embeddings. We show that the extent of revealed biases in word embeddings depends on the descriptive statistics and similarity measures used to measure the bias. We found that over the ten categories of word embedding association tests, Mahalanobis distance reveals the smallest bias, and Euclidean distance reveals the largest bias in word embeddings. In addition, the contextual models reveal less severe biases than the non-contextual word embedding models",
    "checked": true,
    "id": "300c10da7c3d5920e652d3bef2493baaa228f56a",
    "semantic_title": "measuring biases of word embeddings: what similarity measures and descriptive statistics to use?",
    "citation_count": 3,
    "authors": [
      "Hossein Azarpanah",
      "Mohsen Farhadloo"
    ]
  },
  "https://aclanthology.org/2021.trustnlp-1.3": {
    "title": "Private Release of Text Embedding Vectors",
    "volume": "workshop",
    "abstract": "Ensuring strong theoretical privacy guarantees on text data is a challenging problem which is usually attained at the expense of utility. However, to improve the practicality of privacy preserving text analyses, it is essential to design algorithms that better optimize this tradeoff. To address this challenge, we propose a release mechanism that takes any (text) embedding vector as input and releases a corresponding private vector. The mechanism satisfies an extension of differential privacy to metric spaces. Our idea based on first randomly projecting the vectors to a lower-dimensional space and then adding noise in this projected space generates private vectors that achieve strong theoretical guarantees on its utility. We support our theoretical proofs with empirical experiments on multiple word embedding models and NLP datasets, achieving in some cases more than 10% gains over the existing state-of-the-art privatization techniques",
    "checked": true,
    "id": "a0bf20f71771938c1ff69644e90cf317f8ee85d2",
    "semantic_title": "private release of text embedding vectors",
    "citation_count": 9,
    "authors": [
      "Oluwaseyi Feyisetan",
      "Shiva Kasiviswanathan"
    ]
  },
  "https://aclanthology.org/2021.trustnlp-1.4": {
    "title": "Accountable Error Characterization",
    "volume": "workshop",
    "abstract": "Customers of machine learning systems demand accountability from the companies employing these algorithms for various prediction tasks. Accountability requires understanding of system limit and condition of erroneous predictions, as customers are often interested in understanding the incorrect predictions, and model developers are absorbed in finding methods that can be used to get incremental improvements to an existing system. Therefore, we propose an accountable error characterization method, AEC, to understand when and where errors occur within the existing black-box models. AEC, as constructed with human-understandable linguistic features, allows the model developers to automatically identify the main sources of errors for a given classification system. It can also be used to sample for the set of most informative input points for a next round of training. We perform error detection for a sentiment analysis task using AEC as a case study. Our results on the sample sentiment task show that AEC is able to characterize erroneous predictions into human understandable categories and also achieves promising results on selecting erroneous samples when compared with the uncertainty-based sampling",
    "checked": true,
    "id": "b993b5fa8139e178e8e0262bd0ce62f838db7c64",
    "semantic_title": "accountable error characterization",
    "citation_count": 0,
    "authors": [
      "Amita Misra",
      "Zhe Liu",
      "Jalal Mahmud"
    ]
  },
  "https://aclanthology.org/2021.trustnlp-1.5": {
    "title": "xER: An Explainable Model for Entity Resolution using an Efficient Solution for the Clique Partitioning Problem",
    "volume": "workshop",
    "abstract": "In this paper, we propose a global, self- explainable solution to solve a prominent NLP problem: Entity Resolution (ER). We formu- late ER as a graph partitioning problem. Every mention of a real-world entity is represented by a node in the graph, and the pairwise sim- ilarity scores between the mentions are used to associate these nodes to exactly one clique, which represents a real-world entity in the ER domain. In this paper, we use Clique Partition- ing Problem (CPP), which is an Integer Pro- gram (IP) to formulate ER as a graph partition- ing problem and then highlight the explainable nature of this method. Since CPP is NP-Hard, we introduce an efficient solution procedure, the xER algorithm, to solve CPP as a combi- nation of finding maximal cliques in the graph and then performing generalized set packing using a novel formulation. We discuss the advantages of using xER over the traditional methods and provide the computational exper- iments and results of applying this method to ER data sets",
    "checked": true,
    "id": "92195c63332958a77b04cebef1ad33e4379380ff",
    "semantic_title": "xer: an explainable model for entity resolution using an efficient solution for the clique partitioning problem",
    "citation_count": 1,
    "authors": [
      "Samhita Vadrevu",
      "Rakesh Nagi",
      "JinJun Xiong",
      "Wen-mei Hwu"
    ]
  },
  "https://aclanthology.org/2021.trustnlp-1.6": {
    "title": "Gender Bias in Natural Language Processing Across Human Languages",
    "volume": "workshop",
    "abstract": "Natural Language Processing (NLP) systems are at the heart of many critical automated decision-making systems making crucial recommendations about our future world. Gender bias in NLP has been well studied in English, but has been less studied in other languages. In this paper, a team including speakers of 9 languages - Chinese, Spanish, English, Arabic, German, French, Farsi, Urdu, and Wolof - reports and analyzes measurements of gender bias in the Wikipedia corpora for these 9 languages. We develop extensions to profession-level and corpus-level gender bias metric calculations originally designed for English and apply them to 8 other languages, including languages that have grammatically gendered nouns including different feminine, masculine, and neuter profession words. We discuss future work that would benefit immensely from a computational linguistics perspective",
    "checked": true,
    "id": "c92a28b6a17b9b5bce3e84852d8c5a6900c5e250",
    "semantic_title": "gender bias in natural language processing across human languages",
    "citation_count": 9,
    "authors": [
      "Abigail Matthews",
      "Isabella Grasso",
      "Christopher Mahoney",
      "Yan Chen",
      "Esma Wali",
      "Thomas Middleton",
      "Mariama Njie",
      "Jeanna Matthews"
    ]
  },
  "https://aclanthology.org/2021.trustnlp-1.7": {
    "title": "Interpreting Text Classifiers by Learning Context-sensitive Influence of Words",
    "volume": "workshop",
    "abstract": "Many existing approaches for interpreting text classification models focus on providing importance scores for parts of the input text, such as words, but without a way to test or improve the interpretation method itself. This has the effect of compounding the problem of understanding or building trust in the model, with the interpretation method itself adding to the opacity of the model. Further, importance scores on individual examples are usually not enough to provide a sufficient picture of model behavior. To address these concerns, we propose MOXIE (MOdeling conteXt-sensitive InfluencE of words) with an aim to enable a richer interface for a user to interact with the model being interpreted and to produce testable predictions. In particular, we aim to make predictions for importance scores, counterfactuals and learned biases with MOXIE. In addition, with a global learning objective, MOXIE provides a clear path for testing and improving itself. We evaluate the reliability and efficiency of MOXIE on the task of sentiment analysis",
    "checked": true,
    "id": "0274b1ac36f9eb933c13faf8cb7e862ea219eb25",
    "semantic_title": "interpreting text classifiers by learning context-sensitive influence of words",
    "citation_count": 3,
    "authors": [
      "Sawan Kumar",
      "Kalpit Dixit",
      "Kashif Shah"
    ]
  },
  "https://aclanthology.org/2021.trustnlp-1.8": {
    "title": "Towards Benchmarking the Utility of Explanations for Model Debugging",
    "volume": "workshop",
    "abstract": "Post-hoc explanation methods are an important class of approaches that help understand the rationale underlying a trained model's decision. But how useful are they for an end-user towards accomplishing a given task? In this vision paper, we argue the need for a benchmark to facilitate evaluations of the utility of post-hoc explanation methods. As a first step to this end, we enumerate desirable properties that such a benchmark should possess for the task of debugging text classifiers. Additionally, we highlight that such a benchmark facilitates not only assessing the effectiveness of explanations but also their efficiency",
    "checked": true,
    "id": "6d87906313fae84c8a260503ecb19612b9149c57",
    "semantic_title": "towards benchmarking the utility of explanations for model debugging",
    "citation_count": 14,
    "authors": [
      "Maximilian Idahl",
      "Lijun Lyu",
      "Ujwal Gadiraju",
      "Avishek Anand"
    ]
  }
}