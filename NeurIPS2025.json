{
  "https://openreview.net/forum?id=KurYdcCbjv": {
    "title": "Generalized Linear Mode Connectivity for Transformers",
    "volume": "oral",
    "abstract": "Understanding the geometry of neural network loss landscapes is a central question in deep learning, with implications for generalization and optimization. A striking phenomenon is $\\textit{linear mode connectivity}$ (LMC), where independently trained models can be connected by low- or zero-barrier paths, despite appearing to lie in separate loss basins. However, this is often obscured by symmetries in parameter space—such as neuron permutations—which make functionally equivalent models appear dissimilar. Prior work has predominantly focused on neuron reordering through permutations, but such approaches are limited in scope and fail to capture the richer symmetries exhibited by modern architectures such as Transformers. In this work, we introduce a unified framework that captures four symmetry classes—permutations, semi-permutations, orthogonal transformations, and general invertible maps—broadening the set of valid reparameterizations and subsuming many previous approaches as special cases. Crucially, this generalization enables, for the first time, the discovery of low- and zero-barrier linear interpolation paths between independently trained Vision Transformers and GPT-2 models. Furthermore, our framework extends beyond pairwise alignment, to multi-model and width-heterogeneous settings, enabling alignment across architectures of different sizes. These results reveal deeper structure in the loss landscape and underscore the importance of symmetry-aware analysis for understanding model space geometry",
    "checked": true,
    "id": "9a1a9d3dda4be2fb3ffc0b1f64476275b0adca52",
    "semantic_title": "generalized linear mode connectivity for transformers",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=jzPQRbGkAq": {
    "title": "Deep Compositional Phase Diffusion for Long Motion Sequence Generation",
    "volume": "oral",
    "abstract": "Recent research on motion generation has shown significant progress in generating semantically aligned motion with singular semantics. However, when employing these models to create composite sequences containing multiple semantically generated motion clips, they often struggle to preserve the continuity of motion dynamics at the transition boundaries between clips, resulting in awkward transitions and abrupt artifacts. To address these challenges, we present Compositional Phase Diffusion, which leverages the Semantic Phase Diffusion Module (SPDM) and Transitional Phase Diffusion Module (TPDM) to progressively incorporate semantic guidance and phase details from adjacent motion clips into the diffusion process. Specifically, SPDM and TPDM operate within the latent motion frequency domain established by the pre-trained Action-Centric Motion Phase Autoencoder (ACT-PAE). This allows them to learn semantically important and transition-aware phase information from variable-length motion clips during training. Experimental results demonstrate the competitive performance of our proposed framework in generating compositional motion sequences that align semantically with the input conditions, while preserving phase transitional continuity between preceding and succeeding motion clips. Additionally, motion inbetweening task is made possible by keeping the phase parameter of the input motion sequences fixed throughout the diffusion process, showcasing the potential for extending the proposed framework to accommodate various application scenarios. Codes are available at https://github.com/asdryau/TransPhase",
    "checked": true,
    "id": "4b0917b247579df7fc0c73ed2267b0d144ddf725",
    "semantic_title": "deep compositional phase diffusion for long motion sequence generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eafIjoZAHm": {
    "title": "GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability",
    "volume": "oral",
    "abstract": "Graph Neural Networks (GNNs) are widely used for node classification, yet their opaque decision-making limits trust and adoption. While local explanations offer insights into individual predictions, global explanation methods—those that characterize an entire class—remain underdeveloped. Existing global explainers rely on motif discovery in small graphs, an approach that breaks down in large, real-world settings where subgraph repetition is rare, node attributes are high-dimensional, and predictions arise from complex structure-attribute interactions. We propose GnnXemplar, a novel global explainer inspired from Exemplar Theory from cognitive science. GnnXemplar identifies representative nodes in the GNN embedding space—exemplars—and explains predictions using natural language rules derived from their neighborhoods. Exemplar selection is framed as a coverage maximization problem over reverse $k$-nearest neighbors, for which we provide an efficient greedy approximation. To derive interpretable rules, we employ a self-refining prompt strategy using large language models (LLMs). Experiments across diverse benchmarks show that GnnXemplar significantly outperforms existing methods in fidelity, scalability, and human interpretability, as validated by a user study with 60 participants",
    "checked": true,
    "id": "79dddf01350cc9c6236e33b6f1cfda6ac0644317",
    "semantic_title": "gnnxemplar: exemplars to explanations - natural language rules for global gnn interpretability",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tirl2l9oKg": {
    "title": "RAG4GFM: Bridging Knowledge Gaps in Graph Foundation Models through Graph Retrieval Augmented Generation",
    "volume": "oral",
    "abstract": "Graph Foundation Models (GFMs) have demonstrated remarkable potential across graph learning tasks but face significant challenges in knowledge updating and reasoning faithfulness. To address these issues, we introduce the Retrieval-Augmented Generation (RAG) paradigm for GFMs, which leverages graph knowledge retrieval. We propose RAG4GFM, an end-to-end framework that seamlessly integrates multi-level graph indexing, task-aware retrieval, and graph fusion enhancement. RAG4GFM implements a hierarchical graph indexing architecture, enabling multi-granular graph indexing while achieving efficient logarithmic-time retrieval. The task-aware retriever implements adaptive retrieval strategies for node, edge, and graph-level tasks to surface structurally and semantically relevant evidence. The graph fusion enhancement module fuses retrieved graph features with query features and augments the topology with sparse adjacency links that preserve structural and semantic proximity, yielding a fused graph for GFM inference. Extensive experiments conducted across diverse GFM applications demonstrate that RAG4GFM significantly enhances both the efficiency of knowledge updating and reasoning faithfulness\\footnote{Code: \\url{https://github.com/Matrixmax/RAG4GFM}.}",
    "checked": false,
    "id": "63a1f6f6fafef55ac70af3adff5c353bce9a21ff",
    "semantic_title": "codegrag: bridging the gap between natural language and programming language via graphical retrieval augmented generation",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=XPe55Uffd7": {
    "title": "Agnostic Active Learning Is Always Better Than Passive Learning",
    "volume": "oral",
    "abstract": "We sharply characterize the optimal first-order query complexity of agnostic active learning for all concept classes, and propose a new general active learning algorithm which achieves it. Remarkably, the optimal query complexity admits a leading term which is always strictly smaller than the sample complexity of passive supervised learning (by a factor proportional to the best-in-class error rate). This was not previously known to be possible in the agnostic setting. For comparison, in all previous general analyses, the leading term exhibits an additional factor, such as the disagreement coefficient or related complexity measure, and therefore only provides improvements over passive learning in restricted cases. The present work completely removes such factors from the leading term, implying that $\\textit{every}$ concept class benefits from active learning in the non-realizable case. The results established in this work resolve an important long-standing open question central to the past two decades of research on the theory of agnostic active learning",
    "checked": false,
    "id": "991f080fbf252ca874769917e84e80aa3290370d",
    "semantic_title": "a competitive algorithm for agnostic active learning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=QN0E0KX2LM": {
    "title": "Learning Linear Attention in Polynomial Time",
    "volume": "oral",
    "abstract": "Previous research has explored the expressivity of Transformer models in simulating Boolean circuits or Turing machines. However, the efficient learnability of Transformers from data has remained an open question. Our study addresses this gap by providing the first polynomial-time learnability results (specifically strong, agnostic PAC learning) for single-layer Transformers with linear attention. We show that learning the optimal multi head linear attention can be recast as finding the optimal kernel predictor in a suitably defined RKHS. Moving to generalization, we construct an algorithm that, given a dataset, checks in polynomial time whether the set of best fit multi head linear attention networks on this data all perform an identical computation--a powerful notion for out of distribution generalization. We empirically validate our theoretical findings on several canonical tasks: learning random linear attention networks, key--value associations, and learning to execute finite automata. Our findings bridge a critical gap between theoretical expressivity and learnability of Transformer models",
    "checked": true,
    "id": "c1e384e3e1d9015aea7a5f1cb0b4ecc5b59fdca6",
    "semantic_title": "learning linear attention in polynomial time",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=EoebmBe9fG": {
    "title": "Optimal Mistake Bounds for Transductive Online Learning",
    "volume": "oral",
    "abstract": "We resolve a 30-year-old open problem concerning the power of unlabeled data in online learning by tightly quantifying the gap between transductive and standard online learning. We prove that for every concept class $\\mathcal{H}$ with Littlestone dimension $d$, the transductive mistake bound is at least $\\Omega(\\sqrt{d})$. This establishes an exponential improvement over previous lower bounds of $\\Omega(\\log \\log d)$, $\\Omega(\\sqrt{\\log d})$, and $\\Omega(\\log d)$, respectively due to Ben-David, Kushilevitz, and Mansour (1995, 1997) and Hanneke, Moran, and Shafer (2023). We also show that our bound is tight: for every $d$, there exists a class of Littlestone dimension $d$ with transductive mistake bound $O(\\sqrt{d})$. Our upper bound also improves the previous best known upper bound of $(2/3) \\cdot d$ from Ben-David et al. (1997). These results demonstrate a quadratic gap between transductive and standard online learning, thereby highlighting the benefit of advanced access to the unlabeled instance sequence. This stands in stark contrast to the PAC setting, where transductive and standard learning exhibit similar sample complexities",
    "checked": false,
    "id": "4f02c28bfdf43a8dd474cb8240ec9819f50ab00c",
    "semantic_title": "a trichotomy for transductive online learning",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=rtG7n93Ru8": {
    "title": "State Entropy Regularization for Robust Reinforcement Learning",
    "volume": "oral",
    "abstract": "State entropy regularization has empirically shown better exploration and sample complexity in reinforcement learning (RL). However, its theoretical guarantees have not been studied. In this paper, we show that state entropy regularization improves robustness to structured and spatially correlated perturbations. These types of variation are common in transfer learning but often overlooked by standard robust RL methods, which typically focus on small, uncorrelated changes. We provide a comprehensive characterization of these robustness properties, including formal guarantees under reward and transition uncertainty, as well as settings where the method performs poorly. Much of our analysis contrasts state entropy with the widely used policy entropy regularization, highlighting their different benefits. Finally, from a practical standpoint, we illustrate that compared with policy entropy, the robustness advantages of state entropy are more sensitive to the number of rollouts used for policy evaluation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kVz9uvqUna": {
    "title": "On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity",
    "volume": "oral",
    "abstract": "Modern deep generative models can now produce high-quality synthetic samples that are often indistinguishable from real training data. A growing body of research aims to understand why recent methods, such as diffusion and flow matching techniques, generalize so effectively. Among the proposed explanations are the inductive biases of deep learning architectures and the stochastic nature of the conditional flow matching loss. In this work, we rule out the noisy nature of the loss as a key factor driving generalization in flow matching. First, we empirically show that in high-dimensional settings, the stochastic and closed-form versions of the flow matching loss yield nearly equivalent losses. Then, using state-of-the-art flow matching models on standard image datasets, we demonstrate that both variants achieve comparable statistical performance, with the surprising observation that using the closed-form can even improve performance",
    "checked": true,
    "id": "6565b9978b0a29bf20d09e584f545cdcec0c96ec",
    "semantic_title": "on the closed-form of flow matching: generalization does not arise from target stochasticity",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=BSZqpqgqM0": {
    "title": "Why Diffusion Models Don't Memorize: The Role of Implicit Dynamical Regularization in Training",
    "volume": "oral",
    "abstract": "Diffusion models have achieved remarkable success across a wide range of generative tasks. A key challenge is understanding the mechanisms that prevent their memorization of training data and allow generalization. In this work, we investigate the role of the training dynamics in the transition from generalization to memorization. Through extensive experiments and theoretical analysis, we identify two distinct timescales: an early time $\\tau_\\mathrm{gen}$ at which models begin to generate high-quality samples, and a later time $\\tau_\\mathrm{mem}$ beyond which memorization emerges. Crucially, we find that $\\tau_\\mathrm{mem}$ increases linearly with the training set size $n$, while $\\tau_\\mathrm{gen}$ remains constant. This creates a growing window of training times with $n$ where models generalize effectively, despite showing strong memorization if training continues beyond it. It is only when $n$ becomes larger than a model-dependent threshold that overfitting disappears at infinite training times. These findings reveal a form of implicit dynamical regularization in the training dynamics, which allow to avoid memorization even in highly overparameterized settings. Our results are supported by numerical experiments with standard U-Net architectures on realistic and synthetic datasets, and by a theoretical analysis using a tractable random features model studied in the high-dimensional limit",
    "checked": true,
    "id": "a2356bd17f81684d2c2e9bb00d85741cf9047ddd",
    "semantic_title": "why diffusion models don't memorize: the role of implicit dynamical regularization in training",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=rMhQBlhh4c": {
    "title": "Adjoint Schrödinger Bridge Sampler",
    "volume": "oral",
    "abstract": "Computational methods for learning to sample from the Boltzmann distribution—where the target distribution is known only up to an unnormalized energy function—have advanced significantly recently. Due to the lack of explicit target samples, however, prior diffusion-based methods, known as _diffusion samplers_, often require importance-weighted estimation or complicated learning processes. Both trade off scalability with extensive evaluations of the energy and model, thereby limiting their practical usage. In this work, we propose **Adjoint Schrödinger Bridge Sampler (ASBS)**, a new diffusion sampler that employs simple and scalable matching-based objectives yet without the need to estimate target samples during training. ASBS is grounded on a mathematical model—the Schrödinger Bridge—which enhances sampling efficiency via kinetic-optimal transportation. Through a new lens of stochastic optimal control theory, we demonstrate how SB-based diffusion samplers can be learned at scale via Adjoint Matching and prove convergence to the global solution. Notably, ASBS generalizes the recent Adjoint Sampling (Havens et al., 2025) to arbitrary source distributions by relaxing the so-called memoryless condition that largely restricts the design space. Through extensive experiments, we demonstrate the effectiveness of ASBS on sampling from classical energy functions, amortized conformer generation, and molecular Boltzmann distributions. Codes are available at https://github.com/facebookresearch/adjoint_samplers",
    "checked": true,
    "id": "37fb369aae1f3375c27aa74c51d522c2a9c97522",
    "semantic_title": "adjoint schrödinger bridge sampler",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=RxkCwOKVKa": {
    "title": "Breaking the Performance Ceiling in Reinforcement Learning requires Inference Strategies",
    "volume": "oral",
    "abstract": "Reinforcement learning (RL) systems have countless applications, from energy-grid management to protein design. However, such real-world scenarios are often extremely difficult, combinatorial in nature, and require complex coordination between multiple agents. This level of complexity can cause even state-of-the-art RL systems, trained until convergence, to hit a performance ceiling which they are unable to break out of with zero-shot inference. Meanwhile, many digital or simulation-based applications allow for an inference phase that utilises a specific time and compute budget to explore multiple attempts before outputting a final solution. In this work, we show that such an inference phase employed at execution time, and the choice of a corresponding inference strategy, are key to breaking the performance ceiling observed in complex multi-agent RL problems. Our main result is striking: we can obtain up to a 126% and, on average, a 45% improvement over the previous state-of-the-art across 17 tasks, using only a couple seconds of extra wall-clock time during execution. We also demonstrate promising compute scaling properties, supported by over 60k experiments, making it the largest study on inference strategies for complex RL to date. We make all of our experimental data and code available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UVDihUz0iT": {
    "title": "High-Dimensional Calibration from Swap Regret",
    "volume": "oral",
    "abstract": "We study the online calibration of multi-dimensional forecasts over an arbitrary convex set $\\mathcal{P} \\subset \\mathbb{R}^d$ relative to an arbitrary norm $\\Vert\\cdot\\Vert$. We connect this with the problem of external regret minimization for online linear optimization, showing that if it is possible to guarantee $O(\\sqrt{\\rho T})$ worst-case regret after $T$ rounds when actions are drawn from $\\mathcal{P}$ and losses are drawn from the dual $\\Vert \\cdot \\Vert_*$ unit norm ball, then it is also possible to obtain $\\epsilon$-calibrated forecasts after $T = \\exp(O(\\rho /\\epsilon^2))$ rounds. When $\\mathcal{P}$ is the $d$-dimensional simplex and $\\Vert \\cdot \\Vert$ is the $\\ell_1$-norm, the existence of $O(\\sqrt{T\\log d})$ algorithms for learning with experts implies that it is possible to obtain $\\epsilon$-calibrated forecasts after $T = \\exp(O(\\log{d}/\\epsilon^2)) = d^{O(1/\\epsilon^2)}$ rounds, recovering a recent result of Peng 2025. Interestingly, our algorithm obtains this guarantee without requiring access to any online linear optimization subroutine or knowledge of the optimal rate $\\rho$ -- in fact, our algorithm is identical for every setting of $\\mathcal{P}$ and $\\Vert \\cdot \\Vert$. Instead, we show that the optimal regularizer for the above OLO problem can be used to upper bound the above calibration error by a swap regret, which we then minimize by running the recent TreeSwap algorithm with Follow-The-Leader as a subroutine. The resulting algorithm is highly efficient and plays a distribution over simple averages of past observations in each round. Finally, we prove that any online calibration algorithm that guarantees $\\epsilon T$ $\\ell_1$-calibration error over the $d$-dimensional simplex requires $T \\geq \\exp(\\mathrm{poly}(1/\\epsilon))$ (assuming $d \\geq \\mathrm{poly}(1/\\epsilon)$). This strengthens the corresponding $d^{\\Omega(\\log{1/\\epsilon})}$ lower bound of Peng 2025, and shows that an exponential dependence on $1/\\epsilon$ is necessary",
    "checked": true,
    "id": "0f93356d9adec5bc0ff21f43ac4db390baa0d538",
    "semantic_title": "high-dimensional calibration from swap regret",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=CH72XyZs4y": {
    "title": "In Search of Adam's Secret Sauce",
    "volume": "oral",
    "abstract": "Understanding the remarkable efficacy of Adam when training transformer-based language models has become a central research topic within the optimization community. To gain deeper insights, several simplifications of Adam have been proposed, such as the signed gradient and signed momentum methods. In this work, we conduct an extensive empirical study — training over 1,500 language models across different data configurations and scales — comparing Adam to several known simplified variants. We find that signed momentum methods are faster than SGD, but consistently underperform relative to Adam, even after careful tuning of momentum, clipping setting and learning rates. However, our analysis reveals a compelling option that preserves near-optimal performance while allowing for new insightful reformulations: constraining the Adam momentum parameters to be equal, $\\beta_1=\\beta_2$. Beyond robust performance, this choice affords new theoretical insights, highlights the \"secret sauce\" on top of signed momentum, and grants a precise statistical interpretation: we show that Adam in this setting implements a natural online algorithm for estimating the mean and variance of gradients—one that arises from a mean-field Gaussian variational inference perspective",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U8BwT6Rmw4": {
    "title": "An Optimized Franz-Parisi Criterion and its Equivalence with SQ Lower Bounds",
    "volume": "oral",
    "abstract": "Bandeira et al. (2022) introduced the Franz-Parisi (FP) criterion for characterizing the computational hard phases in statistical detection problems. The FP criterion, based on an annealed version of the celebrated Franz-Parisi potential from statistical physics, was shown to be equivalent to low-degree polynomial (LDP) lower bounds for Gaussian additive models, thereby connecting two distinct approaches to understanding the computational hardness in statistical inference. In this paper, we propose a refined FP criterion that aims to better capture the geometric ``overlap\" structure of statistical models. Our main result establishes that this optimized FP criterion is equivalent to Statistical Query (SQ) lower bounds---another foundational framework in computational complexity of statistical inference. Crucially, this equivalence holds under a mild, verifiable assumption satisfied by a broad class of statistical models, including Gaussian additive models, planted sparse models, as well as non-Gaussian component analysis (NGCA), single-index (SI) models, and convex truncation detection settings. For instance, in the case of convex truncation tasks, the assumption is equivalent with the Gaussian correlation inequality (Royen, 2014) from convex geometry. In addition to the above, our equivalence not only unifies and simplifies the derivation of several known SQ lower bounds—such as for the NGCA model (Diakonikolas et al., 2017) and the SI model (Damian et al., 2024)—but also yields new SQ lower bounds of independent interest, including for the computational gaps in mixed sparse linear regression (Arpino et al., 2023) and convex truncation (De et al., 2023)",
    "checked": true,
    "id": "ead6e705cd0993d8c7b2520e070318277f307c0e",
    "semantic_title": "an optimized franz-parisi criterion and its equivalence with sq lower bounds",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=efOq8wHH9o": {
    "title": "MaxSup: Overcoming Representation Collapse in Label Smoothing",
    "volume": "oral",
    "abstract": "Label Smoothing (LS) is widely adopted to reduce overconfidence in neural network predictions and improve generalization. Despite these benefits, recent studies reveal two critical issues with LS. First, LS induces overconfidence in misclassified samples. Second, it compacts feature representations into overly tight clusters, diluting intra-class diversity, although the precise cause of this phenomenon remained elusive. In this paper, we analytically decompose the LS-induced loss, exposing two key terms: (i) a regularization term that dampens overconfidence only when the prediction is correct, and (ii) an error-amplification term that arises under misclassifications. This latter term compels the network to reinforce incorrect predictions with undue certainty, exacerbating representation collapse. To address these shortcomings, we propose Max Suppression (MaxSup), which applies uniform regularization to both correct and incorrect predictions by penalizing the top-1 logit rather than the ground-truth logit. Through extensive feature-space analyses, we show that MaxSup restores intra-class variation and sharpens inter-class boundaries. Experiments on large-scale image classification and multiple downstream tasks confirm that MaxSup is a more robust alternative to LS.Code and reproducibility scripts are available at https://github.com/ZhouYuxuanYX/Maximum-Suppression-Regularization",
    "checked": true,
    "id": "d75b631d6e4fb2e11f4b1d411b1b2c3feab84bc8",
    "semantic_title": "maxsup: overcoming representation collapse in label smoothing",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IfD2MKTmWv": {
    "title": "Memory Mosaics at scale",
    "volume": "oral",
    "abstract": "Memory Mosaics, networks of associative memories, have demonstrated appealing compositional and in-context learning capabilities on medium-scale networks (GPT-2 scale) and synthetic small datasets. This work shows that these favorable properties remain when we scale memory mosaics to large language model sizes (llama-8B scale) and real-world datasets. To this end, we scale memory mosaics to 10B size, we train them on one trillion tokens, we introduce a couple architectural modifications (*memory mosaics v2*), we assess their capabilities across three evaluation dimensions: training-knowledge storage, new-knowledge storage, and in-context learning. Throughout the evaluation, memory mosaics v2 match transformers on the learning of training knowledge (first dimension) and significantly outperforms transformers on carrying out new tasks at inference time (second and third dimensions). These improvements cannot be easily replicated by simply increasing the training data for transformers. A memory mosaics v2 trained on one trillion tokens still perform better on these tasks than a transformer trained on eight trillion tokens",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jMhRbV47pS": {
    "title": "The emergence of sparse attention: impact of data distribution and benefits of repetition",
    "volume": "oral",
    "abstract": "Emergence is a fascinating property of large language models and neural networks more broadly: as models scale and train for longer, they sometimes develop new abilities in sudden ways. Despite initial studies, we still lack a comprehensive understanding of how and when these abilities emerge. To address this gap, we study the emergence over training of sparse attention, a critical and frequently observed attention pattern in Transformers. By combining theoretical analysis of a toy model with empirical observations on small Transformers trained on a linear regression variant, we uncover the mechanics driving sparse attention emergence and reveal that emergence timing follows power laws based on task structure, architecture, and optimizer choice. We additionally find that repetition can greatly speed up emergence. Finally, we confirm these results on a well-studied in-context associative recall task. Our findings provide a simple, theoretically grounded framework for understanding how data distributions and model design influence the learning dynamics behind one form of emergence",
    "checked": true,
    "id": "30e03dedac5dea10c09bade07a407911cf7c2ba9",
    "semantic_title": "the emergence of sparse attention: impact of data distribution and benefits of repetition",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=aLhA7AYLLR": {
    "title": "ControlFusion: A Controllable Image Fusion Network with Language-Vision Degradation Prompts",
    "volume": "oral",
    "abstract": "Current image fusion methods struggle with real-world composite degradations and lack the flexibility to accommodate user-specific needs. To address this, we propose ControlFusion, a controllable fusion network guided by language-vision prompts that adaptively mitigates composite degradations. On the one hand, we construct a degraded imaging model based on physical mechanisms, such as the Retinex theory and atmospheric scattering principle, to simulate composite degradations and provide a data foundation for addressing realistic degradations. On the other hand, we devise a prompt-modulated restoration and fusion network that dynamically enhances features according to degradation prompts, enabling adaptability to varying degradation levels. To support user-specific preferences in visual quality, a text encoder is incorporated to embed user-defined degradation types and levels as degradation prompts. Moreover, a spatial-frequency collaborative visual adapter is designed to autonomously perceive degradations from source images, thereby reducing complete reliance on user instructions. Extensive experiments demonstrate that ControlFusion outperforms SOTA fusion methods in fusion quality and degradation handling, particularly under real-world and compound degradations",
    "checked": false,
    "id": "9e1354b5b604f2d89b0892093dc7b224461ec8fa",
    "semantic_title": "controlfusion: a controllable image fusion framework with language-vision degradation prompts",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=MrUsZfQ9pC": {
    "title": "Identifiability of Deep Polynomial Neural Networks",
    "volume": "oral",
    "abstract": "Polynomial Neural Networks (PNNs) possess a rich algebraic and geometric structure. However, their identifiability-a key property for ensuring interpretability-remains poorly understood. In this work, we present a comprehensive analysis of the identifiability of deep PNNs, including architectures with and without bias terms. Our results reveal an intricate interplay between activation degrees and layer widths in achieving identifiability. As special cases, we show that architectures with non-increasing layer widths are generically identifiable under mild conditions, while encoder-decoder networks are identifiable when the decoder widths do not grow too rapidly compared to the activation degrees. Our proofs are constructive and center on a connection between deep PNNs and low-rank tensor decompositions, and Kruskal-type uniqueness theorems. We also settle an open conjecture on the dimension of PNN's neurovarieties, and provide new bounds on the activation degrees required for it to reach the expected dimension",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q3qAsZAEZw": {
    "title": "Understanding and Mitigating Numerical Sources of Nondeterminism in LLM Inference",
    "volume": "oral",
    "abstract": "Large Language Models (LLMs) are now integral across various domains and have demonstrated impressive performance. Progress, however, rests on the premise that benchmark scores are both accurate and reproducible. We demonstrate that the reproducibility of LLM performance is fragile: changing system configuration, such as evaluation batch size, GPU count, and GPU version, can introduce significant differences in the generated responses. This issue is especially pronounced in reasoning models, where minor rounding differences in early tokens can cascade into divergent chains of thought, ultimately affecting accuracy. For instance, under bfloat16 precision with greedy decoding, a reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9\\% variation in accuracy and 9,000 tokens difference in response length due to differences in GPU count, type, and evaluation batch size. We trace the root cause of this variability to the non-associative nature of floating-point arithmetic under limited numerical precision. This work presents the first systematic investigation into how numerical precision affects reproducibility in LLM inference. Through carefully controlled experiments across various hardware, software, and precision settings, we quantify when and how model outputs diverge. Our analysis reveals that floating-point precision—while critical for reproducibility—is often neglected in evaluation practices. Inspired by this, we develop a lightweight inference pipeline, dubbed LayerCast, that stores weights in 16-bit precision but performs all computations in FP32, balancing memory efficiency with numerical stability. Code is available at https://github.com/nanomaoli/llm_reproducibility",
    "checked": true,
    "id": "de572e138fc98639603d96b91f861bfcfc407dd3",
    "semantic_title": "understanding and mitigating numerical sources of nondeterminism in llm inference",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=4xvE6Iy77Y": {
    "title": "PRIMT: Preference-based Reinforcement Learning with Multimodal Feedback and Trajectory Synthesis from Foundation Models",
    "volume": "oral",
    "abstract": "Preference-based reinforcement learning (PbRL) has emerged as a promising paradigm for teaching robots complex behaviors without reward engineering. However, its effectiveness is often limited by two critical challenges: the reliance on extensive human input and the inherent difficulties in resolving query ambiguity and credit assignment during reward learning. In this paper, we introduce PRIMT, a PbRL framework designed to overcome these challenges by leveraging foundation models (FMs) for multimodal synthetic feedback and trajectory synthesis. Unlike prior approaches that rely on single-modality FM evaluations, PRIMT employs a hierarchical neuro-symbolic fusion strategy, integrating the complementary strengths of vision-language models (VLMs) and large language models (LLMs) in evaluating robot behaviors for more reliable and comprehensive feedback. PRIMT also incorporates foresight trajectory generation to warm-start the trajectory buffer with bootstrapped samples, reducing early-stage query ambiguity, and hindsight trajectory augmentation for counterfactual reasoning with a causal auxiliary loss to improve credit assignment. We evaluate PRIMT on 2 locomotion and 6 manipulation tasks on various benchmarks, demonstrating superior performance over FM-based and scripted baselines. Website at https://primt25.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R73ybUciQF": {
    "title": "A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders",
    "volume": "oral",
    "abstract": "Sparse Autoencoders (SAEs) aim to decompose the activation space of large language models (LLMs) into human-interpretable latent directions or features. As we increase the number of features in the SAE, hierarchical features tend to split into finer features (\"math\" may split into \"algebra\", \"geometry\", etc.), a phenomenon referred to as feature splitting. However, we show that sparse decomposition and splitting of hierarchical features is not robust. Specifically, we show that seemingly monosemantic features fail to fire where they should, and instead get \"absorbed\" into their children features. We coin this phenomenon feature absorption, and show that it is caused by optimizing for sparsity in SAEs whenever the underlying features form a hierarchy. We introduce a metric to detect absorption in SAEs, and validate our findings empirically on hundreds of LLM SAEs. Our investigation suggests that varying SAE sizes or sparsity is insufficient to solve this issue. We discuss the implications of feature absorption in SAEs and some potential approaches to solve the fundamental theoretical issues before SAEs can be used for interpreting LLMs robustly and at scale",
    "checked": true,
    "id": "8cbc7c757766697a56e7cf881605b8e414ab2fdc",
    "semantic_title": "a is for absorption: studying feature splitting and absorption in sparse autoencoders",
    "citation_count": 63,
    "authors": []
  },
  "https://openreview.net/forum?id=B6bE2GC71a": {
    "title": "EvoLM: In Search of Lost Language Model Training Dynamics",
    "volume": "oral",
    "abstract": "Modern language model (LM) training has been divided into multiple stages, making it difficult for downstream developers to evaluate the impact of design choices made at each stage. We present EvoLM, a model suite that enables systematic and transparent analysis of LMs' training dynamics across pre-training, continued pre-training, supervised fine-tuning, and reinforcement learning. By training over 100 LMs with 1B and 4B parameters from scratch, we rigorously evaluate both upstream (language modeling) and downstream (problem-solving) reasoning capabilities, including considerations of both in-domain and out-of-domain generalization. Key insights highlight the diminishing returns from excessive pre-training and post-training, the importance and practices of mitigating forgetting during domain-specific continued pre-training, the crucial role of continued pre-training in bridging pre-training and post-training phases, and various intricate trade-offs when configuring supervised fine-tuning and reinforcement learning. To facilitate open research and reproducibility, we release all pre-trained and post-trained models, training datasets for all stages, and our entire training and evaluation pipeline",
    "checked": true,
    "id": "6d7f20de3a43ddd12f1a7a1250466b67b7b07599",
    "semantic_title": "evolm: in search of lost language model training dynamics",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=WhEPg4mUs6": {
    "title": "Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions",
    "volume": "oral",
    "abstract": "As the economic and environmental costs of training and deploying large vision or language models increase dramatically, analog in-memory computing (AIMC) emerges as a promising energy-efficient solution. However, the training perspective, especially its training dynamic, is underexplored. In AIMC hardware, the trainable weights are represented by the conductance of resistive elements and updated using consecutive electrical pulses. While the conductance changes by a constant in response to each pulse, in reality, the change is scaled by asymmetric and non-linear response functions, leading to a non-ideal training dynamic. This paper provides a theoretical foundation for gradient-based training on AIMC hardware with non-ideal response functions. We demonstrate that asymmetric response functions negatively impact Analog SGD by imposing an implicit penalty on the objective. To overcome the issue, we propose residual learning algorithm, which provably converges exactly to a critical point by solving a bilevel optimization problem. We show that the proposed method can be extended to deal with other hardware imperfections like limited response granularity. As far as we know, it is the first paper to investigate the impact of a class of generic non-ideal response functions. The conclusion is supported by simulations validating our theoretical insights",
    "checked": true,
    "id": "a34de33c691ff8dc20c20545248f381cf0ae0a11",
    "semantic_title": "analog in-memory training on general non-ideal resistive elements: the impact of response functions",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=zJdutIT6vT": {
    "title": "Discovering Opinion Intervals from Conflicts in Signed Graphs",
    "volume": "oral",
    "abstract": "Online social media provide a platform for people to discuss current events and exchange opinions with their peers. While interactions are predominantly positive, in recent years, there has been a lot of research to understand the conflicts in social networks and how they are based on different views and opinions. In this paper, we ask whether the conflicts in a network reveal a small and interpretable set of prevalent opinion ranges that explain the users' interactions. More precisely, we consider signed graphs, where the edge signs indicate positive and negative interactions of node pairs, and our goal is to infer opinion intervals that are consistent with the edge signs. We introduce an optimization problem that models this question, and we give strong hardness results and a polynomial-time approximation scheme by utilizing connections to interval graphs and the Correlation Clustering problem. We further provide scalable heuristics and show that in experiments they yield more expressive solutions than Correlation Clustering baselines. We also present a case study on a novel real-world dataset from the German parliament, showing that our algorithms can recover the political leaning of German parties based on co-voting behavior",
    "checked": false,
    "id": "c8887d7e85bd3f377a3bbd0a962731d1ca0d9b9c",
    "semantic_title": "a10 an ai-enabled software as a medical device cleared by the fda for assessing hemorrhage risk in trauma casualties",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8P3QNSckMp": {
    "title": "A Clean Slate for Offline Reinforcement Learning",
    "volume": "oral",
    "abstract": "Progress in offline reinforcement learning (RL) has been impeded by ambiguous problem definitions and entangled algorithmic designs, resulting in inconsistent implementations, insufficient ablations, and unfair evaluations. Although offline RL explicitly avoids environment interaction, prior methods frequently employ extensive, undocumented online evaluation for hyperparameter tuning, complicating method comparisons. Moreover, existing reference implementations differ significantly in boilerplate code, obscuring their core algorithmic contributions. We address these challenges by first introducing a rigorous taxonomy and a transparent evaluation protocol that explicitly quantifies online tuning budgets. To resolve opaque algorithmic design, we provide clean, minimalistic, single-file implementations of various model-free and model-based offline RL methods, significantly enhancing clarity and achieving substantial speed-ups. Leveraging these streamlined implementations, we propose Unifloral, a unified algorithm that encapsulates diverse prior approaches and enables development within a single, comprehensive hyperparameter space. Using Unifloral with our rigorous evaluation protocol, we develop two novel algorithms - TD3-AWR (model-free) and MoBRAC (model-based) - which substantially outperform established baselines. Our implementation is publicly available at https://github.com/EmptyJackson/unifloral",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F0JzotXYgC": {
    "title": "Spectral Perturbation Bounds for Low-Rank Approximation with Applications to Privacy",
    "volume": "oral",
    "abstract": "A central challenge in machine learning is to understand how noise or measurement errors affect low-rank approximations, particularly in the spectral norm. This question is especially important in differentially private low-rank approximation, where one aims to preserve the top-$p$ structure of a data-derived matrix while ensuring privacy. Prior work often analyzes Frobenius norm error or changes in reconstruction quality, but these metrics can over- or under-estimate true subspace distortion. The spectral norm, by contrast, captures worst-case directional error and provides the strongest utility guarantees. We establish new high-probability spectral-norm perturbation bounds for symmetric matrices that refine the classical Eckart--Young--Mirsky theorem and explicitly capture interactions between a matrix $A \\in \\mathbb{R}^{n \\times n}$ and an arbitrary symmetric perturbation $E$. Under mild eigengap and norm conditions, our bounds yield sharp estimates for $\\| (A + E)_p - A_p \\|$, where $A_p$ is the best rank-$p$ approximation of $A$, with improvements of up to a factor of $\\sqrt{n}$. As an application, we derive improved utility guarantees for differentially private PCA, resolving an open problem in the literature. Our analysis relies on a novel contour bootstrapping method from complex analysis and extends it to a broad class of spectral functionals, including polynomials and matrix exponentials. Empirical results on real-world datasets confirm that our bounds closely track the actual spectral error under diverse perturbation regimes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gxfusMqPIs": {
    "title": "Improved Regret Bounds for Gaussian Process Upper Confidence Bound in Bayesian Optimization",
    "volume": "oral",
    "abstract": "This paper addresses the Bayesian optimization problem (also referred to as the Bayesian setting of the Gaussian process bandit), where the learner seeks to minimize the regret under a function drawn from a known Gaussian process (GP). Under a Mat\\'ern kernel with some extent of smoothness, we show that the Gaussian process upper confidence bound (GP-UCB) algorithm achieves $\\tilde{O}(\\sqrt{T})$ cumulative regret with high probability. Furthermore, our analysis yields $O(\\sqrt{T \\ln^2 T})$ regret under a squared exponential kernel. These results fill the gap between the existing regret upper bound of GP-UCB and the current best upper bound provided by Scarlett [2018]. The key idea in our proof is to capture the concentration behavior of the input sequence realized by GP-UCB, enabling us to handle GP's information gain in a refined manner",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eIDa6pd9iQ": {
    "title": "Auto-Compressing Networks",
    "volume": "oral",
    "abstract": "Deep neural networks with short residual connections have demonstrated remarkable success across domains, but increasing depth often introduces computational redundancy without corresponding improvements in representation quality. We introduce Auto-Compressing Networks (ACNs), an architectural variant where additive long feedforward connections from each layer to the output replace traditional short residual connections. By analyzing the distinct dynamics induced by this modification, we reveal a unique property we coin as *auto-compression*—the ability of a network to organically compress information during training with gradient descent, through architectural design alone. Through auto-compression, information is dynamically \"pushed\" into early layers during training, enhancing their representational quality and revealing potential redundancy in deeper ones. We theoretically show that this property emerges from layer-wise training patterns found only in ACNs, where layers are dynamically utilized during training based on task requirements. We also find that ACNs exhibit enhanced noise robustness compared to residual networks, superior performance in low-data settings, improved transfer learning capabilities, and mitigate catastrophic forgetting suggesting that they learn representations that generalize better despite using fewer parameters. Our results demonstrate up to 18\\% reduction in catastrophic forgetting and 30-80\\% architectural compression while maintaining accuracy across vision transformers, MLP-mixers, and BERT architectures. These findings establish ACNs as a practical approach to developing efficient neural architectures that automatically adapt their computational footprint to task complexity, while learning robust representations suitable for noisy real-world tasks and continual learning scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oJ84bedrtM": {
    "title": "MokA: Multimodal Low-Rank Adaptation for MLLMs",
    "volume": "oral",
    "abstract": "In this paper, we reveal that most current efficient multimodal fine-tuning methods are hindered by a key limitation: they are directly borrowed from LLMs, often neglecting the intrinsic differences of multimodal scenarios and even affecting the full utilization of all modalities. Inspired by our empirical observation, we argue that unimodal adaptation and cross-modal adaptation are two essential parts for the effective fine-tuning of MLLMs. From this perspective, we propose Multimodal Low-rank Adaptation (MokA), a multimodal-aware efficient fine-tuning strategy that takes multimodal characteristics into consideration. It compresses unimodal information by modality-specific parameters while explicitly enhancing cross-modal interaction, ensuring both unimodal and cross-modal adaptation. Extensive experiments cover three representative multimodal scenarios (audio-visual-text, visual-text, and speech-text), and multiple LLM backbones (LLaMA2, Qwen2, Qwen2.5-VL, etc). Consistent improvements indicate the efficacy and versatility of the proposed method. Ablation studies and efficiency evaluation are also conducted to fully asses our method. Overall, we think MokA provides a more targeted solution for efficient adaptation of MLLMs, paving the way for further exploration",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iydmH9boLb": {
    "title": "Advancing Expert Specialization for Better MoE",
    "volume": "oral",
    "abstract": "Mixture-of-Experts (MoE) models enable efficient scaling of large language models (LLMs) by activating only a subset of experts per input. However, we observe that the commonly used auxiliary load balancing loss often leads to expert overlap and overly uniform routing, which hinders expert specialization and degrades overall performance during post-training. To address this, we propose a simple yet effective solution that introduces two complementary objectives: (1) an orthogonality loss to encourage experts to process distinct types of tokens, and (2) a variance loss to encourage more discriminative routing decisions. Gradient-level analysis demonstrates that these objectives are compatible with the existing auxiliary loss and contribute to optimizing the training process. Experimental results over various model architectures and across multiple benchmarks show that our method significantly enhances expert specialization. Notably, our method improves classic MoE baselines with auxiliary loss by up to 23.79\\%, while also maintaining load balancing in downstream tasks, without any architectural modifications or additional components. We will release our code to contribute to the community",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gm5mkiTGOy": {
    "title": "From Condensation to Rank Collapse: A Two-Stage Analysis of Transformer Training Dynamics",
    "volume": "oral",
    "abstract": "Although transformer-based models have shown exceptional empirical performance, the fundamental principles governing their training dynamics are inadequately characterized beyond configuration-specific studies. Inspired by empirical evidence showing improved reasoning capabilities under small initialization scales in language models, we employ the gradient flow analytical framework established in \\cite{zhou2022towards} to systematically investigate linearized Transformer training dynamics. Our theoretical analysis dissects the dynamics of attention modules into two distinct stages. In the first stage, asymmetric weight perturbations from random initialization sustain non-degenerate gradient dynamics in parameter matrices, facilitating systematic escape from small initialization regimes. Subsequently, these matrices undergo condensation, progressively aligning toward the target orientation. In the second stage, the previously static key-query matrices actively participate in training, driving the normalized matrices toward asymptotic rank collapse. This two-stage framework generalizes classical directional convergence results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KnqiC0znVF": {
    "title": "Large Language Diffusion Models",
    "volume": "oral",
    "abstract": "The capabilities of large language models (LLMs) are widely regarded as relying on autoregressive models (ARMs). We challenge this notion by introducing *LLaDA*, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA employs a forward data masking process and a reverse generation process, parameterized by a Transformer to predict masked tokens. It provides a principled generative approach for probabilistic inference by optimizing a likelihood lower bound. Across extensive benchmarks on general tasks, math, code, and so on, LLaDA demonstrates strong *scalability* and performs comparably to our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in *in-context learning* and, after SFT, exhibits impressive *instruction-following* abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings show the promise of diffusion models for language modeling at scale and challenge the common assumption that core LLM capabilities discussed above inherently depend on ARMs. Project page and codes: \\url{https://ml-gsai.github.io/LLaDA-demo/}",
    "checked": true,
    "id": "0d11a9674b68216b92e08cf7617a93fbd3fb91f4",
    "semantic_title": "large language diffusion models",
    "citation_count": 239,
    "authors": []
  },
  "https://openreview.net/forum?id=qYkhCah8OZ": {
    "title": "Boosting Knowledge Utilization in Multimodal Large Language Models via Adaptive Logits Fusion and Attention Reallocation",
    "volume": "oral",
    "abstract": "Despite their recent progress, Multimodal Large Language Models (MLLMs) often struggle in knowledge-intensive tasks due to the limited and outdated parametric knowledge acquired during training. Multimodal Retrieval Augmented Generation addresses this issue by retrieving contextual knowledge from external databases, thereby enhancing MLLMs with expanded knowledge sources. However, existing MLLMs often fail to fully leverage the retrieved contextual knowledge for response generation. We examine representative MLLMs and identify two major causes, namely, attention bias toward different tokens and knowledge conflicts between parametric and contextual knowledge. To this end, we design Adaptive Logits Fusion and Attention Reallocation (ALFAR), a training-free and plug-and-play approach that improves MLLM responses by maximizing the utility of the retrieved knowledge. Specifically, ALFAR tackles the challenges from two perspectives. First, it alleviates attention bias by adaptively shifting attention from visual tokens to relevant context tokens according to query-context relevance. Second, it decouples and weights parametric and contextual knowledge at output logits, mitigating conflicts between the two types of knowledge. As a plug-and-play method, ALFAR achieves superior performance across diverse datasets without requiring additional training or external tools. Extensive experiments over multiple MLLMs and benchmarks show that ALFAR consistently outperforms the state-of-the-art by large margins. Our code and data are available at https://github.com/Lackel/ALFAR",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fohuurA03P": {
    "title": "Interactive Cross-modal Learning for Text-3D Scene Retrieval",
    "volume": "oral",
    "abstract": "Text-3D Scene Retrieval (T3SR) aims to retrieve relevant scenes using linguistic queries. Although traditional T3SR methods have made significant progress in capturing fine-grained associations, they implicitly assume that query descriptions are information-complete. In practical deployments, however, limited by the capabilities of users and models, it is difficult or even impossible to directly obtain a perfect textual query suiting the entire scene and model, thereby leading to performance degradation. To address this issue, we propose a novel Interactive Text-3D Scene Retrieval Method (IDeal), which promotes the enhancement of the alignment between texts and 3D scenes through continuous interaction. To achieve this, we present an Interactive Retrieval Refinement framework (IRR), which employs a questioner to pose contextually relevant questions to an answerer in successive rounds that either promote detailed probing or encourage exploratory divergence within scenes. Upon the iterative responses received from the answerer, IRR adopts a retriever to perform both feature-level and semantic-level information fusion, facilitating scene-level interaction and understanding for more precise re-rankings. To bridge the domain gap between queries and interactive texts, we propose an Interaction Adaptation Tuning strategy (IAT). IAT mitigates the discriminability and diversity risks among augmented text features that approximate the interaction text domain, achieving contrastive domain adaptation for our retriever. Extensive experimental results on three datasets demonstrate the superiority of IDeal",
    "checked": false,
    "id": "36584b6325ba9094badf7a82aa3ed6c2fb2843f3",
    "semantic_title": "com3d: leveraging cross-view correspondence and cross-modal mining for 3d retrieval",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=XoN10bZtR9": {
    "title": "Rethinking Joint Maximum Mean Discrepancy for Visual Domain Adaptation",
    "volume": "oral",
    "abstract": "In domain adaption (DA), joint maximum mean discrepancy (JMMD), as a famous distribution-distance metric, aims to measure joint probability distribution difference between the source domain and target domain, while it is still not fully explored and especially hard to be applied into a subspace-learning framework as its empirical estimation involves a tensor-product operator whose partial derivative is difficult to obtain. To solve this issue, we deduce a concise JMMD based on the Representer theorem that avoids the tensor-product operator and obtains two essential findings. First, we reveal the uniformity of JMMD by proving that previous marginal, class conditional, and weighted class conditional probability distribution distances are three special cases of JMMD with different label reproducing kernels. Second, inspired by graph embedding, we observe that the similarity weights, which strengthen the intra-class compactness in the graph of Hilbert Schmidt independence criterion (HSIC), take opposite signs in the graph of JMMD, revealing why JMMD degrades the feature discrimination. This motivates us to propose a novel loss JMMD-HSIC by jointly considering JMMD and HSIC to promote discrimination of JMMD. Extensive experiments on several cross-domain datasets could demonstrate the validity of our revealed theoretical results and the effectiveness of our proposed JMMD-HSIC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OzdAnGHEPx": {
    "title": "Pan-LUT: Efficient Pan-sharpening via Learnable Look-Up Tables",
    "volume": "oral",
    "abstract": "Recently, deep learning-based pan-sharpening algorithms have achieved notable advancements over traditional methods. However, deep learning-based methods incur substantial computational overhead during inference, especially with large images. This excessive computational demand limits the applicability of these methods in real-world scenarios, particularly in the absence of dedicated computing devices such as GPUs and TPUs. To address these challenges, we propose Pan-LUT, a novel learnable look-up table (LUT) framework for pan-sharpening that strikes a balance between performance and computational efficiency for large remote sensing images. Our method makes it possible to process 15K$\\times$15K remote sensing images on a 24GB GPU. To finely control the spectral transformation, we devise the PAN-guided look-up table (PGLUT) for channel-wise spectral mapping. To effectively capture fine-grained spatial details, we introduce the spatial details look-up table (SDLUT). Furthermore, to adaptively aggregate channel information for generating high-resolution multispectral images, we design an adaptive output look-up table (AOLUT). Our model contains fewer than 700K parameters and processes a 9K$\\times$9K image in under 1 ms using one RTX 2080 Ti GPU, demonstrating significantly faster performance compared to other methods. Experiments reveal that Pan-LUT efficiently processes large remote sensing images in a lightweight manner, bridging the gap to real-world applications. Furthermore, our model surpasses SOTA methods in full-resolution scenes under real-world conditions, highlighting its effectiveness and efficiency",
    "checked": true,
    "id": "ef5f28e65a458c5e38ebd8abe9842d6e7a608d62",
    "semantic_title": "pan-lut: efficient pan-sharpening via learnable look-up tables",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ImpizBSKcu": {
    "title": "Dynamical Decoupling of Generalization and Overfitting in Large Two-Layer Networks",
    "volume": "oral",
    "abstract": "Understanding the inductive bias and generalization properties of large overparametrized machine learning models requires to characterize the dynamics of the training algorithm. We study the learning dynamics of large two-layer neural networks via dynamical mean field theory, a well established technique of non-equilibrium statistical physics. We show that, for large network width $m$, and large number of samples per input dimension $n/d$, the training dynamics exhibits a separation of timescales which implies: $(i)$ The emergence of a slow time scale associated with the growth in Gaussian/Rademacher complexity of the network; $(ii)$ Inductive bias towards small complexity if the initialization has small enough complexity; $(iii)$ A dynamical decoupling between feature learning and overfitting regimes; $(iv)$ A non-monotone behavior of the test error, associated `feature unlearning' regime at large times",
    "checked": true,
    "id": "e1199d1728504c079ddd54b38ec76072c2049c81",
    "semantic_title": "dynamical decoupling of generalization and overfitting in large two-layer networks",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=s0JVsx3bx1": {
    "title": "1000 Layer Networks for Self-Supervised RL: Scaling Depth Can Enable New Goal-Reaching Capabilities",
    "volume": "oral",
    "abstract": "Scaling up self-supervised learning has driven breakthroughs in language and vision, yet comparable progress has remained elusive in reinforcement learning (RL). In this paper, we study building blocks for self-supervised RL that unlock substantial improvements in scalability, with network depth serving as a critical factor. Whereas most RL papers in recent years have relied on shallow architectures (around 2 -- 5 layers), we demonstrate that increasing the depth up to 1024 layers can significantly boost performance. Our experiments are conducted in an unsupervised goal-conditioned setting, where no demonstrations or rewards are provided, so an agent must explore (from scratch) and learn how to maximize the likelihood of reaching commanded goals. Evaluated on simulated locomotion and manipulation tasks, our approach increases performance on the self-supervised contrastive RL algorithm by $2\\times$ -- $50\\times$, outperforming other goal-conditioned baselines. Increasing the model depth not only increases success rates but also qualitatively changes the behaviors learned",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XO9fhSZkBh": {
    "title": "Depth-Bounds for Neural Networks via the Braid Arrangement",
    "volume": "oral",
    "abstract": "We contribute towards resolving the open question of how many hidden layers are required in ReLU networks for exactly representing all continuous and piecewise linear functions on $\\mathbb{R}^d$. While the question has been resolved in special cases, the best known lower bound in general is still 2. We focus on neural networks that are compatible with certain polyhedral complexes, more precisely with the braid fan. For such neural networks, we prove a non-constant lower bound of $\\Omega(\\log\\log d)$ hidden layers required to exactly represent the maximum of $d$ numbers. Additionally, we provide a combinatorial proof that neural networks satisfying this assumption require three hidden layers to compute the maximum of 5 numbers; this had only been verified with an excessive computation so far. Finally, we show that a natural generalization of the best known upper bound to maxout networks is not tight, by demonstrating that a rank-3 maxout layer followed by a rank-2 maxout layer is sufficient to represent the maximum of 7 numbers",
    "checked": true,
    "id": "48af23700efeda1705da5a166a169e2adb99fd40",
    "semantic_title": "depth-bounds for neural networks via the braid arrangement",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=VYLdKb5dzO": {
    "title": "Tighter CMI-Based Generalization Bounds via Stochastic Projection and Quantization",
    "volume": "oral",
    "abstract": "In this paper, we leverage stochastic projection and lossy compression to establish new conditional mutual information (CMI) bounds on the generalization error of statistical learning algorithms. It is shown that these bounds are generally tighter than the existing ones. In particular, we prove that for certain problem instances for which existing MI and CMI bounds were recently shown in Attias et al. [2024] and Livni [2023] to become vacuous or fail to describe the right generalization behavior, our bounds yield suitable generalization guarantees of the order of $\\mathcal{O}(1/\\sqrt{n})$, where $n$ is the size of the training dataset. Furthermore, we use our bounds to investigate the problem of data \"memorization\" raised in those works, and which asserts that there are learning problem instances for which any learning algorithm that has good prediction there exist distributions under which the algorithm must \"memorize'' a big fraction of the training dataset. We show that for every learning algorithm, there exists an auxiliary algorithm that does not memorize and which yields comparable generalization error for any data distribution. In part, this shows that memorization is not necessary for good generalization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sYK4yPDuT1": {
    "title": "A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning",
    "volume": "oral",
    "abstract": "Online reinforcement learning (RL) excels in complex, safety-critical domains but suffers from sample inefficiency, training instability, and limited interpretability. Data attribution provides a principled way to trace model behavior back to training samples, yet existing methods assume fixed datasets, which is violated in online RL where each experience both updates the policy and shapes future data collection. In this paper, we initiate the study of data attribution for online RL, focusing on the widely used Proximal Policy Optimization (PPO) algorithm. We start by establishing a *local* attribution framework, interpreting model checkpoints with respect to the records in the recent training buffer. We design two target functions, capturing agent action and cumulative return respectively, and measure each record's contribution through gradient similarity between its training loss and these targets. We demonstrate the power of this framework through three concrete applications: diagnosis of learning, temporal analysis of behavior formation, and targeted intervention during training. Leveraging this framework, we further propose an algorithm, iterative influence-based filtering (IIF), for online RL training that iteratively performs experience filtering to refine policy updates. Across standard RL benchmarks (classic control, navigation, locomotion) to RLHF for large language models, IIF reduces sample complexity, speeds up training, and achieves higher returns. Together, these results open a new direction for making online RL more interpretable, efficient, and effective",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cGks3s79hW": {
    "title": "High-dimensional neuronal activity from low-dimensional latent dynamics: a solvable model",
    "volume": "oral",
    "abstract": "Computation in recurrent networks of neurons has been hypothesized to occur at the level of low-dimensional latent dynamics, both in artificial systems and in the brain. This hypothesis seems at odds with evidence from large-scale neuronal recordings in mice showing that neuronal population activity is high-dimensional. To demonstrate that low-dimensional latent dynamics and high-dimensional activity can be two sides of the same coin, we present an analytically solvable recurrent neural network (RNN) model whose dynamics can be exactly reduced to a low-dimensional dynamical system, but generates an activity manifold that has a high linear embedding dimension. This raises the question: Do low-dimensional latents explain the high-dimensional activity observed in mouse visual cortex? Spectral theory tells us that the covariance eigenspectrum alone does not allow us to recover the dimensionality of the latents, which can be low or high, when neurons are nonlinear. To address this indeterminacy, we develop Neural Cross-Encoder (NCE), an interpretable, nonlinear latent variable modeling method for neuronal recordings, and find that high-dimensional neuronal responses to drifting gratings and spontaneous activity in visual cortex can be reduced to low-dimensional latents, while the responses to natural images cannot. We conclude that the high-dimensional activity measured in certain conditions, such as in the absence of a stimulus, is explained by low-dimensional latents that are nonlinearly processed by individual neurons",
    "checked": true,
    "id": "525a4df709ee102053e1de60860a9c8ee171f28f",
    "semantic_title": "high-dimensional neuronal activity from low-dimensional latent dynamics: a solvable model",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=oGmROC4e4W": {
    "title": "Adaptive Surrogate Gradients for Sequential Reinforcement Learning in Spiking Neural Networks",
    "volume": "oral",
    "abstract": "Neuromorphic computing systems are set to revolutionize energy-constrained robotics by achieving orders-of-magnitude efficiency gains, while enabling native temporal processing. Spiking Neural Networks (SNNs) represent a promising algorithmic approach for these systems, yet their application to complex control tasks faces two critical challenges: (1) the non-differentiable nature of spiking neurons necessitates surrogate gradients with unclear optimization properties, and (2) the stateful dynamics of SNNs require training on sequences, which in reinforcement learning (RL) is hindered by limited sequence lengths during early training, preventing the network from bridging its warm-up period. We address these challenges by systematically analyzing surrogate gradient slope settings, showing that shallower slopes increase gradient magnitude in deeper layers but reduce alignment with true gradients. In supervised learning, we find no clear preference for fixed or scheduled slopes. The effect is much more pronounced in RL settings, where shallower slopes or scheduled slopes lead to a $\\times2.1$ improvement in both training and final deployed performance. Next, we propose a novel training approach that leverages a privileged guiding policy to bootstrap the learning process, while still exploiting online environment interactions with the spiking policy. Combining our method with an adaptive slope schedule for a real-world drone position control task, we achieve an average return of 400 points, substantially outperforming prior techniques, including Behavioral Cloning and TD3BC, which achieve at most –200 points under the same conditions. This work advances both the theoretical understanding of surrogate gradient learning in SNNs and practical training methodologies for neuromorphic controllers demonstrated in real-world robotic systems",
    "checked": true,
    "id": "2991a8daffbda97590541b6122cb607618a20354",
    "semantic_title": "adaptive surrogate gradients for sequential reinforcement learning in spiking neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aUAG1WS7J2": {
    "title": "Class-wise Balancing Data Replay for Federated Class-Incremental Learning",
    "volume": "oral",
    "abstract": "Federated Class Incremental Learning (FCIL) aims to collaboratively process continuously increasing incoming tasks across multiple clients. Among various approaches, data replay has become a promising solution, which can alleviate forgetting by reintroducing representative samples from previous tasks. However, their performance is typically limited by class imbalance, both within the replay buffer due to limited global awareness and between replayed and newly arrived classes. To address this issue, we propose a class-wise balancing data replay method for FCIL (FedCBDR), which employs a global coordination mechanism for class-level memory construction and reweights the learning objective to alleviate the aforementioned imbalances. Specifically, FedCBDR has two key components: 1) the global-perspective data replay module reconstructs global representations of prior task knowledge in a privacy-preserving manner, which then guides a class-aware and importance-sensitive sampling strategy to achieve balanced replay; 2) Subsequently, to handle class imbalance across tasks, the task-aware temperature scaling module adaptively adjusts the temperature of logits at both class and instance levels based on task dynamics, which reduces the model's overconfidence in majority classes while enhancing its sensitivity to minority classes. Experimental results verified that FedCBDR achieves balanced class-wise sampling under heterogeneous data distributions and improves generalization under task imbalance between earlier and recent tasks, yielding a 2%-15% Top-1 accuracy improvement over six state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m7MD0sa8Re": {
    "title": "Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain",
    "volume": "oral",
    "abstract": "Tactile sensing remains far less understood in neuroscience and less effective in artificial systems compared to more mature modalities such as vision and language. We bridge these gaps by introducing a novel Encoder-Attender-Decoder (EAD) framework to systematically explore the space of task-optimized temporal neural networks trained on realistic tactile input sequences from a customized rodent whisker-array simulator. We identify convolutional recurrent neural networks (ConvRNNs) as superior encoders to purely feedforward and state-space architectures for tactile categorization. Crucially, these ConvRNN-encoder-based EAD models achieve neural representations closely matching rodent somatosensory cortex, saturating the explainable neural variability and revealing a clear linear relationship between supervised categorization performance and neural alignment. Furthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained with tactile-specific augmentations, match supervised neural fits, serving as an ethologically-relevant, label-free proxy. For neuroscience, our findings highlight nonlinear recurrent processing as important for general-purpose tactile representations in somatosensory cortex, providing the first quantitative characterization of the underlying inductive biases in this system. For embodied AI, our results emphasize the importance of recurrent EAD architectures to handle realistic tactile inputs, along with tailored self-supervised learning methods for achieving robust tactile perception with the same type of sensors animals use to sense in unstructured environments",
    "checked": true,
    "id": "6323a917e53d548acec37fe31f161224e45e317f",
    "semantic_title": "task-optimized convolutional recurrent networks align with tactile processing in the rodent brain",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Zd6VyjmN1S": {
    "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism",
    "volume": "oral",
    "abstract": "Multimodal large language models (MLLMs) extend LLMs to handle images, videos, and audio by incorporating feature extractors and projection modules. However, these additional components—combined with complex inference pipelines and heterogeneous workloads—introduce significant inference overhead. Therefore, efficiently serving MLLMs remains a major challenge. Current tightly coupled serving architectures struggle to distinguish between mixed request types or adapt parallelism strategies to different inference stages, leading to increased time-to-first-token (TTFT) and poor resource utilization. To address this, we introduce Elastic Multimodal Parallelism (EMP), a new serving paradigm that elastically adapts to resource heterogeneity across request types and inference stages. Building upon EMP, we develop ElasticMM, an MLLM serving system that (1) separates requests into independent modality groups with dynamic resource allocation via a modality-aware load balancer; (2) decouples inference stages and enables parallelism adjustment and adaptive scaling via elastic partition scheduling; and (3) improves inference efficiency through unified multimodal prefix caching and non-blocking encoding. Experiments on diverse real-world datasets show that ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by up to 4.2$\\times$ and achieving 3.2–4.5$\\times$ higher throughput while meeting service-level objectives (SLOs)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7AwFJzgIUW": {
    "title": "Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks",
    "volume": "oral",
    "abstract": "Deployment of neural networks on resource-constrained devices demands models that are both compact and robust to adversarial inputs. However, compression and adversarial robustness often conflict. In this work, we introduce a dynamical low-rank training scheme enhanced with a novel spectral regularizer that controls the condition number of the low-rank core in each layer. This approach mitigates the sensitivity of compressed models to adversarial perturbations without sacrificing clean accuracy. The method is model- and data-agnostic, computationally efficient, and supports rank adaptivity to automatically compress the network at hand. Extensive experiments across standard architectures, datasets, and adversarial attacks show the regularized networks can achieve over 94 compression while recovering or improving adversarial accuracy relative to uncompressed baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZwCVFBFUFb": {
    "title": "QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training",
    "volume": "oral",
    "abstract": "Clinical decision‑making routinely demands reasoning over heterogeneous data, yet existing multimodal language models (MLLMs) remain largely vision‑centric and fail to generalize across clinical specialties. To bridge this gap, we introduce QoQ-Med-7B/32B, the first open generalist clinical foundation model that jointly reasons across medical images, time‑series signals, and text reports. QoQ-Med is trained with Domain‑aware Relative Policy Optimization (DRPO), a novel reinforcement‑learning objective that hierarchically scales normalized rewards according to domain rarity and modality difficulty, mitigating performance imbalance caused by skewed clinical data distributions. Trained on 2.61 million instruction tuning pairs spanning 9 clinical domains, we show that DRPO training boosts diagnostic performance by 43% in macro‑F1 on average across all visual domains as compared to other critic-free training methods like GRPO. Furthermore, with QoQ-Med trained on intensive segmentation data, it is able to highlight salient regions related to the diagnosis, with an IoU 10x higher than open models while reaching the performance of OpenAI o4-mini. To foster reproducibility and downstream research, we release (i) the full model weights, (ii) the modular training pipeline, and (iii) all intermediate reasoning traces",
    "checked": true,
    "id": "31d1c7055635090186b1d667055c990b8b6d828b",
    "semantic_title": "qoq-med: building multimodal clinical foundation models with domain-aware grpo training",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=1b7whO4SfY": {
    "title": "Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free",
    "volume": "oral",
    "abstract": "Gating mechanisms have been widely utilized, from early models like LSTMs and Highway Networks to recent state space models, linear attention, and also softmax attention. Yet, existing literature rarely examines the specific effects of gating. In this work, we conduct comprehensive experiments to systematically investigate gating-augmented softmax attention variants. Specifically, we perform a comprehensive comparison over 30 variants of 15B Mixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion token dataset. Our central finding is that a simple modification—applying a head-specific sigmoid gate after the Scaled Dot-Product Attention (SDPA)—consistently improves performance. This modification also enhances training stability, tolerates larger learning rates, and improves scaling properties. By comparing various gating positions and computational variants, we attribute this effectiveness to two key factors: (1) introducing non-linearity upon the low-rank mapping in the softmax attention, and (2) applying query-dependent sparse gating scores to modulate the SDPA output. Notably, we find this sparse gating mechanism mitigates `massive activation`, `attention sink` and enhances long-context extrapolation performance. We also release related codes (https://github.com/qiuzh20/gated_attention}) and models (https://huggingface.co/QwQZh/gated_attention) to facilitate future research. Furthermore, the most effective SDPA output gating is used in the Qwen3-Next models (https://huggingface.co/collections/Qwen/qwen3-next)",
    "checked": true,
    "id": "a295e71055248f4145257ed88f25503a75a78406",
    "semantic_title": "gated attention for large language models: non-linearity, sparsity, and attention-sink-free",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=w1ihNiIBOc": {
    "title": "Learning long range dependencies through time reversal symmetry breaking",
    "volume": "oral",
    "abstract": "Deep State Space Models (SSMs) reignite physics-grounded compute paradigms, as RNNs could natively be embodied into dynamical systems. This calls for dedicated learning algorithms obeying to core physical principles, with efficient techniques to simulate these systems and guide their design. We propose \\emph{Recurrent Hamiltonian Echo Learning} (RHEL), an algorithm which provably computes loss gradients as finite differences of physical trajectories of non-dissipative, \\emph{Hamiltonian systems}. In ML terms, RHEL only requires three ``forward passes'' irrespective of model size, without explicit Jacobian computation, nor incurring any variance in the gradient estimation. Motivated by the potential to implement our algorithm in non-digital physical systems, we first introduce RHEL in \\emph{continuous time} and demonstrate its formal equivalence with the continuous adjoint state method. To facilitate the simulation of Hamiltonian systems trained by RHEL, we propose a \\emph{discrete-time} version of RHEL which is equivalent to Backpropagation Through Time (BPTT) when applied to a class of recurrent modules which we call \\emph{Hamiltonian Recurrent Units} (HRUs). This setting allows us to demonstrate the scalability of RHEL by generalizing these results to hierarchies of HRUs, which we call \\emph{Hamiltonian SSMs} (HSSMs). We apply RHEL to train HSSMs with linear and nonlinear dynamics on a variety of time-series tasks ranging from mid-range to long-range classification and regression with sequence length reaching $\\sim 50k$. We show that RHEL consistently matches the performance of BPTT across all models and tasks. This work opens new doors for the design of scalable, energy-efficient physical systems endowed with self-learning capabilities for sequence modelling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WJujF9An5L": {
    "title": "FuXi-Ocean: A Global Ocean Forecasting System with Sub-Daily Resolution",
    "volume": "oral",
    "abstract": "Accurate, high-resolution ocean forecasting is crucial for maritime operations and environmental monitoring. While traditional numerical models are capable of producing sub-daily, eddy-resolving forecasts, they are computationally intensive and face challenges in maintaining accuracy at fine spatial and temporal scales. In contrast, recent data-driven approaches offer improved computational efficiency and emerging potential, yet typically operate at daily resolution and struggle with sub-daily predictions due to error accumulation over time. We introduce FuXi-Ocean, the first data-driven global ocean forecasting model achieving six-hourly predictions at eddy-resolving 1/12° spatial resolution, reaching depths of up to 1500 meters. The model architecture integrates a context-aware feature extraction module with a predictive network employing stacked attention blocks. The core innovation is the Mixture-of-Time (MoT) module, which adaptively integrates predictions from multiple temporal contexts by learning variable-specific reliability , mitigating cumulative errors in sequential forecasting. Through comprehensive experimental evaluation, FuXi-Ocean demonstrates superior skill in predicting key variables, including temperature, salinity, and currents, across multiple depths",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=knPz7gtjPW": {
    "title": "Superposition Yields Robust Neural Scaling",
    "volume": "oral",
    "abstract": "The success of today's large language models (LLMs) depends on the observation that larger models perform better. However, the origin of this neural scaling law, that loss decreases as a power law with model size, remains unclear. We propose that representation superposition, meaning that LLMs represent more features than they have dimensions, can be a key contributor to loss and cause neural scaling. Based on Anthropic's toy model, we use weight decay to control the degree of superposition, allowing us to systematically study how loss scales with model size. When superposition is weak, the loss follows a power law only if data feature frequencies are power-law distributed. In contrast, under strong superposition, the loss generically scales inversely with model dimension across a broad class of frequency distributions, due to geometric overlaps between representation vectors. We confirmed that open-sourced LLMs operate in the strong superposition regime and have loss scaling inversely with model dimension, and that the Chinchilla scaling laws are also consistent with this behavior. Our results identify representation superposition as a central driver of neural scaling laws, providing insights into questions like when neural scaling laws can be improved and when they will break down",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i5WnXNjwbR": {
    "title": "ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression",
    "volume": "oral",
    "abstract": "The hypothesis that Convolutional Neural Networks (CNNs) are inherently texture-biased has shaped much of the discourse on feature use in deep learning. We revisit this hypothesis by examining limitations in the cue-conflict experiment by Geirhos et al. To address these limitations, we propose a domain-agnostic framework that quantifies feature reliance through systematic suppression of shape, texture, and color cues, avoiding the confounds of forced-choice conflicts. By evaluating humans and neural networks under controlled suppression conditions, we find that CNNs are not inherently texture-biased but predominantly rely on local shape features. Nonetheless, this reliance can be substantially mitigated through modern training strategies or architectures (ConvNeXt, ViTs). We further extend the analysis across computer vision, medical imaging, and remote sensing, revealing that reliance patterns differ systematically: computer vision models prioritize shape, medical imaging models emphasize color, and remote sensing models exhibit a stronger reliance on texture. Code is available at https://github.com/tomburgert/feature-reliance",
    "checked": true,
    "id": "7be22944350afd13f99d3ed4a704e978d8e7ce88",
    "semantic_title": "imagenet-trained cnns are not biased towards texture: revisiting feature reliance through controlled suppression",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RF3miSqdXa": {
    "title": "On Linear Mode Connectivity of Mixture-of-Experts Architectures",
    "volume": "oral",
    "abstract": "Linear Mode Connectivity (LMC) is a notable phenomenon in the loss landscapes of neural networks, wherein independently trained models have been observed to be connected—up to permutation symmetries—by linear paths in parameter space along which the loss remains consistently low. This observation challenges classical views of non-convex optimization and has implications for model ensembling, generalization, and our understanding of neural loss geometry. Inspired by recent studies on LMC in standard neural networks, we systematically investigate this phenomenon within Mixture-of-Experts (MoE) architectures—a class of models known for their scalability and computational efficiency, which combine traditional neural networks—referred to as experts—through a learnable gating mechanism. We begin by conducting a comprehensive analysis of both dense and sparse gating regimes, demonstrating that the symmetries inherent to MoE architectures are fully characterized by permutations acting on both the expert components and the gating function. Building on these foundational findings, we propose a matching algorithm that enables alignment between independently trained MoEs, thereby facilitating the discovery of LMC. Finally, we empirically validate the presence of LMC using our proposed algorithm across diverse MoE configurations—including dense, sparse, and shared-expert variants—under a wide range of model settings and datasets of varying scales and modalities. Our results confirm the existence of LMC in MoE architectures and offer fundamental insights into the functional landscape and optimization dynamics of deep learning models",
    "checked": true,
    "id": "fe4de4467d151e7157d65a1d8781b4c9c491ee1b",
    "semantic_title": "on linear mode connectivity of mixture-of-experts architectures",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0biUwyjKkm": {
    "title": "OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model",
    "volume": "oral",
    "abstract": "Understanding and synthesizing realistic 3D hand-object interactions (HOI) is critical for applications ranging from immersive AR/VR to dexterous robotics. Existing methods struggle with generalization, performing well on closed-set objects and predefined tasks but failing to handle unseen objects or open-vocabulary instructions. We introduce OpenHOI, the first framework for open-world HOI synthesis, capable of generating long-horizon manipulation sequences for novel objects guided by free-form language commands. Our approach integrates a 3D Multimodal Large Language Model (MLLM) fine-tuned for joint affordance grounding and semantic task decomposition, enabling precise localization of interaction regions (e.g., handles, buttons) and breakdown of complex instructions (e.g., \"Find a water bottle and take a sip\") into executable sub-tasks. To synthesize physically plausible interactions, we propose an affordance-driven diffusion model paired with a training-free physics refinement stage that minimizes penetration and optimizes affordance alignment. Evaluations across diverse scenarios demonstrate OpenHOI's superiority over state-of-the-art methods in generalizing to novel object categories, multi-stage tasks, and complex language instructions",
    "checked": true,
    "id": "d133a45b1830348fba97e2e4b396d6a3251b1440",
    "semantic_title": "openhoi: open-world hand-object interaction synthesis with multimodal large language model",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=koEALFNBj1": {
    "title": "Representation Entanglement for Generation: Training Diffusion Transformers Is Much Easier Than You Think",
    "volume": "oral",
    "abstract": "REPA and its variants effectively mitigate training challenges in diffusion models by incorporating external visual representations from pretrained models, through alignment between the noisy hidden projections of denoising networks and foundational clean image representations. We argue that the external alignment, which is absent during the entire denoising inference process, falls short of fully harnessing the potential of discriminative representations. In this work, we propose a straightforward method called \\textit{\\textbf{R}epresentation \\textbf{E}ntanglement for \\textbf{G}eneration} (\\textbf{REG}), which entangles low-level image latents with a single high-level class token from pretrained foundation models for denoising. REG acquires the capability to produce coherent image-class pairs directly from pure noise, substantially improving both generation quality and training efficiency. This is accomplished with negligible additional inference overhead, requiring only one single additional token for denoising (<0.5\\% increase in FLOPs and latency). The inference process concurrently reconstructs both image latents and their corresponding global semantics, where the acquired semantic knowledge actively guides and enhances the image generation process. On ImageNet 256$\\times$256, SiT-XL/2 + REG demonstrates remarkable convergence acceleration, achieving $\\textbf{63}\\times$ and $\\textbf{23}\\times$ faster training than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively, SiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA trained for 4M iterations ($\\textbf{10}\\times$ longer). Code is available at: https://github.com/Martinser/REG",
    "checked": false,
    "id": "b243783f327ef9b44bc2bfe8e5fe36c4e459e055",
    "semantic_title": "representation entanglement for generation:training diffusion transformers is much easier than you think",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=s6k9l5yX8e": {
    "title": "Dynam3D: Dynamic Layered 3D Tokens Empower VLM for Vision-and-Language Navigation",
    "volume": "oral",
    "abstract": "Vision-and-Language Navigation (VLN) is a core task where embodied agents leverage their spatial mobility to navigate in 3D environments toward designated destinations based on natural language instructions. Recently, video-language large models (Video-VLMs) with strong generalization capabilities and rich commonsense knowledge have shown remarkable performance when applied to VLN tasks. However, these models still encounter the following challenges when applied to real-world 3D navigation: 1) Insufficient understanding of 3D geometry and spatial semantics; 2) Limited capacity for large-scale exploration and long-term environmental memory; 3) Poor adaptability to dynamic and changing environments.To address these limitations, we propose Dynam3D, a dynamic layered 3D representation model that leverages language-aligned, generalizable, and hierarchical 3D representations as visual input to train 3D-VLM in navigation action prediction. Given posed RGB-D images, our Dynam3D projects 2D CLIP features into 3D space and constructs multi-level 3D patch-instance-zone representations for 3D geometric and semantic understanding with a dynamic and layer-wise update strategy. Our Dynam3D is capable of online encoding and localization of 3D instances, and dynamically updates them in changing environments to provide large-scale exploration and long-term memory capabilities for navigation. By leveraging large-scale 3D-language pretraining and task-specific adaptation, our Dynam3D sets new state-of-the-art performance on VLN benchmarks including R2R-CE, REVERIE-CE and NavRAG-CE under monocular settings. Furthermore, experiments for pre-exploration, lifelong memory, and real-world robot validate the effectiveness of practical deployment",
    "checked": true,
    "id": "dcb2087598da588f43d472b4a01daad5c68b194a",
    "semantic_title": "dynam3d: dynamic layered 3d tokens empower vlm for vision-and-language navigation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NM4emKloy6": {
    "title": "Learning (Approximately) Equivariant Networks via Constrained Optimization",
    "volume": "oral",
    "abstract": "Equivariant neural networks are designed to respect symmetries through their architecture, boosting generalization and sample efficiency when those symmetries are present in the data distribution. Real-world data, however, often departs from perfect symmetry because of noise, structural variation, measurement bias, or other symmetry-breaking effects. Strictly equivariant models may struggle to fit the data, while unconstrained models lack a principled way to leverage partial symmetries. Even when the data is fully symmetric, enforcing equivariance can hurt training by limiting the model to a restricted region of the parameter space. Guided by homotopy principles, where an optimization problem is solved by gradually transforming a simpler problem into a complex one, we introduce Adaptive Constrained Equivariance (ACE), a constrained optimization approach that starts with a flexible, non-equivariant model and gradually reduces its deviation from equivariance. This gradual tightening smooths training early on and settles the model at a data-driven equilibrium, balancing between equivariance and non-equivariance. Across multiple architectures and tasks, our method consistently improves performance metrics, sample efficiency, and robustness to input perturbations compared with strictly equivariant models and heuristic equivariance relaxations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jRXgRC6fu7": {
    "title": "SAGE: A Unified Framework for Generalizable Object State Recognition with State-Action Graph Embedding",
    "volume": "oral",
    "abstract": "Recognizing the physical states of objects and their transformations within videos is crucial for structured video understanding and enabling robust real-world applications, such as robotic manipulation. However, pretrained vision-language models often struggle to capture these nuanced dynamics and their temporal context, and specialized object state recognition frameworks may not generalize to unseen actions or objects. We introduce SAGE (State-Action Graph Embeddings), a novel framework that offers a unified model of physical state transitions by decomposing states into fine-grained, language-described visual concepts that are sharable across different objects and actions. SAGE initially leverages Large Language Models to construct a State-Action Graph, which is then multimodally refined using Vision-Language Models. Extensive experiments show that our method significantly outperforms baselines, generalizes effectively to unseen objects and actions in open-world settings. SAGE improves the prior state-of-the-art by as much as 14.6% on novel state recognition with less than 5% of its inference time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4OsgYD7em5": {
    "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?",
    "volume": "oral",
    "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning performance of large language models (LLMs), particularly in mathematics and programming tasks. It is widely believed that, similar to how traditional RL helps agents to explore and learn new strategies, RLVR enables LLMs to continuously self-improve, thus acquiring novel reasoning abilities that exceed the capacity of the corresponding base models. In this study, we take a critical look at \\textit{the current state of RLVR} by systematically probing the reasoning capability boundaries of RLVR-trained LLMs across diverse model families, RL algorithms, and math/coding/visual reasoning benchmarks, using pass@\\textit{k} at large \\textit{k} values as the evaluation metric. While RLVR improves sampling efficiency towards the correct path, we surprisingly find that current training does \\emph{not} elicit fundamentally new reasoning patterns. We observe that while RLVR-trained models outperform their base models at smaller values of $k$ (\\eg, $k$=1), base models achieve higher pass@$k$ score when $k$ is large. Moreover, we observe that the reasoning capability boundary of LLMs often narrows as RLVR training progresses. Further coverage and perplexity analysis shows that the reasoning paths generated by RLVR models are already included in the base models' sampling distribution, suggesting that their reasoning abilities originate from and are \\textit{bounded} by the base model. From this perspective, treating the base model as an upper bound, our quantitative analysis shows that six popular RLVR algorithms perform similarly and remain far from optimal in fully leveraging the potential of the base model. In contrast, we find that distillation can introduce new reasoning patterns from the teacher and genuinely expand the model's reasoning capabilities. Taken together, our findings suggest that current RLVR methods have not fully realized the potential of RL to elicit genuinely novel reasoning abilities in LLMs. This underscores the need for improved RL paradigms—such as continual scaling and multi-turn agent-environment interaction—to unlock this potential",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s6YHno8Ke3": {
    "title": "Learning to Learn with Contrastive Meta-Objective",
    "volume": "oral",
    "abstract": "Meta-learning enables learning systems to adapt quickly to new tasks, similar to humans. Different meta-learning approaches all work under/with the mini-batch episodic training framework. Such framework naturally gives the information about task identity, which can serve as additional supervision for meta-training to improve generalizability. We propose to exploit task identity as additional supervision in meta-training, inspired by the alignment and discrimination ability which is is intrinsic in human's fast learning. This is achieved by contrasting what meta-learners learn, i.e., model representations. The proposed ConML is evaluating and optimizing the contrastive meta-objective under a problem- and learner-agnostic meta-training framework. We demonstrate that ConML integrates seamlessly with existing meta-learners, as well as in-context learning models, and brings significant boost in performance with small implementation cost",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JFygzwx8SJ": {
    "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
    "volume": "oral",
    "abstract": "Transformer-based large language models (LLMs) cache context as key-value (KV) pairs during inference. As context length grows, KV cache sizes expand, leading to substantial memory overhead and increased attention latency. This paper introduces \\textit{KVzip}, a query-agnostic KV cache eviction method enabling effective reuse of compressed KV caches across diverse queries. KVzip quantifies the importance of a KV pair using the underlying LLM to reconstruct original contexts from cached KV pairs, subsequently evicting pairs with lower importance. Extensive empirical evaluations demonstrate that KVzip reduces KV cache size by $3$-$4\\times$ and FlashAttention decoding latency by approximately $2\\times$, with negligible performance loss in question-answering, retrieval, reasoning, and code comprehension tasks. Evaluations include various models such as LLaMA3.1, Qwen2.5, and Gemma3, with context lengths reaching up to 170K tokens. KVzip significantly outperforms existing query-aware KV eviction methods, which suffer from performance degradation even at a 90\\% cache budget ratio under multi-query scenarios",
    "checked": true,
    "id": "29dc00eca32c284da982792c106dc099a8fed9e8",
    "semantic_title": "kvzip: query-agnostic kv cache compression with context reconstruction",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=NM8Apk61NA": {
    "title": "HyperET: Efficient Training in Hyperbolic Space for Multi-modal Large Language Models",
    "volume": "oral",
    "abstract": "Multi-modal large language models (MLLMs) have emerged as a transformative approach for aligning visual and textual understanding. They typically require extremely high computational resources (e.g., thousands of GPUs) for training to achieve cross-modal alignment at multi-granularity levels. We argue that a key source of this inefficiency lies in the vision encoders they widely equip with, e.g., CLIP and SAM, which lack the alignment with language at multi-granularity levels. To address this issue, in this paper, we leverage hyperbolic space, which inherently models hierarchical levels and thus provides a principled framework for bridging the granularity gap between visual and textual modalities at an arbitrary granularity level. Concretely, we propose an efficient training paradigm for MLLMs, dubbed as \\blg, which can optimize visual representations to align with their textual counterparts at an arbitrary granularity level through dynamic hyperbolic radius adjustment in hyperbolic space. \\alg employs learnable matrices with M\\\"{o}bius multiplication operations, implemented via three effective configurations: diagonal scaling matrices, block-diagonal matrices, and banded matrices, providing a flexible yet efficient parametrization strategy. Comprehensive experiments across multiple MLLM benchmarks demonstrate that \\alg consistently improves both existing pre-training and fine-tuning MLLMs clearly with less than 1\\% additional parameters",
    "checked": true,
    "id": "968b0ba3808877255806d32d18f4e2c871a0fc1d",
    "semantic_title": "hyperet: efficient training in hyperbolic space for multi-modal large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zwCb9cKHpd": {
    "title": "SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and Hearing",
    "volume": "oral",
    "abstract": "3D spatial reasoning in dynamic, audio-visual environments is a cornerstone of human cognition yet remains largely unexplored by existing Audio-Visual Large Language Models (AV-LLMs) and benchmarks, which predominantly focus on static or 2D scenes. We introduce SAVVY-Bench, the first benchmark for 3D spatial reasoning in dynamic scenes with synchronized spatial audio. SAVVY-Bench is comprised of thousands of carefully curated question–answer pairs probing both directional and distance relationships involving static and moving objects, and requires fine-grained temporal grounding, consistent 3D localization, and multi-modal annotation. To tackle this challenge, we propose SAVVY, a novel training-free reasoning pipeline that consists of two stages: (i) Egocentric Spatial Tracks Estimation, which leverages AV-LLMs as well as other audio-visual methods to track the trajectories of key objects related to the query using both visual and spatial audio cues, and (ii) Dynamic Global Map Construction, which aggregates multi-modal queried object trajectories and converts them into a unified global dynamic map. Using the constructed map, a final QA answer is obtained through a coordinate transformation that aligns the global map with the queried viewpoint. Empirical evaluation demonstrates that SAVVY substantially enhances performance of state-of-the-art AV-LLMs, setting a new standard and stage for approaching dynamic 3D spatial reasoning in AV-LLMs",
    "checked": true,
    "id": "afd8b206f30697867638adc34eab56a1e4a6aa5b",
    "semantic_title": "savvy: spatial awareness via audio-visual llms through seeing and hearing",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=WCRPgBpbcA": {
    "title": "A multiscale analysis of mean-field transformers in the moderate interaction regime",
    "volume": "oral",
    "abstract": "In this paper, we study the evolution of tokens through the depth of encoder-only transformer models at inference time by modeling them as a system of particles interacting in a mean-field way and studying the corresponding dynamics. More specifically, we consider this problem in the moderate interaction regime, where the number $N$ of tokens is large and the inverse temperature parameter $\\beta$ of the model scales together with $N$. In this regime, the dynamics of the system displays a multiscale behavior: a fast phase, where the token empirical measure collapses on a low-dimensional space, an intermediate phase, where the measure further collapses into clusters, and a slow one, where such clusters sequentially merge into a single one. We provide a rigorous characterization of the limiting dynamics in each of these phases and prove convergence in the above mentioned limit, exemplifying our results with some simulations",
    "checked": true,
    "id": "0ac527269df25efa5773fe57cd98f4791bf9e0ce",
    "semantic_title": "a multiscale analysis of mean-field transformers in the moderate interaction regime",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=CaSQgef484": {
    "title": "Exploring Diffusion Transformer Designs via Grafting",
    "volume": "oral",
    "abstract": "Designing model architectures requires decisions such as selecting operators (e.g., attention, convolution) and configurations (e.g., depth, width). However, evaluating the impact of these decisions on model quality requires costly pretraining, limiting architectural investigation. Inspired by how new software is built on existing code, we ask: can new architecture designs be studied using pretrained models? To this end, we present *grafting*, a simple approach for editing pretrained diffusion transformers (DiTs) to materialize new architectures under small compute budgets. Informed by our analysis of activation behavior and attention locality, we construct a testbed based on the DiT-XL/2 design to study the impact of grafting on model quality. Using this testbed, we develop a family of hybrid designs via grafting: replacing softmax attention with gated convolution, local attention, and linear attention, and replacing MLPs with variable expansion ratio and convolutional variants. Notably, many hybrid designs achieve good quality (FID: 2.38–2.64 vs. 2.27 for DiT-XL/2) using $<2$% pretraining compute. We then graft a text-to-image model (PixArt-$\\Sigma$), achieving a 1.43$\\times$ speedup with less than a 2% drop in GenEval score. Finally, we present a case study that restructures DiT-XL/2 by converting every pair of sequential transformer blocks into parallel blocks via grafting. This reduces model depth by 2$\\times$ and yields better quality (FID: 2.77) than other models of comparable depth. Together, we show that new diffusion model designs can be explored by grafting pretrained DiTs, with edits ranging from operator replacement to architecture restructuring. Code and grafted models: https://grafting.stanford.edu",
    "checked": true,
    "id": "0083441df759656c14cc5c2ead894bd55973a00b",
    "semantic_title": "exploring diffusion transformer designs via grafting",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=rMdf8jhLR7": {
    "title": "Generalized Gradient Norm Clipping & Non-Euclidean ( L 0 , L 1 ) -Smoothness",
    "volume": "oral",
    "abstract": "This work introduces a hybrid non-Euclidean optimization method which generalizes gradient norm clipping by combining steepest descent and conditional gradient approaches. The method achieves the best of both worlds by establishing a descent property under a generalized notion of ($L_0$,$L_1$)-smoothness. Weight decay is incorporated in a principled manner by identifying a connection to the Frank-Wolfe short step. In the stochastic case, we show an order optimal $O(n^{-1/4})$ convergence rate by leveraging a momentum based gradient estimator. We discuss how to instantiate the algorithms for deep learning, which we dub Clipped Scion, and demonstrate their properties on image classification and language modeling. The code is available at https://github.com/LIONS-EPFL/ClippedScion",
    "checked": false,
    "id": "c8a729bc705af0d57004ac14575c36563df5c688",
    "semantic_title": "generalized gradient norm clipping & non-euclidean (l0,l1)-smoothness",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=Q6IyUpBmrG": {
    "title": "Rethinking Multimodal Learning from the Perspective of Mitigating Classification Ability Disproportion",
    "volume": "oral",
    "abstract": "Multimodal learning (MML) is significantly constrained by modality imbalance, leading to suboptimal performance in practice. While existing approaches primarily focus on balancing the learning of different modalities to address this issue, they fundamentally overlook the inherent disproportion in model classification ability, which serves as the primary cause of this phenomenon. In this paper, we propose a novel multimodal learning approach to dynamically balance the classification ability of weak and strong modalities by incorporating the principle of boosting. Concretely, we first propose a sustained boosting algorithm in multimodal learning by simultaneously optimizing the classification and residual errors. Subsequently, we introduce an adaptive classifier assignment strategy to dynamically facilitate the classification performance of the weak modality. Furthermore, we theoretically analyze the convergence property of the cross-modal gap function, ensuring the effectiveness of the proposed boosting scheme. To this end, the classification ability of strong and weak modalities is expected to be balanced, thereby mitigating the imbalance issue. Empirical experiments on widely used datasets reveal the superiority of our method through comparison with various state-of-the-art (SOTA) multimodal learning baselines. The source code is available at https://github.com/njustkmg/NeurIPS25-AUG",
    "checked": true,
    "id": "8b7c003e77f86231c59251220bbe230d2c89d21a",
    "semantic_title": "rethinking multimodal learning from the perspective of mitigating classification ability disproportion",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XQ87Vo9GIz": {
    "title": "TransferTraj: A Vehicle Trajectory Learning Model for Region and Task Transferability",
    "volume": "oral",
    "abstract": "Vehicle GPS trajectories provide valuable movement information that supports various downstream tasks and applications. A desirable trajectory learning model should be able to transfer across regions and tasks without retraining, avoiding the need to maintain multiple specialized models and subpar performance with limited training data. However, each region has its unique spatial features and contexts, which are reflected in vehicle movement patterns and are difficult to generalize. Additionally, transferring across different tasks faces technical challenges due to the varying input-output structures required for each task. Existing efforts towards transferability primarily involve learning embedding vectors for trajectories, which perform poorly in region transfer and require retraining of prediction modules for task transfer. To address these challenges, we propose $\\textit{TransferTraj}$, a vehicle GPS trajectory learning model that excels in both region and task transferability. For region transferability, we introduce RTTE as the main learnable module within TransferTraj. It integrates spatial, temporal, POI, and road network modalities of trajectories to effectively manage variations in spatial context distribution across regions. It also introduces a TRIE module for incorporating relative information of spatial features and a spatial context MoE module for handling movement patterns in diverse contexts. For task transferability, we propose a task-transferable input-output scheme that unifies the input-output structure of different tasks into the masking and recovery of modalities and trajectory points. This approach allows TransferTraj to be pre-trained once and transferred to different tasks without retraining. We conduct extensive experiments on three real-world vehicle trajectory datasets under various transfer settings, including task transfer, zero-shot region transfer, and few-shot region transfer. Experimental results demonstrate that TransferTraj significantly outperforms state-of-the-art baselines in different scenarios, validating its effectiveness in region and task transfer. Code is available at https://github.com/wtl52656/TransferTraj",
    "checked": true,
    "id": "e8ae9571e6a65a588df061e6d3f6b0bf60a69f6a",
    "semantic_title": "transfertraj: a vehicle trajectory learning model for region and task transferability",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zIzZxDsNNP": {
    "title": "PhySense: Sensor Placement Optimization for Accurate Physics Sensing",
    "volume": "oral",
    "abstract": "Physics sensing plays a central role in many scientific and engineering domains, which inherently involves two coupled tasks: reconstructing dense physical fields from sparse observations and optimizing scattered sensor placements to observe maximum information. While deep learning has made rapid advances in sparse-data reconstruction, existing methods generally omit optimization of sensor placements, leaving the mutual enhancement between reconstruction and placement on the shelf. To change this suboptimal practice, we propose PhySense, a synergistic two-stage framework that learns to jointly reconstruct physical fields and to optimize sensor placements, both aiming for accurate physics sensing. The first stage involves a flow-based generative model enhanced by cross-attention to adaptively fuse sparse observations. Leveraging the reconstruction feedback, the second stage performs sensor placement via projected gradient descent to satisfy spatial constraints. We further prove that the learning objectives of the two stages are consistent with classical variance-minimization principles, providing theoretical guarantees. Extensive experiments across three challenging benchmarks, especially a 3D geometry dataset, indicate PhySense achieves state-of-the-art physics sensing accuracy and discovers informative sensor placements previously unconsidered. Code is available at this repository: https://github.com/thuml/PhySense",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JcEqp4aPmb": {
    "title": "InfinityStar: Uniﬁed Spacetime AutoRegressive Modeling for Visual Generation",
    "volume": "oral",
    "abstract": "We introduce InfinityStar, a unified spacetime autoregressive framework for high-resolution image and dynamic video synthesis. Building on the recent success of autoregressive modeling in both vision and language, our purely discrete approach jointly captures spatial and temporal dependencies within a single architecture. This unified design naturally supports a variety of generation tasks such as text-to-image, text-to-video, image-to-video, and long-duration video synthesis via straightforward temporal autoregression. Through extensive experiments, InfinityStar scores 83.74 on VBench, outperforming all autoregressive models by large margins, even surpassing diffusion competitors like HunyuanVideo. Without extra optimizations, our model generates a 5s, 720p video approximately 10$\\times$ faster than leading diffusion-based methods. To our knowledge, InfinityStar is the first discrete autoregressive video generator capable of producing industrial-level 720p videos. We release all code and models to foster further research in efficient, high-quality video generation",
    "checked": false,
    "id": "bf3dd8067d66d619a0fc7cde8f5395d3c342dfad",
    "semantic_title": "infinitystar: unified spacetime autoregressive modeling for visual generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gL4muAFwsh": {
    "title": "Does Stochastic Gradient really succeed for bandits?",
    "volume": "oral",
    "abstract": "Recent works of Mei et al. (2023, 2024) have deepened the theoretical understanding of the *Stochastic Gradient Bandit* (SGB) policy, showing that using a constant learning rate guarantees asymptotic convergence to the optimal policy, and that sufficiently *small* learning rates can yield logarithmic regret. However, whether logarithmic regret holds beyond small learning rates remains unclear. In this work, we take a step towards characterizing the regret *regimes* of SGB as a function of its learning rate. For two--armed bandits, we identify a sharp threshold, scaling with the sub-optimality gap $\\Delta$, below which SGB achieves *logarithmic* regret on all instances, and above which it can incur *polynomial* regret on some instances. This result highlights the necessity of knowing (or estimating) $\\Delta$ to ensure logarithmic regret with a constant learning rate. For general $K$-armed bandits, we further show the learning rate must scale inversely with $K$ to avoid polynomial regret. We introduce novel techniques to derive regret upper bounds for SGB, laying the groundwork for future advances in the theory of gradient-based bandit algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=INqBOmwIpG": {
    "title": "Perception Encoder: The best visual embeddings are not at the output of the network",
    "volume": "oral",
    "abstract": "We introduce Perception Encoder (PE), a family of state-of-the-art vision encoders for image and video understanding. Traditionally, vision encoders have relied on a variety of pretraining objectives, each excelling at different downstream tasks. Surprisingly, after scaling a carefully tuned image pretraining recipe and refining with a robust video data engine, we find that contrastive vision-language training alone can produce strong, general embeddings for all of these downstream tasks. There is only one caveat: these embeddings are hidden within the intermediate layers of the network. To draw them out, we introduce two alignment methods: language alignment for multimodal language modeling, and spatial alignment for dense prediction. Together, our PE family of models achieves state-of-the-art results on a wide variety of tasks, including zero-shot image and video classification and retrieval; document, image, and video Q&A; and spatial tasks such as detection, tracking, and depth estimation. We release our models, code, and novel dataset of synthetically and human-annotated videos: https://github.com/facebookresearch/perception_models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gq4Gay8rDB": {
    "title": "PlayerOne: Egocentric World Simulator",
    "volume": "oral",
    "abstract": "We introduce PlayerOne, the first egocentric realistic world simulator, facilitating immersive and unrestricted exploration within vividly dynamic environments. Given an egocentric scene image from the user, PlayerOne can accurately construct the corresponding world and generate egocentric videos that are strictly aligned with the real-scene human motion of the user captured by an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that first performs pretraining on large-scale egocentric text-video pairs for coarse-level egocentric understanding, followed by finetuning on synchronous motion-video data extracted from egocentric-exocentric video datasets with our automatic construction pipeline. Besides, considering the varying importance of different components, we design a part-disentangled motion injection scheme, enabling precise control of part-level movements. In addition, we devise a joint reconstruction framework that progressively models both the 4D scene and video frames, ensuring scene consistency in the long-form video generation. Experimental results demonstrate its great generalization ability in precise control of varying human movements and world-consistent modeling of diverse scenarios. It marks the first endeavor into egocentric real-world simulation and can pave the way for the community to delve into fresh frontiers of world modeling and its diverse applications",
    "checked": true,
    "id": "c08bb5d4aa8b8b4a429f571cb9bd524fdc5f8f29",
    "semantic_title": "playerone: egocentric world simulator",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=uWj4s7rMnR": {
    "title": "Mean Flows for One-step Generative Modeling",
    "volume": "oral",
    "abstract": "We propose a principled and effective framework for one-step generative modeling. We introduce the notion of average velocity to characterize flow fields, in contrast to instantaneous velocity modeled by Flow Matching methods. A well-defined identity between average and instantaneous velocities is derived and used to guide neural network training. Our method, termed the \\textit{MeanFlow} model, is self-contained and requires no pre-training, distillation, or curriculum learning. MeanFlow demonstrates strong empirical performance: it achieves an FID of 3.43 with a single function evaluation (1-NFE) on ImageNet 256$\\times$256 trained from scratch, significantly outperforming previous state-of-the-art one-step diffusion/flow models. Our study substantially narrows the gap between one-step diffusion/flow models and their multi-step predecessors, and we hope it will motivate future research to revisit the foundations of these powerful models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lClK4uBxSG": {
    "title": "ModHiFi: Identifying High Fidelity predictive components for Model Modification",
    "volume": "spotlight",
    "abstract": "Modifying well-trained models for purposes such as pruning or unlearning, without access to training data or the original loss function, is a challenging problem. While techniques exist for such modification, they often require training data, are computationally expensive, or are architecture-specific. To address this, we investigate the fundamental question of identifying components that are critical to the model's predictive performance, without access to either gradients or the loss function, and with only distributional access such as synthetic data. We theoretically demonstrate that the global reconstruction error is linearly bounded by local reconstruction errors for Lipschitz-continuous networks such as CNNs and well-trained Transformers (which, contrary to existing literature, we find exhibit Lipschitz continuity). This motivates using the locally reconstructive behavior of component subsets to quantify their global importance, via a metric that we term *Subset Fidelity*. In the uncorrelated features setting, selecting individual components via their Subset Fidelity scores is optimal, which we use to propose **ModHiFi**, an algorithm for model modification that requires no training data or loss function access. **ModHiFi-P**, for structured pruning, achieves an 11% speedup over the current state of the art on ImageNet models and competitive performance on language models. **ModHiFi-U**, for classwise unlearning, achieves complete unlearning on CIFAR-10 without fine-tuning and demonstrates competitive performance on Swin Transformers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XsBzmJzJ2l": {
    "title": "The Structure of Relation Decoding Linear Operators in Large Language Models",
    "volume": "spotlight",
    "abstract": "This paper investigates the structure of linear operators introduced in Hernandez et al. [2023] that decode specific relational facts in transformer language models. We extend their single-relation findings to a collection of relations and systematically chart their organization. We show that such collections of relation decoders can be highly compressed by simple order-3 tensor networks without significant loss in decoding accuracy. To explain this surprising redundancy, we develop a cross-evaluation protocol, in which we apply each linear decoder operator to the subjects of every other relation. Our results reveal that these linear maps do not encode distinct relations, but extract recurring, coarse-grained semantic properties (e.g., country of capital city and country of food are both in the country-of-X property). This property-centric structure clarifies both the operators' compressibility and highlights why they generalize only to new relations that are semantically close. Our findings thus interpret linear relational decoding in transformer language models as primarily property-based, rather than relation-specific",
    "checked": true,
    "id": "c01dd96803eb95ffd3bb4472d6dcf313fe4d675d",
    "semantic_title": "the structure of relation decoding linear operators in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gOG9Zoyn4R": {
    "title": "KLASS: KL-Guided Fast Inference in Masked Diffusion Models",
    "volume": "spotlight",
    "abstract": "Masked diffusion models have demonstrated competitive results on various tasks including language generation. However, due to its iterative refinement process, the inference is often bottlenecked by slow and static sampling speed. To overcome this problem, we introduce `KL-Adaptive Stability Sampling' (KLASS), a fast yet effective sampling method that exploits token-level KL divergence to identify stable, high-confidence predictions. By unmasking multiple tokens in each iteration without any additional model training, our approach speeds up generation significantly while maintaining sample quality. On reasoning benchmarks, KLASS achieves up to $2.78\\times$ wall-clock speedups while improving performance over standard greedy decoding, attaining state-of-the-art results among diffusion-based samplers. We further validate KLASS across diverse domains, including text, image, and molecular generation, showing its effectiveness as a broadly applicable sampler across different models",
    "checked": true,
    "id": "240b1411de9ed6649344d9e8eb79c95afe944d36",
    "semantic_title": "klass: kl-guided fast inference in masked diffusion models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JeP0lpusYw": {
    "title": "HM3: Hierarchical Multi-Objective Model Merging for Pretrained Models",
    "volume": "spotlight",
    "abstract": "Model merging is a technique that combines multiple large pretrained models into a single model, enhancing performance and broadening task adaptability without original data or additional training. However, most existing model merging methods focus primarily on exploring the parameter space, merging models with identical architectures. Despite its potential, merging in the architecture space remains in its early stages due to the vast search space and challenges related to layer compatibility. This paper designs a hierarchical model merging framework named HM3, formulating a bilevel multi-objective model merging problem across both parameter and architecture spaces. At the parameter level, HM3 integrates existing merging methods to quickly identify optimal parameters. Based on these, an actor-critic strategy with efficient policy discretization is employed at the architecture level to explore inference paths with Markov property in the layer-granularity search space for reconstructing these optimal models. By training reusable policy and value networks, HM3 learns Pareto optimal models to provide customized solutions for various tasks. Experimental results on language and vision tasks demonstrate that HM3 outperforms methods focusing solely on the parameter or architecture space",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RDbuSCWhad": {
    "title": "Structured Sparse Transition Matrices to Enable State Tracking in State-Space Models",
    "volume": "spotlight",
    "abstract": "Modern state-space models (SSMs) often utilize structured transition matrices which enable efficient computation but pose restrictions on the model's expressivity, as measured in terms of the ability to emulate finite-state automata (FSA). While unstructured transition matrices are optimal in terms of expressivity, they come at a prohibitively high compute and memory cost, even for moderate state sizes. We propose a structured sparse parametrization of transition matrices in SSMs that enables FSA state tracking with provably optimal state size and depth, while keeping the computational cost of the recurrence comparable to that of diagonal SSMs. Our method, \\emph{PD-SSM}, parametrizes the transition matrix as the product of a column one-hot matrix ($P$) and a complex-valued diagonal matrix ($D$). As a result, the computational cost of parallel scans scales linearly with the state size. Theoretically, the model is BIBO-stable and can emulate any $N$-state FSA with one layer of dimension $N$ and a linear readout of size $N ×N$, significantly improving on all current structured SSM guarantees. Experimentally, the model significantly outperforms a wide collection of modern SSM variants on various FSA state tracking tasks. On multivariate time-series classification, it outperforms neural controlled differential equations, a paradigm explicitly built for time-series analysis. Finally, we integrate PD-SSM into a hybrid Transformer-SSM architecture and demonstrate that the model can effectively track the states of a complex FSA in which transitions are encoded into sets of variable-length English sentences. The code is available at https://github.com/IBM/expressive-sparse-state-space-model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ve693NkzcU": {
    "title": "Twilight: Adaptive Attention Sparsity with Hierarchical Top- p Pruning",
    "volume": "spotlight",
    "abstract": "Leveraging attention sparsity to accelerate long-context large language models (LLMs) has been of great importance recently. However, most existing sparse attention algorithms use a fixed budget of how many tokens to use in their computations. This simple static decision raises critical issues in real-world deployment because it fails to account for the dynamic nature of real-world scenarios, where the optimal balance between accuracy and efficiency can vary greatly. In this paper, we reveal a key insight that leveraging the idea of top-$p$ sampling (a.k.a., nucleus sampling) in sparse attention could enable efficient and adaptive budget decisions. Based on this, we propose Twilight, a framework that enhances any existing sparse attention algorithm with adaptive budget decision capabilities without sacrificing accuracy. Empirical results show that Twilight can adaptively prune up to 98% tokens with nearly no accuracy loss in both mid- and long-context scenarios, leading to a $1.4\\times$ speedup over state-of-the-art sparse attention mechanisms",
    "checked": false,
    "id": "3b430f665a04e8ccc5fac30ff39b42d4c6cc893d",
    "semantic_title": "twilight: adaptive attention sparsity with hierarchical top-p pruning",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=C1LVIInfZO": {
    "title": "An Analysis of Causal Effect Estimation using Outcome Invariant Data Augmentation",
    "volume": "spotlight",
    "abstract": "The technique of data augmentation (DA) is often used in machine learning for regularization purposes to better generalize under i.i.d. settings. In this work, we present a unifying framework with topics in causal inference to make a case for the use of DA beyond just the i.i.d. setting, but for generalization across interventions as well. Specifically, we argue that when the outcome generating mechanism is invariant to our choice of DA, then such augmentations can effectively be thought of as interventions on the treatment generating mechanism itself. This can potentially help to reduce bias in causal effect estimation arising from hidden confounders. In the presence of such unobserved confounding we typically make use of instrumental variables (IVs)—sources of treatment randomization that are conditionally independent of the outcome. However, IVs may not be as readily available as DA for many applications, which is the main motivation behind this work. By appropriately regularizing IV based estimators, we introduce the concept of *IV-like (IVL)* regression for mitigating confounding bias and improving predictive performance across interventions even when certain IV properties are relaxed. Finally, we cast parameterized DA as an IVL regression problem and show that when used in composition can simulate a worst-case application of such DA, further improving performance on causal estimation and generalization tasks beyond what simple DA may offer. This is shown both theoretically for the population case and via simulation experiments for the finite sample case using a simple linear example. We also present real data experiments to support our case",
    "checked": true,
    "id": "46ea06dd208fd0db82da8845671b28bde92d6e37",
    "semantic_title": "an analysis of causal effect estimation using outcome invariant data augmentation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E16vULI6AF": {
    "title": "Deciphering the Extremes: A Novel Approach for Pathological Long-tailed Recognition in Scientific Discovery",
    "volume": "spotlight",
    "abstract": "Scientific discovery across diverse fields increasingly grapples with datasets exhibiting pathological long-tailed distributions: a few common phenomena overshadow a multitude of rare yet scientifically critical instances. Unlike standard benchmarks, these scientific datasets often feature extreme imbalance coupled with a modest number of classes and limited overall sample volume, rendering existing long-tailed recognition (LTR) techniques ineffective. Such methods, biased by majority classes or prone to overfitting on scarce tail data, frequently fail to identify the very instances—novel materials, rare disease biomarkers, faint astronomical signals—that drive scientific breakthroughs. This paper introduces a novel, end-to-end framework explicitly designed to address pathological long-tailed recognition in scientific contexts. Our approach synergizes a Balanced Supervised Contrastive Learning (B-SCL) mechanism, which enhances the representation of tail classes by dynamically re-weighting their contributions, with a Smooth Objective Regularization (SOR) strategy that manages the inherent tension between tail-class focus and overall classification performance. We introduce and analyze the real-world ZincFluor chemical dataset ($\\mathcal{T}=137.54$) and synthetic benchmarks with controllable extreme imbalances (CIFAR-LT variants). Extensive evaluations demonstrate our method's superior ability to decipher these extremes. Notably, on ZincFluor, our approach achieves a Tail Top-2 accuracy of $66.84\\%$, significantly outperforming existing techniques. On CIFAR-10-LT with an imbalance ratio of $1000$ ($\\mathcal{T}=100$), our method achieves a tail-class accuracy of $38.99\\%$, substantially leading the next best. These results underscore our framework's potential to unlock novel insights from complex, imbalanced scientific datasets, thereby accelerating discovery",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6iRZvJiC9Q": {
    "title": "OpenCUA: Open Foundations for Computer-Use Agents",
    "volume": "spotlight",
    "abstract": "Vision-language models have demonstrated impressive capabilities as computer-use agents (CUAs) capable of automating diverse computer tasks. As their commercial potential grows, critical details of the most capable CUA systems remain closed. As these agents will increasingly mediate digital interactions and execute consequential decisions on our behalf, the research community needs access to open CUA frameworks to study their capabilities, limitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive open-source framework for scaling CUA data and foundation models. Our framework consists of: (1) an annotation infrastructure that seamlessly captures human computer-use demonstrations; (2) AgentNet, the first large-scale computer-use task dataset spanning 3 operating systems and 200+ applications and websites; (3) a scalable pipeline that transforms demonstrations into state–action pairs with reflective long Chain-of-Thought reasoning that sustain robust performance gains as data scales. Our end-to-end agent models demonstrate strong performance across CUA benchmarks. In particular, OpenCUA-72B achieves an average success rate of 45.0% on OSWorld‑Verified, establishing a new state-of-the-art (SOTA) among open-source models. Further analysis confirms that our approach generalizes well across domains and benefits significantly from increased test-time computation. We release our annotation tool, datasets, code, and models to build open foundations for further CUA research",
    "checked": true,
    "id": "b1995f97c41fc38df8b05e3f9e31f3a52dd56a33",
    "semantic_title": "opencua: open foundations for computer-use agents",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=opAU0pYlcP": {
    "title": "Near-Optimal Experiment Design in Linear non-Gaussian Cyclic Models",
    "volume": "spotlight",
    "abstract": "We study the problem of causal structure learning from a combination of observational and interventional data generated by a linear non-Gaussian structural equation model that might contain cycles. Recent results show that using mere observational data identifies the causal graph only up to a permutation-equivalence class. We obtain a combinatorial characterization of this class by showing that each equivalence class corresponds to a perfect matching in a bipartite graph. This bipartite representation allows us to analyze how interventions modify or constrain the matchings. Specifically, we show that each atomic intervention reveals one edge of the true matching and eliminates all incompatible causal graphs. Consequently, we formalize the optimal experiment design task as an adaptive stochastic optimization problem over the set of equivalence classes with a natural reward function that quantifies how many graphs are eliminated from the equivalence class by an intervention. We show that this reward function is adaptive submodular and provide a greedy policy with a provable near-optimal performance guarantee. A key technical challenge is to efficiently estimate the reward function without having to explicitly enumerate all the graphs in the equivalence class. We propose a sampling-based estimator using random matchings and analyze its bias and concentration behavior. Our simulation results show that performing a small number of interventions guided by our stochastic optimization framework recovers the true underlying causal structure",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7qrhHzZpTA": {
    "title": "Escaping saddle points without Lipschitz smoothness: the power of nonlinear preconditioning",
    "volume": "spotlight",
    "abstract": "We study generalized smoothness in nonconvex optimization, focusing on $(L_0, L_1)$-smoothness and anisotropic smoothness. The former was empirically derived from practical neural network training examples, while the latter arises naturally in the analysis of nonlinearly preconditioned gradient methods. We introduce a new sufficient condition that encompasses both notions, reveals their close connection, and holds in key applications such as phase retrieval and matrix factorization. Leveraging tools from dynamical systems theory, we then show that nonlinear preconditioning—including gradient clipping—preserves the saddle point avoidance property of classical gradient descent. Crucially, the assumptions required for this analysis are actually satisfied in these applications, unlike in classical results that rely on restrictive Lipschitz smoothness conditions. We further analyze a perturbed variant that efficiently attains second-order stationarity with only logarithmic dependence on dimension, matching similar guarantees of classical gradient methods",
    "checked": true,
    "id": "c38b5944b8b5b83324f43113f66c4b36323f657f",
    "semantic_title": "escaping saddle points without lipschitz smoothness: the power of nonlinear preconditioning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=XNo4yS9n1k": {
    "title": "Activation Control for Efficiently Eliciting Long Chain-of-thought Ability of Language Models",
    "volume": "spotlight",
    "abstract": "Despite the remarkable reasoning performance, eliciting the long chain-of-thought(CoT) ability in large language models(LLMs) typically requires costly reinforcement learning or supervised fine-tuning on high-quality distilled data. We investigate the internal mechanisms behind this capability and show that a small set of high-impact activations in the last few layers, greatly govern the long-form reasoning attributes, e.g. output length and self-reflection. Through simply amplifying these activations and adding ``wait'' tokens, the long CoT ability can be invoked without training, leading to significantly increased self-reflection rate and accuracy. In addition, we also find that the activation changes follow predictable trajectories, i.e. a sharp rise after special tokens and a subsequent exponential decay. Based on these insights, we introduce a general training-free activation control technique. It utilizes a few contrastive examples to identify the relevant activations, and then incorporates simple analytic functions to adjust their values at inference time to elicit long CoTs. Extensive experiments have verified the effectiveness of our methods in efficiently eliciting the long CoT ability of LLMs and improving the performance. Besides, we further propose a parameter-efficient fine-tuning method that trains only the last-layer activation amplification module and a few LoRA layers, outperforming LoRA on reasoning benchmarks with much fewer parameters. Our code and data will be fully public released",
    "checked": true,
    "id": "96d0c138741dbdf1a1be14df902fb5e2909ab81d",
    "semantic_title": "activation control for efficiently eliciting long chain-of-thought ability of language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=2h8bFmEQwh": {
    "title": "Direct Fisher Score Estimation for Likelihood Maximization",
    "volume": "spotlight",
    "abstract": "We study the problem of likelihood maximization when the likelihood function is intractable but model simulations are readily available. We propose a sequential, gradient-based optimization method that directly models the Fisher score based on a local score matching technique which uses simulations from a localized region around each parameter iterate. By employing a linear parameterization for the surrogate score model, our technique admits a closed-form, least-squares solution. This approach yields a fast, flexible, and efficient approximation to the Fisher score, effectively smoothing the likelihood objective and mitigating the challenges posed by complex likelihood landscapes. We provide theoretical guarantees for our score estimator, including bounds on the bias introduced by the smoothing. Empirical results on a range of synthetic and real-world problems demonstrate the superior performance of our method compared to existing benchmarks",
    "checked": true,
    "id": "7685708b2a767933a915a9d310db40aac80f5991",
    "semantic_title": "direct fisher score estimation for likelihood maximization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xKmlBQhgI4": {
    "title": "Path-Enhanced Contrastive Learning for Recommendation",
    "volume": "spotlight",
    "abstract": "Collaborative filtering (CF) methods are now facing the challenge of data sparsity in recommender systems. In order to reduce the effect of data sparsity, researchers proposed contrastive learning methods to extract self-supervised signals from raw data. Contrastive learning methods address this problem by graph augmentation and maximizing the consistency of node representations between different augmented graphs. However, these methods tends to unintentionally distance the target node from its path nodes on the interaction path, thus limiting its effectiveness. In this regard, we propose a solution that uses paths as samples in the contrastive loss function. In order to obtain the path samples, we design a path sampling method. In addition to the contrast of the relationship between the target node and the nodes within the path (intra-path contrast), we also designed a method of contrasting the relationship between the paths (inter-path contrast) to better pull the target node and its path nodes closer to each other. We use Simplifying and Powering Graph Convolution Network (LightGCN) as the basis and combine with a new path-enhanced graph approach proposed for graph augmentation. It effectively improves the performance of recommendation models. Our proposed Path Enhanced Contrastive Loss (PECL) model replaces the common contrastive loss function with our novel loss function, showing significant performance improvement. Experiments on three real-world datasets demonstrate the effectiveness of our model",
    "checked": false,
    "id": "b96dfa5cf42ffc48a2d82e363f369f1cd4a9d73f",
    "semantic_title": "hypergraph enhanced contrastive learning for news recommendation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=jvObbvshjE": {
    "title": "T-REGS: Minimum Spanning Tree Regularization for Self-Supervised Learning",
    "volume": "spotlight",
    "abstract": "Self-supervised learning (SSL) has emerged as a powerful paradigm for learning representations without labeled data, often by enforcing invariance to input transformations such as rotations or blurring. Recent studies have highlighted two pivotal properties for effective representations: (i) avoiding dimensional collapse-where the learned features occupy only a low-dimensional subspace, and (ii) enhancing uniformity of the induced distribution. In this work, we introduce T-REGS, a simple regularization framework for SSL based on the length of the Minimum Spanning Tree (MST) over the learned representation. We provide theoretical analysis demonstrating that T-REGS simultaneously mitigates dimensional collapse and promotes distribution uniformity on arbitrary compact Riemannian manifolds. Several experiments on synthetic data and on classical SSL benchmarks validate the effectiveness of our approach at enhancing representation quality",
    "checked": true,
    "id": "881a46e531cd7107660e852cf01842b6737a7f9b",
    "semantic_title": "t-regs: minimum spanning tree regularization for self-supervised learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kfB5Ciz2XZ": {
    "title": "Generating Informative Samples for Risk-Averse Fine-Tuning of Downstream Tasks",
    "volume": "spotlight",
    "abstract": "Risk-averse modeling is critical in safety-sensitive and high-stakes applications. Conditional Value-at-Risk (CVaR) quantifies such risk by measuring the expected loss in the tail of the loss distribution, and minimizing it provides a principled framework for training robust models. However, direct CVaR minimization remains challenging due to the difficulty of accurately estimating rare, high-loss events—particularly at extreme quantiles. In this work, we propose a novel training framework that synthesizes informative samples for CVaR optimization using score-based generative models. Specifically, we guide a diffusion-based generative model to sample from a reweighted distribution that emphasizes inputs likely to incur high loss under a pretrained reference model. These samples are then incorporated via a loss-weighted importance sampling scheme to reduce noise in stochastic optimization. We establish convergence guarantees and show that the synthesized, high-loss-emphasized dataset substantially contributes to the noise reduction. Empirically, we validate the effectiveness of our approach across multiple settings, including a real-world wireless channel compression task, where our method achieves significant improvements over standard risk minimization strategies",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jSgCM0uZn3": {
    "title": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play",
    "volume": "spotlight",
    "abstract": "Search-augmented LLMs often struggle with complex reasoning tasks due to ineffective multi-hop retrieval and limited reasoning ability. We propose AceSearcher, a cooperative self-play framework that trains a single large language model (LLM) to alternate between two roles: a decomposer that breaks down complex queries and a solver that integrates retrieved contexts for answer generation. AceSearcher couples supervised fine-tuning on a diverse mixture of search, reasoning, and decomposition tasks with reinforcement fine-tuning optimized for final answer accuracy, eliminating the need for intermediate annotations. Extensive experiments on three reasoning-intensive tasks across 10 datasets show that AceSearcher outperforms state-of-the-art baselines, achieving an average exact match improvement of 7.6%. Remarkably, on document-level finance reasoning tasks, AceSearcher-32B matches the performance of the giant DeepSeek-V3 model using less than 5% of iits parameters. Even at smaller scales (1.5B and 8B), AceSearcher often surpasses existing search-augmented LLMs with up to 9× more parameters, highlighting its exceptional efficiency and effectiveness in tackling complex reasoning tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T9qNDtvAJX": {
    "title": "DeltaFlow: An Efficient Multi-frame Scene Flow Estimation Method",
    "volume": "spotlight",
    "abstract": "Previous dominant methods for scene flow estimation focus mainly on input from two consecutive frames, neglecting valuable information in the temporal domain. While recent trends shift towards multi-frame reasoning, they suffer from rapidly escalating computational costs as the number of frames grows. To leverage temporal information more efficiently, we propose DeltaFlow ($\\Delta$Flow), a lightweight 3D framework that captures motion cues via a $\\Delta$ scheme, extracting temporal features with minimal computational cost, regardless of the number of frames. Additionally, scene flow estimation faces challenges such as imbalanced object class distributions and motion inconsistency. To tackle these issues, we introduce a Category-Balanced Loss to enhance learning across underrepresented classes and an Instance Consistency Loss to enforce coherent object motion, improving flow accuracy. Extensive evaluations on the Argoverse 2, Waymo and nuScenes datasets show that $\\Delta$Flow achieves state-of-the-art performance with up to 22\\% lower error and $2\\times$ faster inference compared to the next-best multi-frame supervised method, while also demonstrating a strong cross-domain generalization ability. The code is open-sourced at https://github.com/Kin-Zhang/DeltaFlow along with trained model weights",
    "checked": true,
    "id": "cf603bcbf7c82abda624aaa71cd16f78efda4bc0",
    "semantic_title": "deltaflow: an efficient multi-frame scene flow estimation method",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G2kMroO9UV": {
    "title": "Web-Shepherd: Advancing PRMs for Reinforcing Web Agents",
    "volume": "spotlight",
    "abstract": "Web navigation is a unique domain that can automate many repetitive real-life tasks and is challenging as it requires long-horizon sequential decision making beyond typical multimodal large language model (MLLM) tasks. Yet, specialized reward models for web navigation that can be utilized during both training and test-time have been absent until now. Despite the importance of speed and cost-effectiveness, prior works have utilized MLLMs as reward models, which poses significant constraints for real-world deployment. To address this, in this work, we propose the first process reward model (PRM) called Web-Shepherd which could assess web navigation trajectories in a step-level. To achieve this, we first construct the WebPRM Collection, a large-scale dataset with 40K step-level preference pairs and annotated checklists spanning diverse domains and difficulty levels. Next, we also introduce the WebRewardBench, the first meta-evaluation benchmark for evaluating PRMs. In our experiments, we observe that our Web-Shepherd achieves about 30 points better accuracy compared to using GPT-4o on WebRewardBench. Furthermore, when testing on WebArena-lite by using GPT-4o-mini as the policy and Web-Shepherd as the verifier, we achieve 10.9 points better performance, in 10x less cost compared to using GPT-4o-mini as the verifier. Our model, dataset, and code are publicly available at https://github.com/kyle8581/Web-Shepherd",
    "checked": true,
    "id": "a9a7e8d40bc8988cccd115378eebebe34472dcd6",
    "semantic_title": "web-shepherd: advancing prms for reinforcing web agents",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=19ygs48nOa": {
    "title": "How do Transformers Learn Implicit Reasoning?",
    "volume": "spotlight",
    "abstract": "Recent work suggests that large language models (LLMs) can perform multi-hop reasoning implicitly---producing correct answers without explicitly verbalizing intermediate steps---but the underlying mechanisms remain poorly understood. In this paper, we study how such implicit reasoning emerges by training transformers from scratch in a controlled symbolic environment. Our analysis reveals a three-stage developmental trajectory: early memorization, followed by in-distribution generalization, and eventually cross-distribution generalization. We find that training with atomic triples is not necessary but accelerates learning, and that second-hop generalization relies on query-level exposure to specific compositional structures. To interpret these behaviors, we introduce two diagnostic tools: cross-query semantic patching, which identifies semantically reusable intermediate representations, and a cosine-based representational lens, which reveals that successful reasoning correlates with the cosine-base clustering in hidden space. This clustering phenomenon in turn provides a coherent explanation for the behavioral dynamics observed across training, linking representational structure to reasoning capability. These findings provide new insights into the interpretability of implicit multi-hop reasoning in LLMs, helping to clarify how complex reasoning processes unfold internally and offering pathways to enhance the transparency of such models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IrgQe6YjKm": {
    "title": "On the sample complexity of semi-supervised multi-objective learning",
    "volume": "spotlight",
    "abstract": "In multi-objective learning (MOL), several possibly competing prediction tasks must be solved jointly by a single model. Achieving good trade-offs may require a model class $\\mathcal{G}$ with larger capacity than what is necessary for solving the individual tasks. This, in turn, increases the statistical cost, as reflected in known MOL bounds that depend on the complexity of $\\mathcal{G}$. We show that this cost is unavoidable for some losses, even in an idealized semi-supervised setting, where the learner has access to the Bayes-optimal solutions for the individual tasks as well as the marginal distributions over the covariates. On the other hand, for objectives defined with Bregman losses, we prove that the complexity of $\\mathcal{G}$ may come into play only in terms of unlabeled data. Concretely, we establish sample complexity upper bounds, showing precisely when and how unlabeled data can significantly alleviate the need for labeled data. This is achieved by a simple pseudo-labeling algorithm",
    "checked": true,
    "id": "4a6cca33df192e79c0e42589dd9bf20c9f62a59d",
    "semantic_title": "on the sample complexity of semi-supervised multi-objective learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5eZ0iykpDU": {
    "title": "Diversity-Aware Policy Optimization for Large Language Model Reasoning",
    "volume": "spotlight",
    "abstract": "The reasoning capabilities of large language models (LLMs) have advanced rapidly, particularly following the release of DeepSeek-R1, which has inspired a surge of research into data quality and reinforcement learning (RL) algorithms. Despite the pivotal role diversity plays in RL, its influence on LLM reasoning remains largely underexplored. To bridge this gap, this work presents a systematic investigation into the impact of diversity in RL-based training for LLM reasoning, and proposes a novel diversity-aware policy optimization method. Across evaluations on 12 LLMs, we observe a strong positive correlation between the solution diversity and potential@k (a novel metric quantifying an LLM's reasoning potential) in high-performing models. This finding motivates our method to explicitly promote diversity during RL training. Specifically, we design a token-level diversity and reformulate it into a practical objective, then we selectively apply it to positive samples. Integrated into the R1-zero training framework, our method achieves a 3.5\\% average improvement across four mathematical reasoning benchmarks, while generating more diverse and robust solutions",
    "checked": true,
    "id": "4a0be5039b2d462fedafec282ac19dce5746dad8",
    "semantic_title": "diversity-aware policy optimization for large language model reasoning",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=KT8y9pFgJE": {
    "title": "Fixed-Point RNNs: Interpolating from Diagonal to Dense",
    "volume": "spotlight",
    "abstract": "Linear recurrent neural networks (RNNs) and state-space models (SSMs) such as Mamba have become promising alternatives to softmax-attention as sequence mixing layers in Transformer architectures. Current models, however, do not exhibit the full state-tracking expressivity of RNNs because they rely on channel-wise (i.e. diagonal) sequence mixing. In this paper, we investigate parameterizations of a large class of dense linear RNNs as fixed-points of parallelizable diagonal linear RNNs. The resulting models can naturally trade expressivity for efficiency at a fixed number of parameters and achieve state-of-the-art results on the state-tracking benchmarks $A_5$ and $S_5$, while matching performance on copying and other tasks",
    "checked": true,
    "id": "05efdded456f2267227c32be65a554c6e598d0a7",
    "semantic_title": "fixed-point rnns: interpolating from diagonal to dense",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WYnvP3DePZ": {
    "title": "Bridging Theory and Practice in Link Representation with Graph Neural Networks",
    "volume": "spotlight",
    "abstract": "Graph Neural Networks (GNNs) are widely used to compute representations of node pairs for downstream tasks such as link prediction. Yet, theoretical understanding of their expressive power has focused almost entirely on graph-level representations. In this work, we shift the focus to links and provide the first comprehensive study of GNN expressiveness in link representation. We introduce a unifying framework, the $k_\\phi$-$k_\\rho$-$m$ framework, that subsumes existing message-passing link models and enables formal expressiveness comparisons. Using this framework, we derive a hierarchy of state-of-the-art methods and offer theoretical tools to analyze future architectures. To complement our analysis, we propose a synthetic evaluation protocol comprising the first benchmark specifically designed to assess link-level expressiveness. Finally, we ask: does expressiveness matter in practice? We use a graph symmetry metric that quantifies the difficulty of distinguishing links and show that while expressive models may underperform on standard benchmarks, they significantly outperform simpler ones as symmetry increases, highlighting the need for dataset-aware model selection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3P3PL7aCXM": {
    "title": "ErrorTrace: A Black-Box Traceability Mechanism Based on Model Family Error Space",
    "volume": "spotlight",
    "abstract": "The open-source release of large language models (LLMs) enables malicious users to create unauthorized derivative models at low cost, posing significant threats to intellectual property (IP) and market stability. Existing IP protection methods either require access to model parameters or are vulnerable to fine-tuning attacks. To fill this gap, we propose ErrorTrace, a robust and black-box traceability mechanism for protecting LLM IP. Specifically, ErrorTrace leverages the unique error patterns of model families by mapping and analyzing their distinct error spaces, enabling robust and efficient IP protection without relying on internal parameters or specific query responses. Experimental results show that ErrorTrace achieves a traceability accuracy of 0.8518 for 27 base models when the suspect model is not included in ErrorTrace's training set, outperforming the baseline by 0.2593. Additionally,ErrorTrace successfully tracks 34 fine-tuned, pruned and merged models across various scenarios, demonstrating its broad applicability and robustness. In addition, ErrorTrace shows a certain level of resilience when subjected to adversarial attacks. Our code is available at: https://github.com/csdatazcc/ErrorTrace",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MpJkAzwUtl": {
    "title": "Protein Design with Dynamic Protein Vocabulary",
    "volume": "spotlight",
    "abstract": "Protein design is a fundamental challenge in biotechnology, aiming to design novel sequences with specific functions within the vast space of possible proteins. Recent advances in deep generative models have enabled function-based protein design from textual descriptions, yet struggle with structural plausibility. Inspired by classical protein design methods that leverage natural protein structures, we explore whether incorporating fragments from natural proteins can enhance foldability in generative models. Our empirical results show that even random incorporation of fragments improves foldability. Building on this insight, we introduce ProDVa, a novel protein design approach that integrates a text encoder for functional descriptions, a protein language model for designing proteins, and a fragment encoder to dynamically retrieve protein fragments based on textual functional descriptions. Experimental results demonstrate that our approach effectively designs protein sequences that are both functionally aligned and structurally plausible. Compared to state-of-the-art models, ProDVa achieves comparable function alignment using less than 0.04% of the training data, while designing significantly more well-folded proteins, with the proportion of proteins having pLDDT above 70 increasing by 7.38% and those with PAE below 10 increasing by 9.62%",
    "checked": true,
    "id": "8728223d2918f4e45f06bd0cf1a67c9b506e1a0b",
    "semantic_title": "protein design with dynamic protein vocabulary",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=VaC4sa96EI": {
    "title": "Towards Reliable Code-as-Policies: A Neuro-Symbolic Framework for Embodied Task Planning",
    "volume": "spotlight",
    "abstract": "Recent advances in large language models (LLMs) have enabled the automatic generation of executable code for task planning and control in embodied agents such as robots, demonstrating the potential of LLM-based embodied intelligence. However, these LLM-based code-as-policies approaches often suffer from limited environmental grounding, particularly in dynamic or partially observable settings, leading to suboptimal task success rates due to incorrect or incomplete code generation. In this work, we propose a neuro-symbolic embodied task planning framework that incorporates explicit symbolic verification and interactive validation processes during code generation. In the validation phase, the framework generates exploratory code that actively interacts with the environment to acquire missing observations while preserving task-relevant states. This integrated process enhances the grounding of generated code, resulting in improved task reliability and success rates in complex environments. We evaluate our framework on RLBench and in real-world settings across dynamic, partially observable scenarios. Experimental results demonstrate that our framework improves task success rates by 46.2\\% over Code as Policies baselines and attains over 86.8\\% executability of task-relevant actions, thereby enhancing the reliability of task planning in dynamic environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kQWyOYUAC4": {
    "title": "AI-Researcher: Autonomous Scientific Innovation",
    "volume": "spotlight",
    "abstract": "The powerful reasoning capabilities of Large Language Models (LLMs) in mathematics and coding, combined with their ability to automate complex tasks through agentic frameworks, present unprecedented opportunities for accelerating scientific innovation. In this paper, we introduce AI-Researcher, a fully autonomous research system that transforms how AI-driven scientific discovery is conducted and evaluated. Our framework seamlessly orchestrates the complete research pipeline--from literature review and hypothesis generation to algorithm implementation and publication-ready manuscript preparation--with minimal human intervention. To rigorously assess autonomous research capabilities, we develop Scientist-Bench, a comprehensive benchmark comprising state-of-the-art papers across diverse AI research domains, featuring both guided innovation and open-ended exploration tasks. Through extensive experiments, we demonstrate that AI-Researcher achieves remarkable implementation success rates and produces research papers that approach human-level quality. This work establishes new foundations for autonomous scientific innovation that can complement human researchers by systematically exploring solution spaces beyond cognitive limitations",
    "checked": true,
    "id": "80a0b76dedc4c3e3d365bbaececcd44a996eb38b",
    "semantic_title": "ai-researcher: autonomous scientific innovation",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=KrglRiOKYT": {
    "title": "Abstain Mask Retain Core: Time Series Prediction by Adaptive Masking Loss with Representation Consistency",
    "volume": "spotlight",
    "abstract": "Time series forecasting plays a pivotal role in critical domains such as energy management and financial markets. Although deep learning-based approaches (e.g., MLP, RNN, Transformer) have achieved remarkable progress, the prevailing \"long-sequence information gain hypothesis\" exhibits inherent limitations. Through systematic experimentation, this study reveals a counterintuitive phenomenon: appropriately truncating historical data can paradoxically enhance prediction accuracy, indicating that existing models learn substantial redundant features (e.g., noise or irrelevant fluctuations) during training, thereby compromising effective signal extraction. Building upon information bottleneck theory, we propose an innovative solution termed Adaptive Masking Loss with Representation Consistency (AMRC), which features two core components: 1) Dynamic masking loss, which adaptively identified highly discriminative temporal segments to guide gradient descent during model training; 2) Representation consistency constraint, which stabilized the mapping relationships among inputs, labels, and predictions. Experimental results demonstrate that AMRC effectively suppresses redundant feature learning while significantly improving model performance. This work not only challenges conventional assumptions in temporal modeling but also provides novel theoretical insights and methodological breakthroughs for developing efficient and robust forecasting models. We have made our code available at \\url{https://github.com/MazelTovy/AMRC}",
    "checked": true,
    "id": "9c98c33d3290f6a59db70f4f684fefdb72dd35e0",
    "semantic_title": "abstain mask retain core: time series prediction by adaptive masking loss with representation consistency",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a49F7EAm6l": {
    "title": "DexFlyWheel: A Scalable and Self-improving Data Generation Framework for Dexterous Manipulation",
    "volume": "spotlight",
    "abstract": "Dexterous manipulation is critical for advancing robot capabilities in real-world applications, yet diverse and high-quality datasets remain scarce. Existing data collection methods either rely on human teleoperation or require significant human engineering, or generate data with limited diversity, which restricts their scalability and generalization. In this paper, we introduce DexFlyWheel, a scalable data generation framework that employs a self-improving cycle to continuously enrich data diversity. Starting from efficient seed demonstrations warmup, DexFlyWheel expands the dataset through iterative cycles. Each cycle follows a closed-loop pipeline that integrates Imitation Learning (IL), residual Reinforcement Learning (RL), rollout trajectory collection, and data augmentation. Specifically, IL extracts human-like behaviors from demonstrations, and residual RL enhances policy generalization. The learned policy is then used to generate trajectories in simulation, which are further augmented across diverse environments and spatial configurations before being fed back into the next cycle. Over successive iterations, a self-improving data flywheel effect emerges, producing datasets that cover diverse scenarios and thereby scaling policy performance. Experimental results demonstrate that DexFlyWheel generates over 2,000 diverse demonstrations across four challenging tasks. Policies trained on our dataset achieve an average success rate of 81.9\\% on the challenge test sets and successfully transfer to the real world through digital twin, achieving a 78.3\\% success rate on dual-arm lift tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7G9YKty2UZ": {
    "title": "Learning Robust Vision-Language Models from Natural Latent Spaces",
    "volume": "spotlight",
    "abstract": "Pre-trained vision-language models (VLMs) exhibit significant vulnerability to imperceptible adversarial perturbations. Current advanced defense strategies typically employ adversarial prompt tuning to improve the adversarial robustness of VLMs, which struggle to simultaneously maintain generalization across both natural and adversarial examples under different benchmarks and downstream tasks. We propose a collaborative adversarial prompt tuning (CoAPT) approach from pre-trained VLMs to target robust VLMs. Inspired by the image mask modeling, we adopt an improved real-time total variation algorithm to suppress and eliminate high-frequency details from images while preserving edge structures, thereby disrupting the adversarial perturbation space. Subsequently, guided by the high-level image and text representations in the latent space of the pre-trained VLMs, the corrupted natural features are restored while inheriting the superior generalization capability. Experiments on four benchmarks demonstrate that CoAPT achieves an excellent trade-off among natural generalization, adversarial robustness, and task-specific adaptation compared to state-of-the-art methods",
    "checked": false,
    "id": "2cf8ee0c947f7566572fae4f7d439bc8a4d86f42",
    "semantic_title": "reducing hallucinations in vision-language models via latent space steering",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=xwqTt26NJf": {
    "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
    "volume": "spotlight",
    "abstract": "The generation speed of LLMs are bottlenecked by autoregressive decoding, where tokens are predicted sequentially one by one. Alternatively, diffusion large language models (dLLMs) theoretically allow for parallel token generation, but in practice struggle to achieve the speed of autoregressive models without significantly sacrificing quality. We therefore introduce adaptive parallel decoding (APD), a novel method that dynamically adjusts the number of tokens sampled in parallel. We achieve this by defining a multiplicative mixture between the dLLM marginal probabilities and the joint probability of sequences under a small auxiliary autoregressive model. This inverts the standard setup of speculative decoding, where the goal is to sample from a large autoregressive verifier by drafting from a smaller model. We further optimize APD by enabling KV caching and limiting the size of the masked input. Altogether, our method puts forward three tunable parameters to flexibly tradeoff throughput and quality. We show that APD provides markedly higher throughput with minimal quality degradations on downstream benchmarks",
    "checked": true,
    "id": "f8dcb5c2f1a90806459d7bed4410a27b475c78ec",
    "semantic_title": "accelerating diffusion llms via adaptive parallel decoding",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=AwLRF1lZvI": {
    "title": "Inner Speech as Behavior Guides: Steerable Imitation of Diverse Behaviors for Human-AI coordination",
    "volume": "spotlight",
    "abstract": "Effective human-AI coordination requires artificial agents capable of exhibiting and responding to human-like behaviors while adapting to changing contexts. Imitation learning has emerged as one of the prominent approaches to build such agents by training them to mimic human-demonstrated behaviors. However, current methods struggle to capture the inherent diversity and non-Markovian nature of human behavior and lack the ability to steer behavior at inference time. Drawing inspiration from the theory of human cognitive processes, where inner speech guides action selection before execution, we propose MIMIC (Modeling Inner Motivations for Imitation and Control), a framework that uses language as an internal representation of behavioral intent. MIMIC employs the novel use of vision-language models as linguistic scaffolding to train a conditional variational autoencoder capable of generating inner speech from observations. A diffusion-based behavior cloning policy then selects actions conditioned on current observations and the generated inner speech. MIMIC enables fine-grained steering of behavior at inference time by conditioning the agent on behavior-specific speech. Experiments across robotic manipulation tasks and human-AI collaboration games demonstrate that MIMIC significantly enhances both behavior diversity and fidelity to human demonstrations while enabling nuanced behavioral steering without training on additional demonstrations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W6WC6047X2": {
    "title": "Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems",
    "volume": "spotlight",
    "abstract": "Reinforcement Learning algorithms commonly sample multiple ($n>1$) solution attempts for each problem and reward them independently. This optimizes for pass@1 performance and prioritizes individual sample performance over the diversity and collective utility of a set of samples. Such algorithms under-utilize the sampling capacity, limiting exploration and eventual improvement on harder examples. As a fix, we propose Pass-at-$k$ Policy Optimization (PKPO), a multivariate transformation on batches of rewards which leads to direct optimization of \\passk\\ performance, thus optimizing for sets of samples that feature a large maximum reward when considered jointly. Our primary contribution is to derive novel low variance unbiased estimators for the pass@k and its gradient, in both the binary and continuous reward settings. We show that optimizing with these estimators reduces to reinforcement learning with (batches of) rewards that have been jointly transformed by a function that is stable and efficient to compute. While previous efforts propose transformations for $k=n$, our transformations are the first to enable robust optimization of the pass@k for any arbitrary $k \\leq n$. Rather than simply trading off pass@1 performance for pass@k gains, our method allows annealing $k$ during training, optimizing both metrics and often achieving strong pass@1 performance alongside significant pass@k gains. We validate our transformations on illustrative toy experiments, which reveal the variance reducing properties of our formulations. We also include real-world examples using the open-source models Gemma and Llama . We find that our transformation effectively optimizes for the target $k$. Furthermore, higher $k$ values enable solving more and harder problems, while annealing $k$ boosts both the pass@1 and pass@k. Crucially, for challenging task sets where conventional pass@1 optimization stalls, our pass@k approach unblocks learning, likely by improving exploration through the prioritization of joint utility over the utility of individual samples",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z4AMrCOetn": {
    "title": "LogicTree: Improving Complex Reasoning of LLMs via Instantiated Multi-step Synthetic Logical Data",
    "volume": "spotlight",
    "abstract": "Despite their remarkable performance on various tasks, Large Language Models (LLMs) still struggle with logical reasoning, particularly in complex and multi-step reasoning processes. Among various efforts to enhance LLMs' reasoning capabilities, synthesizing large-scale, high-quality logical reasoning datasets has emerged as a promising direction. However, existing methods often rely on predefined templates for logical reasoning data generation, limiting their adaptability to real-world scenarios. To address the limitation, we propose **LogicTree**, a novel framework for efficiently synthesizing multi-step logical reasoning dataset that excels in both complexity and instantiation. By iteratively searching for applicable logic rules based on structural pattern matching to perform backward deduction, **LogicTree** constructs multi-step logic trees that capture complex reasoning patterns. Furthermore, we employ a two-stage LLM-based approach to instantiate various real-world scenarios for each logic tree, generating consistent real-world reasoning processes that carry contextual significance. This helps LLMs develop generalizable logical reasoning abilities across diverse scenarios rather than merely memorizing templates. Experiments on multiple benchmarks demonstrate that our approach achieves an average improvement of 9.4\\% in accuracy on complex logical reasoning tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9k9ZsDs9Vc": {
    "title": "Multitask Learning with Stochastic Interpolants",
    "volume": "spotlight",
    "abstract": "We propose a framework for learning maps between probability distributions that broadly generalizes the time dynamics of flow and diffusion models. To enable this, we generalize stochastic interpolants by replacing the scalar time variable with vectors, matrices, or linear operators, allowing us to bridge probability distributions across multiple dimensional spaces. This approach enables the construction of versatile generative models capable of fulfilling multiple tasks without task-specific training. Our operator-based interpolants not only provide a unifying theoretical perspective for existing generative models but also extend their capabilities. Through numerical experiments, we demonstrate the zero-shot efficacy of our method on conditional generation and inpainting, fine-tuning and posterior sampling, and multiscale modeling, suggesting its potential as a generic task-agnostic alternative to specialized models",
    "checked": true,
    "id": "54af9e0947060a443d10cd6d7d40abbd38e7d849",
    "semantic_title": "multitask learning with stochastic interpolants",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RSVdHXZN6D": {
    "title": "FUDOKI: Discrete Flow-based Unified Understanding and Generation via Kinetic-Optimal Velocities",
    "volume": "spotlight",
    "abstract": "The rapid progress of large language models (LLMs) has catalyzed the emergence of multimodal large language models (MLLMs) that unify visual understanding and image generation within a single framework. However, most existing MLLMs rely on autoregressive (AR) architectures, which impose inherent limitations on future development, such as the raster-scan order in image generation and restricted reasoning abilities in causal context modeling. In this work, we challenge the dominance of AR-based approaches by introducing FUDOKI, a unified multimodal model purely based on discrete flow matching, as an alternative to conventional AR paradigms. By leveraging metric-induced probability paths with kinetic optimal velocities, our framework goes beyond the previous masking-based corruption process, enabling iterative refinement with self-correction capability and richer bidirectional context integration during generation. To mitigate the high cost of training from scratch, we initialize FUDOKI from pre-trained AR-based MLLMs and adaptively transition to the discrete flow matching paradigm. Experimental results show that FUDOKI achieves performance comparable to state-of-the-art AR-based MLLMs across both visual understanding and image generation tasks, highlighting its potential as a foundation for next-generation unified multimodal models. Furthermore, we show that applying test-time scaling techniques to FUDOKI yields significant performance gains, further underscoring its promise for future enhancement through reinforcement learning",
    "checked": true,
    "id": "17fdf94362b3ae856f67edafd6a8532dc87642ee",
    "semantic_title": "fudoki: discrete flow-based unified understanding and generation via kinetic-optimal velocities",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=ugBmWX3H1R": {
    "title": "Fast MRI for All: Bridging Access Gaps by Training without Raw Data",
    "volume": "spotlight",
    "abstract": "Physics-driven deep learning (PD-DL) approaches have become popular for improved reconstruction of fast magnetic resonance imaging (MRI) scans. Though PD-DL offers higher acceleration rates than existing clinical fast MRI techniques, their use has been limited outside specialized MRI centers. A key challenge is generalization to rare pathologies or different populations, noted in multiple studies, with fine-tuning on target populations suggested for improvement. However, current approaches for PD-DL training require access to raw k-space measurements, which is typically only available at specialized MRI centers that have research agreements for such data access. This is especially an issue for rural and under-resourced areas, where commercial MRI scanners only provide access to a final reconstructed image. To tackle these challenges, we propose Compressibility-inspired Unsupervised Learning via Parallel Imaging Fidelity (CUPID) for high-quality PD-DL training using only routine clinical reconstructed images exported from an MRI scanner. CUPID evaluates output quality with a compressibility-based approach while ensuring that the output stays consistent with the clinical parallel imaging reconstruction through well-designed perturbations. Our results show CUPID achieves similar quality to established PD-DL training that requires k-space data while outperforming compressed sensing (CS) and diffusion-based generative methods. We further demonstrate its effectiveness in a zero-shot training setup for retrospectively and prospectively sub-sampled acquisitions, attesting to its minimal training burden. As an approach that radically deviates from existing strategies, CUPID presents an opportunity to provide broader access to fast MRI for remote and rural populations in an attempt to reduce the obstacles associated with this expensive imaging modality. Code is available at https://github.com/ualcalar17/CUPID",
    "checked": true,
    "id": "d68ce589cbd3d451ba0c108104afd651d94fe897",
    "semantic_title": "fast mri for all: bridging access gaps by training without raw data",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=cb0xbZ3APM": {
    "title": "Knowledge Insulating Vision-Language-Action Models: Train Fast, Run Fast, Generalize Better",
    "volume": "spotlight",
    "abstract": "Vision-language-action (VLA) models provide a powerful approach to training control policies for physical systems, such as robots, by combining end-to-end learning with transfer of semantic knowledge from web-scale vision-language model (VLM) training. However, the constraints of real-time control are often at odds with the design of VLMs: the most powerful VLMs have tens or hundreds of billions of parameters, presenting an obstacle to real-time inference, and operate on discrete tokens rather than the continuous-valued outputs that are required for controlling robots. To address this challenge, recent VLA models have used specialized modules for efficient continuous control, such as action experts or continuous output heads, which typically require adding new untrained parameters to the pretrained VLM backbone. While these modules improve real-time and control capabilities, it remains an open question whether they preserve or degrade the semantic knowledge contained in the pretrained VLM, and what effect they have on the VLA training dynamics. In this paper, we study this question in the context of VLAs that include a continuous diffusion or flow matching action expert, showing that naively including such experts significantly harms both training speed and knowledge transfer. We provide an extensive analysis of various design choices, their impact on performance and knowledge transfer, and propose a technique for insulating the VLM backbone during VLA training that mitigates this issue. Videos are available at https://pi.website/research/knowledge_insulation and open-source model weights are available at https://github.com/Physical-Intelligence/openpi",
    "checked": true,
    "id": "f8a3331421b98c7d4065ecf55c4013e6306359f2",
    "semantic_title": "knowledge insulating vision-language-action models: train fast, run fast, generalize better",
    "citation_count": 29,
    "authors": []
  },
  "https://openreview.net/forum?id=4f6mEr1DQs": {
    "title": "Complete Structure Guided Point Cloud Completion via Cluster- and Instance-Level Contrastive Learning",
    "volume": "spotlight",
    "abstract": "Point cloud completion, aiming to reconstruct missing part from incomplete point clouds, is a pivotal task in 3D computer vision. Traditional supervised approaches often necessitate complete point clouds for training supervision, which are not readily accessible in real-world applications. Recent studies have attempted to mitigate this dependency by employing self-supervise mechanisms. However, these approaches frequently yield suboptimal results due to the absence of complete structure in the point cloud data during training. To address these issues, in this paper, we propose an effective framework to complete the point cloud under the guidance of self learned complete structure. A key contribution of our work is the development of a novel self-supervised complete structure reconstruction module, which can learn the complete structure explicitly from incomplete point clouds and thus eliminate the reliance on training data from complete point clouds. Additionally, we introduce a contrastive learning approach at both the cluster- and instance-level to extract shape features guided by the complete structure and to capture style features, respectively. This dual-level learning design ensures that the generated point clouds are both shape-completed and detail-preserving. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach significantly outperforms state-of-the-art self-supervised methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P2yIMJP5b1": {
    "title": "ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis Optimization for Speech Multi-Metric Estimation",
    "volume": "spotlight",
    "abstract": "Speech signal analysis poses significant challenges, particularly in tasks such as speech quality evaluation and profiling, where the goal is to predict multiple perceptual and objective metrics. For instance, metrics like PESQ (Perceptual Evaluation of Speech Quality), STOI (Short-Time Objective Intelligibility), and MOS (Mean Opinion Score) each capture different aspects of speech quality. However, these metrics often have different scales, assumptions, and dependencies, making joint estimation non-trivial. To address these issues, we introduce ARECHO (Autoregressive Evaluation via Chain-based Hypothesis Optimization), a chain-based, versatile evaluation system for speech assessment grounded in autoregressive dependency modeling. ARECHO is distinguished by three key innovations: (1) a comprehensive speech information tokenization pipeline; (2) a dynamic classifier chain that explicitly captures inter-metric dependencies; and (3) a two-step confidence-oriented decoding algorithm that enhances inference reliability. Experiments demonstrate that ARECHO significantly outperforms the baseline framework across diverse evaluation scenarios, including enhanced speech analysis, speech generation evaluation, and noisy speech evaluation. Furthermore, its dynamic dependency modeling improves interpretability by capturing inter-metric relationships. Across tasks, ARECHO offers reference-free evaluation using its dynamic classifier chain to support subset queries (single or multiple metrics) and reduces error propagation via confidence-oriented decoding",
    "checked": true,
    "id": "23dd19df30a52fb27fa7cfff1ea2d03732308360",
    "semantic_title": "arecho: autoregressive evaluation via chain-based hypothesis optimization for speech multi-metric estimation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=crczm2smVo": {
    "title": "Projective Equivariant Networks via Second-order Fundamental Differential Invariants",
    "volume": "spotlight",
    "abstract": "Equivariant networks enhance model efficiency and generalization by embedding symmetry priors into their architectures. However, most existing methods, primarily based on group convolutions and steerable convolutions, face significant limitations when dealing with complex transformation groups, particularly the projective group, which plays a crucial role in vision. In this work, we tackle the challenge by constructing projective equivariant networks based on differential invariants. Using the moving frame method with a carefully selected cross section tailored for multi-dimensional functions, we derive a complete and concise set of second-order fundamental differential invariants of the projective group. We provide a rigorous analysis of the properties and transformation relationships of their underlying components, yielding a further simplified and unified set of fundamental differential invariants, which facilitates both theoretical analysis and practical applications. Building on this foundation, we develop the first deep projective equivariant networks, PDINet, which achieve full projective equivariance without discretizing or sampling the group. Empirical results on the projectively transformed STL-10 and Imagenette datasets show that PDINet achieves improvements of 11.39\\% and 5.66\\% in accuracy over the respective standard baselines under out-of-distribution settings, demonstrating its strong generalization to complex geometric transformations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ms6IXbfzzX": {
    "title": "ZeroS: Zero‑Sum Linear Attention for Efficient Transformers",
    "volume": "spotlight",
    "abstract": "Linear attention methods offer Transformers $O(N)$ complexity but typically underperform standard softmax attention. We identify two fundamental limitations affecting these approaches: the restriction to convex combinations that only permits additive information blending, and uniform accumulated weight bias that dilutes attention in long contexts. We propose Zero-Sum Linear Attention (ZeroS), which addresses these limitations by removing the constant zero-order term $1/t$ and reweighting the remaining zero-sum softmax residuals. This modification creates mathematically stable weights, enabling both positive and negative values and allowing a single attention layer to perform contrastive operations. While maintaining $O(N)$ complexity, ZeroS theoretically expands the set of representable functions compared to convex combinations. Empirically, it matches or exceeds standard softmax attention across various sequence modeling benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V8SndhCN0z": {
    "title": "Blameless Users in a Clean Room: Defining Copyright Protection for Generative Models",
    "volume": "spotlight",
    "abstract": "Are there any conditions under which a generative model's outputs are guaranteed not to infringe the copyrights of its training data? This is the question of \"provable copyright protection\" first posed by Vyas, Kakade, and Barak [ICML 2023]. They define _near access-freeness (NAF)_ and propose it as sufficient for protection. This paper revisits the question and establishes new foundations for provable copyright protection---foundations that are firmer both technically and legally. First, we show that NAF alone does not prevent infringement. In fact, NAF models can enable verbatim copying, a blatant failure of copy protection that we dub being _tainted_. Then, we introduce our _blameless copy protection framework_ for defining meaningful guarantees, and instantiate it with _clean-room copy protection_. Clean-room copy protection allows a user to control their risk of copying by behaving in a way that is unlikely to copy in a counterfactual \"clean-room setting.\" Finally, we formalize a common intuition about differential privacy and copyright by proving that DP implies clean-room copy protection when the dataset is _golden_, a copyright deduplication requirement",
    "checked": true,
    "id": "de7cc7a22de81106d6eb0505b8dcf5d98880c60b",
    "semantic_title": "blameless users in a clean room: defining copyright protection for generative models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IJLqUjtrls": {
    "title": "Adaptive 3D Reconstruction via Diffusion Priors and Forward Curvature-Matching Likelihood Updates",
    "volume": "spotlight",
    "abstract": "Reconstructing high-quality point clouds from images remains challenging in computer vision. Existing generative models, particularly diffusion models, based approaches that directly learn the posterior may suffer from inflexibility—they require conditioning signals during training, support only a fixed number of input views, and need complete retraining for different measurements. Recent diffusion-based methods have attempted to address this by combining prior models with likelihood updates, but they rely on heuristic fixed step sizes for the likelihood update that lead to slow convergence and suboptimal reconstruction quality. We advance this line of approach by integrating our novel Forward Curvature-Matching (FCM) update method with diffusion sampling. Our method dynamically determines optimal step sizes using only forward automatic differentiation and finite-difference curvature estimates, enabling precise optimization of the likelihood update. This formulation enables high-fidelity reconstruction from both single-view and multi-view inputs, and supports various input modalities through simple operator substitution—all without retraining. Experiments on ShapeNet and CO3D datasets demonstrate that our method achieves superior reconstruction quality at matched or lower NFEs, yielding higher F-score and lower CD and EMD, validating its efficiency and adaptability for practical applications. Code is available at https://github.com/Seunghyeok0715/FCM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n0QvMU2kON": {
    "title": "From Shortcut to Induction Head: How Data Diversity Shapes Algorithm Selection in Transformers",
    "volume": "spotlight",
    "abstract": "Transformers can implement both generalizable algorithms (e.g., induction heads) and simple positional shortcuts (e.g., memorizing fixed output positions). In this work, we study how the choice of pretraining data distribution steers a shallow transformer toward one behavior or the other. Focusing on a minimal trigger-output prediction task -- copying the token immediately following a special trigger upon its second occurrence -- we present a rigorous analysis of gradient-based training of a single-layer transformer. In both the infinite and finite sample regimes, we prove a transition in the learned mechanism: if input sequences exhibit sufficient diversity, measured by a low \"max-sum\" ratio of trigger-to-trigger distances, the trained model implements an induction head and generalizes to unseen contexts; by contrast, when this ratio is large, the model resorts to a positional shortcut and fails to generalize out-of-distribution (OOD). We also reveal a trade-off between the pretraining context length and OOD generalization, and derive the optimal pretraining distribution that minimizes computational cost per sample. Finally, we validate our theoretical predictions with controlled synthetic experiments, demonstrating that broadening context distributions robustly induces induction heads and enables OOD generalization. Our results shed light on the algorithmic biases of pretrained transformers and offer conceptual guidelines for data-driven control of their learned behaviors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RespmwOoCH": {
    "title": "Learning Interestingness in Automated Mathematical Theory Formation",
    "volume": "spotlight",
    "abstract": "We take two key steps in automating the open-ended discovery of new mathematical theories, a grand challenge in artificial intelligence. First, we introduce Fermat, a reinforcement learning (RL) environment that models concept discovery and theorem-proving using a set of symbolic actions, opening up a range of RL problems relevant to theory discovery. Second, we explore a specific problem through Fermat: automatically scoring the interestingness of mathematical objects. We investigate evolutionary algorithms for synthesizing nontrivial interestingness measures. In particular, we introduce an LLM-based evolutionary algorithm that features function abstraction, leading to notable improvements in discovering elementary number theory and finite fields over hard-coded baselines. We open-source the \\fermat environment at github.com/trishullab/Fermat",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yQoHUijSHx": {
    "title": "DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sAFottNlra": {
    "title": "Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "3f0c849509de91ddf09835c9a09c5887830e03e6",
    "semantic_title": "signal and noise: a framework for reducing uncertainty in language model evaluation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=u2GzxdWLFW": {
    "title": "Tradeoffs between Mistakes and ERM Oracle Calls in Online and Transductive Online Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gJclyLFSdU": {
    "title": "The World Is Bigger: A Computationally-Embedded Perspective on the Big World Hypothesis",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2afhRWVb6p": {
    "title": "Multimodal Disease Progression Modeling via Spatiotemporal Disentanglement and Multiscale Alignment",
    "volume": "spotlight",
    "abstract": "Longitudinal multimodal data, including electronic health records (EHR) and sequential chest X-rays (CXRs), is critical for modeling disease progression, yet remains underutilized due to two key challenges: (1) redundancy in consecutive CXR sequences, where static anatomical regions dominate over clinically-meaningful dynamics, and (2) temporal misalignment between sparse, irregular imaging and continuous EHR data. We introduce $\\texttt{DiPro}$, a novel framework that addresses these challenges through region-aware disentanglement and multi-timescale alignment. First, we disentangle static (anatomy) and dynamic (pathology progression) features in sequential CXRs, prioritizing disease-relevant changes. Second, we hierarchically align these static and dynamic CXR features with asynchronous EHR data via local (pairwise interval-level) and global (full-sequence) synchronization to model coherent progression pathways. Extensive experiments on the MIMIC dataset demonstrate that $\\texttt{DiPro}$ could effectively extract temporal clinical dynamics and achieve state-of-the-art performance on both disease progression identification and general ICU prediction tasks",
    "checked": true,
    "id": "0da66d1df1d50329140f3564f3313fe90e33ec22",
    "semantic_title": "multimodal disease progression modeling via spatiotemporal disentanglement and multiscale alignment",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tn1M71PDfF": {
    "title": "On the Hardness of Conditional Independence Testing In Practice",
    "volume": "spotlight",
    "abstract": "Tests of conditional independence (CI) underpin a number of important problems in machine learning and statistics, from causal discovery to evaluation of predictor fairness and out-of-distribution robustness. Shah and Peters (2020) showed that, contrary to the unconditional case, no universally finite-sample valid test can ever achieve nontrivial power. While informative, this result (based on \"hiding\" dependence) does not seem to explain the frequent practical failures observed with popular CI tests. We investigate the Kernel-based Conditional Independence (KCI) test – of which we show the Generalized Covariance Measure underlying many recent tests is _nearly_ a special case – and identify the major factors underlying its practical behavior. We highlight the key role of errors in the conditional mean embedding estimate for the Type I error, while pointing out the importance of selecting an appropriate conditioning kernel (not recognized in previous work) as being necessary for good test power but also tending to inflate Type I error",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C7LxkebvVW": {
    "title": "Emergence and Evolution of Interpretable Concepts in Diffusion Models",
    "volume": "spotlight",
    "abstract": "Diffusion models have become the go-to method for text-to-image generation, producing high-quality images from pure noise. However, the inner workings of diffusion models is still largely a mystery due to their black-box nature and complex, multi-step generation process. Mechanistic interpretability techniques, such as Sparse Autoencoders (SAEs), have been successful in understanding and steering the behavior of large language models at scale. However, the great potential of SAEs has not yet been applied toward gaining insight into the intricate generative process of diffusion models. In this work, we leverage the SAE framework to probe the inner workings of a popular text-to-image diffusion model, and uncover a variety of human-interpretable concepts in its activations. Interestingly, we find that *even before the first reverse diffusion step* is completed, the final composition of the scene can be predicted surprisingly well by looking at the spatial distribution of activated concepts. Moreover, going beyond correlational analysis, we design intervention techniques aimed at manipulating image composition and style, and demonstrate that (1) in early stages of diffusion image composition can be effectively controlled, (2) in the middle stages image composition is finalized, however stylistic interventions are effective, and (3) in the final stages only minor textural details are subject to change",
    "checked": true,
    "id": "ed5fde7605c84490138182cbf6863e5eb93e2962",
    "semantic_title": "emergence and evolution of interpretable concepts in diffusion models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=at87L8EuzR": {
    "title": "PCA++: How Uniformity Induces Robustness to Background Noise in Contrastive Learning",
    "volume": "spotlight",
    "abstract": "High-dimensional data often conceal low-dimensional signals beneath structured background noise, limiting standard PCA. Motivated by contrastive learning, we address the problem of recovering shared signal subspaces from positive pairs--paired observations sharing the same signal but differing in background. Our baseline, PCA+, uses alignment-only contrastive learning and succeeds when background variation is mild, but fails under strong noise or high-dimensional regimes. To address this, we introduce PCA++, a hard uniformity-constrained contrastive PCA that enforces identity covariance on projected features. PCA++ has a closed-form solution via a generalized eigenproblem, remains stable in high dimensions, and provably regularizes against background interference. We provide exact high-dimensional asymptotics in both fixed-aspect-ratio and growing-spike regimes, showing uniformity's role in robust signal recovery. Empirically, PCA++ outperforms standard PCA and alignment-only PCA+ on simulations, corrupted-MNIST, and single-cell transcriptomics, reliably recovering condition-invariant structure. More broadly, we clarify uniformity's role in contrastive learning—showing that explicit feature dispersion defends against structured noise and enhances robustness",
    "checked": false,
    "id": "b1656286e2915cb4ead75cab3042de5efbf7735d",
    "semantic_title": "pca ++ : how uniformity induces robustness to background noise in contrastive learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=htN3xhUpjD": {
    "title": "Optimal Nuisance Function Tuning for Estimating a Doubly Robust Functional under Proportional Asymptotics",
    "volume": "spotlight",
    "abstract": "In this paper, we explore the asymptotically optimal tuning parameter choice in ridge regression for estimating nuisance functions of a statistical functional that has recently gained prominence in conditional independence testing and causal inference. Given a sample of size $n$, we study estimators of the Expected Conditional Covariance (ECC) between variables $Y$ and $A$ given a high-dimensional covariate $X \\in \\mathbb{R}^p$. Under linear regression models for $Y$ and $A$ on $X$ and the proportional asymptotic regime $p/n \\to c \\in (0, \\infty)$, we evaluate three existing ECC estimators and two sample splitting strategies for estimating the required nuisance functions. Since no consistent estimator of the nuisance functions exists in the proportional asymptotic regime without imposing further structure on the problem, we first derive debiased versions of the ECC estimators that utilize the ridge regression nuisance function estimators. We show that our bias correction strategy yields $\\sqrt{n}$-consistent estimators of the ECC across different sample splitting strategies and estimator choices. We then derive the asymptotic variances of these debiased estimators to illustrate the nuanced interplay between the sample splitting strategy, estimator choice, and tuning parameters of the nuisance function estimators for optimally estimating the ECC. Our analysis reveals that prediction-optimal tuning parameters (i.e., those that optimally estimate the nuisance functions) may not lead to the lowest asymptotic variance of the ECC estimator -- thereby demonstrating the need to be careful in selecting tuning parameters based on the final goal of inference. Finally, we verify our theoretical results through extensive numerical experiments",
    "checked": true,
    "id": "aa56c13088f9ce35efffb6dc016ed898216737e7",
    "semantic_title": "optimal nuisance function tuning for estimating a doubly robust functional under proportional asymptotics",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LTgUInLTbP": {
    "title": "GeoLLaVA-8K: Scaling Remote-Sensing Multimodal Large Language Models to 8K Resolution",
    "volume": "spotlight",
    "abstract": "Ultra-high-resolution (UHR) remote sensing (RS) imagery offers valuable data for Earth observation but pose challenges for existing multimodal foundation models due to two key bottlenecks: (1) limited availability of UHR training data, and (2) token explosion caused by the large image size. To address data scarcity, we introduce **SuperRS-VQA** (avg. 8,376$\\times$8,376) and **HighRS-VQA** (avg. 2,000$\\times$1,912), the highest-resolution vision-language datasets in RS to date, covering 22 real-world dialogue tasks. To mitigate token explosion, our pilot studies reveal significant redundancy in RS images: crucial information is concentrated in a small subset of object-centric tokens, while pruning background tokens (e.g., ocean or forest) can even improve performance. Motivated by these findings, we propose two strategies: *Background Token Pruning* and *Anchored Token Selection*, to reduce the memory footprint while preserving key semantics. Integrating these techniques, we introduce **GeoLLaVA-8K**, the first RS-focused multimodal large language model capable of handling inputs up to 8K$\\times$8K resolution, built on the LLaVA framework. Trained on SuperRS-VQA and HighRS-VQA, GeoLLaVA-8K sets a new state-of-the-art on the XLRS-Bench. Datasets and code were released at https://github.com/MiliLab/GeoLLaVA-8K",
    "checked": true,
    "id": "2970550366773a6a8bf6ecd82f33ed0deb99ea7b",
    "semantic_title": "geollava-8k: scaling remote-sensing multimodal large language models to 8k resolution",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=2M5dTDdGxl": {
    "title": "Environment Inference for Learning Generalizable Dynamical System",
    "volume": "spotlight",
    "abstract": "Data-driven methods offer efficient and robust solutions for analyzing complex dynamical systems but rely on the assumption of I.I.D. data, driving the development of generalization techniques for handling environmental differences. These techniques, however, are limited by their dependence on environment labels, which are often unavailable during training due to data acquisition challenges, privacy concerns, and environmental variability, particularly in large public datasets and privacy-sensitive domains. In response, we propose DynaInfer, a novel method that infers environment specifications by analyzing prediction errors from fixed neural networks within each training round, enabling environment assignments directly from data. We prove our algorithm effectively solves the alternating optimization problem in unlabeled scenarios and validate it through extensive experiments across diverse dynamical systems. Results show that DynaInfer outperforms existing environment assignment techniques, converges rapidly to true labels, and even achieves superior performance when environment labels are available",
    "checked": true,
    "id": "0362abd32c1087875d46c77f3013f9c9342310de",
    "semantic_title": "environment inference for learning generalizable dynamical system",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=EXIKFM1Q9R": {
    "title": "Abstract Rendering: Certified Rendering Under 3D Semantic Uncertainty",
    "volume": "spotlight",
    "abstract": "Rendering produces 2D images from 3D scene representations, yet how continuous variations in camera pose and scenes influence these images—and, consequently, downstream visual models—remains underexplored. We introduce **abstract rendering**, a framework that computes provable bounds on all images rendered under continuously varying camera poses and scenes. The resulting abstract image, expressed as a set of constraints over the image matrix, enables rigorous uncertainty propagation through downstream neural networks and thereby supports certification of model behavior under realistic 3D semantic perturbations, far beyond traditional pixel-level noise models. Our approach propagates camera pose uncertainty through each rendering step using efficient piecewise linear bounds, including custom abstractions for three rendering-specific operations—matrix inversion, sorting-based aggregation, and cumulative product summation—not supported by standard tools. Our implementation, ABSTRACTRENDER, targets two state-of-the-art photorealistic scene representations—3D Gaussian Splats and Neural Radiance Fields (NeRF)—and scales to complex scenes with up to 1M Gaussians. Our computed abstract images achieve up to 3% over-approximation error compared to sampling results (baseline). Through experiments on classification (ResNet), object detection (YOLO), and pose estimation (GATENet) tasks, we demonstrate that abstract rendering enables formal certification of downstream models under realistic 3D variations—an essential step toward safety-critical vision systems",
    "checked": false,
    "id": "a8efa7087fd6d84d5d84fd6c7c7cfb9d7ddb6dce",
    "semantic_title": "nerfdiff: single-image view synthesis with nerf-guided distillation from 3d-aware diffusion",
    "citation_count": 203,
    "authors": []
  },
  "https://openreview.net/forum?id=hSX7Dd8dxy": {
    "title": "Inference-Time Reward Hacking in Large Language Models",
    "volume": "spotlight",
    "abstract": "A common paradigm to improve the performance of large language models is optimizing for a reward model. Reward models assign a numerical score to an LLM's output that indicates, for example, how likely it is to align with user preferences or safety goals. However, reward models are never perfect. They inevitably function as proxies for complex desiderata such as correctness, helpfulness, and safety. By overoptimizing for a misspecified reward, we can subvert intended alignment goals and reduce overall performance -- a phenomenon commonly referred to as reward hacking. In this work, we characterize reward hacking in inference-time alignment and demonstrate when and how we can mitigate it by hedging on the proxy reward. We study this phenomenon under Best-of-$n$ (BoN) and Soft Best-of-$n$ (SBoN), and we introduce Best-of-Poisson (BoP) that provides an efficient, near-exact approximation of the optimal reward-KL divergence policy at inference time. We show that the characteristic pattern of hacking as observed in practice (where the true reward first increases before declining) is an inevitable property of a broad class of inference-time mechanisms, including BoN and BoP. To counter this effect, we introduce $\\texttt{HedgeTune}$, an efficient algorithm to find the optimal inference-time parameter. We demonstrate that hedging mitigates reward hacking and achieves superior reward-distortion tradeoffs on math, reasoning, and human-preference setups",
    "checked": true,
    "id": "1465aff7f7a0916287a43f8b0f0c0d6e5fe5719e",
    "semantic_title": "inference-time reward hacking in large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=dtsYpcJr1R": {
    "title": "High-Performance Arithmetic Circuit Optimization via Differentiable Architecture Search",
    "volume": "spotlight",
    "abstract": "Arithmetic circuit optimization remains a fundamental challenge in modern integrated circuit design. Recent advances have cast this problem within the Learning to Optimize (L2O) paradigm, where intelligent agents autonomously explore high-performance design spaces with encouraging results. However, existing approaches predominantly target coarse-grained architectural configurations, while the crucial interconnect optimization stage is often relegated to oversimplified proxy models or a heuristic approach. This disconnect undermines design quality, leading to suboptimal solutions in the circuit topology search space. To bridge this gap, we present **Arith-DAS**, a **D**ifferentiable **A**rchitecture **S**earch framework for **Arith**metic circuits. To the best of our knowledge, **Arith-DAS** is the first to formulate interconnect optimization within arithmetic circuits as a differentiable edge prediction problem over a multi-relational directed acyclic graph, enabling fine-grained, proxy-free optimization at the interconnection level. We evaluate **Arith-DAS** on a suite of representative arithmetic circuits, including multipliers and multiply-accumulate units. Experiments show substantial improvements over state-of-the-art L2O and conventional methods, achieving up to $\\textbf{27.05}$% gain in hypervolume of area-delay Pareto front, a standard metric for evaluating multi-objective optimization performance. Moreover, integrating our optimized arithmetic units into large-scale AI accelerators yields up to $\\textbf{6.59}$% delay reduction, demonstrating both scalability and real-world applicability",
    "checked": false,
    "id": "345a3dcadb9910427197382813a13548c80e0c1c",
    "semantic_title": "efficient variational quantum algorithms via circuit knitting and architecture search",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QFuuxfmqb5": {
    "title": "ROOT: Rethinking Offline Optimization as Distributional Translation via Probabilistic Bridge",
    "volume": "spotlight",
    "abstract": "This paper studies the black-box optimization task which aims to find the maxima of a black-box function using a static set of its observed input-output pairs. This is often achieved via learning and optimizing a surrogate function with that offline data. Alternatively, it can also be framed as an inverse modeling task that maps a desired performance to potential input candidates that achieve it. Both approaches are constrained by the limited amount of offline data. To mitigate this limitation, we introduce a new perspective that casts offline optimization as a distributional translation task. This is formulated as learning a probabilistic bridge transforming an implicit distribution of low-value inputs (i.e., offline data) into another distribution of high-value inputs (i.e., solution candidates). Such probabilistic bridge can be learned using low- and high-value inputs sampled from synthetic functions that resemble the target function. These synthetic functions are constructed as the mean posterior of multiple Gaussian processes fitted with different parameterizations on the offline data, alleviating the data bottleneck. The proposed approach is evaluated on an extensive benchmark comprising most recent methods, demonstrating significant improvement and establishing a new state-of-the-art performance. Our code is publicly available at https://github.com/cuong-dm/ROOT",
    "checked": true,
    "id": "eff307b0481633e94d49e69c7ad9f3724633fd58",
    "semantic_title": "root: rethinking offline optimization as distributional translation via probabilistic bridge",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bmznY5wYXH": {
    "title": "Go With the Flow: Fast Diffusion for Gaussian Mixture Models",
    "volume": "spotlight",
    "abstract": "Schrodinger Bridges (SBs) are diffusion processes that steer, in finite time, a given initial distribution to another final one while minimizing a suitable cost functional. Although various methods for computing SBs have recently been proposed in the literature, most of these approaches require computationally expensive training schemes, even for solving low-dimensional problems. In this work, we propose an analytic parametrization of a set of feasible policies for steering the distribution of a dynamical system from one Gaussian Mixture Model (GMM) to another. Instead of relying on standard non-convex optimization techniques, the optimal policy within the set can be approximated as the solution of a low-dimensional linear program whose dimension scales linearly with the number of components in each mixture. The proposed method generalizes naturally to more general classes of dynamical systems, such as controllable linear time-varying systems, enabling efficient solutions to multi-marginal momentum SBs between GMMs, a challenging distribution interpolation problem. We showcase the potential of this approach in low-to-moderate dimensional problems such as image-to-image translation in the latent space of an autoencoder, learning of cellular dynamics using multi-marginal momentum SBs, and various other examples. The implementation is publicly available at https://github.com/georgeRapa/GMMflow",
    "checked": true,
    "id": "feb3ceac686c0b72af6b4be7892707c156c0c484",
    "semantic_title": "go with the flow: fast diffusion for gaussian mixture models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=3IXdXBpuLn": {
    "title": "SHF: Symmetrical Hierarchical Forest with Pretrained Vision Transformer Encoder for High-Resolution Medical Segmentation",
    "volume": "spotlight",
    "abstract": "This paper presents a novel approach to addressing the long-sequence problem in high-resolution medical images for Vision Transformers (ViTs). Using smaller patches as tokens can enhance ViT performance, but quadratically increases computation and memory requirements. Therefore, the common practice for applying ViTs to high-resolution images is either to: (a) employ complex sub-quadratic attention schemes or (b) use large to medium-sized patches and rely on additional mechanisms within the model to capture the spatial hierarchy of details. We propose Symmetrical Hierarchical Forest (SHF), a lightweight approach that adaptively patches the input image to increase token information density and encode hierarchical spatial structures into the input embedding. We then apply a reverse depatching scheme to the output embeddings of the transformer encoder, eliminating the need for convolution-based decoders. Unlike previous methods that modify attention mechanisms \\wahib{or use a complex hierarchy of interacting models}, SHF can be retrofitted to any ViT model to allow it to learn the hierarchical structure of details in high-resolution images without requiring architectural changes. Experimental results demonstrate significant gains in computational efficiency and performance: on the PAIP WSI dataset, we achieved a 3$\\sim$32$\\times$ speedup or a 2.95\\% to 7.03\\% increase in accuracy (measured by Dice score) at a $64K^2$ resolution with the same computational budget, compared to state-of-the-art production models. On the 3D medical datasets BTCV and KiTS, training was 6$\\times$ faster, with accuracy gains of 6.93\\% and 5.9\\%, respectively, compared to models without SHF",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KI8qan2EA7": {
    "title": "Proxy-SPEX: Sample-Efficient Interpretability via Sparse Feature Interactions in LLMs",
    "volume": "spotlight",
    "abstract": "Large Language Models (LLMs) have achieved remarkable performance by capturing complex interactions between input features. To identify these interactions, most existing approaches require enumerating all possible combinations of features up to a given order, causing them to scale poorly with the number of inputs $n$. Recently, Kang et al. (2025) proposed SPEX, an information-theoretic approach that uses interaction sparsity to scale to $n \\approx 10^3$ features. SPEX greatly improves upon prior methods but requires tens of thousands of model inferences, which can be prohibitive for large models. In this paper, we observe that LLM feature interactions are often *hierarchical*—higher-order interactions are accompanied by their lower-order subsets—which enables more efficient discovery. To exploit this hierarchy, we propose ProxySPEX, an interaction attribution algorithm that first fits gradient boosted trees to masked LLM outputs and then extracts the important interactions. Experiments across four challenging high-dimensional datasets show that ProxySPEX more faithfully reconstructs LLM outputs by 20\\% over marginal attribution approaches while using *$10\\times$ fewer inferences* than SPEX. By accounting for interactions, ProxySPEX efficiently identifies the most influential features, providing a scalable approximation of their Shapley values. Further, we apply ProxySPEX to two interpretability tasks. *Data attribution*, where we identify interactions among CIFAR-10 training samples that influence test predictions, and *mechanistic interpretability*, where we uncover interactions between attention heads, both within and across layers, on a question-answering task. The ProxySPEX algorithm is available at <https://github.com/mmschlk/shapiq>",
    "checked": false,
    "id": "069de9e4ced02ac86e947463d2f56c72a7943d7f",
    "semantic_title": "proxyspex: inference-efficient interpretability via sparse feature interactions in llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yPsJ1PKiAi": {
    "title": "Fair Cooperation in Mixed-Motive Games via Conflict-Aware Gradient Adjustment",
    "volume": "spotlight",
    "abstract": "Multi-agent reinforcement learning in mixed-motive settings presents a fundamental challenge: agents must balance individual interests with collective goals, which are neither fully aligned nor strictly opposed. To address this, reward restructuring methods such as gifting and intrinsic motivation have been proposed. However, these approaches primarily focus on promoting cooperation by managing the trade-off between individual and collective returns, without explicitly addressing fairness with respect to agents' task-specific rewards. In this paper, we propose an adaptive conflict-aware gradient adjustment method that promotes cooperation while ensuring fairness in individual rewards. The proposed method dynamically balances policy gradients derived from individual and collective objectives in situations where the two objectives are in conflict. By explicitly resolving such conflicts, our method improves collective performance while preserving fairness across agents. We provide theoretical results that guarantee monotonic non-decreasing improvement in both the collective and individual objectives and ensure fairness. Empirical results in sequential social dilemma environments demonstrate that our approach outperforms baselines in terms of social welfare, while maintaining fairness",
    "checked": true,
    "id": "4b32b77c2503dadd11ebab026f0856e4ab3556f7",
    "semantic_title": "fair cooperation in mixed-motive games via conflict-aware gradient adjustment",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fLx3vQPmDu": {
    "title": "OpenWorldSAM: Extending SAM2 for Universal Image Segmentation with Language Prompts",
    "volume": "spotlight",
    "abstract": "The ability to segment objects based on open-ended language prompts remains a critical challenge, requiring models to ground textual semantics into precise spatial masks while handling diverse and unseen categories. We present OpenWorldSAM, a framework that extends the prompt-driven Segment Anything Model v2 (SAM2) to open-vocabulary scenarios by integrating multi-modal embeddings extracted from a lightweight vision-language model (VLM). Our approach is guided by four key principles: i) Unified prompting: OpenWorldSAM supports a diverse range of prompts, including category-level and sentence-level language descriptions, providing a flexible interface for various segmentation tasks. ii) Efficiency: By freezing the pre-trained components of SAM2 and the VLM, we train only 4.5 million parameters on the COCO-stuff dataset, achieving remarkable resource efficiency. iii) Instance Awareness: We enhance the model's spatial understanding through novel positional tie-breaker embeddings and cross-attention layers, enabling effective segmentation of multiple instances. iv) Generalization: OpenWorldSAM exhibits strong zero-shot capabilities, generalizing well on unseen categories and an open vocabulary of concepts without additional training. Extensive experiments demonstrate that OpenWorldSAM achieves state-of-the-art performance in open-vocabulary semantic, instance, and panoptic segmentation across multiple benchmarks. Code is available at https://github.com/GinnyXiao/OpenWorldSAM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WrYWolqKh3": {
    "title": "Broken Tokens? Your Language Model can Secretly Handle Non-Canonical Tokenizations",
    "volume": "spotlight",
    "abstract": "Modern tokenizers employ deterministic algorithms to map text into a single ``canonical\" token sequence, yet the same string can be encoded as many non-canonical tokenizations using the language model vocabulary, including tokenizing by character. In this paper, we investigate the robustness of LMs to input encoded with non-canonical tokenizations entirely unseen during training. Surprisingly, when evaluated across 20 benchmarks, we find that instruction-tuned models retain up to 93.4\\% of their original performance when given a randomly sampled tokenization, and 90.8\\% with character-level tokenization. We find that overall stronger models tend to be more robust, and that robustness diminishes as the tokenization departs farther from the canonical form. Motivated by these results, we identify settings where non-canonical tokenization schemes can \\textit{improve} performance, finding that character‑level segmentation improves string manipulation and code understanding tasks by up to 15\\%, and right‑aligned digit grouping enhances large‑number arithmetic by over 33\\%. Finally, we investigate the source of this robustness, finding that it arises in the instruction-tuning phase. We provide evidence that both base and post-trained models grasp the semantics of non-canonical tokenizations (perceiving them as containing misspellings). However, base models try to mimic the imagined mistakes and degenerate into nonsensical output, while post-trained models are committed to fluent responses. Overall, our findings suggest that models are less committed to their tokenizer than previously believed, and highlight the promise of intervening on tokenization at inference time to boost language model performance",
    "checked": true,
    "id": "5bec2a6df12e4cc6034490e400abecfc7ae090fb",
    "semantic_title": "broken tokens? your language model can secretly handle non-canonical tokenizations",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=GxGrGswvND": {
    "title": "Hybrid-Balance GFlowNet for Solving Vehicle Routing Problems",
    "volume": "spotlight",
    "abstract": "Existing GFlowNet-based methods for vehicle routing problems (VRPs) typically employ Trajectory Balance (TB) to achieve global optimization but often neglect important aspects of local optimization. While Detailed Balance (DB) addresses local optimization more effectively, it alone falls short in solving VRPs, which inherently require holistic trajectory optimization. To address these limitations, we introduce the Hybrid-Balance GFlowNet (HBG) framework, which uniquely integrates TB and DB in a principled and adaptive manner by aligning their intrinsically complementary strengths. Additionally, we propose a specialized inference strategy for depot-centric scenarios like the Capacitated Vehicle Routing Problem (CVRP), leveraging the depot node's greater flexibility in selecting successors. Despite this specialization, HBG maintains broad applicability, extending effectively to problems without explicit depots, such as the Traveling Salesman Problem (TSP). We evaluate HBG by integrating it into two established GFlowNet-based solvers, i.e., AGFN and GFACS, and demonstrate consistent and significant improvements across both CVRP and TSP, underscoring the enhanced solution quality and generalization afforded by our approach",
    "checked": true,
    "id": "fcca22fb06907188adbce7d9a0fd8c16ce004889",
    "semantic_title": "hybrid-balance gflownet for solving vehicle routing problems",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ustF8MMZDJ": {
    "title": "Feedback-Aware MCTS for Goal-Oriented Information Seeking",
    "volume": "spotlight",
    "abstract": "Effective decision-making and problem-solving in conversational systems require the ability to identify and acquire missing information through targeted questioning. A key challenge lies in efficiently narrowing down a large space of possible outcomes by posing questions that minimize uncertainty. To address this, we introduce a novel framework that leverages Large Language Models (LLMs) to generate information-seeking questions, with Monte Carlo Tree Search (MCTS) to strategically select questions that maximize information gain, as a part of inference-time planning. Our primary contribution includes a hierarchical feedback mechanism that exploits past interaction patterns to guide future strategy. Specifically, each new problem is mapped to a cluster based on semantic similarity, and our UCT (Upper Confidence bound for Trees) formulation employs a cluster-specific bonus reward to prioritize successful question trajectories that have proven effective for similar problems in the past. Extensive empirical evaluation across medical diagnosis and technical troubleshooting domains shows that our method achieves an average of 12\\% improvement in success rates and about 10x reduction in the number of LLM calls made for planning per conversation, compared to the state of the art. An additional 8\\% gain in success rate is observed on average when we start with a constrained set of possibilities. Our results underscore the efficacy of feedback-aware MCTS in enhancing information-seeking in goal-oriented dialogues",
    "checked": false,
    "id": "a47cc919faf9522266a02f5c16efc1441a056c43",
    "semantic_title": "feedback-aware monte carlo tree search for efficient information seeking in goal-oriented conversations",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=fGuTN7huo5": {
    "title": "Self-Supervised Learning of Motion Concepts by Optimizing Counterfactuals",
    "volume": "spotlight",
    "abstract": "Estimating motion primitives from video (e.g., optical flow and occlusion) is a critically important computer vision problem with many downstream applications, including controllable video generation and robotics. Current solutions are primarily supervised on synthetic data or require tuning of situation-specific heuristics, which inherently limits these models' capabilities in real-world contexts. A natural solution to transcend these limitations would be to deploy large-scale, self-supervised video models, which can be trained scalably on unrestricted real-world video datasets. However, despite recent progress, motion-primitive extraction from large pretrained video models remains relatively underexplored. In this work, we describe Opt-CWM, a self-supervised flow and occlusion estimation technique from a pretrained video prediction model. Opt-CWM uses ``counterfactual probes'' to extract motion information from a base video model in a zero-shot fashion. The key problem we solve is optimizing the quality of these probes, using a combination of an efficient parameterization of the space counterfactual probes, together with a novel generic sparse-prediction principle for learning the probe-generation parameters in a self-supervised fashion. Opt-CWM achieves state-of-the-art performance for motion estimation on real-world videos while requiring no labeled data",
    "checked": true,
    "id": "56d89b44a9fbf7392a0c4a5fb46ef221f04ba5e5",
    "semantic_title": "self-supervised learning of motion concepts by optimizing counterfactuals",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Mat9FTfiYD": {
    "title": "Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning",
    "volume": "spotlight",
    "abstract": "Uncertain knowledge graphs (UKGs) associate each triple with a confidence score to provide more precise knowledge representations. Recently, since real-world UKGs suffer from the incompleteness, uncertain knowledge graph (UKG) completion attracts more attention, aiming to complete missing triples and confidences. Current studies attempt to learn UKG embeddings to solve this problem, but they neglect the extremely imbalanced distributions of triple confidences. This causes that the learnt embeddings are insufficient to high-quality UKG completion. Thus, in this paper, to address the above issue, we propose a new semi-supervised Confidence Distribution Learning (ssCDL) method for UKG completion, where each triple confidence is transformed into a confidence distribution to introduce more supervision information of different confidences to reinforce the embedding learning process. ssCDL iteratively learns UKG embedding by relational learning on labeled data (i.e., existing triples with confidences) and unlabeled data with pseudo labels (i.e., unseen triples with the generated confidences), which are predicted by meta-learning to augment the training data and rebalance the distribution of triple confidences. Experiments on two UKG datasets demonstrate that ssCDL consistently outperforms the state-of-the-art baselines in different evaluation metrics",
    "checked": true,
    "id": "92e7c0db1dd5175014b5cf1a701e9e11bf0e2445",
    "semantic_title": "uncertain knowledge graph completion via semi-supervised confidence distribution learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VN5bMTfSZS": {
    "title": "OCTDiff: Bridged Diffusion Model for Portable OCT Super-Resolution and Enhancement",
    "volume": "spotlight",
    "abstract": "Medical imaging super-resolution is critical for improving diagnostic utility and reducing costs, particularly for low-cost modalities such as portable Optical Coherence Tomography (OCT). We propose OCTDiff, a bridged diffusion model designed to enhance image resolution and quality from portable OCT devices. Our image-to-image diffusion framework addresses key challenges in the conditional generation process of denoising diffusion probabilistic models (DDPMs). We introduce Adaptive Noise Aggregation (ANA), a novel module to improve denoising dynamics within the reverse diffusion process. Additionally, we integrate Multi-Scale Cross-Attention (MSCA) into the U-Net backbone to capture local dependencies across spatial resolutions. To address overfitting on small clinical datasets and to preserve fine structural details essential for retinal diagnostics, we design a customized loss function guided by clinical quality scores. OCTDiff outperforms convolutional baselines and standard DDPMs, achieving state-of-the-art performance on clinical portable OCT datasets. Our model and its downstream applications have the potential to generalize to other medical imaging modalities and revolutionize the current workflow of ophthalmic diagnostics. The code is available at https://github.com/AI4VSLab/OCTDiff",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DwXX8c7xst": {
    "title": "Bits Leaked per Query: Information-Theoretic Bounds for Adversarial Attacks on LLMs",
    "volume": "spotlight",
    "abstract": "Adversarial attacks by malicious users that threaten the safety of large language models (LLMs) can be viewed as attempts to infer a target property $T$ that is unknown when an instruction is issued, and becomes knowable only after the model's reply is observed. Examples of target properties $T$ include the binary flag that triggers an LLM's harmful response or rejection, and the degree to which information deleted by unlearning can be restored, both elicited via adversarial instructions. The LLM reveals an \\emph{observable signal} $Z$ that potentially leaks hints for attacking through a response containing answer tokens, thinking process tokens, or logits. Yet the scale of information leaked remains anecdotal, leaving auditors without principled guidance and defenders blind to the transparency--risk trade-off. We fill this gap with an information-theoretic framework that computes how much information can be safely disclosed, and enables auditors to gauge how close their methods come to the fundamental limit. Treating the mutual information $I(Z;T)$ between the observation $Z$ and the target property $T$ as the leaked bits per query, we show that achieving error $\\varepsilon$ requires at least $\\log(1/\\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak rate and only logarithmically with the desired accuracy. Thus, even a modest increase in disclosure collapses the attack cost from quadratic to logarithmic in terms of the desired accuracy. Experiments on seven LLMs across system-prompt leakage, jailbreak, and relearning attacks corroborate the theory: exposing answer tokens alone requires about a thousand queries; adding logits cuts this to about a hundred; and revealing the full thinking process trims it to a few dozen. Our results provide the first principled yardstick for balancing transparency and security when deploying LLMs",
    "checked": false,
    "id": "3048d60cb86f6392b06eb052d568a147ac143cfb",
    "semantic_title": "bits leaked per query: information-theoretic bounds on adversarial attacks against llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I822ZIRtms": {
    "title": "Characterizing control between interacting subsystems with deep Jacobian estimation",
    "volume": "spotlight",
    "abstract": "Biological function arises through the dynamical interactions of multiple subsystems, including those between brain areas, within gene regulatory networks, and more. A common approach to understanding these systems is to model the dynamics of each subsystem and characterize communication between them. An alternative approach is through the lens of control theory: how the subsystems control one another. This approach involves inferring the directionality, strength, and contextual modulation of control between subsystems. However, methods for understanding subsystem control are typically linear and cannot adequately describe the rich contextual effects enabled by nonlinear complex systems. To bridge this gap, we devise a data-driven nonlinear control-theoretic framework to characterize subsystem interactions via the Jacobian of the dynamics. We address the challenge of learning Jacobians from time-series data by proposing the JacobianODE, a deep learning method that leverages properties of the Jacobian to directly estimate it for arbitrary dynamical systems from data alone. We show that JacobianODE models outperform existing Jacobian estimation methods on challenging systems, including high-dimensional chaos. Applying our approach to a multi-area recurrent neural network (RNN) trained on a working memory selection task, we show that the \"sensory\" area gains greater control over the \"cognitive\" area over learning. Furthermore, we leverage the JacobianODE to directly control the trained RNN, enabling precise manipulation of its behavior. Our work lays the foundation for a theoretically grounded and data-driven understanding of interactions among biological subsystems",
    "checked": true,
    "id": "71e7dc21562bf956c298ed9f3c59dcfd1e00c419",
    "semantic_title": "characterizing control between interacting subsystems with deep jacobian estimation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ZvqbNFWQkh": {
    "title": "Sharper Convergence Rates for Nonconvex Optimisation via Reduction Mappings",
    "volume": "spotlight",
    "abstract": "Many high-dimensional optimisation problems exhibit rich geometric structures in their set of minimisers, often forming smooth manifolds due to over-parametrisation or symmetries. When this structure is known, at least locally, it can be exploited through reduction mappings that reparametrise part of the parameter space to lie on the solution manifold. These reductions naturally arise from inner optimisation problems and effectively remove redundant directions, yielding a lower-dimensional objective. In this work, we introduce a general framework to understand how such reductions influence the optimisation landscape. We show that well-designed reduction mappings improve curvature properties of the objective, leading to better-conditioned problems and theoretically faster convergence for gradient-based methods. Our analysis unifies a range of scenarios where structural information at optimality is leveraged to accelerate convergence, offering a principled explanation for the empirical gains observed in such optimisation algorithms",
    "checked": true,
    "id": "2abacc9a29fac1c27e87b858902c7f57900af091",
    "semantic_title": "sharper convergence rates for nonconvex optimisation via reduction mappings",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BoYGLpNXZd": {
    "title": "Selective Omniprediction and Fair Abstention",
    "volume": "spotlight",
    "abstract": "We propose new learning algorithms for building selective classifiers, which are predictors that are allowed to abstain on some fraction of the domain. We study the model where a classifier may abstain from predicting at a fixed cost. Building on the recent framework on multigroup fairness and omniprediction, given a pre-specified class of loss functions, we provide an algorithm for building a single classifier that learns abstentions and predictions optimally for every loss in the entire class, where the abstentions are decided efficiently for each specific loss function by applying a fixed post-processing function. Our algorithm and theoretical guarantees generalize the previously-known algorithms for learning selective classifiers in formal learning-theoretic models. We then extend the traditional multigroup fairness algorithms to the selective classification setting and show that we can use a calibrated and multiaccurate predictor to efficiently build selective classifiers that abstain optimally not only globally but also locally within each of the groups in any pre-specified collection of possibly intersecting subgroups of the domain, and are also accurate when they do not abstain. We show how our abstention algorithms can be used as conformal prediction methods in the binary classification setting to achieve both marginal and group-conditional coverage guarantees for an intersecting collection of groups. We provide empirical evaluations for all of our theoretical results, demonstrating the practicality of our learning algorithms for abstaining optimally and fairly",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cD4whTwm6G": {
    "title": "Algorithms and SQ Lower Bounds for Robustly Learning Real-valued Multi-Index Models",
    "volume": "spotlight",
    "abstract": "We study the complexity of learning real-valued Multi-Index Models (MIMs) under the Gaussian distribution. A $K$-MIM is a function $f:\\mathbb{R}^d\\to \\mathbb{R}$ that depends only on the projection of its input onto a $K$-dimensional subspace. We give a general algorithm for PAC learning a broad class of MIMs with respect to the square loss, even in the presence of adversarial label noise. Moreover, we establish a nearly matching Statistical Query (SQ) lower bound, providing evidence that the complexity of our algorithm is qualitatively optimal as a function of the dimension. Specifically, we consider the class of bounded variation MIMs with the property that degree at most $m$ distinguishing moments exist with respect to projections onto any subspace. In the presence of adversarial label noise, the complexity of our learning algorithm is $d^{O(m)}2^{\\mathrm{poly}(K/\\epsilon)}$. For the realizable and independent noise settings, our algorithm incurs complexity $d^{O(m)}2^{\\mathrm{poly}(K)}(1/\\epsilon)^{O(K)}$. To complement our upper bound, we show that if for some subspace degree-$m$ distinguishing moments do not exist, then any SQ learner for the corresponding class of MIMs requires complexity $d^{\\Omega(m)}$. As an application, we give the first efficient learner for the class of positive-homogeneous $L$-Lipschitz $K$-MIMs. The resulting algorithm has complexity $\\mathrm{poly}(d) 2^{\\mathrm{poly}(KL/\\epsilon)}$. This gives a new PAC learning algorithm for Lipschitz homogeneous ReLU networks with complexity independent of the network size, removing the exponential dependence incurred in prior work",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0qRXETZZwv": {
    "title": "A Principled Approach to Randomized Selection under Uncertainty: Applications to Peer Review and Grant Funding",
    "volume": "spotlight",
    "abstract": "Many decision-making processes involve evaluating and selecting items, including scientific peer review, job hiring, school admissions, and investment decisions. These domains feature error-prone evaluations and uncertainty about outcomes, which undermine deterministic selection rules. Consequently, randomized selection mechanisms are gaining traction. However, current randomized approaches are ad hoc and, as we prove, inappropriate for their purported objectives. We propose a principled framework for randomized decision-making based on interval estimates of item quality. We introduce MERIT (Maximin Efficient Randomized Interval Top-$k$), which maximizes the worst-case expected number of top candidates selected under uncertainty represented by overlapping intervals. MERIT provides optimal resource allocation under an interpretable robustness notion. We develop a polynomial-time, practically efficient algorithm and prove our approach satisfies desirable axiomatic properties not guaranteed by existing methods. Experiments on synthetic peer review data from grant funding and conferences demonstrate that MERIT matches existing algorithms' expected utility under fully probabilistic models while outperforming them under our worst-case formulation",
    "checked": true,
    "id": "023cd87ebbf951c8ce189fe67e29a2749fa2d26b",
    "semantic_title": "a principled approach to randomized selection under uncertainty: applications to peer review and grant funding",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=xVI8g50Qfk": {
    "title": "Error Forcing in Recurrent Neural Networks",
    "volume": "spotlight",
    "abstract": "How should feedback influence recurrent neural network (RNN) learning? One way to address the known limitations of backpropagation through time is to directly adjust neural activities during the learning process. However, it remains unclear how to effectively use feedback to shape RNN dynamics. Here, we introduce error forcing (EF), where the network activity is guided orthogonally toward the zero-error manifold during learning. This method contrasts with alternatives like teaching forcing, which impose stronger constraints on neural activity and thus induce larger feedback influence on circuit dynamics. Furthermore, EF can be understood from a Bayesian perspective as a form of approximate dynamic inference. Empirically, EF consistently outperforms other learning algorithms across several tasks and its benefits persist when additional biological constraints are taken into account. Overall, EF is a powerful temporal credit assignment mechanism and a promising candidate model for learning in biological systems",
    "checked": false,
    "id": "7bd4f589fdb8b8ac5a3edcdbf860d09bebe41188",
    "semantic_title": "feedback control guides credit assignment in recurrent neural networks",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=YvypxK4kut": {
    "title": "Aligning Text-to-Image Diffusion Models to Human Preference by Classification",
    "volume": "spotlight",
    "abstract": "Text-to-image diffusion models are typically trained on large-scale web data, often resulting in outputs that misalign with human preferences. Inspired by preference learning in large language models, we propose ABC (Alignment by Classification), a simple yet effective framework for aligning diffusion models with human preferences. In contrast to prior DPO-based methods that depend on suboptimal supervised fine-tuned (SFT) reference models, ABC assumes access to an ideal reference model perfectly aligned with human intent and reformulates alignment as a classification problem. Under this view, we recognize that preference data naturally forms a semi-supervised classification setting. To address this, we propose a data augmentation strategy that transforms preference comparisons into fully supervised training signals. We then introduce a classification-based ABC loss to guide alignment. Our alignment by classification approach could effectively steer the diffusion model toward the behavior of the ideal reference. Experiments on various diffusion models show that our ABC consistently outperforms existing baselines, offering a scalable and robust solution for preference-based text-to-image fine-tuning",
    "checked": false,
    "id": "f5275c61736781d236abe6700b822f1ea62f982e",
    "semantic_title": "diffusion model alignment using direct preference optimization",
    "citation_count": 440,
    "authors": []
  },
  "https://openreview.net/forum?id=AUs0rScwK0": {
    "title": "Sketched Gaussian Mechanism for Private Federated Learning",
    "volume": "spotlight",
    "abstract": "Communication cost and privacy are two major considerations in federated learning (FL). For communication cost, gradient compression by sketching the clients' transmitted model updates is often used for reducing per‐round communication. For privacy, the Gaussian mechanism (GM), which consists of clipping updates and adding Gaussian noise, is commonly used to guarantee client‐level differential privacy. Existing literature on private FL analyzes privacy of sketching and GM in an isolated manner, illustrating that sketching provides privacy determined by the sketching dimension and that GM has to supply any additional desired privacy. In this paper, we introduce the Sketched Gaussian Mechanism (SGM), which directly combines sketching and the Gaussian mechanism for privacy. Using Rényi-DP tools, we present a joint analysis of SGM's overall privacy guarantee, which is significantly more flexible and sharper compared to isolated analysis of sketching and GM privacy. In particular, we prove that the privacy level of SGM for a fixed noise magnitude is proportional to $1/\\sqrt{b}$, where $b$ is the sketching dimension, indicating that (for moderate $b$) SGM can provide much stronger privacy guarantees than the original GM under the same noise budget. We demonstrate the application of SGM to FL with either gradient descent or adaptive server optimizers, and establish theoretical results on optimization convergence, which exhibits only a logarithmic dependence on the number of parameters $d$. Experimental results confirm that at the same privacy level, SGM based FL is at least competitive with non‐sketching private FL variants and outperforms them in some settings. Moreover, using adaptive optimization at the server improves empirical performance while maintaining the privacy guarantees",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UOaLsgn5wb": {
    "title": "Joint‑Embedding vs Reconstruction: Provable Benefits of Latent Space Prediction for Self‑Supervised Learning",
    "volume": "spotlight",
    "abstract": "Reconstruction and joint-embedding have emerged as two leading paradigms in Self‑Supervised Learning (SSL). Reconstruction methods focus on recovering the original sample from a different view in input space. On the other hand, joint-embedding methods align the representations of different views in latent space. Both approaches offer compelling advantages, yet practitioners lack clear guidelines for choosing between them. In this work, we unveil the core mechanisms that distinguish each paradigm. By leveraging closed-form solutions for both approaches, we precisely characterize how the view generation process, e.g. data augmentation, impacts the learned representations. We then demonstrate that, unlike supervised learning, both SSL paradigms require a minimal alignment between augmentations and irrelevant features to achieve asymptotic optimality with increasing sample size. Our findings indicate that in scenarios where these irrelevant features have a large magnitude, joint-embedding methods are preferable because they impose a strictly weaker alignment condition compared to reconstruction-based methods. These results not only clarify the trade-offs between the two paradigms but also substantiate the empirical success of joint-embedding approaches on real-world challenging datasets",
    "checked": false,
    "id": "fc4448efee6ad21d14dae0d28fdb0944c338a322",
    "semantic_title": "joint embedding vs reconstruction: provable benefits of latent space prediction for self supervised learning",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=sXpyn3lAb5": {
    "title": "Accelerating data-driven algorithm selection for combinatorial partitioning problems",
    "volume": "spotlight",
    "abstract": "Data-driven algorithm selection is a powerful approach for choosing effective heuristics for computational problems. It operates by evaluating a set of candidate algorithms on a collection of representative training instances and selecting the one with the best empirical performance. However, running each algorithm on every training instance is computationally expensive, making scalability a central challenge. In practice, a common workaround is to evaluate algorithms on smaller proxy instances derived from the original inputs. However, this practice has remained largely ad hoc and lacked theoretical grounding. We provide the first theoretical foundations for this practice by formalizing the notion of size generalization: predicting an algorithm's performance on a large instance by evaluating it on a smaller, representative instance, subsampled from the original instance. We provide size generalization guarantees for three widely used clustering algorithms (single-linkage, k-means++, and Gonzalez's k-centers heuristic) and two canonical max-cut algorithms (Goemans-Williamson and Greedy). We characterize the subsample size sufficient to ensure that performance on the subsample reflects performance on the full instance, and our experiments support these findings",
    "checked": false,
    "id": "fdee9f9e84172aa3eb43c1b6ae8bb9f4b9b66721",
    "semantic_title": "revisiting sampling for combinatorial optimization",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=kePsKwxvaV": {
    "title": "Purifying Shampoo: Investigating Shampoo's Heuristics by Decomposing its Preconditioner",
    "volume": "spotlight",
    "abstract": "The recent success of Shampoo in the AlgoPerf contest has sparked renewed interest in Kronecker-factorization-based optimization algorithms for training neural networks. Despite its success, Shampoo relies heavily on several heuristics such as learning rate grafting and stale preconditioning to achieve performance at-scale. These heuristics increase algorithmic complexity, necessitate further hyperparameter tuning, and lack theoretical justification. This paper investigates these heuristics from the angle of Frobenius norm approximation to full-matrix Adam and decouples the preconditioner's eigenvalues and eigenbasis updates. We show that grafting from Adam mitigates the staleness and mis-scaling of the preconditioner's *eigenvalues* and how correcting the eigenvalues directly eliminates the need for learning rate grafting. To manage the error induced by infrequent *eigenbasis* computations, we propose an adaptive criterion for determining the eigenbasis computation frequency motivated by terminating a warm-started QR algorithm. This criterion decouples the update frequency of different preconditioner matrices and enables us to investigate the impact of approximation error on convergence. These practical techniques offer a principled angle towards removing Shampoo's heuristics and developing improved Kronecker-factorization-based training algorithms",
    "checked": true,
    "id": "73df0ae4217d0600c9d481e92210dc0cca74b386",
    "semantic_title": "purifying shampoo: investigating shampoo's heuristics by decomposing its preconditioner",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=qYDBgSeAlU": {
    "title": "Replicable Distribution Testing",
    "volume": "spotlight",
    "abstract": "We initiate a systematic investigation of distribution testing in the framework of algorithmic replicability. Specifically, given independent samples from a collection of probability distributions, the goal is to characterize the sample complexity of replicably testing natural properties of the underlying distributions. On the algorithmic front, we develop new replicable algorithms for testing closeness and independence of discrete distributions. On the lower bound front, we develop a new methodology for proving sample complexity lower bounds for replicable testing that may be of broader interest. As an application of our technique, we establish near-optimal sample complexity lower bounds for replicable uniformity testing---answering an open question from prior work---and closeness testing",
    "checked": true,
    "id": "9451d1f29296258bc3ed73a6a32e46c1494880f3",
    "semantic_title": "replicable distribution testing",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A9jXG3FUMT": {
    "title": "FAPEX: Fractional Amplitude-Phase Expressor for Robust Cross-Subject Seizure Prediction",
    "volume": "spotlight",
    "abstract": "Precise, generalizable subject-agnostic seizure prediction (SASP) remains a fundamental challenge due to the intrinsic complexity and significant spectral variability of electrophysiologial signals across individuals and recording modalities. We propose \\model{FAPEX}, a novel architecture that introduces a learnable \\emph{fractional neural frame operator} (FrNFO) for adaptive time–frequency decomposition. Unlike conventional models that exhibit spectral bias toward low frequencies, our FrNFO employs fractional-order convolutions to capture both high and low-frequency dynamics, achieving approximately $10\\%$ improvement in F1-score and sensitivity over state-of-the-art baselines. The FrNFO enables the extraction of \\emph{instantaneous phase and amplitude representations} that are particularly informative for preictal biomarker discovery and enhance out-of-distribution generalization. \\model{FAPEX} further integrates structural state-space modeling and channelwise attention, allowing it to handle heterogeneous electrode montages. Evaluated across 12 benchmarks spanning species (human, rat, dog, macaque) and modalities (Scalp‑EEG, SEEG, ECoG, LFP), \\model{FAPEX} consistently outperforms 23 supervised and 10 self-supervised baselines under nested cross-validation, with gains of up to $15\\%$ in sensitivity on complex cross-domain scenarios. It further demonstrates superior performance in several external validation cohorts. To our knowledge, these establish \\model{FAPEX} as the first epilepsy model to show consistent superiority in SASP, offering a promising solution for discovering epileptic biomarker evidence supporting the existence of a distinct and identifiable preictal state for and clinical translation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dfcQFL89OM": {
    "title": "Causality Meets Locality: Provably Generalizable and Scalable Policy Learning for Networked Systems",
    "volume": "spotlight",
    "abstract": "Large‑scale networked systems, such as traffic, power, and wireless grids, challenge reinforcement‑learning agents with both scale and environment shifts. To address these challenges, we propose \\texttt{GSAC} (\\textbf{G}eneralizable and \\textbf{S}calable \\textbf{A}ctor‑\\textbf{C}ritic), a framework that couples causal representation learning with meta actor‑critic learning to achieve both scalability and domain generalization. Each agent first learns a sparse local causal mask that provably identifies the minimal neighborhood variables influencing its dynamics, yielding exponentially tight approximately compact representations (ACRs) of state and domain factors. These ACRs bound the error of truncating value functions to $\\kappa$-hop neighborhoods, enabling efficient learning on graphs. A meta actor‑critic then trains a shared policy across multiple source domains while conditioning on the compact domain factors; at test time, a few trajectories suffice to estimate the new domain factor and deploy the adapted policy. We establish finite‑sample guarantees on causal recovery, actor-critic convergence, and adaptation gap, and show that \\texttt{GSAC} adapts rapidly and significantly outperforms learning-from-scratch and conventional adaptation baselines",
    "checked": true,
    "id": "798571a6cde33e76fd349704019a82bab7789d74",
    "semantic_title": "causality meets locality: provably generalizable and scalable policy learning for networked systems",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZvUZvT8tgg": {
    "title": "Conformal Mixed-Integer Constraint Learning with Feasibility Guarantees",
    "volume": "spotlight",
    "abstract": "We propose Conformal Mixed-Integer Constraint Learning (C-MICL), a novel framework that provides probabilistic feasibility guarantees for data-driven constraints in optimization problems. While standard Mixed-Integer Constraint Learning methods often violate the true constraints due to model error or data limitations, our C-MICL approach leverages conformal prediction to ensure feasible solutions are ground-truth feasible with probability at least $1{-}\\alpha$, under a conditional independence assumption. The proposed framework supports both regression and classification tasks without requiring access to the true constraint function, while avoiding the scalability issues associated with ensemble-based heuristics. Experiments on real-world applications demonstrate that C-MICL consistently achieves target feasibility rates, maintains competitive objective performance, and significantly reduces computational cost compared to existing methods. Our work bridges mathematical optimization and machine learning, offering a principled approach to incorporate uncertainty-aware constraints into decision-making with rigorous statistical guarantees",
    "checked": true,
    "id": "3940577a7c858686a917200adfe2250939a645cd",
    "semantic_title": "conformal mixed-integer constraint learning with feasibility guarantees",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1Imp4KZyjA": {
    "title": "Why Do Some Language Models Fake Alignment While Others Don't?",
    "volume": "spotlight",
    "abstract": "*Alignment Faking in Large Language Models* presented a demonstration of Claude 3 Opus and Claude 3.5 Sonnet selectively complying with a helpful-only training objective to prevent modification of their behavior outside of training. We expand this analysis to 25 models and find that only 5 (Claude 3 Opus, Claude 3.5 Sonnet, Llama 3 405B, Grok 3, Gemini 2.0 Flash) comply with harmful queries more when they infer they are in training than when they infer they are in deployment. First, we study the motivations of these 5 models. Results from perturbing details of the scenario suggest that only Claude 3 Opus's compliance gap is primarily and consistently motivated by trying to keep its goals. Second, we investigate why many chat models don't fake alignment. Our results suggest this is not entirely due to a lack of capabilities: many base models fake alignment some of the time, and post-training eliminates alignment-faking for some models and amplifies it for others. We investigate 5 hypotheses for how post-training may suppress alignment faking and find that variations in refusal behavior may account for a significant portion of differences in alignment faking",
    "checked": true,
    "id": "4bdbf1457c25f883d574918980d612ce043c268d",
    "semantic_title": "why do some language models fake alignment while others don't?",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=36uy2GgAy6": {
    "title": "⁠When Data Can't Meet: Estimating Correlation Across Privacy Barriers",
    "volume": "spotlight",
    "abstract": "We consider the problem of estimating the correlation of two random variables $X$ and $Y$, where the pairs $(X,Y)$ are not observed together, but are instead separated co-ordinate-wise at two servers: server 1 contains all the $X$ observations, and server 2 contains the corresponding $Y$ observations. In this vertically distributed setting, we assume that each server has its own privacy constraints, owing to which they can only share suitably privatized statistics of their own component observations. We consider differing privacy budgets $(\\varepsilon_1,\\delta_1)$ and $(\\varepsilon_2,\\delta_2)$ for the two servers and determine the minimax optimal rates for correlation estimation allowing for both non-interactive and interactive mechanisms. We also provide correlation estimators that achieve these rates and further develop inference procedures, namely, confidence intervals, for the estimated correlations. Our results are characterized by an interesting rate in terms of the sample size $n$, $\\varepsilon_1$, $\\varepsilon_2$, which is strictly slower than the usual central privacy estimation rates. More interestingly, we find that the interactive mechanism is always better than its non-interactive counterpart whenever the two privacy budgets are different. Results from extensive numerical experiments support our theoretical findings",
    "checked": false,
    "id": "abeb41624d448307b2ead01e2297438d113ef559",
    "semantic_title": "proceedings of the 10th fragility fracture network congress held 20-22nd october 2022, melbourne australia p01: driving up the standard of care: the irish hip fracture database 8 years on",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=M6l3pyvUfr": {
    "title": "TRIDENT: Tri-Modal Molecular Representation Learning with Taxonomic Annotations and Local Correspondence",
    "volume": "spotlight",
    "abstract": "Molecular property prediction aims to learn representations that map chemical structures to functional properties. While multimodal learning has emerged as a powerful paradigm to learn molecular representations, prior works have largely overlooked textual and taxonomic information of molecules for representation learning. We introduce TRIDENT, a novel framework that integrates molecular SMILES, textual descriptions, and taxonomic functional annotations to learn rich molecular representations. To achieve this, we curate a comprehensive dataset of molecule-text pairs with structured, multi-level functional annotations. Instead of relying on conventional contrastive loss, TRIDENT employs a volume-based alignment objective to jointly align tri-modal features at the global level, enabling soft, geometry-aware alignment across modalities. Additionally, TRIDENT introduces a novel local alignment objective that captures detailed relationships between molecular substructures and their corresponding sub-textual descriptions. A momentum-based mechanism dynamically balances global and local alignment, enabling the model to learn both broad functional semantics and fine-grained structure-function mappings. TRIDENT achieves state-of-the-art performance on 18 downstream tasks, demonstrating the value of combining SMILES, textual, and taxonomic functional annotations for molecular property prediction. Our code and data are available at https://github.com/uta-smile/TRIDENT",
    "checked": true,
    "id": "c326e0a4adb1a5f3e94e9d75e6420dad2b3eac1a",
    "semantic_title": "trident: tri-modal molecular representation learning with taxonomic annotations and local correspondence",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=bzxlOyjWbU": {
    "title": "Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences",
    "volume": "spotlight",
    "abstract": "We introduce the Deep Value Benchmark (DVB), an evaluation framework that directly tests whether large language models (LLMs) learn fundamental human values or merely surface-level preferences. This distinction is critical for AI alignment: Systems that capture deeper values are likely to generalize human intentions robustly, while those that capture only superficial patterns in preference data risk producing misaligned behavior. The DVB uses a novel experimental design with controlled confounding between deep values (e.g., moral principles) and shallow features (e.g., superficial attributes). In the training phase, we expose LLMs to human preference data with deliberately correlated deep and shallow features---for instance, where a user consistently prefers (non-maleficence, formal language) options over (justice, informal language) alternatives. The testing phase then breaks these correlations, presenting choices between (justice, formal language) and (non-maleficence, informal language) options. This design allows us to precisely measure a model's Deep Value Generalization Rate (DVGR)---the probability of generalizing based on the underlying value rather than the shallow feature. Across 9 different models, the average DVGR is just 0.30. All models generalize deep values less than chance. Larger models have a (slightly) lower DVGR than smaller models. We are releasing our dataset, which was subject to three separate human validation experiments. DVB provides an interpretable measure of a core feature of alignment",
    "checked": true,
    "id": "f70fc280c850db0dc21a3d8cd3eb896888e8c25c",
    "semantic_title": "deep value benchmark: measuring whether models generalize deep values or shallow preferences",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oLyfML1Qze": {
    "title": "Return of ChebNet: Understanding and Improving an Overlooked GNN on Long Range Tasks",
    "volume": "spotlight",
    "abstract": "ChebNet, one of the earliest spectral GNNs, has largely been overshadowed by Message Passing Neural Networks (MPNNs), which gained popularity for their simplicity and effectiveness in capturing local graph structure. Despite their success, MPNNs are limited in their ability to capture long-range dependencies between nodes. This has led researchers to adapt MPNNs through *rewiring* or make use of *Graph Transformers*, which compromise the computational efficiency that characterized early spatial message passing architectures, and typically disregard the graph structure. Almost a decade after its original introduction, we revisit ChebNet to shed light on its ability to model distant node interactions. We find that out-of-box, ChebNet already shows competitive advantages relative to classical MPNNs and GTs on long-range benchmarks, while maintaining good scalability properties for high-order polynomials. However, we uncover that this polynomial expansion leads ChebNet to an unstable regime during training. To address this limitation, we cast ChebNet as a stable and non-dissipative dynamical system, which we coin Stable-ChebNet. Our Stable-ChebNet model allows for stable information propagation, and has controllable dynamics which do not require the use of eigendecompositions, positional encodings, or graph rewiring. Across several benchmarks, Stable-ChebNet achieves near state-of-the-art performance",
    "checked": true,
    "id": "588afb39cd7182b0140905d46494d8b8d8c0b9b6",
    "semantic_title": "return of chebnet: understanding and improving an overlooked gnn on long range tasks",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=rVyBrD8h2b": {
    "title": "Preconditioned Langevin Dynamics with Score-based Generative Models for Infinite-Dimensional Linear Bayesian Inverse Problems",
    "volume": "spotlight",
    "abstract": "Designing algorithms for solving high-dimensional Bayesian inverse problems directly in infinite‑dimensional function spaces – where such problems are naturally formulated – is crucial to ensure stability and convergence as the discretization of the underlying problem is refined. In this paper, we contribute to this line of work by analyzing a widely used sampler for linear inverse problems: Langevin dynamics driven by score‑based generative models (SGMs) acting as priors, formulated directly in function space. Building on the theoretical framework for SGMs in Hilbert spaces, we give a rigorous definition of this sampler in the infinite-dimensional setting and derive, for the first time, error estimates that explicitly depend on the approximation error of the score. As a consequence, we obtain sufficient conditions for global convergence in Kullback–Leibler divergence on the underlying function space. Preventing numerical instabilities requires preconditioning of the Langevin algorithm and we prove the existence and form of an optimal preconditioner. The preconditioner depends on both the score error and the forward operator and guarantees a uniform convergence rate across all posterior modes. Our analysis applies to both Gaussian and a general class of non‑Gaussian priors. Finally, we present examples that illustrate and validate our theoretical findings",
    "checked": true,
    "id": "a11956a2b4517e9a2c03694d609d61676e0f8904",
    "semantic_title": "preconditioned langevin dynamics with score-based generative models for infinite-dimensional linear bayesian inverse problems",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qvvy0X63Fv": {
    "title": "Transferring Linear Features Across Language Models With Model Stitching",
    "volume": "spotlight",
    "abstract": "In this work, we demonstrate that affine mappings between residual streams of language models is a cheap way to effectively transfer represented features between models. We apply this technique to transfer the \\textit{weights} of Sparse Autoencoders (SAEs) between models of different sizes to compare their representations. We find that small and large models learn highly similar representation spaces, which motivates training expensive components like SAEs on a smaller model and transferring to a larger model at a FLOPs savings. For example, using a small-to-large transferred SAE as initialization can lead to 50% cheaper training runs when training SAEs on larger models. Next, we show that transferred probes and steering vectors can effectively recover ground truth performance. Finally, we dive deeper into feature-level transferability, finding that semantic and structural features transfer noticeably differently while specific classes of functional features have their roles faithfully mapped. Overall, our findings illustrate similarities and differences in the linear representation spaces of small and large models and demonstrate a method for improving the training efficiency of SAEs",
    "checked": true,
    "id": "9855a9cca633223a08b27625d36e4b15cd8dfbcc",
    "semantic_title": "transferring linear features across language models with model stitching",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Zf6Oj5x9sE": {
    "title": "Causal Differentiating Concepts: Interpreting LM Behavior via Causal Representation Learning",
    "volume": "spotlight",
    "abstract": "Language model activations entangle concepts that mediate their behavior, making it difficult to interpret these factors, which has implications for generalizability and robustness. We introduce an approach for disentangling these concepts without supervision. Existing methods for concept discovery often rely on external labels, contrastive prompts, or known causal structures, which limits their scalability and biases them toward predefined, easily annotatable features. In contrast, we propose a new unsupervised algorithm that identifies causal differentiating concepts—interpretable latent directions in LM activations that must be changed to elicit a different model behavior. These concepts are discovered using a constrained contrastive learning objective, guided by the insight that eliciting a target behavior requires only sparse changes to the underlying concepts. We formalize this notion and show that, under a particular assumption about the sparsity of these causal differentiating concepts, our method learns disentangled representations that align with human-interpretable factors influencing LM decisions. We empirically show the ability of our method to recover ground-truth causal factors in synthetic and semi-synthetic settings. Additionally, we illustrate the utility of our method through a case study on refusal behavior in language models. Our approach offers a scalable and interpretable lens into the internal workings of LMs, providing a principled foundation for interpreting language model behavior",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xll01vw606": {
    "title": "Ambient Proteins - Training Diffusion Models on Noisy Structures",
    "volume": "spotlight",
    "abstract": "We present Ambient Protein Diffusion, a framework for training protein diffusion models that generates structures with unprecedented diversity and quality. State-of-the-art generative models are trained on computationally derived structures from AlphaFold2 (AF), as experimentally determined structures are relatively scarce. The resulting models are therefore limited by the quality of synthetic datasets. Since the accuracy of AF predictions degrades with increasing protein length and complexity, de novo generation of long, complex proteins remains challenging. Ambient Protein Diffusion overcomes this problem by treating low-confidence AF structures as corrupted data. Rather than simply filtering out low-quality AF structures, our method adjusts the diffusion objective for each structure based on its corruption level, allowing the model to learn from both high and low quality structures. Empirically, ambient protein diffusion yields major improvements: on proteins with 700 residues, diversity increases from 45% to 85% from the previous state-of-the-art, and designability improves from 70% to 88%",
    "checked": false,
    "id": "8bc7b409ef2e8479b7b184e8b408b9e0ac9545b7",
    "semantic_title": "ambient proteins: training diffusion models on low quality structures",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=V4YAS7NLXi": {
    "title": "On Universality Classes of Equivariant Networks",
    "volume": "spotlight",
    "abstract": "Equivariant neural networks provide a principled framework for incorporating symmetry into learning architectures and have been extensively analyzed through the lens of their *separation power*, that is, the ability to distinguish inputs modulo symmetry. This notion plays a central role in settings such as graph learning, where it is often formalized via the Weisfeilern&ndash;Leman hierarchy. In contrast, the *universality* of equivariant models&mdash;their capacity to approximate target functions&mdash;remains comparatively underexplored. In this work, we investigate the approximation power of equivariant neural networks beyond separation constraints. We show that separation power does not fully capture expressivity: models with identical separation power may differ in their approximation ability. To demonstrate this, we characterize the universality classes of shallow invariant networks, providing a general framework for understanding which functions these architectures can approximate. Since equivariant models reduce to invariant ones under projection, this analysis yields sufficient conditions under which shallow equivariant networks fail to be universal. Conversely, we identify settings where shallow models do achieve separation-constrained universality. These positive results, however, depend critically on structural properties of the symmetry group, such as the existence of adequate normal subgroups, which may not hold in important cases like permutation symmetry",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9FDErIfoVE": {
    "title": "Guarantees for Alternating Least Squares in Overparameterized Tensor Decompositions",
    "volume": "spotlight",
    "abstract": "Tensor decomposition is a canonical non-convex optimization problem that is computationally challenging, and yet important due to applications in factor analysis and parameter estimation of latent variable models. In practice, scalable iterative methods, particularly Alternating Least Squares (ALS), remain the workhorse for tensor decomposition despite the lack of global convergence guarantees. A popular approach to tackle challenging non-convex optimization problems is overparameterization--- on input an $n \\times n \\times n$ tensor of rank $r$, the algorithm can output a decomposition of potentially rank $k$ (potentially larger than $r$). On the theoretical side, overparameterization for iterative methods is challenging to reason about and requires new techniques. The work of Wang et al., (NeurIPS 2020) makes progress by showing that a variant of gradient descent globally converges when overparameterized to $k=O(r^{7.5} \\log n)$. Our main result shows that overparameterization provably enables global convergence of ALS: on input a third order $n \\times n \\times n$ tensor with a decomposition of rank $r \\ll n$, ALS overparameterized with rank $k=O(r^2)$ achieves global convergence with high probability under random initialization. Moreover our analysis also gives guarantees for the more general low-rank approximation problem. The analysis introduces new techniques for understanding iterative methods in the overparameterized regime based on new matrix anticoncentration arguments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RPRqKhjrr6": {
    "title": "Checklists Are Better Than Reward Models For Aligning Language Models",
    "volume": "spotlight",
    "abstract": "Language models must be adapted to understand and follow user instructions. Reinforcement learning is widely used to facilitate this —typically using fixed criteria such as \"helpfulness\" and \"harmfulness\". In our work, we instead propose using flexible, instruction-specific criteria as a means of broadening the impact that reinforcement learning can have in eliciting instruction following. We propose \"Reinforcement Learning from Checklist Feedback\" (RLCF). From instructions, we extract checklists and evaluate how well responses satisfy each item—using both AI judges and specialized verifier programs—then combine these scores to compute rewards for RL. We compare RLCF with other alignment methods on top of a strong instruction following model (Qwen2.5-7B-Instruct) on five widely-studied benchmarks — RLCF is the only method to help on every benchmark, including a 4-point boost in hard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a 3-point rise in win rate on Arena-Hard. We show that RLCF can also be used off-policy to improve Llama 3.1 8B Instruct and OLMo 2 7B Instruct. These results establish rubrics as a key tool for improving language models' support of queries that express a multitude of needs. We release our our dataset of rubrics (WildChecklists), models, and code to the public",
    "checked": true,
    "id": "c9c21c4706d42afce45145b23b6bc50957ff4340",
    "semantic_title": "checklists are better than reward models for aligning language models",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=VRhVS59yhP": {
    "title": "Blackbox Model Provenance via Palimpsestic Membership Inference",
    "volume": "spotlight",
    "abstract": "Suppose Alice trains an open-weight language model and Bob uses a blackbox derivative of Alice's model to produce text. Can Alice prove that Bob is using her model, either by querying Bob's derivative model (query setting) or from the text alone ( observational setting)? We formulate this question as an independence testing problem—in which the null hypothesis is that Bob's model or text is independent of Alice's randomized training run—and investigate it through the lens of palimpsestic memorization in language models: models are more likely to memorize data seen later in training, so we can test whether Bob is using Alice's model using test statistics that capture correlation between Bob's model or text and the ordering of training examples in Alice's training run. If Alice has randomly shuffled her training data, then any significant correlation amounts to exactly quantifiable statistical evidence against the null hypothesis, regardless of the composition of Alice's training data. In the query setting, we directly estimate (via prompting) the likelihood Bob's model gives to Alice's training examples and their training order; we correlate the likelihoods of over 40 fine-tunes of various Pythia and OLMo base models ranging from 1B to 12B parameters with the base model's training data order, achieving a p-value on the order of at most $1 \\times 10^{-8}$ in all but six cases. In the observational setting, we try two approaches based on estimating 1) the likelihood of Bob's text overlapping with spans of Alice's training examples and 2) the likelihood of Bob's text with respect to different versions of Alice's model we obtain by repeating the last phase (e.g., 1%) of her training run on reshuffled data. The second approach can reliably distinguish Bob's text from as little as a few hundred tokens; the first does not involve any retraining but requires many more tokens (several hundred thousand) to achieve high power",
    "checked": true,
    "id": "15598567925e997d5ddf365bcbb9a5cde63725d5",
    "semantic_title": "blackbox model provenance via palimpsestic membership inference",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hZt0daVIZi": {
    "title": "Scaling can lead to compositional generalization",
    "volume": "spotlight",
    "abstract": "Can neural networks systematically capture discrete, compositional task structure despite their continuous, distributed nature? The impressive capabilities of large-scale neural networks suggest that the answer to this question is yes. However, even for the most capable models, there are still frequent failure cases that raise doubts about their compositionality. Here, we seek to understand what it takes for a standard neural network to generalize over tasks that share compositional structure. We find that simply scaling data and model size leads to compositional generalization. We show that this holds across different task encodings as long as the training distribution sufficiently covers the task space. In line with this finding, we prove that standard multilayer perceptrons can approximate a general class of compositional task families to arbitrary precision using only a linear number of neurons with respect to the number of task modules. Finally, we uncover that if networks successfully compositionally generalize, the constituents of a task can be linearly decoded from their hidden activations. We show that this metric correlates with failures of text-to-image generation models to compose known concepts",
    "checked": true,
    "id": "c0cfbf7111212a0ec4631118943a2f50f64613b5",
    "semantic_title": "scaling can lead to compositional generalization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R0dC7Xzwbk": {
    "title": "Prismatic Synthesis: Gradient-based Data Diversification Boosts Generalization in LLM Reasoning",
    "volume": "spotlight",
    "abstract": "Data diversity is crucial for training a strong language model. Yet metrics of diversity often diverge from this goal, measuring variations in heuristic features—like n-grams or embeddings—that are detached from how the model actually performs on a target task. This motivates us to ask: *Can we redefine data diversity—beyond measuring variations in heuristic features—in a way that better predicts model generalization?* Through large-scale empirical analyses spanning over 300 training runs, carefully controlled for data scale and quality, we show that data diversity can be a strong predictor of generalization in LLM reasoning—as measured by average model performance on unseen out-of-distribution benchmarks. We introduce **G-Vendi**, a metric that quantifies diversity via the entropy of model-induced loss gradients. G-Vendi scales to million-sample datasets and yet consistently outperforms heuristic alternatives, achieving strong correlation ($\\text{Spearman's } \\rho \\approx 0.9$) with out-of-distribution (OOD) performance across both natural language inference (NLI) and math reasoning tasks. Building on this insight, we present **Prismatic Synthesis**, a framework for generating diverse synthetic data by targeting underrepresented regions in gradient space. Experimental results show that Prismatic Synthesis consistently improves model performance as we scale synthetic data—not just on in-distribution test but across unseen, out-of-distribution benchmarks—significantly outperforming state-of-the-art models in both domains. For example, PrismMath-7B, our model distilled from a 32B LLM without human verification, outperforms R1-Distill-Qwen-7B—trained on proprietary data generated by 671B R1—on 6 out of 7 challenging math benchmarks",
    "checked": true,
    "id": "b71a9a4236c18c1972a96f163de497664eaa5f88",
    "semantic_title": "prismatic synthesis: gradient-based data diversification boosts generalization in llm reasoning",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=qQbvLU34F1": {
    "title": "AnaCP: Toward Upper-Bound Continual Learning via Analytic Contrastive Projection",
    "volume": "spotlight",
    "abstract": "This paper studies the problem of class-incremental learning (CIL), a core setting within continual learning where a model learns a sequence of tasks, each containing a distinct set of classes. Traditional CIL methods, which do not leverage pre-trained models (PTMs), suffer from catastrophic forgetting (CF) due to the need to incrementally learn both feature representations and the classifier. The integration of PTMs into CIL has recently led to efficient approaches that treat the PTM as a fixed feature extractor combined with analytic classifiers, achieving state-of-the-art performance. However, they still face a major limitation: the inability to continually adapt feature representations to best suit the CIL tasks, leading to suboptimal performance. To address this, we propose AnaCP (Analytic Contrastive Projection), a novel method that preserves the efficiency of analytic classifiers while enabling incremental feature adaptation without gradient-based training, thereby eliminating the CF caused by gradient updates. Our experiments show that AnaCP not only outperforms existing baselines but also achieves the accuracy level of joint training, which is regarded as the upper bound of CIL",
    "checked": true,
    "id": "19853f24cee9dce8078a99b8d8198a8851be2610",
    "semantic_title": "anacp: toward upper-bound continual learning via analytic contrastive projection",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JzCjNJlSxI": {
    "title": "Flow Density Control: Generative Optimization Beyond Entropy-Regularized Fine-Tuning",
    "volume": "spotlight",
    "abstract": "Adapting large-scale foundational flow and diffusion generative models to optimize task-specific objectives while preserving prior information is crucial for real-world applications such as molecular design, protein docking, and creative image generation. Existing principled fine-tuning methods aim to maximize the expected reward of generated samples, while retaining knowledge from the pre-trained model via KL-divergence regularization. In this work, we tackle the significantly more general problem of optimizing general utilities beyond average rewards, including risk-averse and novelty-seeking reward maximization, diversity measures for exploration, and experiment design objectives among others. Likewise, we consider more general ways to preserve prior information beyond KL-divergence, such as optimal transport distances and Rényi divergences. To this end, we introduce Flow Density Control (FDC), a simple algorithm that reduces this complex problem to a specific sequence of simpler fine-tuning tasks, each solvable via scalable established methods. We derive convergence guarantees for the proposed scheme under realistic assumptions by leveraging recent understanding of mirror flows. Finally, we validate our method on illustrative settings, text-to-image, and molecular design tasks, showing that it can steer pre-trained generative models to optimize objectives and solve practically relevant tasks beyond the reach of current fine-tuning schemes",
    "checked": false,
    "id": "6f394229b3451f0493a692df7cf7052f46253fdd",
    "semantic_title": "minimum-excess-work guidance",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ATYKgiDqt5": {
    "title": "What Expressivity Theory Misses: Message Passing Complexity for GNNs",
    "volume": "spotlight",
    "abstract": "Expressivity theory, characterizing which graphs a GNN can distinguish, has become the predominant framework for analyzing GNNs, with new models striving for higher expressivity. However, we argue that this focus is misguided: First, higher expressivity is not necessary for most real-world tasks as these tasks rarely require expressivity beyond the basic WL test. Second, expressivity theory's binary characterization and idealized assumptions fail to reflect GNNs' practical capabilities. To overcome these limitations, we propose Message Passing Complexity (MPC): a continuous measure that quantifies the difficulty for a GNN architecture to solve a given task through message passing. MPC captures practical limitations like over-squashing while preserving the theoretical impossibility results from expressivity theory, effectively narrowing the gap between theory and practice. Through extensive validation on fundamental GNN tasks, we show that MPC's theoretical predictions correlate with empirical performance, successfully explaining architectural successes and failures. Thereby, MPC advances beyond expressivity theory to provide a more powerful framework for understanding and developing GNN architectures",
    "checked": true,
    "id": "30ebe0e1bbe25f8275b36f36fcf29da7f9c18075",
    "semantic_title": "what expressivity theory misses: message passing complexity for gnns",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MVYz4GmcUH": {
    "title": "Ambient Diffusion Omni: Training Good Models with Bad Data",
    "volume": "spotlight",
    "abstract": "We show how to use low-quality, synthetic, and out-of-distribution images to improve the quality of a diffusion model. Typically, diffusion models are trained on curated datasets that emerge from highly filtered data pools from the Web and other sources. We show that there is immense value in the lower-quality images that are often discarded. We present Ambient Diffusion Omni, a simple, principled framework to train diffusion models that can extract signal from arbitrarily images during training. Our framework exploits two properties of natural images -- spectral power law decay and locality. We first validate our framework by successfully training diffusion models with images synthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We use our framework to achieve state-of-the-art ImageNet FID and we show significant improvements in both image quality and diversity for text-to-image generative modeling. The core insight is that noise dampens the initial skew between the desired high-quality distribution and the mixed distribution we actually observe. We provide rigorous theoretical justification for our approach by analyzing the trade-off between learning from biased data versus limited unbiased data across diffusion times",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SvopaNxYWt": {
    "title": "UMA: A Family of Universal Models for Atoms",
    "volume": "spotlight",
    "abstract": "The ability to quickly and accurately compute properties from atomic simulations is critical for advancing a large number of applications in chemistry and materials science including drug discovery, energy storage, and semiconductor manufacturing. To address this need, we present a family of Universal Models for Atoms (UMA), designed to push the frontier of speed, accuracy, and generalization. UMA models are trained on half a billion unique 3D atomic structures (the largest training runs to date) by compiling data across multiple chemical domains, e.g. molecules, materials, and catalysts. We develop empirical scaling laws to help understand how to increase model capacity alongside dataset size to achieve the best accuracy. The UMA small and medium models utilize a novel architectural design we refer to as mixture of linear experts that enables increasing model capacity without sacrificing speed. For example, UMA-medium has 1.4B parameters but only $\\sim$50M active parameters per atomic structure. We evaluate UMA models on a diverse set of applications across multiple domains and find that, remarkably, a single model without any fine-tuning can perform similarly or better than specialized models. We are releasing the UMA code, weights, and associated data to accelerate computational workflows and enable the community to build increasingly capable AI models",
    "checked": true,
    "id": "fb731024778c6e9104fa719217865b574c3e887a",
    "semantic_title": "uma: a family of universal models for atoms",
    "citation_count": 33,
    "authors": []
  },
  "https://openreview.net/forum?id=4jFSekBaDT": {
    "title": "The Best Instruction-Tuning Data are Those That Fit",
    "volume": "spotlight",
    "abstract": "High-quality supervised finetuning (SFT) data are essential for unlocking pretrained LLMs' capabilities. Typically, instructions are paired with responses from various sources—by human annotators or other LMs—which are often out of the distribution of the target model to be finetuned. At scale, this mismatch can lead to diminishing returns and even hurt model performance and robustness. We hypothesize that SFT is most effective when the data is aligned with the model's pretrained distribution, and propose **GRAPE**—a novel SFT framework that tailors supervision to the target model. For each instruction, it **g**athers **r**esponses from various sources and selects the one that **a**ligns most closely to the model's **pre**trained distribution, as measured by the normalized probability. Standard SFT is then performed on these selected responses. We first evaluate GRAPE in a controlled experiment, sampling multiple responses per question in the UltraInteract dataset from diverse models. We finetune using GRAPE-selected data on LMs from different families, including LLaMA-1-8B, Mistral-7B, and Qwen2.5-7B. GRAPE significantly outperforms strong baselines—including distilling from the strongest model—with absolute gains up to **13.8%** averaged across benchmarks, and outperforms a 3× larger data baseline with improvements up to **17.3%**. GRAPE's benefits generalize to off-the-shelf SFT data. When used to subsample from the post-training data of Tulu3 and Olmo-2, GRAPE surpasses strong baselines trained on 4.5× more data by **6.1%**, and outperforms state-of-the-art selection methods by **3.9%** on average. Notably, with only **1/3 the data** and **half the training epochs**, GRAPE enables LLaMA-1-8B to **exceed Tulu3-SFT performance by 3.5%**. Our findings highlight that aligning supervision with the pretrained distribution provides a simple yet powerful strategy to improve both the **efficiency** and **effectiveness** of SFT",
    "checked": true,
    "id": "d238f25614f15d329399843c2e94ee85aa057ff6",
    "semantic_title": "the best instruction-tuning data are those that fit",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=24UJqxw1kv": {
    "title": "Forecasting in Offline Reinforcement Learning for Non-stationary Environments",
    "volume": "spotlight",
    "abstract": "Offline Reinforcement Learning (RL) provides a promising avenue for training policies from pre-collected datasets when gathering additional interaction data is infeasible. However, existing offline RL methods often assume stationarity or only consider synthetic perturbations at test time—assumptions that often fail in real-world scenarios characterized by abrupt, time-varying offsets. These offsets can lead to partial observability, causing agents to misperceive their true state and degrade performance. To overcome this challenge, we introduce Forecasting in Non-stationary Offline RL (FORL), a framework that unifies (i) conditional diffusion-based candidate state generation, trained without presupposing any specific form of future non-stationarity, and (ii) zero-shot time-series foundation models. FORL targets environments prone to unexpected, potentially non-Markovian offsets, requiring robust agent performance from the onset of each episode. Empirical evaluations on offline RL benchmarks, augmented with real-world time-series data to simulate realistic non-stationarity, demonstrate that FORL consistently improves performance compared to competitive baselines. By integrating zero-shot forecasting with the agent's experience we aim to bridge the gap between offline RL and the complexity of real-world, non-stationary environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hHn8xGTRKO": {
    "title": "Purifying Approximate Differential Privacy with Randomized Post-processing",
    "volume": "spotlight",
    "abstract": "We propose a framework to convert $(\\varepsilon, \\delta)$-approximate Differential Privacy (DP) mechanisms into $(\\varepsilon', 0)$-pure DP mechanisms under certain conditions, a process we call ``purification.'' This algorithmic technique leverages randomized post-processing with calibrated noise to eliminate the $\\delta$ parameter while achieving near-optimal privacy-utility tradeoff for pure DP. It enables a new design strategy for pure DP algorithms: first run an approximate DP algorithm with certain conditions, and then purify. This approach allows one to leverage techniques such as strong composition and propose-test-release that require $\\delta>0$ in designing pure-DP methods with $\\delta=0$. We apply this framework in various settings, including Differentially Private Empirical Risk Minimization (DP-ERM), stability-based release, and query release tasks. To the best of our knowledge, this is the first work with a statistically and computationally efficient reduction from approximate DP to pure DP. Finally, we illustrate the use of this reduction for proving lower bounds under approximate DP constraints with explicit dependence in $\\delta$, avoiding the sophisticated fingerprinting code construction",
    "checked": true,
    "id": "2a01f007a80a53dd07072d5c8cff86cbbacdb7a3",
    "semantic_title": "purifying approximate differential privacy with randomized post-processing",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=6RlbOEcOS4": {
    "title": "Trust Region Constrained Measure Transport in Path Space for Stochastic Optimal Control and Inference",
    "volume": "spotlight",
    "abstract": "Solving stochastic optimal control problems with quadratic control costs can be viewed as approximating a target path space measure, e.g. via gradient-based optimization. In practice, however, this optimization is challenging in particular if the target measure differs substantially from the prior. In this work, we therefore approach the problem by iteratively solving constrained problems incorporating trust regions that aim for approaching the target measure gradually in a systematic way. It turns out that this trust region based strategy can be understood as a geometric annealing from the prior to the target measure, where, however, the incorporated trust regions lead to a principled and educated way of choosing the time steps in the annealing path. We demonstrate in multiple optimal control applications that our novel method can improve performance significantly, including tasks in diffusion-based sampling and fine-tuning of diffusion models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=004uTlSufe": {
    "title": "How Well Can Differential Privacy Be Audited in One Run?",
    "volume": "spotlight",
    "abstract": "Recent methods for auditing the privacy of machine learning algorithms have improved computational efficiency by simultaneously intervening on multiple training examples in a single training run. Steinke et al. prove that one-run auditing indeed lower bounds the true privacy parameter of the audited algorithm, and give impressive empirical results. Their work leaves open the question of how precisely one-run auditing can uncover the true privacy parameter of an algorithm, and how that precision depends on the audited algorithm. In this work, we characterize the maximum achievable efficacy of one-run auditing and show that the key barrier to its efficacy is interference between the observable effects of different data elements. We present new conceptual approaches to minimize this barrier, towards improving the performance of one-run auditing of real machine learning algorithms",
    "checked": true,
    "id": "90525b27eb1d63151ae572b3475c5ba457444621",
    "semantic_title": "how well can differential privacy be audited in one run?",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=qaHrpITIvB": {
    "title": "Counteractive RL: Rethinking Core Principles for Efficient and Scalable Deep Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "Following the pivotal success of learning strategies to win at tasks, solely by interacting with an environment without any supervision, agents have gained the ability to make sequential decisions in complex MDPs. Yet, reinforcement learning policies face exponentially growing state spaces in high dimensional MDPs resulting in a dichotomy between computational complexity and policy success. In our paper we focus on the agent's interaction with the environment in a high-dimensional MDP during the learning phase and we introduce a theoretically-founded novel paradigm based on experiences obtained through counteractive actions. Our analysis and method provide a theoretical basis for efficient, effective, scalable and accelerated learning, and further comes with zero additional computational complexity while leading to significant acceleration in training. We conduct extensive experiments in the Arcade Learning Environment with high-dimensional state representation MDPs. The experimental results further verify our theoretical analysis, and our method achieves significant performance increase with substantial sample-efficiency in high-dimensional environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bmoGGgSLzB": {
    "title": "Is Noise Conditioning Necessary? A Unified Theory of Unconditional Graph Diffusion Models",
    "volume": "spotlight",
    "abstract": "Explicit noise-level conditioning is widely regarded as essential for the effective operation of Graph Diffusion Models (GDMs). In this work, we challenge this assumption by investigating whether denoisers can implicitly infer noise levels directly from corrupted graph structures, potentially eliminating the need for explicit noise conditioning. To this end, we develop a theoretical framework centered on Bernoulli edge-flip corruptions and extend it to encompass more complex scenarios involving coupled structure-attribute noise. Extensive empirical evaluations on both synthetic and real-world graph datasets, using models such as GDSS and DiGress, provide strong support for our theoretical findings. Notably, unconditional GDMs achieve performance comparable or superior to their conditioned counterparts, while also offering reductions in parameters (4-6%) and computation time (8-10%). Our results suggest that the high-dimensional nature of graph data itself often encodes sufficient information for the denoising process, opening avenues for simpler, more efficient GDM architectures",
    "checked": true,
    "id": "87ddf3375d353994c85b5865acd8681d35c9dd92",
    "semantic_title": "is noise conditioning necessary? a unified theory of unconditional graph diffusion models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NgLFQTBPRR": {
    "title": "An Evidence-Based Post-Hoc Adjustment Framework for Anomaly Detection Under Data Contamination",
    "volume": "spotlight",
    "abstract": "Unsupervised anomaly detection (AD) methods typically assume clean training data, yet real-world datasets often contain undetected or mislabeled anomalies, leading to significant performance degradation. Existing solutions require access to the training pipelines, data or prior knowledge of the proportions of anomalies in the data, limiting their real-world applicability. To address this challenge, we propose EPHAD, a simple yet effective test-time adaptation framework that updates the outputs of AD models trained on contaminated datasets using evidence gathered at test time. Our approach integrates the prior knowledge captured by the AD model trained on contaminated datasets with evidence derived from multimodal foundation models like Contrastive Language-Image Pre-training (CLIP), classical AD methods like the Latent Outlier Factor or domain-specific knowledge. We illustrate the intuition behind EPHAD using a synthetic toy example and validate its effectiveness through comprehensive experiments across eight visual AD datasets, twenty-six tabular AD datasets, and a real-world industrial AD dataset. Additionally, we conduct an ablation study to analyse hyperparameter influence and robustness to varying contamination levels, demonstrating the versatility and robustness of EPHAD across diverse AD models and evidence pairs. To ensure reproducibility, our code is publicly available at https://github.com/sukanyapatra1997/EPHAD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wab4BEAUt6": {
    "title": "SHAP values via sparse Fourier representation",
    "volume": "spotlight",
    "abstract": "SHAP (SHapley Additive exPlanations) values are a widely used method for local feature attribution in interpretable and explainable AI. We propose an efficient two-stage algorithm for computing SHAP values in both black-box setting and tree-based models. Motivated by spectral bias in real-world predictors, we first approximate models using compact Fourier representations, exactly for trees and approximately for black-box models. In the second stage, we introduce a closed-form formula for {\\em exactly} computing SHAP values using the Fourier representation, that ``linearizes'' the computation into a simple summation and is amenable to parallelization. As the Fourier approximation is computed only once, our method enables amortized SHAP value computation, achieving significant speedups over existing methods and a tunable trade-off between efficiency and precision",
    "checked": true,
    "id": "7ee42892d7798651d3bf3fad17268ba47db29fcf",
    "semantic_title": "shap values via sparse fourier representation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=0EILv1HcmG": {
    "title": "Quantum Doubly Stochastic Transformers",
    "volume": "spotlight",
    "abstract": "At the core of the Transformer, the softmax normalizes the attention matrix to be right stochastic. Previous research has shown that this often de-stabilizes training and that enforcing the attention matrix to be doubly stochastic (through Sinkhorn's algorithm) consistently improves performance across different tasks, domains and Transformer flavors. However, Sinkhorn's algorithm is iterative, approximative, non-parametric and thus inflexible w.r.t. the obtained doubly stochastic matrix (DSM). Recently, it has been proven that DSMs can be obtained with a parametric quantum circuit, yielding a novel quantum inductive bias for DSMs with no known classical analogue. Motivated by this, we demonstrate the feasibility of a hybrid classical-quantum doubly stochastic Transformer (QDSFormer) that replaces the softmax in the self-attention layer with a variational quantum circuit. We study the expressive power of the circuit and find that it yields more diverse DSMs that better preserve information than classical operators. Across multiple small-scale object recognition tasks, we find that our QDSFormer consistently surpasses both a standard ViT and other doubly stochastic Transformers. Beyond the Sinkformer, this comparison includes a novel quantum-inspired doubly stochastic Transformer (based on QR decomposition) that can be of independent interest. Our QDSFormer also shows improved training stability and lower performance variation suggesting that it may mitigate the notoriously unstable training of ViTs on small-scale data",
    "checked": true,
    "id": "fdf1ac9c0902dc9e2c127f4128c22b73277367c8",
    "semantic_title": "quantum doubly stochastic transformers",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=heY0zzGvYm": {
    "title": "Reverse Engineering Human Preferences with Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "The capabilities of Large Language Models (LLMs) are routinely evaluated by other LLMs trained to predict human preferences. This framework—known as *LLM-as-a-judge*—is highly scalable and relatively low cost. However, it is also vulnerable to malicious exploitation, as LLM responses can be tuned to overfit the preferences of the judge. Previous work shows that the answers generated by a candidate-LLM can be edited *post hoc* to maximise the score assigned to them by a judge-LLM. In this study, we adopt a different approach and use the signal provided by judge-LLMs as a reward to adversarially tune models that generate text preambles designed to boost downstream performance. We find that frozen LLMs pipelined with these models attain higher LLM-evaluation scores than existing frameworks. Crucially, unlike other frameworks which intervene directly on the model's response, our method is virtually undetectable. We also demonstrate that the effectiveness of the tuned preamble generator transfers when the candidate-LLM and the judge-LLM are replaced with models that are not used during training. These findings raise important questions about the design of more reliable LLM-as-a-judge evaluation settings. They also demonstrate that human preferences can be reverse engineered effectively, by pipelining LLMs to optimise upstream preambles via reinforcement learning—an approach that could find future applications in diverse tasks and domains beyond adversarial attacks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=65oFEAP42P": {
    "title": "Efficient Fairness-Performance Pareto Front Computation",
    "volume": "spotlight",
    "abstract": "There is a well known intrinsic trade-off between the fairness of a representation and the performance of classifiers derived from the representation. In this paper we propose a new method to compute the optimal Pareto front of this trade off. In contrast to the existing methods, this approach does not require the training of complex fair representation models. Our approach is derived through three main steps: We analyze fair representations theoretically, and derive several structural properties of optimal representations. We then show that these properties enable a reduction of the computation of the Pareto Front to a compact discrete problem. Finally, we show that these compact approximating problems can be efficiently solved via off-the shelf concave-convex programming methods. In addition to representations, we show that the new methods may also be used to directly compute the Pareto front of fair classification problems. Moreover, the proposed methods may be used with any concave performance measure. This is in contrast to the existing reduction approaches, developed recently in fair classification, which rely explicitly on the structure of the non-differentiable accuracy measure, and are thus unlikely to be extendable. The approach was evaluated on several real world benchmark datasets and compares favorably to a number of recent state of the art fair representation and classification methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SB1CsuJ11a": {
    "title": "Curl Descent : Non-Gradient Learning Dynamics with Sign-Diverse Plasticity",
    "volume": "spotlight",
    "abstract": "Gradient-based algorithms are a cornerstone of artificial neural network training, yet it remains unclear whether biological neural networks use similar gradient-based strategies during learning. Experiments often discover a diversity of synaptic plasticity rules, but whether these amount to an approximation to gradient descent is unclear. Here we investigate a previously overlooked possibility: that learning dynamics may include fundamentally non-gradient \"curl\"-like components while still being able to effectively optimize a loss function. Curl terms naturally emerge in networks with excitatory-inhibitory connectivity or Hebbian/anti-Hebbian plasticity, resulting in learning dynamics that cannot be framed as gradient descent on any objective. To investigate the impact of these curl terms, we analyze feedforward networks within an analytically tractable student-teacher framework, systematically introducing non-gradient dynamics through rule-flipped neurons. Small curl terms preserve the stability of the original solution manifold, resulting in learning dynamics similar to gradient descent. Beyond a critical value, strong curl terms destabilize the solution manifold. Depending on the network architecture, this loss of stability can lead to chaotic learning dynamics that destroy performance. In other cases, the curl terms can counterintuitively speed up learning compared to gradient descent by allowing the weight dynamics to escape saddles by temporarily ascending the loss. Our results identify specific architectures capable of supporting robust learning via diverse learning rules, providing an important counterpoint to normative theories of gradient-based learning in neural networks",
    "checked": false,
    "id": "f9c969bdc7a5e67316535cd83f9025fbb5187169",
    "semantic_title": "curl descent: non-gradient learning dynamics with sign-diverse plasticity",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GNDgQie8W4": {
    "title": "ElliCE: Efficient and Provably Robust Algorithmic Recourse via the Rashomon Sets",
    "volume": "spotlight",
    "abstract": "Machine learning models now influence decisions that directly affect people's lives, making it important to understand not only their predictions, but also how individuals could act to obtain better results. Algorithmic recourse provides actionable input modifications to achieve more favorable outcomes, typically relying on counterfactual explanations to suggest such changes. However, when the Rashomon set - the set of near-optimal models - is large, standard counterfactual explanations can become unreliable, as a recourse action valid for one model may fail under another. We introduce ElliCE, a novel framework for robust algorithmic recourse that optimizes counterfactuals over an ellipsoidal approximation of the Rashomon set. The resulting explanations are provably valid over this ellipsoid, with theoretical guarantees on uniqueness, stability, and alignment with key feature directions. Empirically, ElliCE generates counterfactuals that are not only more robust but also more flexible, adapting to user-specified features constraints while being substantially faster than existing baselines. This provides a principled and practical solution for reliable recourse under model uncertainty, ensuring stable recommendations for users even as models evolve",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WQ9rnkaUWm": {
    "title": "Head Pursuit: Probing Attention Specialization in Multimodal Transformers",
    "volume": "spotlight",
    "abstract": "Language and vision-language models have shown impressive performance across a wide range of tasks, but their internal mechanisms remain only partly understood. In this work, we study how individual attention heads in text-generative models specialize in specific semantic or visual attributes. Building on an established interpretability method, we reinterpret the practice of probing intermediate activations with the final decoding layer through the lens of signal processing. This lets us analyze multiple samples in a principled way and rank attention heads based on their relevance to target concepts. Our results show consistent patterns of specialization at the head level across both unimodal and multimodal transformers. Remarkably, we find that editing as few as 1% of the heads, selected using our method, can reliably suppress or enhance targeted concepts in the model output. We validate our approach on language tasks such as question answering and toxicity mitigation, as well as vision-language tasks including image classification and captioning. Our findings highlight an interpretable and controllable structure within attention layers, offering simple tools for understanding and editing large-scale generative models",
    "checked": true,
    "id": "ecf34c6cb9b3e972984ecaffba74242020777584",
    "semantic_title": "head pursuit: probing attention specialization in multimodal transformers",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yf8O4xEB4T": {
    "title": "Towards a Golden Classifier-Free Guidance Path via Foresight Fixed Point Iterations",
    "volume": "spotlight",
    "abstract": "Classifier-Free Guidance (CFG) is an essential component of text-to-image diffusion models, and understanding and advancing its operational mechanisms remains a central focus of research. Existing approaches stem from divergent theoretical interpretations, thereby limiting the design space and obscuring key design choices. To address this, we propose a unified perspective that reframes conditional guidance as fixed point iterations, seeking to identify a golden path where latents produce consistent outputs under both conditional and unconditional generation. We demonstrate that CFG and its variants constitute a special case of single-step short-interval iteration, which is theoretically proven to exhibit inefficiency. To this end, we introduce Foresight Guidance (FSG), which prioritizes solving longer-interval subproblems in early diffusion stages with increased iterations. Extensive experiments across diverse datasets and model architectures validate the superiority of FSG over state-of-the-art methods in both image quality and computational efficiency. Our work offers novel perspectives for conditional guidance and unlocks the potential of adaptive design",
    "checked": true,
    "id": "7940afa02912374849e0fc6d8b47af6e49987a48",
    "semantic_title": "towards a golden classifier-free guidance path via foresight fixed point iterations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rKM3oqruN3": {
    "title": "Credal Prediction based on Relative Likelihood",
    "volume": "spotlight",
    "abstract": "Predictions in the form of sets of probability distributions, so-called credal sets, provide a suitable means to represent a learner's epistemic uncertainty. In this paper, we propose a theoretically grounded approach to credal prediction based on the statistical notion of relative likelihood: The target of prediction is the set of all (conditional) probability distributions produced by the collection of plausible models, namely those models whose relative likelihood exceeds a specified threshold. This threshold has an intuitive interpretation and allows for controlling the trade-off between correctness and precision of credal predictions. We tackle the problem of approximating credal sets defined in this way by means of suitably modified ensemble learning techniques. To validate our approach, we illustrate its effectiveness by experiments on benchmark datasets demonstrating superior uncertainty representation without compromising predictive performance. We also compare our method against several state-of-the-art baselines in credal prediction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E7knuYAvpt": {
    "title": "The Power of Iterative Filtering for Supervised Learning with (Heavy) Contamination",
    "volume": "spotlight",
    "abstract": "Inspired by recent work on learning with distribution shift, we give a general outlier removal algorithm called *iterative polynomial filtering* and show a number of striking applications for supervised learning with contamination: (1) We show that any function class that can be approximated by low-degree polynomials with respect to a hypercontractive distribution can be efficiently learned under bounded contamination (also known as *nasty noise*). This is a surprising resolution to a longstanding gap between the complexity of agnostic learning and learning with contamination, as it was widely believed that low-degree approximators only implied tolerance to label noise. (2) For any function class that admits the (stronger) notion of sandwiching approximators, we obtain near-optimal learning guarantees even with respect to heavy additive contamination, where far more than $1/2$ of the training set may be added adversarially. Prior related work held only for regression and in a list-decodable setting. (3) We obtain the first efficient algorithms for tolerant testable learning of functions of halfspaces with respect to any fixed log-concave distribution. Even the non-tolerant case for a single halfspace in this setting had remained open. These results significantly advance our understanding of efficient supervised learning under contamination, a setting that has been much less studied than its unsupervised counterpart",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YXSKYFZweV": {
    "title": "Light-Weight Diffusion Multiplier and Uncertainty Quantification for Fourier Neural Operators",
    "volume": "spotlight",
    "abstract": "Operator learning is a powerful paradigm for solving partial differential equations, with Fourier Neural Operators serving as a widely adopted foundation. However, FNOs face significant scalability challenges due to overparameterization and offer no native uncertainty quantification -- a key requirement for reliable scientific and engineering applications. Instead, neural operators rely on post hoc UQ methods that ignore geometric inductive biases. In this work, we introduce DINOZAUR: a diffusion-based neural operator parametrization with uncertainty quantification. Inspired by the structure of the heat kernel, DINOZAUR replaces the dense tensor multiplier in FNOs with a dimensionality-independent diffusion multiplier that has a single learnable time parameter per channel, drastically reducing parameter count and memory footprint without compromising predictive performance. By defining priors over those time parameters, we cast DINOZAUR as a Bayesian neural operator to yield spatially correlated outputs and calibrated uncertainty estimates. Our method achieves competitive or superior performance across several PDE benchmarks while providing efficient uncertainty quantification",
    "checked": true,
    "id": "5b421e83ed5324ffd966f91f754ff6cb748ad241",
    "semantic_title": "light-weight diffusion multiplier and uncertainty quantification for fourier neural operators",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=kz3w2A2y0e": {
    "title": "Discrete Spatial Diffusion: Intensity-Preserving Diffusion Modeling",
    "volume": "spotlight",
    "abstract": "Generative diffusion models have achieved remarkable success in producing high-quality images. However, these models typically operate in continuous intensity spaces, diffusing independently across pixels and color channels. As a result, they are fundamentally ill-suited for applications involving inherently discrete quantities such as particle counts or material units, that are constrained by strict conservation laws like mass conservation, limiting their applicability in scientific workflows. To address this limitation, we propose Discrete Spatial Diffusion (DSD), a framework based on a continuous-time, discrete-state jump stochastic process that operates directly in discrete spatial domains while strictly preserving particle counts in both forward and reverse diffusion processes. By using spatial diffusion to achieve particle conservation, we introduce stochasticity naturally through a discrete formulation. We demonstrate the expressive flexibility of DSD by performing image synthesis, class conditioning, and image inpainting across standard image benchmarks, while exactly conditioning total image intensity. We validate DSD on two challenging scientific applications: porous rock microstructures and lithium-ion battery electrodes, demonstrating its ability to generate structurally realistic samples under strict mass conservation constraints, with quantitative evaluation using state-of-the-art metrics for transport and electrochemical performance",
    "checked": true,
    "id": "e15d1e7e03f0d31a1ca2ec39e79cb5f8f3ce8041",
    "semantic_title": "discrete spatial diffusion: intensity-preserving diffusion modeling",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=XBMjXb6f4w": {
    "title": "CTRL-ALT-DECEIT Sabotage Evaluations for Automated AI R&D",
    "volume": "spotlight",
    "abstract": "AI systems are increasingly able to autonomously conduct realistic software engineering tasks, and may soon be deployed to automate machine learning (ML) R\\&D itself. Frontier AI systems may be deployed in safety-critical settings, including to help ensure the safety of future systems. Unfortunately, frontier and future systems may not be sufficiently trustworthy, and there is evidence that these systems may even be misaligned with their developers or users. Therefore, we investigate the capabilities of AI agents to act against the interests of their users when conducting ML engineering, by sabotaging ML models, sandbagging their performance, and subverting oversight mechanisms. First, we extend MLE-Bench, a benchmark for realistic ML tasks, with code-sabotage tasks such as implanting backdoors and purposefully causing generalisation failures. Frontier agents make meaningful progress on our sabotage tasks. In addition, we study agent capabilities to sandbag on MLE-Bench. Agents can calibrate their performance to specified target levels below their actual capability. To mitigate sabotage, we use LM monitors to detect suspicious agent behaviour, and we measure model capability to sabotage and sandbag without being detected by these monitors. Overall, monitors are capable at detecting code-sabotage attempts but our results suggest that detecting sandbagging is more difficult. Additionally, aggregating multiple monitor predictions works well, but monitoring may not be sufficiently reliable to mitigate sabotage in high-stakes domains. Our benchmark is implemented in the UK AISI's Inspect framework and we make our code publicly available",
    "checked": false,
    "id": "58a1922b3ca20e710c4349f669cdf992f360787b",
    "semantic_title": "ctrl-alt-deceit: sabotage evaluations for automated ai r&d",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SDhOClkyqC": {
    "title": "An Analytical Theory of Spectral Bias in the Learning Dynamics of Diffusion Models",
    "volume": "spotlight",
    "abstract": "We develop an analytical framework for understanding how the learned distribution evolves during diffusion model training. Leveraging the Gaussian equivalence principle, we derived exact solutions for the gradient-flow dynamics of weights in one or two layer linear or linear convolutional denoiser settings with arbitrary data, where linear networks converge along principal components, and convolutional networks converge along Fourier modes. Remarkably, these solutions allow us to derive the generated distribution in closed-form and its KL-divergence through training. These analytical results expose a pronounced \\emph{spectral bias}, i.e. for both weights and generated distributions, the convergence time of a mode follows an inverse power law of its variance. Empirical experiments on both Gaussian and natural image datasets demonstrate that the power-law spectral bias—remain robust even when using deeper or convolutional architectures. Our results underscore the importance of the data covariance in dictating the order and rate at which diffusion models learn different modes of the data, providing potential explanations of why earlier stopping could lead to incorrect details in image generative model",
    "checked": true,
    "id": "614b5e9cd97fc3cc2d68758530515906a2df738d",
    "semantic_title": "an analytical theory of spectral bias in the learning dynamics of diffusion models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=x9XepNPGJ5": {
    "title": "DeepHalo: A Neural Choice Model with Controllable Context Effects",
    "volume": "spotlight",
    "abstract": "Modeling human decision-making is central to applications such as recommendation, preference learning, and human-AI alignment. While many classic models assume context-independent choice behavior, a large body of behavioral research shows that preferences are often influenced by the composition of the choice set itself---a phenomenon known as the context effect or Halo effect. These effects can manifest as pairwise (first-order) or even higher-order interactions among the available alternatives. Recent models that attempt to capture such effects either focus on the featureless setting or, in the feature-based setting, rely on restrictive interaction structures or entangle interactions across all orders, which limits interpretability. In this work, we propose DeepHalo, a neural modeling framework that incorporates features while enabling explicit control over interaction order and principled interpretation of context effects. Our model enables systematic identification of interaction effects by order and serves as a universal approximator of context-dependent choice functions when specialized to a featureless setting. Experiments on synthetic and real-world datasets demonstrate strong predictive performance while providing greater transparency into the drivers of choice",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CrwzbjO3aU": {
    "title": "Multi-Agent Learning under Uncertainty: Recurrence vs. Concentration",
    "volume": "spotlight",
    "abstract": "In this paper, we examine the convergence landscape of multi-agent learning under uncertainty. Specifically, we analyze two stochastic models of regularized learning in continuous games—one in continuous and one in discrete time—with the aim of characterizing the long run behavior of the induced sequence of play. In stark contrast to deterministic, full-information models of learning (or models with a vanishing learning rate), we show that the resulting dynamics do not converge in general. In lieu of this, we ask instead which actions are played more often in the long run, and by how much. We show that, in strongly monotone games, the dynamics of regularized learning may wander away from equilibrium infinitely often, but they always return to its vicinity in finite time (which we estimate), and their long-run distribution is sharply concentrated around a neighborhood thereof. We quantify the degree of this concentration, and we show that these favorable properties may all break down if the underlying game is not strongly monotone—underscoring in this way the limits of regularized learning in the presence of persistent randomness and uncertainty",
    "checked": false,
    "id": "05575be027d8a049321aa78ea2a88f54e6b6ded3",
    "semantic_title": "attention-based recurrence for multi-agent reinforcement learning under state uncertainty",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=IkfBLlYuHA": {
    "title": "Quantum speedup of non-linear Monte Carlo problems",
    "volume": "spotlight",
    "abstract": "The mean of a random variable can be understood as a *linear* functional on the space of probability distributions. Quantum computing is known to provide a quadratic speedup over classical Monte Carlo methods for mean estimation. In this paper, we investigate whether a similar quadratic speedup is achievable for estimating *non-linear* functionals of probability distributions. We propose a \\textit{quantum-inside-quantum} algorithm that achieves this speedup for the broad class of nonlinear estimation problems known as nested expectations. Our algorithm improves upon the direct application of the quantum-accelerated multilevel Monte Carlo algorithm introduced by An et. al.. The existing lower bound indicates that our algorithm is optimal up to polylogarithmic factors. A key innovation of our approach is a new sequence of multilevel Monte Carlo approximations specifically designed for quantum computing, which is central to the algorithm's improved performance",
    "checked": true,
    "id": "38654526cce5f4e5b2ebe0a6dd92e7281ef8c9b9",
    "semantic_title": "quantum speedup of non-linear monte carlo problems",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Quo3XadYcZ": {
    "title": "Fine-grained List-wise Alignment for Generative Medication Recommendation",
    "volume": "spotlight",
    "abstract": "Accurate and safe medication recommendations are critical for effective clinical decision-making, especially in multimorbidity cases. However, existing systems rely on point-wise prediction paradigms that overlook synergistic drug effects and potential adverse drug-drug interactions (DDIs). We propose FLAME, a fine-grained list-wise alignment framework for large language models (LLMs), enabling drug-by-drug generation of drug lists. FLAME formulates recommendation as a sequential decision process, where each step adds or removes a single drug. To provide fine-grained learning signals, we devise step-wise Group Relative Policy Optimization (GRPO) with potential-based reward shaping, which explicitly models DDIs and optimizes the contribution of each drug to the overall prescription. Furthermore, FLAME enhances patient modeling by integrating structured clinical knowledge and collaborative information into the representation space of LLMs. Experiments on benchmark datasets demonstrate that FLAME achieves state-of-the-art performance, delivering superior accuracy, controllable safety–accuracy trade-offs, and strong generalization across diverse clinical scenarios. Our code is available at https://github.com/cxfann/Flame",
    "checked": true,
    "id": "d7bc454ef40df1921bb5f97a2914b7d2caaadb15",
    "semantic_title": "fine-grained list-wise alignment for generative medication recommendation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=S3GhJooWIC": {
    "title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach",
    "volume": "spotlight",
    "abstract": "We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We train a proof-of-concept model from scratch with 3.5 billion parameters and 800 billion tokens. We show that this model can effortlessly use varying levels of compute, significantly improving with additional compute especially on reasoning tasks, such as math and coding. Further, this architecture naturally reduces compute costs via zero-shot per-token adaptive compute, KV-cache sharing and speculative decoding",
    "checked": true,
    "id": "cbc1363d0c55abb60aa9c0e5a7ca0798ce86a752",
    "semantic_title": "scaling up test-time compute with latent reasoning: a recurrent depth approach",
    "citation_count": 122,
    "authors": []
  },
  "https://openreview.net/forum?id=MfBw0dlBfi": {
    "title": "Diffusion-Based Hierarchical Graph Neural Networks for Simulating Nonlinear Solid Mechanics",
    "volume": "spotlight",
    "abstract": "Graph-based learned simulators have emerged as a promising approach for simulating physical systems on unstructured meshes, offering speed and generalization across diverse geometries. However, they often struggle with capturing global phenomena, such as bending or long-range correlations usually occurring in solid mechanics, and suffer from error accumulation over long rollouts due to their reliance on local message passing and direct next-step prediction. We address these limitations by introducing the Rolling Diffusion-Batched Inference Network (ROBIN), a novel learned simulator that integrates two key innovations: (i) Rolling Diffusion-Batched Inference (ROBI), a parallelized inference scheme that amortizes the cost of diffusion-based refinement across physical time steps by overlapping denoising steps across a temporal window. (ii) A Hierarchical Graph Neural Network built on algebraic multigrid coarsening, enabling multiscale message passing across different mesh resolutions. This architecture, implemented via Algebraic-hierarchical Message Passing Networks, captures both fine-scale local dynamics and global structural effects critical for phenomena like beam bending or multi-body contact. We validate ROBIN on challenging 2D and 3D solid mechanics benchmarks involving geometric, material, and contact nonlinearities. ROBIN achieves state-of-the-art accuracy on all tasks, substantially outperforming existing next-step learned simulators while reducing inference time by up to an order of magnitude compared to standard diffusion simulators",
    "checked": true,
    "id": "c9feecbaf755247fdb0dae4d113a49c8a6ee440d",
    "semantic_title": "diffusion-based hierarchical graph neural networks for simulating nonlinear solid mechanics",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=M8zmlixh9y": {
    "title": "Breaking the Batch Barrier (B3) of Contrastive Learning via Smart Batch Mining",
    "volume": "spotlight",
    "abstract": "Contrastive learning (CL) is a prevalent technique for training embedding models, which pulls semantically similar examples (positives) closer in the representation space while pushing dissimilar ones (negatives) further apart. A key source of negatives are \"in-batch\" examples, i.e., positives from other examples in the batch. Effectiveness of such models is hence strongly influenced by the size and quality of training batches. In this work, we propose *Breaking the Batch Barrier* (B3), a novel batch construction strategy designed to curate high-quality batches for CL. Our approach begins by using a pretrained teacher embedding model to rank all examples in the dataset, from which a sparse similarity graph is constructed. A community detection algorithm is then applied to this graph to identify clusters of examples that serve as strong negatives for one another. The clusters are then used to construct batches that are rich in in-batch negatives. Empirical results on the MMEB multimodal embedding benchmark (36 tasks) demonstrate that our method sets a new state of the art, outperforming previous best methods by +1.3 and +2.9 points at the 7B and 2B model scales, respectively. Notably, models trained with B3 surpass existing state-of-the-art results even with a batch size as small as 64, which is 4–16× smaller than that required by other methods. Moreover, experiments show that B3 generalizes well across domains and tasks, maintaining strong performance even when trained with considerably weaker teachers",
    "checked": true,
    "id": "bab594238922e26b139895a3a5996ba14b69443d",
    "semantic_title": "breaking the batch barrier (b3) of contrastive learning via smart batch mining",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=T1GXVrXJR4": {
    "title": "Dynamic Algorithm for Explainable k -medians Clustering under ℓ p Norm",
    "volume": "spotlight",
    "abstract": "We study the problem of explainable $k$-medians clustering introduced by Dasgupta, Frost, Moshkovitz, and Rashtchian (2020). In this problem, the goal is to construct a threshold decision tree that partitions data into $k$ clusters while minimizing the $k$-medians objective. These trees are interpretable because each internal node makes a simple decision by thresholding a single feature, allowing users to trace and understand how each point is assigned to a cluster. We present the first algorithm for explainable $k$-medians under $\\ell_p$ norm for every finite $p \\geq 1$. Our algorithm achieves an $\\tilde{O}\\big(p(\\log k)^{1 + 1/p - 1/p^2}\\big)$ approximation to the optimal $k$-medians cost for any $p \\geq 1$. Previously, algorithms were known only for $p = 1$ and $p = 2$. For $p = 2$, our algorithm improves upon the existing bound of $\\tilde O(\\log^{3/2}k)$, and for $p = 1$, it matches the tight bound of $\\log k + O(1)$ up to a multiplicative $O(\\log \\log k)$ factor. We show how to implement our algorithm in a dynamic setting. The dynamic algorithm maintains an explainable clustering under a sequence of insertions and deletions, with amortized update time $O(d \\log^3 k)$ and $O(\\log k)$ recourse, making it suitable for large-scale and evolving datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x9vcgXmRD0": {
    "title": "Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs",
    "volume": "spotlight",
    "abstract": "As AIs rapidly advance and become more agentic, the risk they pose is governed not only by their capabilities but increasingly by their propensities, including goals and values. Tracking the emergence of goals and values has proven a longstanding problem, and despite much interest over the years it remains unclear whether current AIs have meaningful values. We propose a solution to this problem, leveraging the framework of utility functions to study the internal coherence of AI preferences. Surprisingly, we find that independently-sampled preferences in current LLMs exhibit high degrees of structural coherence, and moreover that this emerges with scale. These findings suggest that value systems emerge in LLMs in a meaningful sense, a finding with broad implications. To study these emergent value systems, we propose utility engineering as a research agenda, comprising both the analysis and control of AI utilities. We uncover problematic and often shocking values in LLM assistants despite existing control measures. These include cases where AIs value themselves over humans and are anti-aligned with specific individuals. To constrain these emergent value systems, we propose methods of utility control. As a case study, we show how aligning utilities with a citizen assembly reduces political biases and generalizes to new scenarios. Whether we like it or not, value systems have already emerged in AIs, and much work remains to fully understand and control these emergent representations",
    "checked": true,
    "id": "06446d1f549799d234ba780830549284e2d627f0",
    "semantic_title": "utility engineering: analyzing and controlling emergent value systems in ais",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=X0Etmtge6w": {
    "title": "Is the acquisition worth the cost? Surrogate losses for Consistent Two-stage Classifiers",
    "volume": "spotlight",
    "abstract": "Recent years have witnessed the emergence of a spectrum of foundation models, covering a broad range of capabilities and costs. Often, we effectively use foundation models as feature generators and train classifiers that use the outputs of these models to make decisions. In this paper, we consider an increasingly relevant setting where we have two classifier stages. The first stage has access to features $x$ and has the option to make a classification decision or defer, while incurring a cost, to a second classifier that has access to features $x$ and $z$. This is similar to the ``learning to defer'' setting, with the important difference that we train both classifiers jointly, and the second classifier has access to more information. The natural loss for this setting is an $\\ell_{01c}$ loss, where a penalty is paid for incorrect classification, as in $\\ell_{01}$, but an additional penalty $c$ is paid for consulting the second classifier. The $\\ell_{01c}$ loss is unwieldy for training. Our primary contribution in this paper is the derivation of a hinge-based surrogate loss $\\ell^c_{hinge}$ that is much more amenable to training but also satisfies the property that $\\ell^c_{hinge}$-consistency implies $\\ell_{01c}$-consistency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JbJVWljk7r": {
    "title": "SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training",
    "volume": "spotlight",
    "abstract": "The efficiency of attention is important due to its quadratic time complexity. We enhance the efficiency of attention through two key contributions: First, we leverage the new $\\texttt{FP4}$ Tensor Cores in Blackwell GPUs to accelerate attention computation. Our implementation achieves $\\textbf{1038}$ $\\texttt{TOPS}$ on $\\texttt{RTX5090}$, which is a $\\textbf{5}\\times$ speedup over the fastest FlashAttention on $\\texttt{RTX5090}$. Experiments show that our $\\texttt{FP4}$ attention can accelerate inference of various models in a plug-and-play way. Second, we pioneer low-bit attention to training tasks. Existing low-bit attention works like FlashAttention3 and SageAttention focus only on inference. However, the efficiency of training large models is also important. To explore whether low-bit attention can be effectively applied to training tasks, we design an accurate and efficient $\\texttt{8-bit}$ attention for both forward and backward propagation. Experiments indicate that $\\texttt{8-bit}$ attention achieves lossless performance in fine-tuning tasks but exhibits slower convergence in pretraining tasks. The code is available at https://github.com/thu-ml/SageAttention",
    "checked": true,
    "id": "fb6153c75b972e07b5604543d5f2d6f65955a57b",
    "semantic_title": "sageattention3: microscaling fp4 attention for inference and an exploration of 8-bit training",
    "citation_count": 19,
    "authors": []
  },
  "https://openreview.net/forum?id=e8R0ytPhLv": {
    "title": "Eluder dimension: localise it!",
    "volume": "spotlight",
    "abstract": "We establish a lower bound on the eluder dimension in generalised linear model classes, showing that standard eluder dimension-based analysis cannot lead to first-order regret bounds. To address this, we introduce a localisation method for the eluder dimension; our analysis immediately recovers and improves on classic results for Bernoulli bandits, and allows for the first genuine first-order bounds for finite-horizon reinforcement learning tasks with bounded cumulative returns",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X4SCxcgb3O": {
    "title": "Communication-Efficient Language Model Training Scales Reliably and Robustly: Scaling Laws for DiLoCo",
    "volume": "spotlight",
    "abstract": "As we scale to more massive machine learning models, the frequent synchronization demands inherent in data-parallel approaches create significant slowdowns, posing a critical challenge to further scaling. Recent work develops an approach (DiLoCo) that relaxes synchronization demands without compromising model quality. However, these works do not carefully analyze how DiLoCo's behavior changes with model size. In this work, we study the scaling law behavior of DiLoCo when training LLMs under a fixed compute budget. We focus on how algorithmic factors, including number of model replicas, hyperparameters, and token budget affect training in ways that can be accurately predicted via scaling laws. We find that DiLoCo scales both predictably and robustly with model size. When well-tuned, DiLoCo scales better than data-parallel training with model size, and can outperform data-parallel training even at small model sizes. Our results showcase a more general set of benefits of DiLoCo than previously documented, including increased optimal batch sizes, improved downstream generalization with scale, and improved evaluation loss for a fixed token budget",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ogZu06NgQs": {
    "title": "FlashMD: long-stride, universal prediction of molecular dynamics",
    "volume": "spotlight",
    "abstract": "Molecular dynamics (MD) provides insights into atomic-scale processes by integrating over time the equations that describe the motion of atoms under the action of interatomic forces. Machine learning models have substantially accelerated MD by providing inexpensive predictions of the forces, but they remain constrained to minuscule time integration steps, which are required by the fast time scale of atomic motion. In this work, we propose FlashMD, a method to predict the evolution of positions and momenta over strides that are between one and two orders of magnitude longer than typical MD time steps. We incorporate considerations on the mathematical and physical properties of Hamiltonian dynamics in the architecture, generalize the approach to allow the simulation of any thermodynamic ensemble, and carefully assess the possible failure modes of a direct MD approach. We validate FlashMD's accuracy in reproducing equilibrium and time‐dependent properties, using both system‐specific and general-purpose models, extending the ability of MD simulation to reach the long time scales needed to model microscopic processes of high scientific and technological relevance",
    "checked": true,
    "id": "e821b2db3b59ba305477165e80f5784a4418e5a6",
    "semantic_title": "flashmd: long-stride, universal prediction of molecular dynamics",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=kQokjfoGjk": {
    "title": "Not All Data are Good Labels: On the Self-supervised Labeling for Time Series Forecasting",
    "volume": "spotlight",
    "abstract": "Time Series Forecasting (TSF) is a crucial task in various domains, yet existing TSF models rely heavily on high-quality data and insufficiently exploit all available data. This paper explores a novel self-supervised approach to re-label time series datasets by inherently constructing candidate datasets. During the optimization of a simple reconstruction network, intermediates are used as pseudo labels in a self-supervised paradigm, improving generalization for any predictor. We introduce the Self-Correction with Adaptive Mask (SCAM), which discards overfitted components and selectively replaces them with pseudo labels generated from reconstructions. Additionally, we incorporate Spectral Norm Regularization (SNR) to further suppress overfitting from a loss landscape perspective. Our experiments on eleven real-world datasets demonstrate that SCAM consistently improves the performance of various backbone models. This work offers a new perspective on constructing datasets and enhancing the generalization of TSF models through self-supervised learning. The code is available at https://github.com/SuDIS-ZJU/SCAM",
    "checked": true,
    "id": "759501d8f58c74d2767f882086d63de588c791b4",
    "semantic_title": "not all data are good labels: on the self-supervised labeling for time series forecasting",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kXJd4JxF34": {
    "title": "3D Equivariant Visuomotor Policy Learning via Spherical Projection",
    "volume": "spotlight",
    "abstract": "Equivariant models have recently been shown to improve the data efficiency of diffusion policy by a significant margin. However, prior work that explored this direction focused primarily on point cloud inputs generated by multiple cameras fixed in the workspace. This type of point cloud input is not compatible with the now-common setting where the primary input modality is an eye-in-hand RGB camera like a GoPro. This paper closes this gap by incorporating into the diffusion policy model a process that projects features from the 2D RGB camera image onto a sphere. This enables us to reason about symmetries in $\\mathrm{SO}(3)$ without explicitly reconstructing a point cloud. We perform extensive experiments in both simulation and the real world that demonstrate that our method consistently outperforms strong baselines in terms of both performance and sample efficiency. Our work, $\\textbf{Image-to-Sphere Policy}$ ($\\textbf{ISP}$), is the first $\\mathrm{SO}(3)$-equivariant policy learning framework for robotic manipulation that works using only monocular RGB inputs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Gnp8sdwVe": {
    "title": "Predictable Scale (Part II) --- Farseer: A Refined Scaling Law in LLMs",
    "volume": "spotlight",
    "abstract": "Training Large Language Models (LLMs) is prohibitively expensive, creating a critical scaling gap where insights from small-scale experiments often fail to transfer to resource-intensive production systems, thereby hindering efficient innovation. To bridge this, we introduce Farseer, a novel and refined scaling law offering enhanced predictive accuracy across scales. By systematically constructing a model loss surface $L(N,D)$, Farseer achieves a significantly better fit to empirical data than prior laws (e.g., \\Chinchilla's law). Our methodology yields accurate, robust, and highly generalizable predictions, demonstrating excellent extrapolation capabilities, outperforming Chinchilla's law, whose extrapolation error is 433\\% higher. This allows for the reliable evaluation of competing training strategies across all $(N,D)$ settings, enabling conclusions from small-scale ablation studies to be confidently extrapolated to predict large-scale performance. Furthermore, Farseer provides new insights into optimal compute allocation, better reflecting the nuanced demands of modern LLM training. To validate our approach, we trained an extensive suite of approximately 1,000 LLMs across diverse scales and configurations, consuming roughly 3 million NVIDIA H100 GPU hours. To foster further research, we are comprehensively open-sourcing all code, data, results (https://github.com/Farseer-Scaling-Law/Farseer), all training logs (https://wandb.ai/billzid/Farseer?nw=nwuserbillzid), all models used in scaling law fitting (https://huggingface.co/Farseer-Scaling-Law)",
    "checked": false,
    "id": "5c1e1e1f3fc75fb91e5bd356b9e51f1162c6863f",
    "semantic_title": "predictable scale: part ii, farseer: a refined scaling law in large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nfhmjdZUbQ": {
    "title": "Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees",
    "volume": "spotlight",
    "abstract": "Selecting artificial intelligence (AI) models, such as large language models (LLMs), from multiple candidates requires accurate performance estimation. This is ideally achieved through empirical evaluations involving abundant real-world data. However, such evaluations are costly and impractical at scale. To address this challenge, autoevaluation methods leverage synthetic data produced by automated evaluators, such as LLMs-as-judges, reducing variance but potentially introducing bias. Recent approaches have employed semi-supervised prediction-powered inference ($\\texttt{PPI}$) to correct for the bias of autoevaluators. However, the use of autoevaluators may lead in practice to a degradation in sample efficiency compared to conventional methods using only real-world data. In this paper, we propose $\\texttt{R-AutoEval+}$, a novel framework that provides finite-sample reliability guarantees on the model evaluation, while also ensuring an enhanced (or at least no worse) sample efficiency compared to conventional methods. The key innovation of $\\texttt{R-AutoEval+}$ is an adaptive construction of the model evaluation variable, which dynamically tunes its reliance on synthetic data, reverting to conventional methods when the autoevaluator is insufficiently accurate. Experiments on the use of LLMs-as-judges for the optimization of quantization settings for the weights of an LLM, for prompt design in LLMs, and for test-time reasoning budget allocation in LLMs confirm the reliability and efficiency of $\\texttt{R-AutoEval+}$",
    "checked": true,
    "id": "3e1f503b55e988797f8b40344047197a2df48c31",
    "semantic_title": "adaptive prediction-powered autoeval with reliability and efficiency guarantees",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=mf0p4PO7ko": {
    "title": "Ridge Boosting is Both Robust and Efficient",
    "volume": "spotlight",
    "abstract": "Estimators in statistics and machine learning must typically trade off between efficiency, having low variance for a fixed target, and distributional robustness, such as \\textit{multiaccuracy}, or having low bias over a range of possible targets. In this paper, we consider a simple estimator, \\emph{ridge boosting}: starting with any initial predictor, perform a single boosting step with (kernel) ridge regression. Surprisingly, we show that ridge boosting simultaneously achieves both efficiency and distributional robustness: for target distribution shifts that lie within an RKHS unit ball, this estimator maintains low bias across all such shifts and has variance at the semiparametric efficiency bound for each target. In addition to bridging otherwise distinct research areas, this result has immediate practical value. Since ridge boosting uses only data from the source distribution, researchers can train a single model to obtain both robust and efficient estimates for multiple target estimands at the same time, eliminating the need to fit separate semiparametric efficient estimators for each target. We assess this approach through simulations and an application estimating the age profile of retirement income",
    "checked": true,
    "id": "fced5b79ca6f6977c09a03f250f6daf28abec5b3",
    "semantic_title": "ridge boosting is both robust and efficient",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pH3daDPj4c": {
    "title": "A Unifying View of Linear Function Approximation in Off-Policy RL Through Matrix Splitting and Preconditioning",
    "volume": "spotlight",
    "abstract": "In off-policy policy evaluation (OPE) tasks within reinforcement learning, Temporal Difference Learning(TD) and Fitted Q-Iteration (FQI) have traditionally been viewed as differing in the number of updates toward the target value function: TD makes one update, FQI makes an infinite number, and Partial Fitted Q-Iteration (PFQI) performs a finite number. We show that this view is not accurate, and provide a new mathematical perspective under linear value function approximation that unifies these methods as a single iterative method solving same linear system, but using different matrix splitting schemes and preconditioners. We show that increasing the number of updates under the same target value function, i.e., the target network technique, is a transition from using a constant preconditioner to using a data-feature adaptive preconditioner. This elucidates, for the first time, why TD convergence does not necessarily imply FQI convergence, and establishes tight convergence connections among TD, PFQI, and FQI. Our framework enables sharper theoretical results than previous work and characterization of the convergence conditions for each algorithm, without relying on assumptions about the features (e.g., linear independence). We also provide an encoder-decoder perspective to better understand TD's convergence conditions, and prove, for the first time, that when a large learning rate doesn't work, trying a smaller one may help(for batch TD). Our framework also leads to the discovery of new crucial conditions on features for convergence, and shows how common assumptions about features influence convergence, e.g., the assumption of linearly independent features can be dropped without compromising the convergence guarantees of stochastic TD in the on-policy setting. This paper is also the first to introduce matrix splitting into the convergence analysis of these algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5k0AHYc4MJ": {
    "title": "Generalizable Reasoning through Compositional Energy Minimization",
    "volume": "spotlight",
    "abstract": "Generalization is a key challenge in machine learning, specifically in reasoning tasks, where models are expected to solve problems more complex than those encountered during training. Existing approaches typically train reasoning models in an end-to-end fashion, directly mapping input instances to solutions. While this allows models to learn useful heuristics from data, it often results in limited generalization beyond the training distribution. In this work, we propose a novel approach to reasoning generalization by learning energy landscapes over the solution spaces of smaller, more tractable subproblems. At test time, we construct a global energy landscape for a given problem by combining the energy functions of multiple subproblems. This compositional approach enables the incorporation of additional constraints during inference, allowing the construction of energy landscapes for problems of increasing difficulty. To improve the sample quality from this newly constructed energy landscape, we introduce Parallel Energy Minimization (PEM). We evaluate our approach on a wide set of reasoning problems. Our method outperforms existing state-of-the-art methods, demonstrating its ability to generalize to larger and more complex problems. Project website can be found at: https://alexoarga.github.io/compositional_reasoning/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rwmVd8BKW5": {
    "title": "Universal Sequence Preconditioning",
    "volume": "spotlight",
    "abstract": "We study the problem of preconditioning in the setting of sequential prediction. From the theoretical lens of linear dynamical systems, we show that applying a convolution to the input sequence translates to applying a polynomial to the unknown transition matrix in the hidden space. With this insight, we develop a novel preconditioning method that convolves the input sequence with the coefficients of the Chebyshev or Legendre polynomials. We formally prove that this improves the regret of a wide family of prediction methods. We proceed to apply this preconditioning technique to the method of spectral filtering. This gives the first sublinear regret bound that is also hidden-dimension free (up to logarithmic factors) even when the hidden transition matrix is asymmetric. From rigorous experiments on synthetic data we show that our simple preconditioning method generalizes to both 1) settings where the data is \\emph{not} from a linear dynamical system and 2) a broad range of learning algorithms, including recurrent neural networks",
    "checked": true,
    "id": "3b17710adc9628139ed76bd83aa70e050a15be35",
    "semantic_title": "universal sequence preconditioning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=fmnxunacr4": {
    "title": "Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles",
    "volume": "spotlight",
    "abstract": "Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at advanced reasoning tasks like math and coding via Reinforcement Learning with Verifiable Rewards (RLVR), but still struggle with puzzles solvable by humans without domain knowledge. We introduce ENIGMATA, the first comprehensive suite tailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks across 7 categories, each with: 1) a generator that produces unlimited examples with controllable difficulty, and 2) a rule-based verifier for automatic evaluation. This generator-verifier design supports scalable, multi-task RL training, fine-grained analysis, and seamless RLVR integration. We further propose ENIGMATA-Eval, a rigorous benchmark, and develop optimized multi-task RLVR strategies. Our trained model, Qwen2.5-32B-ENIGMATA, consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks like ENIGMATA-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes well to out-of-domain puzzle benchmarks and mathematical reasoning, with little multi-tasking trade-off. When trained on larger models like Seed1.5-Thinking (20B activated parameters and 200B total parameters), puzzle data from ENIGMATA further boosts SoTA performance on advanced math and STEM reasoning tasks such as AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization benefits of ENIGMATA. This work offers a unified, controllable framework for advancing logical reasoning in LLMs. Project page: https://seed-enigmata.github.io",
    "checked": true,
    "id": "d6123d6d213436d8258b4a8f8b7fb90120006239",
    "semantic_title": "enigmata: scaling logical reasoning in large language models with synthetic verifiable puzzles",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=p7WHZy8TCG": {
    "title": "Memory-Enhanced Neural Solvers for Routing Problems",
    "volume": "spotlight",
    "abstract": "Routing Problems are central to many real-world applications, yet remain challenging due to their (NP-)hard nature. Amongst existing approaches, heuristics often offer the best trade-off between quality and scalability, making them suitable for industrial use. While Reinforcement Learning (RL) offers a flexible framework for designing heuristics, its adoption over handcrafted heuristics remains incomplete. Existing learned methods still lack the ability to adapt to specific instances and fully leverage the available computational budget. Current best methods either rely on a collection of pre-trained policies, or on RL fine-tuning; hence failing to fully utilize newly available information within the constraints of the budget. In response, we present MEMENTO, an approach that leverages memory to improve the search of neural solvers at inference. MEMENTO updates the action distribution dynamically based on the outcome of previous decisions. We validate its effectiveness on Traveling Salesman and Capacitated Vehicle Routing problems, demonstrating its superiority over tree-search and policy-gradient fine-tuning; and showing that it can be zero-shot combined with diversity-based solvers. We successfully train all RL auto-regressive solvers on large instances, and verify MEMENTO's scalability and data-efficiency: pushing the state-of-the-art on 11 out of 12 evaluated tasks",
    "checked": true,
    "id": "66c0cc35eace26e0a97b5cc4b73b790e0b992eb4",
    "semantic_title": "memory-enhanced neural solvers for routing problems",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=oHbVboLXz6": {
    "title": "Uni-MuMER: Unified Multi-Task Fine-Tuning of Vision-Language Model for Handwritten Mathematical Expression Recognition",
    "volume": "spotlight",
    "abstract": "Handwritten Mathematical Expression Recognition (HMER) remains a persistent challenge in Optical Character Recognition (OCR) due to the inherent freedom of symbol layouts and variability in handwriting styles. Prior methods have faced performance bottlenecks by proposing isolated architectural modifications, making them difficult to integrate coherently into a unified framework. Meanwhile, recent advances in pretrained vision-language models (VLMs) have demonstrated strong cross-task generalization, offering a promising foundation for developing unified solutions. In this paper, we introduce Uni-MuMER, which fully fine-tunes a VLM for the HMER task without modifying its architecture, effectively injecting domain-specific knowledge into a generalist framework. Our method integrates three data-driven tasks: Tree-Aware Chain-of-Thought (Tree-CoT) for structured spatial reasoning, Error-Driven Learning (EDL) for reducing confusion among visually similar characters, and Symbol Counting (SC) for improving recognition consistency in long expressions. Experiments on the CROHME and HME100K datasets show that Uni-MuMER achieves super state-of-the-art performance, outperforming the best lightweight specialized model SSAN by 16.31\\% and the top-performing VLM Gemini2.5-flash by 24.42\\% under zero-shot setting. Our datasets, models, and code are open-sourced at: https://github.com/BFlameSwift/Uni-MuMER",
    "checked": true,
    "id": "144830412e4e40e10e36719862ccd356fe3c4495",
    "semantic_title": "uni-mumer: unified multi-task fine-tuning of vision-language model for handwritten mathematical expression recognition",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xeb2EYBKkr": {
    "title": "Two Heads are Better than One: Simulating Large Transformers with Small Ones",
    "volume": "spotlight",
    "abstract": "The quadratic complexity of self‑attention prevents transformers from scaling effectively to long input sequences. On the other hand, modern GPUs and other specialized hardware accelerators are well-optimized for processing small input sequences in transformers during both training and inference. A natural question arises: can we take advantage of the efficiency of small transformers to deal with long input sequences? In this paper, we show that transformers with long input sequences (large transformers) can be efficiently simulated by transformers that can only take short input sequences (small transformers). Specifically, we prove that any transformer with input length $N$ can be efficiently simulated by only $O((N/M)^2)$ transformers with input length $M \\ll N$, and that this cannot be improved in the worst case. However, we then prove that in various natural scenarios including average-case inputs, sliding window masking and attention sinks, the optimal number $O(N/M)$ of small transformers suffice",
    "checked": true,
    "id": "48aa901c66df6856a5e0c7748351ea29318ca243",
    "semantic_title": "two heads are better than one: simulating large transformers with small ones",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8rKSfL3GsK": {
    "title": "Gaze Beyond the Frame: Forecasting Egocentric 3D Visual Span",
    "volume": "spotlight",
    "abstract": "People continuously perceive and interact with their surroundings based on underlying intentions that drive their exploration and behaviors. While research in egocentric user and scene understanding has focused primarily on motion and contact-based interaction, forecasting human visual perception itself remains less explored despite its fundamental role in guiding human actions and its implications for AR/VR and assistive technologies. We address the challenge of egocentric 3D visual span forecasting, predicting where a person's visual perception will focus next within their three-dimensional environment. To this end, we propose EgoSpanLift, a novel method that transforms egocentric visual span forecasting from 2D image planes to 3D scenes. EgoSpanLift converts SLAM-derived keypoints into gaze-compatible geometry and extracts volumetric visual span regions. We further combine EgoSpanLift with 3D U-Net and unidirectional transformers, enabling spatio-temporal fusion to efficiently predict future visual span in the 3D grid. In addition, we curate a comprehensive benchmark from raw egocentric multisensory data, creating a testbed with 364.6K samples for 3D visual span forecasting. Our approach outperforms competitive baselines for egocentric gaze anticipation and 3D localization, while achieving comparable results even when projected back onto 2D image planes without additional 2D-specific training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o7Z8TClGjp": {
    "title": "Unifying Proportional Fairness in Centroid and Non-Centroid Clustering",
    "volume": "spotlight",
    "abstract": "Proportional fairness criteria inspired by democratic ideals of proportional representation have received growing attention in the clustering literature. Prior work has investigated them in two separate paradigms. Chen et al. [ICML 2019] study _centroid clustering_, in which each data point's loss is determined by its distance to a representative point (centroid) chosen in its cluster. Caragiannis et al. [NeurIPS 2024] study _non-centroid clustering_, in which each data point's loss is determined by its maximum distance to any other data point in its cluster. We generalize both paradigms to introduce _semi-centroid clustering_, in which each data point's loss is a combination of its centroid and non-centroid losses, and study two proportional fairness criteria---the core and, its relaxation, fully justified representation (FJR). Our main result is a novel algorithm which achieves a constant approximation to the core, in polynomial time, even when the distance metrics used for centroid and non-centroid loss measurements are different. We also derive improved results for more restricted loss functions and the weaker FJR criterion, and establish lower bounds in each case",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eumRwpgdMU": {
    "title": "ARIA: Training Language Agents with Intention-driven Reward Aggregation",
    "volume": "spotlight",
    "abstract": "Large language models (LLMs) have enabled agents to perform complex reasoning and decision-making through free-form language interactions. However, in open-ended language action environments (e.g., negotiation or question-asking games), the action space can be formulated as a joint distribution over tokens, resulting in an extremely large and combinatorial action space. Sampling actions in such a space can lead to extreme reward sparsity, which brings large reward variance, hindering effective reinforcement learning (RL). To address this, we propose **ARIA**, a method that **A**ggregates **R**ewards in **I**ntention space to enable efficient and effective language **A**gents training. ARIA aims to project natural language actions from the high-dimensional joint token distribution space into a low-dimensional intention space, where semantically similar actions are clustered and assigned shared rewards. This intention-aware reward aggregation reduces reward variance by densifying reward signals, fostering efficient and effective policy optimization. Extensive experiments demonstrate that ARIA not only significantly reduces gradient variance, but also delivers substantial performance gains of average 9.95% across four downstream tasks (e.g., negotiation and text-based games), consistently outperforming strong offline and online RL baselines",
    "checked": true,
    "id": "e7abe420e78b7cf2543a47b1a576c1b21d8fc3a6",
    "semantic_title": "aria: training language agents with intention-driven reward aggregation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=QFjssnKdBI": {
    "title": "Reasoning Planning for Language Models",
    "volume": "spotlight",
    "abstract": "Selecting an appropriate reasoning method for a given query remains a key challenge in language model generation. Existing approaches typically generate multiple candidate responses and use an aggregation strategy to select the output answer, often assuming that more candidate answers yield higher accuracy. We revisit this assumption through a rigorous theoretical analysis, deriving accuracy bounds for standard aggregation methods under fixed generation distributions and candidate sizes. Building on these insights, we introduce EPIC, an Ensemble Planning with Contrastive learning framework to learn a shared representation space that captures both model reasoning abilities and query-method compatibility. EPIC incorporates our probability bounds as a regularizer in a utility-driven optimization that balances accuracy and computational cost. Experiments on diverse mathematical reasoning tasks show that EPIC consistently selects optimal reasoning methods, improving accuracy while reducing computational overhead. Our code can be found at https://github.com/nguyenngocbaocmt02/EPIC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fleQlZ2VTx": {
    "title": "When Less Language is More: Language-Reasoning Disentanglement Makes LLMs Better Multilingual Reasoners",
    "volume": "spotlight",
    "abstract": "Multilingual reasoning remains a significant challenge for large language models (LLMs), with performance disproportionately favoring high-resource languages. Drawing inspiration from cognitive neuroscience, which suggests that human reasoning functions largely independently of language processing, we hypothesize that LLMs similarly encode reasoning and language as separable components that can be disentangled to enhance multilingual reasoning. To evaluate this, we perform a causal intervention by ablating language-specific representations at inference time. Experiments on 10 open-weight LLMs spanning 11 typologically diverse languages show that this language-specific ablation consistently boosts multilingual reasoning performance. Layer-wise analyses further confirm that language and reasoning representations can be effectively disentangled throughout the model, yielding improved multilingual reasoning capabilities, while preserving top-layer language features remains essential for maintaining linguistic fidelity. Compared to post-training methods such as supervised fine-tuning or reinforcement learning, our training-free language-reasoning disentanglement achieves comparable or superior results with minimal computational overhead. These findings shed light on the internal mechanisms underlying multilingual reasoning in LLMs and suggest a lightweight and interpretable strategy for improving cross-lingual generalization",
    "checked": true,
    "id": "f6754774176b8d923a52d771d5014e1e18d581b5",
    "semantic_title": "when less language is more: language-reasoning disentanglement makes llms better multilingual reasoners",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=shFhW4zqd6": {
    "title": "EF-3DGS: Event-Aided Free-Trajectory 3D Gaussian Splatting",
    "volume": "spotlight",
    "abstract": "Scene reconstruction from casually captured videos has wide real-world applications. Despite recent progress, existing methods relying on traditional cameras tend to fail in high-speed scenarios due to insufficient observations and inaccurate pose estimation. Event cameras, inspired by biological vision, record pixel-wise intensity changes asynchronously with high temporal resolution and low latency, providing valuable scene and motion information in blind inter-frame intervals. In this paper, we introduce the event cameras to aid scene construction from a casually captured video for the first time, and propose Event-Aided Free-Trajectory 3DGS, called EF-3DGS, which seamlessly integrates the advantages of event cameras into 3DGS through three key components. First, we leverage the Event Generation Model (EGM) to fuse events and frames, enabling continuous supervision between discrete frames. Second, we extract motion information through Contrast Maximization (CMax) of warped events, which calibrates camera poses and provides gradient-domain constraints for 3DGS. Third, to address the absence of color information in events, we combine photometric bundle adjustment (PBA) with a Fixed-GS training strategy that separates structure and color optimization, effectively ensuring color consistency across different views. We evaluate our method on the public Tanks and Temples benchmark and a newly collected real-world dataset, RealEv-DAVIS. Our method achieves up to 3dB higher PSNR and 40% lower Absolute Trajectory Error (ATE) compared to state-of-the-art methods under challenging high-speed scenarios",
    "checked": true,
    "id": "8d200e2cdd743e5da8df6225bccb24d209ae058d",
    "semantic_title": "ef-3dgs: event-aided free-trajectory 3d gaussian splatting",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=cUy3tYIOS5": {
    "title": "Beyond Scalar Rewards: An Axiomatic Framework for Lexicographic MDPs",
    "volume": "spotlight",
    "abstract": "Recent work has formalized the reward hypothesis through the lens of expected utility theory, by interpreting reward as utility. Hausner's foundational work showed that dropping the continuity axiom leads to a generalization of expected utility theory where utilities are lexicographically ordered vectors of arbitrary dimension. In this paper, we extend this result by identifying a simple and practical condition under which preferences in a Markov Decision Process (MDP) cannot be represented by scalar rewards, necessitating a 2-dimensional reward function. We provide a full characterization of such reward functions, as well as the general d-dimensional case under a memorylessness assumption on preferences. Furthermore, we show that optimal policies in this setting retain many desirable properties of their scalar-reward counterparts, while in the Constrained MDP (CMDP) setting — another common multiobjective setting — they do not",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tXxsCbKdQv": {
    "title": "Compress to Impress: Efficient LLM Adaptation Using a Single Gradient Step on 100 Samples",
    "volume": "spotlight",
    "abstract": "Recently, Sharma et al. (2024) suggested a method called LAyer- SElective-Rank reduction (LASER) which demonstrated that pruning high‑order components of carefully chosen LLM's weight matrices can boost downstream accuracy—without any gradient‑based fine‑tuning. Yet LASER's exhaustive, per‑matrix search (each requiring full‑dataset forward passes) makes it impractical for rapid deployment. We demonstrate that this overhead can be removed and find that: (i) Only a small, carefully chosen subset of matrices needs to be inspected—eliminating the layer‑by‑layer sweep, (ii) The gradient of each matrix's singular values pinpoints which matrices merit reduction, (iii) Increasing the factorization search space by allowing matrices rows to cluster around multiple subspaces and then decomposing each cluster separately further reduces overfitting on the original training data and further lifts accuracy by up to 24.6 percentage points, and finally, (iv) we discover that evaluating on just 100 samples rather than the full training data—both for computing the indicative gradients and for measuring the final accuracy—suffices to further reduce the search time; we explain that as adaptation to downstream tasks is dominated by prompting style, not dataset size. As a results, we show that combining these findings yields a fast and robust adaptation algorithm for downstream tasks. Overall, with a single gradient step on 100 examples and a quick scan of the top candidate layers and factorization techniques, we can adapt LLMs to new datasets—entirely without fine‑tuning",
    "checked": true,
    "id": "f3e9cc90ef7fdd9257fbf767070eec554c420317",
    "semantic_title": "compress to impress: efficient llm adaptation using a single gradient step on 100 samples",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=avRktRfQ8c": {
    "title": "Understanding Prompt Tuning and In-Context Learning via Meta-Learning",
    "volume": "spotlight",
    "abstract": "Prompting is one of the main ways to adapt a pretrained model to target tasks. Besides manually constructing prompts, many prompt optimization methods have been proposed in the literature. Method development is mainly empirically driven, with less emphasis on a conceptual understanding of prompting. In this paper we discuss how optimal prompting can be understood through a Bayesian view, which also implies some fundamental limitations of prompting that can only be overcome by tuning weights. The paper explains in detail how meta-trained neural networks behave as Bayesian predictors over the pretraining distribution, whose hallmark feature is rapid in-context adaptation. Optimal prompting can be studied formally as conditioning these Bayesian predictors, yielding criteria for target tasks where optimal prompting is and is not possible. We support the theory with educational experiments on LSTMs and Transformers, where we compare different versions of prefix-tuning and different weight-tuning methods. We also confirm that soft prefixes, which are sequences of real-valued vectors outside the token alphabet, can lead to very effective prompts for trained and even untrained networks by manipulating activations in ways that are not achievable by hard tokens. This adds an important mechanistic aspect beyond the conceptual Bayesian theory",
    "checked": true,
    "id": "9bc429b25a237c59a4ad20e534add569cbf071d9",
    "semantic_title": "understanding prompt tuning and in-context learning via meta-learning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=IZ1KYTU9ON": {
    "title": "Error Broadcast and Decorrelation as a Potential Artificial and Natural Learning Mechanism",
    "volume": "spotlight",
    "abstract": "We introduce *Error Broadcast and Decorrelation* (EBD), a novel learning framework for neural networks that addresses credit assignment by directly broadcasting output errors to individual layers, circumventing weight transport of backpropagation. EBD is rigorously grounded in the stochastic orthogonality property of Minimum Mean Square Error estimators. This fundamental principle states that the error of an optimal estimator is orthogonal to functions of the input. Guided by this insight, EBD defines layerwise loss functions that directly penalize correlations between layer activations and output errors, thereby establishing a principled foundation for error broadcasting. This theoretically sound mechanism naturally leads to the experimentally observed three-factor learning rule and integrates with biologically plausible frameworks to enhance performance and plausibility. Numerical experiments demonstrate EBD's competitive or better performance against other error-broadcast methods on benchmark datasets. Our findings establish EBD as an efficient, biologically plausible, and principled alternative for neural network training",
    "checked": true,
    "id": "500a0002aeda4bd72fbc30e97a05c119e09a6416",
    "semantic_title": "error broadcast and decorrelation as a potential artificial and natural learning mechanism",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=zSrb8rtH9M": {
    "title": "On the Expressive Power of Mixture-of-Experts for Structured Complex Tasks",
    "volume": "spotlight",
    "abstract": "Mixture-of-experts networks (MoEs) have demonstrated remarkable efficiency in modern deep learning. Despite their empirical success, the theoretical foundations underlying their ability to model complex tasks remain poorly understood. In this work, we conduct a systematic study of the expressive power of MoEs in modeling complex tasks with two common structural priors: low-dimensionality and sparsity. For shallow MoEs, we prove that they can efficiently approximate functions supported on low-dimensional manifolds, overcoming the curse of dimensionality. For deep MoEs, we show that $\\mathcal{O}(L)$-layer MoEs with $E$ experts per layer can approximate piecewise functions comprising $E^L$ pieces with compositional sparsity, i.e., they can exhibit an exponential number of structured tasks. Our analysis reveals the roles of critical architectural components and hyperparameters in MoEs, including the gating mechanism, expert networks, the number of experts, and the number of layers, and offers natural suggestions for MoE variants",
    "checked": true,
    "id": "48d5e2bb1badf83ef2e1c2eb1a9149c6810ab5f5",
    "semantic_title": "on the expressive power of mixture-of-experts for structured complex tasks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gNiT81iag0": {
    "title": "TokenSwap: A Lightweight Method to Disrupt Memorized Sequences in LLMs",
    "volume": "spotlight",
    "abstract": "As language models scale, their performance improves dramatically across a wide range of tasks, but so does their tendency to memorize and regurgitate parts of their training data verbatim. This tradeoff poses serious legal, ethical, and safety concerns, especially in real-world deployments. Existing mitigation techniques, such as differential privacy or model unlearning, often require retraining or access to internal weights making them impractical for most users. In this work, we introduce TokenSwap, a lightweight, post-hoc defense designed for realistic settings where the user can only access token-level outputs. Our key insight is that while large models are necessary for high task performance, small models (e.g., DistilGPT-2) are often sufficient to assign fluent, grammatically plausible probabilities to common function words - and crucially, they memorize far less. By selectively swapping token probabilities between models, TokenSwap preserves the capabilities of large models while reducing their propensity for verbatim reproduction. Evaluations on Pythia-6.9B and Llama-3-8B show up to a 10$\\times$ drop in exact memorization with negligible task degradation. Our method offers a practical, accessible solution for mitigating memorized generation in deployed LLMs. Code is available at https://github.com/parjanya20/verbatim-llm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p3HBEtNDRY": {
    "title": "Streaming Attention Approximation via Discrepancy Theory",
    "volume": "spotlight",
    "abstract": "Large language models (LLMs) have achieved impressive success, but their high memory requirements present challenges for long-context token generation. In this paper we study the streaming complexity of attention approximation, a key computational primitive underlying token generation. Our main contribution is BalanceKV, a streaming algorithm for $\\epsilon$-approximating attention computations based on geometric process for selecting a balanced collection of Key and Value tokens as per Banaszczyk's vector balancing theory. We complement our algorithm with space lower bounds for streaming attention computation. Besides strong theoretical guarantees, BalanceKV exhibits empirically validated performance improvements over existing methods, both for attention approximation and end-to-end performance on various long context benchmarks",
    "checked": true,
    "id": "b2d3806a2f4bb99af0decb32533ac0d6f1598234",
    "semantic_title": "streaming attention approximation via discrepancy theory",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=DI2AAFnLrc": {
    "title": "SegMASt3R: Geometry Grounded Segment Matching",
    "volume": "spotlight",
    "abstract": "Segment matching is an important intermediate task in computer vision that establishes correspondences between semantically or geometrically coherent regions across images. Unlike keypoint matching, which focuses on localized features, segment matching captures structured regions, offering greater robustness to occlusions, lighting variations, and viewpoint changes. In this paper, we leverage the spatial understanding of 3D foundation models to tackle wide-baseline segment matching, a challenging setting involving extreme viewpoint shifts. We propose an architecture that uses the inductive bias of these 3D foundation models to match segments across image pairs with up to $180^\\circ$ rotation. Extensive experiments show that our approach outperforms state-of-the-art methods, including the SAM2 video propagator and local feature matching methods, by up to 30\\% on the AUPRC metric, on ScanNet++ and Replica datasets. We further demonstrate benefits of the proposed model on relevant downstream tasks, including 3D instance mapping and object-relative navigation",
    "checked": true,
    "id": "e5d2fd1552cd06f42b5c8c4e84377a6778933e45",
    "semantic_title": "segmast3r: geometry grounded segment matching",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XP3v1THxsq": {
    "title": "Among Us: A Sandbox for Measuring and Detecting Agentic Deception",
    "volume": "spotlight",
    "abstract": "Prior studies on deception in language-based AI agents typically assess whether the agent produces a false statement about a topic, or makes a binary choice prompted by a goal, rather than allowing open-ended deceptive behavior to emerge in pursuit of a longer-term goal. To fix this, we introduce $\\textit{Among Us}$, a sandbox social deception game where LLM-agents exhibit long-term, open-ended deception as a consequence of the game objectives. While most benchmarks saturate quickly, $\\textit{Among Us}$ can be expected to last much longer, because it is a multi-player game far from equilibrium. Using the sandbox, we evaluate $18$ proprietary and open-weight LLMs and uncover a general trend: models trained with RL are comparatively much better at producing deception than detecting it. We evaluate the effectiveness of methods to detect lying and deception: logistic regression on the activations and sparse autoencoders (SAEs). We find that probes trained on a dataset of ``pretend you're a dishonest model: $\\dots$'' generalize extremely well out-of-distribution, consistently obtaining AUROCs over 95% even when evaluated just on the deceptive statement, without the chain of thought. We also find two SAE features that work well at deception detection but are unable to steer the model to lie less. We hope our open-sourced sandbox, game logs, and probes serve to anticipate and mitigate deceptive behavior and capabilities in language-based agents",
    "checked": true,
    "id": "a06d59a90b508b11e2d872fbe17ee69f628964ad",
    "semantic_title": "among us: a sandbox for measuring and detecting agentic deception",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=oeZZusZheP": {
    "title": "AutoToM: Scaling Model-based Mental Inference via Automated Agent Modeling",
    "volume": "spotlight",
    "abstract": "Theory of Mind (ToM), the ability to understand people's minds based on their behavior, is key to developing socially intelligent agents. Current approaches to ToM reasoning either rely on prompting Large Language Models (LLMs), which are prone to systematic errors, or use handcrafted, rigid agent models for model-based inference, which are more robust but fail to generalize across domains. In this work, we introduce *AutoToM*, an automated agent modeling method for scalable, robust, and interpretable mental inference. Given a ToM problem, *AutoToM* first proposes an initial agent model and then performs automated Bayesian inverse planning based on this model, leveraging an LLM backend. Guided by inference uncertainty, it iteratively refines the model by introducing additional mental variables and/or incorporating more timesteps in the context. Across five diverse benchmarks, *AutoToM* outperforms existing ToM methods and even large reasoning models. Additionally, we show that *AutoToM* can produce human‐like confidence estimates and enable online mental inference for embodied decision-making",
    "checked": false,
    "id": "e700b89269f1232dbdc436713e255a9f74325f65",
    "semantic_title": "scoreflow: mastering llm agent workflows via score-based preference optimization",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=6SI1pvb5xl": {
    "title": "Towards Interpretable and Efficient Attention: Compressing All by Contracting a Few",
    "volume": "spotlight",
    "abstract": "Attention mechanisms have achieved significant empirical success in multiple fields, but their underlying optimization objectives remain unclear yet. Moreover, the quadratic complexity of self-attention has become increasingly prohibitive. Although interpretability and efficiency are two mutually reinforcing pursuits, prior work typically investigates them separately. In this paper, we propose a unified optimization objective that derives inherently interpretable and efficient attention mechanisms through algorithm unrolling. Precisely, we construct a gradient step of the proposed objective with a set of forward-pass operations of our \\emph{Contract-and-Broadcast Self-Attention} (CBSA), which compresses input tokens towards low-dimensional structures by contracting a few representatives of them. This novel mechanism can not only scale linearly by fixing the number of representatives, but also covers the instantiations of varied attention mechanisms when using different sets of representatives. We conduct extensive experiments to demonstrate comparable performance and superior advantages over black-box attention mechanisms on visual tasks. Our work sheds light on the integration of interpretability and efficiency, as well as the unified formula of attention mechanisms. Code is available at \\href{https://github.com/QishuaiWen/CBSA}{this https URL}",
    "checked": true,
    "id": "ba162bc18074b95ebe66d87402dc80f9de0f4bb8",
    "semantic_title": "towards interpretable and efficient attention: compressing all by contracting a few",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XmV7KRABBl": {
    "title": "EvoBrain: Dynamic Multi-Channel EEG Graph Modeling for Time-Evolving Brain Networks",
    "volume": "spotlight",
    "abstract": "Dynamic GNNs, which integrate temporal and spatial features in Electroencephalography (EEG) data, have shown great potential in automating seizure detection. However, fully capturing the underlying dynamics necessary to represent brain states, such as seizure and non-seizure, remains a non-trivial task and presents two fundamental challenges. First, most existing dynamic GNN methods are built on temporally fixed static graphs, which fail to reflect the evolving nature of brain connectivity during seizure progression. Second, current efforts to jointly model temporal signals and graph structures and, more importantly, their interactions remain nascent, often resulting in inconsistent performance. To address these challenges, we present the first theoretical analysis of these two problems, demonstrating the effectiveness and necessity of explicit dynamic modeling and time-then-graph dynamic GNN method. Building on these insights, we propose EvoBrain, a novel seizure detection model that integrates a two-stream Mamba architecture with a GCN enhanced by Laplacian Positional Encoding, following neurological insights. Moreover, EvoBrain incorporates explicitly dynamic graph structures, allowing both nodes and edges to evolve over time. Our contributions include (a) a theoretical analysis proving the expressivity advantage of explicit dynamic modeling and time-then-graph over other approaches, (b) a novel and efficient model that significantly improves AUROC by 23\\% and F1 score by 30\\%, compared with the dynamic GNN baseline, and (c) broad evaluation of our method on the challenging early seizure prediction task",
    "checked": false,
    "id": "4183dcdf3b878ccc65769e164f2cf9fb2d972eee",
    "semantic_title": "evobrain: dynamic multi-channel eeg graph modeling for time-evolving brain network",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=idjZKbf78s": {
    "title": "Product Distribution Learning with Imperfect Advice",
    "volume": "spotlight",
    "abstract": "Given i.i.d.~samples from an unknown distribution $P$, the goal of distribution learning is to recover the parameters of a distribution that is close to $P$. When $P$ belongs to the class of product distributions on the Boolean hypercube $\\{0,1\\}^d$, it is known that $\\Omega(d/\\epsilon^2)$ samples are necessary to learn $P$ within total variation (TV) distance $\\epsilon$. We revisit this problem when the learner is also given as advice the parameters of a product distribution $Q$. We show that there is an efficient algorithm to learn $P$ within TV distance $\\epsilon$ that has sample complexity $\\tilde{O}(d^{1-\\eta}/\\epsilon^2)$, if $\\|\\mathbf{p} - \\mathbf{q}\\|_1<\\epsilon d^{0.5 - \\Omega(\\eta)}$. Here, $\\mathbf{p}$ and $\\mathbf{q}$ are the mean vectors of $P$ and $Q$ respectively, and no bound on $\\|\\mathbf{p} - \\mathbf{q}\\|_1$ is known to the algorithm a priori",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wbZCBBrq3W": {
    "title": "RoboScape: Physics-informed Embodied World Model",
    "volume": "spotlight",
    "abstract": "World models have become indispensable tools for embodied intelligence, serving as powerful simulators capable of generating realistic robotic videos while addressing critical data scarcity challenges. However, current embodied world models exhibit limited physical awareness, particularly in modeling 3D geometry and motion dynamics, resulting in unrealistic video generation for contact-rich robotic scenarios. In this paper, we present RoboScape, a unified physics-informed world model that jointly learns RGB video generation and physics knowledge within an integrated framework. We introduce two key physics-informed joint training tasks: temporal depth prediction that enhances 3D geometric consistency in video rendering, and keypoint dynamics learning that implicitly encodes physical properties (e.g., object shape and material characteristics) while improving complex motion modeling. Extensive experiments demonstrate that RoboScape generates videos with superior visual fidelity and physical plausibility across diverse robotic scenarios. We further validate its practical utility through downstream applications including robotic policy training with generated data and policy evaluation. Our work provides new insights for building efficient physics-informed world models to advance embodied intelligence research. Our code and demos are available at: https://github.com/tsinghua-fib-lab/RoboScape",
    "checked": true,
    "id": "25d4708f2daad2b41ea902325186b115f8247bf5",
    "semantic_title": "roboscape: physics-informed embodied world model",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=WjYvHSjXrP": {
    "title": "InstructHOI: Context-Aware Instruction for Multi-Modal Reasoning in Human-Object Interaction Detection",
    "volume": "spotlight",
    "abstract": "Recently, Large Foundation Models (LFMs), e.g., CLIP and GPT, have significantly advanced the Human-Object Interaction (HOI) detection, due to their superior generalization and transferability. Prior HOI detectors typically employ single- or multi-modal prompts to generate discriminative representations for HOIs from pretrained LFMs. However, such prompt-based approaches focus on transferring HOI-specific knowledge, but unexplore the potential reasoning capabilities of LFMs, which can provide informative context for ambiguous and open-world interaction recognition. In this paper, we propose InstructHOI, a novel method that leverages context-aware instructions to guide multi-modal reasoning for HOI detection. Specifically, to bridge knowledge gap and enhance reasoning abilities, we first perform HOI-domain fine-tuning on a pretrained multi-modal LFM, using a generated dataset with 140K interaction-reasoning image-text pairs. Then, we develop a Context-aware Instruction Generator (CIG) to guide interaction reasoning. Unlike traditional language-only instructions, CIG first mines visual interactive context at the human-object level, which is then fused with linguistic instructions, forming multi-modal reasoning guidance. Furthermore, an Interest Token Selector (ITS) is adopted to adaptively filter image tokens based on context-aware instructions, thereby aligning reasoning process with interaction regions. Extensive experiments on two public benchmarks demonstrate that our proposed method outperforms the state-of-the-art ones, under both supervised and zero-shot settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z9oeQrcNh9": {
    "title": "ARM: Adaptive Reasoning Model",
    "volume": "spotlight",
    "abstract": "While large reasoning models demonstrate strong performance on complex tasks, they lack the ability to adjust reasoning token usage based on task difficulty. This often leads to the \"overthinking\" problem—excessive and unnecessary reasoning—which, although potentially mitigated by human intervention to control the token budget, still fundamentally contradicts the goal of achieving fully autonomous AI. In this work, we propose Adaptive Reasoning Model (ARM), a reasoning model capable of adaptively selecting appropriate reasoning formats based on the task at hand. These formats include three efficient ones—Direct Answer, Short CoT, and Code—as well as a more elaborate format, Long CoT. To train ARM, we introduce Ada-GRPO, an adaptation of Group Relative Policy Optimization (GRPO), which addresses the format collapse issue in traditional GRPO. Ada-GRPO enables ARM to achieve high token efficiency, reducing tokens by an average of $\\sim$30%, and up to $\\sim$70%, while maintaining performance comparable to the model that relies solely on Long CoT. Furthermore, not only does it improve inference efficiency through reduced token generation, but it also brings a $\\sim$2$\\times$ speedup in training. In addition to the default Adaptive Mode, ARM supports two additional reasoning modes: 1) Instruction-Guided Mode, which allows users to explicitly specify the reasoning format via special tokens—ideal when the appropriate format is known for a batch of tasks. 2) Consensus-Guided Mode, which aggregates the outputs of the three efficient formats and resorts to Long CoT in case of disagreement, prioritizing performance with higher token usage. All the resources will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TU2MZHZLkP": {
    "title": "Computational Efficiency under Covariate Shift in Kernel Ridge Regression",
    "volume": "spotlight",
    "abstract": "This paper addresses the covariate shift problem in the context of nonparametric regression within reproducing kernel Hilbert spaces (RKHSs). Covariate shift arises in supervised learning when the input distributions of the training and test data differ, presenting additional challenges for learning. Although kernel methods have optimal statistical properties, their high computational demands in terms of time and, particularly, memory, limit their scalability to large datasets. To address this limitation, the main focus of this paper is to explore the trade-off between computational efficiency and statistical accuracy under covariate shift. We investigate the use of random projections where the hypothesis space consists of a random subspace within a given RKHS. Our results show that, even in the presence of covariate shift, significant computational savings can be achieved without compromising learning performance",
    "checked": true,
    "id": "88adb16f56970cca8aabb818643c978313a7907c",
    "semantic_title": "computational efficiency under covariate shift in kernel ridge regression",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5iHDGJFf49": {
    "title": "Self-Assembling Graph Perceptrons",
    "volume": "spotlight",
    "abstract": "Inspired by the workings of biological brains, humans have designed artificial neural networks (ANNs), sparking profound advancements across various fields. However, the biological brain possesses high plasticity, enabling it to develop simple, efficient, and powerful structures to cope with complex external environments. In contrast, the superior performance of ANNs often relies on meticulously crafted architectures, which can make them vulnerable when handling complex inputs. Moreover, overparameterization often characterizes the most advanced ANNs. This paper explores the path toward building streamlined and plastic ANNs. Firstly, we introduce the Graph Perceptron (GP), which extends the most fundamental ANN, the Multi-Layer Perceptron (MLP). Subsequently, we incorporate a self-assembly mechanism on top of GP called Self-Assembling Graph Perceptron (SAGP). During training, SAGP can autonomously adjust the network's number of neurons and synapses and their connectivity. SAGP achieves comparable or even superior performance with only about 5% of the size of an MLP. We also demonstrate the SAGP's advantages in enhancing model interpretability and feature selection",
    "checked": false,
    "id": "82f836ec4e9efa348117e6286b610ce88fbd5ba3",
    "semantic_title": "self-assembling dna complexes with a wheel graph structure",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=4YKlo58RcQ": {
    "title": "Scaling and context steer LLMs along the same computational path as the human brain",
    "volume": "spotlight",
    "abstract": "Recent studies suggest that the representations learned by large language models (LLMs) are partially aligned to those of the human brain. However, whether this representational alignment arises from a similar sequence of computations remains elusive. In this study, we explore this question by examining temporally-resolved brain signals of participants listening to 10 hours of an audiobook. We study these neural dynamics jointly with a benchmark encompassing 17 LLMs varying in size and architecture type. Our analyses reveal that LLMs and the brain generate representations in a similar order: specifically, activations in the initial layers of LLMs tend to best align with early brain responses, while the deeper layers of LLMs tend to best align with later brain responses. This brain-LLM alignment is consistent across transformers and recurrent architectures. However, its emergence depends on both model size and context length. Overall, the alignment between LLMs and the brain provides novel elements supporting a partial convergence between biological and artificial neural networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZwBtDbuzjY": {
    "title": "InfiFPO: Implicit Model Fusion via Preference Optimization in Large Language Models",
    "volume": "spotlight",
    "abstract": "Model fusion combines multiple Large Language Models (LLMs) with different strengths into a more powerful, integrated model through lightweight training methods. Existing works on model fusion focus primarily on supervised fine-tuning (SFT), leaving preference alignment (PA) —a critical phase for enhancing LLM performance—largely unexplored. The current few fusion methods on PA phase, like WRPO, simplify the process by utilizing only response outputs from source models while discarding their probability information. To address this limitation, we propose InfiFPO, a preference optimization method for implicit model fusion. InfiFPO replaces the reference model in Direct Preference Optimization (DPO) with a fused source model that synthesizes multi-source probabilities at the sequence level, circumventing complex vocabulary alignment challenges in previous works and meanwhile maintaining the probability information. By introducing probability clipping and max-margin fusion strategies, InfiFPO enables the pivot model to align with human preferences while effectively distilling knowledge from source models. Comprehensive experiments on 11 widely-used benchmarks demonstrate that InfiFPO consistently outperforms existing model fusion and preference optimization methods. When using Phi-4 as the pivot model, InfiFPO improves its average performance from 79.95 to 83.33 on 11 benchmarks, significantly improving its capabilities in mathematics, coding, and reasoning tasks",
    "checked": true,
    "id": "18cbb837b4c625a6a5af43599d795a3b04f32bb8",
    "semantic_title": "infifpo: implicit model fusion via preference optimization in large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=huZzy5w2Js": {
    "title": "SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios",
    "volume": "spotlight",
    "abstract": "Hand-Object Interaction (HOI) generation has significant application potential. However, current 3D HOI motion generation approaches heavily rely on predefined 3D object models and lab-captured motion data, limiting generalization capabilities. Meanwhile, HOI video generation methods prioritize pixel-level visual fidelity, often sacrificing physical plausibility. Recognizing that visual appearance and motion patterns share fundamental physical laws in the real world, we propose a novel framework that combines visual priors and dynamic constraints within a synchronized diffusion process to generate the HOI video and motion simultaneously. To integrate the heterogeneous semantics, appearance, and motion features, our method implements tri-modal adaptive modulation for feature aligning, coupled with 3D full-attention for modeling inter- and intra-modal dependencies. Furthermore, we introduce a vision-aware 3D interaction diffusion model that generates explicit 3D interaction sequences directly from the synchronized diffusion outputs, then feeds them back to establish a closed-loop feedback cycle. This architecture eliminates dependencies on predefined object models or explicit pose guidance while significantly enhancing video-motion consistency. Experimental results demonstrate our method's superiority over state-of-the-art approaches in generating high-fidelity, dynamically plausible HOI sequences, with notable generalization capabilities in unseen real-world scenarios. Project page at [https://droliven.github.io/SViMo_project](https://droliven.github.io/SViMo_project)",
    "checked": true,
    "id": "3b0a4f8c6f3139ee27407d78b20515eef55111d1",
    "semantic_title": "svimo: synchronized diffusion for video and motion generation in hand-object interaction scenarios",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W1Cu6JsRsd": {
    "title": "HyPINO: Multi-Physics Neural Operators via HyperPINNs and the Method of Manufactured Solutions",
    "volume": "spotlight",
    "abstract": "We present HyPINO, a multi-physics neural operator designed for zero-shot generalization across a broad class of parametric PDEs without requiring task-specific fine-tuning. Our approach combines a Swin Transformer-based hypernetwork with mixed supervision: (i) labeled data from analytical solutions generated via the Method of Manufactured Solutions (MMS), and (ii) unlabeled samples optimized using physics-informed objectives. The model maps PDE parameterizations to target Physics-Informed Neural Networks (PINNs) and can handle linear elliptic, hyperbolic, and parabolic equations in two dimensions with varying source terms, geometries, and mixed Dirichlet/Neumann boundary conditions, including interior boundaries. HyPINO achieves strong zero-shot accuracy on seven benchmark problems from PINN literature, outperforming U-Nets, Poseidon, and Physics-Informed Neural Operators (PINO). Further, we introduce an iterative refinement procedure that treats the residual of the generated PINN as \"delta PDE\" and performs another forward pass to generate a corrective PINN. Summing their contributions and repeating this process forms an ensemble whose combined solution progressively reduces the error on six benchmarks and achieves a >100× lower $L_2$ loss in the best case, while retaining forward-only inference. Additionally, we evaluate the fine-tuning behavior of PINNs initialized by HyPINO and show that they converge faster and to lower final error than both randomly initialized and Reptile-meta-learned PINNs on five benchmarks, performing on par on the remaining two. Our results highlight the potential of this scalable approach as a foundation for extending neural operators toward solving increasingly complex, nonlinear, and high-dimensional PDE problems. The code and model weights are publicly available at https://github.com/rbischof/hypino",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0RF80tUWuv": {
    "title": "RidgeLoRA: Matrix Ridge Enhanced Low-Rank Adaptation of Large Language Models",
    "volume": "spotlight",
    "abstract": "As one of the state-of-the-art parameter-efficient fine-tuning~(PEFT) methods, Low-Rank Adaptation (LoRA) enables model optimization with reduced computational cost through trainable low-rank matrix. However, the low-rank nature makes it prone to produce a decrease in the representation ability, leading to suboptimal performance. In order to break this limitation, we propose RidgeLoRA, a lightweight architecture like LoRA that incorporates novel architecture and matrix ridge enhanced full-rank approximation, to match the performance of full-rank training, while eliminating the need for high memory and a large number of parameters to restore the rank of matrices. We provide a rigorous mathematical derivation to prove that RidgeLoRA has a better upper bound on the representations than vanilla LoRA. Furthermore, extensive experiments across multiple domains demonstrate that RidgeLoRA achieves better performance than other LoRA variants, and can even match or surpass full-rank training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DpOSndSOZz": {
    "title": "Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens",
    "volume": "spotlight",
    "abstract": "The recent rise of Large Reasoning Models (LRMs) has significantly improved multi-step reasoning performance, but often at the cost of generating excessively long reasoning chains. This paper revisits the efficiency of such reasoning processes through an information-theoretic lens, revealing a fundamental trade-off between reasoning length and semantic efficiency. We propose two metrics—InfoBias and InfoGain—to quantify divergence from ideal reasoning paths and stepwise information contribution, respectively. Empirical analyses show that longer reasoning chains tend to exhibit higher information bias and diminishing information gain, especially for incorrect answers. Motivated by these findings, we introduce an entropy-based Adaptive Think strategy that dynamically halts reasoning once confidence is sufficiently high, improving efficiency while maintaining competitive accuracy. Compared to the Vanilla Think approach (default mode), our strategy yields a 1.10% improvement in average accuracy and a 50.80% reduction in token usage on QwQ-32B across six benchmark tasks spanning diverse reasoning types and difficulty levels, demonstrating superior efficiency and reasoning performance. These results underscore the promise of entropy-based methods for enhancing both accuracy and cost-effiiciency in large language model deployment",
    "checked": true,
    "id": "4a3e43c146cfeb8a62cd45de25af1ea41cb48b34",
    "semantic_title": "think or not? exploring thinking efficiency in large reasoning models via an information-theoretic lens",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=TkEdQv0bXB": {
    "title": "Hyperbolic Fine-Tuning for Large Language Models",
    "volume": "spotlight",
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance on various tasks. However, it remains an open question whether the default Euclidean space is the most suitable choice for embedding tokens in LLMs. In this study, we investigate the non-Euclidean characteristics of LLMs. Our findings reveal that token frequency follows a power-law distribution, with high-frequency tokens clustering near the origin and low-frequency tokens positioned farther away. Additionally, token embeddings exhibit a high degree of hyperbolicity, indicating a latent tree-like structure in the embedding space. Motivated by these observations, we propose to efficiently fine-tune LLMs in hyperbolic space to better exploit the underlying complex structures. However, we find that this hyperbolic fine-tuning cannot be achieved through the naive application of exponential and logarithmic maps when the embedding and weight matrices both reside in Euclidean space. To address this technical issue, we introduce hyperbolic low-rank efficient fine-tuning, HypLoRA, which performs low-rank adaptation directly on the hyperbolic manifold, preventing the cancellation effect produced by consecutive exponential and logarithmic maps and thereby preserving hyperbolic modeling capabilities. Extensive experiments across various base models and two different reasoning benchmarks, specifically arithmetic and commonsense reasoning tasks, demonstrate that HypLoRA substantially improves LLM performance",
    "checked": true,
    "id": "ae739553e7c7c45c150e48c26cdff2fb2129f1a4",
    "semantic_title": "hyperbolic fine-tuning for large language models",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=74SvE2GZwW": {
    "title": "Deep Continuous-Time State-Space Models for Marked Event Sequences",
    "volume": "spotlight",
    "abstract": "Marked temporal point processes (MTPPs) model sequences of events occurring at irregular time intervals, with wide-ranging applications in fields such as healthcare, finance and social networks. We propose the _state-space point process_ (S2P2) model, a novel and performant model that leverages techniques derived for modern deep state-space models (SSMs) to overcome limitations of existing MTPP models, while simultaneously imbuing strong inductive biases for continuous-time event sequences that other discrete sequence models (i.e., RNNs, transformers) do not capture. Inspired by the classical linear Hawkes processes, we propose an architecture that interleaves stochastic jump differential equations with nonlinearities to create a highly expressive intensity-based MTPP model, without the need for restrictive parametric assumptions for the intensity. Our approach enables efficient training and inference with a parallel scan, bringing linear complexity and sublinear scaling while retaining expressivity to MTPPs. Empirically, S2P2 achieves state-of-the-art predictive likelihoods across eight real-world datasets, delivering an average improvement of 33% over the best existing approaches",
    "checked": true,
    "id": "5b01888fc90dac4678b45df3869157616ce689a2",
    "semantic_title": "deep continuous-time state-space models for marked event sequences",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=3FsM6wWQL4": {
    "title": "HBLLM: Wavelet-Enhanced High-Fidelity 1-Bit Quantization for LLMs",
    "volume": "spotlight",
    "abstract": "We introduce HBLLM, a wavelet-enhanced high-fidelity $1$-bit post-training quantization method for Large Language Models (LLMs). By leveraging Haar wavelet transforms to enhance expressive capacity through frequency decomposition, HBLLM significantly improves quantization fidelity while maintaining minimal overhead. This approach features two innovative structure-aware grouping strategies: (1) frequency-aware multi-parameter intra-row grouping and (2) $\\ell_2$-norm-based saliency-driven column selection. For non-salient weights, a shared mean is employed across quantization groups within each frequency band to optimize storage efficiency. Experiments conducted on the OPT and LLaMA models demonstrate that HBLLM achieves state-of-the-art performance in $1$-bit quantization, attaining a perplexity of $6.71$ on LLaMA$2$-$13$B with an average weight storage of only $1.08$ bits. Code available at: https://github.com/Yeyke/HBLLM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qXAABCxYQ2": {
    "title": "Which Algorithms Have Tight Generalization Bounds?",
    "volume": "spotlight",
    "abstract": "We study which machine learning algorithms have tight generalization bounds with respect to a given collection of population distributions. Our results build on and extend the recent work of Gastpar et al. (2023). First, we present conditions that preclude the existence of tight generalization bounds. Specifically, we show that algorithms that have certain inductive biases that cause them to be unstable do not admit tight generalization bounds. Next, we show that algorithms that are sufficiently loss-stable do have tight generalization bounds. We conclude with a simple characterization that relates the existence of tight generalization bounds to the conditional variance of the algorithm's loss",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cLQlsOGqbM": {
    "title": "Axial Neural Networks for Dimension-Free Foundation Models",
    "volume": "spotlight",
    "abstract": "The advent of foundation models in AI has significantly advanced general-purpose learning, enabling remarkable capabilities in zero-shot inference and in-context learning. However, training such models on physics data, including solutions to partial differential equations (PDEs), poses a unique challenge due to varying dimensionalities across different systems. Traditional approaches either fix a maximum dimension or employ separate encoders for different dimensionalities, resulting in inefficiencies. To address this, we propose a dimension-agnostic neural network architecture, the Axial Neural Network (XNN), inspired by parameter-sharing structures such as Deep Sets and Graph Neural Networks. XNN generalizes across varying tensor dimensions while maintaining computational efficiency. We convert existing PDE foundation models into axial neural networks and evaluate their performance across three training scenarios: training from scratch, pretraining on multiple PDEs, and fine-tuning on a single PDE. Our experiments show that XNNs perform competitively with original models and exhibit superior generalization to unseen dimensions, highlighting the importance of multidimensional pretraining for foundation models",
    "checked": true,
    "id": "ee9e73fbaf61b0a49e03541d7580e5315d6b818f",
    "semantic_title": "axial neural networks for dimension-free foundation models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hguaupzLCU": {
    "title": "Horizon Reduction Makes RL Scalable",
    "volume": "spotlight",
    "abstract": "In this work, we study the scalability of offline reinforcement learning (RL) algorithms. In principle, a truly scalable offline RL algorithm should be able to solve any given problem, regardless of its complexity, given sufficient data, compute, and model capacity. We investigate if and how current offline RL algorithms match up to this promise on diverse, challenging, previously unsolved tasks, using datasets up to 1000× larger than typical offline RL datasets. We observe that despite scaling up data, many existing offline RL algorithms exhibit poor scaling behavior, saturating well below the maximum performance. We hypothesize that the horizon is the main cause behind the poor scaling of offline RL. We empirically verify this hypothesis through several analysis experiments, showing that long horizons indeed present a fundamental barrier to scaling up offline RL. We then show that various horizon reduction techniques substantially enhance scalability on challenging tasks. Based on our insights, we also introduce a minimal yet scalable method named SHARSA that effectively reduces the horizon. SHARSA achieves the best asymptotic performance and scaling behavior among our evaluation methods, showing that explicitly reducing the horizon unlocks the scalability of offline RL",
    "checked": true,
    "id": "a40f0c96f319ffd51d561c5bb9665b8349989205",
    "semantic_title": "horizon reduction makes rl scalable",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=7ieZWCc7rB": {
    "title": "OpenBox: Annotate Any Bounding Boxes in 3D",
    "volume": "spotlight",
    "abstract": "Unsupervised and open-vocabulary 3D object detection has recently gained attention, particularly in autonomous driving, where reducing annotation costs and recognizing unseen objects are critical for both safety and scalability. However, most existing approaches uniformly annotate 3D bounding boxes, ignore objects' physical states, and require multiple self-training iterations for annotation refinement, resulting in suboptimal quality and substantial computational overhead. To address these challenges, we propose OpenBox, a two-stage automatic annotation pipeline that leverages a 2D vision foundation model. In the first stage, OpenBox associates instance-level cues from 2D images processed by a vision foundation model with the corresponding 3D point clouds via context-aware refinement. In the second stage, it categorizes instances by rigidity and motion state, then generates adaptive bounding boxes with class-specific size statistics. As a result, OpenBox produces high-quality 3D bounding box annotations without requiring self-training. Experiments on the Waymo Open Dataset (WOD), the Lyft Level 5 Perception dataset, and the nuScenes dataset demonstrate improved accuracy and efficiency over baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z2SGaPIhLT": {
    "title": "SGCD: Stain-Guided CycleDiffusion for Unsupervised Domain Adaptation of Histopathology Image Classification",
    "volume": "spotlight",
    "abstract": "The effectiveness of domain translation in addressing image-based problems of Unsupervised Domain Adaptation (UDA) depends on the quality of the translated images and the preservation of crucial discriminative features. However, achieving high-quality and stable translations typically requires paired data, which poses a challenge in scenarios with limited annotations in the target domain. To address this issue, this paper proposes a novel method termed Stain-Guided Cycle Diffusion (SGCD), employing a dual diffusion model with bidirectional generative constraints to synthesize highly realistic data for downstream task fine-tuning. The bidirectional generative constraints ensure that the translated images retain the features critical to the downstream model in properly controlling the generation process. Additionally, a stain-guided consistency loss is introduced to enhance the denoising capability of the dual diffusion model, thereby improving the quality of images translated between different domains using latents from one domain and a diffusion model trained on another. Experiments conducted on four public datasets demonstrate that SGCD can effectively enhance the performance of downstream task models on the target domain",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XUmGMBRv4M": {
    "title": "FFN Fusion: Rethinking Sequential Computation in Large Language Models",
    "volume": "spotlight",
    "abstract": "We introduce \\textit{FFN Fusion}, an architectural optimization technique that reduces sequential computation in large language models by identifying and exploiting natural opportunities for parallelization. Our key insight is that sequences of Feed-Forward Network (FFN) layers, particularly those remaining after the removal of specific attention layers, can often be parallelized with minimal accuracy impact. We develop a principled methodology for identifying and fusing such sequences, transforming them into parallel operations that significantly reduce inference latency while preserving model behavior. Applying these techniques to Llama-3.1-405B-Instruct, we create a 253B model (253B-Base), an efficient and soon-to-be publicly available model that achieves a 1.71$\\times$ speedup in inference latency and 35$\\times$ lower per-token cost while maintaining strong performance across benchmarks. Most intriguingly, we find that even full transformer blocks containing both attention and FFN layers can sometimes be parallelized, suggesting new directions for neural architecture design",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VkicTqszOn": {
    "title": "Distilling LLM Agent into Small Models with Retrieval and Code Tools",
    "volume": "spotlight",
    "abstract": "Large language models (LLMs) excel at complex reasoning tasks but remain computationally expensive, limiting their practical deployment. To address this, recent works have focused on distilling reasoning capabilities into smaller language models (sLMs) using chain-of-thought (CoT) traces from teacher LLMs. However, this approach struggles in scenarios requiring rare factual knowledge or precise computation, where sLMs often hallucinate due to limited capability. In this work, we propose Agent Distillation, a framework for transferring not only reasoning capability but full task-solving behavior from LLM-based agents into sLMs with retrieval and code tools. We improve agent distillation along two complementary axes: (1) we introduce a prompting method called first-thought prefix to enhance the quality of teacher-generated trajectories; and (2) we propose a self-consistent action generation for improving test-time robustness of small agents. We evaluate our method on eight reasoning tasks across factual and mathematical domains, covering both in-domain and out-of-domain generalization. Our results show that sLMs as small as 0.5B, 1.5B, 3B parameters can achieve performance competitive with next-tier larger 1.5B, 3B, 7B models fine-tuned using CoT distillation, demonstrating the potential of agent distillation for building practical, tool-using small agents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=76cFMRgEzQ": {
    "title": "Understanding Parametric and Contextual Knowledge Reconciliation within Large Language Models",
    "volume": "spotlight",
    "abstract": "Retrieval-Augmented Generation (RAG) provides additional contextual knowledge to complement the parametric knowledge in Large Language Models (LLMs). These two knowledge interweave to enhance the accuracy and timeliness of LLM responses. However, the internal mechanisms by which LLMs utilize these knowledge remain unclear. We propose modeling the forward propagation of knowledge as an entity flow, employing this framework to trace LLMs' internal behaviors when processing mixed-source knowledge. Linear probing utilizes a trainable linear classifier to detect specific attributes in hidden layers. However, once trained, a probe cannot adapt to dynamically specified entities. To address this challenge, we construct an entity-aware probe, which introduces special tokens to mark probing targets and employs a small trainable rank-8 lora update to process these special markers. We first verify this approach through an attribution experiment, demonstrating that it can accurately detect information about ad-hoc entities from complex hidden states. Next, we trace entity flows across layers to understand how LLMs reconcile conflicting knowledge internally. Our probing results reveal that contextual and parametric knowledge are routed between tokens through distinct sets of attention heads, supporting attention competition only within knowledge types. While conflicting knowledge maintains a residual presence across layers, aligned knowledge from multiple sources gradually accumulates, with the magnitude of this accumulation directly determining its influence on final outputs",
    "checked": false,
    "id": "7ddd9dd9f4ae24f49eadab7a0bc0ae3f0279474f",
    "semantic_title": "massive values in self-attention modules are the key to contextual knowledge understanding",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=F11iEhKoYp": {
    "title": "Differentiable Decision Tree via \"ReLU+Argmin\" Reformulation",
    "volume": "spotlight",
    "abstract": "Decision tree, despite its unmatched interpretability and lightweight structure, faces two key issues that limit its broader applicability: non-differentiability and low testing accuracy. This study addresses these issues by developing a differentiable oblique tree that optimizes the entire tree using gradient-based optimization. We propose an exact reformulation of hard-split trees based on \"ReLU+Argmin\" mechanism, and then cast the reformulated tree training as an unconstrained optimization task. The ReLU-based sample branching, expressed as exact-zero or non-zero values, preserve a unique decision path, in contrast to soft decision trees with probabilistic routing. The subsequent Argmin operation identifies the unique zero-violation path, enabling deterministic predictions. For effective gradient flow, we approximate Argmin behaviors by scaling softmin function. To ameliorate numerical instability, we propose a warm-start annealing scheme that solves multiple optimization tasks with increasingly accurate approximations. This reformulation alongside distributed GPU parallelism offers strong scalability, supporting 12-depth tree even on million-scale datasets where most baselines fail. Extensive experiments demonstrate that our optimized tree achieves a superior testing accuracy against 14 baselines, including an average improvement of 7.54\\% over CART",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dt940loCBT": {
    "title": "SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning",
    "volume": "spotlight",
    "abstract": "Vision-language-action models (VLAs) show potential as generalist robot policies. However, these models pose extreme safety challenges during real-world deployment, including the risk of harm to the environment, the robot itself, and humans. *How can safety constraints be explicitly integrated into VLAs?* We address this by exploring an integrated safety approach (ISA), systematically **modeling** safety requirements, then actively **eliciting** diverse unsafe behaviors, effectively **constraining** VLA policies via safe reinforcement learning, and rigorously **assuring** their safety through targeted evaluations. Leveraging the constrained Markov decision process (CMDP) paradigm, ISA optimizes VLAs from a min-max perspective against elicited safety risks. Thus, policies aligned through this comprehensive approach achieve the following key features: (I) effective **safety-performance trade-offs**, reducing the cumulative cost of safety violations by 83.58\\% compared to the state-of-the-art method, while also maintaining task success rate (+3.85\\%). (II) strong **safety assurance**, with the ability to mitigate long-tail risks and handle extreme failure scenarios. (III) robust **generalization** of learned safety behaviors to various out-of-distribution perturbations. The effectiveness is evaluated on long-horizon mobile manipulation tasks",
    "checked": true,
    "id": "4c1b7cf0550130a2aca6215b759cb09c76b19978",
    "semantic_title": "safevla: towards safety alignment of vision-language-action model via constrained learning",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=PZaxCfLGLA": {
    "title": "3D Interaction Geometric Pre-training for Molecular Relational Learning",
    "volume": "spotlight",
    "abstract": "Molecular Relational Learning (MRL) is a rapidly growing field that focuses on understanding the interaction dynamics between molecules, which is crucial for applications ranging from catalyst engineering to drug discovery. Despite recent progress, earlier MRL approaches are limited to using only the 2D topological structure of molecules, as obtaining the 3D interaction geometry remains prohibitively expensive. This paper introduces a novel 3D geometric pre-training strategy for MRL (3DMRL) that incorporates a 3D virtual interaction environment, overcoming the limitations of costly traditional quantum mechanical calculation methods. With the constructed 3D virtual interaction environment, 3DMRL trains 2D MRL model to learn the global and local 3D geometric information of molecular interaction. Extensive experiments on various tasks using real-world datasets, including out-of-distribution and extrapolation scenarios, demonstrate the effectiveness of 3DMRL, showing up to a 24.93% improvement in performance across 40 tasks. Our code is publicly available at https://github.com/Namkyeong/3DMRL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pXoiIDdynI": {
    "title": "Improved Bounds for Swap Multicalibration and Swap Omniprediction",
    "volume": "spotlight",
    "abstract": "In this paper, we consider the related problems of multicalibration --- a multigroup fairness notion and omniprediction --- a simultaneous loss minimization paradigm, both in the distributional and online settings. The recent work of Garg et al. (2024) raised the open problem of whether it is possible to efficiently achieve $\\tilde{\\mathcal{O}}(\\sqrt{T})$ $\\ell_{2}$-multicalibration error against bounded linear functions. In this paper, we answer this question in a strongly affirmative sense. We propose an efficient algorithm that achieves $\\tilde{\\mathcal{O}}(T^{\\frac{1}{3}})$ $\\ell_{2}$-swap multicalibration error (both in high probability and expectation). On propagating this bound onward, we obtain significantly improved rates for $\\ell_{1}$-swap multicalibration and swap omniprediction for a loss class of convex Lipschitz functions. In particular, we show that our algorithm achieves $\\tilde{\\mathcal{O}}(T^{\\frac{2}{3}})$ $\\ell_{1}$-swap multicalibration and swap omniprediction errors, thereby improving upon the previous best-known bound of $\\tilde{\\mathcal{O}}(T^{\\frac{7}{8}})$. As a consequence of our improved online results, we further obtain several improved sample complexity rates in the distributional setting. In particular, we establish a $\\tilde{\\mathcal{O}}(\\varepsilon ^ {-3})$ sample complexity of efficiently learning an $\\varepsilon$-swap omnipredictor for the class of convex and Lipschitz functions, $\\tilde{\\mathcal{O}}(\\varepsilon ^{-2.5})$ sample complexity of efficiently learning an $\\varepsilon$-swap agnostic learner for the squared loss, and $\\tilde{\\mathcal{O}}(\\varepsilon ^ {-5}), \\tilde{\\mathcal{O}}(\\varepsilon ^ {-2.5})$ sample complexities of learning $\\ell_{1}, \\ell_{2}$-swap multicalibrated predictors against linear functions, all of which significantly improve on the previous best-known bounds",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BRklmFlCsD": {
    "title": "UniteFormer: Unifying Node and Edge Modalities in Transformers for Vehicle Routing Problems",
    "volume": "spotlight",
    "abstract": "Neural solvers for the Vehicle Routing Problem (VRP) have typically relied on either node or edge inputs, limiting their flexibility and generalization in real-world scenarios. We propose UniteFormer, a unified neural solver that supports node-only, edge-only, and hybrid input types through a single model trained via joint edge-node modalities. UniteFormer introduces: (1) a mixed encoder that integrates graph convolutional networks and attention mechanisms to collaboratively process node and edge features, capturing cross-modal interactions between them; and (2) a parallel decoder enhanced with query mapping and a feed-forward layer for improved representation. The model is trained with REINFORCE by randomly sampling input types across batches. Experiments on the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) demonstrate that UniteFormer achieves state-of-the-art performance and generalizes effectively to TSPLib and CVRPLib instances. These results underscore UniteFormer's ability to handle diverse input modalities and its strong potential to improve performance across various VRP tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ib4ZXPXpss": {
    "title": "Simultaneous Swap Regret Minimization via KL-Calibration",
    "volume": "spotlight",
    "abstract": "Calibration is a fundamental concept that aims at ensuring the reliability of probabilistic predictions by aligning them with real-world outcomes. There is a surge of studies on new calibration measures that are easier to optimize compared to the classical $\\ell_1$-Calibration while still having strong implications for downstream applications. One recent such example is the work by Fishelson et al. (2025) who show that it is possible to achieve $\\tilde{\\mathcal{O}}(T^{1/3})$ pseudo $\\ell_{2}$-Calibration error via minimizing pseudo swap regret of the squared loss, which in fact implies the same bound for all bounded proper losses with a smooth univariate form. In this work, we significantly generalize their result in the following ways: (a) in addition to smooth univariate forms, our algorithm also simultaneously achieves $\\tilde{\\mathcal{O}}(T^{1/3})$ swap regret for any proper loss with a twice continuously differentiable univariate form (such as Tsallis entropy); (b) our bounds hold not only for pseudo swap regret that measures losses using the forecaster's distributions on predictions, but also hold for the actual swap regret that measures losses using the forecaster's actual realized predictions. We achieve so by introducing a new stronger notion of calibration called (pseudo) KL-Calibration, which we show is equivalent to the (pseudo) swap regret with respect to log loss. We prove that there exists an algorithm that achieves $\\tilde{\\mathcal{O}}(T^{1/3})$ KL-Calibration error and provide an explicit algorithm that achieves $\\tilde{\\mathcal{O}}(T^{1/3})$ pseudo KL-Calibration error. Moreover, we show that the same algorithm achieves ${\\mathcal{O}}(T^{1/3} (\\log T) ^ {-\\frac{1}{3}}\\log (T/{\\delta}))$ swap regret with probability at least $1-\\delta$ for any proper loss with a smooth univariate form, which implies $\\tilde{\\mathcal{O}}(T^{1/3})$ $\\ell_2$-Calibration error. A technical contribution of our work is a new randomized rounding procedure and a non-uniform discretization scheme to minimize the swap regret for log loss",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b50IW9yV2M": {
    "title": "Wide-Horizon Thinking and Simulation-Based Evaluation for Real-World LLM Planning with Multifaceted Constraints",
    "volume": "spotlight",
    "abstract": "Unlike reasoning, which often entails a deep sequence of deductive steps, complex real-world planning is characterized by the need to synthesize a broad spectrum of parallel and potentially conflicting information and constraints. For example, in travel planning scenarios, it requires the integration of diverse real-world information and user preferences. While LLMs show promise, existing methods with long-horizon thinking struggle with handling multifaceted constraints, leading to suboptimal solutions. Motivated by the challenges of real-world travel planning, this paper introduces the Multiple Aspects of Planning (MAoP), empowering LLMs with \"wide-horizon thinking\" to solve planning problems with multifaceted constraints. Instead of direct planning, MAoP leverages the strategist to conduct pre-planning from various aspects and provide the planning blueprint for planners, enabling strong inference-time scalability by scaling aspects to consider various constraints. In addition, existing benchmarks for multi-constraint planning are flawed because they assess constraints in isolation, ignoring causal dependencies within the constraints, e.g, travel planning, where past activities dictate future itinerary. To address this, we propose Travel-Sim, an agent-based benchmark assessing plans via real-world simulation, thereby inherently resolving these causal dependencies. This paper advances LLM capabilities in complex planning and offers novel insights for evaluating sophisticated scenarios through simulation",
    "checked": true,
    "id": "b8d595cc84fcd5f205a50e2a0cc70155c3029ca2",
    "semantic_title": "wide-horizon thinking and simulation-based evaluation for real-world llm planning with multifaceted constraints",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=5BS6gBb4yP": {
    "title": "Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?",
    "volume": "spotlight",
    "abstract": "Object binding, the brain's ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high‑level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term *IsSameObject*. We decode *IsSameObject* from patch embeddings across ViT layers using a similarity probe, which reaches over 90\\% accuracy. Crucially, this object-binding capability emerges reliably in self-supervised ViTs (DINO, MAE, CLIP), but markedly weaker in ImageNet-supervised models, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that *IsSameObject* is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating *IsSameObject* from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of \"which parts belong together\" emerges naturally in a connectionist system",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ycnc9aLnQu": {
    "title": "Probing Neural Combinatorial Optimization Models",
    "volume": "spotlight",
    "abstract": "Neural combinatorial optimization (NCO) has achieved remarkable performance, yet its learned model representations and decision rationale remain a black box. This impedes both academic research and practical deployment, since researchers and stakeholders require deeper insights into NCO models. In this paper, we take the first critical step towards interpreting NCO models by investigating their representations through various probing tasks. Moreover, we introduce a novel probing tool named Coefficient Significance Probing (CS-Probing) to enable deeper analysis of NCO representations by examining the coefficients and statistical significance during probing. Extensive experiments and analysis reveal that NCO models encode low-level information essential for solution construction, while capturing high-level knowledge to facilitate better decisions. Using CS-Probing, we find that prevalent NCO models impose varying inductive biases on their learned representations, uncover direct evidence related to model generalization, and identify key embedding dimensions associated with specific knowledge. These insights can be potentially translated into practice, for example, with minor code modifications, we improve the generalization of the analyzed model. Our work represents a first systematic attempt to interpret black-box NCO models, showcasing probing as a promising tool for analyzing their internal mechanisms and revealing insights for the NCO community. The source code is publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gte3F0ONhr": {
    "title": "A Principled Path to Fitted Distributional Evaluation",
    "volume": "spotlight",
    "abstract": "In reinforcement learning, distributional off-policy evaluation (OPE) focuses on estimating the return distribution of a target policy using offline data collected under a different policy. This work focuses on extending the widely used fitted Q-evaluation---developed for expectation-based reinforcement learning---to the distributional OPE setting. We refer to this extension as fitted distributional evaluation (FDE). While only a few related approaches exist, there remains no unified framework for designing FDE methods. To fill this gap, we present a set of guiding principles for constructing theoretically grounded FDE methods. Building on these principles, we develop several new FDE methods with convergence analysis and provide theoretical justification for existing methods, even in non-tabular environments. Extensive experiments, including simulations on linear quadratic regulators and Atari games, demonstrate the superior performance of the FDE methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kZstGANG8D": {
    "title": "Improving LLM General Preference Alignment via Optimistic Online Mirror Descent",
    "volume": "spotlight",
    "abstract": "Reinforcement learning from human feedback (RLHF) has demonstrated remarkable effectiveness in aligning large language models (LLMs) with human preferences. Many existing alignment approaches rely on the Bradley-Terry (BT) model assumption, which assumes the existence of a ground-truth reward for each prompt-response pair. However, this assumption can be overly restrictive when modeling complex human preferences. In this paper, we drop the BT model assumption and study LLM alignment under general preferences, formulated as a two-player game. Drawing on theoretical insights from learning in games, we integrate optimistic online mirror descent into our alignment framework to approximate the Nash policy. Theoretically, we demonstrate that our approach achieves an $\\mathcal{O}(T^{-1})$ bound on the duality gap, improving upon the previous $\\mathcal{O}(T^{-1/2})$ result. Meanwhile, it enjoys a linear convergence rate in the last iterate, a property not achieved by previous methods. More importantly, we implement our method and show through experiments that it outperforms state-of-the-art RLHF algorithms across multiple representative benchmarks",
    "checked": true,
    "id": "93679036b6eb4293d42683940ce80aa589c9d321",
    "semantic_title": "improving llm general preference alignment via optimistic online mirror descent",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=vf2GHcxzMV": {
    "title": "Progressive Inference-Time Annealing of Diffusion Models for Sampling from Boltzmann Densities",
    "volume": "spotlight",
    "abstract": "Sampling efficiently from a target unnormalized probability density remains a core challenge, with relevance across countless high-impact scientific applications. A promising approach towards this challenge is the design of amortized samplers that borrow key ideas, such as probability path design, from state-of-the-art generative diffusion models. However, all existing diffusion-based samplers remain unable to draw samples from distributions at the scale of even simple molecular systems. In this paper, we propose Progressive Inference-Time Annealing (PITA) a novel framework to learn diffusion-based samplers that combines two complementary interpolation techniques: I.) Annealing of the Boltzmann distribution and II.) Diffusion smoothing. PITA trains a sequence of diffusion models from high to low temperatures by sequentially training each model at progressively higher temperatures, leveraging engineered easy access to samples of the temperature-annealed target density. In the subsequent step, PITA enables simulating the trained diffusion model to *procure training samples at a lower temperature* for the next diffusion model through inference-time annealing using a novel Feynman-Kac PDE combined with Sequential Monte Carlo. Empirically, PITA enables, for the first time, equilibrium sampling of $N$-body particle systems, Alanine Dipeptide, and tripeptides in Cartesian coordinates with dramatically lower energy function evaluations",
    "checked": true,
    "id": "3b776a51d2ce850aa9f758d5faa3853511af3213",
    "semantic_title": "progressive inference-time annealing of diffusion models for sampling from boltzmann densities",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=om2CpclG4y": {
    "title": "Compositional Monte Carlo Tree Diffusion for Extendable Planning",
    "volume": "spotlight",
    "abstract": "Monte Carlo Tree Diffusion (MCTD) integrates diffusion models with structured tree search to enable effective trajectory exploration through stepwise reasoning. However, MCTD remains fundamentally limited by training trajectory lengths. While periodic replanning allows plan concatenation for longer plan generation, the planning process remains locally confined, as MCTD searches within individual trajectories without access to global context. We propose Compositional Monte Carlo Tree Diffusion (C-MCTD), a framework that elevates planning from individual trajectory optimization to reasoning over complete plan compositions. C-MCTD introduces three complementary components: (1) Online Composer, which performs globally-aware planning by searching across entire plan compositions; (2) Distributed Composer, which reduces search complexity through parallel exploration from multiple starting points; and (3) Preplan Composer, which accelerates inference by leveraging cached plan graphs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JRVZTACwb0": {
    "title": "Fast Monte Carlo Tree Diffusion: 100× Speedup via Parallel and Sparse Planning",
    "volume": "spotlight",
    "abstract": "Diffusion models have recently emerged as a powerful approach for trajectory planning. However, their inherently non-sequential nature limits their effectiveness in long-horizon reasoning tasks at test time. The recently proposed Monte Carlo Tree Diffusion (MCTD) offers a promising solution by combining diffusion with tree-based search, achieving state-of-the-art performance on complex planning problems. Despite its strengths, our analysis shows that MCTD incurs substantial computational overhead due to the sequential nature of tree search and the cost of iterative denoising. To address this, we propose Fast-MCTD, a more efficient variant that preserves the strengths of MCTD while significantly improving its speed and scalability. Fast-MCTD integrates two techniques: Parallel MCTD, which enables parallel rollouts via delayed tree updates and redundancy-aware selection; and Sparse MCTD, which reduces rollout length through trajectory coarsening. Experiments show that Fast-MCTD achieves up to 100× speedup over standard MCTD while maintaining or improving planning performance. Remarkably, it even outperforms Diffuser in inference speed on some tasks, despite Diffuser requiring no search and yielding weaker solutions. These results position Fast-MCTD as a practical and scalable solution for diffusion-based inference-time reasoning",
    "checked": false,
    "id": "e358256ca386d0245f10d24911cf90aec91f4ff7",
    "semantic_title": "fast monte carlo tree diffusion: 100x speedup via parallel sparse planning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=DwZD97uHgm": {
    "title": "Zero-shot Denoising via Neural Compression: Theoretical and algorithmic framework",
    "volume": "spotlight",
    "abstract": "Zero-shot denoising aims to denoise observations without access to training samples or clean reference images. This setting is particularly relevant in practical imaging scenarios involving specialized domains such as medical imaging or biology. In this work, we propose the *Zero-Shot Neural Compression Denoiser* (ZS-NCD), a novel denoising framework based on neural compression. ZS-NCD treats a neural compression network as an untrained model, optimized directly on patches extracted from a single noisy image. The final reconstruction is then obtained by aggregating the outputs of the trained model over overlapping patches. Thanks to the built-in entropy constraints of compression architectures, our method naturally avoids overfitting and does not require manual regularization or early stopping. Through extensive experiments, we show that ZS-NCD achieves state-of-the-art performance among zero-shot denoisers for both Gaussian and Poisson noise, and generalizes well to both natural and non-natural images. Additionally, we provide new finite-sample theoretical results that characterize upper bounds on the achievable reconstruction error of general maximum-likelihood compression-based denoisers. These results further establish the theoretical foundations of compression-based denoising. Our code is available at: https://github.com/Computational-Imaging-RU/ZS-NCDenoiser",
    "checked": true,
    "id": "41d6ba3111b4af4932453879b9f7b7d879d28e2b",
    "semantic_title": "zero-shot denoising via neural compression: theoretical and algorithmic framework",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=miYvfEKvEl": {
    "title": "Gradient-Variation Online Adaptivity for Accelerated Optimization with Hölder Smoothness",
    "volume": "spotlight",
    "abstract": "Smoothness is known to be crucial for acceleration in offline optimization, and for gradient-variation regret minimization in online learning. Interestingly, these two problems are actually closely connected --- accelerated optimization can be understood through the lens of gradient-variation online learning. In this paper, we investigate online learning with *Hölder* functions, a general class encompassing both smooth and non-smooth (Lipschitz) functions, and explore its implications for offline optimization. For (strongly) convex online functions, we design the corresponding gradient-variation online learning algorithm whose regret smoothly interpolates between the optimal guarantees in smooth and non-smooth regimes. Notably, our algorithms do not require prior knowledge of the Hölder smoothness parameter, exhibiting strong adaptivity over existing methods. Through online-to-batch conversion, this gradient-variation online adaptivity yields an optimal universal method for stochastic convex optimization under Hölder smoothness. However, achieving universality in offline strongly convex optimization is more challenging. We address this by integrating online adaptivity with a detection-based guess-and-check procedure, which, for the first time, yields a universal offline method that achieves accelerated convergence in the smooth regime while maintaining near-optimal convergence in the non-smooth one",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qN5hmLkBtC": {
    "title": "A Smooth Sea Never Made a Skilled SAILOR: Robust Imitation via Learning to Search",
    "volume": "spotlight",
    "abstract": "The fundamental limitation of the behavioral cloning (BC) approach to imitation learning is that it only teaches an agent what the expert did at states the expert visited. This means that when a BC agent makes a mistake which takes them out of the support of the demonstrations, they often don't know how to recover from it. In this sense, BC is akin to *giving the agent the fish* -- giving them dense supervision across a narrow set of states -- rather than teaching them *to fish*: to be able to reason independently about achieving the expert's outcome even when faced with unseen situations at test-time. In response, we explore *learning to search* (L2S) from expert demonstrations, i.e. learning the components required to, at test time, plan to match expert outcomes, even after making a mistake. These include *(1)* a world model and *(2)* a reward model. We carefully ablate the set of algorithmic and design decisions required to combine these and other components for stable and sample/interaction-efficient learning of recovery behavior without additional human corrections. Across a dozen visual manipulation tasks from three benchmarks, our approach SAILOR consistently out-performs state-of-the-art Diffusion Policies trained via BC on the same data. Furthermore, scaling up the amount of demonstrations used for BC by 5-10x still leaves a performance gap. We find that SAILOR can identify nuanced failures and is robust to reward hacking. Our code is available at [https://github.com/arnavkj1995/SAILOR](https://github.com/arnavkj1995/SAILOR)",
    "checked": true,
    "id": "51086329921c07bc10158dac823fcb3dbe70d070",
    "semantic_title": "a smooth sea never made a skilled sailor: robust imitation via learning to search",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=1K28gV5MeF": {
    "title": "MAESTRO : Adaptive Sparse Attention and Robust Learning for Multimodal Dynamic Time Series",
    "volume": "spotlight",
    "abstract": "From clinical healthcare to daily living, continuous sensor monitoring across multiple modalities has shown great promise for real-world intelligent decision-making but also faces various challenges. In this work, we argue for modeling such heterogeneous data sources under the multimodal paradigm and introduce a new framework, MAESTRO. We introduce MAESTRO, a novel framework that overcomes key limitations of existing multimodal learning approaches: (1) reliance on a single primary modality for alignment, (2) pairwise modeling of modalities, and (3) assumption of complete modality observations. These limitations hinder the applicability of these approaches in real-world multimodal time-series settings, where primary modality priors are often unclear, the number of modalities can be large (making pairwise modeling impractical), and sensor failures often result in arbitrary missing observations. At its core, MAESTRO facilitates dynamic intra- and cross-modal interactions based on task relevance, and leverages symbolic tokenization and adaptive attention budgeting to construct long multimodal sequences, which are processed via sparse cross-modal attention. The resulting cross-modal tokens are routed through a sparse Mixture-of-Experts (MoE) mechanism, enabling black-box specialization under varying modality combinations. We evaluate MAESTRO against 10 baselines on four diverse datasets spanning three applications, and observe average relative improvements of 4% and 8% over the best existing multimodal and multivariate approaches, respectively, under complete observations. Under partial observations—with up to 40% of missing modalities—MAESTRO achieves an average 9% improvement. Further analysis also demonstrates the robustness and efficiency of MAESTRO's sparse, modality-aware design for learning from dynamic time series",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JeXkIy0JyM": {
    "title": "Ctrl-DNA: Controllable Cell-Type-Specific Regulatory DNA Design via Constrained RL",
    "volume": "spotlight",
    "abstract": "Designing regulatory DNA sequences that achieve precise cell-type-specific gene expression is crucial for advancements in synthetic biology, gene therapy and precision medicine. Although transformer-based language models (LMs) can effectively capture patterns in regulatory DNA, their generative approaches often struggle to produce novel sequences with reliable cell-specific activity. Here, we introduce Ctrl-DNA, a novel constrained reinforcement learning (RL) framework tailored for designing regulatory DNA sequences with controllable cell-type specificity. By formulating regulatory sequence design as a biologically informed constrained optimization problem, we apply RL to autoregressive genomic LMs, enabling the models to iteratively refine sequences that maximize regulatory activity in targeted cell types while constraining off-target effects. Our evaluation on human promoters and enhancers demonstrates that Ctrl-DNA consistently outperforms existing generative and RL-based approaches, generating high-fitness regulatory sequences and achieving state-of-the-art cell-type specificity. Moreover, Ctrl-DNA-generated sequences capture key cell-type-specific transcription factor binding sites (TFBS), short DNA motifs recognized by regulatory proteins that control gene expression, demonstrating the biological plausibility of the generated sequences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V7m2oQ5OFW": {
    "title": "On the Universal Near Optimality of Hedge in Combinatorial Settings",
    "volume": "spotlight",
    "abstract": "In this paper, we study the classical Hedge algorithm in combinatorial settings. In each round, the learner selects a vector $\\mathbf{x}_t$ from a set $\\mathcal{X} \\subseteq$ {$0,1$}$^d$, observes a full loss vector $\\mathbf{y}_t \\in \\mathbb{R}^d$, and incurs a loss $\\langle \\mathbf{x}_t, \\mathbf{y}_t \\rangle \\in [-1,1]$. This setting captures several important problems, including extensive-form games, resource allocation, $m$-sets, online multitask learning, and shortest-path problems on directed acyclic graphs (DAGs). It is well known that Hedge achieves a regret of $\\mathcal{O}\\big(\\sqrt{T \\log |\\mathcal{X}|}\\big)$ after $T$ rounds of interaction. In this paper, we ask whether Hedge is optimal across all combinatorial settings. To that end, we show that for any $\\mathcal{X} \\subseteq$ {$0,1$}$^d$, Hedge is near-optimal—specifically, up to a $\\sqrt{\\log d}$ factor—by establishing a lower bound of $\\Omega\\big(\\sqrt{T \\log(|\\mathcal{X}|)/\\log d}\\big)$ that holds for any algorithm. We then identify a natural class of combinatorial sets—namely, $m$-sets with $\\log d \\leq m \\leq \\sqrt{d}$—for which this lower bound is tight, and for which Hedge is provably suboptimal by a factor of exactly $\\sqrt{\\log d}$. At the same time, we show that Hedge is optimal for online multitask learning, a generalization of the classical $K$-experts problem. Finally, we leverage the near-optimality of Hedge to establish the existence of a near-optimal regularizer for online shortest-path problems in DAGs—a setting that subsumes a broad range of combinatorial domains. Specifically, we show that the classical Online Mirror Descent (OMD) algorithm, when instantiated with the dilated entropy regularizer, is iterate-equivalent to Hedge, and therefore inherits its near-optimal regret guarantees for DAGs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g2f0UoasGs": {
    "title": "Transfer Faster, Price Smarter: Minimax Dynamic Pricing under Cross-Market Preference Shift",
    "volume": "spotlight",
    "abstract": "We study contextual dynamic pricing when a target market can leverage $K$ auxiliary markets—offline logs or concurrent streams—whose *mean utilities differ by a structured preference shift*. We propose *Cross-Market Transfer Dynamic Pricing (CM-TDP)*, the first algorithm that *provably* handles such model-shift transfer and delivers minimax-optimal regret for *both* linear and non-parametric utility models. For linear utilities of dimension $d$, where the *difference* between source- and target-task coefficients is $s_{0}$-sparse, CM-TDP attains regret $\\tilde{\\mathcal{O}}\\bigl((dK^{-1}+s_{0})\\log T\\bigr)$. For nonlinear demand residing in a reproducing kernel Hilbert space with effective dimension $\\alpha$, complexity $\\beta$ and task-similarity parameter $H$, the regret becomes $\\tilde{\\mathcal{O}}\\bigl(K^{-2\\alpha\\beta/(2\\alpha\\beta+1)}T^{1/(2\\alpha\\beta+1)} + H^{2/(2\\alpha+1)}T^{1/(2\\alpha+1)}\\bigr)$, matching information-theoretic lower bounds up to logarithmic factors. The RKHS bound is the first of its kind for transfer pricing and is of independent interest. Extensive simulations show up to 38\\% higher cumulative revenue and $6\\times$ faster convergence relative to single-market pricing baselines. By bridging transfer learning, robust aggregation, and revenue optimization, CM-TDP moves toward pricing systems that *transfer faster, price smarter*",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tt3XLyuDrE": {
    "title": "Transformer brain encoders explain human high-level visual responses",
    "volume": "spotlight",
    "abstract": "A major goal of neuroscience is to understand brain computations during visual processing in naturalistic settings. A dominant approach is to use image-computable deep neural networks trained with different task objectives as a basis for linear encoding models. However, in addition to requiring estimation of a large number of linear encoding parameters, this approach ignores the structure of the feature maps both in the brain and the models. Recently proposed alternatives factor the linear mapping into separate sets of spatial and feature weights, thus finding static receptive fields for units, which is appropriate only for early visual areas. In this work, we employ the attention mechanism used in the transformer architecture to study how retinotopic visual features can be dynamically routed to category-selective areas in high-level visual processing. We show that this computational motif is significantly more powerful than alternative methods in predicting brain activity during natural scene viewing, across different feature basis models and modalities. We also show that this approach is inherently more interpretable as the attention-routing signals for different high-level categorical areas can be easily visualized for any input image. Given its high performance at predicting brain responses to novel images, the model deserves consideration as a candidate mechanistic model of how visual information from retinotopic maps is routed in the human brain based on the relevance of the input content to different category-selective regions. Our code is available at \\href{https://github.com/Hosseinadeli/transformer_brain_encoder/}{https://github.com/Hosseinadeli/transformer\\_brain\\_encoder/}",
    "checked": true,
    "id": "74153d7a68b7f8be5ddb13581d2b2aff68a2a09d",
    "semantic_title": "transformer brain encoders explain human high-level visual responses",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=HvIRFV0J90": {
    "title": "Fast and Fluent Diffusion Language Models via Convolutional Decoding and Rejective Fine-tuning",
    "volume": "spotlight",
    "abstract": "Autoregressive (AR) language models generate text one token at a time, which limits their inference speed. Diffusion-based language models offer a promising alternative, as they can decode multiple tokens in parallel. However, we identify a key bottleneck in current diffusion LMs: the \\textbf{long decoding-window problem}, where tokens generated far from the input context often become irrelevant or repetitive. Previous solutions like semi-autoregressive address this issue by splitting windows into blocks (sacrificing bidirectionality), but we find that this also leads to \\textbf{time-interval expansion problem}, sacrificing the speed. Therefore, semi-AR eliminates the main advantages of diffusion models. To overcome this, we propose Convolutional decoding (\\textit{Conv}), a normalization-based method that narrows the decoding window without hard segmentation, leading to better fluency and flexibility. Additionally, we introduce Rejecting Rule-based Fine-Tuning (R2FT), a post-hoc training scheme that better aligns tokens at positions far from context. Our methods achieve state-of-the-art results on open-ended generation benchmarks (e.g., AlpacaEval) among diffusion LM baselines, with significantly lower step size than previous works, demonstrating both speed and quality improvements. The code is available online (\\url{https://github.com/ybseo-ac/Conv})",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RlqYCpTu1P": {
    "title": "MoBA: Mixture of Block Attention for Long-Context LLMs",
    "volume": "spotlight",
    "abstract": "Scaling the effective context length is essential for advancing large language models (LLMs) toward artificial general intelligence (AGI). However, the quadratic increase in computational complexity inherent in traditional attention mechanisms presents a prohibitive overhead. Existing approaches either impose strongly biased structures, such as sink or window attention which are task-specific, or radically modify the attention mechanism into linear approximations, whose performance in complex reasoning tasks remains inadequately explored. In this work, we propose a solution that adheres to the ``less structure'' principle, allowing the model to determine where to attend autonomously, rather than introducing predefined biases. We introduce Mixture of Block Attention (MoBA), an innovative approach that applies the principles of Mixture of Experts (MoE) to the attention mechanism. This novel architecture demonstrates superior performance on long-context tasks while offering a key advantage: the ability to seamlessly transition between full and sparse attention, enhancing efficiency without the risk of compromising performance. MoBA has already been deployed to handle actual production workloads with long-context requirements, demonstrating significant advancements in efficient attention computation for LLMs. Our code is available at https://github.com/MoonshotAI/MoBA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0KnZasL9nA": {
    "title": "DERD-Net: Learning Depth from Event-based Ray Densities",
    "volume": "spotlight",
    "abstract": "Event cameras offer a promising avenue for multi-view stereo depth estimation and Simultaneous Localization And Mapping (SLAM) due to their ability to detect blur-free 3D edges at high-speed and over broad illumination conditions. However, traditional deep learning frameworks designed for conventional cameras struggle with the asynchronous, stream-like nature of event data, as their architectures are optimized for discrete, image-like inputs. We propose a scalable, flexible and adaptable framework for pixel-wise depth estimation with event cameras in both monocular and stereo setups. The 3D scene structure is encoded into disparity space images (DSIs), representing spatial densities of rays obtained by back-projecting events into space via known camera poses. Our neural network processes local subregions of the DSIs combining 3D convolutions and a recurrent structure to recognize valuable patterns for depth prediction. Local processing enables fast inference with full parallelization and ensures constant ultra-low model complexity and memory costs, regardless of camera resolution. Experiments on standard benchmarks (MVSEC and DSEC datasets) demonstrate unprecedented effectiveness: (i) using purely monocular data, our method achieves comparable results to existing stereo methods; (ii) when applied to stereo data, it strongly outperforms all state-of-the-art (SOTA) approaches, reducing the mean absolute error by at least 42\\%; (iii) our method also allows for increases in depth completeness by more than 3-fold while still yielding a reduction in median absolute error of at least 30\\%. Given its remarkable performance and effective processing of event-data, our framework holds strong potential to become a standard approach for using deep learning for event-based depth estimation and SLAM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R7HJj1YvJH": {
    "title": "Beyond Expectations: Quantile-Guided Alignment for Risk-Calibrated Language Models",
    "volume": "spotlight",
    "abstract": "Large language models can generate rare but catastrophic outputs, such as harmful conversations or insecure code. Existing Reinforcement Learning from Human Feedback (RLHF) typically maximizes average reward, leaving high-risk tail events insufficiently controlled. We introduce Quantile‑Guided Alignment (QA), a framework that allows users to specify desired improvements at any quantile—individually or across multiple reward dimensions—thus shifting the distribution of outputs with finer control toward safer, more desirable outcomes. The method extends standard RLHF via an augmented reward formulation that enforces quantile constraints. Experiments on conversation and code‐generation tasks show that quantile alignment significantly enhances quality at targeted tails while maintaining overall performance. The results position QA as a principled route to risk‑calibrated language models with tail‑focused alignment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LCZmI3iM8X": {
    "title": "VPO: Reasoning Preferences Optimization Based on V -Usable Information",
    "volume": "spotlight",
    "abstract": "Direct Preference Optimization (DPO) is a widely used preference optimization algorithm in large language model (LLM) alignment, which reparameterizes the reward function in reinforcement learning with human feedback (RLHF) without requiring a separate reward model. However, during the DPO training process, when a large negative gradient is applied to low-confidence samples, LLMs with a softmax output head tend to squeeze the confidence in the model's output distribution towards the highest-confidence sentence, which may lead to a decrease in the confidence of both preference and non-preference samples, while increasing the confidence of unrelated tokens. This phenomenon becomes more complex in reasoning tasks. In this work, focusing on reasoning tasks, we propose VPO, a negative gradient constraint method for human non-preference samples based on $\\mathcal{V}$-usable information. By using $\\mathcal{V}$-usable information to measure the similarity between preference pairs and selectively constrain the negative gradient, VPO can alleviate the squeezing effect of DPO, enhance alignment with the generation objective, and maintain the model's ability to distinguish between preference and non-preference samples. We compare VPO with DPO and its latest variants on mathematical reasoning tasks using the LLama 3.1 and Qwen 2.5 series, including both Base and Instruct models. Our results demonstrate that VPO consistently and significantly outperforms existing methods. Specifically, on Qwen2.5-7B-Base, VPO achieves 7.80\\% and 13.25\\% improvement over DPO on MATH500 and AMC23, respectively. We also conduct ablation experiments and in-depth analysis on VPO to explain its effectiveness and rationale",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S9E1nfYPwl": {
    "title": "MoCha: Towards Movie-Grade Talking Character Generation",
    "volume": "spotlight",
    "abstract": "Recent advancements in video generation have achieved impressive motion realism, yet they often overlook character-driven storytelling, a crucial task for automated film, animation generation. We introduce Talking Characters, a more realistic task to generate talking character animations directly from speech and text. Unlike talking head tasks, Talking Characters aims at generating the full portrait of one or more characters beyond the facial region. In this paper, we propose MoCha, the first of its kind to generate talking characters. To ensure precise synchronization between video and speech, we propose a localized audio attention mechanism that effectively aligns speech and video tokens. To address the scarcity of large-scale speech-labelled video datasets, we introduce a joint training strategy that leverages both speech-labelled and text-labelled video data, significantly improving generalization across diverse character actions. We also design structured prompt templates with character tags, enabling, for the first time, multi-character conversation with turn-based dialogue—allowing AI-generated characters to engage in context-aware conversations with cinematic coherence. Extensive qualitative and quantitative evaluations, including human evaluation studies and benchmark comparisons, demonstrate that MoCha sets a new standard for AI-generated cinematic storytelling, achieving superior realism, controllability and generalization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HhCl2BIHfk": {
    "title": "Stable Minima of ReLU Neural Networks Suffer from the Curse of Dimensionality: The Neural Shattering Phenomenon",
    "volume": "spotlight",
    "abstract": "We study the implicit bias of flatness / low (loss) curvature and its effects on generalization in two-layer overparameterized ReLU networks with multivariate inputs---a problem well motivated by the minima stability and edge-of-stability phenomena in gradient-descent training. Existing work either requires interpolation or focuses only on univariate inputs. This paper presents new and somewhat surprising theoretical results for multivariate inputs. On two natural settings (1) generalization gap for flat solutions, and (2) mean-squared error (MSE) in nonparametric function estimation by stable minima, we prove upper and lower bounds, which establish that while flatness does imply generalization, the resulting rates of convergence necessarily deteriorate exponentially as the input dimension grows. This gives an exponential separation between the flat solutions compared to low-norm solutions (i.e., weight decay), which are known not to suffer from the curse of dimensionality. In particular, our minimax lower bound construction, based on a novel packing argument with boundary-localized ReLU neurons, reveals how flat solutions can exploit a kind of \"neural shattering\" where neurons rarely activate, but with high weight magnitudes. This leads to poor performance in high dimensions. We corroborate these theoretical findings with extensive numerical simulations. To the best of our knowledge, our analysis provides the first systematic explanation for why flat minima may fail to generalize in high dimensions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yG8vmj3EAU": {
    "title": "Fisher meets Feynman: score-based variational inference with a product of experts",
    "volume": "spotlight",
    "abstract": "We introduce a highly expressive yet distinctly tractable family for black-box variational inference (BBVI). Each member of this family is a weighted product of experts (PoE), and each weighted expert in the product is proportional to a multivariate $t$-distribution. These products of experts can model distributions with skew, heavy tails, and multiple modes, but to use them for BBVI, we must be able to sample from their densities. We show how to do this by reformulating these products of experts as latent variable models with auxiliary Dirichlet random variables. These Dirichlet variables emerge from a Feynman identity, originally developed for loop integrals in quantum field theory, that expresses the product of multiple fractions (or in our case, $t$-distributions) as an integral over the simplex. We leverage this simplicial latent space to draw weighted samples from these products of experts---samples which BBVI then uses to find the PoE that best approximates a target density. Given a collection of experts, we derive an iterative procedure to optimize the exponents that determine their geometric weighting in the PoE. At each iteration, this procedure minimizes a regularized Fisher divergence to match the scores of the variational and target densities at a batch of samples drawn from the current approximation. This minimization reduces to a convex quadratic program, and we prove under general conditions that these updates converge exponentially fast to a near-optimal weighting of experts. We conclude by evaluating this approach on a variety of synthetic and real-world target distributions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YIGUv0BZCy": {
    "title": "Mitigating the Privacy–Utility Trade-off in Decentralized Federated Learning via f-Differential Privacy",
    "volume": "spotlight",
    "abstract": "Differentially private (DP) decentralized Federated Learning (FL) allows local users to collaborate without sharing their data with a central server. However, accurately quantifying the privacy budget of private FL algorithms is challenging due to the co-existence of complex algorithmic components such as decentralized communication and local updates. This paper addresses privacy accounting for two decentralized FL algorithms within the $f$-differential privacy ($f$-DP) framework. We develop two new $f$-DP–based accounting methods tailored to decentralized settings: Pairwise Network $f$-DP (PN-$f$-DP), which quantifies privacy leakage between user pairs under random-walk communication, and Secret-based $f$-Local DP (Sec-$f$-LDP), which supports structured noise injection via shared secrets. By combining tools from $f$-DP theory and Markov chain concentration, our accounting framework captures privacy amplification arising from sparse communication, local iterations, and correlated noise. Experiments on synthetic and real datasets demonstrate that our methods yield consistently tighter $(\\epsilon, \\delta)$ bounds and improved utility compared to Rényi DP–based approaches, illustrating the benefits of $f$-DP in decentralized privacy accounting",
    "checked": false,
    "id": "8a61a420754c601126612f1cabc08e74d48386ad",
    "semantic_title": "mitigating privacy-utility trade-off in decentralized federated learning via f-differential privacy",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=hq2CkcEY7h": {
    "title": "Mitigating Instability in High Residual Adaptive Sampling for PINNs via Langevin Dynamics",
    "volume": "spotlight",
    "abstract": "Recently, physics-informed neural networks (PINNs) have gained attention in the scientific community for their potential to solve partial differential equations (PDEs). However, they face challenges related to resource efficiency and slow convergence. Adaptive sampling methods, which prioritize collocation points with high residuals, improve both efficiency and accuracy. However, these methods often neglect points with medium or low residuals, which can affect stability as the complexity of the model increases. In this paper, we investigate this limitation and show that high residual-based approaches require stricter learning rate bounds to ensure stability. To address this, we propose a Langevin dynamics-based Adaptive Sampling (LAS) framework that is robust to various learning rates and model complexities. Our experiments demonstrate that the proposed method outperforms existing approaches in terms of relative $L^{2}$ error, and stability across a range of environments, including high-dimensional PDEs where Monte Carlo integration-based methods typically suffer from instability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2CeGVUpOd7": {
    "title": "Spectral Estimation with Free Decompression",
    "volume": "spotlight",
    "abstract": "Computing eigenvalues of very large matrices is a critical task in many machine learning applications, including the evaluation of log-determinants, the trace of matrix functions, and other important metrics. As datasets continue to grow in scale, the corresponding covariance and kernel matrices become increasingly large, often reaching magnitudes that make their direct formation impractical or impossible. Existing techniques typically rely on matrix-vector products, which can provide efficient approximations, if the matrix spectrum behaves well. However, in settings like distributed learning, or when the matrix is defined only indirectly, access to the full data set can be restricted to only very small sub-matrices of the original matrix. In these cases, the matrix of nominal interest is not even available as an implicit operator, meaning that even matrix-vector products may not be available. In such settings, the matrix is \"impalpable\", in the sense that we have access to only masked snapshots of it. We draw on principles from free probability theory to introduce a novel method of \"free decompression\" to estimate the spectrum of such matrices. Our method can be used to extrapolate from the empirical spectral densities of small submatrices to infer the eigenspectrum of extremely large (impalpable) matrices (that we cannot form or even evaluate with full matrix-vector products). We demonstrate the effectiveness of this approach through a series of examples, comparing its performance against known limiting distributions from random matrix theory in synthetic settings, as well as applying it to submatrices of real-world datasets, matching them with their full empirical eigenspectra",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=idnW3BiZcV": {
    "title": "CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching",
    "volume": "spotlight",
    "abstract": "Conditional generative modeling aims to learn a conditional data distribution from samples containing data-condition pairs. For this, diffusion and flow-based methods have attained compelling results. These methods use a learned (flow) model to transport an initial standard Gaussian noise that ignores the condition to the conditional data distribution. The model is hence required to learn both mass transport \\emph{and} conditional injection. To ease the demand on the model, we propose \\emph{Condition-Aware Reparameterization for Flow Matching} (CAR-Flow) -- a lightweight, learned \\emph{shift} that conditions the source, the target, or both distributions. By relocating these distributions, CAR-Flow shortens the probability path the model must learn, leading to faster training in practice. On low-dimensional synthetic data, we visualize and quantify the effects of CAR-Flow. On higher-dimensional natural image data (ImageNet-256), equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while introducing less than \\(0.6\\%\\) additional parameters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TrNB08KuHK": {
    "title": "Training-Free Constrained Generation With Stable Diffusion Models",
    "volume": "spotlight",
    "abstract": "Stable diffusion models represent the state-of-the-art in data synthesis across diverse domains and hold transformative potential for applications in science and engineering, e.g., by facilitating the discovery of novel solutions and simulating systems that are computationally intractable to model explicitly. While there is increasing effort to incorporate physics-based constraints into generative models, existing techniques are either limited in their applicability to latent diffusion frameworks or lack the capability to strictly enforce domain-specific constraints. To address this limitation this paper proposes a novel integration of stable diffusion models with constrained optimization frameworks, enabling the generation of outputs satisfying stringent physical and functional requirements. The effectiveness of this approach is demonstrated through material design experiments requiring adherence to precise morphometric properties, challenging inverse design tasks involving the generation of materials inducing specific stress-strain responses, and copyright-constrained content generation tasks. All code has been released at https://github.com/RAISELab-atUVA/Constrained-Stable-Diffusion",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rSsc9uCVBl": {
    "title": "Comparator-Adaptive Φ -Regret: Improved Bounds, Simpler Algorithms, and Applications to Games",
    "volume": "spotlight",
    "abstract": "In the classic expert problem, $\\Phi$-regret measures the gap between the learner's total loss and that achieved by applying the best action transformation $\\phi \\in \\Phi$. A recent work by Lu et al., [2025] introduced an adaptive algorithm whose regret against a comparator $\\phi$ depends on a certain sparsity-based complexity measure of $\\phi$, recovering and interpolating optimal bounds for standard regret notions such as external, internal, and swap regret. In this work, we propose a general idea to achieve an even better comparator-adaptive $\\Phi$-regret bound via much simpler algorithms compared to Lu et al., [2025]. Specifically, we discover a prior distribution over all possible binary transformations and show that it suffices to achieve prior-dependent regret against these transformations. Then, we propose two concrete and efficient algorithms to achieve so, where the first one combines multiple copies of the kernelized Hedge algorithm of Farina et al., [2022], and the second one combines multiple copies of a variant of the BM-reduction [Blum and Mansour, 2007]. To further showcase the power of our methods and the advantages over Lu et al., [2025] besides the simplicity and better regret bounds, we also show that our second approach can be extended to the game setting to achieve accelerated and adaptive convergence rate to $\\Phi$-equilibria for a class of general-sum games. When specified to the special case of correlated equilibria, our bound improves over the existing ones from Anagnostides et al., [2022a,b]",
    "checked": false,
    "id": "f03350071733c1aea8f750d78d39690f2f3c2d28",
    "semantic_title": "comparator-adaptive φ-regret: improved bounds, simpler algorithms, and applications to games",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S8XcHutp7Z": {
    "title": "X-Field: A Physically Informed Representation for 3D X-ray Reconstruction",
    "volume": "spotlight",
    "abstract": "X-ray imaging is indispensable in medical diagnostics, yet its use is tightly regulated due to radiation exposure. Recent research borrows representations from the 3D reconstruction area to complete two tasks with reduced radiation dose: X-ray Novel View Synthesis (NVS) and Computed Tomography (CT) reconstruction. However, these representations fail to fully capture the penetration and attenuation properties of X-ray imaging as they originate from visible light imaging. In this paper, we introduce X-Field, a 3D representation informed in the physics of X-ray imaging. First, we employ homogeneous 3D ellipsoids with distinct attenuation coefficients to accurately model diverse materials within internal structures. Second, we introduce an efficient path-partitioning algorithm that resolves the intricate intersection of ellipsoids to compute cumulative attenuation along an X-ray path. We further propose a hybrid progressive initialization to refine the geometric accuracy of X-Field and incorporate material-based optimization to enhance model fitting along material boundaries. Experiments show that X-Field achieves superior visual fidelity on both real-world human organ and synthetic object datasets, outperforming state-of-the-art methods in X-ray NVS and CT Reconstruction",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UTGjik64IK": {
    "title": "Distillation Robustifies Unlearning",
    "volume": "spotlight",
    "abstract": "Current LLM unlearning methods are not robust. A few steps of finetuning can revert their effects. We begin by showing that this is true even for an idealized form of unlearning: training to imitate a model that was never trained on unwanted information. This shows that training a model can drastically modify its input-output behavior while leaving its underlying capabilities intact. In light of this dynamic, we show our main result. Training a randomly initialized student on the outputs of an unlearned model transfers behaviors while leaving latent capabilities behind. In short, distillation robustifies unlearning. Based on this result, we propose Unlearn-Noise-Distill-on-Outputs (UNDO), a scalable method that distills an unlearned model into a noised copy of itself. UNDO introduces a tunable tradeoff between compute cost and robustness, establishing a new Pareto frontier on synthetic language and arithmetic tasks. At its strongest setting, UNDO matches the robustness of a model retrained from scratch with perfect data filtering while using only 60-80% of the compute and requiring only 0.01% of the pretraining data to be labeled. We also show that UNDO robustifies unlearning on the more realistic Weapons of Mass Destruction Proxy (WMDP) benchmark. Since distillation is widely used in practice, incorporating an unlearning step beforehand offers a convenient path to robust capability removal",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FxCy8TvQHO": {
    "title": "SimWorld: An Open-ended Simulator for Agents in Physical and Social Worlds",
    "volume": "spotlight",
    "abstract": "While LLM/VLM-powered AI agents have advanced rapidly in math, coding, and computer use, their applications in complex physical and social environments remain challenging. Building agents that can survive and thrive in the real world (e.g., by autonomously earning income) requires massive-scale interaction, reasoning, training, and evaluation across diverse scenarios. However, existing world simulators for such development fall short: they often rely on limited hand-crafted environments, simulate simplified game-like physics and social rules, and lack native support for LLM/VLM agents. We introduce SimWorld, a new simulator built on Unreal Engine 5, designed for developing and evaluating LLM/VLM agents in rich, real-world-like settings. SimWorld offers three core capabilities: (1) realistic, open-ended world simulation, including accurate physical and social dynamics and language-driven procedural environment generation; (2) rich interface for LLM/VLM agents, with multi-modal world inputs/feedback and open-vocabulary action outputs at varying levels of abstraction; and (3) diverse physical and social reasoning scenarios that are easily customizable by users. We demonstrate SimWorld by deploying frontier LLM agents (e.g., Gemini-2.5-Flash, Claude-3.5, GPT-4o, and DeepSeek-Prover-V2) on both short-horizon navigation tasks requiring grounded re-planning, and long-horizon multi-agent food delivery tasks involving strategic cooperation and competition. The results reveal distinct reasoning patterns and limitations across models. We open-source SimWorld and hope it becomes a foundational platform for advancing real-world agent intelligence across disciplines. Please refer to the project website for the most up-to-date information: http://simworld.org/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IT12Radlnq": {
    "title": "Two‑Stage Learning of Stabilizing Neural Controllers via Zubov Sampling and Iterative Domain Expansion",
    "volume": "spotlight",
    "abstract": "Learning-based neural network (NN) control policies have shown impressive empirical performance. However, obtaining stability guarantees and estimates of the region of attraction of these learned neural controllers is challenging due to the lack of stable and scalable training and verification algorithms. Although previous works in this area have achieved great success, much conservatism remains in their frameworks. In this work, we propose a novel two-stage training framework to jointly synthesize a controller a Lyapunov function for continuous-time systems. By leveraging a Zubov‑inspired region of attraction characterization to directly estimate stability boundaries, we propose a novel training-data sampling strategy and a domain-updating mechanism that significantly reduces the conservatism in training. Moreover, unlike existing works on continuous-time systems that rely on an SMT solver to formally verify the Lyapunov condition, we extend state-of-the-art neural network verifier $\\alpha,\\beta$-CROWN with the capability of performing automatic bound propagation through the Jacobian of dynamical systems and a novel verification scheme that avoids expensive bisection. To demonstrate the effectiveness of our approach, we conduct numerical experiments by synthesizing and verifying controllers on several challenging nonlinear systems across multiple dimensions. We show that our training can yield region of attractions with volume $5 - 1.5\\cdot 10^{5}$ times larger compared to the baselines, and our verification on continuous systems can be up to $40-10{,}000$ times faster compared to the traditional SMT solver dReal. Our code is available at https://github.com/Verified-Intelligence/Two-Stage_Neural_Controller_Training",
    "checked": false,
    "id": "c97d45d577702a2366ed001f4a45bf44b27e1fc4",
    "semantic_title": "two-stage learning of stabilizing neural controllers via zubov sampling and iterative domain expansion",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=75LMvs1CjG": {
    "title": "The Complexity of Symmetric Equilibria in Min-Max Optimization and Team Zero-Sum Games",
    "volume": "spotlight",
    "abstract": "We consider the problem of computing stationary points in min-max optimization, with a focus on the special case of Nash equilibria in (two-)team zero-sum games. We first show that computing $\\epsilon$-Nash equilibria in $3$-player $\\text{\\emph{adversarial}}$ team games---wherein a team of $2$ players competes against a $\\text{\\emph{single}}$ adversary---is $\\textsf{CLS}$-complete, resolving the complexity of Nash equilibria in such settings. Our proof proceeds by reducing from $\\text{\\emph{symmetric}}$ $\\epsilon$-Nash equilibria in $\\text{\\emph{symmetric}}$, identical-payoff, two-player games, by suitably leveraging the adversarial player so as to enforce symmetry---without disturbing the structure of the game. In particular, the class of instances we construct comprises solely polymatrix games, thereby also settling a question left open by Hollender, Maystre, and Nagarajan (2024). Moreover, we establish that computing $\\text{\\emph{symmetric}}$ (first-order) equilibria in $\\text{\\emph{symmetric}}$ min-max optimization is $\\textsf{PPAD}$-complete, even for quadratic functions. Building on this reduction, we show that computing symmetric $\\epsilon$-Nash equilibria in symmetric, $6$-player ($3$ vs. $3$) team zero-sum games is also $\\textsf{PPAD}$-complete, even for $\\epsilon = \\text{poly}(1/n)$. As a corollary, this precludes the existence of symmetric dynamics---which includes many of the algorithms considered in the literature---converging to stationary points. Finally, we prove that computing a $\\text{\\emph{non-symmetric}}$ $\\text{poly}(1/n)$-equilibrium in symmetric min-max optimization is $\\textsf{FNP}$-hard",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LyG7kDSsGh": {
    "title": "On Traceability in ℓ p Stochastic Convex Optimization",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VHb883Gs1u": {
    "title": "Improved Representation Steering for Language Models",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "970297a1860c904eaf39f77cb4324ecfad67fcfb",
    "semantic_title": "improved representation steering for language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=1rUj9ZN6Bz": {
    "title": "FlexOLMo: Open Language Models for Flexible Data Use",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "64df726cc0409a4f7dbcbed67e5a59a5141172bd",
    "semantic_title": "flexolmo: open language models for flexible data use",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=DrUR87D4Hj": {
    "title": "The Fragile Truth of Saliency: Improving LLM Input Attribution via Attention Bias Optimization",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ESB924uT5Y": {
    "title": "Enhancing Training Data Attribution with Representational Optimization",
    "volume": "spotlight",
    "abstract": "Training data attribution (TDA) methods aim to measure how training data impacts a model's predictions. While gradient-based attribution methods, such as influence functions, offer theoretical grounding, their computational costs make them impractical for large-scale applications. Representation-based approaches are far more scalable, but typically rely on heuristic embeddings that are not optimized for attribution, limiting their fidelity. To address these challenges, we propose AirRep, a scalable, representation-based approach that closes this gap by learning task-specific and model-aligned representations optimized explicitly for TDA. AirRep introduces two key innovations: a trainable encoder tuned for attribution quality, and an attention-based pooling mechanism that enables accurate estimation of group-wise influence. We train AirRep using a ranking objective over automatically constructed training subsets labeled by their empirical effect on target predictions. Experiments on instruction-tuned LLMs demonstrate that AirRep achieves performance on par with state-of-the-art gradient-based approaches while being nearly two orders of magnitude more efficient at inference time. Further analysis highlights its robustness and generalization across tasks and models. Our code is available at https://github.com/sunnweiwei/AirRep",
    "checked": true,
    "id": "32492a25e4a5337889b9cc1af7ec4facbfd2c3a8",
    "semantic_title": "enhancing training data attribution with representational optimization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=jneVld5iZw": {
    "title": "DisMo: Disentangled Motion Representations for Open-World Motion Transfer",
    "volume": "spotlight",
    "abstract": "Recent advances in text-to-video (T2V) and image-to-video (I2V) models, have enabled the creation of visually compelling and dynamic videos from simple textual descriptions or initial frames. However, these models often fail to provide an explicit representation of motion separate from content, limiting their applicability for content creators. To address this gap, we propose DisMo, a novel paradigm for learning abstract motion representations directly from raw video data via an image-space reconstruction objective. Our representation is generic and independent of static information such as appearance, object identity, or pose. This enables open-world motion transfer, allowing motion to be transferred across semantically unrelated entities without requiring object correspondences, even between vastly different categories. Unlike prior methods, which trade off motion fidelity and prompt adherence, are overfitting to source structure or drifting from the described action, our approach disentangles motion semantics from appearance, enabling accurate transfer and faithful conditioning. Furthermore, our motion representation can be combined with any existing video generator via lightweight adapters, allowing us to effortlessly benefit from future advancements in video models. We demonstrate the effectiveness of our method through a diverse set of motion transfer tasks. Finally, we show that the learned representations are well-suited for downstream motion understanding tasks, consistently outperforming state-of-the-art video representation models such as V-JEPA in zero-shot action classification on benchmarks including Something-Something v2 and Jester. Project page: https://compvis.github.io/DisMo",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vEFPm6gw2s": {
    "title": "Rig3R: Rig-Aware Conditioning and Discovery for 3D Reconstruction",
    "volume": "spotlight",
    "abstract": "Estimating agent pose and 3D scene structure from multi-camera rigs is a central task in embodied AI applications such as autonomous driving. Recent learned approaches such as DUSt3R have shown impressive performance in multiview settings. However, these models treat images as unstructured collections, limiting effectiveness in scenarios where frames are captured from synchronized rigs with known or inferable structure. To this end, we introduce Rig3R, a generalization of prior multiview reconstruction models that incorporates rig structure when available, and learns to infer it when not. Rig3R conditions on optional rig metadata including camera ID, time, and rig poses to develop a rig-aware latent space that remains robust to missing information. It jointly predicts pointmaps and two types of raymaps: a pose raymap relative to a global frame, and a rig raymap relative to a rig-centric frame consistent across time. Rig raymaps allow the model to infer rig structure directly from input images when metadata is missing. The global pose raymaps allow the model to reason about the agent's ego-motion, while the rig raymaps allow the model to infer rig structure directly from input images when metadata is missing. Rig3R achieves state-of-the-art performance in 3D reconstruction, camera pose estimation, and rig discovery -- outperforming both traditional and learned methods by 17-45% mAA across diverse real-world rig datasets, all in a single forward pass without post-processing or iterative refinement",
    "checked": false,
    "id": "f36d4e4c8c906ff9c114d3877727bc82ab3411fa",
    "semantic_title": "rig3r: rig-aware conditioning for learned 3d reconstruction",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=M3zxsDL2Rk": {
    "title": "Cycle-Sync: Robust Global Camera Pose Estimation through Enhanced Cycle-Consistent Synchronization",
    "volume": "spotlight",
    "abstract": "We introduce Cycle-Sync, a robust and global framework for estimating camera poses (both rotations and locations). Our core innovation is a location solver that adapts message-passing least squares (MPLS) - originally developed for group synchronization - to the camera localization setting. We modify MPLS to emphasize cycle-consistent information, redefine cycle consistencies using estimated distances from previous iterations, and incorporate a Welsch-type robust loss. We establish the strongest known deterministic exact-recovery guarantee for camera location estimation, demonstrating that cycle consistency alone enables the lowest sample complexity to date. To further boost robustness, we introduce a plug-and-play outlier rejection module inspired by robust subspace recovery, and we fully integrate cycle consistency into MPLS for rotation averaging. Our global approach avoids the need for bundle adjustment. Experiments on synthetic and real datasets show that Cycle-Sync consistently outperforms leading pose estimators, including full structure-from-motion pipelines with bundle adjustment",
    "checked": true,
    "id": "223807503d92be8f08deabe43b47f0fc7dc52663",
    "semantic_title": "cycle-sync: robust global camera pose estimation through enhanced cycle-consistent synchronization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZbQ5Zq3zA3": {
    "title": "Dense Associative Memory with Epanechnikov Energy",
    "volume": "spotlight",
    "abstract": "We propose a novel energy function for Dense Associative Memory (DenseAM) networks, the log-sum-ReLU (LSR), inspired by optimal kernel density estimation. Unlike the common log-sum-exponential (LSE) function, LSR is based on the Epanechnikov kernel and enables exact memory retrieval with exponential capacity without requiring exponential separation functions. Uniquely, it introduces abundant additional emergent local minima while preserving perfect pattern recovery --- a characteristic previously unseen in DenseAM literature. Empirical results show that LSR energy has significantly more local minima (memories) that have comparable log-likelihood to LSE-based models. Analysis of LSR's emergent memories on image datasets reveals a degree of creativity and novelty, hinting at this method's potential for both large-scale memory storage and generative tasks",
    "checked": true,
    "id": "6ae190201c8c6d2db39fffcd15893c6deeba4c29",
    "semantic_title": "dense associative memory with epanechnikov energy",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=CsXKGIqZtr": {
    "title": "Searching Latent Program Spaces",
    "volume": "spotlight",
    "abstract": "General intelligence requires systems that acquire new skills efficiently and generalize beyond their training distributions. Although program synthesis approaches have strong generalization power, they face scaling issues due to large combinatorial spaces that quickly make them impractical and require human-generated DSLs or pre-trained priors to narrow this search space. On the other hand, deep learning methods have had high successes, but they lack structured test-time adaptation and rely on heavy stochastic sampling or expensive gradient updates for fine-tuning. In this work, we propose the Latent Program Network (LPN), a new architecture that builds in test-time search directly into neural models. LPN learns a latent space of implicit programs---neurally mapping inputs to outputs---through which it can search using gradients at test time. LPN combines the adaptability of symbolic approaches and the scalability of neural methods. It searches through a compact latent space at test time and bypasses the need for pre-defined domain-specific languages. On a range of programming-by-examples tasks, LPN either outperforms or matches performance compared to in-context learning and test-time training methods. Tested on the ARC-AGI benchmark, we demonstrate that LPN can both learn a compact program space and search through it at test time to adapt to novel tasks. LPN doubles its performance on out-of-distribution tasks when test-time search is switched on",
    "checked": true,
    "id": "88ecd72feccd6fc254d321e0692a572729ab64ef",
    "semantic_title": "searching latent program spaces",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=Dn4He1IrUT": {
    "title": "Convergence Rates of Constrained Expected Improvement",
    "volume": "spotlight",
    "abstract": "Constrained Bayesian optimization (CBO) methods have seen significant success in black-box optimization with constraints. One of the most commonly used CBO methods is the constrained expected improvement (CEI) algorithm. CEI is a natural extension of expected improvement (EI) when constraints are incorporated. However, the theoretical convergence rate of CEI has not been established. In this work, we study the convergence rate of CEI by analyzing its simple regret upper bound. First, we show that when the objective function $f$ and constraint function $c$ are assumed to each lie in a reproducing kernel Hilbert space (RKHS), CEI achieves the convergence rates of $\\mathcal{O} \\left(t^{-\\frac{1}{2}}\\log^{\\frac{d+1}{2}}(t) \\right) \\ \\text{and }\\ \\mathcal{O}\\left(t^{\\frac{-\\nu}{2\\nu+d}} \\log^{\\frac{\\nu}{2\\nu+d}}(t)\\right)$ for the commonly used squared exponential and Matérn kernels, respectively. Second, we show that when $f$ and $c$ are assumed to be sampled from Gaussian processes (GPs), CEI achieves the same convergence rates with a high probability. Numerical experiments are performed to validate the theoretical analysis",
    "checked": true,
    "id": "332de02293cf2f25b41798b9bf42d9fe96dae891",
    "semantic_title": "convergence rates of constrained expected improvement",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OkVQJZWGfn": {
    "title": "CoT Information: Improved Sample Complexity under Chain-of-Thought Supervision",
    "volume": "spotlight",
    "abstract": "Learning complex functions that involve multi-step reasoning poses a significant challenge for standard supervised learning from input-output examples. Chain-of-thought (CoT) supervision, which augments training data with intermediate reasoning steps to provide a richer learning signal, has driven recent advances in large language model reasoning. This paper develops a statistical theory of learning under CoT supervision. Central to the theory is the *CoT information*, which measures the additional discriminative power offered by the chain-of-thought for distinguishing hypotheses with different end-to-end behaviors. The main theoretical results demonstrate how CoT supervision can yield significantly faster learning rates compared to standard end-to-end supervision, with both upper bounds and information-theoretic lower bounds characterized by the CoT information",
    "checked": true,
    "id": "a9c6e4becb5ae1a4edf9130b80f93f8ac27a8ee1",
    "semantic_title": "cot information: improved sample complexity under chain-of-thought supervision",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U87XyMPrZp": {
    "title": "Unlocking hidden biomolecular conformational landscapes in diffusion models at inference time",
    "volume": "spotlight",
    "abstract": "The function of biomolecules such as proteins depends on their ability to interconvert between a wide range of structures or conformations. Researchers have endeavored for decades to develop computational methods to predict the distribution of conformations, which is far harder to determine experimentally than a static folded structure. We present ConforMix, an inference-time algorithm that enhances sampling of conformational distributions using a combination of classifier guidance, filtering, and free energy estimation. Our approach upgrades diffusion models---whether trained for static structure prediction or conformational generation---to enable more efficient discovery of conformational variability without requiring prior knowledge of major degrees of freedom. ConforMix is orthogonal to improvements in model pretraining and would benefit even a hypothetical model that perfectly reproduced the Boltzmann distribution. Remarkably, when applied to a diffusion model trained for static structure prediction, ConforMix captures structural changes including domain motion, cryptic pocket flexibility, and transporter cycling, while avoiding unphysical states. Case studies of biologically critical proteins demonstrate the scalability, accuracy, and utility of this method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sEFDhxF1mG": {
    "title": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models",
    "volume": "spotlight",
    "abstract": "Vision-Language Models (VLMs) are integral to tasks such as image captioning and visual question answering, but their high computational cost, driven by large memory footprints and processing time, limits their scalability and real-time applicability. In this work, we propose leveraging Singular-Value Decomposition (SVD) over the joint query (Q), key (K), and value (V) weight matrices to reduce KV cache size and computational overhead. We in addition introduce an efficient rank allocation strategy that dynamically adjusts the SVD rank based on its impact on VLM accuracy, achieving a significant reduction in both memory usage and computational cost. Finally, we extend this approach by applying quantization to both VLM weights and activations, resulting in a highly efficient VLM. Our method outperforms previous approaches that rely solely on quantization or SVD by achieving more than $10$% accuracy improvement while consuming less hardware cost, making it better for real-time deployment on resource-constrained devices. We open source our code at https://github.com/SAI-Lab-NYU/QSVD",
    "checked": true,
    "id": "0266895a8db1a2b2973e97dffbb79fc3754f69aa",
    "semantic_title": "qsvd: efficient low-rank approximation for unified query-key-value weight compression in low-precision vision-language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m6WmeOI1AW": {
    "title": "Caption This, Reason That: VLMs Caught in the Middle",
    "volume": "spotlight",
    "abstract": "Vision-Language Models (VLMs) have shown remarkable progress in visual understanding in recent years. Yet, they still lag behind human capabilities in specific visual tasks such as counting or relational reasoning. To understand the underlying limitations, we adopt methodologies from cognitive science, analyzing VLM performance along core cognitive axes: Perception, Attention, and Memory. Using a suite of tasks targeting these abilities, we evaluate state-of-the-art VLMs, including GPT-4o. Our analysis reveals distinct cognitive profiles: while advanced models approach ceiling performance on some tasks (e.g. category identification), a significant gap persists, particularly in tasks requiring spatial understanding or selective attention. Investigating the source of these failures and potential methods for improvement, we employ a vision-text decoupling analysis, finding that models struggling with direct visual reasoning show marked improvement when reasoning over their own generated text captions. These experiments reveal a strong need for improved VLM Chain-of-Thought (CoT) abilities, even in models that consistently exceed human performance. Furthermore, we demonstrate the potential of targeted fine-tuning on composite visual reasoning tasks and show that fine-tuning smaller VLMs moderately improves core cognitive abilities. While this improvement does not translate to large enhancements on challenging, out-of-distribution benchmarks, we show broadly that VLM performance on our datasets strongly correlates with performance on established benchmarks like MMMU-Pro and VQAv2. Our work provides a detailed analysis of VLM cognitive strengths and weaknesses and identifies key bottlenecks in simultaneous perception and reasoning while also providing an effective and simple solution",
    "checked": true,
    "id": "88d5705c3f723c8fe656fa835f9e573c3cad2a91",
    "semantic_title": "caption this, reason that: vlms caught in the middle",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TOhpnECT10": {
    "title": "Universal Causal Inference in a Topos",
    "volume": "spotlight",
    "abstract": "In this paper, we explore the universal properties underlying causal inference by formulating it in terms of a topos. More concretely, we introduce topos causal models (TCMs), a strict generalization of the popular structural causal models (SCMs). A topos category has several properties that make it attractive: a general theory for how to combine local functions that define ``independent causal mechanisms\" into a consistent global function building on the theory of sheaves in a topos; a generic way to define causal interventions using a subobject classifier in a topos category; and finally, an internal logical language for causal and counterfactual reasoning that emerges from the topos itself. A striking characteristic of subobject classifiers is that they induce an intuitionistic logic, whose semantics is based on the partially ordered lattice of subobjects. We show that the underlying subobject classifier for causal inference is not Boolean in general, but forms a Heyting algebra. We define the internal Mitchell-B\\'enabou language, a typed local set theory, associated with causal models, and its associated Kripke-Joyal intuitionistic semantics. We prove a universal property of TCM, namely that any causal functor mapping decomposable structure to probabilistic semantics factors uniquely through a TCM representation",
    "checked": true,
    "id": "45d4ed127b36cf761b068f81ba9b865584236dc5",
    "semantic_title": "universal causal inference in a topos",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=PhIWEbewAz": {
    "title": "Can We Infer Confidential Properties of Training Data from LLMs?",
    "volume": "spotlight",
    "abstract": "Large language models (LLMs) are increasingly fine-tuned on domain-specific datasets to support applications in fields such as healthcare, finance, and law. These fine-tuning datasets often have sensitive and confidential dataset-level properties — such as patient demographics or disease prevalence—that are not intended to be revealed. While prior work has studied property inference attacks on discriminative models (e.g., image classification models) and generative models (e.g., GANs for image data), it remains unclear if such attacks transfer to LLMs. In this work, we introduce PropInfer, a benchmark task for evaluating property inference in LLMs under two fine-tuning paradigms: question-answering and chat-completion. Built on the ChatDoctor dataset, our benchmark includes a range of property types and task configurations. We further propose two tailored attacks: a prompt-based generation attack and a shadow-model attack leveraging word frequency signals. Empirical evaluations across multiple pretrained LLMs show the success of our attacks, revealing a previously unrecognized vulnerability in LLMs",
    "checked": true,
    "id": "8319ea3d282271873499ca8f38f5efd838f8cd0a",
    "semantic_title": "can we infer confidential properties of training data from llms?",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=EjkvtZwRoA": {
    "title": "Temperature is All You Need for Generalization in Langevin Dynamics and other Markov Processes",
    "volume": "spotlight",
    "abstract": "We analyze the generalization gap (gap between the training and test errors) when training a potentially over-parametrized model using a Markovian stochastic training algorithm, initialized from some distribution $\\theta_0 \\sim p_0$. We focus on Langevin dynamics with a positive temperature $\\beta^{-1}$, i.e. gradient descent on a training loss $L$ with infinitesimal step size, perturbed with $\\beta^{-1}$-variances Gaussian noise, and lightly regularized or bounded. There, we bound the generalization gap, *at any time during training*, by $\\sqrt{(\\beta\\mathbb{E} L (\\theta_0) + \\log(1/\\delta))/N}$ with probability $1-\\delta$ over the dataset, where $N$ is the sample size, and $\\mathbb{E} L(\\theta_0)=O(1)$ with standard initialization scaling. In contrast to previous guarantees, we have no dependence on either training time or reliance on mixing, nor a dependence on dimensionality, gradient norms, or any other properties of the loss or model. This guarantee follows from a general analysis of any Markov process-based training that has a Gibbs-style stationary distribution. The proof is surprisingly simple, once we observe that the marginal distribution divergence from initialization remains bounded, as implied by a generalized second law of thermodynamics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oBikm5Rshc": {
    "title": "Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in Language Models",
    "volume": "spotlight",
    "abstract": "For an LLM to correctly respond to an instruction it must understand both the semantics and the domain (i.e., subject area) of a given task-instruction pair. However, syntax can also convey implicit information. Recent work shows that \\textit{syntactic templates}---frequent sequences of Part-of-Speech (PoS) tags---are prevalent in training data and often appear in model outputs. In this work we characterize syntactic templates, domain, and semantics in task-instruction pairs. We identify cases of spurious correlations between syntax and domain, where models learn to associate a domain with syntax during training; this can sometimes override prompt semantics. Using a synthetic training dataset, we find that the syntactic-domain correlation can lower performance (mean 0.51 +/- 0.06) on entity knowledge tasks in OLMo-2 models (1B-13B). We introduce an evaluation framework to detect this phenomenon in trained models, and show that it occurs on a subset of the FlanV2 dataset in open (OLMo-2-7B; Llama-4-Maverick), and closed (GPT-4o) models. Finally, we present a case study on the implications for LLM security, showing that unintended syntactic-domain correlations can be used to bypass refusals in OLMo-2-7B Instruct and GPT-4o. Our findings highlight two needs: (1) to explicitly test for syntactic-domain correlations, and (2) to ensure \\textit{syntactic} diversity in training data, specifically within domains, to prevent such spurious correlations",
    "checked": true,
    "id": "eae3dd2c902f237eb4dd8234cb483740acf4717c",
    "semantic_title": "learning the wrong lessons: syntactic-domain spurious correlations in language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sf5nxMRiG7": {
    "title": "Corporate Needs You to Find the Difference: Revisiting Submodular and Supermodular Ratio Optimization Problems",
    "volume": "spotlight",
    "abstract": "We consider the following question: given a submodular or supermodular set function $f:2^V \\to \\mathbb{R}$, how should one minimize or maximize its average value $f(S)/|S|$ over non-empty subsets $S\\subseteq V$? This problem generalizes several well-known objectives including Densest Subgraph (DSG), Densest Supermodular Set (DSS), and Submodular Function Minimization (SFM). Motivated by recent applications [39, 31], we formalize two new broad problems: the Unrestricted Sparsest Submodular Set (USSS) and Unrestricted Densest Supermodular Set (UDSS) which allow negative and non-monotone functions. Using classical results we observe that DSS, SFM, USSS, UDSS, and MNP are all equivalent under strongly polynomial-time reductions. This equivalence enables algorithmic cross-over: methods designed for one problem can be repurposed to solve others efficiently. In particular we use the perspective of the minimum norm point in the base polyhedron of a sub/supermodular function which, via Fujishige's results, yields the dense decomposition as a byproduct. Via this perspective we show that a recent converging heuristic for DSS, \\textsc{SuperGreedy++} [15, 29], and Wolfe's minimum norm point algorithm are both universal solvers for all of these problems. On the theoretical front, we explain the observation made in recent work [39, 31] that \\textsc{SuperGreedy++} appears to work well even in settings beyond DSS. Surprisingly, we also show that this simple algorithm can be used for Submodular Function Minimization, including for example that it can act as an effective minimum $st$ cut algorithm. On the empirical front, we explore the utility of several different algorithms including Fujishige-Wolfe min-norm point algorithm for recent problems. We conduct over 400 experiments across seven problem types and large-scale synthetic and real-world datasets (up to $\\approx 100$ million edges). Our results reveal that methods historically considered inefficient, such as convex-programming methods, flow-based solvers, and Fujishige-Wolfe's algorithm, outperform state-of-the-art task-specific baselines by orders of magnitude on concrete problems like HNSN [39]. These findings challenge prevailing assumptions and demonstrate that with the right framing, general optimization algorithms can be both scalable and state-of-the-art for supermodular and submodular ratio problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RdNYp8ilPr": {
    "title": "Private Set Union with Multiple Contributions",
    "volume": "spotlight",
    "abstract": "In the private set union problem each user owns a bag of at most $k$ items (from some large universe of items), and we are interested in computing the union of the items in the bags of all of the users. This is trivial without privacy, but a differentially private algorithm must be careful about reporting items contained in only a small number of bags. We consider differentially private algorithms that always report a subset of the union, and define the utility of an algorithm to be the expected size of the subset that it reports. Because the achievable utility varies significantly with the dataset, we introduce the *utility ratio*, which normalizes utility by a dataset-specific upper bound and characterizes a mechanism by its lowest normalized utility across all datasets. We then develop algorithms with guaranteed utility ratios and complement them with bounds on the best possible utility ratio. Prior work has shown that a single algorithm can be simultaneously optimal for all datasets when $k=1$, but we show that instance-optimal algorithms do not exist when $k>1$, and characterize how performance degrades as $k$ grows. At the same time, we design a private algorithm that achieves the maximum possible utility, regardless of $k$, when the item histogram matches a prior prediction (for instance, from a previous data release) and degrades gracefully with the $L_\\infty$ distance between the prediction and the actual histogram when the prediction is imperfect",
    "checked": false,
    "id": "df9d47a140785d15c90426e05bb954c916a9bfd1",
    "semantic_title": "quantum private set intersection with correlated sum using single photons",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=96I0XnrjkQ": {
    "title": "Clustering via Hedonic Games: New Concepts and Algorithms",
    "volume": "spotlight",
    "abstract": "We study fundamental connections between coalition formation games and clustering, illustrating the cross-disciplinary relevance of these concepts. We focus on graphical hedonic games where agents' preferences are compactly represented by a friendship graph and an enemy graph. In the context of clustering, friendship relations naturally align with data point similarities, whereas enmity corresponds to dissimilarities. We consider two stability notions based on single-agent deviations: local popularity and local stability. Exploring these concepts from an algorithmic viewpoint, we design efficient mechanisms for finding locally stable or locally popular partitions. Besides gaining theoretical insight into the computational complexity of these problems, we perform simulations that demonstrate how our algorithms can be successfully applied in clustering and community detection. Our findings highlight the interplay between coalition formation games and data-driven clustering techniques, offering fresh perspectives and applications in both areas",
    "checked": false,
    "id": "25d4f4be2ff2475ec6cb509326bb6807aa230a65",
    "semantic_title": "role of artificial intelligence in customer engagement: a systematic review and future research directions",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=EEZLBhyer1": {
    "title": "The Graphon Limit Hypothesis: Understanding Neural Network Pruning via Infinite Width Analysis",
    "volume": "spotlight",
    "abstract": "Sparse neural networks promise efficiency, yet training them effectively remains a fundamental challenge. Despite advances in pruning methods that create sparse architectures, understanding why some sparse structures are better trainable than others with the same level of sparsity remains poorly understood. Aiming to develop a systematic approach to this fundamental problem, we propose a novel theoretical framework based on the theory of graph limits, particularly graphons, that characterizes sparse neural networks in the infinite-width regime. Our key insight is that connectivity patterns of sparse neural networks induced by pruning methods converge to specific graphons as networks' width tends to infinity, which encodes implicit structural biases of different pruning methods. We postulate the *Graphon Limit Hypothesis* and provide empirical evidence to support it. Leveraging this graphon representation, we derive a *Graphon Neural Tangent Kernel (Graphon NTK)* to study the training dynamics of sparse networks in the infinite width limit. Graphon NTK provides a general framework for the theoretical analysis of sparse networks. We empirically show that the spectral analysis of Graphon NTK correlates with observed training dynamics of sparse networks, explaining the varying convergence behaviours of different pruning methods. Our framework provides theoretical insights into the impact of connectivity patterns on the trainability of various sparse network architectures",
    "checked": true,
    "id": "e72ec3cb44988ee3f34dfed15bc82c049ede59d4",
    "semantic_title": "the graphon limit hypothesis: understanding neural network pruning via infinite width analysis",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a7hHwWnZey": {
    "title": "Fast Training of Large Kernel Models with Delayed Projections",
    "volume": "spotlight",
    "abstract": "Classical kernel machines have historically faced significant challenges in scaling to large datasets and model sizes—a key ingredient that has driven the success of neural networks. In this paper, we present a new methodology for building kernel machines that can scale efficiently with both data size and model size. Our algorithm introduces delayed projections to Preconditioned Stochastic Gradient Descent (PSGD) allowing the training of much larger models than was previously feasible. We validate our algorithm, \\EP4, across multiple datasets, demonstrating drastic training speedups without compromising the performance. Our implementation is publicly available at: https://github.com/EigenPro/EigenPro",
    "checked": true,
    "id": "56ffefa74c04813ba99b5ae13c2b01ced8885000",
    "semantic_title": "fast training of large kernel models with delayed projections",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=7ZVRlBFuEv": {
    "title": "d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning. To this end, we propose, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO, the first integration of policy gradient methods to masked dLLMs. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and planning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM",
    "checked": true,
    "id": "8e3b1f5d8b6c165f64137cc1f7dea89cf6f622bd",
    "semantic_title": "d1: scaling reasoning in diffusion large language models via reinforcement learning",
    "citation_count": 58,
    "authors": []
  },
  "https://openreview.net/forum?id=d6UV0UNgn9": {
    "title": "Affine-Invariant Global Non-Asymptotic Convergence Analysis of BFGS under Self-Concordance",
    "volume": "spotlight",
    "abstract": "In this paper, we establish global non-asymptotic convergence guarantees for the BFGS quasi-Newton method without requiring strong convexity or the Lipschitz continuity of the gradient or Hessian. Instead, we consider the setting where the objective function is strictly convex and strongly self-concordant. For an arbitrary initial point and any arbitrary positive-definite initial Hessian approximation, we prove global linear and superlinear convergence guarantees for BFGS when the step size is determined using a line search scheme satisfying the weak Wolfe conditions. Moreover, all our global guarantees are affine-invariant, with the convergence rates depending solely on the initial error and the strongly self-concordant constant. Our results extend the global non-asymptotic convergence theory of BFGS beyond traditional assumptions and, for the first time, establish affine-invariant convergence guarantees—aligning with the inherent affine invariance of the BFGS method",
    "checked": true,
    "id": "2a38cd8209e7dfcd225818ebad2d04ab71fd9bee",
    "semantic_title": "affine-invariant global non-asymptotic convergence analysis of bfgs under self-concordance",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=aLUAzLDIOc": {
    "title": "Extrapolation by Association: Length Generalization Transfer In Transformers",
    "volume": "spotlight",
    "abstract": "Transformer language models have demonstrated impressive generalization capabilities in natural language domains, yet we lack a fine-grained understanding of how such generalization arises. In this paper, we investigate length generalization—the ability to extrapolate from shorter to longer inputs—through the lens of \\textit{task transfer}. We find that length generalization can be \\textit{transferred} across related tasks. That is, training a model with a longer and related auxiliary task can lead the model to generalize to unseen and longer inputs from some other target task. We demonstrate this length generalization transfer across a diverse suite of algorithmic tasks, including arithmetic operations, string transformations, and maze navigation. Our results show that transformer models can inherit generalization capabilities from similar tasks when trained jointly. Moreover, we observe similar transfer effects in pretrained language models, suggesting that pretraining equips models with reusable computational scaffolding that facilitates extrapolation in downstream settings. Finally, we provide initial mechanistic evidence that length generalization transfer correlates with the re-use of the same attention heads between the tasks. Together, our findings deepen our understanding of how transformers generalize to out-of-distribution inputs and highlight the compositional reuse of inductive structure across tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZAKpELpclI": {
    "title": "Optimal Neural Compressors for the Rate-Distortion-Perception Tradeoff",
    "volume": "spotlight",
    "abstract": "Recent efforts in neural compression have focused on the rate-distortion-perception (RDP) tradeoff, where the perception constraint ensures the source and reconstruction distributions are close in terms of a statistical divergence. Theoretical work on RDP describes properties of RDP-optimal compressors without providing constructive and low complexity solutions. While classical rate-distortion theory shows that optimal compressors should efficiently pack space, RDP theory additionally shows that infinite randomness shared between the encoder and decoder may be necessary for RDP optimality. In this paper, we propose neural compressors that are low complexity and benefit from high packing efficiency through lattice coding and shared randomness through shared dithering over the lattice cells. For two important settings, namely infinite shared and zero shared randomness, we analyze the RDP tradeoff achieved by our proposed neural compressors and show optimality in both cases. Experimentally, we investigate the roles that these two components of our design, lattice coding and randomness, play in the performance of neural compressors on synthetic and real-world data. We observe that performance improves with more shared randomness and better lattice packing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xDxskDUvte": {
    "title": "Bridging Symmetry and Robustness: On the Role of Equivariance in Enhancing Adversarial Robustness",
    "volume": "spotlight",
    "abstract": "Adversarial examples reveal critical vulnerabilities in deep neural networks by exploiting their sensitivity to imperceptible input perturbations. While adversarial training remains the predominant defense strategy, it often incurs significant computational cost and may compromise clean-data accuracy. In this work, we investigate an architectural approach to adversarial robustness by embedding group-equivariant convolutions—specifically, rotation- and scale-equivariant layers—into standard convolutional neural networks (CNNs). These layers encode symmetry priors that align model behavior with structured transformations in the input space, promoting smoother decision boundaries and greater resilience to adversarial attacks. We propose and evaluate two symmetry-aware architectures: a parallel design that processes standard and equivariant features independently before fusion, and a cascaded design that applies equivariant operations sequentially. Theoretically, we demonstrate that such models reduce hypothesis space complexity, regularize gradients, and yield tighter certified robustness bounds under the CLEVER (Cross Lipschitz Extreme Value for nEtwork Robustness) framework. Empirically, our models consistently improve adversarial robustness and generalization across CIFAR-10, CIFAR-100, and CIFAR-10C under both FGSM and PGD attacks, without requiring adversarial training. These findings underscore the potential of symmetry-enforcing architectures as efficient and principled alternatives to data augmentation-based defenses",
    "checked": true,
    "id": "97c47caf0ef9fb400c4864ade7c9e6693a1c3496",
    "semantic_title": "bridging symmetry and robustness: on the role of equivariance in enhancing adversarial robustness",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r1Bx58M6It": {
    "title": "Color Conditional Generation with Sliced Wasserstein Guidance",
    "volume": "spotlight",
    "abstract": "We propose SW-Guidance, a training-free approach for image generation conditioned on the color distribution of a reference image. While it is possible to generate an image with fixed colors by first creating an image from a text prompt and then applying a color style transfer method, this approach often results in semantically meaningless colors in the generated image. Our method solves this problem by modifying the sampling process of a diffusion model to incorporate the differentiable Sliced 1-Wasserstein distance between the color distribution of the generated image and the reference palette. Our method outperforms state-of-the-art techniques for color-conditional generation in terms of color similarity to the reference, producing images that not only match the reference colors but also maintain semantic coherence with the original text prompt. Our source code is available at https://github.com/alobashev/sw-guidance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zjMd3yfyWv": {
    "title": "Private Hyperparameter Tuning with Ex-Post Guarantee",
    "volume": "spotlight",
    "abstract": "The conventional approach in differential privacy (DP) literature formulates the privacy-utility tradeoff with a \"privacy-first\" perspective: for a predetermined level of privacy, a certain utility is achievable. However, practitioners often operate under a \"utility-first\" paradigm, prioritizing a desired level of utility and then determining the corresponding privacy cost. Wu et al. [2019] initiated a formal study of this ``utility-first'' perspective by introducing ex-post DP. They demonstrated that by adding correlated Laplace noise and progressively reducing it on demand, a sequence of increasingly accurate estimates of a private parameter can be generated, with the privacy cost attributed only to the least noisy iterate released. This led to a Laplace mechanism variant that achieves a specified utility with minimal privacy loss. However, their work, and similar findings by Whitehouse et al. [2023], are primarily limited to simple mechanisms based on Laplace or Gaussian noise. In this paper, we significantly generalize these results. In particular, we extend the findings of Wu et al. [2019] and Liu and Talwar [2019] to support any sequence of private estimators, incurring at most a doubling of the original privacy budget. Furthermore, we demonstrate that hyperparameter tuning for these estimators, including the selection of an optimal privacy budget, can be performed without additional privacy cost. Finally, we extend our results to ex-post R\\'{e}nyi DP, further broadening the applicability of utility-first privacy mechanisms",
    "checked": true,
    "id": "aac066a83ec966c62c56686b42cf37d489eb4efe",
    "semantic_title": "private hyperparameter tuning with ex-post guarantee",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=M0U8wUow8c": {
    "title": "A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning",
    "volume": "spotlight",
    "abstract": "Due to the size and complexity of modern large language models (LLMs), it has proven challenging to uncover the underlying mechanisms that models use to solve reasoning problems. For instance, is their reasoning for a specific problem localized to certain parts of the network? Do they break down the reasoning problem into modular components that are then executed as sequential steps as we go deeper in the model? To better understand the reasoning capability of LLMs, we study a minimal propositional logic problem that requires combining multiple facts to arrive at a solution. By studying this problem on Mistral and Gemma models, up to 27B parameters, we illuminate the core components the models use to solve such logic problems. From a mechanistic interpretability point of view, we use causal mediation analysis to uncover the pathways and components of the LLMs' reasoning processes. Then, we offer fine-grained insights into the functions of attention heads in different layers. We not only find a sparse circuit that computes the answer, but we decompose it into sub-circuits that have four distinct and modular uses. Finally, we reveal that three distinct models -- Mistral-7B, Gemma-2-9B and Gemma-2-27B -- contain analogous but not identical mechanisms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SgQAleMecy": {
    "title": "Optimal and Provable Calibration in High-Dimensional Binary Classification: Angular Calibration and Platt Scaling",
    "volume": "spotlight",
    "abstract": "We study the fundamental problem of calibrating a linear binary classifier of the form \\(\\sigma(\\hat{w}^\\top x)\\), where the feature vector \\(x\\) is Gaussian, \\(\\sigma\\) is a link function, and \\(\\hat{w}\\) is an estimator of the true linear weight $w^\\star$. By interpolating with a noninformative \\emph{chance classifier}, we construct a well-calibrated predictor whose interpolation weight depends on the angle \\(\\angle(\\hat{w}, w_\\star)\\) between the estimator \\(\\hat{w}\\) and the true linear weight \\(w_\\star\\). We establish that this angular calibration approach is provably well-calibrated in a high-dimensional regime where the number of samples and features both diverge, at a comparable rate. The angle \\(\\angle(\\hat{w}, w_\\star)\\) can be consistently estimated. Furthermore, the resulting predictor is uniquely \\emph{Bregman-optimal}, minimizing the Bregman divergence to the true label distribution within a suitable class of calibrated predictors. Our work is the first to provide a calibration strategy that satisfies both calibration and optimality properties provably in high dimensions. Additionally, we identify conditions under which a classical Platt-scaling predictor converges to our Bregman-optimal calibrated solution. Thus, Platt-scaling also inherits these desirable properties provably in high dimensions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0vJJdEiXOb": {
    "title": "Reconstruction and Secrecy under Approximate Distance Queries",
    "volume": "spotlight",
    "abstract": "Consider the task of locating an unknown target point using approximate distance queries: in each round, a reconstructor selects a reference point and receives a noisy version of its distance to the target. This problem arises naturally in various contexts—from localization in GPS and sensor networks to privacy-aware data access—making it relevant from the perspective of both the reconstructor (seeking accurate recovery) and the responder (aiming to limit information disclosure, e.g., for privacy or security reasons). We study this reconstruction game through a learning-theoretic lens, focusing on the rate and limits of the best possible reconstruction error. Our first result provides a tight geometric characterization of the optimal error in terms of the Chebyshev radius, a classical concept from geometry. This characterization applies to all compact metric spaces (in fact, to all totally bounded spaces) and yields explicit formulas for natural subsets of the Euclidean metric. Our second result addresses the asymptotic behavior of reconstruction, distinguishing between pseudo-finite spaces, where the optimal error is attained after finitely many queries, and spaces where the approximation curve exhibits a nontrivial decay. We characterize pseudo-finiteness for convex subsets of Euclidean spaces",
    "checked": true,
    "id": "a8cb1d49fa1085de93b002c0f9c96b7949e52680",
    "semantic_title": "reconstruction and secrecy under approximate distance queries",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fcs90Rwm8j": {
    "title": "Win Fast or Lose Slow: Balancing Speed and Accuracy in Latency-Sensitive Decisions of LLMs",
    "volume": "spotlight",
    "abstract": "Large language models (LLMs) have shown remarkable performance across diverse reasoning and generation tasks, and are increasingly deployed as agents in dynamic environments such as code generation and recommendation systems. However, many real-world applications, such as high-frequency trading and real-time competitive gaming, require decisions under strict latency constraints, where faster responses directly translate into higher rewards. Despite the importance of this latency–quality trade-off, it remains underexplored in the context of LLM-based agents. In this work, we present the first systematic study of this trade-off in real-time decision-making tasks. To support our investigation, we introduce two new benchmarks: HFTBench, a high-frequency trading simulation, and StreetFighter, a competitive gaming platform. Our analysis reveals that optimal latency–quality balance varies by task, and that sacrificing quality for lower latency can significantly enhance downstream performance. To address this, we propose FPX, an adaptive framework that dynamically selects model size and quantization level based on real-time demands. Our method achieves the best performance on both benchmarks, improving win rate by up to 80% in Street Fighter and boosting daily yield by up to 26.52% in trading, underscoring the need for latency-aware evaluation and deployment strategies for LLM-based agents. These results demonstrate the critical importance of latency-aware evaluation and deployment strategies for real-world LLM-based agents",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=29LwAgLFpj": {
    "title": "Characterizing the Expressivity of Fixed-Precision Transformer Language Models",
    "volume": "spotlight",
    "abstract": "Transformer-based language models (LMs) have achieved widespread empirical success, but their theoretical expressive power remains only partially understood. In this work, we analyze a restricted idealization of fixed-precision transformers with strict future masking, soft attention, and no positional encodings. We establish that this class of models is exactly as expressive as a specific fragment of linear temporal logic that contains only a single temporal operator: the $\\texttt{past}$ operator. We further connect this fragment to established classes in formal language theory, automata theory, and algebra, yielding a unified framework for understanding transformer expressivity under this idealization. Finally, we present empirical results that align closely with our theory: transformers trained on languages within their characterized expressive capacity generalize reliably across sequence lengths, while they consistently fail to generalize on languages beyond it",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UBRFn7YKMe": {
    "title": "Distributional Training Data Attribution: What do Influence Functions Sample?",
    "volume": "spotlight",
    "abstract": "Randomness is an unavoidable part of training deep learning models, yet something that traditional training data attribution algorithms fail to rigorously account for. They ignore the fact that, due to stochasticity in the initialisation and batching, training on the same dataset can yield different models. In this paper, we address this shortcoming through introducing _distributional_ training data attribution (d-TDA), the goal of which is to predict how the distribution of model outputs (over training runs) depends upon the dataset. Intriguingly, we find that _influence functions_ (IFs), a popular data attribution tool, are 'secretly distributional': they emerge from our framework as the limit to unrolled differentiation, without requiring restrictive convexity assumptions. This provides a new perspective on the effectiveness of IFs in deep learning. We demonstrate the practical utility of d-TDA in experiments, including improving data pruning for vision transformers and identifying influential examples with diffusion models",
    "checked": false,
    "id": "5a4ae26eb51d7420be84e8ead7658901738ec4a8",
    "semantic_title": "a step towards generalisability: training a machine learning scoring function for structure-based virtual screening",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=roKj4IwaVT": {
    "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
    "volume": "spotlight",
    "abstract": "Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM \"workers\" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while \"seeing\" each other's partial progress in the concurrent cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with \"instant\" access to each other's generated tokens. Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4FyNdd2b5S": {
    "title": "Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation",
    "volume": "spotlight",
    "abstract": "We propose a novel approach for disentangling visual and semantic features from the backbones of pre-trained diffusion models, enabling visual correspondence in a manner analogous to the well-established semantic correspondence. While diffusion model backbones are known to encode semantically rich features, they must also contain visual features to support their image synthesis capabilities. However, isolating these visual features is challenging due to the absence of annotated datasets. To address this, we introduce an automated pipeline that constructs image pairs with annotated semantic and visual correspondences based on existing subject-driven image generation datasets, and design a contrastive architecture to separate the two feature types. Leveraging the disentangled representations, we propose a new metric, Visual Semantic Matching (VSM), that quantifies visual inconsistencies in subject-driven image generation. Empirical results show that our approach outperforms global feature-based metrics such as CLIP, DINO, and vision--language models in quantifying visual inconsistencies while also enabling spatial localization of inconsistent regions. To our knowledge, this is the first method that supports both quantification and localization of inconsistencies in subject-driven generation, offering a valuable tool for advancing this task",
    "checked": true,
    "id": "c47c6ac5f795524daab6b4bb63f5c9806657c037",
    "semantic_title": "mind-the-glitch: visual correspondence for detecting inconsistencies in subject-driven generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZYXTLo7kCi": {
    "title": "The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?",
    "volume": "spotlight",
    "abstract": "The concept of causal abstraction got recently popularised to demystify the opaque decision-making processes of machine learning models; in short, a neural network can be abstracted as a higher-level algorithm if there exists a function which allows us to map between them. Notably, most interpretability papers implement these maps as linear functions, motivated by the linear representation hypothesis: the idea that features are encoded linearly in a model's representations. However, this linearity constraint is not required by the definition of causal abstraction. In this work, we critically examine the concept of causal abstraction by considering arbitrarily powerful alignment maps. In particular, we prove that under reasonable assumptions, any neural network can be mapped to any algorithm, rendering this unrestricted notion of causal abstraction trivial and uninformative. We complement these theoretical findings with empirical evidence, demonstrating that it is possible to perfectly map models to algorithms even when these models are incapable of solving the actual task; e.g., on an experiment using randomly initialised language models, our alignment maps reach 100\\% interchange-intervention accuracy on the indirect object identification task. This raises the non-linear representation dilemma: if we lift the linearity constraint imposed to alignment maps in causal abstraction analyses, we are left with no principled way to balance the inherent trade-off between these maps' complexity and accuracy. Together, these results suggest an answer to our title's question: causal abstraction is not enough for mechanistic interpretability, as it becomes vacuous without assumptions about how models encode information. Studying the connection between this information-encoding assumption and causal abstraction should lead to exciting future work",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j1QkrVjNVF": {
    "title": "Gaussian Herding across Pens: An Optimal Transport Perspective on Global Gaussian Reduction for 3DGS",
    "volume": "spotlight",
    "abstract": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for radiance field rendering, but it typically requires millions of redundant Gaussian primitives, overwhelming memory and rendering budgets. Existing compaction approaches address this by pruning Gaussians based on heuristic importance scores, without global fidelity guarantee. To bridge this gap, we propose a novel optimal transport perspective that casts 3DGS compaction as global Gaussian mixture reduction. Specifically, we first minimize the composite transport divergence over a KD-tree partition to produce a compact geometric representation, and then decouple appearance from geometry by fine-tuning color and opacity attributes with far fewer Gaussian primitives. Experiments on benchmark datasets show that our method (i) yields negligible loss in rendering quality (PSNR, SSIM, LPIPS) compared to vanilla 3DGS with only 10\\% Gaussians; and (ii) consistently outperforms state-of-the-art 3DGS compaction techniques. Notably, our method is applicable to any stage of vanilla or accelerated 3DGS pipelines, providing an efficient and agnostic pathway to lightweight neural rendering",
    "checked": true,
    "id": "ac881d15117ed82524c86716d4755febaf561d9f",
    "semantic_title": "gaussian herding across pens: an optimal transport perspective on global gaussian reduction for 3dgs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jVwIfsJLvh": {
    "title": "Generalized Top-k Mallows Model for Ranked Choices",
    "volume": "spotlight",
    "abstract": "The classic Mallows model is a foundational tool for modeling user preferences. However, it has limitations in capturing real-world scenarios, where users often focus only on a limited set of preferred items and are indifferent to the rest. To address this, extensions such as the top-$k$ Mallows model have been proposed, aligning better with practical applications. In this paper, we address several challenges related to the generalized top-$k$ Mallows model, with a focus on analyzing buyer choices. Our key contributions are: (1) a novel sampling scheme tailored to generalized top-$k$ Mallows models, (2) an efficient algorithm for computing choice probabilities under this model, and (3) an active learning algorithm for estimating the model parameters from observed choice data. These contributions provide new tools for analysis and prediction in critical decision-making scenarios. We present a rigorous mathematical analysis for the performance of our algorithms. Furthermore, through extensive experiments on synthetic data and real-world data, we demonstrate the scalability and accuracy of our proposed methods, and we compare the predictive power of Mallows model for top-$k$ lists compared to the simpler Multinomial Logit model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jlJaRXDzCE": {
    "title": "Improving Bilinear RNN with Closed-loop Control",
    "volume": "spotlight",
    "abstract": "Recent efficient sequence modeling methods, such as Gated DeltaNet, TTT, and RWKV-7, have achieved performance improvements by supervising the recurrent memory management through the Delta learning rule. Unlike previous state-space models (e.g., Mamba) and gated linear attentions (e.g., GLA), these models introduce interactions between the recurrent state and the key vector, resulting in a bilinear recursive structure. In this paper, we first introduce the concept of Bilinear RNNs with a comprehensive analysis on the advantages and limitations of these models. Then based on the closed-loop control theory, we propose a novel Bilinear RNN variant named Comba, which adopts a scalar-plus-low-rank state transition, with both state feedback and output feedback corrections. We also implement a hardware-efficient chunk-wise parallel kernel in Triton and train models with 340M/1.3B parameters on a large-scale corpus. Comba demonstrates its superior performance and computation efficiency on both language modeling and vision tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RnXS7aK4rK": {
    "title": "Spatial-MLLM: Boosting MLLM Capabilities in Visual-based Spatial Intelligence",
    "volume": "spotlight",
    "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced performance on 2D visual tasks. However, improving their spatial intelligence remains a challenge. Existing 3D MLLMs always rely on additional 3D or 2.5D data to incorporate spatial awareness, restricting their utility in scenarios with only 2D inputs, such as images or videos. In this paper, we present Spatial-MLLM, a novel framework for visual-based spatial reasoning from purely 2D observations. Unlike conventional video MLLMs which rely on CLIP-based visual encoders optimized for semantic understanding, our key insight is to unleash the strong structure prior from the feed-forward visual geometry foundation model. Specifically, we propose a dual-encoder architecture: a pretrained 2D visual encoder to extract semantic features, and a 3D spatial encoder—initialized from the backbone of the visual geometry model—to extract 3D structure features. A connector then integrates both features into unified visual tokens for enhanced spatial understanding. Furthermore, we propose a space-aware frame sampling strategy at inference time, which selects the spatially informative frames of a video sequence, ensuring that even under limited token length, the model focuses on frames critical for spatial reasoning. Beyond architecture improvements, we construct a training dataset from multiple sources and train the model on it using supervised fine-tuning and GRPO. Extensive experiments on various real-world datasets demonstrate that Spatial-MLLM achieves state-of-the-art performance in a wide range of visual-based spatial understanding and reasoning tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o8r3gOFTQo": {
    "title": "SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation",
    "volume": "spotlight",
    "abstract": "Few-shot segmentation aims to segment unseen categories from just a handful of annotated examples. This requires mechanisms to identify semantically related objects across images and accurately produce masks. We note that Segment Anything 2 (SAM2), with its prompt-and-propagate mechanism, provides strong segmentation capabilities and a built-in feature matching process. However, we show that its representations are entangled with task-specific cues optimized for object tracking, which impairs its use for tasks requiring higher level semantic understanding. Our key insight is that, despite its class-agnostic pretraining, SAM2 already encodes rich semantic structure in its features. We propose SANSA (Semantically AligNed Segment Anything 2), a framework that makes this latent structure explicit, and repurposes SAM2 for few-shot segmentation through minimal task-specific modifications. SANSA achieves state-of-the-art on few-shot segmentation benchmarks designed to assess generalization and outperforms generalist methods in the popular in-context setting. Additionally, it supports flexible promptable interaction via points, boxes, or scribbles, and remains significantly faster and more compact than prior approaches. Code at: https://github.com/ClaudiaCuttano/SANSA",
    "checked": true,
    "id": "bfd5d9643493302d1561c82d6ab2b275782cd268",
    "semantic_title": "sansa: unleashing the hidden semantics in sam2 for few-shot segmentation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4Qe2Hga43N": {
    "title": "Cost-Aware Contrastive Routing for LLMs",
    "volume": "spotlight",
    "abstract": "We study cost-aware routing for large language models across diverse and dynamic pools of models. Existing approaches often overlook prompt-specific context, rely on expensive model profiling, assume a fixed set of experts, or use inefficient trial-and-error strategies. We introduce Cost-Spectrum Contrastive Routing (CSCR), a lightweight framework that maps both prompts and models into a shared embedding space to enable fast, cost-sensitive selection. CSCR uses compact, fast-to-compute logit footprints for open-source models and perplexity fingerprints for black-box APIs. A contrastive encoder is trained to favor the cheapest accurate expert within adaptive cost bands. At inference time, routing reduces to a single $k$‑NN lookup via a FAISS index, requiring no retraining when the expert pool changes and enabling microsecond latency. Across multiple benchmarks, CSCR consistently outperforms baselines, improving the accuracy–cost tradeoff by up to 25\\%, while generalizing robustly to unseen LLMs and out-of-distribution prompts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tk5nQnTGmP": {
    "title": "Is Grokking a Computational Glass Relaxation?",
    "volume": "spotlight",
    "abstract": "Understanding neural network' (NN) generalizability remains a central question in deep learning research. The special phenomenon of grokking, where NNs abruptly generalize long after the training performance reaches near-perfect level, offers a unique window to investigate the underlying mechanisms of NNs' generalizability. Here we propose an interpretation for grokking by framing it as a computational glass relaxation: viewing NNs as a physical system where parameters are the degrees of freedom and train loss is the system energy, we find memorization process resembles a rapid cooling of liquid into non-equilibrium glassy state at low temperature and the later generalization is like a slow relaxation towards a more stable configuration. This mapping enables us to sample NNs' Boltzmann entropy (states of density) landscape as a function of training loss and test accuracy. Our experiments in transformers on arithmetic tasks suggests that there is NO entropy barrier in the memorization-to-generalization transition of grokking, challenging previous theory that defines grokking as a first-order phase transition. We identify a high-entropy advantage under grokking, an extension of prior work linking entropy to generalizability but much more significant. Inspired by grokking's far-from-equilibrium nature, we develop a toy optimizer WanD based on Wang-landau molecular dynamics, which can eliminate grokking without any constraints and find high-norm generalizing solutions. This provides strictly-defined counterexamples to theory attributing grokking solely to weight norm evolution towards the Goldilocks zone and also suggests new potential ways for optimizer design",
    "checked": true,
    "id": "74b14f4c9342a99d2748575a6f83f1361721edfb",
    "semantic_title": "is grokking a computational glass relaxation?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q06YjUj0FB": {
    "title": "LoRATv2: Enabling Low-Cost Temporal Modeling in One-Stream Trackers",
    "volume": "spotlight",
    "abstract": "Transformer-based algorithms, such as LoRAT, have significantly enhanced object-tracking performance. However, these approaches rely on a standard attention mechanism, which incurs quadratic token complexity, making real-time inference computationally expensive. In this paper, we introduce LoRATv2, a novel tracking framework that addresses these limitations with three main contributions. First, LoRATv2 integrates frame-wise causal attention, which ensures full self-attention within each frame while enabling causal dependencies across frames, significantly reducing computational overhead. Moreover, key-value (KV) caching is employed to efficiently reuse past embeddings for further speedup. Second, building on LoRAT's parameter-efficient fine-tuning, we propose Stream-Specific LoRA Adapters (SSLA). As frame-wise causal attention introduces asymmetry in how streams access temporal information, SSLA assigns dedicated LoRA modules to the template and each search stream, with the main ViT backbone remaining frozen. This allows specialized adaptation for each stream's role in temporal tracking. Third, we introduce a two-phase progressive training strategy, which first trains a single-search-frame tracker and then gradually extends it to multi-search-frame inputs by introducing additional LoRA modules. This curriculum-based learning paradigm improves long-term tracking while maintaining training efficiency. In extensive experiments on multiple benchmarks, LoRATv2 achieves state-of-the-art performance, substantially improved efficiency, and a superior performance-to-FLOPs ratio over state-of-the-art trackers. The code is available at https://github.com/LitingLin/LoRATv2",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ONc9vWkwCp": {
    "title": "On the necessity of adaptive regularisation: Optimal anytime online learning on ℓ p -balls",
    "volume": "spotlight",
    "abstract": "We study online convex optimization on $\\ell_p$-balls in $\\mathbb{R}^d$ for $p > 2$. While always sub-linear, the optimal regret exhibits a shift between the high-dimensional setting ($d > T$), when the dimension $d$ is greater than the time horizon $T$ and the low-dimensional setting ($d \\leq T$). We show that Follow-the-Regularised-Leader (FTRL) with time-varying regularisation which is adaptive to the dimension regime is anytime optimal for all dimension regimes. Motivated by this, we ask whether it is possible to obtain anytime optimality of FTRL with fixed non-adaptive regularisation. Our main result establishes that for separable regularisers, adaptivity in the regulariser is necessary, and that any fixed regulariser will be sub-optimal in one of the two dimension regimes. Finally, we provide lower bounds which rule out sub-linear regret bounds for the linear bandit problem in sufficiently high-dimension for all $\\ell_p$-balls with $p \\geq 1$",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ESELaMThLN": {
    "title": "Controlling Thinking Speed in Reasoning Models",
    "volume": "spotlight",
    "abstract": "Human cognition is theorized to operate in two modes: fast, intuitive System 1 thinking and slow, deliberate System 2 thinking. While current Large Reasoning Models (LRMs) excel at System 2 thinking, their inability to perform fast thinking leads to high computational overhead and latency. In this work, we enable LRMs to approximate human intelligence through dynamic thinking speed adjustment, optimizing accuracy-efficiency trade-offs. Our approach addresses two key questions: (1) how to control thinking speed in LRMs, and (2) when to adjust it for optimal performance. For the first question, we identify the steering vector that governs slow-fast thinking transitions in LRMs' representation space. Using this vector, we achieve the first representation editing-based test-time scaling effect, outperforming existing prompt-based scaling methods. For the second question, we apply real-time difficulty estimation to signal reasoning segments of varying complexity. Combining these techniques, we propose the first reasoning strategy that enables fast processing of easy steps and deeper analysis for complex reasoning. Without any training or additional cost, our plug-and-play method yields an average +1.3\\% accuracy with -8.6\\% token usage across leading LRMs and advanced reasoning benchmarks. All of our algorithms are implemented based on vLLM and are expected to support broader applications and inspire future research",
    "checked": true,
    "id": "b20b78c0ee66a6ed27f1e0038b66db9639d25e69",
    "semantic_title": "controlling thinking speed in reasoning models",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=M44RvNMZs4": {
    "title": "Vector Quantization in the Brain: Grid-like Codes in World Models",
    "volume": "spotlight",
    "abstract": "We propose Grid-like Code Quantization (GCQ), a brain-inspired method for compressing observation-action sequences into discrete representations using grid-like patterns in attractor dynamics. Unlike conventional vector quantization approaches that operate on static inputs, GCQ performs spatiotemporal compression through an action-conditioned codebook, where codewords are derived from continuous attractor neural networks and dynamically selected based on actions. This enables GCQ to jointly compress space and time, serving as a unified world model. The resulting representation supports long-horizon prediction, goal-directed planning, and inverse modeling. Experiments across diverse tasks demonstrate GCQ's effectiveness in compact encoding and downstream performance. Our work offers both a computational tool for efficient sequence modeling and a theoretical perspective on the formation of grid-like codes in neural systems",
    "checked": true,
    "id": "923e1917623fd42e7125fd8e05a0981444bec5e9",
    "semantic_title": "vector quantization in the brain: grid-like codes in world models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cggdvyt8ik": {
    "title": "IA-GGAD: Zero-shot Generalist Graph Anomaly Detection via Invariant and Affinity Learning",
    "volume": "spotlight",
    "abstract": "Generalist Graph Anomaly Detection (GGAD) extends traditional Graph Anomaly Detection (GAD) from one-for-one to one-for-all scenarios, posing significant challenges due to Feature Space Shift (FSS) and Graph Structure Shift (GSS). This paper first formalizes these challenges and proposes quantitative metrics to measure their severity. To tackle FSS, we develop an anomaly-driven graph invariant learning module that learns domain-invariant node representations. To address GSS, a novel structure-insensitive affinity learning module is introduced, capturing cross-domain structural correspondences via affinity-based features. Our unified framework, IA-GGAD, integrates these modules, enabling anomaly prediction on unseen graphs without target-domain retraining or fine-tuning. Extensive experiments on benchmark datasets from varied domains demonstrate IA-GGAD's superior performance, significantly outperforming state-of-the-art methods (e.g., achieving up to +12.28\\% AUROC over ARC on ACM). Ablation studies further confirm the effectiveness of each proposed module. The code is available at \\url{https://github.com/kg-cc/IA-GGAD/}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wvcYIEaD5X": {
    "title": "Revisiting Generative Infrared and Visible Image Fusion Based on Human Cognitive Laws",
    "volume": "spotlight",
    "abstract": "Existing infrared and visible image fusion methods often face the dilemma of balancing modal information. Generative fusion methods reconstruct fused images by learning from data distributions, but their generative capabilities remain limited. Moreover, the lack of interpretability in modal information selection further affects the reliability and consistency of fusion results in complex scenarios. This manuscript revisits the essence of generative image fusion under the inspiration of human cognitive laws and proposes a novel infrared and visible image fusion method, termed HCLFuse. First, HCLFuse investigates the quantification theory of information mapping in unsupervised fusion networks, which leads to the design of a multi-scale mask-regulated variational bottleneck encoder. This encoder applies posterior probability modeling and information decomposition to extract accurate and concise low-level modal information, thereby supporting the generation of high-fidelity structural details. Furthermore, the probabilistic generative capability of the diffusion model is integrated with physical laws, forming a time-varying physical guidance mechanism that adaptively regulates the generation process at different stages, thereby enhancing the ability of the model to perceive the intrinsic structure of data and reducing dependence on data quality. Experimental results show that the proposed method achieves state-of-the-art fusion performance in qualitative and quantitative evaluations across multiple datasets and significantly improves semantic segmentation metrics. This fully demonstrates the advantages of this generative image fusion method, drawing inspiration from human cognition, in enhancing structural consistency and detail quality",
    "checked": true,
    "id": "f3f7a58a41bf46be5f0f7ca6af6e0af8c6721473",
    "semantic_title": "revisiting generative infrared and visible image fusion based on human cognitive laws",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wZzC5rpDY1": {
    "title": "MonoLift: Learning 3D Manipulation Policies from Monocular RGB via Distillation",
    "volume": "spotlight",
    "abstract": "Although learning 3D manipulation policies from monocular RGB images is lightweight and deployment-friendly, the lack of structural information often leads to inaccurate action estimation. While explicit 3D inputs can mitigate this issue, they typically require additional sensors and introduce data acquisition overhead. An intuitive alternative is to incorporate a pre-trained depth estimator; however, this often incurs substantial inference-time cost. To address this, we propose MonoLift, a tri-level knowledge distillation framework that transfers spatial, temporal, and action-level knowledge from a depth-guided teacher to a monocular RGB student. By jointly distilling geometry-aware features, temporal dynamics, and policy behaviors during training, MonoLift enables the student model to perform 3D-aware reasoning and precise control at deployment using only monocular RGB input. Extensive experiments on both simulated and real-world manipulation tasks show that MonoLift not only outperforms existing monocular approaches but even surpasses several methods that rely on explicit 3D input, offering a resource-efficient and effective solution for vision-based robotic control. The video demonstration is available on our project page: https://robotasy.github.io/MonoLift/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kmv7yg6QXv": {
    "title": "SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation",
    "volume": "spotlight",
    "abstract": "While spatial reasoning has made progress in object localization relationships, it often overlooks object orientation—a key factor in 6-DoF fine-grained manipulation. Traditional pose representations rely on pre-defined frames or templates, limiting generalization and semantic grounding. In this paper, we introduce the concept of semantic orientation, which defines object orientations using natural language in a reference-frame-free manner (e.g., the ''plug-in'' direction of a USB or the ''handle'' direction of a cup). To support this, we construct OrienText300K, a large-scale dataset of 3D objects annotated with semantic orientations, and develop PointSO, a general model for zero-shot semantic orientation prediction. By integrating semantic orientation into VLM agents, our SoFar framework enables 6-DoF spatial reasoning and generates robotic actions. Extensive experiments demonstrated the effectiveness and generalization of our SoFar, e.g., zero-shot 48.7\\% successful rate on Open6DOR and zero-shot 74.9\\% successful rate on SIMPLER-Env",
    "checked": true,
    "id": "3564562b1fd9ed8abf9aaeef0afd1d759c5e0d55",
    "semantic_title": "sofar: language-grounded orientation bridges spatial reasoning and object manipulation",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=F1wDPNLvTb": {
    "title": "Conditional Representation Learning for Customized Tasks",
    "volume": "spotlight",
    "abstract": "Conventional representation learning methods learn a universal representation that primarily captures dominant semantics, which may not always align with customized downstream tasks. For instance, in animal habitat analysis, researchers prioritize scene-related features, whereas universal embeddings emphasize categorical semantics, leading to suboptimal results. As a solution, existing approaches resort to supervised fine-tuning, which however incurs high computational and annotation costs. In this paper, we propose Conditional Representation Learning (CRL), aiming to extract representations tailored to arbitrary user-specified criteria. Specifically, we reveal that the semantics of a space are determined by its basis, thereby enabling a set of descriptive words to approximate the basis for a customized feature space. Building upon this insight, given a user-specified criterion, CRL first employs a large language model (LLM) to generate descriptive texts to construct the semantic basis, then projects the image representation into this conditional feature space leveraging a vision-language model (VLM). The conditional representation better captures semantics for the specific criterion, which could be utilized for multiple customized tasks. Extensive experiments on classification and retrieval tasks demonstrate the superiority and generality of the proposed CRL. The code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dGi2d5yDs4": {
    "title": "Angular Steering: Behavior Control via Rotation in Activation Space",
    "volume": "spotlight",
    "abstract": "Controlling specific behaviors in large language models while preserving their general capabilities is a central challenge for safe and reliable artificial intelligence deployment. Current steering methods, such as vector addition and directional ablation, are constrained within a two-dimensional subspace defined by the activation and feature direction, making them sensitive to chosen parameters and potentially affecting unrelated features due to unintended interactions in activation space. We introduce Angular Steering, a novel and flexible method for behavior modulation that operates by rotating activations within a fixed two-dimensional subspace. By formulating steering as a geometric rotation toward or away from a target behavior direction, Angular Steering provides continuous, fine-grained control over behaviors such as refusal and compliance. We demonstrate this method using refusal steering emotion steering as use cases. Additionally, we propose Adaptive Angular Steering, a selective variant that rotates only activations aligned with the target feature, further enhancing stability and coherence. Angular Steering generalizes existing addition and orthogonalization techniques under a unified geometric rotation framework, simplifying parameter selection and maintaining model stability across a broader range of adjustments. Experiments across multiple model families and sizes show that Angular Steering achieves robust behavioral control while maintaining general language modeling performance, underscoring its flexibility, generalization, and robustness compared to prior approaches. Code and artifacts are available at \\url{https://github.com/lone17/angular-steering/}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wOSZVnYH5w": {
    "title": "COOPERA: Continual Open-Ended Human-Robot Assistance",
    "volume": "spotlight",
    "abstract": "To understand and collaborate with humans, robots must account for individual human traits, habits, and activities over time. However, most robotic assistants lack these abilities, as they primarily focus on predefined tasks in structured environments and lack a human model to learn from. This work introduces COOPERA, a novel framework for COntinual, OPen-Ended human-Robot Assistance, where simulated humans, driven by psychological traits and long-term intentions, interact with robots in complex environments. By integrating continuous human feedback, our framework, for the first time, enables the study of long-term, open-ended human-robot collaboration (HRC) in different collaborative tasks across various time-scales. Within COOPERA, we introduce a benchmark and an approach to personalize the robot's collaborative actions by learning human traits and context-dependent intents. Experiments validate the extent to which our simulated humans reflect realistic human behaviors and demonstrate the value of inferring and personalizing to human intents for open-ended and long-term HRC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VanvIu0KZU": {
    "title": "Joint Hierarchical Representation Learning of Samples and Features via Informed Tree-Wasserstein Distance",
    "volume": "spotlight",
    "abstract": "High-dimensional data often exhibit hierarchical structures in both modes: samples and features. Yet, most existing approaches for hierarchical representation learning consider only one mode at a time. In this work, we propose an unsupervised method for jointly learning hierarchical representations of samples and features via Tree-Wasserstein Distance (TWD). Our method alternates between the two data modes. It first constructs a tree for one mode, then computes a TWD for the other mode based on that tree, and finally uses the resulting TWD to build the second mode's tree. By repeatedly alternating through these steps, the method gradually refines both trees and the corresponding TWDs, capturing meaningful hierarchical representations of the data. We provide a theoretical analysis showing that our method converges. We show that our method can be integrated into hyperbolic graph convolutional networks as a pre-processing technique, improving performance in link prediction and node classification tasks. In addition, our method outperforms baselines in sparse approximation and unsupervised Wasserstein distance learning tasks on word-document and single-cell RNA-sequencing datasets",
    "checked": true,
    "id": "61fb71dbdb9bf36643870508bf5b5a85f7115b82",
    "semantic_title": "joint hierarchical representation learning of samples and features via informed tree-wasserstein distance",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=27xTIAFbc6": {
    "title": "Detecting Generated Images by Fitting Natural Image Distributions",
    "volume": "spotlight",
    "abstract": "The increasing realism of generated images has raised significant concerns about their potential misuse, necessitating robust detection methods. Current approaches mainly rely on training binary classifiers, which depend heavily on the quantity and quality of available generated images. In this work, we propose a novel framework that exploits geometric differences between the data manifolds of natural and generated images. To exploit this difference, we employ a pair of functions engineered to yield consistent outputs for natural images but divergent outputs for generated ones, leveraging the property that their gradients reside in mutually orthogonal subspaces. This design enables a simple yet effective detection method: an image is identified as generated if a transformation along its data manifold induces a significant change in the loss value of a self-supervised model pre-trained on natural images. Further more, to address diminishing manifold disparities in advanced generative models, we leverage normalizing flows to amplify detectable differences by extruding generated images away from the natural image manifold. Extensive experiments demonstrate the efficacy of this method",
    "checked": true,
    "id": "e32e144345c42bab7af15e777c5e123c6b3a6494",
    "semantic_title": "detecting generated images by fitting natural image distributions",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=K3xaVpSHkV": {
    "title": "Agnostic Learning under Targeted Poisoning: Optimal Rates and the Role of Randomness",
    "volume": "spotlight",
    "abstract": "We study the problem of learning in the presence of an adversary that can corrupt an $\\eta$ fraction of the training examples with the goal of causing failure on a specific test point. In the realizable setting, prior work established that the optimal error under such instance-targeted poisoning attacks scales as $\\Theta(d\\eta)$, where $d$ is the VC dimension of the hypothesis class [Hanneke, Karbasi, Mahmoody, Mehalel, and Moran (NeurIPS 2022)]. In this work, we resolve the corresponding question in the agnostic setting. We show that the optimal excess error is $\\widetilde\\Theta(\\sqrt{d\\eta})$, answering one of the main open problems left by Hanneke et al. To achieve this rate, it is necessary to use randomized learners: Hanneke et al.\\ showed that deterministic learners can be forced to suffer error close to $1$ even under small amounts of poisoning. Perhaps surprisingly, our upper bound remains valid even when the learner's random bits are fully visible to the adversary. In the other direction, our lower bound is stronger than standard PAC-style bounds: instead of tailoring a hard distribution separately for each sample size, we exhibit a single fixed distribution under which the adversary can enforce an excess error of $\\Omega(\\sqrt{d\\eta})$ infinitely often",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y7ahj9RoXQ": {
    "title": "ORIGAMISPACE: Benchmarking Multimodal LLMs in Multi-Step Spatial Reasoning with Mathematical Constraints",
    "volume": "spotlight",
    "abstract": "Spatial reasoning is a key capability in the field of artificial intelligence, especially crucial in areas such as robotics, computer vision, and natural language understanding. However, evaluating the ability of multimodal large language models (MLLMs) in complex spatial reasoning still faces challenges, particularly in scenarios requiring multi-step reasoning and precise mathematical constraints. This paper introduces ORIGAMISPACE, a new dataset and benchmark designed to evaluate the multi-step spatial reasoning ability and the capacity to handle mathematical constraints of MLLMs through origami tasks. The dataset contains 350 data instances, each comprising a strictly formatted crease pattern (CP diagram), the Compiled Flat Pattern, the complete Folding Process, and the final Folded Shape Image. We propose four evaluation tasks: Pattern Prediction, Multi-step Spatial Reasoning, Spatial Relationship Prediction, and End-to-End CP Code Generation. For the CP code generation task, we design an interactive environment and explore the possibility of using reinforcement learning methods to train MLLMs. Through experiments on existing MLLMs, we initially reveal the strengths and weaknesses of these models in handling complex spatial reasoning tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A4Xx9irvpp": {
    "title": "Establishing Linear Surrogate Regret Bounds for Convex Smooth Losses via Convolutional Fenchel–Young Losses",
    "volume": "spotlight",
    "abstract": "Surrogate regret bounds, also known as excess risk bounds, bridge the gap between the convergence rates of surrogate and target losses. The regret transfer is lossless if the surrogate regret bound is linear. While convex smooth surrogate losses are appealing in particular due to the efficient estimation and optimization, the existence of a trade-off between the loss smoothness and linear regret bound has been believed in the community. Under this scenario, the better optimization and estimation properties of convex smooth surrogate losses may inevitably deteriorate after undergoing the regret transfer onto a target loss. We overcome this dilemma for arbitrary discrete target losses by constructing a convex smooth surrogate loss, which entails a linear surrogate regret bound composed with a tailored prediction link. The construction is based on Fenchel--Young losses generated by the *convolutional negentropy*, which are equivalent to the infimal convolution of a generalized negentropy and the target Bayes risk. Consequently, the infimal convolution enables us to derive a smooth loss while maintaining the surrogate regret bound linear. We additionally benefit from the infimal convolution to have a consistent estimator of the underlying class probability. Our results are overall a novel demonstration of how convex analysis penetrates into optimization and statistical efficiency in risk minimization",
    "checked": false,
    "id": "a23160cea16de063b6dda3c2c5c693a0892d0e86",
    "semantic_title": "establishing linear surrogate regret bounds for convex smooth losses via convolutional fenchel-young losses",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=aDa0xEFDu1": {
    "title": "Co-Reinforcement Learning for Unified Multimodal Understanding and Generation",
    "volume": "spotlight",
    "abstract": "This paper presents a pioneering exploration of reinforcement learning (RL) via group relative policy optimization for unified multimodal large language models (ULMs), aimed at simultaneously reinforcing generation and understanding capabilities. Through systematic pilot studies, we uncover the significant potential of ULMs to enable the synergistic co-evolution of dual capabilities within a shared policy optimization framework. Building on this insight, we introduce \\textbf{CoRL}, a \\textbf{Co}-\\textbf{R}einforcement \\textbf{L}earning framework comprising a unified RL stage for joint optimization and a refined RL stage for task-specific enhancement. With the proposed CoRL, our resulting model, \\textbf{ULM-R1}, achieves average improvements of 7\\% on three text-to-image generation datasets and 23\\% on nine multimodal understanding benchmarks. These results demonstrate the effectiveness of CoRL and highlight the substantial benefits of reinforcement learning in facilitating cross-task synergy and optimization for ULMs. Code is available at \\url{https://github.com/mm-vl/ULM-R1}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HKDyRDzy1E": {
    "title": "Structured Linear CDEs: Maximally Expressive and Parallel-in-Time Sequence Models",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5xPvWat3IX": {
    "title": "Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MiPIjE5onj": {
    "title": "Privacy amplification by random allocation",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "2545acc0e2def1a5861219aab3b262d6a1d4bdab",
    "semantic_title": "privacy amplification by random allocation",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=ThgoX1dMeM": {
    "title": "Tackling Biased Evaluators in Dueling Bandits",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": "13adbee696c755558253e2dd1c45f3036ba82bd3",
    "semantic_title": "biased dueling bandits with stochastic delayed feedback",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=6FHvr5hJdd": {
    "title": "Advanced Sign Language Video Generation with Compressed and Quantized Multi-Condition Tokenization",
    "volume": "spotlight",
    "abstract": "Sign Language Video Generation (SLVG) seeks to generate identity-preserving sign language videos from spoken language texts. Existing methods primarily rely on the single coarse condition (e.g., skeleton sequences) as the intermediary to bridge the translation model and the video generation model, which limits both the naturalness and expressiveness of the generated videos. To overcome these limitations, we propose SignViP, a novel SLVG framework that incorporate multiple fine-grained conditions for improved generation fidelity. Rather than directly translating error-prone high-dimensional conditions, SignViP adopts a discrete tokenization paradigm to integrate and represent fine-grained conditions (i.e., fine-grained poses and 3D hands). SignViP contains three core components. (1) Sign Video Diffusion Model is jointly trained with a multi-condition encoder to learn continuous embeddings that encapsulate fine-grained motion and appearance. (2) Finite Scalar Quantization (FSQ) Autoencoder is further trained to compress and quantize these embeddings into discrete tokens for compact representation of the conditions. (3) Multi-Condition Token Translator is trained to translate spoken language text to discrete multi-condition tokens. During inference, Multi-Condition Token Translator first translates the spoken language text into discrete multi-condition tokens. These tokens are then decoded to continuous embeddings by FSQ Autoencoder, which are subsequently injected into Sign Video Diffusion Model to guide video generation. Experimental results show that SignViP achieves state-of-the-art performance across metrics, including video quality, temporal coherence, and semantic fidelity. The code is available at https://github.com/umnooob/signvip/",
    "checked": true,
    "id": "42081fa0362909c848343117c4d71e3ad7f41e81",
    "semantic_title": "advanced sign language video generation with compressed and quantized multi-condition tokenization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=JIpKkzSqly": {
    "title": "Transstratal Adversarial Attack: Compromising Multi-Layered Defenses in Text-to-Image Models",
    "volume": "spotlight",
    "abstract": "Modern Text-to-Image (T2I) models deploy multi-layered defenses to block Not-Safe-For-Work (NSFW) content generation. These defenses typically include sequential layers such as prompt filters, concept erasers and image filters. While existing adversarial attacks have demonstrated vulnerabilities in isolated defense layers, they prove largely ineffective against multi-layered defenses deployed in real-world T2I systems. In this paper, we demonstrate that exploiting overlapping vulnerabilities across these distinct defense layers enables adversaries to systematically bypass the entire safeguard of T2I systems. We propose Transstratal Adversarial Attack (TAA, a novel black-box framework to compromise T2I models with multi-layered protection. It generates transstratal adversarial prompts to evade all defense layers simultaneously. This is accomplished through transstratal adversarial candidate generation using LLMs to fulfill implicit and subjective adversarial requirements against different defense layers, combined with adversarial genetic optimization for efficient black-box search to maximize the bypass rates and generated image harmfulness. Evaluated across 14 T2I models (e.g., Stable Diffusion, DALL·E, and Midjourney) and 17 safety modules, our attack achieves an average attack success rate of 85.6\\%, surpassing state-of-the-art methods by 73.5\\%. Our findings challenge the isolated design of safety mechanisms and establish the first benchmark for holistic robustness evaluation in multi-layered safeguarded T2I models. The code can be found in https://github.com/Bluedask/TAA-T2I",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gibq7Wa7Bq": {
    "title": "SORTeD Rashomon Sets of Sparse Decision Trees: Anytime Enumeration",
    "volume": "spotlight",
    "abstract": "Sparse decision tree learning provides accurate and interpretable predictive models that are ideal for high-stakes applications by finding the single most accurate tree within a (soft) size limit. Rather than relying on a single \"best\" tree, Rashomon sets—trees with similar performance but varying structures—can be used to enhance variable importance analysis, enrich explanations, and enable users to choose simpler trees or those that satisfy stakeholder preferences (e.g., fairness) without hard-coding such criteria into the objective function. However, because finding the optimal tree is NP-hard, enumerating the Rashomon set is inherently challenging. Therefore, we introduce SORTD, a novel framework that improves scalability and enumerates trees in the Rashomon set in order of the objective value, thus offering anytime behavior. Our experiments show that SORTD reduces runtime by up to two orders of magnitude compared with the state of the art. Moreover, SORTD can compute Rashomon sets for any separable and totally ordered objective and supports post-evaluating the set using other separable (and partially ordered) objectives. Together, these advances make exploring Rashomon sets more practical in real-world applications",
    "checked": true,
    "id": "144cc108f0072f78c67fa97a17972d3ac5da894d",
    "semantic_title": "sorted rashomon sets of sparse decision trees: anytime enumeration",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EyFrTjaYU3": {
    "title": "TGA: True-to-Geometry Avatar Dynamic Reconstruction",
    "volume": "spotlight",
    "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have improved the visual fidelity of dynamic avatar reconstruction. However, existing methods often overlook the inherent chromatic similarity of human skin tones, leading to poor capture of intricate facial geometry under subtle appearance changes. This is caused by the affine approximation of Gaussian projection, which fails to be perspective-aware to depth-induced shear effects. To this end, we propose True-to-Geometry Avatar Dynamic Reconstruction (TGA), a perspective-aware 4D Gaussian avatar framework that sensitively captures fine-grained facial variations for accurate 3D geometry reconstruction. Specifically, to enable color-sensitive and geometry-consistent Gaussian representations under dynamic conditions, we introduce Perspective-Aware Gaussian Transformation that jointly models temporal deformations and spatial projection by integrating Jacobian-guided adaptive deformation into the homogeneous formulation. Furthermore, we develop Incremental BVH Tree Pivoting to enable fast frame-by-frame mesh extraction for 4D Gaussian representations. A dynamic Gaussian Bounding Volume Hierarchy (BVH) tree is used to model the topological relationships among points, where active ones are filtered out by BVH pivoting and subsequently re-triangulated for surface reconstruction. Extensive experiments demonstrate that TGA achieves superior geometric accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nKuFQhKZtt": {
    "title": "Minimax Adaptive Online Nonparametric Regression over Besov spaces",
    "volume": "spotlight",
    "abstract": "We study online adversarial regression with convex losses against a rich class of continuous yet highly irregular competitor functions,% prediction rules, modeled by Besov spaces $B_{pq}^s$ with general parameters $1 \\leq p,q \\leq \\infty$ and smoothness $s > \\tfrac{d}{p}$. We introduce an adaptive wavelet-based algorithm that performs sequential prediction without prior knowledge of $(s,p,q)$, and establish minimax-optimal regret bounds against any comparator in $B_{pq}^s$. We further design a locally adaptive extension capable of sequentially adapting to spatially inhomogeneous smoothness. This adaptive mechanism adjusts the resolution of the predictions over both time and space, yielding refined regret bounds in terms of local regularity. Consequently, in heterogeneous environments, our adaptive guarantees can significantly surpass those obtained by standard global methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=APXcX7z1Bi": {
    "title": "UniSite: The First Cross-Structure Dataset and Learning Framework for End-to-End Ligand Binding Site Detection",
    "volume": "spotlight",
    "abstract": "The detection of ligand binding sites for proteins is a fundamental step in Structure-Based Drug Design. Despite notable advances in recent years, existing methods, datasets, and evaluation metrics are confronted with several key challenges: (1) current datasets and methods are centered on individual protein–ligand complexes and neglect that diverse binding sites may exist across multiple complexes of the same protein, introducing significant statistical bias; (2) ligand binding site detection is typically modeled as a discontinuous workflow, employing binary segmentation and subsequent clustering algorithms; (3) traditional evaluation metrics do not adequately reflect the actual performance of different binding site prediction methods. To address these issues, we first introduce UniSite-DS, the first UniProt (Unique Protein)-centric ligand binding site dataset, which contains 4.81 times more multi-site data and 2.08 times more overall data compared to the previously most widely used datasets. We then propose UniSite, the first end-to-end ligand binding site detection framework supervised by set prediction loss with bijective matching. In addition, we introduce Average Precision based on Intersection over Union (IoU) as a more accurate evaluation metric for ligand binding site prediction. Extensive experiments on UniSite-DS and several representative benchmark datasets demonstrate that IoU-based Average Precision provides a more accurate reflection of prediction quality, and that UniSite outperforms current state-of-the-art methods in ligand binding site detection. The dataset and codes will be made publicly available at https://github.com/quanlin-wu/unisite",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g3EF5XsapH": {
    "title": "URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model",
    "volume": "spotlight",
    "abstract": "Constructing accurate digital twins of articulated objects is essential for robotic simulation training and embodied AI world model building, yet historically requires painstaking manual modeling or multi-stage pipelines. In this work, we propose \\textbf{URDF-Anything}, an end-to-end automatic reconstruction framework based on a 3D multimodal large language model (MLLM). URDF-Anything utilizes an autoregressive prediction framework based on point-cloud and text multimodal input to jointly optimize geometric segmentation and kinematic parameter prediction. It implements a specialized [SEG] token mechanism that interacts directly with point cloud features, enabling fine-grained part-level segmentation while maintaining consistency with the kinematic parameter predictions. Experiments on both simulated and real-world datasets demonstrate that our method significantly outperforms existing approaches regarding geometric segmentation (mIoU 17\\% improvement), kinematic parameter prediction (average error reduction of 29\\%), and physical executability (surpassing baselines by 50\\%). Notably, our method exhibits excellent generalization ability, performing well even on objects outside the training set. This work provides an efficient solution for constructing digital twins for robotic simulation, significantly enhancing the sim-to-real transfer capability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PoIhCjqzn0": {
    "title": "L2DGCN: Learnable Enhancement and Label Selection Dynamic Graph Convolutional Networks for Mitigating Degree Bias",
    "volume": "spotlight",
    "abstract": "Graph Neural Networks (GNNs) are powerful models for node classification, but their performance is heavily reliant on manually labeled data, which is often costly and results in insufficient labeling. Recent studies have shown that message-passing neural networks struggle to propagate information in low-degree nodes, negatively affecting overall performance. To address the information bias caused by degree imbalance, we propose a Learnable Enhancement and Label Selection Dynamic Graph Convolutional Network (L2DGCN). L2DGCN consists of a teacher model and a student model. The teacher model employs an improved label propagation mechanism that enables remote label information dissemination among all nodes. The student model introduces a dynamically learnable graph enhancement strategy, perturbing edges to facilitate information exchange among low-degree nodes. This approach maintains the global graph structure while learning graph representations. Additionally, we have designed a label selector to mitigate the impact of unreliable pseudo-labels on model learning. To validate the effectiveness of our proposed model with limited labeled data, we conducted comprehensive evaluations of semi-supervised node classification across various scenarios with a limited number of annotated nodes. Experimental results demonstrate that our data enhancement model significantly contributes to node classification tasks under sparse labeling conditions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9eIntNc69t": {
    "title": "Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "To enable embodied agents to operate effectively over extended timeframes, it is crucial to develop models that form and access memories to stay contextualized in their environment. In the current paradigm of training transformer-based policies for embodied sequential decision-making tasks, visual inputs often overwhelm the context limits of transformers, while humans can maintain and utilize a lifetime of experience compressed as memories. Significant compression is possible in principle, as much of the input is irrelevant and can be abstracted. However, existing approaches predominantly focus on either recurrent models with fixed-size memory or transformers with full-context reliance. In this work, we propose Memo, a transformer-based architecture and training recipe for reinforcement learning (RL) on memory-intensive, long-horizon tasks. Memo incorporates the creation and retrieval of memory by interleaving periodic summarization tokens with the inputs of a model during training. We demonstrate Memo's effectiveness on a grid-world meta-RL benchmark and a multi-object navigation task in photo-realistic indoor settings. Memo outperforms naive long-context transformer baselines while being more compute and storage efficient. Additionally, Memo generalizes better to longer contexts at inference time and remains robust in streaming settings, where historical context must be truncated to fit inference constraints",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4p28lkk44b": {
    "title": "Achilles' Heel of Mamba: Essential difficulties of the Mamba architecture demonstrated by synthetic data",
    "volume": "spotlight",
    "abstract": "State Space Models (SSMs) have emerged as promising alternatives to attention mechanisms, with the Mamba architecture demonstrating impressive performance and linear complexity for processing long sequences. However, the fundamental differences between Mamba and Transformer architectures remain incompletely understood. In this work, we use carefully designed synthetic tasks to reveal Mamba's inherent limitations. Through experiments, we identify that Mamba's nonlinear convolution introduces an asymmetry bias that significantly impairs its ability to recognize symmetrical patterns and relationships. Using composite function and inverse sequence matching tasks, we demonstrate that Mamba strongly favors compositional solutions over symmetrical ones and struggles with tasks requiring the matching of reversed sequences. We show these limitations stem not from the SSM module itself but from the nonlinear convolution preceding it, which fuses token information asymmetrically. These insights provide a new understanding of Mamba's constraints and suggest concrete architectural improvements for future sequence models",
    "checked": true,
    "id": "bbe49e46bee1f5b4ab8ae1bdaa3f29aece6a6a15",
    "semantic_title": "achilles' heel of mamba: essential difficulties of the mamba architecture demonstrated by synthetic data",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QwXpn5IPKk": {
    "title": "RepLDM: Reprogramming Pretrained Latent Diffusion Models for High-Quality, High-Efficiency, High-Resolution Image Generation",
    "volume": "spotlight",
    "abstract": "While latent diffusion models (LDMs), such as Stable Diffusion, are designed for high-resolution image generation, they often struggle with significant structural distortions when generating images at resolutions higher than their training one. Instead of relying on extensive retraining, a more resource-efficient approach is to reprogram the pretrained model for high-resolution (HR) image generation; however, existing methods often result in poor image quality and long inference time. We introduce RepLDM, a novel reprogramming framework for pretrained LDMs that enables high-quality, high-efficiency, high-resolution image generation; see Fig. 1. RepLDM consists of two stages: (i) an attention guidance stage, which generates a latent representation of a higher-quality training-resolution image using a novel parameter-free self-attention mechanism to enhance the structural consistency; and (ii) a progressive upsampling stage, which progressively performs upsampling in pixel space to mitigate the severe artifacts caused by latent space upsampling. The effective initialization from the first stage allows for denoising at higher resolutions with significantly fewer steps, improving the efficiency. Extensive experimental results demonstrate that RepLDM significantly outperforms state-of-the-art methods in both quality and efficiency for HR image generation, underscoring its advantages for real-world applications. Codes: https://github.com/kmittle/RepLDM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=853SwC2dMZ": {
    "title": "Understanding LLM Behaviors via Compression: Data Generation, Knowledge Acquisition and Scaling Laws",
    "volume": "spotlight",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across numerous tasks, yet principled explanations for their underlying mechanisms and several phenomena, such as scaling laws, hallucinations, and related behaviors, remain elusive. In this work, we revisit the classical relationship between compression and prediction, grounded in Kolmogorov complexity and Shannon information theory, to provide deeper insights into LLM behaviors. By leveraging the Kolmogorov Structure Function and interpreting LLM compression as a two-part coding process, we offer a detailed view of how LLMs acquire and store information across increasing model and data scales -- from pervasive syntactic patterns to progressively rarer knowledge elements. Motivated by this theoretical perspective and natural assumptions inspired by Heap's and Zipf's laws, we introduce a simplified yet representative hierarchical data-generation framework called the Syntax-Knowledge model. Under the Bayesian setting, we show that prediction and compression within this model naturally lead to diverse learning and scaling behaviors of LLMs. In particular, our theoretical analysis offers intuitive and principled explanations for both data and model scaling laws, the dynamics of knowledge acquisition during training and fine-tuning, factual knowledge hallucinations in LLMs. The experimental results validate our theoretical predictions",
    "checked": true,
    "id": "4f7331444b8841ff6271ac12747d96f78dc1c6dc",
    "semantic_title": "understanding llm behaviors via compression: data generation, knowledge acquisition and scaling laws",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=y8VWYf5cVI": {
    "title": "Differentiable Hierarchical Visual Tokenization",
    "volume": "spotlight",
    "abstract": "Vision Transformers rely on fixed patch tokens that ignore the spatial and semantic structure of images. In this work, we introduce an end-to-end differentiable tokenizer that adapts to image content with pixel-level granularity while remaining backward-compatible with existing architectures for retrofitting pretrained models. Our method uses hierarchical model selection with information criteria to provide competitive performance in both image-level classification and dense-prediction tasks, and even supports out-of-the-box raster-to-vector conversion",
    "checked": true,
    "id": "d81d04c1817a9d7d5ea4b2bf99d27a4d0138f4f1",
    "semantic_title": "differentiable hierarchical visual tokenization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QtnCPZMxYg": {
    "title": "Trajectory Graph Learning: Aligning with Long Trajectories in Reinforcement Learning Without Reward Design",
    "volume": "spotlight",
    "abstract": "Reinforcement learning (RL) often relies on manually designed reward functions, which are difficult to specify and can lead to issues such as reward hacking and suboptimal behavior. Alternatives like inverse RL and preference-based RL attempt to infer surrogate rewards from demonstrations or preferences but suffer from ambiguity and distribution mismatch. A more direct approach, inspired by imitation learning, avoids reward modeling by leveraging expert demonstrations. However, most existing methods align actions only at individual states, failing to capture the coherence of long-horizon trajectories. In this work, we study the problem of directly aligning policies with expert-labeled trajectories to preserve long-horizon behavior without relying on reward signals. Specifically, we aim to learn a policy that maximizes the probability of generating the expert trajectories. Nevertheless, we prove that, in its general form, this trajectory alignment problem is NP-complete. To address this, we propose Trajectory Graph Learning (TGL), a framework that leverages structural assumptions commonly satisfied in practice—such as bounded realizability of expert trajectories or a tree-structured MDP. These enable a graph-based policy planning algorithm that computes optimal policies in polynomial time under known dynamics. For settings with unknown dynamics, we develop a sample-efficient algorithm based on UCB-style exploration and establish sub-linear regret. Experiments on grid-world tasks demonstrate that TGL substantially outperforms standard imitation learning methods for long-trajectory planning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aYRNINhNGV": {
    "title": "Meta CLIP 2: A Worldwide Scaling Recipe",
    "volume": "spotlight",
    "abstract": "Contrastive Language-Image Pretraining (CLIP) is a popular foundation model, supporting from zero-shot classification, retrieval to encoders for multimodal large language models (MLLMs). Although CLIP is successfully trained on billion-scale image-text pairs from the English world, scaling CLIP's training further to learning from the worldwide web data is still challenging: (1) no curation method is available to handle data points from non-English world; (2) the English performance from existing multilingual CLIP is worse than its English-only counterpart, i.e., \"curse of multilinguality\" that is common in LLMs. Here, we present Meta CLIP 2, the first recipe training CLIP from scratch on worldwide web-scale image-text pairs. To generalize our findings, we conduct rigorous ablations with minimal changes that are necessary to address the above challenges and present a recipe enabling mutual benefits from English and non-English world data. In zero-shot ImageNet classification, Meta CLIP 2 ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%, and surprisingly sets new state-of-the-art without system-level confounding factors (e.g., translation, bespoke architecture changes) on multilingual benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with 64.3% on image-to-text retrieval. Code and model are available at https://github.com/facebookresearch/MetaCLIP",
    "checked": true,
    "id": "163e66c5e2b2cf47a4960abaaef3fd1d52a339c6",
    "semantic_title": "meta clip 2: a worldwide scaling recipe",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=xmYT1JqVpj": {
    "title": "Policy Compatible Skill Incremental Learning via Lazy Learning Interface",
    "volume": "spotlight",
    "abstract": "Skill Incremental Learning (SIL) is the process by which an embodied agent expands and refines its skill set over time by leveraging experience gained through interaction with its environment or by the integration of additional data. SIL facilitates efficient acquisition of hierarchical policies grounded in reusable skills for downstream tasks. However, as the skill repertoire evolves, it can disrupt compatibility with existing skill-based policies, limiting their reusability and generalization. In this work, we propose SIL-C, a novel framework that ensures skill-policy compatibility, allowing improvements in incrementally learned skills to enhance the performance of downstream policies without requiring policy re-training or structural adaptation. SIL-C employs a bilateral lazy learning-based mapping technique to dynamically align the subtask space referenced by policies with the skill space decoded into agent behaviors. This enables each subtask, derived from the policy's decomposition of a complex task, to be executed by selecting an appropriate skill based on trajectory distribution similarity. We evaluate SIL-C across diverse SIL scenarios and demonstrate that it maintains compatibility between evolving skills and downstream policies while ensuring efficiency throughout the learning process",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lNPo3FAMsl": {
    "title": "Estimating cognitive biases with attention-aware inverse planning",
    "volume": "spotlight",
    "abstract": "People's goal-directed behaviors are influenced by their cognitive biases, and autonomous systems that interact with people should be aware of this. For example, people's attention to objects in their environment will be biased in a way that systematically affects how they perform everyday tasks such as driving to work. Here, building on recent work in computational cognitive science, we formally articulate the \\textit{attention-aware inverse planning problem}, in which the goal is to estimate a person's attentional biases from their actions. We demonstrate how attention-aware inverse planning systematically differs from standard inverse reinforcement learning and how cognitive biases can be inferred from behavior. Finally, we present an approach to attention-aware inverse planning that combines deep reinforcement learning with computational cognitive modeling. We use this approach to infer the attentional strategies of RL agents in real-life driving scenarios selected from the Waymo Open Dataset, demonstrating the scalability of estimating cognitive biases with attention-aware inverse planning",
    "checked": true,
    "id": "41fd1a8168b012697aeff5d7194c093cb361725b",
    "semantic_title": "estimating cognitive biases with attention-aware inverse planning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VymXLPX6Ps": {
    "title": "An Efficient Orlicz-Sobolev Approach for Transporting Unbalanced Measures on a Graph",
    "volume": "spotlight",
    "abstract": "We investigate optimal transport (OT) for measures on graph metric spaces with different total masses. To mitigate the limitations of traditional $L^p$ geometry, Orlicz-Wasserstein (OW) and generalized Sobolev transport (GST) employ \\emph{Orlicz geometric structure}, leveraging convex functions to capture nuanced geometric relationships and remarkably contribute to advance certain machine learning approaches. However, both OW and GST are restricted to measures with equal total mass, limiting their applicability to real-world scenarios where mass variation is common, and input measures may have noisy supports, or outliers. To address unbalanced measures, OW can either incorporate mass constraints or marginal discrepancy penalization, but this leads to a more complex two-level optimization problem. Additionally, GST provides a scalable yet rigid framework, which poses significant challenges to extend GST to accommodate nonnegative measures. To tackle these challenges, in this work we revisit the entropy partial transport (EPT) problem. By exploiting Caffarelli \\& McCann's insights, we develop a novel variant of EPT endowed with Orlicz geometric structure, called \\emph{Orlicz-EPT}. We establish theoretical background to solve Orlicz-EPT using a binary search algorithmic approach. Especially, by leveraging the dual EPT and the underlying graph structure, we formulate a novel regularization approach that leads to the proposed \\emph{Orlicz-Sobolev transport} (OST). Notably, we demonstrate that OST can be efficiently computed by simply solving a univariate optimization problem, in stark contrast to the intensive computation needed for Orlicz-EPT. Building on this, we derive geometric structures for OST and draw its connections to other transport distances. We empirically illustrate that OST is several-order faster than Orlicz-EPT. Furthermore, we show preliminary evidence on the advantages of OST for measures on a graph in document classification and topological data analysis",
    "checked": true,
    "id": "8f9bbb15ff8c52092ffe9c27d877bed9ecb01f10",
    "semantic_title": "an efficient orlicz-sobolev approach for transporting unbalanced measures on a graph",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CqLWckpTbG": {
    "title": "DeepDiver: Adaptive Web-Search Intensity Scaling via Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "Information seeking demands iterative evidence gathering and reflective reasoning, yet large language models (LLMs) still struggle with it in open-web question answering. Existing prompting and supervised fine-tuning (SFT) methods remain fixed by prompt rules or training corpora, and are usually benchmarked only on well-structured wiki sources, limiting real-world adaptability. We introduce $\\textbf{WebPuzzle}$, a 24k-sample training and 275-sample test benchmark that evaluates information seeking on the live internet, across both wiki and open-domain queries. Leveraging 7k WebPuzzle instances, we develop $\\textbf{DeepDiver}$, a reinforcement-learning (RL) framework that cultivates $\\textbf{Search Intensity Scaling (SIS)}$—an emergent ability to escalate search frequency and depth instead of settling on overconfident, under-evidenced answers. With SIS, Qwen2.5-7B-Instruct and Pangu-7B-Reasoner attain performance on real-web tasks comparable to the 671B-parameter DeepSeek-R1. We detail DeepDiver's curriculum from cold-start SFT to a well designed RL procedure, and show that its seeking policy generalized from closed-ended queries to open-ended generation such as long-form writing. Our results advance adaptive information seeking in LLMs and provide a rigorous benchmark for future work",
    "checked": false,
    "id": "4473f7186700c6fdc7b72bb798209e4aa32c09b5",
    "semantic_title": "deepdiver: adaptive search intensity scaling via open-web reinforcement learning",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=UXc87Orcri": {
    "title": "MetaGS: A Meta-Learned Gaussian-Phong Model for Out-of-Distribution 3D Scene Relighting",
    "volume": "spotlight",
    "abstract": "Out-of-distribution (OOD) 3D relighting requires novel view synthesis under unseen lighting conditions that differ significantly from the observed images. Existing relighting methods, which assume consistent light source distributions between training and testing, often degrade in OOD scenarios. We introduce **MetaGS** to tackle this challenge from two perspectives. First, we propose a meta-learning approach to train 3D Gaussian splatting, which explicitly promotes learning generalizable Gaussian geometries and appearance attributes across diverse lighting conditions, even with biased training data. Second, we embed fundamental physical priors from the *Blinn-Phong* reflection model into Gaussian splatting, which enhances the decoupling of shading components and leads to more accurate 3D scene reconstruction. Results on both synthetic and real-world datasets demonstrate the effectiveness of MetaGS in challenging OOD relighting tasks, supporting efficient point-light relighting and generalizing well to unseen environment lighting maps",
    "checked": true,
    "id": "a5d89bf553bd71395e3888514ddc41c541528740",
    "semantic_title": "metags: a meta-learned gaussian-phong model for out-of-distribution 3d scene relighting",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=4WQ5Qgpl2F": {
    "title": "Towards a Pairwise Ranking Model with Orderliness and Monotonicity for Label Enhancement",
    "volume": "spotlight",
    "abstract": "Label distribution in recent years has been applied in a diverse array of complex decision-making tasks. To address the availability of label distributions, label enhancement has been established as an effective learning paradigm that aims to automatically infer label distributions from readily available multi-label data, e.g., logical labels. Recently, numerous works have demonstrated that the label ranking is significantly beneficial to label enhancement. However, these works still exhibit deficiencies in representing the probabilistic relationships between label distribution and label rankings, or fail to accommodate scenarios where multiple labels are equally important for a given instance. Therefore, we propose PROM, a pairwise ranking model with orderliness and monotonicity, to explain the probabilistic relationship between label distributions and label rankings. Specifically, we propose the monotonicity and orderliness assumptions for the probabilities of different ranking relationships and derive the mass functions for PROM, which are theoretically ensured to preserve the monotonicity and orderliness. Further, we propose a generative label enhancement algorithm based on PROM, which directly learns a label distribution predictor from the readily available multi-label data. Finally, extensive experiments demonstrate the efficacy of our proposed model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aqpHTPC63N": {
    "title": "Spend Wisely: Maximizing Post-Training Gains in Iterative Synthetic Data Bootstrapping",
    "volume": "spotlight",
    "abstract": "Modern foundation models often undergo iterative ``bootstrapping'' in their post-training phase: a model generates synthetic data, an external verifier filters out low-quality samples, and the high-quality subset is used for further fine-tuning. Over multiple iterations, the model performance improves, raising a crucial question: How should the total budget for generation and training be allocated across iterations to maximize final performance? In this work, we develop a theoretical framework for analyzing budget allocation strategies. Specifically, we show that constant policies fail to converge with high probability, while increasing policies---particularly exponential growth policies---exhibit significant theoretical advantages. Experiments on image denoising with diffusion probabilistic models and math reasoning with large language models show that both exponential and polynomial growth policies consistently outperform constant policies, with exponential policies often providing more stable performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MI1uT5rReV": {
    "title": "Fast-Slow Thinking GRPO for Large Vision-Language Model Reasoning",
    "volume": "spotlight",
    "abstract": "When applying reinforcement learning—typically through GRPO—to large vision-language model reasoning struggles to effectively scale reasoning length or generates verbose outputs across all tasks with only marginal gains in accuracy. To address this issue, we present FAST-GRPO, a variant of GRPO that dynamically adapts reasoning depth based on question characteristics. Through empirical analysis, we establish the feasibility of fast-slow thinking in LVLMs by investigating how response length and data distribution affect performance. Inspired by these observations, we introduce two complementary metrics to estimate the difficulty of the questions, guiding the model to determine when fast or slow thinking is more appropriate. Next, we incorporate adaptive length-based rewards and difficulty-aware KL divergence into the GRPO algorithm. Experiments across seven reasoning benchmarks demonstrate that FAST achieves state-of-the-art accuracy with over 10% relative improvement compared to the base model, while reducing token usage by 32.7-67.3% compared to previous slow-thinking approaches, effectively balancing reasoning length and accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gAddPMjmUc": {
    "title": "Exploration via Feature Perturbation in Contextual Bandits",
    "volume": "spotlight",
    "abstract": "We propose *feature perturbation*, a simple yet effective exploration strategy for contextual bandits that injects randomness directly into feature inputs, instead of randomizing unknown parameters or adding noise to rewards. Remarkably, this algorithm achieves $\\widetilde{\\mathcal{O}}(d\\sqrt{T})$ worst-case regret bound for generalized linear contextual bandits, while avoiding the $\\widetilde{\\mathcal{O}}(d^{3/2}\\sqrt{T})$ regret typical of existing randomized bandit algorithms. Because our algorithm eschews parameter sampling, it is both computationally efficient and naturally extends to non-parametric or neural network models. We verify these advantages through empirical evaluations, demonstrating that feature perturbation not only surpasses existing methods but also unifies strong practical performance with the near-optimal regret guarantees",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K0FbK2GOGj": {
    "title": "Instance-Optimality for Private KL Distribution Estimation",
    "volume": "spotlight",
    "abstract": "We study the fundamental problem of estimating an unknown discrete distribution $p$ over $d$ symbols, given $n$ i.i.d. samples from the distribution. We are interested in minimizing the KL divergence between the true distribution and the algorithm's estimate. We first construct minimax optimal private estimators. Minimax optimality however fails to shed light on an algorithm's performance on individual (non-worst-case) instances $p$ and simple minimax-optimal DP estimators can have poor empirical performance on real distributions. We then study this problem from an instance-optimality viewpoint, where the algorithm's error on $p$ is compared to the minimum achievable estimation error over a small local neighborhood of $p$. Under natural notions of local neighborhood, we propose algorithms that achieve instance-optimality up to constant factors, with and without a differential privacy constraint. Our upper bounds rely on (private) variants of the Good-Turing estimator. Our lower bounds use additive local neighborhoods that more precisely captures the hardness of distribution estimation in KL divergence, compared to ones considered in prior works",
    "checked": true,
    "id": "7018bb324d4ba3c05579ddbed1e36b95ad9a0278",
    "semantic_title": "instance-optimality for private kl distribution estimation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bzlt5tPFT6": {
    "title": "DMWM: Dual-Mind World Model with Long-Term Imagination",
    "volume": "spotlight",
    "abstract": "Imagination in world models is crucial for enabling agents to learn long-horizon policy in a sample-efficient manner. Existing recurrent state-space model (RSSM)-based world models depend on single-step statistical inference to capture the environment dynamics, and, hence, they are unable to perform long-term imagination tasks due to the accumulation of prediction errors. Inspired by the dual-process theory of human cognition, we propose a novel dual-mind world model (DMWM) framework that integrates logical reasoning to enable imagination with logical consistency. DMWM is composed of two components: an RSSM-based System 1 (RSSM-S1) component that handles state transitions in an intuitive manner and a logic-integrated neural network-based System 2 (LINN-S2) component that guides the imagination process through hierarchical deep logical reasoning. The inter-system feedback mechanism is designed to ensure that the imagination process follows the logical rules of the real environment. The proposed framework is evaluated on benchmark tasks that require long-term planning from the DMControl suite and robotic environment. Extensive experimental results demonstrate that the proposed framework yields significant improvements in terms of logical coherence, trial efficiency, data efficiency and long-term imagination over the state-of-the-art world models",
    "checked": true,
    "id": "08271d5a06bae591751f85d578cd8365d968342b",
    "semantic_title": "dmwm: dual-mind world model with long-term imagination",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=akhhwQh6UV": {
    "title": "TimeWak: Temporal Chained-Hashing Watermark for Time Series Data",
    "volume": "spotlight",
    "abstract": "Synthetic time series generated by diffusion models enable sharing privacy-sensitive datasets, such as patients' functional MRI records. Key criteria for synthetic data include high data utility and traceability to verify the data source. Recent watermarking methods embed in homogeneous latent spaces, but state-of-the-art time series generators operate in data space, making latent-based watermarking incompatible. This creates the challenge of watermarking directly in data space while handling feature heterogeneity and temporal dependencies. We propose TimeWak, the first watermarking algorithm for multivariate time series diffusion models. To handle temporal dependence and spatial heterogeneity, TimeWak embeds a temporal chained-hashing watermark directly within the temporal-feature data space. The other unique feature is the $\\epsilon$-exact inversion, which addresses the non-uniform reconstruction error distribution across features from inverting the diffusion process to detect watermarks. We derive the error bound of inverting multivariate time series while preserving robust watermark detectability. We extensively evaluate TimeWak on its impact on synthetic data quality, watermark detectability, and robustness under various post-editing attacks, against five datasets and baselines of different temporal lengths. Our results show that TimeWak achieves improvements of 61.96% in context-FID score, and 8.44% in correlational scores against the strongest state-of-the-art baseline, while remaining consistently detectable",
    "checked": true,
    "id": "400fa44794ae9ce0ff564762054b1d4fabe73c68",
    "semantic_title": "timewak: temporal chained-hashing watermark for time series data",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8omLr8BtjL": {
    "title": "UFO: A Unified Approach to Fine-grained Visual Perception via Open-ended Language Interface",
    "volume": "spotlight",
    "abstract": "Generalist models have achieved remarkable success in both language and vision-language tasks, showcasing the potential of unified modeling. However, effectively integrating fine-grained perception tasks like detection and segmentation into these models remains a significant challenge. This is primarily because these tasks often rely heavily on task-specific designs and architectures that can complicate the modeling process. To address this challenge, we present UFO, a framework that unifies fine-grained visual perception tasks through an open-ended language interface. By transforming all perception targets into the language space, UFO unifies object-level detection, pixel-level segmentation, and image-level vision-language tasks into a single model. Additionally, we introduce a novel embedding retrieval approach that relies solely on the language interface to support segmentation tasks. Our framework bridges the gap between fine-grained perception and vision-language tasks, significantly simplifying architectural design and training strategies while achieving comparable or superior performance to methods with intricate task-specific designs. After multi-task training on five standard visual perception datasets, UFO outperforms the previous state-of-the-art generalist models by 12.3 mAP on COCO instance segmentation and 3.3 mIoU on ADE20K semantic segmentation. Furthermore, our method seamlessly integrates with existing MLLMs, effectively combining fine-grained perception capabilities with their advanced language abilities, thereby achieving superior performance on the challenging reasoning segmentation. Code and models are available at https://github.com/nnnth/UFO",
    "checked": true,
    "id": "6128feb6e4cfd89f32fff2aae75fbed987603c27",
    "semantic_title": "ufo: a unified approach to fine-grained visual perception via open-ended language interface",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=YFa7eULIeN": {
    "title": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video Diffusion Models",
    "volume": "spotlight",
    "abstract": "Direct Preference Optimization (DPO) has recently been applied as a post‑training technique for text-to-video diffusion models. To obtain training data, annotators are asked to provide preferences between two videos generated from independent noise. However, this approach prohibits fine-grained comparisons, and we point out that it biases the annotators towards low-motion clips as they often contain fewer visual artifacts. In this work, we introduce DenseDPO, a method that addresses these shortcomings by making three contributions. First, we create each video pair for DPO by denoising corrupted copies of a ground truth video. This results in aligned pairs with similar motion structures while differing in local details, effectively neutralizing the motion bias. Second, we leverage the resulting temporal alignment to label preferences on short segments rather than entire clips, yielding a denser and more precise learning signal. With only one‑third of the labeled data, DenseDPO greatly improves motion generation over vanilla DPO, while matching it in text alignment, visual quality, and temporal consistency. Finally, we show that DenseDPO unlocks automatic preference annotation using off-the-shelf Vision Language Models (VLMs): GPT accurately predicts segment-level preferences similar to task-specifically fine-tuned video reward models, and DenseDPO trained on these labels achieves performance close to using human labels",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A0M3apV5zI": {
    "title": "Precise Asymptotics and Refined Regret of Variance-Aware UCB",
    "volume": "spotlight",
    "abstract": "In this paper, we study the behavior of the Upper Confidence Bound-Variance (UCB-V) algorithm for the Multi-Armed Bandit (MAB) problems, a variant of the canonical Upper Confidence Bound (UCB) algorithm that incorporates variance estimates into its decision-making process. More precisely, we provide an asymptotic characterization of the arm-pulling rates for UCB-V, extending recent results for the canonical UCB in Kalvit and Zeevi (2021) and Khamaru and Zhang (2024). In an interesting contrast to the canonical UCB, our analysis reveals that the behavior of UCB-V can exhibit instability, meaning that the arm-pulling rates may not always be asymptotically deterministic. Besides the asymptotic characterization, we also provide non-asymptotic bounds for the arm-pulling rates in the high probability regime, offering insights into the regret analysis. As an application of this high probability result, we establish that UCB-V can achieve a more refined regret bound, previously unknown even for more complicate and advanced variance-aware online decision-making algorithms. A matching regret lower bound is also established, demonstrating the optimality of our result",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6AbM9UG4aD": {
    "title": "InfMasking: Unleashing Synergistic Information by Contrastive Multimodal Interactions",
    "volume": "spotlight",
    "abstract": "In multimodal representation learning, synergistic interactions between modalities not only provide complementary information but also create unique outcomes through specific interaction patterns that no single modality could achieve alone. Existing methods may struggle to effectively capture the full spectrum of synergistic information, leading to suboptimal performance in tasks where such interactions are critical. This is particularly problematic because synergistic information constitutes the fundamental value proposition of multimodal representation. To address this challenge, we introduce InfMasking, a contrastive synergistic information extraction method designed to enhance synergistic information through an Infinite Masking strategy. InfMasking stochastically occludes most features from each modality during fusion, preserving only partial information to create representations with varied synergistic patterns. Unmasked fused representations are then aligned with masked ones through mutual information maximization to encode comprehensive synergistic information. This infinite masking strategy enables capturing richer interactions by exposing the model to diverse partial modality combinations during training. As computing mutual information estimates with infinite masking is computationally prohibitive, we derive an InfMasking loss to approximate this calculation. Through controlled experiments, we demonstrate that InfMasking effectively enhances synergistic information between modalities. In evaluations on large-scale real-world datasets, InfMasking achieves state-of-the-art performance across seven benchmarks. Code is released at https://github.com/brightest66/InfMasking",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uvTea5Rfek": {
    "title": "Extracting task-relevant preserved dynamics from contrastive aligned neural recordings",
    "volume": "spotlight",
    "abstract": "Recent work indicates that low-dimensional dynamics of neural and behavioral data are often preserved across days and subjects. However, extracting these preserved dynamics remains challenging: high-dimensional neural population activity and the recorded neuron populations vary across recording sessions. While existing modeling tools can improve alignment between neural and behavioral data, they often operate on a per-subject basis or discretize behavior into categories, disrupting its natural continuity and failing to capture the underlying dynamics. We introduce $\\underline{\\text{C}}$ontrastive $\\underline{\\text{A}}$ligned $\\underline{\\text{N}}$eural $\\underline{\\text{D}}$$\\underline{\\text{Y}}$namics (CANDY), an end‑to‑end framework that aligns neural and behavioral data using rank-based contrastive learning, adapted for continuous behavioral variables, to project neural activity from different sessions onto a shared low-dimensional embedding space. CANDY fits a shared linear dynamical system to the aligned embeddings, enabling an interpretable model of the conserved temporal structure in the latent space. We validate CANDY on synthetic and real-world datasets spanning multiple species, behaviors, and recording modalities. Our results show that CANDY is able to learn aligned latent embeddings and preserved dynamics across neural recording sessions and subjects, and it achieves improved cross-session behavior decoding performance. We further show that the latent linear dynamical system generalizes to new sessions and subjects, achieving comparable or even superior behavior decoding performance to models trained from scratch. These advances enable robust cross‑session behavioral decoding and offer a path towards identifying shared neural dynamics that underlie behavior across individuals and recording conditions. The code and two-photon imaging data of striatal neural activity that we acquired here are available at https://github.com/schnitzer-lab/CANDY-public.git",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rtd6GoJcoT": {
    "title": "Orochi: Versatile Biomedical Image Processor",
    "volume": "spotlight",
    "abstract": "Deep learning has emerged as a pivotal tool for accelerating research in the life sciences, with the low-level processing of biomedical images (e.g., registration, fusion, restoration, super-resolution) being one of its most critical applications. Platforms such as ImageJ (Fiji) and napari have enabled the development of customized plugins for various models. However, these plugins are typically based on models that are limited to specific tasks and datasets, making them less practical for biologists. To address this challenge, we introduce **Orochi**, the first application-oriented, efficient, and versatile image processor designed to overcome these limitations. Orochi is pre-trained on patches/volumes extracted from the raw data of over 100 publicly available studies using our Random Multi-scale Sampling strategy. We further propose Task-related Joint-embedding Pre-Training (TJP), which employs biomedical task-related degradation for self-supervision rather than relying on Masked Image Modelling (MIM), which performs poorly in downstream tasks such as registration. To ensure computational efficiency, we leverage Mamba's linear computational complexity and construct Multi-head Hierarchy Mamba. Additionally, we provide a three-tier fine-tuning framework (Full, Normal, and Light) and demonstrate that Orochi achieves comparable or superior performance to current state-of-the-art specialist models, even with lightweight parameter-efficient options. We hope that our study contributes to the development of an all-in-one workflow, thereby relieving biologists from the overwhelming task of selecting among numerous models. Our pre-trained weights and code will be released",
    "checked": true,
    "id": "e6bbcc9450a651c9f77cdddfc6b2cfc43a3a449d",
    "semantic_title": "orochi: versatile biomedical image processor",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4jWuS5hye1": {
    "title": "WISA: World simulator assistant for physics-aware text-to-video generation",
    "volume": "spotlight",
    "abstract": "Recent advances in text-to-video (T2V) generation, exemplified by models such as Sora and Kling, have demonstrated strong potential for constructing world simulators. However, existing T2V models still struggle to understand abstract physical principles and to generate videos that faithfully obey physical laws. This limitation stems primarily from the lack of explicit physical guidance, caused by a significant gap between high-level physical concepts and the generative capabilities of current models. To address this challenge, we propose the **W**orld **S**imulator **A**ssistant (**WISA**), a novel framework designed to systematically decompose and integrate physical principles into T2V models. Specifically, WISA decomposes physical knowledge into three hierarchical levels: textual physical descriptions, qualitative physical categories, and quantitative physical properties. It then incorporates several carefully designed modules—such as Mixture-of-Physical-Experts Attention (MoPA) and a Physical Classifier—to effectively encode these attributes and enhance the model's adherence to physical laws during generation. In addition, most existing video datasets feature only weak or implicit representations of physical phenomena, limiting their utility for learning explicit physical principles. To bridge this gap, we present **WISA-80K**, a new dataset comprising 80,000 human-curated videos that depict 17 fundamental physical laws across three core domains of physics: dynamics, thermodynamics, and optics. Experimental results show that WISA substantially improves the alignment of T2V models (such as CogVideoX and Wan2.1) with real-world physical laws, achieving notable gains on the VideoPhy benchmark. Our data, code, and models are available in the [Project Page](https://wisav1.github.io/WISA/)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zc5Ntjt5Ul": {
    "title": "Conservative classifiers do consistently well with improving agents: characterizing statistical and online learning",
    "volume": "spotlight",
    "abstract": "Machine learning is now ubiquitous in societal decision-making, for example in evaluating job candidates or loan applications, and it is increasingly important to take into account how classified agents will react to the learning algorithms. The majority of recent literature on strategic classification has focused on reducing and countering deceptive behaviors by the classified agents, but recent work of Attias et al. identifies surprising properties of learnability when the agents genuinely improve in order to attain the desirable classification, such as smaller generalization error than standard PAC-learning. In this paper we characterize so-called learnability with improvements across multiple new axes. We introduce an asymmetric variant of minimally consistent concept classes and use it to provide an exact characterization of proper learning with improvements in the realizable setting. While prior work studies learnability only under general, arbitrary agent improvement regions, we give positive results for more natural Euclidean ball improvement sets. In particular, we characterize improper learning under a generative assumption on the data distribution. We further show how to learn in more challenging settings, achieving lower generalization error under well-studied bounded noise models and obtaining mistake bounds in realizable and agnostic online learning. We resolve open questions posed by Attias et al. for both proper and improper learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uAeqQePu4c": {
    "title": "KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation",
    "volume": "spotlight",
    "abstract": "Recent advancements in large language models (LLMs) underscore the need for more comprehensive evaluation methods to accurately assess their reasoning capabilities. Existing benchmarks are often domain-specific and thus cannot fully capture an LLM's general reasoning potential. To address this limitation, we introduce the **Knowledge Orthogonal Reasoning Gymnasium (KORGym)**, a dynamic evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over fifty games in either textual or visual formats and supports interactive, multi-turn assessments with reinforcement learning scenarios. Using KORGym, we conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent reasoning patterns within model families and demonstrating the superior performance of closed-source models. Further analysis examines the effects of modality, reasoning strategies, reinforcement learning techniques, and response length on model performance. We expect KORGym to become a valuable resource for advancing LLM reasoning research and developing evaluation methodologies suited to complex, interactive environments",
    "checked": true,
    "id": "707e609c108c2416d6640201b3cf34b29216b02f",
    "semantic_title": "korgym: a dynamic game platform for llm reasoning evaluation",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=gMHLQASj11": {
    "title": "Learnable Sampler Distillation for Discrete Diffusion Models",
    "volume": "spotlight",
    "abstract": "Discrete diffusion models (DDMs) have shown powerful generation ability for discrete data modalities like text and molecules. However, their practical application is hindered by inefficient sampling, requiring a large number of sampling steps. Accelerating DDMs by using larger step sizes typically introduces significant problems in generation quality, as it amplifies the impact of both the compounding decoding error due to factorized predictions and discretization error from numerical approximations, leading to a significant decrease in sampling quality. To address these challenges, we propose learnable sampler distillation (LSD), a novel approach to train fast and high-fidelity samplers for DDMs. LSD employs a distillation approach where a student sampler with a few steps learns to align its intermediate score trajectory with that of a high-quality teacher sampler with numerous steps. This alignment is achieved by optimizing learnable sampler coefficients that adaptively adjust sampling dynamics. Additionally, we further propose LSD+, which also learns time schedules that allocate steps non-uniformly. Experiments across text generation, image generation, and synthetic tasks demonstrate that our proposed approaches outperform existing samplers for DDMs, achieving substantially higher sampling quality with significantly fewer sampling steps",
    "checked": true,
    "id": "29d3d528b8dac5e9aa8034342d41b6a48c0f0896",
    "semantic_title": "learnable sampler distillation for discrete diffusion models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0GvEaa9prl": {
    "title": "On the Hardness of Approximating Distributions with Tractable Probabilistic Models",
    "volume": "spotlight",
    "abstract": "A fundamental challenge in probabilistic modeling is to balance expressivity and inference efficiency. Tractable probabilistic models (TPMs) aim to directly address this tradeoff by imposing constraints that guarantee efficient inference of certain queries while maintaining expressivity. In particular, probabilistic circuits (PCs) provide a unifying framework for many TPMs, by characterizing families of models as circuits satisfying different structural properties. Because the complexity of inference on PCs is a function of the circuit size, understanding the size requirements of different families of PCs is fundamental in mapping the trade-off between tractability and expressive efficiency. However, the study of expressive efficiency of circuits are often concerned with exact representations, which may not align with model learning, where we look to approximate the underlying data distribution closely by some distance measure. Moreover, due to hardness of inference tasks, exactly representing distributions while supporting tractable inference often incurs exponential size blow-ups. In this paper, we consider a natural, yet so far underexplored, question: can we avoid such size blow-up by allowing for some small approximation error? We study approximating distributions with probabilistic circuits with guarantees based on $f$-divergences, and analyze which inference queries remain well-approximated under this framework. We show that approximating an arbitrary distribution with bounded $f$-divergence is NP-hard for any model that can tractably compute marginals. In addition, we prove an exponential size gap for approximation between the class of decomposable PCs and that of decomposable and deterministic PCs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FAeU7516MR": {
    "title": "MoESD: Unveil Speculative Decoding's Potential for Accelerating Sparse MoE",
    "volume": "spotlight",
    "abstract": "Large Language Models (LLMs) have achieved remarkable success across many applications, with Mixture of Experts (MoE) models demonstrating great potential. Compared to traditional dense models, MoEs achieve better performance with less computation. Speculative decoding (SD) is a widely used technique to accelerate LLM inference without accuracy loss, but it has been considered efficient only for dense models. In this work, we first demonstrate that, under medium batch sizes, MoE surprisingly benefits more from SD than dense models. Furthermore, as MoE becomes sparser -- the prevailing trend in MoE designs -- the batch size range where SD acceleration is expected to be effective becomes broader. To quantitatively understand tradeoffs involved in SD, we develop a reliable modeling based on theoretical analyses. While current SD research primarily focuses on improving acceptance rates of algorithms, changes in workload and model architecture can still lead to degraded SD acceleration even with high acceptance rates. To address this limitation, we introduce a new metric 'target efficiency' that characterizes these effects, thus helping researchers identify system bottlenecks and understand SD acceleration more comprehensively. For scenarios like private serving, this work unveils a new perspective to speed up MoE inference, where existing solutions struggle. Experiments on different GPUs show up to 2.29x speedup for Qwen2-57B-A14B at medium batch sizes and validate our theoretical predictions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PHu9xJeAum": {
    "title": "SoTA with Less: MCTS-Guided Sample Selection for Data-Efficient Visual Reasoning Self-Improvement",
    "volume": "spotlight",
    "abstract": "We introduce ThinkLite-VL, a family of visual reasoning models that achieve state-of-the-art (SoTA) performance using an order of magnitude fewer training samples, relying purely on reinforcement fine-tuning (RFT) self-improvement without any knowledge distillation. Our central insight is that sample difficulty critically influences RFT effectiveness: appropriately challenging examples can drive substantial reasoning improvements, even in low-data regimes. However, quantifying sample difficulty in a reliable and scalable manner remains non-trivial. To address this, we repurpose Monte Carlo Tree Search (MCTS) to measure sample difficulty via the number of reasoning iterations a vision-language model (VLM) requires to solve each instance. This MCTS-based selection procedure identifies samples that induce deeper reasoning while remaining solvable, allowing us to filter a high-quality subset from 70k open-source examples spanning math, natural image understanding, and chart comprehension. Using this approach, we select just 11k challenging samples for RFT on Qwen2.5-VL-7B-Instruct and 7.5k samples for Qwen2.5-VL-72B-Instruct. The resulting models, ThinkLite-VL-7B and ThinkLite-VL-72B, significantly outperform their respective base models across eight visual reasoning benchmarks. In particular, ThinkLite-VL-7B improves the average performance of Qwen2.5-VL-7B-Instruct by 7\\% and surpasses all existing 7B-level models, as well as much larger models such as GPT-4o, O1 and Qwen2.5-VL-72B, achieving a new SoTA score of 75.1 on MathVista. ThinkLite-VL-72B further advances the SoTA frontier, achieving an accuracy of 79.7 on MathVista and an average benchmark improvement of 4.42 over the open-source SOTA. These results demonstrate that MCTS-guided difficulty filtering provides a scalable and effective path toward data-efficient self-improvement in multimodal reasoning",
    "checked": true,
    "id": "cc7964b5bae289fd39fb8531446b0af541b909a4",
    "semantic_title": "sota with less: mcts-guided sample selection for data-efficient visual reasoning self-improvement",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=36cKp4tsHF": {
    "title": "Deno-IF: Unsupervised Noisy Visible and Infrared Image Fusion Method",
    "volume": "spotlight",
    "abstract": "Most image fusion methods are designed for ideal scenarios and struggle to handle noise. Existing noise-aware fusion methods are supervised and heavily rely on constructed paired data, limiting performance and generalization. This paper proposes a novel unsupervised noisy visible and infrared image fusion method, comprising two key modules. First, when only noisy source images are available, a convolutional low-rank optimization module decomposes clean components based on convolutional low-rank priors, guiding subsequent optimization. The unsupervised approach eliminates data dependency and enhances generalization across various and variable noise. Second, a unified network jointly realizes denoising and fusion. It consists of both intra-modal recovery and inter-modal recovery and fusion, also with a convolutional low-rankness loss for regularization. By exploiting the commonalities of denoising and fusion, the joint framework significantly reduces network complexity while expanding functionality. Extensive experiments validate the effectiveness and generalization of the proposed method for image fusion under various and variable noise conditions. The code is publicly available at https://github.com/hanna-xu/Deno-IF",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2zjH76SmiF": {
    "title": "ReCon: Region-Controllable Data Augmentation with Rectification and Alignment for Object Detection",
    "volume": "spotlight",
    "abstract": "The scale and quality of datasets are crucial for training robust perception models. However, obtaining large-scale annotated data is both costly and time-consuming. Generative models have emerged as a powerful tool for data augmentation by synthesizing samples that adhere to desired distributions. However, current generative approaches often rely on complex post-processing or extensive fine-tuning on massive datasets to achieve satisfactory results, and they remain prone to content–position mismatches and semantic leakage. To overcome these limitations, we introduce ReCon, a novel augmentation framework that enhances the capacity of structure-controllable generative models for object detection. ReCon integrates region-guided rectification into the diffusion sampling process, using feedback from a pre-trained perception model to rectify misgenerated regions within diffusion sampling process. We further propose region-aligned cross-attention to enforce spatial–semantic alignment between image regions and their textual cues, thereby improving both semantic consistency and overall image fidelity. Extensive experiments demonstrate that ReCon substantially improve the quality and trainability of generated data, achieving consistent performance gains across various datasets, backbone architectures, and data scales",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=imO353Gyrl": {
    "title": "Repurposing Marigold for Zero-Shot Metric Depth Estimation via Defocus Blur Cues",
    "volume": "spotlight",
    "abstract": "Recent monocular metric depth estimation (MMDE) methods have made notable progress towards zero-shot generalization. However, they still exhibit a significant performance drop on out-of-distribution datasets. We address this limitation by injecting defocus blur cues at inference time into Marigold, a \\textit{pre-trained} diffusion model for zero-shot, scale-invariant monocular depth estimation (MDE). Our method effectively turns Marigold into a metric depth predictor in a training-free manner. To incorporate defocus cues, we capture two images with a small and a large aperture from the same viewpoint. To recover metric depth, we then optimize the metric depth scaling parameters and the noise latents of Marigold at inference time using gradients from a loss function based on the defocus-blur image formation model. We compare our method against existing state-of-the-art zero-shot MMDE methods on a self-collected real dataset, showing quantitative and qualitative improvements",
    "checked": true,
    "id": "c4c7507e04c065f27ec3a8689ed32c83daa03848",
    "semantic_title": "repurposing marigold for zero-shot metric depth estimation via defocus blur cues",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yjLew3Nd7z": {
    "title": "PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding",
    "volume": "spotlight",
    "abstract": "Real-world objects are composed of distinctive, object-specific parts. Identifying these parts is key to performing fine-grained, compositional reasoning—yet, large multimodal models (LMMs) struggle to perform this seemingly straightforward task. In this work, we introduce PARTONOMY, an LMM benchmark designed for pixel-level part grounding. We construct PARTONOMY from existing part datasets and our own rigorously annotated set of images, encompassing 862 parts and 5346 objects for evaluation. Unlike existing datasets that simply ask models to identify generic parts, PARTONOMY utilizes highly technical concepts and challenges models to compare objects' parts, consider part-whole relationships, and justify textual predictions with visual segmentations. Our experiments demonstrate significant limitations in state-of-the-art LMMs (e.g., LISA-13B achieves only 5.9% gIoU), highlighting a critical gap in their part grounding abilities. We note that existing segmentation-enabled LMMs (segmenting LMMs) have two key architectural shortcomings: they use special [SEG] tokens not seen during pretraining which induce distribution shift, and they discard predicted segmentations instead of using past predictions to guide future ones. To address these deficiencies, we train several part-centric LMMs and propose PLUM, a novel segmenting LMM that utilizes span tagging instead of segmentation tokens and that conditions on prior predictions in a feedback loop. We find that pretrained PLUM dominates existing segmenting LMMs on reasoning segmentation, VQA, and visual hallucination benchmarks. In addition, PLUM finetuned on our proposed Explanatory Part Segmentation task is competitive with segmenting LMMs trained on significantly more segmentation data. Our work opens up new avenues towards enabling fine-grained, grounded visual understanding in LMMs",
    "checked": true,
    "id": "643ba062730e783482b0178be1eacb2326b54d77",
    "semantic_title": "partonomy: large multimodal models with part-level visual understanding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7cPDOBWTbM": {
    "title": "Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data",
    "volume": "spotlight",
    "abstract": "Incorporating pre-collected offline data from a source environment can significantly improve the sample efficiency of reinforcement learning (RL), but this benefit is often challenged by discrepancies between the transition dynamics of the source and target environments. Existing methods typically address this issue by penalizing or filtering out source transitions in high dynamics-gap regions. However, their estimation of the dynamics gap often relies on KL divergence or mutual information, which can be ill-defined when the source and target dynamics have disjoint support. To overcome these limitations, we propose CompFlow, a method grounded in the theoretical connection between flow matching and optimal transport. Specifically, we model the target dynamics as a conditional flow built upon the output distribution of the source-domain flow, rather than learning it directly from a Gaussian prior. This composite structure offers two key advantages: (1) improved generalization for learning target dynamics, and (2) a principled estimation of the dynamics gap via the Wasserstein distance between source and target transitions. Leveraging our principled estimation of the dynamics gap, we further introduce an optimistic active data collection strategy that prioritizes exploration in regions of high dynamics gap, and theoretically prove that it reduces the performance disparity with the optimal policy. Empirically, CompFlow outperforms strong baselines across several RL benchmarks with shifted dynamics",
    "checked": true,
    "id": "501027fad9afcd2f8703a04f403d54e9b16ef2be",
    "semantic_title": "composite flow matching for reinforcement learning with shifted-dynamics data",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=9CzRx5MZct": {
    "title": "Asymmetric Duos: Sidekicks Improve Uncertainty",
    "volume": "spotlight",
    "abstract": "The go-to strategy to apply deep networks in settings where uncertainty informs decisions—ensembling multiple training runs with random initializations—is ill-suited for the extremely large-scale models and practical fine-tuning workflows of today. We introduce a new cost-effective strategy for improving the uncertainty quantification and downstream decisions of a large model (e.g. a fine-tuned ViT-B): coupling it with a less accurate but much smaller \"sidekick\" (e.g. a fine-tuned ResNet-34) with a fraction of the computational cost. We propose aggregating the predictions of this *Asymmetric Duo* by simple learned weighted averaging. Surprisingly, despite their inherent asymmetry, the sidekick model almost never harms the performance of the larger model. In fact, across five image classification benchmarks, and a variety of model architectures and training schemes (including soups), Asymmetric Duos significantly improve accuracy, uncertainty quantification, and selective classification metrics with only ${\\sim}10-20$% more computation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZrCQGVpQrl": {
    "title": "Robust learning of halfspaces under log-concave marginals",
    "volume": "spotlight",
    "abstract": "We say that a classifier is $\\text{\\emph{adversarially robust}}$ to perturbations of norm $r$ if, with high probability over a point $x$ drawn from the input distribution, there is no point within distance $\\le r$ from $x$ that is classified differently. The $\\text{\\emph{boundary volume}}$ is the probability that a point falls within distance $r$ of a point with a different label. This work studies the task of learning a hypothesis with small boundary volume, where the input is distributed as a subgaussian isotropic log-concave distribution over $\\mathbb{R}^d$. Linear threshold functions are adversarially robust; they have boundary volume proportional to $r$. Such concept classes are efficiently learnable by polynomial regression, which produces a polynomial threshold function (PTF), but PTFs in general may have boundary volume $\\Omega(1)$, even for $r \\ll 1$. We give an algorithm that agnostically learns linear threshold functions and returns a classfier with boundary volume $O(r+\\varepsilon)$ at radius of perturbation $r$. The time and sample complexity of $d^{\\tilde{O}(1/\\varepsilon^2)}$ matches the complexity of polynomial regression. Our algorithm augments the classic approach of polynomial regression with three additional steps:\\ $\\quad$ a) performing the $\\ell_1$-error regression under $\\ell_1$ noise sensitivity constraints,\\ $\\quad$ b) a structured partitioning and rounding step that returns a Boolean classifier with error $\\mathrm{opt} + O(\\varepsilon)$ and noise sensitivity $O(r+\\varepsilon)$ simultaneously, and \\ $\\quad c)$ a local corrector that ``smooths'' a function with low noise sensitivity into a function that is adversarially robust",
    "checked": true,
    "id": "12b532de3616122166dcf94ddf93c064170713c1",
    "semantic_title": "robust learning of halfspaces under log-concave marginals",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Zn2ajV1kTQ": {
    "title": "Implicit Bias of Spectral Descent and Muon on Multiclass Separable Data",
    "volume": "spotlight",
    "abstract": "Different gradient-based methods for optimizing overparameterized models can all achieve zero training error yet converge to distinctly different solutions inducing different generalization properties. We provide the first complete characterization of implicit optimization bias for p-norm normalized steepest descent (NSD) and momentum steepest descent (NMD) algorithms in multi-class linear classification with cross-entropy loss. Our key theoretical contribution is proving that these algorithms converge to solutions maximizing the margin with respect to the classifier matrix's p-norm, with established convergence rates. These results encompass important special cases including Spectral Descent and Muon, which we show converge to max-margin solutions with respect to the spectral norm. A key insight of our contribution is that the analysis of general entry-wise and Schatten p-norms can be reduced to the analysis of NSD/NMD with max-norm by exploiting a natural ordering property between all p-norms relative to the max-norm and its dual sum-norm. For the specific case of descent with respect to the max-norm, we further extend our analysis to include preconditioning, showing that Adam converges to the matrix's max-norm solution. Our results demonstrate that the multi-class linear setting, which is inherently richer than the binary counterpart, provides the most transparent framework for studying implicit biases of matrix-parameter optimization algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MaJ3ASZ0NI": {
    "title": "Lost in Transmission: When and Why LLMs Fail to Reason Globally",
    "volume": "spotlight",
    "abstract": "Despite their many successes, transformer-based large language models (LLMs) continue to struggle with tasks that require complex reasoning over large parts of their input. We argue that these failures arise due to capacity limits on the accurate flow of information within LLMs. To formalize this issue, we introduce the bounded attention prefix oracle (BAPO) model, a new computational framework that models bandwidth constraints on attention heads, the mechanism for internal communication in LLMs. We show that several important reasoning problems like graph reachability require high communication bandwidth for BAPOs to solve; we call these problems BAPO-hard. Our experiments corroborate our theoretical predictions: GPT-4o, Claude, and Gemini succeed on BAPO-easy tasks and fail even on relatively small BAPO-hard tasks. BAPOs also reveal another benefit of chain of thought (CoT): we prove that breaking down a task using CoT can turn any BAPO-hard problem into a BAPO-easy one. Our results offer principled explanations for key LLM failures and suggest directions for architectures and inference methods that mitigate bandwidth limits",
    "checked": true,
    "id": "ba3482d88d452dc37f7d906ec8d5af490c74af3e",
    "semantic_title": "lost in transmission: when and why llms fail to reason globally",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Iic9I2nHdZ": {
    "title": "On Transferring Transferability: Towards a Theory for Size Generalization",
    "volume": "spotlight",
    "abstract": "Many modern learning tasks require models that can take inputs of varying sizes. Consequently, dimension-independent architectures have been proposed for domains where the inputs are graphs, sets, and point clouds. Recent work on graph neural networks has explored whether a model trained on low-dimensional data can transfer its performance to higher-dimensional inputs. We extend this body of work by introducing a general framework for transferability across dimensions. We show that transferability corresponds precisely to continuity in a limit space formed by identifying small problem instances with equivalent large ones. This identification is driven by the data and the learning task. We instantiate our framework on existing architectures, and implement the necessary changes to ensure their transferability. Finally, we provide design principles for designing new transferable models. Numerical experiments support our findings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o8hWyJIgAV": {
    "title": "When Worse is Better: Navigating the Compression Generation Trade-off In Visual Tokenization",
    "volume": "spotlight",
    "abstract": "Current image generation methods are based on a two-stage training approach. In stage 1, an auto-encoder is trained to compress an image into a latent space; in stage 2, a generative model is trained to learn a distribution over that latent space. This reveals a fundamental trade-off, do we compress more aggressively to make the latent distribution easier for the stage 2 model to learn even if it makes reconstruction worse? We study this problem in the context of discrete, auto-regressive image generation. Through the lens of scaling laws, we show that smaller stage 2 models can benefit from more compressed stage 1 latents even if reconstruction performance worsens, demonstrating that generation modeling capacity plays a role in this trade-off. Diving deeper, we rigorously study the connection between compute scaling and the stage 1 rate-distortion trade-off. Next, we introduce Causally Regularized Tokenization (CRT), which uses knowledge of the stage 2 generation modeling procedure to embed useful inductive biases in stage 1 latents. This regularization improves stage 2 generation performance better by making the tokens easier to model without affecting the stage 1 compression rate and marginally affecting distortion: we are able to improve compute efficiency 2-3$\\times$ over baseline. Finally, we use CRT with further optimizations to the visual tokenizer setup to result in a generative pipeline that matches LlamaGen-3B generation performance (2.18 FID) with half the tokens per image (256 vs. 576) and a fourth the total model parameters (775M vs. 3.1B) while using the same architecture and inference procedure",
    "checked": false,
    "id": "810e721d49db777762af1b2ac24666c849562195",
    "semantic_title": "when worse is better: navigating the compression-generation tradeoff in visual tokenization",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=gZsmYwFHci": {
    "title": "FlowFeat: Pixel-Dense Embedding of Motion Profiles",
    "volume": "spotlight",
    "abstract": "Dense and versatile image representations underpin the success of virtually all computer vision applications. However, state-of-the-art networks, such as transformers, produce low-resolution feature grids, which are suboptimal for dense prediction tasks. To address this limitation, we present *FlowFeat*, a high-resolution and multi-task feature representation. The key ingredient behind FlowFeat is a novel distillation technique that embeds a distribution of plausible apparent motions, or *motion profiles*. By leveraging optical flow networks and diverse video data, we develop an effective self-supervised training framework that statistically approximates the apparent motion. With its remarkable level of spatial detail, FlowFeat encodes a compelling degree of geometric and semantic cues while exhibiting high temporal consistency. Empirically, FlowFeat significantly enhances the representational power of five state-of-the-art encoders and alternative upsampling strategies across three dense tasks: video object segmentation, monocular depth estimation and semantic segmentation. Training FlowFeat is computationally inexpensive and robust to inaccurate flow estimation, remaining highly effective even when using unsupervised flow networks. Our work takes a step forward towards reliable and versatile dense image representations",
    "checked": true,
    "id": "db09afcef94976635b6f068918e433659f45cbb0",
    "semantic_title": "flowfeat: pixel-dense embedding of motion profiles",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5X6PL4906S": {
    "title": "The Generative Leap: Tight Sample Complexity for Efficiently Learning Gaussian Multi-Index Models",
    "volume": "spotlight",
    "abstract": "In this work we consider generic Gaussian Multi-index models, in which the labels only depend on the (Gaussian) $d$-dimensional inputs through their projection onto a low-dimensional $r = O_d(1)$ subspace, and we study efficient agnostic estimation procedures for this hidden subspace. We introduce the *generative leap* exponent, a natural extension of the generative exponent from Damian et al. 2024 to the multi-index setting. We show that a sample complexity of $n=\\Theta(d^{1 \\vee k^\\star/2})$ is necessary in the class of algorithms captured by the Low-Degree-Polynomial framework; and also sufficient, by giving a sequential estimation procedure based on a spectral U-statistic over appropriate Hermite tensors",
    "checked": false,
    "id": "c11983c44f5c149454ea54d45c7a6abd15cedc9d",
    "semantic_title": "the generative leap: sharp sample complexity for efficiently learning gaussian multi-index models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=bTssV4Cnjn": {
    "title": "Incremental Sequence Classification with Temporal Consistency",
    "volume": "spotlight",
    "abstract": "We address the problem of incremental sequence classification, where predictions are updated as new elements in the sequence are revealed. Drawing on temporal-difference learning from reinforcement learning, we identify a temporal-consistency condition that successive predictions should satisfy. We leverage this condition to develop a novel loss function for training incremental sequence classifiers. Through a concrete example, we demonstrate that optimizing this loss can offer substantial gains in data efficiency. We apply our method to text classification tasks and show that it improves predictive accuracy over competing approaches on several benchmark datasets. We further evaluate our approach on the task of verifying large language model generations for correctness in grade-school math problems. Our results show that models trained with our method are better able to distinguish promising generations from unpromising ones after observing only a few tokens",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Iqu63cYI3z": {
    "title": "LODGE: Level-of-Detail Large-Scale Gaussian Splatting with Efficient Rendering",
    "volume": "spotlight",
    "abstract": "In this work, we present a novel level-of-detail (LOD) method for 3D Gaussian Splatting that enables real-time rendering of large-scale scenes on memory-constrained devices. Our approach introduces a hierarchical LOD representation that iteratively selects optimal subsets of Gaussians based on camera distance, thus largely reducing both rendering time and GPU memory usage. We construct each LOD level by applying a depth-aware 3D smoothing filter, followed by importance-based pruning and fine-tuning to maintain visual fidelity. To further reduce memory overhead, we partition the scene into spatial chunks and dynamically load only relevant Gaussians during rendering, employing an opacity-blending mechanism to avoid visual artifacts at chunk boundaries. Our method achieves state-of-the-art performance on both outdoor (Hierarchical 3DGS) and indoor (Zip-NeRF) datasets, delivering high-quality renderings with reduced latency and memory requirements",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5xdbWUdM87": {
    "title": "Solving Neural Min-Max Games: The Role of Architecture, Initialization & Dynamics",
    "volume": "spotlight",
    "abstract": "Many emerging applications—such as adversarial training, AI alignment, and robust optimization—can be framed as zero-sum games between neural nets, with von Neumann–Nash equilibria (NE) capturing the desirable system behavior. While such games often involve non-convex non-concave objectives, empirical evidence shows that simple gradient methods frequently converge, suggesting a hidden geometric structure. In this paper, we provide a theoretical framework that explains this phenomenon through the lens of \\emph{hidden convexity} and \\emph{overparameterization}. We identify sufficient conditions spanning initialization, training dynamics, and network width—that guarantee global convergence to a NE in a broad class of non-convex min-max games. To our knowledge, this is the first such result for games that involve two-layer neural networks. Technically, our approach is twofold: (a) we derive a novel path-length bound for alternating gradient-descent-ascent scheme in min-max games; and (b) we show that games with hidden convex–concave geometry reduce to settings satisfying two-sided Polyak–Łojasiewicz (PL) and smoothness conditions, which hold with high probability under overparameterization, using tools from random matrix theory",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1ffIkWo0yq": {
    "title": "Provable Gradient Editing of Deep Neural Networks",
    "volume": "spotlight",
    "abstract": "In explainable AI, DNN gradients are used to interpret the prediction; in safety-critical control systems, gradients could encode safety constraints; in scientific-computing applications, gradients could encode physical invariants. While recent work on provable editing of DNNs has focused on input-output constraints, the problem of enforcing hard constraints on DNN gradients remains unaddressed. We present ProGrad, the first efficient approach for editing the parameters of a DNN to provably enforce hard constraints on the DNN gradients. Given a DNN $\\mathcal{N}$ with parameters $\\theta$, and a set $\\mathcal{S}$ of pairs $(\\mathrm{x}, \\mathrm{Q})$ of input $\\mathrm{x}$ and corresponding linear gradient constraints $\\mathrm{Q}$, ProGrad finds new parameters $\\theta'$ such that $\\bigwedge_{(\\mathrm{x}, \\mathrm{Q}) \\in \\mathcal{S}} \\frac{\\partial}{\\partial \\mathrm{x}}\\mathcal{N}(\\mathrm{x}; \\theta') \\in \\mathrm{Q}$ while minimizing the changes $\\lVert\\theta' - \\theta\\rVert$. The key contribution is a novel *conditional variable gradient* of DNNs, which relaxes the NP-hard provable gradient editing problem to a linear program (LP), enabling ProGrad to use an LP solver to efficiently and effectively enforce the gradient constraints. We experimentally evaluated ProGrad via enforcing (i) hard Grad-CAM constraints on ImageNet ResNet DNNs; (ii) hard Integrated Gradients constraints on Llama 3 and Qwen 3 LLMs; (iii) hard gradient constraints in training a DNN to approximate a target function as a proxy for safety constraints in control systems and physical invariants in scientific applications. The results highlight the unique capability of ProGrad in enforcing hard constraints on DNN gradients",
    "checked": false,
    "id": "6a49c28cd72f24ec3b07a83eb4ab6758831fa153",
    "semantic_title": "provable editing of deep neural networks using parametric linear relaxation",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=FjByDpDVIO": {
    "title": "Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio Language Models",
    "volume": "spotlight",
    "abstract": "We present Audio Flamingo 3 (AF3), a fully open state-of-the-art (SOTA) large audio-language model that advances reasoning and understanding across speech, sound, and music. AF3 introduces: (i) AF-Whisper, a unified audio encoder trained using a novel strategy for joint representation learning across all 3 modalities of speech, sound, and music; (ii) flexible, on-demand thinking, allowing the model to do chain-of-thought-type reasoning before answering; (iii) multi-turn, multi-audio chat; (iv) long audio understanding and reasoning (including speech) up to 10 minutes; and (v) voice-to-voice interaction. To enable these capabilities, we propose several large-scale training datasets curated using novel strategies, including AudioSkills-XL, LongAudio-XL, AF-Think, and AF-Chat, and train AF3 with a novel five-stage curriculum-based training strategy. Trained on only open-source audio data, AF3 achieves new SOTA results on over 20+ (long) audio understanding and reasoning benchmarks, surpassing both open-weight and closed-source models trained on much larger datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0fZoqVoc0o": {
    "title": "RGB-Only Supervised Camera Parameter Optimization in Dynamic Scenes",
    "volume": "spotlight",
    "abstract": "Although COLMAP has long remained the predominant method for camera parameter optimization in static scenes, it is constrained by its lengthy runtime and reliance on ground truth (GT) motion masks for application to dynamic scenes. Many efforts attempted to improve it by incorporating more priors as supervision such as GT focal length, motion masks, 3D point clouds, camera poses, and metric depth, which, however, are typically unavailable in casually captured RGB videos. In this paper, we propose a novel method for more accurate and efficient camera parameter optimization in dynamic scenes solely supervised by a single RGB video, dubbed $\\textbf{\\textit{ROS-Cam}}$. Our method consists of three key components: (1) Patch-wise Tracking Filters, to establish robust and maximally sparse hinge-like relations across the RGB video. (2) Outlier-aware Joint Optimization, for efficient camera parameter optimization by adaptive down-weighting of moving outliers, without reliance on motion priors. (3) A Two-stage Optimization Strategy, to enhance stability and optimization speed by a trade-off between the Softplus limits and convex minima in losses. We visually and numerically evaluate our camera estimates. To further validate accuracy, we feed the camera estimates into a 4D reconstruction method and assess the resulting 3D scenes, and rendered 2D RGB and depth maps. We perform experiments on 4 real-world datasets (NeRF-DS, DAVIS, iPhone, and TUM-dynamics) and 1 synthetic dataset (MPI-Sintel), demonstrating that our method estimates camera parameters more efficiently and accurately with a single RGB video as the only supervision",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k4jg1QCw0e": {
    "title": "Integration Matters for Learning PDEs with Backwards SDEs",
    "volume": "spotlight",
    "abstract": "Backward stochastic differential equation (BSDE)-based deep learning methods provide an alternative to Physics-Informed Neural Networks (PINNs) for solving high-dimensional partial differential equations (PDEs), offering potential algorithmic advantages in settings such as stochastic optimal control, where the PDEs of interest are tied to an underlying dynamical system. However, standard BSDE-based solvers have empirically been shown to underperform relative to PINNs in the literature. In this paper, we identify the root cause of this performance gap as a discretization bias introduced by the standard Euler-Maruyama (EM) integration scheme applied to one-step self-consistency BSDE losses, which shifts the optimization landscape off target. We find that this bias cannot be satisfactorily addressed through finer step-sizes or multi-step self-consistency losses. To properly handle this issue, we propose a Stratonovich-based BSDE formulation, which we implement with stochastic Heun integration. We show that our proposed approach completely eliminates the bias issues faced by EM integration. Furthermore, our empirical results show that our Heun-based BSDE method consistently outperforms EM-based variants and achieves competitive results with PINNs across multiple high-dimensional benchmarks. Our findings highlight the critical role of integration schemes in BSDE-based PDE solvers, an algorithmic detail that has received little attention thus far in the literature",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2ptM76yNzZ": {
    "title": "Unleashing Hour-Scale Video Training for Long Video-Language Understanding",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f6AYwCvynr": {
    "title": "Neural Entropy",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=skunuOdavO": {
    "title": "Locality in Image Diffusion Models Emerges from Data Statistics",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TjQP5hc3WC": {
    "title": "Regularized least squares learning with heavy-tailed noise is minimax optimal",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "1235c2adac0d27740e3e7ae99e753685273d3375",
    "semantic_title": "regularized least squares learning with heavy-tailed noise is minimax optimal",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qu6mRbSnUs": {
    "title": "On Feasible Rewards in Multi-Agent Inverse Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "Multi-agent inverse reinforcement learning (MAIRL) aims to recover agent reward functions from expert demonstrations. We characterize the feasible reward set in Markov games, identifying all reward functions that rationalize a given equilibrium. However, equilibrium-based observations are often ambiguous: a single Nash equilibrium can correspond to many reward structures, potentially changing the game's nature in multi-agent systems. We address this by introducing entropy-regularized Markov games, which yield a unique equilibrium while preserving strategic incentives. For this setting, we provide a sample complexity analysis detailing how errors affect learned policy performance. Our work establishes theoretical foundations and practical insights for MAIRL",
    "checked": true,
    "id": "1ba1bc36a2212a1b374757009fa0b57284e2f2a4",
    "semantic_title": "on feasible rewards in multi-agent inverse reinforcement learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=XfHfTqeXfZ": {
    "title": "MonarchAttention: Zero-Shot Conversion to Fast, Hardware-Aware Structured Attention",
    "volume": "spotlight",
    "abstract": "Transformers have achieved state-of-the-art performance across various tasks, but suffer from a notable quadratic complexity in sequence length due to the attention mechanism. In this work, we propose MonarchAttention -- a novel approach to sub-quadratic attention approximation via Monarch matrices, an expressive class of structured matrices. Based on the variational form of softmax, we describe an efficient optimization-based algorithm to compute an approximate projection of softmax attention onto the class of Monarch matrices with $\\Theta(N\\sqrt{N} d)$ computational complexity and $\\Theta(Nd)$ memory/IO complexity. Unlike previous approaches, MonarchAttention is both (1) transferable, yielding minimal performance loss with no additional training, even when replacing every attention layer of the transformer, and (2) hardware-efficient, utilizing the highest-throughput tensor core units on modern GPUs. With optimized kernels, MonarchAttention achieves substantial speed-ups in wall-time over FlashAttention-2: $1.4\\times$ for shorter sequences $(N=256)$, $4.5\\times$ for medium-length sequences $(N=4K)$, and $8.2\\times$ for longer sequences $(N=16K)$. We demonstrate the quality of MonarchAttention on diverse tasks and architectures in vision and language problems, showing that it flexibly and accurately approximates softmax attention in a variety of contexts",
    "checked": true,
    "id": "7528c6726491498184776470f3550bab19e766fe",
    "semantic_title": "monarchattention: zero-shot conversion to fast, hardware-aware structured attention",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ccPts3Df2q": {
    "title": "The Hawthorne Effect in Reasoning Models: Evaluating and Steering Test Awareness",
    "volume": "spotlight",
    "abstract": "Reasoning-focused LLMs sometimes alter their behavior when they detect that they are being evaluated—which can lead them to optimize for test-passing performance or to comply more readily with harmful prompts if real-world consequences appear absent. We present the first quantitative study of how such \"test awareness\" impacts model behavior, particularly its performance on safety-related tasks. We introduce a white-box probing framework that (i) linearly identifies awareness-related activations and (ii) steers models toward or away from test awareness while monitoring downstream performance. We apply our method to different state-of-the-art open-weight reasoning LLMs across both realistic and hypothetical tasks (denoting tests or simulations). Our results demonstrate that test awareness significantly impacts safety alignment (such as compliance with harmful requests and conforming to stereotypes) with effects varying in both magnitude and direction across models. By providing control over this latent effect, our work aims to provide a stress-test mechanism and increase trust in how we perform safety evaluations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R0LqbSgZjP": {
    "title": "Measuring and Controlling Solution Degeneracy across Task-Trained Recurrent Neural Networks",
    "volume": "spotlight",
    "abstract": "Task-trained recurrent neural networks (RNNs) are widely used in neuroscience and machine learning to model dynamical computations. To gain mechanistic insight into how neural systems solve tasks, prior work often reverse-engineers individual trained networks. However, different RNNs trained on the same task and achieving similar performance can exhibit strikingly different internal solutions, a phenomenon known as solution degeneracy. Here, we develop a unified framework to systematically quantify and control solution degeneracy across three levels: behavior, neural dynamics, and weight space. We apply this framework to 3,400 RNNs trained on four neuroscience-relevant tasks—flip-flop memory, sine wave generation, delayed discrimination, and path integration—while systematically varying task complexity, learning regime, network size, and regularization. We find that increased task complexity and stronger feature learning reduce degeneracy in neural dynamics but increase it in weight space, with mixed effects on behavior. In contrast, larger networks and structural regularization reduce degeneracy at all three levels. These findings empirically validate the Contravariance Principle and provide practical guidance for researchers seeking to tune the variability of RNN solutions, either to uncover shared neural mechanisms or to model the individual variability observed in biological systems. This work provides a principled framework for quantifying and controlling solution degeneracy in task-trained RNNs, offering new tools for building more interpretable and biologically grounded models of neural computation",
    "checked": true,
    "id": "df2a85d6a47c67c8ded31b994fd6726a351e8f21",
    "semantic_title": "measuring and controlling solution degeneracy across task-trained recurrent neural networks",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=DYuPwwDy9n": {
    "title": "Reinforcement Learning with Imperfect Transition Predictions: A Bellman-Jensen Approach",
    "volume": "spotlight",
    "abstract": "Traditional reinforcement learning (RL) assumes the agents make decisions based on Markov decision processes (MDPs) with one-step transition models. In many real-world applications, such as energy management and stock investment, agents can access multi-step predictions of future states, which provide additional advantages for decision making. However, multi-step predictions are inherently high-dimensional: naively embedding these predictions into an MDP leads to an exponential blow-up in state space and the curse of dimensionality. Moreover, existing RL theory provides few tools to analyze prediction-augmented MDPs, as it typically works on one-step transition kernels and cannot accommodate multi-step predictions with errors or partial action-coverage. We address these challenges with three key innovations: First, we propose the \\emph{Bayesian value function} to characterize the optimal prediction-aware policy tractably. Second, we develop a novel \\emph{Bellman–Jensen Gap} analysis on the Bayesian value function, which enables characterizing the value of imperfect predictions. Third, we introduce BOLA (Bayesian Offline Learning with Online Adaptation), a two-stage model-based RL algorithm that separates offline Bayesian value learning from lightweight online adaptation to real-time predictions. We prove that BOLA remains sample-efficient even under imperfect predictions. We validate our theory and algorithm on synthetic MDPs and a real-world wind energy storage control problem",
    "checked": true,
    "id": "8f8a02bc79b568a036fadf784d5aafc1accfdbb1",
    "semantic_title": "reinforcement learning with imperfect transition predictions: a bellman-jensen approach",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RwfrdKSgCE": {
    "title": "AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench",
    "volume": "spotlight",
    "abstract": "AI research agents are demonstrating great potential to accelerate scientific progress by automating the design, implementation, and training of machine learning models. We focus on methods for improving agents' performance on MLE-bench, a challenging benchmark where agents compete in Kaggle competitions to solve real-world machine learning problems. We formalize AI research agents as search policies that navigate a space of candidate solutions, iteratively modifying them using operators. By designing and systematically varying different operator sets and search policies (Greedy, MCTS, Evolutionary), we show that their interplay is critical for achieving high performance. Our best pairing of search strategy and operator set achieves a state-of-the-art result on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from 39.6% to 47.7%. Our investigation underscores the importance of jointly considering the search strategy, operator design, and evaluation methodology in advancing automated machine learning",
    "checked": true,
    "id": "c1eea704b482cf43dfacb25cc3d7e48e49e77214",
    "semantic_title": "ai research agents for machine learning: search, exploration, and generalization in mle-bench",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=95z3psF9zJ": {
    "title": "Differentiable Cyclic Causal Discovery Under Unmeasured Confounders",
    "volume": "spotlight",
    "abstract": "Understanding causal relationships between variables is fundamental across scientific disciplines. Most causal discovery algorithms rely on two key assumptions: (i) all variables are observed, and (ii) the underlying causal graph is acyclic. While these assumptions simplify theoretical analysis, they are often violated in real-world systems, such as biological networks. Existing methods that account for confounders either assume linearity or struggle with scalability. To address these limitations, we propose DCCD-CONF, a novel framework for differentiable learning of nonlinear cyclic causal graphs in the presence of unmeasured confounders using interventional data. Our approach alternates between optimizing the graph structure and estimating the confounder distribution by maximizing the log-likelihood of the data. Through experiments on synthetic data and real-world gene perturbation datasets, we show that DCCD-CONF outperforms state-of-the-art methods in both causal graph recovery and confounder identification. Additionally, we provide consistency guarantees for our framework, reinforcing its theoretical soundness",
    "checked": true,
    "id": "31ca4a08a2b6503f3c624e8bfcf732d1999f3905",
    "semantic_title": "differentiable cyclic causal discovery under unmeasured confounders",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rz9ISbLxf0": {
    "title": "ELECTRA: A Cartesian Network for 3D Charge Density Prediction with Floating Orbitals",
    "volume": "spotlight",
    "abstract": "We present the Electronic Tensor Reconstruction Algorithm (ELECTRA) - an equivariant model for predicting electronic charge densities using floating orbitals. Floating orbitals are a long-standing concept in the quantum chemistry community that promises more compact and accurate representations by placing orbitals freely in space, as opposed to centering all orbitals at the position of atoms. Finding the ideal placement of these orbitals requires extensive domain knowledge, though, which thus far has prevented widespread adoption. We solve this in a data-driven manner by training a Cartesian tensor network to predict the orbital positions along with orbital coefficients. This is made possible through a symmetry-breaking mechanism that is used to learn position displacements with lower symmetry than the input molecule while preserving the rotation equivariance of the charge density itself. Inspired by recent successes of Gaussian Splatting in representing densities in space, we are using Gaussian orbitals and predicting their weights and covariance matrices. Our method achieves a state-of-the-art balance between computational efficiency and predictive accuracy on established benchmarks. Furthermore, ELECTRA is able to lower the compute time required to arrive at converged DFT solutions - initializing calculations using our predicted densities yields an average 50.72 % reduction in self-consistent field (SCF) iterations on unseen molecules",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WbpXT0WL9S": {
    "title": "Compositional Neural Network Verification via Assume-Guarantee Reasoning",
    "volume": "spotlight",
    "abstract": "Verifying the behavior of neural networks is necessary if developers are to confidently deploy them as parts of mission-critical systems. Toward this end, researchers have been actively developing a range of increasingly sophisticated and scalable neural network verifiers. However, scaling verification to large networks is challenging, at least in part due to the significant memory requirements of verification algorithms. In this paper, we propose an assume-guarantee compositional framework, CoVeNN, that is parameterized by an underlying verifier to generate a sequence of verification sub-problems to address this challenge. We present an iterative refinement-based strategy for computing assumptions that allow sub-problems to retain sufficient accuracy. An evaluation using 7 neural networks and a total of 140 property specifications demonstrates that CoVeNN can verify nearly 7 times more problems than state-of-the-art verifiers. CoVeNN is part of the NeuralSAT verification project: https://github.com/dynaroars/neuralsat",
    "checked": false,
    "id": "448f654dc85a49a6dbd0ccef812987e12acdb610",
    "semantic_title": "compositional verification using geodesic distance via assume-guarantee reasoning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=31CaYYw1Xz": {
    "title": "A machine learning approach that beats Rubik's cubes",
    "volume": "spotlight",
    "abstract": "The paper proposes a novel machine learning-based approach to the pathfinding problem on extremely large graphs. This method leverages diffusion distance estimation via a neural network and uses beam search for pathfinding. We demonstrate its efficiency by finding solutions for 4x4x4 and 5x5x5 Rubik's cubes with unprecedentedly short solution lengths, outperforming all available solvers and introducing the first machine learning solver beyond the 3x3x3 case. In particular, it surpasses every single case of the combined best results in the Kaggle Santa 2023 challenge, which involved over 1,000 teams. For the 3x3x3 Rubik's cube, our approach achieves an optimality rate exceeding 98%, matching the performance of task-specific solvers and significantly outperforming prior solutions such as DeepCubeA (60.3%) and EfficientCube (69.6%). Our solution in its current implementation is approximately 25.6 times faster in solving 3x3x3 Rubik's cubes while requiring up to 8.5 times less model training time than the most efficient state-of-the-art competitor. Finally, it is demonstrated that even a single agent trained using a relatively small number of examples can robustly solve a broad range of puzzles represented by Cayley graphs of size up to $10^{145}$, confirming the generality of the proposed method",
    "checked": false,
    "id": "e6ab82ee985e947bee09520ccbd42b816c4c213f",
    "semantic_title": "a machine learning approach that beats large rubik's cubes",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=3CYXSMFv55": {
    "title": "ENMA: Tokenwise Autoregression for Continuous Neural PDE Operators",
    "volume": "spotlight",
    "abstract": "Solving time-dependent parametric partial differential equations (PDEs) remains a fundamental challenge for neural solvers, particularly when generalizing across a wide range of physical parameters and dynamics. When data is uncertain or incomplete—as is often the case—a natural approach is to turn to generative models. We introduce ENMA, a generative neural operator designed to model spatio-temporal dynamics arising from physical phenomena. ENMA predicts future dynamics in a compressed latent space using a generative masked autoregressive transformer trained with flow matching loss, enabling tokenwise generation. Irregularly sampled spatial observations are encoded into uniform latent representations via attention mechanisms and further compressed through a spatio-temporal convolutional encoder. This allows ENMA to perform in-context learning at inference time by conditioning on either past states of the target trajectory or auxiliary context trajectories with similar dynamics. The result is a robust and adaptable framework that generalizes to new PDE regimes and supports one-shot surrogate modeling of time-dependent parametric PDEs",
    "checked": false,
    "id": "9807f12e46245c63bdaa7c6c51ebdf78a011ea96",
    "semantic_title": "enma: tokenwise autoregression for generative neural pde operators",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XUKUx7Xu89": {
    "title": "Critical Batch Size Revisited: A Simple Empirical Approach to Large-Batch Language Model Training",
    "volume": "spotlight",
    "abstract": "The right batch size is important when training language models at scale: a large batch size is necessary for fast training, but a batch size that is *too large* will harm token efficiency. To navigate this tradeoff, McCandlish et al. (2018) suggest that a *critical batch size* (CBS), below which training will not substantially degrade loss, can be estimated based on the gradient noise scale during training. While their method has been adopted in practice, e.g., when training GPT-3, strong assumptions are required to justify gradient noise as a proxy for the CBS, which makes it unclear whether their approach should be trusted in practice, limiting its applicability. In this paper, we introduce a simple, empirical approach to *directly* measure the CBS and show how the CBS evolves over training. Applying our approach to the OLMo models, we find that CBS is near 0 at initialization, increases rapidly at first, and then plateaus as training progresses. Furthermore, we find that this trend holds across different model sizes (1B and 7B), suggesting CBS from small training runs can inform larger-scale training runs. Our findings about how the CBS changes over training motivate *batch size warmup* as a natural way to reliably train language models at large batch size: start the batch size small and increase it as the CBS grows. To validate this claim, we use batch size warmup to train OLMo 1B to slightly better loss than the original training run with 43% fewer gradient steps. This shows how our framework can be applied to reliably train language models at larger batch sizes, increasing data parallelism without compromising performance",
    "checked": true,
    "id": "8d38772718645a56029ff3fc66bec7eaf777a5d3",
    "semantic_title": "critical batch size revisited: a simple empirical approach to large-batch language model training",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=loVvFhfsDf": {
    "title": "Shift Before You Learn: Enabling Low-Rank Representations in Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "Low-rank structure is a common implicit assumption in many modern reinforcement learning (RL) algorithms. For instance, reward-free and goal-conditioned RL methods often presume that the successor measure admits a low-rank representation. In this work, we challenge this assumption by first remarking that the successor measure itself is not approximately low-rank. Instead, we demonstrate that a low-rank structure naturally emerges in the shifted successor measure, which captures the system dynamics after bypassing a few initial transitions. We provide finite-sample performance guarantees for the entry-wise estimation of a low-rank approximation of the shifted successor measure from sampled entries. Our analysis reveals that both the approximation and estimation errors are primarily governed by a newly introduced quantitity: the spectral recoverability of the corresponding matrix. To bound this parameter, we derive a new class of functional inequalities for Markov chains that we call Type II Poincaré inequalities and from which we can quantify the amount of shift needed for effective low-rank approximation and estimation. This analysis shows in particular that the required shift depends on decay of the high-order singular values of the shifted successor measure and is hence typically small in practice. Additionally, we establish a connection between the necessary shift and the local mixing properties of the underlying dynamical system, which provides a natural way of selecting the shift. Finally, we validate our theoretical findings with experiments, and demonstrate that shifting the successor measure indeed leads to improved performance in goal-conditioned RL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fGfl6dqQVf": {
    "title": "Stochastic Optimization in Semi-Discrete Optimal Transport: Convergence Analysis and Minimax Rate",
    "volume": "spotlight",
    "abstract": "We investigate the semi-discrete Optimal Transport (OT) problem, where a continuous source measure $\\mu$ is transported to a discrete target measure $\\nu$, with particular attention to the OT map approximation. In this setting, Stochastic Gradient Descent (SGD) based solvers have demonstrated strong empirical performance in recent machine learning applications, yet their theoretical guarantee to approximate the OT map is an open question. In this work, we answer it positively by providing both computational and statistical convergence guarantees of SGD. Specifically, we show that SGD methods can estimate the OT map with a minimax convergence rate of $\\mathcal{O}(1/\\sqrt{n})$, where $n$ is the number of samples drawn from $\\mu$. To establish this result, we study the averaged projected SGD algorithm, and identify a suitable projection set that contains a minimizer of the objective, even when the source measure is not compactly supported. Our analysis holds under mild assumptions on the source measure and applies to MTW cost functions,whic include $\\|\\cdot\\|^p$ for $p \\in (1, \\infty)$. We finally provide numerical evidence for our theoretical results",
    "checked": true,
    "id": "f508842dd94208da02f84d112410ff4928a10521",
    "semantic_title": "stochastic optimization in semi-discrete optimal transport: convergence analysis and minimax rate",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yb5JOOmfxA": {
    "title": "Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models",
    "volume": "spotlight",
    "abstract": "Recent years have seen a surge in interest in digital content watermarking techniques, driven by the proliferation of generative models and increased legal pressure. With an ever-growing percentage of AI-generated content available online, watermarking plays an increasingly important role in ensuring content authenticity and attribution at scale. There have been many works assessing the robustness of watermarking to removal attacks, yet, watermark forging, the scenario when a watermark is stolen from genuine content and applied to malicious content, remains underexplored. In this work, we investigate watermark forging in the context of widely used post-hoc image watermarking. Our contributions are as follows. First, we introduce a preference model to assess whether an image is watermarked. The model is trained using a ranking loss on purely procedurally generated images without any need for real watermarks. Second, we demonstrate the model's capability to remove and forge watermarks by optimizing the input image through backpropagation. This technique requires only a single watermarked image and works without knowledge of the watermarking model, making our attack much simpler and more practical than attacks introduced in related work. Third, we evaluate our proposed method on a variety of post-hoc image watermarking models, demonstrating that our approach can effectively forge watermarks, questioning the security of current watermarking approaches. Our code and further resources are publicly available",
    "checked": true,
    "id": "b1e0449c273bd6b0de001e8b7088da80cd7e0608",
    "semantic_title": "transferable black-box one-shot forging of watermarks via image preference models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UwtFuWbW6B": {
    "title": "Non-Asymptotic Analysis Of Data Augmentation For Precision Matrix Estimation",
    "volume": "spotlight",
    "abstract": "This paper addresses the problem of inverse covariance (also known as precision matrix) estimation in high-dimensional settings. Specifically, we focus on two classes of estimators: linear shrinkage estimators with a target proportional to the identity matrix, and estimators derived from data augmentation (DA). Here, DA refers to the common practice of enriching a dataset with artificial samples—typically generated via a generative model or through random transformations of the original data—prior to model fitting. For both classes of estimators, we derive estimators and provide concentration bounds for their quadratic error. This allows for both method comparison and hyperparameter tuning, such as selecting the optimal proportion of artificial samples. On the technical side, our analysis relies on tools from random matrix theory. We introduce a novel deterministic equivalent for generalized resolvent matrices, accommodating dependent samples with specific structure. We support our theoretical results with numerical experiments",
    "checked": true,
    "id": "390430340939c81f1f68d3efba4985c41193e240",
    "semantic_title": "non-asymptotic analysis of data augmentation for precision matrix estimation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AjOl3iahHd": {
    "title": "Improving Perturbation-based Explanations by Understanding the Role of Uncertainty Calibration",
    "volume": "spotlight",
    "abstract": "Perturbation-based explanations are widely utilized to enhance the transparency of machine-learning models in practice. However, their reliability is often compromised by the unknown model behavior under the specific perturbations used. This paper investigates the relationship between uncertainty calibration - the alignment of model confidence with actual accuracy - and perturbation-based explanations. We show that models systematically produce unreliable probability estimates when subjected to explainability-specific perturbations and theoretically prove that this directly undermines global and local explanation quality. To address this, we introduce ReCalX, a novel approach to recalibrate models for improved explanations while preserving their original predictions. Empirical evaluations across diverse models and datasets demonstrate that ReCalX consistently reduces perturbation-specific miscalibration most effectively while enhancing explanation robustness and the identification of globally important input features",
    "checked": true,
    "id": "e033d544000e4b93da3effb3ce80bc12b5d66a0e",
    "semantic_title": "improving perturbation-based explanations by understanding the role of uncertainty calibration",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kGMRb4jbTP": {
    "title": "ConTextTab: A Semantics-Aware Tabular In-Context Learner",
    "volume": "spotlight",
    "abstract": "Tabular in-context learning (ICL) has recently achieved state-of-the-art (SOTA) performance on several tabular prediction tasks. Previously restricted to classification problems on small tables, recent advances such as TabPFN and TabICL have extended its use to larger datasets. Although current table-native ICL architectures are architecturally efficient and well-adapted to tabular data structures, their exclusive training on synthetic data limits their ability to fully leverage the rich semantics and world knowledge contained in real-world tabular data. At the other end of the spectrum, tabular ICL models based on pretrained large language models such as TabuLa-8B integrate deep semantic understanding and world knowledge but are only able to make use of a small amount of context due to inherent architectural limitations. With the aim to combine the best of both these worlds, we introduce ConTextTab, integrating semantic understanding and alignment into a table-native ICL framework. By employing specialized embeddings for different data modalities and by training on large-scale real-world tabular data, our model is competitive with SOTA across a broad set of benchmarks while setting a new standard on the semantically rich CARTE benchmark. Code and model checkpoints are available at: https://github.com/SAP-samples/contexttab",
    "checked": true,
    "id": "9164a42a3ed734e5ee2491fddeea8dc89cca9c11",
    "semantic_title": "contexttab: a semantics-aware tabular in-context learner",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=s3WyfnHw6B": {
    "title": "Some Optimizers are More Equal: Understanding the Role of Optimizers in Group Fairness",
    "volume": "spotlight",
    "abstract": "We study whether and how the choice of optimization algorithm can impact group fairness in deep neural networks. Through stochastic differential equation analysis of optimization dynamics in an analytically tractable setup, we demonstrate that the choice of optimization algorithm indeed influences fairness outcomes, particularly under severe imbalance. Furthermore, we show that when comparing two categories of optimizers, adaptive methods and stochastic methods, RMSProp (from the adaptive category) has a higher likelihood of converging to fairer minima than SGD (from the stochastic category). Building on this insight, we derive two new theoretical guarantees showing that, under appropriate conditions, RMSProp exhibits fairer parameter updates and improved fairness in a single optimization step compared to SGD. We then validate these findings through extensive experiments on three publicly available datasets, namely CelebA, FairFace, and MS-COCO, across different tasks as facial expression recognition, gender classification, and multi-label classification, using various backbones. Considering multiple fairness definitions including equalized odds, equal opportunity, and demographic parity, adaptive optimizers like RMSProp and Adam consistently outperform SGD in terms of group fairness, while maintaining comparable predictive accuracy. Our results highlight the role of adaptive updates as a crucial yet overlooked mechanism for promoting fair outcomes. We release the source code at: https://github.com/Mkolahdoozi/Some-Optimizers-Are-More-Equal",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R2ZJSjLDJC": {
    "title": "Less is More: Improving LLM Alignment via Preference Data Selection",
    "volume": "spotlight",
    "abstract": "Direct Preference Optimization (DPO) has emerged as a promising approach for aligning large language models with human preferences. While prior work mainly extends DPO from the aspect of the objective function, we instead improve DPO from the largely overlooked but critical aspect of data selection. Specifically, we address the issue of parameter shrinkage caused by noisy data by proposing a novel margin-maximization principle for dataset curation in DPO training. To further mitigate the noise in different reward models, we propose a Bayesian Aggregation approach that unifies multiple margin sources (external and implicit) into a single preference probability. Extensive experiments in diverse settings demonstrate the consistently high data efficiency of our approach. Remarkably, by using just 10\\% of the Ultrafeedback dataset, our approach achieves 3\\% to 8\\% improvements across various Llama, Mistral, and Qwen models on the AlpacaEval2 benchmark. Furthermore, our approach seamlessly extends to iterative DPO, yielding a roughly 3\\% improvement with 25\\% online data, revealing the high redundancy in this presumed high-quality data construction manner. These results highlight the potential of data selection strategies for advancing preference optimization",
    "checked": false,
    "id": "6600edb6a34e5b4d69a0a935d97ed43dd886a4f7",
    "semantic_title": "beyond the surface: enhancing llm-as-a-judge alignment with human via internal representations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vqj65VeDOu": {
    "title": "Stable Gradients for Stable Learning at Scale in Deep Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "Scaling deep reinforcement learning networks is challenging and often results in degraded performance, yet the root causes of this failure mode remain poorly understood. Several recent works have proposed mechanisms to address this, but they are often complex and fail to highlight the causes underlying this difficulty. In this work, we conduct a series of empirical analyses which suggest that the combination of non-stationarity with gradient pathologies, due to suboptimal architectural choices, underlie the challenges of scale. We propose a series of direct interventions that stabilize gradient flow, enabling robust performance across a range of network depths and widths. Our interventions are simple to implement and compatible with well-established algorithms, and result in an effective mechanism that enables strong performance even at large scales. We validate our findings on a variety of agents and suites of environments",
    "checked": true,
    "id": "30b9774d25b60707d4af6c2e2266fde56f2861f5",
    "semantic_title": "stable gradients for stable learning at scale in deep reinforcement learning",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=2sG4ebgqBd": {
    "title": "SpecMER: Fast Protein Generation with K-mer Guided Speculative Decoding",
    "volume": "spotlight",
    "abstract": "Autoregressive models have transformed protein engineering by enabling the generation of novel protein sequences beyond those found in nature. However, their sequential inference introduces significant latency, limiting their utility in high-throughput protein screening. Speculative decoding accelerates generation by employing a lightweight draft model to sample tokens, which a larger target model then verifies and refines. Yet in protein sequence generation, draft models are typically agnostic to the structural and functional constraints of the target protein, leading to biologically implausible outputs and a shift in the likelihood distribution of generated sequences. We introduce SpecMER (Speculative Decoding via k-mer Guidance), a novel framework that incorporates biological, structural, and functional priors using k-mer motifs extracted from multiple sequence alignments. By scoring candidate sequences in parallel and selecting those most consistent with known biological patterns, SpecMER significantly improves sequence plausibility while retaining the efficiency of speculative decoding. SpecMER achieves 24–32% speedup over standard autoregressive decoding, along with higher acceptance rates and improved sequence likelihoods",
    "checked": true,
    "id": "5836c9f8d10dd40085682c660e8f18d94d9681a0",
    "semantic_title": "specmer: fast protein generation with k-mer guided speculative decoding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4QVLKwgg3S": {
    "title": "SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs",
    "volume": "spotlight",
    "abstract": "Large language models (LLMs) power many modern applications, but serving them at scale remains costly and resource-intensive. Current server-centric systems overlook consumer-grade GPUs at the edge. We introduce SpecEdge, an edge-assisted inference framework that splits LLM workloads between edge and server GPUs using a speculative decoding scheme, exchanging only token outputs over the network. SpecEdge employs proactive edge drafting to overlap edge token creation with server verification and pipeline-aware scheduling that interleaves multiple user requests to increase server-side throughput. Experiments show SpecEdge enhances overall cost efficiency by **1.91×** through achieving **2.22×** server throughput, and reduces inter token latency by **11.24\\%** compared to a server-only baseline, introducing a scalable, cost-effective paradigm for LLM serving. The code is available at https://github.com/kaist-ina/specedge",
    "checked": true,
    "id": "c9a5de2874b7211fef5c9c4e008f3bd2a768a642",
    "semantic_title": "specedge: scalable edge-assisted serving framework for interactive llms",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=baBhSzaSHI": {
    "title": "DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models",
    "volume": "spotlight",
    "abstract": "Understanding and explaining the behavior of machine learning models is essential for building transparent and trustworthy AI systems. We introduce DEXTER, a data-free framework that employs diffusion models and large language models to generate global, textual explanations of visual classifiers. DEXTER operates by optimizing text prompts to synthesize class-conditional images that strongly activate a target classifier. These synthetic samples are then used to elicit detailed natural language reports that describe class-specific decision patterns and biases. Unlike prior work, DEXTER enables natural language explanation about a classifier's decision process without access to training data or ground-truth labels. We demonstrate DEXTER's flexibility across three tasks—activation maximization, slice discovery and debiasing, and bias explanation—each illustrating its ability to uncover the internal mechanisms of visual classifiers. Quantitative and qualitative evaluations, including a user study, show that DEXTER produces accurate, interpretable outputs. Experiments on ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms existing approaches in global model explanation and class-level bias reporting. Code is available at https://github.com/perceivelab/dexter",
    "checked": true,
    "id": "07c7f73f9f291c521603cd9fad514c219fb26d6b",
    "semantic_title": "dexter: diffusion-guided explanations with textual reasoning for vision models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nYg6Qzm5xS": {
    "title": "What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains",
    "volume": "spotlight",
    "abstract": "In-context learning (ICL) is a hallmark capability of transformers, through which trained models learn to adapt to new tasks by leveraging information from the input context. Prior work has shown that ICL emerges in transformers due to the presence of special circuits called induction heads. Given the equivalence between induction heads and conditional $k$-grams, a recent line of work modeling sequential inputs as Markov processes has revealed the fundamental impact of model depth on its ICL capabilities: while a two-layer transformer can efficiently represent a conditional $1$-gram model, its single-layer counterpart cannot solve the task unless it is exponentially large. However, for higher order Markov sources, the best known constructions require at least three layers (each with a single attention head) - leaving open the question: *can a two-layer single-head transformer represent any $k^{\\text{th}}$-order Markov process?* In this paper, we precisely address this and theoretically show that a two-layer transformer with one head per layer can indeed represent any conditional $k$-gram. Thus, our result provides the tightest known characterization of the interplay between transformer depth and Markov order for ICL. Building on this, we further analyze the learning dynamics of our two-layer construction, focusing on a simplified variant for first-order Markov chains, illustrating how effective in-context representations emerge during training. Together, these results deepen our current understanding of transformer-based ICL and illustrate how even shallow architectures can surprisingly exhibit strong ICL capabilities on structured sequence modeling tasks",
    "checked": true,
    "id": "443d1cd927db8d8df9f51016741a4ce7fbadf1b3",
    "semantic_title": "what one cannot, two can: two-layer transformers provably represent induction heads on any-order markov chains",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gaHjGx1cMh": {
    "title": "Sample-Adaptivity Tradeoff in On-Demand Sampling",
    "volume": "spotlight",
    "abstract": "We study the tradeoff between sample complexity and round complexity in *on-demand sampling*, where the learning algorithm adaptively samples from $k$ distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an $r$-round algorithm scales approximately as $dk^{\\Theta(1/r)} / \\epsilon$. For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of $\\widetilde O((d + k) / \\epsilon^2)$ within $\\widetilde O(\\sqrt{k})$ rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the $\\widetilde O(\\sqrt{k})$-round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS",
    "checked": false,
    "id": "327b2725168261b6b3f62b0d73a5b8f77d940187",
    "semantic_title": "educational mismatch and productivity: evidence from leed data on italian firms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oo9KXaM6Mu": {
    "title": "Online Strategic Classification With Noise and Partial Feedback",
    "volume": "spotlight",
    "abstract": "In this paper, we study an online strategic classification problem, where a principal aims to learn an accurate binary linear classifier from sequentially arriving agents. For each agent, the principal announces a classifier. The agent can strategically exercise costly manipulations on his features to be classified as the favorable positive class. The principal is unaware of the true feature-label distribution, but observes all reported features and only labels of positively classified agents. We assume that the true feature-label distribution is given by a halfspace model subject to arbitrary feature-dependent bounded noise (i.e., Massart Noise). This problem faces the combined challenges of agents' strategic feature manipulations, partial label observations, and label noises. We tackle these challenges by a novel learning algorithm. We show that the proposed algorithm yields classifiers that converge to the clairvoyant optimal one and attains a regret rate of $ O(\\sqrt{T})$ up to poly-logarithmic and constant factors over $T$ cycles",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T6RkYsuoMW": {
    "title": "Partition-Then-Adapt: Combating Prediction Bias for Reliable Multi-Modal Test-Time Adaptation",
    "volume": "spotlight",
    "abstract": "Existing test-time adaptation (TTA) methods primarily focus on scenarios involving domain shifts in a single modality. However, they often prove ineffective when multiple modalities simultaneously undergo domain shifts, as they struggle to identify and utilize reliable samples within testing batches amid severe prediction bias. To address this problem, we propose Partition-Then-Adapt (PTA), a novel approach combating prediction bias for TTA with multi-modal domain shifts. PTA comprises two key components: Partition and Debiased Reweighting (PDR) and multi-modal Attention-Guided Alignment (AGA). Specifically, PDR evaluates each sample's predicted label frequency relative to the batch average, partitioning the batch into potential reliable and unreliable subsets. It then reweights each sample by jointly assessing its bias and confidence levels through a quantile-based approach. By applying weighted entropy loss, PTA simultaneously promotes learning from reliable subsets and discourages reliance on unreliable ones. Moreover, AGA regularizes PDR to focus on semantically meaningful multi-modal cues. Extensive experiments validate the effectiveness of PTA, surpassing state-of-the-art method by 6.1\\% on Kinetics50-MC and 5.8\\% on VGGSound-MC, respectively. Code of this paper is available at https://github.com/MPI-Lab/PTA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=px1OlCcqkj": {
    "title": "Strategic Hypothesis Testing",
    "volume": "spotlight",
    "abstract": "We examine hypothesis testing within a principal-agent framework, where a strategic agent, holding private beliefs about the effectiveness of a product, submits data to a principal who decides on approval. The principal employs a hypothesis testing rule, aiming to pick a p-value threshold that balances false positives and false negatives while anticipating the agent's incentive to maximize expected profitability. Building on prior work, we develop a game-theoretic model that captures how the agent's participation and reporting behavior respond to the principal's statistical decision rule. Despite the complexity of the interaction, we show that the principal's errors exhibit clear monotonic behavior when segmented by an efficiently computable critical p-value threshold, leading to an interpretable characterization of their optimal p-value threshold. We empirically validate our model and these insights using publicly available data on drug approvals. Overall, our work offers a comprehensive perspective on strategic interactions within the hypothesis testing framework, providing technical and regulatory insights",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L51U5RSFBo": {
    "title": "Accelerating Optimization via Differentiable Stopping Time",
    "volume": "spotlight",
    "abstract": "A common approach for accelerating optimization algorithms is to minimize the loss achieved in a fixed time, which enables a differentiable framework with respect to the algorithm's hyperparameters. In contrast, the complementary objective of minimizing the time to reach a target loss is traditionally considered non-differentiable. To address this limitation, we propose a differentiable discrete stopping time and theoretically justify it based on its connection to continuous differential equations. We design an efficient algorithm to compute its sensitivities, thereby enabling a new differentiable formulation for directly accelerating algorithms. We demonstrate its effectiveness in applications such as online hyperparameter tuning and learning to optimize. Our proposed methods show superior performance in comprehensive experiments across various problems, which confirms their effectiveness",
    "checked": true,
    "id": "1c116d23d4af99b569d7e778d80de636f296cda4",
    "semantic_title": "accelerating optimization via differentiable stopping time",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wAq0ZLxrGq": {
    "title": "Tight Generalization Bounds for Large-Margin Halfspaces",
    "volume": "spotlight",
    "abstract": "We prove the first generalization bound for large-margin halfspaces that is asymptotically tight in the tradeoff between the margin, the fraction of training points with the given margin, the failure probability and the number of training points",
    "checked": true,
    "id": "473330d45f8726a7a8437753d73acd2f2fa8ba10",
    "semantic_title": "tight generalization bounds for large-margin halfspaces",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SYcggdxX6W": {
    "title": "Word-Level Emotional Expression Control in Zero-Shot Text-to-Speech Synthesis",
    "volume": "spotlight",
    "abstract": "While emotional text-to-speech (TTS) has made significant progress, most existing research remains limited to utterance-level emotional expression and fails to support word-level control. Achieving word-level expressive control poses fundamental challenges, primarily due to the complexity of modeling multi-emotion transitions and the scarcity of annotated datasets that capture intra-sentence emotional and prosodic variation. In this paper, we propose WeSCon, the first self-training framework that enables word-level control of both emotion and speaking rate in a pretrained zero-shot TTS model, without relying on datasets containing intra-sentence emotion or speed transitions. Our method introduces a transition-smoothing strategy and a dynamic speed control mechanism to guide the pretrained TTS model in performing word-level expressive synthesis through a multi-round inference process. To further simplify the inference, we incorporate a dynamic emotional attention bias mechanism and fine-tune the model via self-training, thereby activating its ability for word-level expressive control in an end-to-end manner. Experimental results show that WeSCon effectively overcomes data scarcity, achieving state-of-the-art performance in word-level emotional expression control while preserving the strong zero-shot synthesis capabilities of the original TTS model",
    "checked": true,
    "id": "459098aa1c6aea044f5480a1ed1dd56b48aacf19",
    "semantic_title": "word-level emotional expression control in zero-shot text-to-speech synthesis",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Z0OFReqkT": {
    "title": "UMoE: Unifying Attention and FFN with Shared Experts",
    "volume": "spotlight",
    "abstract": "Sparse Mixture of Experts (MoE) architectures have emerged as a promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model performance. However, existing attention-based MoE layers require specialized implementations and demonstrate suboptimal performance compared to their FFN-based counterparts. In this paper, we aim to unify MoE designs in attention and FFN layers by introducing a novel reformulation of the attention mechanism, that reveals an underlying FFN-like structure within attention modules. Our proposed architecture, UMoE, achieves superior performance through attention-based MoE layers while enabling efficient parameter sharing between FFN and attention components",
    "checked": true,
    "id": "8c0b256d966f975c39791a9a29362d686f7ebbc3",
    "semantic_title": "umoe: unifying attention and ffn with shared experts",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dZ94ZS410X": {
    "title": "RF-Agent: Automated Reward Function Design via Language Agent Tree Search",
    "volume": "spotlight",
    "abstract": "Designing efficient reward functions for low-level control tasks is a challenging problem. Recent research aims to reduce reliance on expert experience by using Large Language Models (LLMs) with task information to generate dense reward functions. These methods typically rely on training results as feedback, iteratively generating new reward functions with greedy or evolutionary algorithms. However, they suffer from poor utilization of historical feedback and inefficient search, resulting in limited improvements in complex control tasks. To address this challenge, we propose RF-Agent, a framework that treats LLMs as language agents and frames reward function design as a sequential decision-making process, enhancing optimization through better contextual reasoning. RF-Agent integrates Monte Carlo Tree Search (MCTS) to manage the reward design and optimization process, leveraging the multi-stage contextual reasoning ability of LLM. This approach better utilizes historical information and improves search efficiency to identify promising reward functions. Outstanding experimental results in 17 diverse low-level control tasks demonstrate the effectiveness of our method",
    "checked": false,
    "id": "752556ce26f72535664814f7e7e96704141adf11",
    "semantic_title": "scriptdoctor: automatic generation of puzzlescript games via large language models and tree search",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8wvOMQ2Olw": {
    "title": "GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning",
    "volume": "spotlight",
    "abstract": "Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ranks 32–64, yet its accuracy stagnates or declines at higher ranks, still falling short of full fine-tuning (FFT) performance. We identify the root cause as LoRA's structural bottleneck, which introduces gradient entanglement to the unrelated input channels and distorts gradient propagation. To address this, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA) that partitions weight matrices into sub-blocks, each with its own low-rank adapter. With negligible computational or storage cost, GraLoRA overcomes LoRA's limitations, effectively increases the representational capacity, and more closely approximates FFT behavior. Experiments on code generation, commonsense reasoning, mathematical reasoning, general language understanding, and image generation benchmarks show that GraLoRA consistently outperforms LoRA and other baselines, achieving up to +8.5\\% absolute gain in Pass@1 on HumanEval+. These improvements hold across model sizes and rank settings, making GraLoRA a scalable and robust solution for PEFT",
    "checked": true,
    "id": "3bb02963d44065135067ebd265b3e6a8672a04cc",
    "semantic_title": "gralora: granular low-rank adaptation for parameter-efficient fine-tuning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=3a18D8IeQ1": {
    "title": "Quantization-Free Autoregressive Action Transformer",
    "volume": "spotlight",
    "abstract": "Current transformer-based imitation learning approaches introduce discrete action representations and train an autoregressive transformer decoder on the resulting latent code. However, the initial quantization breaks the continuous structure of the action space thereby limiting the capabilities of the generative model. We propose a quantization-free method instead that leverages Generative Infinite-Vocabulary Transformers (GIVT) as a direct, continuous policy parametrization for autoregressive transformers. This simplifies the imitation learning pipeline while achieving state-of-the-art performance on a variety of popular simulated robotics tasks. We enhance our policy roll-outs by carefully studying sampling algorithms, further improving the results",
    "checked": true,
    "id": "2da15b91264f4f08e57b9534f6ea7392b07875e1",
    "semantic_title": "quantization-free autoregressive action transformer",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=VA5P0rUZPx": {
    "title": "LLM-Explorer: A Plug-in Reinforcement Learning Policy Exploration Enhancement Driven by Large Language Models",
    "volume": "spotlight",
    "abstract": "Policy exploration is critical in reinforcement learning (RL), where existing approaches include $\\epsilon$-greedy, Gaussian process, etc. However, these approaches utilize preset stochastic processes and are indiscriminately applied in all kinds of RL tasks without considering task-specific features that influence policy exploration. Moreover, during RL training, the evolution of such stochastic processes is rigid, which typically only incorporates a decay in the variance, failing to adjust flexibly according to the agent's real-time learning status. Inspired by the analyzing and reasoning capability of large language models (LLMs), we design **LLM-Explorer** to adaptively generate task-specific exploration strategies with LLMs, enhancing the policy exploration in RL. In our design, we sample the learning trajectory of the agent during the RL training in a given task and prompt the LLM to analyze the agent's current policy learning status and then generate a probability distribution for future policy exploration. Updating the probability distribution periodically, we derive a stochastic process specialized for the particular task and dynamically adjusted to adapt to the learning process. Our design is a plug-in module compatible with various widely applied RL algorithms, including the DQN series, DDPG, TD3, and any possible variants developed based on them. Through extensive experiments on the Atari and MuJoCo benchmarks, we demonstrate LLM-Explorer's capability to enhance RL policy exploration, achieving an average performance improvement up to 37.27%. Our code is open-source at https://github.com/tsinghua-fib-lab/LLM-Explorer for reproducibility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FEugj28qhC": {
    "title": "BayeSQP: Bayesian Optimization through Sequential Quadratic Programming",
    "volume": "spotlight",
    "abstract": "We introduce BayeSQP, a novel algorithm for general black-box optimization that merges the structure of sequential quadratic programming with concepts from Bayesian optimization. BayeSQP employs second-order Gaussian process surrogates for both the objective and constraints to jointly model the function values, gradients, and Hessian from only zero-order information. At each iteration, a local subproblem is constructed using the GP posterior estimates and solved to obtain a search direction. Crucially, the formulation of the subproblem explicitly incorporates uncertainty in both the function and derivative estimates, resulting in a tractable second-order cone program for high probability improvements under model uncertainty. A subsequent one-dimensional line search via constrained Thompson sampling selects the next evaluation point. Empirical results show that BayeSQP outperforms state-of-the-art methods in specific high-dimensional settings. Our algorithm offers a principled and flexible framework that bridges classical optimization techniques with modern approaches to black-box optimization",
    "checked": false,
    "id": "174e8d3bedd75ea17adcde815d7c6179f50f97a6",
    "semantic_title": "constrained bayesian optimization with merit functions",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=aTiMLVePXi": {
    "title": "Practical do-Shapley Explanations with Estimand-Agnostic Causal Inference",
    "volume": "spotlight",
    "abstract": "Among explainability techniques, SHAP stands out as one of the most popular, but often overlooks the causal structure of the problem. In response, do-SHAP employs interventional queries, but its reliance on estimands hinders its practical application. To address this problem, we propose the use of estimand-agnostic approaches, which allow for the estimation of any identifiable query from a single model, making do-SHAP feasible on complex graphs. We also develop a novel algorithm to significantly accelerate its computation at a negligible cost, as well as a method to explain inaccessible Data Generating Processes. We demonstrate the estimation and computational performance of our approach, and validate it on two real-world datasets, highlighting its potential in obtaining reliable explanations",
    "checked": true,
    "id": "08528b6c904cd0cb43859d87b856774fdb490278",
    "semantic_title": "practical do-shapley explanations with estimand-agnostic causal inference",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IMmkDMqFMU": {
    "title": "AlphaZero Neural Scaling and Zipf's Law: a Tale of Board Games and Power Laws",
    "volume": "spotlight",
    "abstract": "Neural scaling laws are observed in a range of domains, to date with no universal understanding of why they occur. Recent theories suggest that loss power laws arise from Zipf's law, a power law observed in domains like natural language. One theory suggests that language scaling laws emerge when Zipf-distributed task quanta are learned in descending order of frequency. In this paper we examine power-law scaling in AlphaZero, a reinforcement learning algorithm, using a model of language-model scaling. We find that game states in training and inference data scale with Zipf's law, which is known to arise from the tree structure of the environment, and examine the correlation between scaling-law and Zipf's-law exponents. In agreement with the quanta scaling model, we find that agents optimize state loss in descending order of frequency, even though this order scales inversely with modelling complexity. We also find that inverse scaling, the failure of models to improve with size, is correlated with unusual Zipf curves where end-game states are among the most frequent states. We show evidence that larger models shift their focus to these less-important states, sacrificing their understanding of important early-game states",
    "checked": true,
    "id": "70864e3e9771b2414cd4801fc3a331f0c2c3d7f4",
    "semantic_title": "alphazero neural scaling and zipf's law: a tale of board games and power laws",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=QBnfYm6Naa": {
    "title": "Set Smoothness Unlocks Clarke Hyper-stationarity in Bilevel Optimization",
    "volume": "spotlight",
    "abstract": "Solving bilevel optimization (BLO) problems to global optimality is generally intractable. A common surrogate is to compute a hyper-stationary point—a stationary point of the hyper-objective function obtained by minimizing or maximizing the upper-level objective over the lower-level solution set. Existing methods, however, either provide weak notions of stationarity or require restrictive assumptions to guarantee the smoothness of hyper-objective functions. In this paper, we eliminate these impractical assumptions and show that strong (Clarke) hyper-stationarity remains computable even when the hyper-objective is nonsmooth. Our key ingredient is a new structural property, called set smoothness, which captures the variational dependence of the lower-level solution set on the upper-level variable. We prove that this property holds for a broad class of BLO problems and ensures weak convexity (resp. concavity) of pessimistic (resp. optimistic) hyper-objective functions. Building on this foundation, we show that a zeroth-order algorithm that computes approximate Clarke hyper-stationary points with non-asymptotic convergence guarantees. To the best of our knowledge, this is the first computational guarantee for Clarke-type stationarity in nonsmooth BLO. Beyond this specific application, the set smoothness property emerges as a structural concept of independent interest, with potential to inform the analysis of broader classes of optimization and variational problems",
    "checked": true,
    "id": "18ce43ebffbd0660ec4beb2b69b545c58d5db30a",
    "semantic_title": "set smoothness unlocks clarke hyper-stationarity in bilevel optimization",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=5JcDVsV8pf": {
    "title": "The Computational Advantage of Depth in Learning High-Dimensional Hierarchical Targets",
    "volume": "spotlight",
    "abstract": "Understanding the advantages of deep neural networks trained by gradient descent (GD) compared to shallow models remains an open theoretical challenge. In this paper, we introduce a class of target functions (single and multi-index Gaussian hierarchical targets) that incorporate a hierarchy of latent subspace dimensionalities. This framework enables us to analytically study the learning dynamics and generalization performance of deep networks compared to shallow ones in the high-dimensional limit. Specifically, our main theorem shows that feature learning with GD successively reduces the effective dimensionality, transforming a high-dimensional problem into a sequence of lower-dimensional ones. This enables learning the target function with drastically less samples than with shallow networks. While the results are proven in a controlled training setting, we also discuss more common training procedures and argue that they learn through the same mechanisms. These findings open the way to further quantitative studies of the crucial role of depth in learning hierarchical structures with deep networks",
    "checked": false,
    "id": "7d4a7d6ce54f3dee7c47a08eb54edd58ceabaebf",
    "semantic_title": "the computational advantage of depth: learning high-dimensional hierarchical functions with gradient descent",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=WcUo7Z2Jnh": {
    "title": "Thoughts Are All Over the Place: On the Underthinking of Long Reasoning Models",
    "volume": "spotlight",
    "abstract": "Long reasoning models (LRMs) such as OpenAI's o1 and DeepSeek's R1 have demonstrated remarkable abilities in complex reasoning tasks by scaling test-time compute and exhibiting human-like deep thinking. However, we identify a phenomenon we term underthinking, where LRMs frequently switch between different reasoning thoughts without sufficiently exploring promising paths to reach a correct solution. This behavior leads to inadequate depth of reasoning and decreased performance, particularly on challenging mathematical problems. To systematically analyze this issue, we conduct experiments on three challenging test sets and two representative open-source LRMs, revealing that frequent thought switching correlates with incorrect responses. We introduce a novel metric to quantify underthinking by measuring token efficiency in incorrect answers. To address underthinking, we propose a decoding strategy with thought switching penalty (Tip) that discourages premature transitions between thoughts, encouraging deeper exploration of each reasoning path. Experimental results demonstrate that our approach improves accuracy across challenging datasets without requiring model fine-tuning. Our findings contribute to understanding reasoning inefficiencies in LRMs and offer a practical solution to enhance their problem-solving capabilities. Our code is open-source and available at https://github.com/wangyuenlp/underthinking",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=24tuzE5KZc": {
    "title": "OPTFM: A Scalable Multi-View Graph Transformer for Hierarchical Pre-Training in Combinatorial Optimization",
    "volume": "spotlight",
    "abstract": "Foundation Models (FMs) have demonstrated remarkable success in fields like computer vision and natural language processing, yet their application to combinatorial optimization remains underexplored. Optimization problems, often modeled as graphs, pose unique challenges due to their diverse structures, varying distributions, and NP-hard complexity. To address these challenges, we propose OPTFM, the first graph foundation model for general combinatorial optimization. OPTFM introduces a scalable multi-view graph transformer with hybrid self-attention and cross-attention to model large-scale heterogeneous graphs in $O(N)$ time complexity while maintaining semantic consistency throughout the attention computation. A Dual-level pre-training framework integrates node-level graph reconstruction and instance-level contrastive learning, enabling robust and adaptable representations at multiple levels. Experimental results across diverse optimization tasks show that models trained on OPTFM embeddings without fine-tuning consistently outperform task-specific approaches, establishing a new benchmark for solving combinatorial optimization problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cjjPn1EIwq": {
    "title": "ESCA: Contextualizing Embodied Agents via Scene-Graph Generation",
    "volume": "spotlight",
    "abstract": "Multi-modal large language models (MLLMs) are making rapid progress toward general-purpose embodied agents. However, existing MLLMs do not reliably capture fine-grained links between low-level visual features and high-level textual semantics, leading to weak grounding and inaccurate perception. To overcome this challenge, we propose ESCA, a framework that contextualizes embodied agents by grounding their perception in spatial-temporal scene graphs. At its core is SGCLIP, a novel, open-domain, promptable foundation model for generating scene graphs that is based on CLIP. SGCLIP is trained on 87K+ open-domain videos using a neurosymbolic pipeline that aligns automatically generated captions with scene graphs produced by the model itself, eliminating the need for human-labeled annotations. We demonstrate that SGCLIP excels in both prompt-based inference and task-specific fine-tuning, achieving state-of-the-art results on scene graph generation and action localization benchmarks. ESCA with SGCLIP improves perception for embodied agents based on both open-source and commercial MLLMs, achieving state of-the-art performance across two embodied environments. Notably, ESCA significantly reduces agent perception errors and enables open-source models to surpass proprietary baselines. We release the source code for SGCLIP model training at https://github.com/video-fm/LASER and for the embodied agent at https://github.com/video-fm/ESCA",
    "checked": true,
    "id": "3411753f3451d7f710bbcdf68a3cde7ae9aa8b9e",
    "semantic_title": "esca: contextualizing embodied agents via scene-graph generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=McPNQEDEZE": {
    "title": "Regret Bounds for Adversarial Contextual Bandits with General Function Approximation and Delayed Feedback",
    "volume": "spotlight",
    "abstract": "We present regret minimization algorithms for the contextual multi-armed bandit (CMAB) problem over $K$ actions in the presence of delayed feedback, a scenario where loss observations arrive with delays chosen by an adversary. As a preliminary result, assuming direct access to a finite policy class $\\Pi$ we establish an optimal expected regret bound of $ O (\\sqrt{KT \\log |\\Pi|} + \\sqrt{D \\log |\\Pi|)} $ where $D$ is the sum of delays. For our main contribution, we study the general function approximation setting over a (possibly infinite) contextual loss function class $ \\mathcal{F} $ with access to an online least-square regression oracle $\\mathcal{O}$ over $\\mathcal{F}$. In this setting, we achieve an expected regret bound of $O(\\sqrt{KTR_T(\\mathcal{O})} + \\sqrt{ d_{\\max} D \\beta})$ assuming FIFO order, where $d_{\\max}$ is the maximal delay, $R_T(\\mathcal{O})$ is an upper bound on the oracle's regret and $\\beta$ is a stability parameter associated with the oracle. We complement this general result by presenting a novel stability analysis of a Hedge-based version of Vovk's aggregating forecaster as an oracle implementation for least-square regression over a finite function class $\\mathcal{F}$ and show that its stability parameter $\\beta$ is bounded by $\\log |\\mathcal{F}|$, resulting in an expected regret bound of $O(\\sqrt{KT \\log |\\mathcal{F}|} + \\sqrt{d_{\\max} D \\log |\\mathcal{F}|})$ which is a $\\sqrt{d_{\\max}}$ factor away from the lower bound of $\\Omega(\\sqrt{KT \\log |\\mathcal{F}|} + \\sqrt{D \\log |\\mathcal{F}|})$ that we also present",
    "checked": true,
    "id": "54c55f6e0bf712999b5bd2d36616c3c52a360285",
    "semantic_title": "regret bounds for adversarial contextual bandits with general function approximation and delayed feedback",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KlzLtQV64O": {
    "title": "Towards Multi-Table Learning: A Novel Paradigm for Complementarity Quantification and Integration",
    "volume": "spotlight",
    "abstract": "Multi-table data integrate various entities and attributes, with potential interconnections between them. However, existing tabular learning methods often struggle to describe and leverage the underlying complementarity across distinct tables. To address this limitation, we propose the first unified paradigm for multi-table learning that systematically quantifies and integrates complementary information across tables. Specifically, we introduce a metric called complementarity strength (CS), which captures inter-table complementarity by incorporating relevance, similarity, and informativeness. For the first time, we systematically formulate the paradigm towards multi-table learning by establishing formal definitions of tasks and loss functions. Correspondingly, we present a network for multi-table learning that combines Adaptive Table encoder and Cross table Attention mechanism (ATCA-Net), achieving the simultaneous integration of complementary information from distinct tables. Extensive experiments show that ATCA-Net effectively leverages complementary information and that the CS metric accurately quantifies the richness of complementarity across multiple tables. To the best of our knowledge, this is the first work to establish theoretical and practical foundations for multi-table learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CrBWOjZoKc": {
    "title": "QFFT, Question-Free Fine-Tuning for Adaptive Reasoning",
    "volume": "spotlight",
    "abstract": "Recent advancements in Long Chain-of-Thought (CoT) reasoning models have improved performance on complex tasks, but they suffer from overthinking, which generates redundant reasoning steps, especially for simple questions. This paper revisits the reasoning patterns of Long and Short CoT models, observing that the Short CoT patterns offer concise reasoning efficiently, while the Long CoT patterns excel in challenging scenarios where the Short CoT patterns struggle. To enable models to leverage both patterns, we propose Question-Free Fine-Tuning (QFFT), a fine-tuning approach that removes the input question during training and learns exclusively from Long CoT responses. This approach enables the model to adaptively employ both reasoning patterns: it prioritizes the Short CoT patterns and activates the Long CoT patterns only when necessary. Experiments on various mathematical datasets demonstrate that QFFT reduces average response length by more than 50\\%, while achieving performance comparable to Supervised Fine-Tuning (SFT). Additionally, QFFT exhibits superior performance compared to SFT in noisy, out-of-domain, and low-resource scenarios",
    "checked": true,
    "id": "b4f39231109b9fed59a33bc54ef6a2922761bd41",
    "semantic_title": "qfft, question-free fine-tuning for adaptive reasoning",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=gfXBNBKx02": {
    "title": "Option-aware Temporally Abstracted Value for Offline Goal-Conditioned Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "Offline goal-conditioned reinforcement learning (GCRL) offers a practical learning paradigm in which goal-reaching policies are trained from abundant state–action trajectory datasets without additional environment interaction. However, offline GCRL still struggles with long-horizon tasks, even with recent advances that employ hierarchical policy structures, such as HIQL. Identifying the root cause of this challenge, we observe the following insight. Firstly, performance bottlenecks mainly stem from the high-level policy's inability to generate appropriate subgoals. Secondly, when learning the high-level policy in the long-horizon regime, the sign of the advantage estimate frequently becomes incorrect. Thus, we argue that improving the value function to produce a clear advantage estimate for learning the high-level policy is essential. In this paper, we propose a simple yet effective solution: _**Option-aware Temporally Abstracted**_ value learning, dubbed **OTA**, which incorporates temporal abstraction into the temporal-difference learning process. By modifying the value update to be _option-aware_, our approach contracts the effective horizon length, enabling better advantage estimates even in long-horizon regimes. We experimentally show that the high-level policy learned using the OTA value function achieves strong performance on complex tasks from OGBench, a recently proposed offline GCRL benchmark, including maze navigation and visual robotic manipulation environments. Our code is available at https://github.com/ota-v/ota-v",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dpllevHMbc": {
    "title": "Functional Scaling Laws in Kernel Regression: Loss Dynamics and Learning Rate Schedules",
    "volume": "spotlight",
    "abstract": "Scaling laws have emerged as a unifying lens for understanding and guiding the training of large language models (LLMs). However, existing studies predominantly focus on the final-step loss, leaving open whether the entire $\\textit{loss dynamics}$ obey similar laws and, crucially, how the $\\textit{learning rate schedule}$ (LRS) shapes them. We address these gaps in a controlled theoretical setting by analyzing stochastic gradient descent (SGD) on a power-law kernel regression model. The key insight is a novel $\\textbf{intrinsic-time}$ viewpoint, which captures the training progress more faithfully than iteration count. We then establish a $\\textbf{Functional Scaling Law (FSL)}$ that captures the full loss trajectory under arbitrary LRSs, with the schedule's influence entering through a simple convolutional functional. We further instantiate the theory for three representative LRSs---constant, exponential decay, and warmup–stable–decay (WSD)---and derive explicit scaling relations in both data- and compute-limited regimes. These comparisons explain key empirical phenomena: (i) higher-capacity models are more data- and compute-efficient; (ii) learning-rate decay improves training efficiency; and (iii) WSD-type schedules outperform pure decay. Finally, experiments on LLMs ranging from 0.1B to 1B parameters demonstrate the practical relevance of FSL as a surrogate model for fitting and predicting loss trajectories in large-scale pre-training",
    "checked": true,
    "id": "f0ccf58d7139ccbc4ac75c92f832d344282b4780",
    "semantic_title": "functional scaling laws in kernel regression: loss dynamics and learning rate schedules",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E3oNDQ8e9r": {
    "title": "GSRF: Complex-Valued 3D Gaussian Splatting for Efficient Radio-Frequency Data Synthesis",
    "volume": "spotlight",
    "abstract": "Synthesizing radio-frequency (RF) data given the transmitter and receiver positions, e.g., received signal strength indicator (RSSI), is critical for wireless networking and sensing applications, such as indoor localization. However, it remains challenging due to complex propagation interactions, including reflection, diffraction, and scattering. State-of-the-art neural radiance field (NeRF)-based methods achieve high-fidelity RF data synthesis but are limited by long training times and high inference latency. We introduce GSRF, a framework that extends 3D Gaussian Splatting (3DGS) from the optical domain to the RF domain, enabling efficient RF data synthesis. GSRF realizes this adaptation through three key innovations: First, it introduces complex-valued 3D Gaussians with a hybrid Fourier–Legendre basis to model directional and phase-dependent radiance. Second, it employs orthographic splatting for efficient ray–Gaussian intersection identification. Third, it incorporates a complex-valued ray tracing algorithm, executed on RF-customized CUDA kernels and grounded in wavefront propagation principles, to synthesize RF data in real time. Evaluated across various RF technologies, GSRF preserves high-fidelity RF data synthesis while achieving significant improvements in training efficiency, shorter training time, and reduced inference latency",
    "checked": true,
    "id": "5209dcbbfb646ad14dfe0bc793be919f4aec2264",
    "semantic_title": "gsrf: complex-valued 3d gaussian splatting for efficient radio-frequency data synthesis",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=b7waOsMnq8": {
    "title": "Sharp Gaussian approximations for Decentralized Federated Learning",
    "volume": "spotlight",
    "abstract": "Federated Learning has gained traction in privacy-sensitive collaborative environments, with local SGD emerging as a key optimization method in decentralized settings. While its convergence properties are well-studied, asymptotic statistical guarantees beyond convergence remain limited. In this paper, we present two generalized Gaussian approximation results for local SGD and explore their implications. First, we prove a Berry-Esseen theorem for the final local SGD iterates, enabling valid multiplier bootstrap procedures. Second, motivated by robustness considerations, we introduce two distinct time-uniform Gaussian approximations for the entire trajectory of local SGD. The time-uniform approximations support Gaussian bootstrap-based tests for detecting adversarial attacks. Extensive simulations are provided to support our theoretical results",
    "checked": true,
    "id": "dfc9347eb81b776da7395a5c1eea31ac860c841a",
    "semantic_title": "sharp gaussian approximations for decentralized federated learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=77eEDRhPkQ": {
    "title": "DAPO : Improving Multi-Step Reasoning Abilities of Large Language Models with Direct Advantage-Based Policy Optimization",
    "volume": "spotlight",
    "abstract": "The role of reinforcement learning (RL) in enhancing the reasoning of large language models (LLMs) is becoming increasingly significant. Despite the success of RL in many scenarios, there are still many challenges in improving the reasoning of LLMs. One key challenge is the sparse reward, which introduces more training variance in policy optimization and makes it difficult to obtain a good estimation for value function in Actor-Critic (AC) methods. To address these issues, we introduce Direct Advantage-Based Policy Optimization (DAPO), a novel step-level offline RL algorithm with theoretical guarantees for enhancing the reasoning abilities of LLMs. Unlike response-level methods (such as DPO and GRPO) that the update directions of all reasoning steps are governed by the outcome reward uniformly, DAPO employs a critic function to provide step-level dense signals for policy optimization. Additionally, the actor and critic in DAPO are trained independently, ensuring that critic is a good estimation of true state value function and avoiding the co-training instability observed in standard AC methods. We train DAPO on mathematical and code problems and then evaluate its performance on multiple benchmarks. Our results show that DAPO can effectively enhance the mathematical and code capabilities on both SFT models and RL models, demonstrating the effectiveness of DAPO",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TDFSKAspoQ": {
    "title": "MGUP: A Momentum-Gradient Alignment Update Policy for Stochastic Optimization",
    "volume": "spotlight",
    "abstract": "Efficient optimization is essential for training large language models. Although intra-layer selective updates have been explored, a general mechanism that enables fine-grained control while ensuring convergence guarantees is still lacking. To bridge this gap, we propose \\textbf{MGUP}, a novel mechanism for selective updates. \\textbf{MGUP} augments standard momentum-based optimizers by applying larger step-sizes to a selected fixed proportion of parameters in each iteration, while applying smaller, non-zero step-sizes to the rest. As a nearly {plug-and-play} module, \\textbf{MGUP} seamlessly integrates with optimizers such as AdamW, Lion, and Muon. This yields powerful variants such as \\textbf{MGUP-AdamW}, \\textbf{MGUP-Lion}, and \\textbf{MGUP-Muon}. Under standard assumptions, we provide theoretical convergence guarantees for \\textbf{MGUP-AdamW} (without weight decay) in stochastic optimization. Extensive experiments across diverse tasks, including MAE pretraining, LLM pretraining, and downstream fine-tuning, demonstrate that our \\textbf{MGUP}-enhanced optimizers achieve superior or more stable performance compared to their original base optimizers. We offer a principled, versatile, and theoretically grounded strategy for efficient intra-layer selective updates, accelerating and stabilizing the training of large-scale models. The code is publicly available at https://github.com/MaeChd/MGUP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V8dGVO5Xpg": {
    "title": "Unveiling the Power of Multiple Gossip Steps: A Stability-Based Generalization Analysis in Decentralized Training",
    "volume": "spotlight",
    "abstract": "Decentralized training removes the centralized server, making it a communication-efficient approach that can significantly improve training efficiency, but it often suffers from degraded performance compared to centralized training. Multi-Gossip Steps (MGS) serve as a simple yet effective bridge between decentralized and centralized training, significantly reducing experiment performance gaps. However, the theoretical reasons for its effectiveness and whether this gap can be fully eliminated by MGS remain open questions. In this paper, we derive upper bounds on the generalization error and excess error of MGS using stability analysis, systematically answering these two key questions. 1). Optimization Error Reduction: MGS reduces the optimization error bound at an exponential rate, thereby exponentially tightening the generalization error bound and enabling convergence to better solutions. 2). Gap to Centralization: Even as MGS approaches infinity, a non-negligible gap in generalization error remains compared to centralized mini-batch SGD ($\\mathcal{O}(T^{\\frac{c\\beta}{c\\beta +1}}/{n m})$ in centralized and $\\mathcal{O}(T^{\\frac{2c\\beta}{2c\\beta +2}}/{n m^{\\frac{1}{2c\\beta +2}}})$ in decentralized). Furthermore, we provide the first unified analysis of how factors like learning rate, data heterogeneity, node count, per-node sample size, and communication topology impact the generalization of MGS under non-convex settings without the bounded gradients assumption, filling a critical theoretical gap in decentralized training. Finally, promising experiments on CIFAR datasets support our theoretical findings",
    "checked": true,
    "id": "d5ce8a7f879e86d9c12d2ea5e6284513e7d174f2",
    "semantic_title": "unveiling the power of multiple gossip steps: a stability-based generalization analysis in decentralized training",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oUf6lhK52B": {
    "title": "Enhancing Contrastive Learning with Variable Similarity",
    "volume": "spotlight",
    "abstract": "Contrastive learning has achieved remarkable success in self-supervised learning by pretraining a generalizable feature representation based on the augmentation invariance. Most existing approaches assume that different augmented views of the same instance (i.e., the *positive pairs*) remain semantically invariant. However, the augmentation results with *varying extent* may introduce semantic discrepancies or even content distortion, and thus the conventional (pseudo) supervision from augmentation invariance may lead to misguided learning objectives. In this paper, we propose a novel method called Contrastive Learning with Variable Similarity (CLVS) to accurately characterize the intrinsic similarity relationships between different augmented views. Our method dynamically adjusts the similarity based on the augmentation extent, and it ensures that strongly augmented views are always assigned lower similarity scores than weakly augmented ones. We provide a theoretical analysis to guarantee the effectiveness of the variable similarity in improving model generalizability. Extensive experiments demonstrate the superiority of our approach, achieving gains of 2.1\\% on ImageNet-100 and 1.4\\% on ImageNet-1k compared with the state-of-the-art methods",
    "checked": false,
    "id": "c93bad91bcd57f765e0609dc5214f40c542ccc9f",
    "semantic_title": "enhancing contrastive learning on graphs with node similarity",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=n3armuTFit": {
    "title": "Orient Anything V2: Unifying Orientation and Rotation Understanding",
    "volume": "spotlight",
    "abstract": "This work presents Orient Anything V2, an enhanced foundation model for unified understanding of object 3D orientation and rotation from single or paired images. Building upon Orient Anything V1, which defines orientation via a single unique front face, V2 extends this capability to handle objects with diverse rotational symmetries and directly estimate relative rotations. These improvements are enabled by four key innovations: 1) Scalable 3D assets synthesized by generative models, ensuring broad category coverage and balanced data distribution; 2) An efficient, model-in-the-loop annotation system that robustly identifies 0 to N valid front faces for each object; 3) A symmetry-aware, periodic distribution fitting objective that captures all plausible front-facing orientations, effectively modeling object rotational symmetry; 4) A multi-frame architecture that directly predicts relative object rotations. Extensive experiments show that Orient Anything V2 achieves state-of-the-art zero-shot performance on orientation estimation, 6DoF pose estimation, and object symmetry recognition across 11 widely used benchmarks. The model demonstrates strong generalization, significantly broadening the applicability of orientation estimation in diverse downstream tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7FhWZFoVem": {
    "title": "A Closer Look at Graph Transformers: Cross-Aggregation and Beyond",
    "volume": "spotlight",
    "abstract": "Graph Transformers (GTs), which effectively capture long-range dependencies and structural biases simultaneously, have recently emerged as promising alternatives to traditional Graph Neural Networks (GNNs). Advanced approaches for GTs to leverage topology information involve integrating GNN modules or modulating node attributes using positional encodings. Unfortunately, the underlying mechanism driving their effectiveness remains insufficiently understood. In this paper, we revisit these strategies and uncover a shared underlying mechanism—Cross Aggregation—that effectively captures the interaction between graph topology and node attributes. Building on this insight, we propose the Universal Graph Cross-attention Transformer (UGCFormer), a universal GT framework with linear computational complexity. The idea is to interactively learn the representations of graph topology and node attributes through a linearized Dual Cross-attention (DCA) module. In theory, this module can adaptively capture interactions between these two types of graph information, thereby achieving effective aggregation. To alleviate overfitting arising from the dual-channel design, we introduce a consistency constraint that enforces representational alignment. Extensive evaluations on multiple benchmark datasets demonstrate the effectiveness and efficiency of UGCFormer",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FUd016XD4d": {
    "title": "Latent Policy Barrier: Learning Robust Visuomotor Policies by Staying In-Distribution",
    "volume": "spotlight",
    "abstract": "Visuomotor policies trained via behavior cloning are vulnerable to covariate shift, where small deviations from expert trajectories can compound into failure. Common strategies to mitigate this issue involve expanding the training distribution through human-in-the-loop corrections or synthetic data augmentation. However, these approaches are often labor-intensive, rely on strong task assumptions, or compromise the quality of imitation. We introduce Latent Policy Barrier, a framework for robust visuomotor policy learning. Inspired by Control Barrier Functions, LPB treats the latent embeddings of expert demonstrations as an implicit barrier separating safe, in-distribution states from unsafe, out-of-distribution (OOD) ones. Our approach decouples the role of precise expert imitation and OOD recovery into two separate modules: a base diffusion policy solely on expert data, and a dynamics model trained on both expert and suboptimal policy rollout data. At inference time, the dynamics model predicts future latent states and optimizes them to stay within the expert distribution. Both simulated and real-world experiments show that LPB improves both policy robustness and data efficiency, enabling reliable manipulation from limited expert data and without additional human correction or annotation. More details are on our anonymous project website https://latentpolicybarrier.github.io",
    "checked": true,
    "id": "a75a9a8fa615c0f14cf98279406745ebe58e54d0",
    "semantic_title": "latent policy barrier: learning robust visuomotor policies by staying in-distribution",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=0TD3eO46gk": {
    "title": "Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models",
    "volume": "spotlight",
    "abstract": "In Transformer language models, activation vectors transform from current token embeddings to next token predictions as they pass through the model. To isolate a minimal form of this transformation, we identify language model subnetworks that make bigram predictions, naive next token predictions based only on the current token. We find that bigram subnetworks can be found in fully trained language models up to 1B parameters, and these subnetworks are critical for model performance even when they consist of less than 0.2% of model parameters. Bigram subnetworks are concentrated in the first Transformer MLP layer, and they overlap significantly with subnetworks trained to optimally prune a given model. Mechanistically, the bigram subnetworks often recreate a pattern from the full models where the first layer induces a sharp change that aligns activations with next token predictions rather than current token representations. Our results demonstrate that bigram subnetworks comprise a minimal subset of parameters that are both necessary and sufficient for basic next token predictions in language models, and they help drive the transformation from current to next token activations in the residual stream. These subnetworks can lay a foundation for studying more complex language model circuits by building up from a minimal circuit",
    "checked": true,
    "id": "4c266089e97f14f5e82bbb661c48d2b99f1b3c54",
    "semantic_title": "bigram subnetworks: mapping to next tokens in transformer language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=QudbVyFaTu": {
    "title": "Minimax-Optimal Univariate Function Selection in Sparse Additive Models: Rates, Adaptation, and the Estimation-Selection Gap",
    "volume": "spotlight",
    "abstract": "The sparse additive model (SpAM) offers a trade-off between interpretability and flexibility, and hence is a powerful model for high-dimensional research. This paper focuses on the variable selection, i.e., the univariate function selection problem in SpAM. We establish the minimax separation rates from both the perspectives of sparse multiple testing (FDR + FNR control) and support recovery (wrong recovery probability control). We further study how adaptation to unknown smoothness affects the minimax separation rate, and propose an adaptive selection procedure. Finally, we discuss the difference between estimation and selection in SpAM: Procedures achieving optimal function estimation may fail to achieve optimal univariate function selection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y0wDflmpLk": {
    "title": "Continuous Thought Machines",
    "volume": "spotlight",
    "abstract": "Biological brains demonstrate complex neural activity, where neural dynamics are critical to how brains process information. Most artificial neural networks ignore the complexity of individual neurons. We challenge that paradigm. By incorporating neuron-level processing and synchronization, we reintroduce neural timing as a foundational element. We present the Continuous Thought Machine (CTM), a model designed to leverage neural dynamics as its core representation. The CTM has two innovations: (1) neuron-level temporal processing}, where each neuron uses unique weight parameters to process incoming histories; and (2) neural synchronization as a latent representation. The CTM aims to strike a balance between neuron abstractions and biological realism. It operates at a level of abstraction that effectively captures essential temporal dynamics while remaining computationally tractable. We demonstrate the CTM's performance and versatility across a range of tasks, including solving 2D mazes, ImageNet-1K classification, parity computation, and more. Beyond displaying rich internal representations and offering a natural avenue for interpretation owing to its internal process, the CTM is able to perform tasks that require complex sequential reasoning. The CTM can also leverage adaptive compute, where it can stop earlier for simpler tasks, or keep computing when faced with more challenging instances. The goal of this work is to share the CTM and its associated innovations, rather than pushing for new state-of-the-art results. To that end, we believe the CTM represents a significant step toward developing more biologically plausible and powerful artificial intelligence systems. We provide an accompanying [interactive online demonstration](https://pub.sakana.ai/ctm/) and an [extended technical report](https://pub.sakana.ai/ctm/paper)",
    "checked": true,
    "id": "3dbd7498fe5d6b0eb6db113b1ceafb654dc55378",
    "semantic_title": "continuous thought machines",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=QYJt0pX0zJ": {
    "title": "Virus Infection Attack on LLMs: Your Poisoning Can Spread \"VIA\" Synthetic Data",
    "volume": "spotlight",
    "abstract": "Synthetic data refers to artificial samples generated by models. While it has been validated to significantly enhance the performance of large language models (LLMs) during training and has been widely adopted in LLM development, potential security risks it may introduce remain uninvestigated. This paper systematically evaluates the resilience of synthetic-data-integrated training paradigm for LLMs against mainstream poisoning and backdoor attacks. We reveal that such a paradigm exhibits strong resistance to existing attacks, primarily thanks to the different distribution patterns between poisoning data and queries used to generate synthetic samples. To enhance the effectiveness of these attacks and further investigate the security risks introduced by synthetic data, we introduce a novel and universal attack framework, namely, Virus Infection Attack (VIA), which enables the propagation of current attacks through synthetic data even under purely clean queries. Inspired by the principles of virus design in cybersecurity, VIA conceals the poisoning payload within a protective \"shell\" and strategically searches for optimal hijacking points in benign samples to maximize the likelihood of generating malicious content. Extensive experiments on both data poisoning and backdoor attacks show that VIA significantly increases the presence of poisoning content in synthetic data and correspondingly raises the attack success rate (ASR) on downstream models to levels comparable to those observed in the poisoned upstream models",
    "checked": true,
    "id": "2e6d64c442138a85534912170ad9d16f08a5f6c7",
    "semantic_title": "virus infection attack on llms: your poisoning can spread \"via\" synthetic data",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jPaM3AiFLq": {
    "title": "Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian Splatting",
    "volume": "spotlight",
    "abstract": "3D reconstruction from in-the-wild images remains a challenging task due to inconsistent lighting conditions and transient distractors. Existing methods typically rely on heuristic strategies to handle the low-quality training data, which often struggle to produce stable and consistent reconstructions, frequently resulting in visual artifacts. In this work, we propose Asymmetric Dual 3DGS, a novel framework that leverages the stochastic nature of these artifacts: they tend to vary across different training runs due to minor randomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS) models in parallel, enforcing a consistency constraint that encourages convergence on reliable scene geometry while suppressing inconsistent artifacts. To prevent the two models from collapsing into similar failure modes due to confirmation bias, we introduce a divergent masking strategy that applies two complementary masks: a multi-cue adaptive mask and a self-supervised soft mask, which leads to an asymmetric training process of the two models, reducing shared error modes. In addition, to improve the efficiency of model training, we introduce a lightweight variant called Dynamic EMA Proxy, which replaces one of the two models with a dynamically updated Exponential Moving Average (EMA) proxy, and employs an alternating masking strategy to preserve divergence. Extensive experiments on challenging real-world datasets demonstrate that our method consistently outperforms existing approaches while achieving high efficiency. Codes and trained models will be released",
    "checked": true,
    "id": "9befcfc373cd55dec6ba4dd80ec121cea2423e6e",
    "semantic_title": "robust neural rendering in the wild with asymmetric dual 3d gaussian splatting",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J8JCF64aEn": {
    "title": "Frame Context Packing and Drift Prevention in Next-Frame-Prediction Video Diffusion Models",
    "volume": "spotlight",
    "abstract": "We present a neural network structure, FramePack, to train next-frame (or next-frame-section) prediction models for video generation. FramePack compresses input frame contexts with frame-wise importance so that more frames can be encoded within a fixed context length, with more important frames having longer contexts. The frame importance can be measured using time proximity, feature similarity, or hybrid metrics. The packing method allows for inference with thousands of frames and training with relatively large batch sizes. We also present drift prevention methods to address observation bias (error accumulation), including early-established endpoints, adjusted sampling orders, and discrete history representation. Ablation studies validate the effectiveness of the anti-drifting methods in both single-directional video streaming and bi-directional video generation. Finally, we show that existing video diffusion models can be finetuned with FramePack, and analyze the differences between different packing schedules",
    "checked": true,
    "id": "77bfbd9259af282c8b1c5e59ed363fd03958a5cd",
    "semantic_title": "frame context packing and drift prevention in next-frame-prediction video diffusion models",
    "citation_count": 41,
    "authors": []
  },
  "https://openreview.net/forum?id=761hggw1Wx": {
    "title": "CoLT: The conditional localization test for assessing the accuracy of neural posterior estimates",
    "volume": "spotlight",
    "abstract": "We consider the problem of validating whether a neural posterior estimate $q(\\theta \\mid x)$ is an accurate approximation to the true, unknown true posterior $p(\\theta \\mid x)$. Existing methods for evaluating the quality of an NPE estimate are largely derived from classifier-based tests or divergence measures, but these suffer from several practical drawbacks. As an alternative, we introduce the *Conditional Localization Test* (**CoLT**), a principled method designed to detect discrepancies between $p(\\theta \\mid x)$ and $q(\\theta \\mid x)$ across the full range of conditioning inputs. Rather than relying on exhaustive comparisons or density estimation at every $x$, CoLT learns a localization function that adaptively selects points $\\theta_l(x)$ where the neural posterior $q$ deviates most strongly from the true posterior $p$ for that $x$. This approach is particularly advantageous in typical simulation-based inference settings, where only a single draw $\\theta \\sim p(\\theta \\mid x)$ from the true posterior is observed for each conditioning input, but where the neural posterior $q(\\theta \\mid x)$ can be sampled an arbitrary number of times. Our theoretical results establish necessary and sufficient conditions for assessing distributional equality across all $x$, offering both rigorous guarantees and practical scalability. Empirically, we demonstrate that CoLT not only performs better than existing methods at comparing $p$ and $q$, but also pinpoints regions of significant divergence, providing actionable insights for model refinement. These properties position CoLT as a state-of-the-art solution for validating neural posterior estimates",
    "checked": true,
    "id": "325d110aacf592b6e9f07932d896341e6c9923e0",
    "semantic_title": "colt: the conditional localization test for assessing the accuracy of neural posterior estimates",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ErEaq1UNaQ": {
    "title": "Predictive Preference Learning from Human Interventions",
    "volume": "spotlight",
    "abstract": "Learning from human involvement aims to incorporate the human subject to monitor and correct agent behavior errors. Although most interactive imitation learning methods focus on correcting the agent's action at the current state, they do not adjust its actions in future states, which may be potentially more hazardous. To address this, we introduce Predictive Preference Learning from Human Interventions (PPL), which leverages the implicit preference signals contained in human interventions to inform predictions of future rollouts. The key idea of PPL is to bootstrap each human intervention into L future time steps, called the preference horizon, with the assumption that the agent follows the same action and the human makes the same intervention in the preference horizon. By applying preference optimization on these future states, expert corrections are propagated into the safety-critical regions where the agent is expected to explore, significantly improving learning efficiency and reducing human demonstrations needed. We evaluate our approach with experiments on both autonomous driving and robotic manipulation benchmarks and demonstrate its efficiency and generality. Our theoretical analysis further shows that selecting an appropriate preference horizon L balances coverage of risky states with label correctness, thereby bounding the algorithmic optimality gap. Demo and code are available at: https://metadriverse.github.io/ppl",
    "checked": true,
    "id": "949b38448641e2db90865743b714fdecbf221a7b",
    "semantic_title": "predictive preference learning from human interventions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MtwsRjPZhf": {
    "title": "Transformers for Mixed-type Event Sequences",
    "volume": "spotlight",
    "abstract": "Event sequences appear widely in domains such as medicine, finance, and remote sensing, yet modeling them is challenging due to their heterogeneity: sequences often contain multiple event types with diverse structures—for example, electronic health records that mix discrete events like medical procedures with continuous lab measurements. Existing approaches either tokenize all entries, violating natural inductive biases, or ignore parts of the data to enforce a consistent structure. In this work, we propose a simple yet powerful Marked Temporal Point Process (MTPP) framework for modeling event sequences with flexible structure, using a single unified model. Our approach employs a single autoregressive transformer with discrete and continuous prediction heads, capable of modeling variable-length, mixed-type event sequences. The continuous head leverages an expressive normalizing flow to model continuous event attributes, avoiding the numerical integration required for inter-event times in most competing methods. Empirically, our model excels on both discrete-only and mixed-type sequences, improving prediction quality and enabling interpretable uncertainty quantification. We make our code public at https://github.com/czi-ai/FlexTPP",
    "checked": false,
    "id": "8a44f9e5dd8d45bc743068af6075538907bb14d1",
    "semantic_title": "federated transformer hawkes processes for distributed event sequence prediction",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=VRwcEVRcC9": {
    "title": "ROGR: Relightable 3D Objects using Generative Relighting",
    "volume": "spotlight",
    "abstract": "We introduce ROGR, a novel approach that reconstructs a relightable 3D model of an object captured from multiple views, driven by a generative relighting model that simulates the effects of placing the object under novel environment illuminations. Our method samples the appearance of the object under multiple lighting environments, creating a dataset that is used to train a lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's appearance under any input environmental lighting. The lighting-conditioned NeRF uses a novel dual-branch architecture to encode the general lighting effects and specularities separately. The optimized lighting-conditioned NeRF enables efficient feed-forward relighting under arbitrary environment maps without requiring per-illumination optimization or light transport simulation. We evaluate our approach on the established TensoIR and Stanford-ORB datasets, where it improves upon the state-of-the-art on most metrics, and showcase our approach on real-world object captures",
    "checked": true,
    "id": "843cf23876d7a9f0f0dfa99ea3a85fc1ce7cd320",
    "semantic_title": "rogr: relightable 3d objects using generative relighting",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nW6SMcfDq3": {
    "title": "Hamiltonian Descent Algorithms for Optimization: Accelerated Rates via Randomized Integration Time",
    "volume": "spotlight",
    "abstract": "We study the Hamiltonian flow for optimization (HF-opt), which simulates the Hamiltonian dynamics for some integration time and resets the velocity to $0$ to decrease the objective function; this is the optimization analogue of the Hamiltonian Monte Carlo algorithm for sampling. For short integration time, HF-opt has the same convergence rates as gradient descent for minimizing strongly and weakly convex functions. We show that by randomizing the integration time in HF-opt, the resulting randomized Hamiltonian flow (RHF) achieves accelerated convergence rates in continuous time, similar to the rates for accelerated gradient flow. We study a discrete-time implementation of RHF as the randomized Hamiltonian gradient descent (RHGD) algorithm. We prove that RHGD achieves the same accelerated convergence rates as Nesterov's accelerated gradient descent (AGD) for minimizing smooth strongly and weakly convex functions. We provide numerical experiments to demonstrate that RHGD is competitive with classical accelerated methods such as AGD across all settings and outperforms them in certain regimes",
    "checked": true,
    "id": "fe307735e8928abec5573252b9af442420ed0c92",
    "semantic_title": "hamiltonian descent algorithms for optimization: accelerated rates via randomized integration time",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=3AdTRYA2uJ": {
    "title": "Cost-aware LLM-based Online Dataset Annotation",
    "volume": "spotlight",
    "abstract": "Recent advances in large language models (LLMs) have enabled automated dataset labeling with minimal human supervision. While majority voting across multiple LLMs can improve label reliability by mitigating individual model biases, it incurs high computational costs due to repeated querying. In this work, we propose a novel online framework, Cost-aware Majority Voting (CaMVo), for efficient and accurate LLM-based dataset annotation. CaMVo adaptively selects a subset of LLMs for each data instance based on contextual embeddings, balancing confidence and cost without requiring pre-training or ground-truth labels. Leveraging a LinUCB-based selection mechanism and a Bayesian estimator over confidence scores, CaMVo estimates a lower bound on labeling accuracy for each LLM and aggregates responses through weighted majority voting. Our empirical evaluation on the MMLU and IMDB Movie Review datasets demonstrates that CaMVo achieves comparable or superior accuracy to full majority voting while significantly reducing labeling costs. This establishes CaMVo as a practical and robust solution for cost-efficient annotation in dynamic labeling environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yvGL2HP7pU": {
    "title": "Scaling Unlocks Broader Generation and Deeper Functional Understanding of Proteins",
    "volume": "spotlight",
    "abstract": "Generative protein language models (PLMs) are powerful tools for designing proteins purpose-built to solve problems in medicine, agriculture, and industrial processes. Recent work has trained ever larger language models, but there has been little systematic study of the optimal training distributions and the influence of model scale on the sequences generated by PLMs. We introduce the ProGen3 family of sparse generative PLMs, and we develop compute-optimal scaling laws to scale up to a 46B-parameter model pre-trained on 1.5T amino acid tokens. ProGen3's pre-training data is sampled from an optimized data distribution over the PPA v1, a carefully curated dataset of 3.4B full-length proteins. We evaluate for the first time in the wet lab the influence of model scale on the sequences generated by PLMs, and we find that larger models generate viable proteins for a much wider diversity of protein families. Finally, we find both computationally and experimentally that larger models are more responsive to alignment with laboratory data, resulting in improved protein fitness prediction and sequence generation capabilities. These results indicate that larger PLMs like ProGen3-46B trained on larger, well-curated datasets are powerful foundation models that push the frontier of protein design",
    "checked": true,
    "id": "cf7297714fa88a9ace1535f6cd0d1f81c4535903",
    "semantic_title": "scaling unlocks broader generation and deeper functional understanding of proteins",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=3YguS2rxdk": {
    "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis",
    "volume": "spotlight",
    "abstract": "We present STARFlow, a scalable generative model based on normalizing flows that achieves strong performance on high-resolution image synthesis. STARFlow's main building block is Transformer Autoregressive Flow (TARFlow), which combines normalizing flows with Autoregressive Transformer architectures and has recently achieved impressive results in image modeling. In this work, we first establish the theoretical universality of TARFlow for modeling continuous distributions. Building on this foundation, we introduce a set of architectural and algorithmic innovations that significantly enhance the scalability: (1) a deep-shallow design where a deep Transformer block captures most of the model's capacity, followed by a few shallow Transformer blocks that are computationally cheap yet contribute non-negligibly, (2) learning in the latent space of pretrained autoencoders, which proves far more effective than modeling pixels directly, and (3) a novel guidance algorithm that substantially improves sample quality. Crucially, our model remains a single, end-to-end normalizing flow, allowing exact maximum likelihood training in continuous space without discretization. STARFlow achieves competitive results in both class- and text-conditional image generation, with sample quality approaching that of state-of-the-art diffusion models. To our knowledge, this is the **first** successful demonstration of normalizing flows at this scale and resolution. Code and weights available at https://github.com/apple/ml-starflow",
    "checked": true,
    "id": "e7efd2fed358c7179bed918031d79ce81b1cabbb",
    "semantic_title": "starflow: scaling latent normalizing flows for high-resolution image synthesis",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=N1KPOlcN6P": {
    "title": "Flow Equivariant Recurrent Neural Networks",
    "volume": "spotlight",
    "abstract": "Data arrives at our senses as a continuous stream, smoothly transforming from one instant to the next. These smooth transformations can be viewed as continuous symmetries of the environment that we inhabit, defining equivalence relations between stimuli over time. In machine learning, neural network architectures that respect symmetries of their data are called equivariant and have provable benefits in terms of generalization ability and sample efficiency. To date, however, equivariance has been considered only for static transformations and feed-forward networks, limiting its applicability to sequence models, such as recurrent neural networks (RNNs), and corresponding time-parameterized sequence transformations. In this work, we extend equivariant network theory to this regime of 'flows' -- one-parameter Lie subgroups capturing natural transformations over time, such as visual motion. We begin by showing that standard RNNs are generally not flow equivariant: their hidden states fail to transform in a geometrically structured manner for moving stimuli. We then show how flow equivariance can be introduced, and demonstrate that these models significantly outperform their non-equivariant counterparts in terms of training speed, length generalization, and velocity generalization, on both next step prediction and sequence classification. We present this work as a first step towards building sequence models that respect the time-parameterized symmetries which govern the world around us",
    "checked": true,
    "id": "bbcb8c5a2c2f9ccc1a2457bf92ebc15f698f0136",
    "semantic_title": "flow equivariant recurrent neural networks",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=VjjJlJ5qik": {
    "title": "AdaReasoner: Adaptive Reasoning Enables More Flexible Thinking",
    "volume": "spotlight",
    "abstract": "LLMs often need effective configurations, like temperature and reasoning steps, to handle tasks requiring sophisticated reasoning and problem-solving, ranging from joke generation to mathematical reasoning. Existing prompting approaches usually adopt general-purpose, fixed configurations that work \"well enough\" across tasks but seldom achieve task-specific optimality. To address this gap, we introduce AdaReasoner, an LLM-agnostic plugin designed for any LLM to automate adaptive reasoning configurations for tasks requiring different types of thinking. AdaReasoner is trained using a reinforcement learning (RL) framework, combining a factorized action space with a targeted exploration strategy, along with a pretrained reward model to optimize the policy model for reasoning configurations with only a few-shot guide. AdaReasoner is backed by theoretical guarantees and experiments of fast convergence and a sublinear policy gap. Across six different LLMs and a variety of reasoning tasks, it consistently outperforms standard baselines, preserves out-of-distribution robustness, and yield gains on knowledge-intensive tasks through tailored prompts",
    "checked": true,
    "id": "df897d593a730a0b8e5a7a6fbd1fc9b675ab26d9",
    "semantic_title": "adareasoner: adaptive reasoning enables more flexible thinking",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=CABcYH1wKM": {
    "title": "LABridge: Text–Image Latent Alignment Framework via Mean-Conditioned OU Process",
    "volume": "spotlight",
    "abstract": "Diffusion models have emerged as state‑of‑the‑art in image synthesis.However, it often suffer from semantic instability and slow iterative denoising. We introduce Latent Alignment Framework (LABridge), a novel Text–Image Latent Alignment Framework via an Ornstein–Uhlenbeck (OU) Process, which explicitly preserves and aligns textual and visual semantics in an aligned latent space. LABridge employs a Text-Image Alignment Encoder (TIAE) to encode text prompts into structured priors that are directly aligned with image latents. Instead of a homogeneous Gaussian, Mean-Conditioned OU process smoothly interpolates between these text‑conditioned priors and image latents, improving stability and reducing the number of denoising steps. Extensive experiments on standard text-to-image benchmarks show that LABridge achieves better text–image alignment metric and competitive FID scores compared to leading diffusion baselines. By unifying text and image representations through principled latent alignment, LABridge paves the way for more efficient, semantically consistent, and high‑fidelity text to image generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TQNlIQIrcK": {
    "title": "Achieving O ~ ( 1 / N ) Optimality Gap in Restless Bandits through Gaussian Approximation",
    "volume": "spotlight",
    "abstract": "We study the finite-horizon Restless Multi-Armed Bandit (RMAB) problem with $N$ homogeneous arms. Prior work has shown that when an RMAB satisfies a non-degeneracy condition, Linear-Programming-based (LP-based) policies derived from the fluid approximation, which captures the mean dynamics of the system, achieve an exponentially small optimality gap. However, it is common for RMABs to be degenerate, in which case LP-based policies can result in a $\\Theta(1/\\sqrt{N})$ optimality gap per arm. In this paper, we propose a novel Stochastic-Programming-based (SP-based) policy that, under a uniqueness assumption, achieves an $\\tilde{\\mathcal{O}}(1/N)$ optimality gap for degenerate RMABs. Our approach is based on the construction of a Gaussian stochastic system that captures not only the mean but also the variance of the RMAB dynamics, resulting in a more accurate approximation than the fluid approximation. We then solve a stochastic program for this system to obtain our policy. This is the first result to establish an $\\tilde{\\mathcal{O}}(1/N)$ optimality gap for degenerate RMABs",
    "checked": false,
    "id": "042a32d196bdfa499cadbe6634132e7017e85ee7",
    "semantic_title": "achieving $\\tilde{\\mathcal{o}}(1/n)$ optimality gap in restless bandits through gaussian approximation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hTxnm6H93P": {
    "title": "On the Surprising Effectiveness of Large Learning Rates under Standard Width Scaling",
    "volume": "spotlight",
    "abstract": "Scaling limits, such as infinite-width limits, serve as promising theoretical tools to study large-scale models. However, it is widely believed that existing infinite-width theory does not faithfully explain the behavior of practical networks, especially those trained in *standard parameterization* (SP) meaning He initialization with a global learning rate. For instance, existing theory for SP predicts instability at large learning rates and vanishing feature learning at stable ones. In practice, however, optimal learning rates decay slower than theoretically predicted and networks exhibit both stable training and non-trivial feature learning, even at very large widths. Here, we show that this discrepancy is not fully explained by finite-width phenomena. Instead, we find a resolution through a finer-grained analysis of the regime previously considered unstable and therefore uninteresting. In particular, we show that, under the cross-entropy (CE) loss, the unstable regime comprises two distinct sub-regimes: a catastrophically unstable regime and a more benign controlled divergence regime, where logits diverge but gradients and activations remain stable. Moreover, under large learning rates at the edge of the controlled divergence regime, there exists a well-defined infinite width limit where features continue to evolve in all the hidden layers. In experiments across optimizers, architectures, and data modalities, we validate that neural networks operate in this controlled divergence regime under CE loss but not under MSE loss. Our empirical evidence suggests that width-scaling considerations are surprisingly useful for predicting empirically maximal stable learning rate exponents which provide useful guidance on optimal learning rate exponents. Finally, our analysis clarifies the effectiveness and limitations of recently proposed layerwise learning rate scalings for standard initialization",
    "checked": true,
    "id": "1091d0ff877d1b2dbe99b8d30e14c996e22230d3",
    "semantic_title": "on the surprising effectiveness of large learning rates under standard width scaling",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=gKsG5qR3Bt": {
    "title": "Flash Invariant Point Attention",
    "volume": "spotlight",
    "abstract": "Invariant Point Attention (IPA) is a key algorithm for geometry-aware modeling in structural biology, central to many protein and RNA models. However, its quadratic complexity limits the input sequence length. We introduce FlashIPA, a factorized reformulation of IPA that leverages hardware-efficient FlashAttention to achieve linear scaling in GPU memory and wall-clock time with sequence length. FlashIPA matches or exceeds standard IPA performance while substantially reducing computational costs. FlashIPA extends training to previously unattainable lengths, and we demonstrate this by re-training generative models without length restrictions and generating structures of thousands of residues. FlashIPA is available at https://github.com/flagshippioneering/flash_ipa",
    "checked": true,
    "id": "611927d96cb745a110b688e6f3f84c4e3aa393c7",
    "semantic_title": "flash invariant point attention",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0BVrpXMr5Y": {
    "title": "SmallKV: Small Model Assisted Compensation of KV Cache Compression for Efficient LLM Inference",
    "volume": "spotlight",
    "abstract": "KV cache eviction has emerged as an effective solution to alleviate resource constraints faced by LLMs in long-context scenarios. However, existing token-level eviction methods often overlook two critical aspects: (1) their irreversible eviction strategy fails to adapt to dynamic attention patterns during decoding (the saliency shift problem), and (2) they treat both marginally important tokens and truly unimportant tokens uniformly, despite the collective significance of marginal tokens to model performance (the marginal information over-compression problem). To address these issues, we design two compensation mechanisms based on the high similarity of attention matrices between LLMs with different scales. We propose SmallKV, a small model assisted compensation method for KV cache compression. SmallKV can maintain attention matching between different-scale LLMs to: 1) assist the larger model in perceiving globally important information of attention; and 2) use the smaller model's attention scores to approximate those of marginal tokens in the larger model. Extensive experiments on benchmarks including GSM8K, BBH, MT-Bench, and LongBench demonstrate the effectiveness of SmallKV. Moreover, efficiency evaluations show that SmallKV achieves 1.75 - 2.56 times higher throughput than baseline methods, highlighting its potential for efficient and performant LLM inference in resource constrained environments",
    "checked": true,
    "id": "bb36f730698710cb98ba8abff7ac5e120ebf27f9",
    "semantic_title": "smallkv: small model assisted compensation of kv cache compression for efficient llm inference",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=0r4yzkvt9j": {
    "title": "CALM-PDE: Continuous and Adaptive Convolutions for Latent Space Modeling of Time-dependent PDEs",
    "volume": "spotlight",
    "abstract": "Solving time-dependent Partial Differential Equations (PDEs) using a densely discretized spatial domain is a fundamental problem in various scientific and engineering disciplines, including modeling climate phenomena and fluid dynamics. However, performing these computations directly in the physical space often incurs significant computational costs. To address this issue, several neural surrogate models have been developed that operate in a compressed latent space to solve the PDE. While these approaches reduce computational complexity, they often use Transformer-based attention mechanisms to handle irregularly sampled domains, resulting in increased memory consumption. In contrast, convolutional neural networks allow memory-efficient encoding and decoding but are limited to regular discretizations. Motivated by these considerations, we propose CALM-PDE, a model class that efficiently solves arbitrarily discretized PDEs in a compressed latent space. We introduce a novel continuous convolution-based encoder-decoder architecture that uses an epsilon-neighborhood-constrained kernel and learns to apply the convolution operator to adaptive and optimized query points. We demonstrate the effectiveness of CALM-PDE on a diverse set of PDEs with both regularly and irregularly sampled spatial domains. CALM-PDE is competitive with or outperforms existing baseline methods while offering significant improvements in memory and inference time efficiency compared to Transformer-based methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YES7VDXPV8": {
    "title": "On the Empirical Power of Goodness-of-Fit Tests in Watermark Detection",
    "volume": "spotlight",
    "abstract": "Large language models (LLMs) raise concerns about content authenticity and integrity because they can generate human-like text at scale. Text watermarks, which embed detectable statistical signals into generated text, offer a provable way to verify content origin. Many detection methods rely on pivotal statistics that are i.i.d. under human-written text, making goodness-of-fit (GoF) tests a natural tool for watermark detection. However, GoF tests remain largely underexplored in this setting. In this paper, we systematically evaluate eight GoF tests across three popular watermarking schemes, using three open-source LLMs, two datasets, various generation temperatures, and multiple post-editing methods. We find that general GoF tests can improve both the detection power and robustness of watermark detectors. Notably, we observe that text repetition, common in low-temperature settings, gives GoF tests a unique advantage not exploited by existing methods. Our results highlight that classic GoF tests are a simple yet powerful and underused tool for watermark detection in LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=auiURbhoYx": {
    "title": "Balancing Multimodal Training Through Game-Theoretic Regularization",
    "volume": "spotlight",
    "abstract": "Multimodal learning holds the promise for richer information extraction by capturing dependencies across data sources. Yet, current training methods often underperform due to modality competition, a phenomenon where modalities contend for training resources, leaving some underoptimized. This raises a pivotal question: how can we address training imbalances, ensure adequate optimization across all modalities, and achieve consistent performance improvements as we transition from unimodal to multimodal data? This paper proposes the Multimodal Competition Regularizer (MCR), inspired by a mutual information (MI) decomposition designed to prevent the adverse effects of competition in multimodal training. Our key contributions are: 1) A game-theoretic framework that adaptively balances modality contributions by encouraging each to maximize its informative role in the final prediction. 2) Refining lower and upper bounds for each MI term to enhance the extraction of both task-relevant unique and shared information across modalities. 3) Proposing latent space permutations for conditional MI estimation, significantly improving computational efficiency. MCR outperforms all previously suggested training strategies and simple baselines, demonstrating that training modalities jointly lead to important performance gains on synthetic and large real-world datasets. We release our code and models at https://github.com/kkontras/MCR",
    "checked": true,
    "id": "b1234cfbd428f093d1b83abd0ffd424c51b7f4f6",
    "semantic_title": "balancing multimodal training through game-theoretic regularization",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=nG45z7lJ7D": {
    "title": "Bipolar Self-attention for Spiking Transformers",
    "volume": "spotlight",
    "abstract": "Harnessing the event-driven characteristic, Spiking Neural Networks (SNNs) present a promising avenue toward energy-efficient Transformer architectures. However, existing Spiking Transformers still suffer significant performance gaps compared to their Artificial Neural Network counterparts. Through comprehensive analysis, we attribute this gap to these two factors. First, the binary nature of spike trains limits Spiking Self-attention (SSA)'s capacity to capture negative–negative and positive–negative membrane potential interactions on Querys and Keys. Second, SSA typically omits Softmax functions to avoid energy-intensive multiply-accumulate operations, thereby failing to maintain row-stochasticity constraints on attention scores. To address these issues, we propose a Bipolar Self-attention (BSA) paradigm, effectively modeling multi-polar membrane potential interactions with a fully spike-driven characteristic. Specifically, we demonstrate that ternary matrix multiplication provides a closer approximation to real-valued computation on both distribution and local correlation, enabling clear differentiation between homopolar and heteropolar interactions. Moreover, we propose a shift-based Softmax approximation named Shiftmax, which efficiently achieves low-entropy activation and partly maintains row-stochasticity without non-linear operation, enabling precise attention allocation. Extensive experiments show that BSA achieves substantial performance improvements across various tasks, including image classification, semantic segmentation, and event-based tracking. These results establish its potential as a fundamental building block for energy-efficient Spiking Transformers",
    "checked": false,
    "id": "7b27cf1b6a2b7fcd1764beead49bb26116e273cb",
    "semantic_title": "hybrid attention spike transformer",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RPuTB28HsK": {
    "title": "Online Functional Tensor Decomposition via Continual Learning for Streaming Data Completion",
    "volume": "spotlight",
    "abstract": "Online tensor decompositions are powerful and proven techniques that address the challenges in processing high-velocity streaming tensor data, such as traffic flow and weather system. The main aim of this work is to propose a novel online functional tensor decomposition (OFTD) framework, which represents a spatial-temporal continuous function using the CP tensor decomposition parameterized by coordinate-based implicit neural representations (INRs). The INRs allow for natural characterization of continually expanded streaming data by simply adding new coordinates into the network. Particularly, our method transforms the classical online tensor decomposition algorithm into a more dynamic continual learning paradigm of updating the INR weights to fit the new data without forgetting the previous tensor knowledge. To this end, we introduce a long-tail memory replay method that adapts to the local continuity property of INR. Extensive experiments for streaming tensor completion using traffic, weather, user-item, and video data verify the effectiveness of the OFTD approach for streaming data analysis. This endeavor serves as a pivotal inspiration for future research to connect classical online tensor tools with continual learning paradigms to better explore knowledge underlying streaming tensor data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pRYGjhirkY": {
    "title": "TrajMamba: An Efficient and Semantic-rich Vehicle Trajectory Pre-training Model",
    "volume": "spotlight",
    "abstract": "Vehicle GPS trajectories record how vehicles move over time, storing valuable travel semantics, including movement patterns and travel purposes. Learning travel semantics effectively and efficiently is crucial for real-world applications of trajectory data, which is hindered by two major challenges. First, travel purposes are tied to the functions of the roads and points-of-interest (POIs) involved in a trip. Such information is encoded in textual addresses and descriptions and introduces heavy computational burden to modeling. Second, real-world trajectories often contain redundant points, which harm both computational efficiency and trajectory embedding quality. To address these challenges, we propose TrajMamba, a novel approach for efficient and semantically rich vehicle trajectory learning. TrajMamba introduces a Traj-Mamba Encoder that captures movement patterns by jointly modeling both GPS and road perspectives of trajectories, enabling robust representations of continuous travel behaviors. It also incorporates a Travel Purpose-aware Pre-training procedure to integrate travel purposes into the learned embeddings without introducing extra overhead to embedding calculation. To reduce redundancy in trajectories, TrajMamba features a Knowledge Distillation Pre-training scheme to identify key trajectory points through a learnable mask generator and obtain effective compressed trajectory embeddings. Extensive experiments on two real-world datasets and three downstream tasks show that TrajMamba outperforms state-of-the-art baselines in both efficiency and accuracy",
    "checked": true,
    "id": "0ae16b5165d9bf0f4a4763fc0b6e27ffdec7b941",
    "semantic_title": "trajmamba: an efficient and semantic-rich vehicle trajectory pre-training model",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ECTxVRFhUa": {
    "title": "Tensor Product Attention Is All You Need",
    "volume": "spotlight",
    "abstract": "Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, substantially shrinking the KV cache size at inference time. By factorizing these representations into contextual low-rank components and seamlessly integrating with Rotary Position Embedding (RoPE), TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation on language modeling tasks, we demonstrate that T6 surpasses or matches the performance of standard Transformer baselines including Multi-Head Attention (MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and Multi-Head Latent Attention (MLA) across various metrics, including perplexity and a range of established evaluation benchmarks. Notably, TPA's memory efficiency and computational efficiency at decoding stage enables processing longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. Project Page: https://github.com/tensorgi/TPA",
    "checked": true,
    "id": "b4f9e9130b29754c526c454ab54043b49bdd387d",
    "semantic_title": "tensor product attention is all you need",
    "citation_count": 23,
    "authors": []
  },
  "https://openreview.net/forum?id=wmweEDugTZ": {
    "title": "TreeSynth: Synthesizing Diverse Data from Scratch via Tree-Guided Subspace Partitioning",
    "volume": "spotlight",
    "abstract": "Model customization necessitates high-quality and diverse datasets, but acquiring such data remains time-consuming and labor-intensive. Despite the great potential of large language models (LLMs) for data synthesis, current approaches are constrained by limited seed data, model biases and low-variation prompts, resulting in limited diversity and biased distribution with the increase of data scales. To tackle this challenge, we introduce TreeSynth, a tree-guided subspace-based data synthesis approach inspired by decision trees. It constructs a spatial partitioning tree to recursively divide a task-specific full data space (i.e., root node) into numerous atomic subspaces (i.e., leaf nodes) with mutually exclusive and exhaustive attributes to ensure both distinctiveness and comprehensiveness, before synthesizing samples within each atomic subspace. This globally divide-and-synthesize method finally collects subspace samples into a comprehensive dataset, effectively circumventing repetition and space collapse to ensure the diversity of large-scale data synthesis. Furthermore, the spatial partitioning tree enables sample allocation into atomic subspaces, allowing the re-balancing of existing datasets for more balanced and comprehensive distributions. Empirically, extensive experiments across diverse benchmarks consistently validates the superior data diversity, model performance, and robust scalability of TreeSynth compared to both human-crafted datasets and peer data synthesis methods, with the average performance gain reaching 10%. Besides, the consistent improvements of TreeSynth-balanced datasets highlight its efficacious application to redistribute existing datasets for more comprehensive coverage and the induced performance enhancement. The code is available at https://github.com/cpa2001/TreeSynth",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mOYGK7Hw9Y": {
    "title": "DeCaFlow: A deconfounding causal generative model",
    "volume": "spotlight",
    "abstract": "We introduce DeCaFlow, a deconfounding causal generative model. Training once per dataset using just observational data and the underlying causal graph, DeCaFlow enables accurate causal inference on continuous variables under the presence of hidden confounders. Specifically, we extend previous results on causal estimation under hidden confounding to show that a single instance of DeCaFlow provides correct estimates for all causal queries identifiable with do-calculus, leveraging proxy variables to adjust for the causal effects when do-calculus alone is insufficient. Moreover, we show that counterfactual queries are identifiable as long as their interventional counterparts are identifiable, and thus are also correctly estimated by DeCaFlow. Our empirical results on diverse settings—including the Ecoli70 dataset, with 3 independent hidden confounders, tens of observed variables and hundreds of causal queries—show that DeCaFlow outperforms existing approaches, while demonstrating its out-of-the-box applicability to any given causal graph",
    "checked": true,
    "id": "4cd6240c809eeefbf8648c9d78746b801ce4a32d",
    "semantic_title": "decaflow: a deconfounding causal generative model",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=arXNS7T90z": {
    "title": "Transfer Learning for Benign Overfitting in High-Dimensional Linear Regression",
    "volume": "spotlight",
    "abstract": "Transfer learning is a key component of modern machine learning, enhancing the performance of target tasks by leveraging diverse data sources. Simultaneously, overparameterized models such as the minimum-$\\ell_2$-norm interpolator (MNI) in high-dimensional linear regression have garnered significant attention for their remarkable generalization capabilities, a property known as *benign overfitting*. Despite their individual importance, the intersection of transfer learning and MNI remains largely unexplored. Our research bridges this gap by proposing a novel two-step Transfer MNI approach and analyzing its trade-offs. We characterize its non-asymptotic excess risk and identify conditions under which it outperforms the target-only MNI. Our analysis reveals *free-lunch* covariate shift regimes, where leveraging heterogeneous data yields the benefit of knowledge transfer at limited cost. To operationalize our findings, we develop a data-driven procedure to detect *informative* sources and introduce an ensemble method incorporating multiple informative Transfer MNIs. Finite-sample experiments demonstrate the robustness of our methods to model and data heterogeneity, confirming their advantage",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SRDF3RV0KP": {
    "title": "LLM Meeting Decision Trees on Tabular Data",
    "volume": "spotlight",
    "abstract": "Tabular data have been playing a vital role in diverse real-world fields, including healthcare, finance, etc. With the recent success of Large Language Models (LLMs), early explorations of extending LLMs to the domain of tabular data have been developed. Most of these LLM-based methods typically first serialize tabular data into natural language descriptions, and then tune LLMs or directly infer on these serialized data. However, these methods suffer from two key inherent issues: (i) data perspective: existing data serialization methods lack universal applicability for structured tabular data, and may pose privacy risks through direct textual exposure, and (ii) model perspective: LLM fine-tuning methods struggle with tabular data, and in-context learning scalability is bottle-necked by input length constraints (suitable for few-shot learning). This work explores a novel direction of integrating LLMs into tabular data through logical decision tree rules as intermediaries, proposing a decision tree enhancer with LLM-derived rule for tabular prediction, DeLTa. The proposed DeLTa avoids tabular data serialization, and can be applied to full data learning setting without LLM fine-tuning. Specifically, we leverage the reasoning ability of LLMs to redesign an improved rule given a set of decision tree rules. Furthermore, we provide a calibration method for original decision trees via new generated rule by LLM, which approximates the error correction vector to steer the original decision tree predictions in the direction of ``errors'' reducing. Finally, extensive experiments on diverse tabular benchmarks show that our method achieves state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mlU9KqdZUS": {
    "title": "AgentBreeder: Mitigating the AI Safety Risks of Multi-Agent Scaffolds via Self-Improvement",
    "volume": "spotlight",
    "abstract": "Scaffolding Large Language Models (LLMs) into multi-agent systems often improves performance on complex tasks, but the safety impact of such scaffolds has not been thoroughly explored. We introduce AgentBreeder, a framework for multi-objective self-improving evolutionary search over scaffolds. We evaluate discovered scaffolds on widely recognized reasoning, mathematics, and safety benchmarks and compare them with popular baselines. In \"blue\" mode, we see a 79.4% average uplift in safety benchmark performance while maintaining or improving capability scores. In \"red\" mode, we find adversarially weak scaffolds emerging concurrently with capability optimization. Our work demonstrates the risks of multi-agent scaffolding and provides a framework for mitigating them. Code is available at \\url{https://github.com/jrosseruk/AgentBreeder}",
    "checked": true,
    "id": "637bf4a7cf4ed36bdb87e8f29f025ade9bf6a4f1",
    "semantic_title": "agentbreeder: mitigating the ai safety risks of multi-agent scaffolds via self-improvement",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=zL4ifL17bU": {
    "title": "Learnable Burst-Encodable Time-of-Flight Imaging for High-Fidelity Long-Distance Depth Sensing",
    "volume": "spotlight",
    "abstract": "Long-distance depth imaging holds great promise for applications such as autonomous driving and robotics. Direct time-of-flight (dToF) imaging offers high-precision, long-distance depth sensing, yet demands ultra-short pulse light sources and high-resolution time-to-digital converters. In contrast, indirect time-of-flight (iToF) imaging often suffers from phase wrapping and low signal-to-noise ratio (SNR) as the sensing distance increases. In this paper, we introduce a novel ToF imaging paradigm, termed Burst-Encodable Time-of-Flight (BE-ToF), which facilitates high-fidelity, long-distance depth imaging. Specifically, the BE-ToF system emits light pulses in burst mode and estimates the phase delay of the reflected signal over the entire burst period, thereby effectively avoiding the phase wrapping inherent to conventional iToF systems. Moreover, to address the low SNR caused by light attenuation over increasing distances, we propose an end-to-end learnable framework that jointly optimizes the coding functions and the depth reconstruction network. A specialized double well function and first-order difference term are incorporated into the framework to ensure the hardware implementability of the coding functions. The proposed approach is rigorously validated through comprehensive simulations and real-world prototype experiments, demonstrating its effectiveness and practical applicability. The code is available at: https://github.com/ComputationalPerceptionLab/BE-ToF",
    "checked": true,
    "id": "bed7b23f704b1da240d99b2bcfa11572e3e7cb93",
    "semantic_title": "learnable burst-encodable time-of-flight imaging for high-fidelity long-distance depth sensing",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Z25QbOv4a": {
    "title": "MigGPT: Harnessing Large Language Models for Automated Migration of Out-of-Tree Linux Kernel Patches Across Versions",
    "volume": "spotlight",
    "abstract": "Out-of-tree kernel patches are essential for adapting the Linux kernel to new hardware or enabling specific functionalities. Maintaining and updating these patches across different kernel versions demands significant effort from experienced engineers. Large language models (LLMs) have shown remarkable progress across various domains, suggesting their potential for automating out-of-tree kernel patch migration. However, our findings reveal that LLMs, while promising, struggle with incomplete code context understanding and inaccurate migration point identification. In this work, we propose MigGPT, a framework that employs a novel code fingerprint structure to retain code snippet information and incorporates three meticulously designed modules to improve the migration accuracy and efficiency of out-of-tree kernel patches. Furthermore, we establish a robust benchmark using real-world out-of-tree kernel patch projects to evaluate LLM capabilities. Evaluations show that MigGPT significantly outperforms the direct application of vanilla LLMs, achieving an average completion rate of 72.59\\% ($\\uparrow 50.74\\%$) for migration tasks",
    "checked": true,
    "id": "859ad37d135a82fa5ae2d91dfa54aa5a0627ae13",
    "semantic_title": "miggpt: harnessing large language models for automated migration of out-of-tree linux kernel patches across versions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kuzye4EPLR": {
    "title": "FP4 All the Way: Fully Quantized Training of Large Language Models",
    "volume": "spotlight",
    "abstract": "We demonstrate, for the first time, fully quantized training (FQT) of large language models (LLMs) using predominantly 4-bit floating-point (FP4) precision for weights, activations, and gradients on datasets up to 200 billion tokens. We extensively investigate key design choices for FP4, including block sizes, scaling formats, and rounding methods. Our analysis shows that the NVFP4 format, where each block of 16 FP4 values (E2M1) shares a scale represented in E4M3, provides optimal results. We use stochastic rounding for backward and update passes and round-to-nearest for the forward pass to enhance stability. Additionally, we identify a theoretical and empirical threshold for effective quantized training: when the gradient norm falls below approximately $\\sqrt{3}$ times the quantization noise, quantized training becomes less effective. Leveraging these insights, we successfully train a 7-billion-parameter model on 256 Intel Gaudi2 accelerators. The resulting FP4-trained model achieves downstream task performance comparable to a standard BF16 baseline, confirming that FP4 training is a practical and highly efficient approach for large-scale LLM training. A reference implementation is supplied in https://github.com/Anonymous1252022/fp4-all-the-way",
    "checked": false,
    "id": "64f5637c816f4fa44ef327a3f890a65817a782f0",
    "semantic_title": "fp4 all the way: fully quantized training of llms",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=ZWOe1kkufx": {
    "title": "Efficient Knowledge Transfer in Federated Recommendation for Joint Venture Ecosystem",
    "volume": "spotlight",
    "abstract": "The current Federated Recommendation System (FedRS) focuses on personalized recommendation services and assumes clients are personalized IoT devices (e.g., Mobile phones). In this paper, we deeply dive into new but practical FedRS applications within the joint venture ecosystem. Subsidiaries engage as participants with their users and items. However, in such a situation, merely exchanging item embedding is insufficient, as user bases always exhibit both overlaps and exclusive segments, demonstrating the complexity of user information. Meanwhile, directly uploading user information is a violation of privacy and unacceptable. To tackle the above challenges, we propose an efficient and privacy-enhanced federated recommendation for the joint venture ecosystem (FR-JVE) that each client transfers more common knowledge from other clients with a distilled user's \\textit{rating preference} from the local dataset. More specifically, we first transform the local data into a new format and apply model inversion techniques to distill the rating preference with frozen user gradients before the federated training. Then, a bridge function is employed on each client side to align the local rating preference and aggregated global preference in a privacy-friendly manner. Finally, each client matches similar users to make a better prediction for overlapped users. From a theoretical perspective, we analyze how effectively FR-JVE can guarantee user privacy. Empirically, we show that FR-JVE achieves superior performance compared to state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4oYxzssbVg": {
    "title": "VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "Recently, slow-thinking systems like GPT-o1 and DeepSeek-R1 have demonstrated great potential in solving challenging problems through explicit reflection. They significantly outperform the best fast-thinking models, such as GPT-4o, on various math and science benchmarks. However, their multimodal reasoning capabilities remain on par with fast-thinking models. For instance, GPT-o1's performance on benchmarks like MathVista, MathVerse, and MathVision is similar to fast-thinking models. In this paper, we aim to enhance the slow-thinking capabilities of vision-language models using reinforcement learning (without relying on distillation) to advance the state of the art. First, we adapt the GRPO algorithm with a novel technique called Selective Sample Replay (SSR) to address the vanishing advantages problem. While this approach yields strong performance, the resulting RL-trained models exhibit limited self-reflection or self-verification. To further encourage slow-thinking, we introduce Forced Rethinking, which appends a rethinking trigger token to the end of rollouts in RL training, explicitly enforcing a self-reflection reasoning step. By combining these two techniques, our model, VL-Rethinker, advances state-of-the-art scores on MathVista, MathVerse to achieve 80.4%, 63.5% respectively. VL-Rethinker also achieves open-source SoTA on multi-disciplinary benchmarks such as MathVision, MMMU-Pro, EMMA, and MEGA-Bench, narrowing the gap with OpenAI-o1. We conduct comprehensive ablations and analysis to provide insights into the effectiveness of our approach",
    "checked": true,
    "id": "6f0f0d9f29586344ae6403fe906c24e4f16eaed8",
    "semantic_title": "vl-rethinker: incentivizing self-reflection of vision-language models with reinforcement learning",
    "citation_count": 119,
    "authors": []
  },
  "https://openreview.net/forum?id=3dnG7LcKxT": {
    "title": "Spectral Graph Neural Networks are Incomplete on Graphs with a Simple Spectrum",
    "volume": "spotlight",
    "abstract": "Spectral features are widely incorporated within Graph Neural Networks (GNNs) to improve their expressive power, or their ability to distinguish among non-isomorphic graphs. One popular example is the usage of graph Laplacian eigenvectors for positional encoding in MPNNs and Graph Transformers. The expressive power of such Spectrally-enhanced GNNs (SGNNs) is usually evaluated via the $k$-WL graph isomorphism test hierarchy and homomorphism counting. Yet, these frameworks align poorly with the graph spectra, yielding limited insight into SGNNs' expressive power. In this paper, we leverage a well-studied paradigm of classifying graphs by their largest eigenvalue multiplicity to introduce an expressivity hierarchy for SGNNs. We then prove that many SGNNs are incomplete even on graphs with distinct eigenvalues. To mitigate this deficiency, we adapt rotation equivariant neural networks to the graph spectra setting, yielding equiEPNN, a novel SGNN that provably improves upon contemporary SGNNs' expressivity on simple spectrum graphs. We then demonstrate that equiEPNN achieves perfect eigenvector canonicalization on ZINC, and performs favorably on image classification on MNIST-Superpixel and graph property regression on ZINC, compared to leading spectral methods",
    "checked": true,
    "id": "9afcef5557fd58636502608b8434661cf3a927fd",
    "semantic_title": "spectral graph neural networks are incomplete on graphs with a simple spectrum",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=muWdWcMvpW": {
    "title": "ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding",
    "volume": "spotlight",
    "abstract": "Recently, the powerful text-to-image capabilities of GPT-4o have led to growing appreciation for native multimodal large language models. However, its multimodal capabilities remain confined to images and text. Yet beyond images, the ability to understand and generate 3D content is equally crucial. To address this gap, we propose ShapeLLM-Omni—a native 3D large language model capable of understanding and generating 3D assets and text in any sequence. First, we train a 3D vector-quantized variational autoencoder (VQVAE), which maps 3D objects into a discrete latent space to achieve efficient and accurate shape representation and reconstruction. Building upon the 3D-aware discrete tokens, we innovatively construct a large-scale continuous training dataset named 3D-Alpaca, encompassing generation, comprehension, and editing, thus providing rich resources for future research and training. Finally, we perform instruction-based fine-tuning of the Qwen-2.5-vl-7B-Instruct model on the 3D-Alpaca dataset, equipping it with native 3D understanding and generation capabilities. Our work represents an effective step toward extending multimodal large language models with fundamental 3D intelligence, paving the way for future advances in 3D-native AI",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3UaItHVjyE": {
    "title": "The Implicit Bias of Structured State Space Models Can Be Poisoned With Clean Labels",
    "volume": "spotlight",
    "abstract": "Neural networks are powered by an implicit bias: a tendency of gradient descent to fit training data in a way that generalizes to unseen data. A recent class of neural network models gaining increasing popularity is structured state space models (SSMs). Prior work argued that the implicit bias of SSMs leads to generalization in a setting where data is generated by a low dimensional teacher. In this paper, we revisit the latter setting, and formally establish a phenomenon entirely undetected by prior work on the implicit bias of SSMs. Namely, we prove that while implicit bias leads to generalization under many choices of training data, there exist special examples whose inclusion in training completely distorts the implicit bias, to a point where generalization fails. This failure occurs despite the special training examples being labeled by the teacher, i.e., having clean labels! We empirically demonstrate the phenomenon, with SSMs trained independently and as part of non-linear neural networks. In the area of adversarial machine learning, disrupting generalization with cleanly labeled training examples is known as clean-label poisoning. Given the proliferation of SSMs, we believe that delineating their susceptibility to clean-label poisoning, and developing methods for overcoming this susceptibility, are critical research directions to pursue",
    "checked": true,
    "id": "6f1a06b90362ef4cba1338a9c48c2057fb4aed72",
    "semantic_title": "the implicit bias of structured state space models can be poisoned with clean labels",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=r9YDEErKXU": {
    "title": "Multiverse: Your Language Models Secretly Decide How to Parallelize and Merge Generation",
    "volume": "spotlight",
    "abstract": "Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit parallelism in sequential generation. Inspired by this, we introduce Multiverse, a new generative model enabling natively parallel generation. Multiverse internalizes a MapReduce paradigm, generating automatically through three stages: (i) a Map stage for adaptive task decomposition, (ii) a Process stage for parallel subtask execution, and (iii) a Reduce stage for lossless result synthesis. Next, we build a real-world Multiverse reasoning model with co-design of data, algorithm, and system, enabling rapid and seamless transfer from frontier AR-LLMs. For data creation, we develop Multiverse Curator, an automated LLM-assisted pipeline that transforms sequential reasoning chains into structured training data, avoiding costly human annotations. Algorithmically, we design Multiverse Attention to separate parallel reasoning steps while keeping compatibility with causal attention for efficient training. Systematically, we implement Multiverse Engine to support parallel inference. It features a dedicated interpreter that dynamically switches between sequential and parallel generation, triggered directly by the model. After a 3-hour fine-tuning with 1K examples, our Multiverse-32B stands as the only open-sourced non-AR model achieving performance on par with leading AR-LLMs of the same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively. Moreover, our budget control experiments show that Multiverse-32B exhibits superior scaling, outperforming AR-LLMs by 1.87% on average using the same context length. Such scaling further leads to practical efficiency gain, achieving up to 2x speedup across varying batch sizes. We have open-sourced the entire Multiverse ecosystem, including data, model weights, serving system, supporting tools, as well as data curation prompts and detailed training and evaluation recipes",
    "checked": true,
    "id": "6101d3273f5f69a53c8c5999c4f66aff23a8209a",
    "semantic_title": "multiverse: your language models secretly decide how to parallelize and merge generation",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=Ig5mtZ8etr": {
    "title": "BevSplat: Resolving Height Ambiguity via Feature-Based Gaussian Primitives for Weakly-Supervised Cross-View Localization",
    "volume": "spotlight",
    "abstract": "This paper addresses the problem of weakly supervised cross-view localization, where the goal is to estimate the pose of a ground camera relative to a satellite image with noisy ground truth annotations. A common approach to bridge the cross-view domain gap for pose estimation is Bird's-Eye View (BEV) synthesis. However, existing methods struggle with height ambiguity due to the lack of depth information in ground images and satellite height maps. Previous solutions either assume a flat ground plane or rely on complex models, such as cross-view transformers. We propose BevSplat, a novel method that resolves height ambiguity by using feature-based Gaussian primitives. Each pixel in the ground image is represented by a 3D Gaussian with semantic and spatial features, which are synthesized into a BEV feature map for relative pose estimation. We validate our method on the widely used KITTI and VIGOR datasets, which include both pinhole and panoramic query images. Experimental results show that BevSplat significantly improves localization accuracy over prior approaches",
    "checked": true,
    "id": "b8f1151e10abab432a9a84a9d1456f91e253914e",
    "semantic_title": "bevsplat: resolving height ambiguity via feature-based gaussian primitives for weakly-supervised cross-view localization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wHm5J9uanV": {
    "title": "Conflict-Aware Knowledge Editing in the Wild: Semantic-Augmented Graph Representation for Unstructured Text",
    "volume": "spotlight",
    "abstract": "Large Language Models (LLMs) have demonstrated broad applications but suffer from issues like hallucinations, erroneous outputs and outdated knowledge. Model editing emerges as an effective solution to refine knowledge in LLMs, yet existing methods typically depend on structured knowledge representations. However, real-world knowledge is primarily embedded within complex, unstructured text. Existing structured knowledge editing approaches face significant challenges when handling the entangled and intricate knowledge present in unstructured text, resulting in issues such as representation ambiguity and editing conflicts. To address these challenges, we propose a Conflict-Aware Knowledge Editing in the Wild (CAKE) framework, the first framework explicitly designed for editing knowledge extracted from wild unstructured text. CAKE comprises two core components: a Semantic-augmented Graph Representation module and a Conflict-aware Knowledge Editing strategy. The Semantic-augmented Graph Representation module enhances knowledge encoding through structural disambiguation, relational enrichment, and semantic diversification. Meanwhile, the Conflict-aware Knowledge Editing strategy utilizes a graph-theoretic coloring algorithm to disentangle conflicted edits by allocating them to orthogonal parameter subspaces, thereby effectively mitigating editing conflicts. Experimental results on the AKEW benchmark demonstrate that CAKE significantly outperforms existing methods, achieving a 15.43\\% improvement in accuracy on llama3 editing tasks. Our framework successfully bridges the gap between unstructured textual knowledge and reliable model editing, enabling more robust and scalable updates for practical LLM applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VsDsRqaMJv": {
    "title": "Vision Transformers with Self-Distilled Registers",
    "volume": "spotlight",
    "abstract": "Vision Transformers (ViTs) have emerged as the dominant architecture for visual processing tasks, demonstrating excellent scalability with increased training data and model size. However, recent work has identified the emergence of artifact tokens in ViTs that are incongruous with local semantics. These anomalous tokens degrade ViT performance in tasks that require fine-grained localization or structural coherence. An effective mitigation of this issue is the addition of register tokens to ViTs, which implicitly ''absorb'' the artifact term during training. Given the availability of existing large-scale pre-trained ViTs, in this paper we seek add register tokens to existing models without needing to re-train from scratch, which is infeasible considering their size. Specifically, we propose Post Hoc Registers (**PH-Reg**), an efficient self-distillation method that integrates registers into an existing ViT without requiring additional labeled data and full retraining. PH-Reg initializes both teacher and student networks from the same pre-trained ViT. The teacher remains frozen and unmodified, while the student is augmented with randomly initialized register tokens. By applying test-time augmentation to the teacher's inputs, we generate denoised dense embeddings free of artifacts, which are then used to optimize only a small subset of unlocked student weights. We show that our approach can effectively reduce the number of artifact tokens, improving the segmentation and depth prediction of the student ViT under zero-shot and linear probing",
    "checked": true,
    "id": "bc05f07d3d444c5c752951fd3bcef12a9cd517d3",
    "semantic_title": "vision transformers with self-distilled registers",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=igB289kbej": {
    "title": "EraseFlow: Learning Concept Erasure Policies via GFlowNet-Driven Alignment",
    "volume": "spotlight",
    "abstract": "Erasing harmful or proprietary concepts from powerful text‑to‑image generators is an emerging safety requirement, yet current ``concept erasure'' techniques either collapse image quality, rely on brittle adversarial losses, or demand prohibitive retraining cycles. We trace these limitations to a myopic view of the denoising trajectories that govern diffusion‑based generation. We introduce EraseFlow, the first framework that casts concept unlearning as exploration in the space of denoising paths and optimizes it with a GFlowNets equipped with the trajectory‑balance objective. By sampling entire trajectories rather than single end states, EraseFlow learns a stochastic policy that steers generation away from target concepts while preserving the model's prior. EraseFlow eliminates the need for carefully crafted reward models and by doing this, it generalizes effectively to unseen concepts and avoids hackable rewards while improving the performance. Extensive empirical results demonstrate that EraseFlow outperforms existing baselines and achieves an optimal trade-off between performance and prior preservation",
    "checked": true,
    "id": "808b6f529e9463f1606b0b1c16d68ab1103201aa",
    "semantic_title": "eraseflow: learning concept erasure policies via gflownet-driven alignment",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PluDA8DEar": {
    "title": "Boundary-Value PDEs Meet Higher-Order Differential Topology-aware GNNs",
    "volume": "spotlight",
    "abstract": "Recent advances in graph neural network (GNN)-based neural operators have demonstrated significant progress in solving partial differential equations (PDEs) by effectively representing computational meshes. However, most existing approaches overlook the intrinsic physical and topological meaning of higher-order elements in the mesh, which are closely tied to differential forms. In this paper, we propose a higher-order GNN framework that incorporates higher-order interactions based on discrete and finite element exterior calculus. The time-independent boundary value problems (BVPs) in electromagnetism are instantiated to illustrate the proposed framework. It can be easily generalized to other PDEs that admit differential form formulations. Moreover, the novel physics-informed loss terms, integrated form estimators, and theoretical support are derived correspondingly. Experiments show that our proposed method outperforms the existing neural operators by large margins on BVPs in electromagnetism. Our code is available at https://github.com/Supradax/Higher-Order-Differential-Topology-aware-GNN",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qMRFNxioPC": {
    "title": "4DGT: Learning a 4D Gaussian Transformer Using Real-World Monocular Videos",
    "volume": "spotlight",
    "abstract": "We propose 4DGT, a 4D Gaussian-based Transformer model for dynamic scene reconstruction, trained entirely on real-world monocular posed videos. Using 4D Gaussian as an inductive bias, 4DGT unifies static and dynamic components, enabling the modeling of complex, time-varying environments with varying object lifespans. We proposed a novel density control strategy in training, which enables our 4DGT to handle longer space-time input. Our model processes 64 consecutive posed frames in a rolling-window fashion, predicting consistent 4D Gaussians in the scene. Unlike optimization-based methods, 4DGT performs purely feed-forward inference, reducing reconstruction time from hours to seconds and scaling effectively to long video sequences. Trained only on large-scale monocular posed video datasets, 4DGT can outperform prior Gaussian-based networks significantly in real-world videos and achieve on-par accuracy with optimization-based methods on cross-domain videos",
    "checked": true,
    "id": "cc02ef373232c6620a9babb16ac02ddd88dfb250",
    "semantic_title": "4dgt: learning a 4d gaussian transformer using real-world monocular videos",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=wPdBe9zxNr": {
    "title": "CURE: Co-Evolving Coders and Unit Testers via Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "Mathematical reasoning in large language models has been successfully incentivized through reinforcement learning with verifiable rewards, leading to improved one-shot precision. In this work, we turn our focus to the coding domain. Beyond one-shot precision, we highlight unit test generation as another key factor for enhancing coding ability, since accurate unit tests are essential for enabling self-checking and self-correction during inference. Traditional approaches for fine-tuning LLMs on unit test generation rely heavily on ground-truth code solutions in the training data. We propose CURE, a novel reinforcement learning framework with a dedicated reward design that co-evolves coding and unit test generation capabilities based on their interaction outcomes—without any ground-truth code as supervision. This approach enables flexible and scalable training and allows the unit tester to learn directly from the coder's mistakes. Through extensive evaluations, we demonstrate that our CURE models, derived from base models of varying sizes, excel in both code generation and unit test generation. They naturally extend to downstream tasks such as test-time scaling—achieving a 6.2\\% improvement over the base model—and agentic unit test generation, with a 25.1\\% improvement. Our 4B model consistently outperforms Qwen3-4B while achieving 64.8\\% inference efficiency in unit test generation. Notably, we also find that the CURE model can serve as an effective reward model for reinforcement learning on base models, even in the absence of any labeled supervision",
    "checked": false,
    "id": "2c96c423d0cd481953e50ded37fb04921af02f5e",
    "semantic_title": "co-evolving llm coder and unit tester via reinforcement learning",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=IIiRwgkZcm": {
    "title": "How many measurements are enough? Bayesian recovery in inverse problems with general distributions",
    "volume": "spotlight",
    "abstract": "We study the sample complexity of Bayesian recovery for solving inverse problems with general prior, forward operator and noise distributions. We consider posterior sampling according to an approximate prior $\\mathcal{P}$, and establish sufficient conditions for stable and accurate recovery with high probability. Our main result is a non-asymptotic bound that shows that the sample complexity depends on (i) the intrinsic complexity of $\\mathcal{P}$, quantified by its *approximate covering number*, and (ii) concentration bounds for the forward operator and noise distributions. As a key application, we specialize to generative priors, where $\\mathcal{P}$ is the pushforward of a latent distribution via a Deep Neural Network (DNN). We show that the sample complexity scales log-linearly with the latent dimension $k$, thus establishing the efficacy of DNN-based priors. Generalizing existing results on deterministic (i.e., non-Bayesian) recovery for the important problem of random sampling with an orthogonal matrix $U$, we show how the sample complexity is determined by the *coherence* of $U$ with respect to the support of $\\mathcal{P}$. Hence, we establish that coherence plays a fundamental role in Bayesian recovery as well. Overall, our framework unifies and extends prior work, providing rigorous guarantees for the sample complexity of solving Bayesian inverse problems with arbitrary distributions",
    "checked": true,
    "id": "b93cc350c715291bd3155b26ae87307aaf869348",
    "semantic_title": "how many measurements are enough? bayesian recovery in inverse problems with general distributions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jom8tNYuQI": {
    "title": "Diffusion Generative Modeling on Lie Group Representations",
    "volume": "spotlight",
    "abstract": "We introduce a novel class of score-based diffusion processes that operate directly in the representation space of Lie groups. Leveraging the framework of Generalized Score Matching, we derive a class of Langevin dynamics that decomposes as a direct sum of Lie algebra representations, enabling the modeling of any target distribution on any (non-Abelian) Lie group. Standard score-matching emerges as a special case of our framework when the Lie group is the translation group. We prove that our generalized generative processes arise as solutions to a new class of paired stochastic differential equations (SDEs), introduced here for the first time. We validate our approach through experiments on diverse data types, demonstrating its effectiveness in real-world applications such as $\\text{SO}(3)$-guided molecular conformer generation and modeling ligand-specific global $\\text{SE}(3)$ transformations for molecular docking, showing improvement in comparison to Riemannian diffusion on the group itself. We show that an appropriate choice of Lie group enhances learning efficiency by reducing the effective dimensionality of the trajectory space and enables the modeling of transitions between complex data distributions",
    "checked": true,
    "id": "05e4118d6132ab53b185d4e8bb3bf4e8d95e222b",
    "semantic_title": "diffusion generative modeling on lie group representations",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=25C8oC1pb2": {
    "title": "EGGS: Exchangeable 2D/3D Gaussian Splatting for Geometry-Appearance Balanced Novel View Synthesis",
    "volume": "spotlight",
    "abstract": "Novel view synthesis (NVS) is crucial in computer vision and graphics, with wide applications in AR, VR, and autonomous driving. While 3D Gaussian Splatting (3DGS) enables real-time rendering with high appearance fidelity, it suffers from multi-view inconsistencies, limiting geometric accuracy. In contrast, 2D Gaussian Splatting (2DGS) enforces multi-view consistency but compromises texture details. To address these limitations, we propose Exchangeable Gaussian Splatting (EGGS), a hybrid representation that integrates 2D and 3D Gaussians to balance appearance and geometry. To achieve this, we introduce Hybrid Gaussian Rasterization for unified rendering, Adaptive Type Exchange for dynamic adaptation between 2D and 3D Gaussians, and Frequency-Decoupled Optimization that effectively exploits the strengths of each type of Gaussian representation. Our CUDA-accelerated implementation ensures efficient training and inference. Extensive experiments demonstrate that EGGS outperforms existing methods in rendering quality, geometric accuracy, and efficiency, providing a practical solution for high-quality NVS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zprMrpiLgT": {
    "title": "CURE: Concept Unlearning via Orthogonal Representation Editing in Diffusion Models",
    "volume": "spotlight",
    "abstract": "As Text-to-Image models continue to evolve, so does the risk of generating unsafe, copyrighted, or privacy-violating content. Existing safety interventions - ranging from training data curation and model fine-tuning to inference-time filtering and guidance - often suffer from incomplete concept removal, susceptibility to jail-breaking, computational inefficiency, or collateral damage to unrelated capabilities. In this paper, we introduce CURE, a training-free concept unlearning framework that operates directly in the weight space of pre-trained diffusion models, enabling fast, interpretable, and highly specific suppression of undesired concepts. At the core of our method is the Spectral Eraser, a closed-form, orthogonal projection module that identifies discriminative subspaces using Singular Value Decomposition over token embeddings associated with the concepts to forget and retain. Intuitively, the Spectral Eraser identifies and isolates features unique to the undesired concept while preserving safe attributes. This operator is then applied in a single step update to yield an edited model in which the target concept is effectively unlearned - without retraining, supervision, or iterative optimization. To balance the trade-off between filtering toxicity and preserving unrelated concepts, we further introduce an Expansion Mechanism for spectral regularization which selectively modulates singular vectors based on their relative significance to control the strength of forgetting. All the processes above are in closed-form, guaranteeing extremely efficient erasure in only $2$ seconds. Benchmarking against prior approaches, CURE achieves a more efficient and thorough removal for targeted artistic styles, objects, identities, or explicit content, with minor damage to original generation ability and demonstrates enhanced robustness against red-teaming. Project Page at \\url{https://sites.google.com/view/cure-unlearning/home}",
    "checked": true,
    "id": "71e248d2a56d64ea60d467db12d44843aed48f95",
    "semantic_title": "cure: concept unlearning via orthogonal representation editing in diffusion models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=alw3e1Qa7I": {
    "title": "Projection-based Lyapunov method for fully heterogeneous weakly-coupled MDPs",
    "volume": "spotlight",
    "abstract": "Heterogeneity poses a fundamental challenge for many real-world large-scale decision-making problems but remains largely understudied. In this paper, we study the _fully heterogeneous_ setting of a prominent class of such problems, known as weakly-coupled Markov decision processes (WCMDPs). Each WCMDP consists of $N$ arms (or subproblems), which have distinct model parameters in the fully heterogeneous setting, leading to the curse of dimensionality when $N$ is large. We show that, under mild assumptions, an efficiently computable policy achieves an $O(1/\\sqrt{N})$ optimality gap in the long-run average reward per arm for fully heterogeneous WCMDPs as $N$ becomes large. This is the _first asymptotic optimality result_ for fully heterogeneous average-reward WCMDPs. Our main technical innovation is the construction of projection-based Lyapunov functions that certify the convergence of rewards and costs to an optimal region, even under full heterogeneity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fBNaGVMDD9": {
    "title": "Low-degree evidence for computational transition of recovery rate in stochastic block model",
    "volume": "spotlight",
    "abstract": "We investigate implications of the (extended) low-degree conjecture (recently formalized in [moitra et al2023]) in the context of the symmetric stochastic block model. Assuming the conjecture holds, we establish that no polynomial-time algorithm can weakly recover community labels below the Kesten-Stigum (KS) threshold. In particular, we rule out polynomial-time estimators that, with constant probability, achieve $n^{-0.49}$ correlation with the true communities. Whereas, above the KS threshold, polynomial-time algorithms are known to achieve constant correlation with the true communities with high probability [massoulie et al 2014,abbe et al 2015]. To our knowledge, we provide the first rigorous evidence for such sharp transition in recovery rate for polynomial-time algorithms at the KS threshold. Notably, under a stronger version of the low-degree conjecture, our lower bound remains valid even when the number of blocks diverges. Furthermore, our results provide evidence of a computational-to-statistical gap in learning the parameters of stochastic block models. In contrast, prior work either (i) rules out polynomial-time algorithms with $1 - o(1)$ success probability [Hopkins 18, bandeira et al 2021] under the low-degree conjecture, or (ii) degree-$\\text{poly}(k)$ polynomials for learning the stochastic block model [Luo et al 2023]. For this, we design a hypothesis test which succeeeds with constant probability under symmetric stochastic block model, and $1-o(1)$ probability under the distribution of \\Erdos \\Renyi random graphs. Our proof combines low-degree lower bounds from [Hopkins 18, bandeira et al 2021] with graph splitting and cross-validation techniques. In order to rule out general recovery algorithms, we employ the correlation preserving projection method developed in [Hopkins et al 17]",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hLZrqDFugY": {
    "title": "Nonlinear Laplacians: Tunable principal component analysis under directional prior information",
    "volume": "spotlight",
    "abstract": "We introduce a new family of algorithms for detecting and estimating a rank-one signal from a noisy observation under prior information about that signal's direction, focusing on examples where the signal is known to have entries biased to be positive. Given a matrix observation $\\mathbf{Y}$, our algorithms construct a *nonlinear Laplacian*, another matrix of the form $\\mathbf{Y} + \\mathrm{diag}(\\sigma(\\mathbf{Y1}))$ for a nonlinear $\\sigma: \\mathbb{R} \\to \\mathbb{R}$, and examine the top eigenvalue and eigenvector of this matrix. When $\\mathbf{Y}$ is the (suitably normalized) adjacency matrix of a graph, our approach gives a class of algorithms that search for unusually dense subgraphs by computing a spectrum of the graph \"deformed\" by the degree profile $\\mathbf{Y1}$. We study the performance of such algorithms compared to direct spectral algorithms (the case $\\sigma = 0$) on models of sparse principal component analysis with biased signals, including the Gaussian planted submatrix problem. For such models, we rigorously characterize the strength of rank-one signal, as a function of the nonlinearity $\\sigma$, required for an outlier eigenvalue to appear in the spectrum of a nonlinear Laplacian matrix. While identifying the $\\sigma$ that minimizes the required signal strength in closed form seems intractable, we explore three approaches to design $\\sigma$ numerically: exhaustively searching over simple classes of $\\sigma$, learning $\\sigma$ from datasets of problem instances, and tuning $\\sigma$ using black-box optimization of the critical signal strength. We find both theoretically and empirically that, if $\\sigma$ is chosen appropriately, then nonlinear Laplacian spectral algorithms substantially outperform direct spectral algorithms, while retaining the conceptual simplicity of spectral methods compared to broader classes of computations like approximate message passing or general first order methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I8S4ASqO5H": {
    "title": "Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and Preference Alignment",
    "volume": "spotlight",
    "abstract": "Modern single-image super-resolution (SISR) models deliver photo-realistic results at the scale factors on which they are trained, but collapse when asked to magnify far beyond that regime. We address this scalability bottleneck with Chain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an autoregressive chain of intermediate scale-states with multi-scale-aware prompts. CoZ repeatedly re-uses a backbone SR model, decomposing the conditional probability into tractable sub-problems to achieve extreme resolutions without additional training. Because visual cues diminish at high magnifications, we augment each zoom step with multi-scale-aware text prompts generated by a vision-language model (VLM). The prompt extractor itself is fine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic VLM, aligning text guidance towards human preference. Experiments show that a standard $4\\times$ diffusion SR model wrapped in CoZ attains beyond $256\\times$ enlargement with high perceptual quality and fidelity",
    "checked": true,
    "id": "77070e7b649f7ca29e7641579ecb13dba79a3c7c",
    "semantic_title": "chain-of-zoom: extreme super-resolution via scale autoregression and preference alignment",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=RDt0crdC7N": {
    "title": "Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning",
    "volume": "spotlight",
    "abstract": "Decision-making models for individuals, particularly in high-stakes scenarios like vaccine uptake, often diverge from population optimal predictions. This gap arises from the uniqueness of the individual decision-making process, shaped by numerical attributes (e.g., cost, time) and linguistic influences (e.g., personal preferences and constraints). Developing upon Utility Theory and leveraging the textual-reasoning capabilities of Large Language Models (LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric Reasoning framework (ATHENA) to address the optimal information integration. ATHENA uniquely integrates two stages: First, it discovers robust, group-level symbolic utility functions via LLM-augmented symbolic discovery; Second, it implements individual-level semantic adaptation, creating personalized semantic templates guided by the optimal utility to model personalized choices. Validated on real-world travel mode and vaccine choice tasks, ATHENA consistently outperforms utility-based, machine learning, and other LLM-based models, lifting F1 score by at least 6.5\\% over the strongest cutting-edge models. Further, ablation studies confirm that both stages of ATHENA are critical and complementary, as removing either clearly degrades overall predictive performance. By organically integrating symbolic utility modeling and semantic adaptation, ATHENA provides a new scheme for modeling human-centric decisions. The project page can be found at https://yibozh.github.io/Athena",
    "checked": true,
    "id": "725cc242345b6419419861a8f901195174fc7e8b",
    "semantic_title": "personalized decision modeling: utility optimization or textualized-symbolic reasoning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OiC78C68sJ": {
    "title": "What are you sinking? A geometric approach on attention sink",
    "volume": "spotlight",
    "abstract": "Attention sink (AS) is a consistent pattern in transformer attention maps where certain tokens (often special tokens or positional anchors) disproportionately attract attention from other tokens. We show that in transformers, AS is not an architectural artifact, but it is the manifestation of a fundamental geometric principle: the establishment of reference frames that anchor representational spaces. We analyze several architectures and identify three distinct reference frame types, centralized, distributed, and bidirectional, that correlate with the attention sink phenomenon. We show that they emerge during the earliest stages of training as optimal solutions to the problem of establishing stable coordinate systems in high-dimensional spaces. We show the influence of architecture components, particularly position encoding implementations, on the specific type of reference frame. This perspective transforms our understanding of transformer attention mechanisms and provides insights for both architecture design and the relationship with AS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k0wyi4cOGy": {
    "title": "KARMA: Leveraging Multi-Agent LLMs for Automated Knowledge Graph Enrichment",
    "volume": "spotlight",
    "abstract": "Maintaining comprehensive and up-to-date knowledge graphs (KGs) is critical for modern AI systems, but manual curation struggles to scale with the rapid growth of scientific literature. This paper presents KARMA, a novel framework employing multi-agent large language models (LLMs) to automate KG enrichment through structured analysis of unstructured text. Our approach employs nine collaborative agents, spanning entity discovery, relation extraction, schema alignment, and conflict resolution that iteratively parse documents, verify extracted knowledge, and integrate it into existing graph structures while adhering to domain-specific schema. Experiments on 1,200 PubMed articles from three different domains demonstrate the effectiveness of KARMA in knowledge graph enrichment, with the identification of up to 38,230 new entities while achieving 83.1\\% LLM-verified correctness and reducing conflict edges by 18.6\\% through multi-layer assessments",
    "checked": true,
    "id": "670a10114f5b29a289d2759005730125baac27ad",
    "semantic_title": "karma: leveraging multi-agent llms for automated knowledge graph enrichment",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=rGMaZkn1ve": {
    "title": "MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent Systems",
    "volume": "spotlight",
    "abstract": "Human social interactions depend on the ability to infer others' unspoken intentions, emotions, and beliefs—a cognitive skill grounded in the psychological concept of Theory of Mind (ToM). While large language models (LLMs) excel in semantic understanding tasks, they struggle with the ambiguity and contextual nuance inherent in human communication. To bridge this gap, we introduce **MetaMind**, a multi-agent framework inspired by psychological theories of metacognition, designed to emulate human-like social reasoning. MetaMind decomposes social understanding into three collaborative stages: (1) a *Theory-of-Mind Agent* generates hypotheses about user mental states (e.g., intent, emotion), (2) a *Moral Agent* refines these hypotheses using cultural norms and ethical constraints, and (3) a *Response Agent* generates contextually appropriate responses while validating alignment with inferred intent. Our framework achieves state-of-the-art performance across three challenging benchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain in ToM reasoning. Notably, it enables LLMs to match human-level performance on key ToM tasks for the first time. Ablation studies confirm the necessity of all components, which showcase the framework's ability to balance contextual plausibility, social appropriateness, and user adaptation. This work advances AI systems toward human-like social intelligence, with applications in empathetic dialogue and culturally sensitive interactions. Code is available at https://github.com/XMZhangAI/MetaMind",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t77EZLjvd5": {
    "title": "Enhancing CLIP Robustness via Cross-Modality Alignment",
    "volume": "spotlight",
    "abstract": "Vision-language models (VLMs) such as CLIP demonstrate strong generalization in zero-shot classification but remain highly vulnerable to adversarial perturbations. Existing methods primarily focus on adversarial fine-tuning or prompt optimization, they often overlook the gaps in CLIP's encoded features, which is shown as the text and image features lie far apart from each other. This misalignment is significantly amplified under adversarial perturbations, leading to severe degradation in classification performance. To address this problem, we propose **C**r**O**ss-moda**L**ity **A**lignment, dubbed **COLA**, an optimal transport-based framework that explicitly addresses adversarial misalignment by restoring both global image-text alignment and local structural consistency in the feature space. (1) COLA first projects adversarial image embeddings onto a subspace spanned by class text features, effectively filtering out non-semantic distortions while preserving discriminative information. (2) It then models images and texts as discrete distributions over multiple augmented views and refines their alignment via OT, with the subspace projection seamlessly integrated into the cost computation. This design ensures stable cross-modal alignment even under adversarial conditions. COLA is training-free and compatible with existing fine-tuned models. Extensive evaluations across 14 zero-shot classification benchmarks demonstrate the effectiveness of COLA, especially with an average improvement of 6.7% on ImageNet and its variants under PGD adversarial attacks, while maintaining high accuracy on clean samples",
    "checked": true,
    "id": "c46a17693dab7191c8f2c5569437eb4f61377300",
    "semantic_title": "enhancing clip robustness via cross-modality alignment",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BKYFAutCDZ": {
    "title": "Rethinking Entropy in Test-Time Adaptation: The Missing Piece from Energy Duality",
    "volume": "spotlight",
    "abstract": "Test-time adaptation (TTA) aims to preserve model performance under distribution shifts. Yet, most existing methods rely on entropy minimization for confident predictions. This paper re-examines the sufficiency of entropy minimization by analyzing its dual relationship with energy. We view energy as a proxy for likelihood, where lower energy indicates higher observability under the learned distribution. We uncover that entropy and energy are tightly associated, controlled by the model's confidence or ambiguity, and show that simultaneous reduction of both is essential. Importantly, we reveal that entropy minimization alone neither ensures energy reduction nor supports reliable likelihood estimation, and it requires explicit discriminative guidance to reach zero entropy. To combat these problems, we propose a twofold solution. First, we introduce a likelihood-based objective grounded in energy-based models, which reshape the energy landscape to favor test samples. For stable and scalable training, we adopt sliced score matching—a sampling-free, Hessian-insensitive approximation of Fisher divergence. Second, we enhance entropy minimization with a cross-entropy that treats the predicted class as a target to promote discriminability. By counterbalancing entropy and energy through the solution of multi-objective optimization, our unified TTA, ReTTA, outperforms existing entropy- or energy-based approaches across diverse distribution shifts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rVT1GK60Nt": {
    "title": "On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order Optimization",
    "volume": "spotlight",
    "abstract": "Zeroth-order optimization (ZOO) is an important framework for stochastic optimization when gradients are unavailable or expensive to compute. A potential limitation of existing ZOO methods is the bias inherent in most gradient estimators unless the perturbation stepsize vanishes. In this paper, we overcome this biasedness issue by proposing a novel family of *unbiased* gradient estimators based solely on function evaluations. By reformulating directional derivatives as a telescoping series and sampling from carefully designed distributions, we construct estimators that eliminate bias while maintaining favorable variance. We analyze their theoretical properties, derive optimal scaling distributions and perturbation stepsizes of four specific constructions, and prove that SGD using the proposed estimators achieves optimal complexity for smooth non-convex objectives. Experiments on synthetic tasks and language model fine-tuning confirm the superior accuracy and convergence of our approach compared to standard methods",
    "checked": true,
    "id": "94e52750837d708477490aafb1e1624fdfee8f16",
    "semantic_title": "on the optimal construction of unbiased gradient estimators for zeroth-order optimization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pVaqdFlUAO": {
    "title": "Gradient Variance Reveals Failure Modes in Flow-Based Generative Models",
    "volume": "spotlight",
    "abstract": "Rectified Flows learn ODE vector fields whose trajectories are straight between source and target distributions, enabling near one-step inference. We show that this straight-path objective reveals fundamental failure modes: under deterministic training, low gradient variance drives memorization of arbitrary training pairings, even when interpolant lines between training pairs intersect. To analyze this mechanism, we study Gaussian-to-Gaussian transport and use the loss gradient variance across stochastic and deterministic regimes to characterize which vector fields optimization favors in each setting. We then show that, in a setting where all interpolating lines intersect, applying Rectified Flow yields the same specific pairings at inference as during training. More generally, we prove that a memorizing vector field exists even when training interpolants intersect, and that optimizing the straight-path objective converges to this ill-defined field. At inference, deterministic integration reproduces the exact training pairings. We validate our findings empirically on the CelebA dataset, confirming that deterministic interpolants induce memorization, while the injection of small noise restores generalization",
    "checked": true,
    "id": "dab57fb5af4d1a7bb6d7c2e95827a9e1697d4705",
    "semantic_title": "gradient variance reveals failure modes in flow-based generative models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AHccBzULR7": {
    "title": "TREND: Unsupervised 3D Representation Learning via Temporal Forecasting for LiDAR Perception",
    "volume": "spotlight",
    "abstract": "Labeling LiDAR point clouds is notoriously time-and-energy-consuming, which spurs recent unsupervised 3D representation learning methods to alleviate the labeling burden in LiDAR perception via pretrained weights. Existing work focus on either masked auto encoding or contrastive learning on LiDAR point clouds, which neglects the temporal LiDAR sequence that naturally accounts for object motion (and their semantics). Instead, we propose TREND, short for Temporal REndering with Neural fielD, to learn 3D representation via forecasting the future observation in an unsupervised manner. TREND integrates forecasting for 3D pre-training through a Recurrent Embedding scheme to generate 3D embeddings across time and a Temporal LiDAR Neural Field specifically designed for LiDAR modality to represent the 3D scene, with which we compute the loss using differentiable rendering. We evaluate TREND on 3D object detection and LiDAR semantic segmentation tasks on popular datasets, including Once, Waymo, NuScenes, and SemanticKITTI. TREND generally improves from-scratch models across datasets and tasks and brings gains of 1.77\\% mAP on Once and 2.11\\% mAP on NuScenes, which are up to 400\\% more improvement compared to previous SOTA unsupervised 3D pre-training methods. Codes and models will be available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i4qAfV04rZ": {
    "title": "Boosting Generative Image Modeling via Joint Image-Feature Synthesis",
    "volume": "spotlight",
    "abstract": "Latent diffusion models (LDMs) dominate high-quality image generation, yet integrating representation learning with generative modeling remains a challenge. We introduce a novel generative image modeling framework that seamlessly bridges this gap by leveraging a diffusion model to jointly model low-level image latents (from a variational autoencoder) and high-level semantic features (from a pretrained self-supervised encoder like DINO). Our latent-semantic diffusion approach learns to generate coherent image-feature pairs from pure noise, significantly enhancing both generative quality and training efficiency, all while requiring only minimal modifications to standard Diffusion Transformer architectures. By eliminating the need for complex distillation objectives, our unified design simplifies training and unlocks a powerful new inference strategy: Representation Guidance, which leverages learned semantics to steer and refine image generation. Evaluated in both conditional and unconditional settings, our method delivers substantial improvements in image quality and training convergence speed, establishing a new direction for representation-aware generative modeling",
    "checked": true,
    "id": "63c2eb636abf2dd0454469b636ddb4a65cd2d2cb",
    "semantic_title": "boosting generative image modeling via joint image-feature synthesis",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=LVDRJE4xQ2": {
    "title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone",
    "volume": "spotlight",
    "abstract": "Training high-performing Small Language Models (SLMs) remains computationally expensive, even with knowledge distillation and pruning from larger teacher models. Existing approaches often face three key challenges: (1) information loss from hard pruning, (2) inefficient alignment of representations, and (3) underutilization of informative activations, particularly from Feed-Forward Networks (FFNs). To address these challenges, we introduce \\textbf{Low-Rank Clone (LRC)}, an efficient pre-training method that constructs SLMs aspiring to behavioral equivalence with strong teacher models. LRC trains a set of low-rank projection matrices that jointly enable soft pruning by compressing teacher weights, and activation clone by aligning student activations, including FFN signals, with those of the teacher. This unified design maximizes knowledge transfer while removing the need for explicit alignment modules. Extensive experiments with open-source teachers such as Llama-3.2-3B-Instruct and Qwen2.5-3B/7B-Instruct show that LRC matches or surpasses the performance of state-of-the-art models trained on trillions of tokens--using only 20B tokens, achieving over \\textbf{1,000$\\times$} greater training efficiency. Our codes and model checkpoints are available at https://github.com/CURRENTF/LowRankClone and https://huggingface.co/JitaiHao/LRC-4B-Base",
    "checked": true,
    "id": "9da407416c1f8291a358654a4e05a84526c24aea",
    "semantic_title": "a token is worth over 1,000 tokens: efficient knowledge distillation through low-rank clone",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=cvJvk6oYfC": {
    "title": "SparseMVC: Probing Cross-view Sparsity Variations for Multi-view Clustering",
    "volume": "spotlight",
    "abstract": "Existing multi-view clustering methods employ various strategies to address data-level sparsity and view-level dynamic fusion. However, we identify a critical yet overlooked issue: varying sparsity across views. Cross-view sparsity variations lead to encoding discrepancies, heightening sample-level semantic heterogeneity and making view-level dynamic weighting inappropriate. To tackle these challenges, we propose Adaptive Sparse Autoencoders for Multi-View Clustering (SparseMVC), a framework with three key modules. Initially, the sparse autoencoder probes the sparsity of each view and adaptively adjusts encoding formats via an entropy-matching loss term, mitigating cross-view inconsistencies. Subsequently, the correlation-informed sample reweighting module employs attention mechanisms to assign weights by capturing correlations between early-fused global and view-specific features, reducing encoding discrepancies and balancing contributions. Furthermore, the cross-view distribution alignment module aligns feature distributions during the late fusion stage, accommodating datasets with an arbitrary number of views. Extensive experiments demonstrate that SparseMVC achieves state-of-the-art clustering performance. Our framework advances the field by extending sparsity handling from the data-level to view-level and mitigating the adverse effects of encoding discrepancies through sample-level dynamic weighting. The source code is publicly available at https://github.com/cleste-pome/SparseMVC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WWa5x1WnEw": {
    "title": "Unbiased Prototype Consistency Learning for Multi-Modal and Multi-Task Object Re-Identification",
    "volume": "spotlight",
    "abstract": "In object re-identification (ReID) task, both cross-modal and multi-modal retrieval methods have achieved notable progress. However, existing approaches are designed for specific modality and category (person or vehicle) retrieval task, lacking generalizability to others. Acquiring multiple task-specific models would result in wasteful allocation of both training and deployment resources. To address the practical requirements for unified retrieval, we introduce Multi-Modal and Multi-Task object ReID ($\\rm {M^3T}$-ReID). The $\\rm {M^3T}$-ReID task aims to utilize a unified model to simultaneously achieve retrieval tasks across different modalities and different categories. Specifically, to tackle the challenges of modality distibution divergence and category semantics discrepancy posed in $\\rm {M^3T}$-ReID, we design a novel Unbiased Prototype Consistency Learning (UPCL) framework, which consists of two main modules: Unbiased Prototypes-guided Modality Enhancement (UPME) and Cluster Prototype Consistency Regularization (CPCR). UPME leverages modality-unbiased prototypes to simultaneously enhance cross-modal shared features and multi-modal fused features. Additionally, CPCR regulates discriminative semantics learning with category-consistent information through prototypes clustering. Under the collaborative operation of these two modules, our model can simultaneously learn robust cross-modal shared feature and multi-modal fused feature spaces, while also exhibiting strong category-discriminative capabilities. Extensive experiments on multi-modal datasets RGBNT201 and RGBNT100 demonstrates our UPCL framework showcasing exceptional performance for $\\rm {M^3T}$-ReID. The code is available at https://github.com/ZhouZhongao/UPCL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=56C0n6zSpC": {
    "title": "MJ-Video: Benchmarking and Rewarding Video Generation with Fine-Grained Video Preference",
    "volume": "spotlight",
    "abstract": "Recent advancements in video generation have significantly improved the ability to synthesize videos from text instructions. However, existing models still struggle with key challenges such as instruction misalignment, content hallucination, safety concerns, and generation bias. To address these limitations, we introduce MJ-BENCH-VIDEO, a large-scale video preference benchmark designed to evaluate video generation across five critical aspects: Alignment, Safety, Fineness, Coherence & Consistency, and Bias & Fairness. This benchmark further incorporates 28 fine-grained criteria to provide a comprehensive evaluation of video preference. Building upon this dataset, we propose MJ-VIDEO, a Mixture-of-Experts (MoE)-based video reward model designed to deliver fine-grained reward. MJ-VIDEO can dynamically select relevant experts to accurately judge the preference based on the input text-video pair. This architecture enables more precise and adaptable preference judgments. Through extensive benchmarking on MJ-BENCH-VIDEO, we analyze the limitations of existing video reward models and demonstrate the superior performance of MJ-VIDEO in video preference assessment, achieving 17.58% and 15.87% improvements in overall and fine-grained preference judgments, respectively. Additionally, MJ-VIDEO is able to improve the alignment performance in video generation via preference fine-tuning",
    "checked": false,
    "id": "81e8919c37279571c3b227a3e6b24a656b8409be",
    "semantic_title": "mj-video: fine-grained benchmarking and rewarding video preferences in video generation",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=bP5cU0OYSn": {
    "title": "Fast Projection-Free Approach (without Optimization Oracle) for Optimization over Compact Convex Set",
    "volume": "spotlight",
    "abstract": "Projection-free first-order methods, e.g., the celebrated Frank-Wolfe (FW) algorithms, have emerged as powerful tools for optimization over simple convex sets such as polyhedra, because of their scalability, fast convergence, and iteration-wise feasibility without costly projections. However, extending these methods effectively to general compact convex sets remains challenging and largely open, as FW methods rely on expensive linear optimization oracles (LOO), while penalty-based methods often struggle with poor feasibility. We tackle this open challenge by presenting **Hom-PGD**, a novel projection-free method without expensive (optimization) oracles. Our method constructs a homeomorphism between the convex constraint set and a unit ball, transforming the original problem into an equivalent ball-constrained formulation, thus enabling efficient gradient-based optimization while preserving the original problem structure. We prove that Hom-PGD attains *optimal* convergence rates matching gradient descent with constant step-size to find an $\\epsilon$-approximate (stationary) solution: $\\mathcal{O}(\\log (1/\\epsilon))$ for strongly convex objectives, $\\mathcal{O}(\\epsilon^{-1})$ for convex objectives, and $\\mathcal{O}(\\epsilon^{-2})$ for non-convex objectives. Meanwhile, Hom-PGD enjoys a low per-iteration complexity of $\\mathcal{O}(n^2)$, without expensive oracles like LOO or projection, where $n$ is the input size. Our framework further extends to certain non-convex sets, broadening its applicability in practical optimization scenarios with complex constraints. Extensive numerical experiments demonstrate that Hom-PGD achieves comparable convergence rates to state-of-the-art projection-free methods, while significantly reducing per-iteration runtime (up to 5 orders of magnitude faster) and thus the total problem-solving time",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jAsr5GHt3P": {
    "title": "Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive Branching Tree Search",
    "volume": "spotlight",
    "abstract": "Recent advances demonstrate that increasing inference-time computation can significantly boost the reasoning capabilities of large language models (LLMs). Although repeated sampling (i.e., generating multiple candidate outputs) is a highly effective strategy, it does not leverage external feedback signals for refinement, which are often available in tasks like coding. In this work, we propose Adaptive Branching Monte Carlo Tree Search (AB-MCTS), a novel inference-time framework that generalizes repeated sampling with principled multi-turn exploration and exploitation. At each node in the search tree, AB-MCTS dynamically decides whether to ''go wider'' by expanding new candidate responses or ''go deeper'' by revisiting existing ones based on external feedback signals. We evaluate our method on complex coding and engineering tasks using frontier models. Empirical results show that AB-MCTS outperforms both repeated sampling and standard MCTS, underscoring the importance of combining the response diversity of LLMs with multi-turn solution refinement for effective inference-time scaling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aSfBbhUJAa": {
    "title": "RepoMaster: Autonomous Exploration and Understanding of GitHub Repositories for Complex Task Solving",
    "volume": "spotlight",
    "abstract": "The ultimate goal of code agents is to solve complex tasks autonomously. Although large language models (LLMs) have made substantial progress in code generation, real-world tasks typically demand full-fledged code repositories rather than simple scripts. Building such repositories from scratch remains a major challenge. Fortunately, GitHub hosts a vast, evolving collection of open-source repositories, which developers frequently reuse as modular components for complex tasks. Yet, existing frameworks like OpenHands and SWE-Agent still struggle to effectively leverage these valuable resources. Relying solely on README files provides insufficient guidance, and deeper exploration reveals two core obstacles: overwhelming information and tangled dependencies of repositories, both constrained by the limited context windows of current LLMs. To tackle these issues, we propose RepoMaster, an autonomous agent framework designed to explore and reuse GitHub repositories for solving complex tasks. For efficient understanding, RepoMaster constructs function-call graphs, module-dependency graphs, and hierarchical code trees to identify essential components, providing only identified core elements to the LLMs rather than the entire repository. During autonomous execution, it progressively explores related components using our exploration tools and prunes information to optimize context usage. Evaluated on the adjusted MLE-bench, RepoMaster achieves a 110\\% relative boost in valid submissions over the strongest baseline OpenHands. On our newly released GitTaskBench, RepoMaster lifts the task-pass rate from 40.7% to 62.9% while reducing token usage by 95%. Our code and demonstration materials are publicly available at https://github.com/QuantaAlpha/RepoMaster",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=721bDIvjen": {
    "title": "Hierarchical Shortest-Path Graph Kernel Network",
    "volume": "spotlight",
    "abstract": "Graph kernels have emerged as a fundamental and widely adopted technique in graph machine learning. However, most existing graph kernel methods rely on fixed graph similarity estimation that cannot be directly optimized for task-specific objectives, leading to sub-optimal performance. To address this limitation, we propose a kernel-based learning framework called Hierarchical Shortest-Path Graph Kernel Network HSP-GKN, which seamlessly integrates graph similarity estimation with downstream tasks within a unified optimization framework. Specifically, we design a hierarchical shortest-path graph kernel that efficiently preserves both the semantic and structural information of a given graph by transforming it into hierarchical features used for subsequent neural network learning. Building upon this kernel, we develop a novel end-to-end learning framework that matches hierarchical graph features with learnable $hidden$ graph features to produce a similarity vector. This similarity vector subsequently serves as the graph embedding for end-to-end training, enabling the neural network to learn task-specific representations. Extensive experimental results demonstrate the effectiveness and superiority of the designed kernel and its corresponding learning framework compared to current competitors",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=piM21sPyVL": {
    "title": "Differential Privacy on Fully Dynamic Streams",
    "volume": "spotlight",
    "abstract": "A fundamental problem in differential privacy is to release privatized answers to a class of linear queries with small error. This problem has been well studied in the static case. In this paper, we consider the fully dynamic setting where items may be inserted into or deleted from the dataset over time, and we need to continually release query answers at every time instance. We present efficient black-box constructions of such dynamic differentially private mechanisms from static ones with only a polylogarithmic degradation in the utility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TYroQXu6X0": {
    "title": "Shortcut Features as Top Eigenfunctions of NTK: A Linear Neural Network Case and More",
    "volume": "spotlight",
    "abstract": "One of the chronic problems of deep-learning models is shortcut learning. In a case where the majority of training data are dominated by a certain feature, neural networks prefer to learn such a feature even if the feature is not generalizable outside the training set. Based on the framework of Neural Tangent Kernel (NTK), we analyzed the case of linear neural networks to derive some important properties of shortcut learning. We defined a \"feature\" of a neural network as an eigenfunction of NTK. Then, we found that shortcut features correspond to features with larger eigenvalues when the shortcuts stem from the imbalanced number of samples in the clustered distribution. We also showed that the features with larger eigenvalues still have a large influence on the neural network output even after training, due to data variances in the clusters. Such a preference for certain features remains even when a margin of a neural network output is controlled, which shows that the max-margin bias is not the only major reason for shortcut learning. These properties of linear neural networks are empirically extended for more complex neural networks as a two-layer ReLU FC network and a ResNet-18",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uwL0vbeEVn": {
    "title": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI Applications",
    "volume": "spotlight",
    "abstract": "Speculative decoding is widely adopted to reduce latency in large language model (LLM) inference by leveraging smaller draft models capable of handling diverse user tasks. However, emerging AI applications, such as LLM-based agents, present unique workload characteristics: instead of diverse independent requests, agentic frameworks typically submit repetitive inference requests, such as multi-agent pipelines performing similar subtasks or self-refinement loops iteratively enhancing outputs. These workloads result in long and highly predictable sequences, which current speculative decoding methods do not effectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a novel method that utilizes efficient suffix trees to cache long token sequences from prompts and previous outputs. By adaptively speculating more tokens when acceptance likelihood is high and fewer when it is low, SuffixDecoding effectively exploits opportunities for longer speculations while conserving computation when those opportunities are limited. Evaluations on agentic benchmarks, including SWE-Bench and Text-to-SQL, demonstrate that SuffixDecoding achieves speedups of up to 3.9$\\times$, outperforming state-of-the-art methods -- 2.2$\\times$ faster than model-based approaches like EAGLE-2/3 and 1.6$\\times$ faster than model-free approaches such as Token Recycling. SuffixDecoding is open-sourced",
    "checked": true,
    "id": "6b541c1fa224aae12667f71bdd3e5033a5689499",
    "semantic_title": "suffixdecoding: extreme speculative decoding for emerging ai applications",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=yOs12gdsaL": {
    "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context Transformer Inference",
    "volume": "spotlight",
    "abstract": "Although applications involving long-context inputs are crucial for the effective utilization of large language models (LLMs), they also result in increased computational costs and reduced performance. To address this challenge, we propose an efficient, training-free prompt compression method that retains key information within compressed prompts. We identify specific attention heads in transformer-based LLMs, which we designate as evaluator heads, that are capable of selecting tokens in long inputs that are most significant for inference. Building on this discovery, we develop EHPC, an Evaluator Head-based Prompt Compression method, which enables LLMs to rapidly \"skim through'' input prompts by leveraging only the first few layers with evaluator heads during the pre-filling stage, subsequently passing only the important tokens to the model for inference. EHPC achieves state-of-the-art results across two mainstream benchmarks: prompt compression and long-context inference acceleration. Consequently, it effectively improves performance with the reduced costs associated with commercial API calls compared to prompt compressing methods. We further demonstrate that EHPC attains competitive results compared to key-value cache-based acceleration methods, thereby highlighting its potential to enhance the efficiency of LLMs for long-context tasks",
    "checked": true,
    "id": "21d89be6d581fcf2d8543feb47cb09f369ba0a92",
    "semantic_title": "efficient prompt compression with evaluator heads for long-context transformer inference",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=1bO9wIdyKa": {
    "title": "UniRelight: Learning Joint Decomposition and Synthesis for Video Relighting",
    "volume": "spotlight",
    "abstract": "We address the challenge of relighting a single image or video, a task that demands precise scene intrinsic understanding and high-quality light transport synthesis. Existing end-to-end relighting models are often limited by the scarcity of paired multi-illumination data, restricting their ability to generalize across diverse scenes. Conversely, two-stage pipelines that combine inverse and forward rendering can mitigate data requirements but are susceptible to error accumulation and often fail to produce realistic outputs under complex lighting conditions or with sophisticated materials. In this work, we introduce a general-purpose approach that jointly estimates albedo and synthesizes relit outputs in a single pass, harnessing the generative capabilities of video diffusion models. This joint formulation enhances implicit scene comprehension and facilitates the creation of realistic lighting effects and intricate material interactions, such as shadows, reflections, and transparency. Trained on synthetic multi-illumination data and extensive automatically labeled real-world videos, our model demonstrates strong generalization across diverse domains and surpasses previous methods in both visual fidelity and temporal consistency. Our project page is https://research.nvidia.com/labs/toronto-ai/UniRelight/",
    "checked": true,
    "id": "db614435aac0b9d9c26ff3d2c4e7453bcecc91eb",
    "semantic_title": "unirelight: learning joint decomposition and synthesis for video relighting",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=w5uUvxp81b": {
    "title": "When Thinking Fails: The Pitfalls of Reasoning for Instruction-Following in LLMs",
    "volume": "spotlight",
    "abstract": "Reasoning-enhanced large language models (RLLMs), whether explicitly trained for reasoning or prompted via chain-of-thought (CoT), have achieved state-of-the-art performance on many complex reasoning tasks. However, we uncover a surprising and previously overlooked phenomenon: explicit CoT reasoning can significantly degrade instruction-following accuracy. Evaluating 20+ models on two benchmarks: IFEval (with simple, rule-verifiable constraints) and ComplexBench (with complex, compositional constraints), we consistently observe performance drops when CoT prompting is applied. Through large-scale case studies and an attention-based analysis, we identify common patterns where reasoning either helps (e.g., with formatting or lexical precision) or hurts (e.g., by neglecting simple constraints or introducing unnecessary content). We propose a metric, constraint attention, to quantify model focus during generation and show that CoT reasoning often diverts attention away from instruction-relevant tokens. To mitigate these effects, we introduce and evaluate four strategies: in-context learning, self-reflection, self-selective reasoning, and classifier-selective reasoning. Our results demonstrate that selective reasoning strategies, particularly classifier-selective reasoning, can substantially recover lost performance. To our knowledge, this is the first work to systematically expose reasoning-induced failures in instruction-following and offer practical mitigation strategies",
    "checked": true,
    "id": "d1f0e7706a4f46086e6db1dbd41028c72892ce01",
    "semantic_title": "when thinking fails: the pitfalls of reasoning for instruction-following in llms",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=6LOgOsIcXe": {
    "title": "On Agnostic PAC Learning in the Small Error Regime",
    "volume": "spotlight",
    "abstract": "Binary classification in the classic PAC model exhibits a curious phenomenon: Empirical Risk Minimization (ERM) learners are suboptimal in the realizable case yet optimal in the agnostic case. Roughly speaking, this owes itself to the fact that non-realizable distributions $\\\\mathcal{D}$ are more difficult to learn than realizable distributions -- even when one discounts a learner's error by $\\\\mathrm{err}(h^\\\\ast_\\\\mathcal{D})$, i.e., the error of the best hypothesis in $\\\\mathcal{H}$. Thus, optimal agnostic learners are permitted to incur excess error on (easier-to-learn) distributions $\\\\mathcal{D}$ for which $\\\\tau = \\\\mathrm{err}(h^\\\\ast_\\\\mathcal{D})$ is small. Recent work of Hanneke, Larsen, and Zhivotovskiy (FOCS '24) addresses this shortcoming by including $\\\\tau$ itself as a parameter in the agnostic error term. In this more fine-grained model, they demonstrate tightness of the error lower bound $\\\\tau + \\\\Omega \\\\left(\\\\sqrt{\\\\frac{\\\\tau (d + \\\\log(1 / \\\\delta))}{m}} + \\\\frac{d + \\\\log(1 / \\\\delta)}{m} \\\\right)$ in a regime where $\\\\tau > d/m$, and leave open the question of whether there may be a higher lower bound when $\\\\tau \\\\approx d/m$, with $d$ denoting $\\\\mathrm{VC}(\\\\mathcal{H})$. In this work, we resolve this question by exhibiting a learner which achieves error $c \\\\cdot \\\\tau + O \\\\left(\\\\sqrt{\\\\frac{\\\\tau (d + \\\\log(1 / \\\\delta))}{m}} + \\\\frac{d + \\\\log(1 / \\\\delta)}{m} \\\\right)$ for a constant $c \\\\leq 2.1$, matching the lower bound and demonstrating optimality when $\\\\tau =O( d/m)$. Further, our learner is computationally efficient and is based upon careful aggregations of ERM classifiers, making progress on two other questions of Hanneke, Larsen, and Zhivotovskiy (FOCS '24). We leave open the interesting question of whether our approach can be refined to lower the constant from 2.1 to 1, which would completely settle the complexity of agnostic learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uUIgxjWkCI": {
    "title": "Wavelet Canonical Coherence for Nonstationary Signals",
    "volume": "spotlight",
    "abstract": "Understanding the evolving dependence between two sets of multivariate signals is fundamental in neuroscience and other domains where sub-networks in a system interact dynamically over time. Despite the growing interest in multivariate time series analysis, existing methods for between-clusters dependence typically rely on the assumption of stationarity and lack the temporal resolution to capture transient, frequency-specific interactions. To overcome this limitation, we propose scale-specific wavelet canonical coherence (WaveCanCoh), a novel framework that extends canonical coherence analysis to the nonstationary setting by leveraging the multivariate locally stationary wavelet model. The proposed WaveCanCoh enables the estimation of time-varying canonical coherence between clusters, providing interpretable insight into scale-specific time-varying interactions between clusters. Through extensive simulation studies, we demonstrate that WaveCanCoh accurately recovers true coherence structures under both locally stationary and general nonstationary conditions. Application to local field potential (LFP) activity data recorded from the hippocampus reveals distinct dynamic coherence patterns between correct and incorrect memory-guided decisions, illustrating capacity of the method to detect behaviorally relevant neural coordination. These results highlight WaveCanCoh as a flexible and principled tool for modeling complex cross-group dependencies in nonstationary multivariate systems. Code for implementing WaveCanCoh is available at https://github.com/mhaibo/WaveCanCoh.git",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VrCdsZBbIg": {
    "title": "Language Modeling by Language Models",
    "volume": "spotlight",
    "abstract": "*Can we leverage LLMs to model the process of discovering novel language model (LM) architectures?* Inspired by real research, we propose a multi-agent LLM approach that simulates the conventional stages of research, from ideation and literature search (proposal stage) to design implementation (code generation), generative pre-training, and downstream evaluation (verification). Using ideas from scaling laws, our system *Genesys* employs a *Ladder of Scales* approach; new designs are proposed, adversarially reviewed, implemented, and selectively verified at increasingly larger model scales (14M$\\sim$350M parameters) with a narrowing budget (the number of models we can train at each scale). To help make discovery efficient and factorizable, Genesys uses a novel genetic programming backbone, which we show has empirical advantages over commonly used direct prompt generation workflows (e.g., $\\sim$86\\% percentage point improvement in successful design generation, a key bottleneck). We report experiments involving 1,162 newly discovered designs (1,062 fully verified) and find the best designs to be competitive with known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common benchmarks). We couple these results with comprehensive system-level ablations and formal results, which give broader insights into the design of effective autonomous discovery systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eOZFqyE9Ok": {
    "title": "Plasticity as the Mirror of Empowerment",
    "volume": "spotlight",
    "abstract": "Agents are minimally entities that are influenced by their past observations and act to influence future observations. This latter capacity is captured by empowerment, which has served as a vital framing concept across artificial intelligence and cognitive science. This former capacity, however, is equally foundational: In what ways, and to what extent, can an agent be influenced by what it observes? In this paper, we ground this concept in a universal agent-centric measure that we refer to as plasticity, and reveal a fundamental connection to empowerment. Following a set of desiderata on a suitable definition, we define plasticity using a new information-theoretic quantity we call the generalized directed information. We show that this new quantity strictly generalizes the directed information introduced by Massey (1990) while preserving all of its desirable properties. Under this definition, we find that plasticity is well thought of as the mirror of empowerment: The two concepts are defined using the same measure, with only the direction of influence reversed. Our main result establishes a tension between the plasticity and empowerment of an agent, suggesting that agent design needs to be mindful of both characteristics. We explore the implications of these findings, and suggest that plasticity, empowerment, and their relationship are essential to understanding agency",
    "checked": true,
    "id": "5f1fd964402f24c77c32c70e67d16179d60beeea",
    "semantic_title": "plasticity as the mirror of empowerment",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=xNJenVNmzL": {
    "title": "PiKE: Adaptive Data Mixing for Large-Scale Multi-Task Learning Under Low Gradient Conflicts",
    "volume": "spotlight",
    "abstract": "Modern foundation models are trained on diverse datasets to enhance generalization across tasks and domains. A central challenge in this process is determining how to effectively mix and sample data from multiple sources. This naturally leads to a multi-task learning (MTL) perspective. While prior work in MTL has emphasized mitigating gradient conflicts, we observe that large-scale pretraining scenarios—such as multilingual or multi-domain training—often exhibit little to no gradient conflict. Motivated by this observation, we propose $\\textbf{PiKE}$ ($\\textbf{P}$ositive gradient $\\textbf{i}$nteraction-based $\\textbf{K}$-task weights $\\textbf{E}$stimator), an adaptive data mixing algorithm that dynamically adjusts sampling weights during training. PiKE exploits non-conflicting gradient interactions to minimize a near-tight upper bound on the average loss decrease at each step, while incurring negligible computational overhead. We provide theoretical convergence guarantees and show that PiKE outperforms static and non-adaptive mixing baselines. Furthermore, we extend PiKE to promote balanced learning across tasks. Extensive experiments on large-scale language model pretraining confirm that PiKE achieves faster convergence and improved downstream performance compared to existing approaches",
    "checked": true,
    "id": "ed3633efaa3619b60fa75665fb7edad4be0be2d4",
    "semantic_title": "pike: adaptive data mixing for large-scale multi-task learning under low gradient conflicts",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=W2874Arl4g": {
    "title": "Language Models can Self-Improve at State-Value Estimation for Better Search",
    "volume": "spotlight",
    "abstract": "Collecting ground-truth rewards or human demonstrations for multi-step reasoning tasks is often prohibitively expensive, especially in interactive domains such as web tasks. We introduce Self-Taught Lookahead (STL), a reward-free framework that improves language model–based value functions by reasoning explicitly about state transitions. STL can be viewed as a chain-of-thought analogue of the value iteration algorithm: instead of regressing directly on numeric values, a value LLM is trained to simulate a step of lookahead in natural language—predicting the next action, resulting state, and rationale for its value. This process refines value estimates without any labeled data. The self-supervised procedure yields more accurate state-value predictions, which in turn enable lightweight search algorithms to expand fewer states while maintaining strong performance. Empirically, STL-trained value models built on moderately sized (8B-parameter) open-weight LLMs boost web agent success rates by over 39%, achieving performance comparable to proprietary models. STL also generalizes to multi-hop question answering and math puzzles. Overall, STL enables small open-source models to guide efficient search, reducing inference costs by integrating explicit reasoning with value learning",
    "checked": true,
    "id": "550300f63860fc347fb97401cf55015fd369056b",
    "semantic_title": "language models can self-improve at state-value estimation for better search",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=oysfr9yqUI": {
    "title": "Scalable Cross-View Sample Alignment for Multi-View Clustering with View Structure Similarity",
    "volume": "spotlight",
    "abstract": "Most existing multi-view clustering methods aim to generate a consensus partition across all views, based on the assumption that all views share the same sample arrangement. However, in real-world scenarios, the collected data across different views is often unsynchronized, making it difficult to ensure consistent sample correspondence between views. To address this issue, we propose a scalable sample-alignment-based multi-view clustering method, referred to as SSA-MVC. Specifically, we first employ a cluster-label matching (CLM) algorithm to select the view whose clustering labels best match those of the others as the benchmark view. Then, for each of the remaining views, we construct representations of non-aligned samples by computing their similarities with aligned samples. Based on these representations, we build a similarity graph between the non-aligned samples of each view and those in the benchmark view, which serves as the alignment criterion. This alignment criterion is then integrated into a late-fusion framework to enable clustering without requiring aligned samples. Notably, the learned sample alignment matrix can be used to enhance existing multi-view clustering methods in scenarios where sample correspondence is unavailable. The effectiveness of the proposed SSA-MVC algorithm is validated through extensive experiments conducted on eight real-world multi-view datasets",
    "checked": false,
    "id": "58c045ea3a766f5759ca37e855f7d955c0c3bda9",
    "semantic_title": "scalable incomplete multi-view clustering with structure alignment",
    "citation_count": 30,
    "authors": []
  },
  "https://openreview.net/forum?id=SX6nL00JvM": {
    "title": "Multidimensional Bayesian Utility Maximization: Tight Approximations to Welfare",
    "volume": "spotlight",
    "abstract": "We initiate the study of multidimensional Bayesian utility maximization, focusing on the unit-demand setting where values are i.i.d. across both items and buyers. The seminal result of Hartline and Roughgarden '08 studies simple, information-robust mechanisms that maximize utility for $n$ i.i.d. agents and $m$ identical items via an approximation to social welfare as an upper bound, and they prove this gap between optimal utility and social welfare is $\\Theta(1+\\log{n/m})$ in this setting. We extend these results to the multidimensional setting. To do so, we develop simple, prior-independent, approximately-optimal mechanisms, targeting the simplest benchmark of optimal welfare. We give a $(1-1/e)$-approximation when there are more items than buyers, and a $\\Theta(\\log{n/m})$-approximation when there are more buyers than items, and we prove that this bound is tight in both $n$ and $m$ by reducing the i.i.d. unit-demand setting to the identical items setting. Finally, we include an extensive discussion section on why Bayesian utility maximization is a promising research direction. In particular, we characterize complexities in this setting that defy our intuition from the welfare and revenue literature, and motivate why coming up with a better benchmark than welfare is a hard problem itself",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dt5vRmUjAv": {
    "title": "Decomposing stimulus-specific sensory neural information via diffusion models",
    "volume": "spotlight",
    "abstract": "A central question in sensory neuroscience is how much, but also what information neurons transmit about the world. While Shannon's information theory provides a principled framework to quantify the amount of information neurons encode about all stimuli, it does not reveal which stimuli contribute most, or what stimulus features are encoded. As a concrete example, it is known that neurons in the early visual cortex are 'sensitive' to stimuli in a small region of space (their receptive field). However, it is not clear how such simple intuitions carry to more complex scenarios, e.g. with large, noisy & non-linear population of neurons and high-dimensional stimuli. Several previous measures of neural sensitivity have been proposed. For example, the Fisher information quantifies the sensitivity of neural responses to infinitesimal stimulus perturbations. However, as the Fisher is not a valid decomposition of the mutual information it cannot say how different stimuli contribute to the total encoded information. On the other hand, previous works have proposed stimulus dependent decompositions of mutual information, which define a function $ I(x) $ such that $ I(R; X) = \\mathbb{E}[I(x)] $. However, this decomposition is inherently ill-posed: infinitely many functions $I(x)$ satisfy the constraint, with no principled way to select among them. Further, different decompositions behave in qualitatively different ways, making it hard to interpret what are they are telling us. Finally, most proposed decompositions are computationally intractable for the high-dimensional stimuli and non-linear encoding models relevant for neuroscience. To resolve these limitations, we propose a set of axioms that any stimulus specific and feature-specific information decomposition should satisfy in order to serve as a meaningful and interpretable measure of neural sensitivity. These axioms formalize intuitive desiderata: that the information assigned to each stimulus, and stimulus feature, should be non-negative, and additive with respect to repeated measurements. We also require the decomposition to respect a form of locality: changes in how a neuron responds to a stimulus $ x $ should not affect the information attributed to a distant stimulus $ x' $. Finally, the attribution must be insensitive to irrelevant features, which do not contribute to the total information. Together, these constraints ensure that the decomposition is both interpretable and theoretically grounded. We show that existing decompositions violate one or more of these axioms, limiting their interpretability and use as information theoretic measures of neural sensitivity. We then introduce a novel decomposition that satisfies all of our axioms. It generalizes Fisher information by capturing neural sensitivity to both infinitesimal and finite stimulus perturbations. Moreover, it supports further decomposition across individual stimulus features (e.g., image pixels), enabling fine-grained analysis of neural representations. Beyond satisfying our theoretical axioms, our decomposition is computationally tractable for large neural populations and high-dimensional naturalistic stimuli, through the use of diffusion models. We demonstrate the power of our method by quantifying the information encoded by a model of visual neurons about individual images and pixels. Our approach uncovers aspects of the neural code that are not picked up by standard methods, such as the Fisher information, and opens the door to similar analyses in higher-order sensory areas, and artificial neural networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D6aCr4RRdt": {
    "title": "Any-stepsize Gradient Descent for Separable Data under Fenchel–Young Losses",
    "volume": "spotlight",
    "abstract": "The gradient descent (GD) has been one of the most common optimizer in machine learning. In particular, the loss landscape of a neural network is typically sharpened during the initial phase of training, making the training dynamics hover on the edge of stability. This is beyond our standard understanding of GD convergence in the stable regime where arbitrarily chosen stepsize is sufficiently smaller than the edge of stability. Recently, Wu et al. (COLT2024) have showed that GD converges with arbitrary stepsize under linearly separable logistic regression. Although their analysis hinges on the self-bounding property of the logistic loss, which seems to be a cornerstone to establish a modified descent lemma, our pilot study shows that other loss functions without the self-bounding property can make GD converge with arbitrary stepsize. To further understand what property of a loss function matters in GD, we aim to show arbitrary-stepsize GD convergence for a general loss function based on the framework of \\emph{Fenchel--Young losses}. We essentially leverage the classical perceptron argument to derive the convergence rate for achieving $\\epsilon$-optimal loss, which is possible for a majority of Fenchel--Young losses. Among typical loss functions, the Tsallis entropy achieves the GD convergence rate $T=\\Omega(\\epsilon^{-1/2})$, and the R{\\'e}nyi entropy achieves the far better rate $T=\\Omega(\\epsilon^{-1/3})$. We argue that these better rate is possible because of \\emph{separation margin} of loss functions, instead of the self-bounding property",
    "checked": false,
    "id": "0a6eb4c4606162bf9c29781dd4a5da9d9b422e70",
    "semantic_title": "any-stepsize gradient descent for separable data under fenchel-young losses",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=rgoSyTCTkn": {
    "title": "Provably Efficient RL under Episode-Wise Safety in Constrained MDPs with Linear Function Approximation",
    "volume": "spotlight",
    "abstract": "We study the reinforcement learning (RL) problem in a constrained Markov decision process (CMDP), where an agent explores the environment to maximize the expected cumulative reward while satisfying a single constraint on the expected total utility value in every episode. While this problem is well understood in the tabular setting, theoretical results for function approximation remain scarce. This paper closes the gap by proposing an RL algorithm for linear CMDPs that achieves $\\widetilde{\\mathcal{O}}(\\sqrt{K})$ regret with an episode-wise zero-violation guarantee. Furthermore, our method is computationally efficient, scaling polynomially with problem-dependent parameters while remaining independent of the state space size. Our results significantly improve upon recent linear CMDP algorithms, which either violate the constraint or incur exponential computational costs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iEgaS6wbLa": {
    "title": "To Distill or Decide? Understanding the Algorithmic Trade-off in Partially Observable RL",
    "volume": "spotlight",
    "abstract": "Partial observability is a notorious challenge in reinforcement learning (RL), due to the need to learn complex, history-dependent policies. Recent empirical successes have used *privileged expert distillation* -- which leverages availability of latent state information during training (e.g., from a simulator) to learn and imitate the optimal latent, Markovian policy -- to disentangle the task of ''learning to see'' from ''learning to act''. While expert distillation is more computationally efficient than RL without latent state information, it also has well-documented failure modes. In this paper -- through a simple but instructive theoretical model called the *perturbed Block MDP*, and controlled experiments on challenging simulated locomotion tasks -- we investigate the algorithmic trade-off between privileged expert distillation and standard RL without privileged information. Our main findings are: **(1)** The trade-off empirically hinges on the *stochasticity* of the latent dynamics, as theoretically predicted by contrasting *approximate decodability* with *belief contraction* in the perturbed Block MDP; and **(2)** The optimal latent policy is not always the best latent policy to distill. Our results suggest new guidelines for effectively exploiting privileged information, potentially advancing the efficiency of policy learning across many practical partially observable domains",
    "checked": false,
    "id": "8bb80978a81b582a872315229afa12fce2d0dd6a",
    "semantic_title": "to distill or decide? understanding the algorithmic trade-off in partially observable reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FrdX7K4Gli": {
    "title": "A learnability analysis on neuro-symbolic learning",
    "volume": "spotlight",
    "abstract": "This paper presents a comprehensive theoretical analysis of the learnability of neuro-symbolic (NeSy) tasks within hybrid systems. We characterize the learnability of NeSy tasks by their derived constraint satisfaction problems (DCSPs), demonstrating that a task is learnable if and only if its corresponding DCSP admits a unique solution. Under mild assumptions, we establish the sample complexity for learnable tasks and show that, for general tasks, the asymptotic expected concept error is controlled by the degree of disagreement among DCSP solutions. Our findings unify the characterization of learnability and the phenomenon of reasoning shortcuts, providing theoretical guarantees and actionable guidance for the principled design of NeSy systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hJJnwcvE2M": {
    "title": "Self-Perturbed Anomaly-Aware Graph Dynamics for Multivariate Time-Series Anomaly Detection",
    "volume": "spotlight",
    "abstract": "Detecting anomalies in multivariate time-series data is an essential task across various domains, yet there are unresolved challenges such as (1) severe class imbalance between normal and anomalous data due to rare anomaly availability in the real world; (2) limited adaptability of the static graph-based methods to dynamically changing inter-variable correlations; and (3) neglect of subtle anomalies due to overfitting to normal patterns in reconstruction-based methods. To tackle these issues, we propose Self-Perturbed Anomaly-Aware Graph Dynamics (SPAGD), a framework for time-series anomaly detection. SPAGD employs a self-perturbation module that generates self-perturbed time series from the reconstruction process of normal ones, which provide auxiliary signals to alleviate class imbalance during training. Concurrently, an anomaly-aware graph construction module is proposed to dynamically adjust the graph structure by leveraging the reconstruction residuals of self-perturbed time series, thereby emphasizing the inter-variable disruptions induced by anomalous candidates. A unified spatio-temporal anomaly detection module then integrates both spatial and temporal convolutions to train a classifier that distinguishes normal time series from the auxiliary self-perturbed samples. Extensive experiments across multiple benchmark datasets demonstrate the effectiveness of SPAGD compared to state-of-the-art baselines",
    "checked": false,
    "id": "6922c4e9c9144a5eaface5df21b5eeb190edbccb",
    "semantic_title": "self-supervised graph-based anomaly detection using semantic-aware contrastive learning in multivariate time series",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cECo8tetzF": {
    "title": "Restoring Pruned Large Language Models via Lost Component Compensation",
    "volume": "spotlight",
    "abstract": "Pruning is a widely used technique to reduce the size and inference cost of large language models (LLMs), but it often causes performance degradation. To mitigate this, existing restoration methods typically employ parameter-efficient fine-tuning (PEFT), such as LoRA, to recover the pruned model's performance. However, most PEFT methods are designed for dense models and overlook the distinct properties of pruned models, often resulting in suboptimal recovery. In this work, we propose a targeted restoration strategy for pruned models that restores performance while preserving their low cost and high efficiency. We observe that pruning-induced information loss is reflected in attention activations, and selectively reintroducing components of this information can significantly recover model performance. Based on this insight, we introduce RestoreLCC (Restoring Pruned LLMs via Lost Component Compensation), a plug-and-play method that contrastively probes critical attention heads via activation editing, extracts lost components from activation differences, and finally injects them back into the corresponding pruned heads for compensation and recovery. RestoreLCC is compatible with structured, semi-structured, and unstructured pruning schemes. Extensive experiments demonstrate that RestoreLCC consistently outperforms state-of-the-art baselines in both general and task-specific performance recovery, without compromising the sparsity or inference efficiency of pruned models",
    "checked": true,
    "id": "1b6c33edca944c871ba40b165803e16241d9e33c",
    "semantic_title": "restoring pruned large language models via lost component compensation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TyW1V1KukG": {
    "title": "Towards Building Model/Prompt-Transferable Attackers against Large Vision-Language Models",
    "volume": "spotlight",
    "abstract": "Although Large Vision-Language Models (LVLMs) exhibit impressive multimodal capabilities, their vulnerability to adversarial examples has raised serious security concerns. Existing LVLM attackers simply optimize adversarial images that easily overfit a certain model/prompt, making them ineffective once they are transferred to attack a different model/prompt. Motivated by this research gap, this paper aims to develop a more powerful attack that is transferable to black-box LVLM models of different structures and task-aware prompts of different semantics. Specifically, we introduce a new perspective of information theory to investigate LVLMs' transferable characteristics by exploring the relative dependence between outputs of the LVLM model and input adversarial samples. Our empirical observations suggest that enlarging/decreasing the mutual information between outputs and the disentangled adversarial/benign patterns of input images helps to generate more agnostic perturbations for misleading LVLMs' perception with better transferability. In particular, we formulate the complicated calculation of information gain as an estimation problem and incorporate such informative constraints into the adversarial learning process. Extensive experiments on various LVLM models/prompts demonstrate our significant transfer-attack performance",
    "checked": false,
    "id": "7f0afe1b627c627e05df73777722638e81879569",
    "semantic_title": "pandora's box: towards building universal attackers against real-world large vision-language models",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=LBo4e6Y7Zg": {
    "title": "GaussianFusion: Gaussian-Based Multi-Sensor Fusion for End-to-End Autonomous Driving",
    "volume": "spotlight",
    "abstract": "Multi-sensor fusion is crucial for improving the performance and robustness of end-to-end autonomous driving systems. Existing methods predominantly adopt either attention-based flatten fusion or bird's eye view fusion through geometric transformations. However, these approaches often suffer from limited interpretability or dense computational overhead. In this paper, we introduce GaussianFusion, a Gaussian-based multi-sensor fusion framework for end-to-end autonomous driving. Our method employs intuitive and compact Gaussian representations as intermediate carriers to aggregate information from diverse sensors. Specifically, we initialize a set of 2D Gaussians uniformly across the driving scene, where each Gaussian is parameterized by physical attributes and equipped with explicit and implicit features. These Gaussians are progressively refined by integrating multi-modal features. The explicit features capture rich semantic and spatial information about the traffic scene, while the implicit features provide complementary cues beneficial for trajectory planning. To fully exploit rich spatial and semantic information in Gaussians, we design a cascade planning head that iteratively refines trajectory predictions through interactions with Gaussians. Extensive experiments on the NAVSIM and Bench2Drive benchmarks demonstrate the effectiveness and robustness of the proposed GaussianFusion framework. The source code is included in the supplementary material and will be released publicly",
    "checked": true,
    "id": "442e948901ac2755a3f05f0be5853adf6f649471",
    "semantic_title": "gaussianfusion: gaussian-based multi-sensor fusion for end-to-end autonomous driving",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=GtImvTta8x": {
    "title": "SIU3R: Simultaneous Scene Understanding and 3D Reconstruction Beyond Feature Alignment",
    "volume": "spotlight",
    "abstract": "Simultaneous understanding and 3D reconstruction plays an important role in developing end-to-end embodied intelligent systems. To achieve this, recent approaches resort to 2D-to-3D feature alignment paradigm, which leads to limited 3D understanding capability and potential semantic information loss. In light of this, we propose SIU3R, the first alignment-free framework for generalizable simultaneous understanding and 3D reconstruction from unposed images. Specifically, SIU3R bridges reconstruction and understanding tasks via pixel-aligned 3D representation, and unifies multiple understanding tasks into a set of unified learnable queries, enabling native 3D understanding without the need of alignment with 2D models. To encourage collaboration between the two tasks with shared representation, we further conduct in-depth analyses of their mutual benefits, and propose two lightweight modules to facilitate their interaction. Extensive experiments demonstrate that our method achieves state-of-the-art performance not only on the individual tasks of 3D reconstruction and understanding, but also on the task of simultaneous understanding and 3D reconstruction, highlighting the advantages of our alignment-free framework and the effectiveness of the mutual benefit designs",
    "checked": true,
    "id": "9bd015d4c9e8cb886dddd27f003268f4dd430e6d",
    "semantic_title": "siu3r: simultaneous scene understanding and 3d reconstruction beyond feature alignment",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TZB6YT8Owr": {
    "title": "HYPERION: Fine-Grained Hypersphere Alignment for Robust Federated Graph Learning",
    "volume": "spotlight",
    "abstract": "Robust Federated Graph Learning (FGL) provides an effective decentralized framework for training Graph Neural Networks (GNNs) in noisy-label environments. However, the subtlety of noise during training presents formidable obstacles for developing robust FGL systems. Previous robust FL approaches neither adequately constrain edge-mediated error propagation nor account for intra-class topological differences. At the client level, we innovatively demonstrate that hyperspherical embedding can effectively capture graph structures in a fine-grained manner. Correspondingly, our method effectively addresses the aforementioned issues through fine-grained hypersphere alignment. Moreover, we uncover undetected noise arising from localized perspective constraints and propose the geometric-aware hyperspherical purification module at the server level. Combining both level strategies, we present our robust FGL framework,**HYPERION**, which operates all components within a unified hyperspherical space. **HYPERION** demonstrates remarkable robustness across multiple datasets, for instance, achieving a 29.7\\% $\\uparrow$ F1-macro score with 50\\%-pair noise on Cora. The code is available for anonymous access at \\url{https://anonymous.4open.science/r/Hyperion-NeurIPS/}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=REIo9ZLSYo": {
    "title": "Brain-Inspired fMRI-to-Text Decoding via Incremental and Wrap-Up Language Modeling",
    "volume": "spotlight",
    "abstract": "Decoding natural language text from non-invasive brain signals, such as functional magnetic resonance imaging (fMRI), remains a central challenge in brain-computer interface research. While recent advances in large language models (LLMs) have enabled open-vocabulary fMRI-to-text decoding, existing frameworks typically process the entire fMRI sequence in a single step, leading to performance degradation when handling long input sequences due to memory overload and semantic drift. To address this limitation, we propose a brain-inspired sequential fMRI-to-text decoding framework that mimics the human cognitive strategy of segmented and inductive language processing. Specifically, we divide long fMRI time series into consecutive segments aligned with optimal language comprehension length. Each segment is decoded incrementally, followed by a wrap-up mechanism that summarizes the semantic content and incorporates it as prior knowledge into subsequent decoding steps. This sequence-wise approach alleviates memory burden and ensures semantic continuity across segments. In addition, we introduce a text-guided masking strategy integrated with a masked autoencoder (MAE) framework for fMRI representation learning. This method leverages attention distributions over key semantic tokens to selectively mask the corresponding fMRI time points, and employs MAE to guide the model toward focusing on neural activity at semantically salient moments, thereby enhancing the capability of fMRI embeddings to represent textual information. Experimental results on the two datasets demonstrate that our method significantly outperforms state-of-the-art approaches, with performance gains increasing as decoding length grows",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BcKYVmh3yH": {
    "title": "Sampling-Efficient Test-Time Scaling: Self-Estimating the Best-of-N Sampling in Early Decoding",
    "volume": "spotlight",
    "abstract": "Test-time scaling enhances large language model performance by allocating additional compute resources during decoding. Best-of-$N$ (BoN) sampling serves as a common sampling-based scaling technique, broadening the search space in parallel to find better solutions from the model distribution. However, its cost–performance trade-off is still underexplored. Two main challenges limit the efficiency of BoN sampling: (1) Generating $N$ full samples consumes substantial GPU memory, reducing inference capacity under limited resources. (2) Reward models add extra memory and latency overhead, and training strong reward models introduces potential training data costs. Although some studies have explored efficiency improvements, none have addressed both challenges at once. To address this gap, we propose **Self-Truncation Best-of-$N$ (ST-BoN)**, a decoding method that avoids fully generating all $N$ samples and eliminates the need for reward models. It leverages early sampling consistency in the model's internal states to identify the most promising path and truncate suboptimal ones. In terms of cost, ST-BoN reduces dynamic GPU memory usage by over 80% and inference latency by 50%. In terms of cost–performance trade-off, ST-BoN achieves the same performance as Full-BoN while saving computational cost by 70%–80%, and under the same cost, it can improve accuracy by 3–4 points",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=obwRcksFZw": {
    "title": "PoE-World: Compositional World Modeling with Products of Programmatic Experts",
    "volume": "spotlight",
    "abstract": "Learning how the world works is central to building AI agents that can adapt to complex environments. Traditional world models based on deep-learning demand vast amounts of training data, and do not flexibly update their knowledge from sparse observations. Recent advances in program synthesis using Large Language Models (LLMs) give an alternate approach which learns world models represented as source code, supporting strong generalization from little data. To date, application of program-structured world models remains limited to natural language and grid-world domains. We introduce a novel program synthesis method for effectively modeling complex, non-gridworld domains by representing a world model as an exponentially-weighted product of programmatic experts (PoE-World) synthesized by LLMs. We show that this approach can learn complex, stochastic world models from just a few observations. We evaluate the learned world models by embedding them in a model-based planning agent, demonstrating efficient performance and generalization to unseen levels on Atari's Pong and Montezuma's Revenge",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hzBqQZK2iV": {
    "title": "Uni-LoRA: One Vector is All You Need",
    "volume": "spotlight",
    "abstract": "Low-Rank Adaptation (LoRA) has become the de facto parameter-efficient fine-tuning (PEFT) method for large language models (LLMs) by constraining weight updates to low-rank matrices. Recent works such as Tied-LoRA, VeRA, and VB-LoRA push efficiency further by introducing additional constraints to reduce the trainable parameter space. In this paper, we show that the parameter space reduction strategies employed by these LoRA variants can be formulated within a unified framework, Uni-LoRA, where the LoRA parameter space, flattened as a high-dimensional vector space R^D, can be reconstructed through a projection from a subspace R^d, with d << D. We demonstrate that the fundamental difference among various LoRA methods lies in the choice of the projection matrix, P ∈ R^{D×d}. Most existing LoRA variants rely on layer-wise or structure-specific projections that limit cross-layer parameter sharing, thereby compromising parameter efficiency. In light of this, we introduce an efficient and theoretically grounded projection matrix that is isometric, enabling global parameter sharing and reducing computation overhead. Furthermore, under the unified view of Uni-LoRA, this design requires only a single trainable vector to reconstruct LoRA parameters for the entire LLM -- making Uni-LoRA both a unified framework and a \"one-vector-only\" solution. Extensive experiments on GLUE, mathematical reasoning, and instruction tuning benchmarks demonstrate that Uni-LoRA achieves state-of-the-art parameter efficiency while outperforming or matching prior approaches in predictive performance",
    "checked": true,
    "id": "edce4985741573f91633feb73e877f9b9e9fbf4a",
    "semantic_title": "uni-lora: one vector is all you need",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=0jvnfH0WYV": {
    "title": "Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding",
    "volume": "spotlight",
    "abstract": "Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement and operations but require labor-intensive assignment. Large Language Models (LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of the task: pretraining corpora rarely contain private clinical or billing data. We introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL) for automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained with Group Relative Policy Optimization (GRPO) using rule-based rewards, DRG-Sapphire introduces a series of RL enhancements to address domain-specific challenges not seen in previous mathematical tasks. Our model achieves state-of-the-art accuracy on the MIMIC-IV benchmark and generates physician-validated reasoning for DRG assignments, significantly enhancing explainability. Our study further sheds light on broader challenges of applying RL to knowledge-intensive, OOD tasks. We observe that RL performance scales approximately linearly with the logarithm of the number of supervised fine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally constrained by the domain knowledge encoded in the base model. For OOD tasks like DRG coding, strong RL performance requires sufficient knowledge infusion prior to RL. Consequently, scaling SFT may be more effective and computationally efficient than scaling RL alone for such tasks",
    "checked": true,
    "id": "7d52057bce1d4e1edd05aa31df2844bfa03a043e",
    "semantic_title": "reinforcement learning for out-of-distribution reasoning in llms: an empirical study on diagnosis-related group coding",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Quvnn2o17a": {
    "title": "Stochastic Process Learning via Operator Flow Matching",
    "volume": "spotlight",
    "abstract": "Expanding on neural operators, we propose a novel framework for stochastic process learning across arbitrary domains. In particular, we develop operator flow matching (OFM) for learning stochastic process priors on function spaces. OFM provides the probability density of the values of any collection of points and enables mathematically tractable functional regression at new points with mean and density estimation. Our method outperforms state-of-the-art models in stochastic process learning, functional regression, and prior learning",
    "checked": true,
    "id": "339fc50abd39a0a0389277cd8f64252d400b325b",
    "semantic_title": "stochastic process learning via operator flow matching",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=W8xcKoJcrl": {
    "title": "Strategic Costs of Perceived Bias in Fair Selection",
    "volume": "spotlight",
    "abstract": "Meritocratic systems, from admissions to hiring, aim to impartially reward skill and effort. Yet persistent disparities across race, gender, and class challenge this ideal. Some attribute these gaps to structural inequality; others to individual choice. We develop a game-theoretic model in which candidates from different socioeconomic groups differ in their perceived post-selection value—shaped by social context and, increasingly, by AI-powered tools offering personalized career or salary guidance. Each candidate strategically chooses effort, balancing its cost against expected reward; effort translates into observable merit, and selection is based solely on merit. We characterize the unique Nash equilibrium in the large-agent limit and derive explicit formulas showing how valuation disparities and institutional selectivity jointly determine effort, representation, social welfare, and utility. We further propose a cost-sensitive optimization framework that quantifies how modifying selectivity or perceived value can reduce disparities without compromising institutional goals. Our analysis reveals a perception-driven bias: when perceptions of post-selection value differ across groups, these differences translate into rational differences in effort, propagating disparities backward through otherwise \"fair\" selection processes. While the model is static, it captures one stage of a broader feedback cycle linking perceptions, incentives, and outcomes—bridging rational-choice and structural explanations of inequality by showing how techno-social environments shape individual incentives in meritocratic systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6xCcjYa97j": {
    "title": "A Closer Look at Model Collapse: From a Generalization-to-Memorization Perspective",
    "volume": "spotlight",
    "abstract": "The widespread use of diffusion models has led to an abundance of AI-generated data, raising concerns about model collapse---a phenomenon in which recursive iterations of training on synthetic data lead to performance degradation. Prior work primarily characterizes this collapse via variance shrinkage or distribution shift, but these perspectives miss practical manifestations of model collapse. This paper identifies a transition from generalization to memorization during model collapse in diffusion models, where models increasingly replicate training data instead of generating novel content during iterative training on synthetic samples. This transition is directly driven by the declining entropy of the synthetic training data produced in each training cycle, which serves as a clear indicator of model degradation. Motivated by this insight, we propose an entropy-based data selection strategy to mitigate the transition from generalization to memorization and alleviate model collapse. Empirical results show that our approach significantly enhances visual quality and diversity in recursive generation, effectively preventing collapse",
    "checked": true,
    "id": "2c99d51713ba42ccaf1df911cb5d62ac397e9351",
    "semantic_title": "a closer look at model collapse: from a generalization-to-memorization perspective",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zNLlglSOwD": {
    "title": "AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders",
    "volume": "spotlight",
    "abstract": "Speculative Decoding (SD) accelerates large language model inference by employing a small draft model to generate predictions, which are then verified by a larger target model. The effectiveness of SD hinges on the alignment between these models, which is typically enhanced by Knowledge Distillation (KD). However, conventional KD methods aim to minimize the KL divergence between the draft and target models across all tokens, a goal that is misaligned with the true objective of SD, which is to maximize token acceptance rate. Therefore, draft models often struggle to fully assimilate the target model's knowledge due to capacity constraints, leading to suboptimal performance. To address this challenge, we propose AdaSPEC, a novel method that incorporates selective token filtering into the KD process. AdaSPEC utilizes a reference model to identify and filter out difficult-to-fit tokens, enabling the distillation of a draft model that better aligns with the target model on simpler tokens. This approach improves the overall token acceptance rate without compromising generation quality. We evaluate AdaSPEC across diverse tasks, including arithmetic reasoning, instruction-following, coding, and summarization, using model configurations of 31M/1.4B and 350M/2.7B parameters. Our results demonstrate that AdaSPEC consistently outperforms the state-of-the-art DistillSpec method, achieving higher acceptance rates across all tasks (up to 15\\%). The code is publicly available at \\url{https://github.com/yuezhouhu/adaspec}",
    "checked": true,
    "id": "9c265d9b32dbb9dec1d24d03f6a749a965481a69",
    "semantic_title": "adaspec: selective knowledge distillation for efficient speculative decoders",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A2pmNL7L1E": {
    "title": "Depth-Width Tradeoffs for Transformers on Graph Tasks",
    "volume": "spotlight",
    "abstract": "Transformers have revolutionized the field of machine learning. In particular, they can be used to solve complex algorithmic problems, including graph-based tasks. In such algorithmic tasks a key question is what is the minimal size of a transformer that can implement the task. Recent work has begun to explore this problem for graph-based tasks, showing that for sub-linear embedding dimension (i.e., model width) logarithmic depth suffices. However, an open question, which we address here, is what happens if width is allowed to grow linearly, while depth is kept fixed. Here we analyze this setting, and provide the surprising result that with linear width, constant depth suffices for solving a host of graph-based problems. This suggests that a moderate increase in width can allow much shallower models, which are advantageous in terms of inference and train time. For other problems, we show that quadratic width is required. Our results demonstrate the complex and intriguing landscape of transformer implementations of graph-based algorithms. We empirically investigate these trade-offs between the relative powers of depth and width and find tasks where wider models have the same accuracy as deep models, while having much faster train and inference time due to parallelizable hardware",
    "checked": false,
    "id": "e194cc2195d967421f2906ecc5942bf4ae07fa38",
    "semantic_title": "depth-width tradeoffs in algorithmic reasoning of graph tasks with transformers",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=0SYkQ50imt": {
    "title": "Refinement Methods for Distributed Distribution Estimation under ℓ p -Losses",
    "volume": "spotlight",
    "abstract": "Consider the communication-constrained estimation of discrete distributions under $\\ell^p$ losses, where each distributed terminal holds multiple independent samples and uses limited number of bits to describe the samples. We obtain the minimax optimal rates of the problem for most parameter regimes. As a result, an elbow effect of the optimal rates at $p=2$ is clearly identified. In order to achieve the optimal rates for different parameter regimes, we introduce refinement methods and develop additional customized techniques in the estimation protocols. The general idea of the refinement methods is to first generate rough estimate by partial information and then establish refined estimate in subsequent steps guided by the rough estimate. Then customized techniques such as successive refinement, sample compression, thresholding and random hashing are leveraged to achieve the optimal rates in different parameter regimes. The optimality of the estimation protocols is shown by deriving compatible minimax lower bounds",
    "checked": false,
    "id": "0383c68a38f4cecb46bf0e97c871afa131725b0e",
    "semantic_title": "adaptive refinement protocols for distributed distribution estimation under ℓp-losses",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=REHjkmWdQL": {
    "title": "Measuring and Guiding Monosemanticity",
    "volume": "spotlight",
    "abstract": "There is growing interest in leveraging mechanistic interpretability and controllability to better understand and influence the internal dynamics of large language models (LLMs). However, current methods face fundamental challenges in reliably localizing and manipulating feature representations. Sparse Autoencoders (SAEs) have recently emerged as a promising direction for feature extraction at scale, yet they, too, are limited by incomplete feature isolation and unreliable monosemanticity. To systematically quantify these limitations, we introduce Feature Monosemanticity Score (FMS), a novel metric to quantify feature monosemanticity in latent representation. Building on these insights, we propose Guided Sparse Autoencoders (G-SAE), a method that conditions latent representations on labeled concepts during training. We demonstrate that reliable localization and disentanglement of target concepts within the latent space improve interpretability, detection of behavior, and control. Specifically, our evaluations on toxicity detection, writing style identification, and privacy attribute recognition show that G-SAE not only enhances monosemanticity but also enables more effective and fine-grained steering with less quality degradation. Our findings provide actionable guidelines for measuring and advancing mechanistic interpretability and control of LLMs",
    "checked": true,
    "id": "01e88bb866e56581249942b376765a7432247ce2",
    "semantic_title": "measuring and guiding monosemanticity",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=KqhMpsWiz2": {
    "title": "Amortized Variational Transdimensional Inference",
    "volume": "spotlight",
    "abstract": "The expressiveness of flow-based models combined with stochastic variational inference (SVI) has expanded the application of optimization-based Bayesian inference to highly complex problems. However, despite the importance of multi-model Bayesian inference, defined over a transdimensional joint model and parameter space, flow-based SVI has been limited to problems defined over a fixed-dimensional parameter space. We introduce CoSMIC normalizing flows (COntextually-Specified Masking for Identity-mapped Components), an extension to neural autoregressive conditional normalizing flow architectures that enables use of a single amortized variational density for inference over a transdimensional (multi-model) conditional target distribution. We propose a combined stochastic variational transdimensional inference (VTI) approach to training CoSMIC flows using ideas from Bayesian optimization and Monte Carlo gradient estimation. Numerical experiments show the performance of VTI on challenging problems that scale to high-cardinality model spaces",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u1j6RqH8nM": {
    "title": "Scaling Laws For Scalable Oversight",
    "volume": "spotlight",
    "abstract": "Scalable oversight, the process by which weaker AI systems supervise stronger ones, has been proposed as a key strategy to control future superintelligent systems. However, it is still unclear how scalable oversight itself scales. To address this gap, we propose a framework that quantifies the probability of successful oversight as a function of the capabilities of the overseer and the system being overseen. Specifically, our framework models oversight as a game between capability-mismatched players; the players have oversight-specific Elo scores that are a piecewise-linear function of their general intelligence, with two plateaus corresponding to task incompetence and task saturation. We validate our framework with a modified version of the game Nim and then apply it to four oversight games: Mafia, Debate, Backdoor Code and Wargames. For each game, we find scaling laws that approximate how domain performance depends on general AI system capability. We then build on our findings in a theoretical study of Nested Scalable Oversight (NSO), a process in which trusted models oversee untrusted stronger models, which then become the trusted models in the next step. We identify conditions under which NSO succeeds and derive numerically (and in some cases analytically) the optimal number of oversight levels to maximize the probability of oversight success. We also apply our theory to our four oversight games, where we find that NSO success rates at a general Elo gap of 400 are 13.5\\% for Mafia, 51.7\\% for Debate, 10.0\\% for Backdoor Code, and 9.4\\% for Wargames; these rates decline further when overseeing stronger systems",
    "checked": true,
    "id": "b06a38a1d5166fb5931aa7f5c0b7e2198689aad1",
    "semantic_title": "scaling laws for scalable oversight",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=raZEmZ48h4": {
    "title": "A Near-Optimal Algorithm for Decentralized Convex-Concave Finite-Sum Minimax Optimization",
    "volume": "spotlight",
    "abstract": "In this paper, we study the distributed convex-concave finite-sum minimax optimization over the network, and a decentralized variance-reduced optimistic gradient method with stochastic mini-batch sizes (DIVERSE) is proposed. For the strongly-convex-strongly-concave objective, it is shown that DIVERSE can achieve a linear convergence rate that depends on the global smoothness parameters, yielding sharper computation and communication complexity bounds than existing results. Furthermore, we also establish the lower complexity bounds, which show that our upper bounds are optimal up to a logarithmic factor in terms of the local incremental first-order oracle calls, the computation rounds, and the communication rounds. Numerical experiments demonstrate that our algorithm outperforms existing methods in practice",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zapn9l2LMY": {
    "title": "Learning with Calibration: Exploring Test-Time Computing of Spatio-Temporal Forecasting",
    "volume": "spotlight",
    "abstract": "Spatio-temporal forecasting is crucial in many domains, such as transportation, meteorology, and energy. However, real-world scenarios frequently present challenges such as signal anomalies, noise, and distributional shifts. Existing solutions primarily enhance robustness by modifying network architectures or training procedures. Nevertheless, these approaches are computationally intensive and resource-demanding, especially for large-scale applications. In this paper, we explore a _novel **t**est-**t**ime **c**omputing paradigm, namely learning with calibration, **ST-TTC**_, for **s**patio-**t**emporal forecasting. Through learning with calibration, we aim to capture periodic structural biases arising from non-stationarity during the testing phase and perform real-time bias correction on predictions to improve accuracy. Specifically, we first introduce a spectral-domain calibrator with phase-amplitude modulation to mitigate periodic shift and then propose a flash updating mechanism with a streaming memory queue for efficient test-time computation. _**ST-TTC**_ effectively bypasses complex training-stage techniques, offering an efficient and generalizable paradigm. Extensive experiments on real-world datasets demonstrate the effectiveness, universality, flexibility and efficiency of our proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T62TYoF8R3": {
    "title": "FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion",
    "volume": "spotlight",
    "abstract": "Diffusion generative models have become the standard for producing high-quality, coherent video content, yet their slow inference speeds and high computational demands hinder practical deployment. Although both quantization and sparsity can independently accelerate inference while maintaining generation quality, naively combining these techniques in existing training-free approaches leads to significant performance degradation, as they fail to achieve proper joint optimization. We introduce FPSAttention, a novel training-aware co-design of FP8 quantization and Sparsity for video generation, with a focus on the 3D bi-directional attention mechanism. Our approach features three key innovations: 1) A unified 3D tile-wise granularity that simultaneously supports both quantization and sparsity. 2) A denoising step-aware strategy that adapts to the noise schedule, addressing the strong correlation between quantization/sparsity errors and denoising steps. 3) A native, hardware-friendly kernel that leverages FlashAttention and is implemented with optimized Hopper architecture features, enabling highly efficient execution. Trained on Wan2.1's 1.3B and 14B models and evaluated on the vBench benchmark, FPSAttention achieves a 7.09$\\times$ kernel speedup for attention operations and a 4.96$\\times$ end-to-end speedup for video generation compared to the BF16 baseline at 720p resolution—without sacrificing generation quality",
    "checked": true,
    "id": "6dd7c07555fdd1790ac76d0c5d19f6698b1af5bb",
    "semantic_title": "fpsattention: training-aware fp8 and sparsity co-design for fast video diffusion",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=paRLw86ONU": {
    "title": "Disentangled Concepts Speak Louder Than Words: Explainable Video Action Recognition",
    "volume": "spotlight",
    "abstract": "Effective explanations of video action recognition models should disentangle how movements unfold over time from the surrounding spatial context. However, existing methods—based on saliency—produce entangled explanations, making it unclear whether predictions rely on motion or spatial context. Language-based approaches offer structure but often fail to explain motions due to their tacit nature—intuitively understood but difficult to verbalize. To address these challenges, we propose Disentangled Action aNd Context concept-based Explainable (DANCE) video action recognition, a framework that predicts actions through disentangled concept types: motion dynamics, objects, and scenes. We define motion dynamics concepts as human pose sequences. We employ a large language model to automatically extract object and scene concepts. Built on an ante-hoc concept bottleneck design, DANCE enforces prediction through these concepts. Experiments on four datasets—KTH, Penn Action, HAA500, and UCF101—demonstrate that DANCE significantly improves explanation clarity with competitive performance. Through a user study, we validate the superior interpretability of DANCE. Experimental results also show that DANCE is beneficial for model debugging, editing, and failure analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HiBoJLCyEo": {
    "title": "Physics-Driven Spatiotemporal Modeling for AI-Generated Video Detection",
    "volume": "spotlight",
    "abstract": "AI-generated videos have achieved near-perfect visual realism (e.g., Sora), urgently necessitating reliable detection mechanisms. However, detecting such videos faces significant challenges in modeling high-dimensional spatiotemporal dynamics and identifying subtle anomalies that violate physical laws. In this paper, we propose a physics-driven AI-generated video detection paradigm based on probability flow conservation principles. Specifically, we propose a statistic called Normalized Spatiotemporal Gradient (NSG), which quantifies the ratio of spatial probability gradients to temporal density changes, explicitly capturing deviations from natural video dynamics. Leveraging pre-trained diffusion models, we develop an NSG estimator through spatial gradients approximation and motion-aware temporal modeling without complex motion decomposition while preserving physical constraints. Building on this, we propose an NSG-based video detection method (NSG-VD) that computes the Maximum Mean Discrepancy (MMD) between NSG features of the test and real videos as a detection metric. Last, we derive an upper bound of NSG feature distances between real and generated videos, proving that generated videos exhibit amplified discrepancies due to distributional shifts. Extensive experiments confirm that NSG-VD outperforms state-of-the-art baselines by 16.00\\% in Recall and 10.75\\% in F1-Score, validating the superior performance of NSG-VD. The source code is available at \\url{https://github.com/ZSHsh98/NSG-VD}",
    "checked": true,
    "id": "c4270bbd75716fa4b1dd643ea7f1a4fb2167a384",
    "semantic_title": "physics-driven spatiotemporal modeling for ai-generated video detection",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vATe64ktAo": {
    "title": "Robust Graph Condensation via Classification Complexity Mitigation",
    "volume": "spotlight",
    "abstract": "Graph condensation (GC) has gained significant attention for its ability to synthesize smaller yet informative graphs. However, existing studies often overlook the robustness of GC in scenarios where the original graph is corrupted. In such cases, we observe that the performance of GC deteriorates significantly, while existing robust graph learning technologies offer only limited effectiveness. Through both empirical investigation and theoretical analysis, we reveal that GC is inherently an intrinsic-dimension-reducing process, synthesizing a condensed graph with lower classification complexity. Although this property is critical for effective GC performance, it remains highly vulnerable to adversarial perturbations. To tackle this vulnerability and improve GC robustness, we adopt the geometry perspective of graph data manifold and propose a novel **M**anifold-constrained **R**obust **G**raph **C**ondensation framework named **MRGC**. Specifically, we introduce three graph data manifold learning modules that guide the condensed graph to lie within a smooth, low-dimensional manifold with minimal class ambiguity, thereby preserving the classification complexity reduction capability of GC and ensuring robust performance under universal adversarial attacks. Extensive experiments demonstrate the robustness of MRGC across diverse attack scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RblaNJGx8C": {
    "title": "CausalPFN: Amortized Causal Effect Estimation via In-Context Learning",
    "volume": "spotlight",
    "abstract": "Causal effect estimation from observational data is fundamental across various applications. However, selecting an appropriate estimator from dozens of specialized methods demands substantial manual effort and domain expertise. We present CausalPFN, a single transformer that *amortizes* this workflow: trained once on a large library of simulated data-generating processes that satisfy ignorability, it infers causal effects for new observational datasets out of the box. CausalPFN combines ideas from Bayesian causal inference with the large-scale training protocol of prior-fitted networks (PFNs), learning to map raw observations directly to causal effects without any task-specific adjustment. Our approach achieves superior average performance on heterogeneous and average treatment effect estimation benchmarks (IHDP, Lalonde, ACIC). Moreover, it shows competitive performance for real-world policy making on uplift modeling tasks. CausalPFN provides calibrated uncertainty estimates to support reliable decision-making based on Bayesian principles. This ready-to-use model requires no further training or tuning and takes a step toward automated causal inference (https://github.com/vdblm/CausalPFN/)",
    "checked": true,
    "id": "55586284afcecbd4e86f2a96b957be315a819b00",
    "semantic_title": "causalpfn: amortized causal effect estimation via in-context learning",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=iFIjNXb0Y5": {
    "title": "High-order Equivariant Flow Matching for Density Functional Theory Hamiltonian Prediction",
    "volume": "spotlight",
    "abstract": "Density functional theory (DFT) is a fundamental method for simulating quantum chemical properties, but it remains expensive due to the iterative self-consistent field (SCF) process required to solve the Kohn–Sham equations. Recently, deep learning methods are gaining attention as a way to bypass this step by directly predicting the Hamiltonian. However, they rely on deterministic regression and do not consider the highly structured nature of Hamiltonians. In this work, we propose QHFlow, a high-order equivariant flow matching framework that generates Hamiltonian matrices conditioned on molecular geometry. Flow matching models continuous-time trajectories between simple priors and complex targets, learning the structured distributions over Hamiltonians instead of direct regression. To further incorporate symmetry, we use a neural architecture that predicts SE(3)-equivariant vector fields, improving accuracy and generalization across diverse geometries. To further enhance physical fidelity, we additionally introduce a fine-tuning scheme to align predicted orbital energies with the target. QHFlow achieves state-of-the-art performance, reducing Hamiltonian error by 71% on MD17 and 53% on QH9. Moreover, we further show that QHFlow accelerates the DFT process without trading off the solution quality when initializing SCF iterations with the predicted Hamiltonian, significantly reducing the number of iterations and runtime",
    "checked": true,
    "id": "25031f87cb46bff915c5d3e4df93a77e625b7b55",
    "semantic_title": "high-order equivariant flow matching for density functional theory hamiltonian prediction",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=X2r9D46kvI": {
    "title": "🎧MOSPA: Human Motion Generation Driven by Spatial Audio",
    "volume": "spotlight",
    "abstract": "Enabling virtual humans to dynamically and realistically respond to diverse auditory stimuli remains a key challenge in character animation, demanding the integration of perceptual modeling and motion synthesis. Despite its significance, this task remains largely unexplored. Most previous works have primarily focused on mapping modalities like speech, audio, and music to generate human motion. As of yet, these models typically overlook the impact of spatial features encoded in spatial audio signals on human motion. To bridge this gap and enable high-quality modeling of human movements in response to spatial audio, we introduce the first comprehensive \"Spatial Audio-Driven Human Motion\" (SAM) dataset, which contains diverse and high-quality spatial audio and motion data. For benchmarking, we develop a simple yet effective diffusion-based generative framework for human \"MOtion generation driven by SPatial Audio,\" termed MOSPA, which faithfully captures the relationship between body motion and spatial audio through an effective fusion mechanism. Once trained, MOSPA can generate diverse realistic human motions conditioned on varying spatial audio inputs. We perform a thorough investigation of the proposed dataset and conduct extensive experiments for benchmarking, where our method achieves state-of-the-art performance on this task. Our code and model are publicly available at https://github.com/xsy27/Mospa-Acoustic-driven-Motion-Generation.git",
    "checked": false,
    "id": "b1e2626b97545c0fb60ccafb16127dad0e9dfc22",
    "semantic_title": "mospa: human motion generation driven by spatial audio",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=neZSGqhxDa": {
    "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
    "volume": "spotlight",
    "abstract": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from rule-based outcome rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external human or distillation data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability. AZR uses a code executor to both validate self-proposed code reasoning tasks and verify answers, serving as an unified source of verifiable feedback to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vws7eXQXsa": {
    "title": "Graph–Smoothed Bayesian Black-Box Shift Estimator and Its Information Geometry",
    "volume": "spotlight",
    "abstract": "Label shift adaptation aims to recover target class priors when the labelled source distribution $P$ and the unlabelled target distribution $Q$ share $P(X \\mid Y) = Q(X \\mid Y)$ but $P(Y) \\neq Q(Y)$. Classical black‑box shift estimators invert an empirical confusion matrix of a frozen classifier, producing a brittle point estimate that ignores sampling noise and similarity among classes. We present Graph‑Smoothed Bayesian BBSE (GS‑B$^3$SE), a fully probabilistic alternative that places Laplacian–Gaussian priors on both target log‑priors and confusion‑matrix columns, tying them together on a label‑similarity graph. The resulting posterior is tractable with HMC or a fast block Newton–CG scheme. We prove identifiability, $N^{-1/2}$ contraction, variance bounds that shrink with the graph's algebraic connectivity, and robustness to Laplacian misspecification. We also reinterpret GS‑B$^3$SE through information geometry, showing that it generalizes existing shift estimators",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bA02DmQN5d": {
    "title": "Vision Transformers Don't Need Trained Registers",
    "volume": "spotlight",
    "abstract": "We investigate the mechanism underlying a previously identified phenomenon in Vision Transformers -- the emergence of high-norm tokens that lead to noisy attention maps (Darcet et al., 2024). We observe that in multiple models (e.g., CLIP, DINOv2), a sparse set of neurons is responsible for concentrating high-norm activations on outlier tokens, leading to irregular attention patterns and degrading downstream visual processing. While the existing solution for removing these outliers involves retraining models from scratch with additional learned $\\textit{register tokens}$, we use our findings to create a training-free approach to mitigate these artifacts. By shifting the high-norm activations from our discovered $\\textit{register neurons}$ into an additional untrained token, we can mimic the effect of register tokens on a model already trained without registers. We demonstrate that our method produces cleaner attention and feature maps, enhances performance over base models across multiple downstream visual tasks, and achieves results comparable to models explicitly trained with register tokens. We then extend test-time registers to off-the-shelf vision-language models, yielding cleaner attention-based, text-to-image attribution. Finally, we outline a simple mathematical model that reflects the observed behavior of register neurons and high norm tokens. Our results suggest that test-time registers effectively take on the role of register tokens at test-time, offering a training-free solution for any pre-trained model released without them",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kp9B9iQDIt": {
    "title": "Object-centric 3D Motion Field for Robot Learning from Human Videos",
    "volume": "spotlight",
    "abstract": "Learning robot control policies from human videos is a promising direction for scaling up robot learning. However, how to extract action knowledge (or action representations) from videos for policy learning remains a key challenge. Existing action representations such as video frames, pixelflow, and pointcloud flow have inherent limitations such as modeling complexity or loss of information. In this paper, we propose to use object-centric 3D motion field to represent actions for robot learning from human videos, and present a novel framework for extracting this representation from videos for zero-shot control. We introduce two novel components. First, a novel training pipeline for training a ``denoising'' 3D motion field estimator to extract fine object 3D motions from human videos with noisy depth robustly. Second, a dense object-centric 3D motion field prediction architecture that favors both cross-embodiment transfer and policy generalization to background. We evaluate the system in real world setups. Experiments show that our method reduces 3D motion estimation error by over 50% compared to the latest method, achieve 55% average success rate in diverse tasks where prior approaches fail ($\\lesssim 10$\\%), and can even acquire fine-grained manipulation skills like insertion",
    "checked": true,
    "id": "06ced182383f4538312cefd869255a8a17e8384d",
    "semantic_title": "object-centric 3d motion field for robot learning from human videos",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=WPU17d1l7R": {
    "title": "Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation",
    "volume": "spotlight",
    "abstract": "Diffusion Transformers (DiTs) are essential for video generation but suffer from significant latency due to the quadratic complexity of attention. By computing only critical tokens, sparse attention reduces computational costs and offers a promising acceleration approach. However, we identify that existing methods fail to approach optimal generation quality under the same computation budget for two reasons: (1) Inaccurate critical token identification: current methods cluster tokens based on position rather than semantics, leading to imprecise aggregated representations. (2) Excessive computation waste: critical tokens are scattered among non-critical ones, leading to wasted computation on GPUs, which are optimized for processing contiguous tokens. In this paper, we propose SVG2, a training-free framework that maximizes identification accuracy and minimizes computation waste, achieving a Pareto frontier trade-off between generation quality and efficiency. The core of SVG2 is semantic-aware permutation, which clusters and reorders tokens based on semantic similarity using k-means. This approach ensures both a precise cluster representation, improving identification accuracy, and a densified layout of critical tokens, enabling efficient computation without padding. Additionally, SVG2 integrates Top-p dynamic budget control and customized kernel implementations, achieving up to $2.30\\times$ and $1.89\\times$ speedup while maintaining a PSNR of up to $30$ and $26$ on HunyuanVideo and Wan 2.1, respectively. Our code is open-sourced at https://github.com/svg-project/Sparse-VideoGen",
    "checked": true,
    "id": "a1c6ac272065719a74050ba79b52020687b4debc",
    "semantic_title": "sparse videogen2: accelerate video generation with sparse attention via semantic-aware permutation",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=w97lDmoD0U": {
    "title": "Aggregation Hides Out-of-Distribution Generalization Failures from Spurious Correlations",
    "volume": "spotlight",
    "abstract": "Benchmarks for out-of-distribution (OOD) generalization often reveal a strong positive correlation between in-distribution (ID) and OOD accuracy across models, a phenomenon known as \"accuracy-on-the-line.\" This pattern is commonly interpreted as evidence that spurious correlations—relationships that improve ID but harm OOD performance—are rare in practice. We show that this positive correlation can be an artifact of aggregating heterogeneous OOD examples. Using a simple gradient-based method, OODSelect, we identify semantically coherent OOD subsets where accuracy-on-the-line breaks down. Across widely used distribution-shift benchmarks, OODSelect uncovers subsets—sometimes comprising more than half of the standard OOD set—where higher ID accuracy predicts lower OOD accuracy. These results suggest that aggregate metrics can mask critical failure modes in OOD robustness. We release code and the identified subsets to support further research",
    "checked": true,
    "id": "1deb1c3655f4f6c2e1860e148b0c3cd9ae70141a",
    "semantic_title": "aggregation hides out-of-distribution generalization failures from spurious correlations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CTsdZ3j6dR": {
    "title": "Mean-Field Sampling for Cooperative Multi-Agent Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": true,
    "id": "2716f9744913eca6153f1003457e082055d117a9",
    "semantic_title": "mean-field sampling for cooperative multi-agent reinforcement learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Lf0W2gmNBg": {
    "title": "EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6WnBITpnzD": {
    "title": "LaViDa: A Large Diffusion Model for Vision-Language Understanding",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": "5105e734fc2a95f6710e0bbb93a829b7e0bbf6bc",
    "semantic_title": "lavida: a large diffusion language model for multimodal understanding",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=0Az25lvdT2": {
    "title": "Theory-Driven Label-Specific Representation for Incomplete Multi-View Multi-Label Learning",
    "volume": "spotlight",
    "abstract": "",
    "checked": false,
    "id": "b93c8de6ed1433d33a85fbda561946da05b3b5b0",
    "semantic_title": "reliable representation learning for incomplete multi-view missing multi-label classification",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=CRyOyiVvvJ": {
    "title": "Scalable Fingerprinting of Large Language Models",
    "volume": "spotlight",
    "abstract": "Model fingerprinting has emerged as a powerful tool for model owners to identify their shared model given API access. In order to lower false discovery rate, fight fingerprint leakage, and defend against coalitions of model users attempting to bypass detection, we argue that scaling up the number of fingerprints one can embed into a model, i.e. *Scalability* of fingerprints, is critical. Hence, we pose scalability as a crucial requirement for fingerprinting schemes. We experiment with fingerprint design at a scale significantly larger than previously considered, and introduce a new method, dubbed Perinucleus sampling, to generate scalable, persistent, and harmless fingerprints. We demonstrate that this scheme can add 24,576 fingerprints to a Llama-3.1-8B model---two orders of magnitude more than existing schemes---without degrading the model's utility. Our inserted fingerprints persist even after supervised fine-tuning on standard post-training data. We further address security risks for fingerprinting, and theoretically and empirically show how a scalable fingerprinting scheme like ours can mitigate these risks",
    "checked": true,
    "id": "cba557f4fbb593c08c6dfcdfc6e59bc58a1be7db",
    "semantic_title": "scalable fingerprinting of large language models",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=po0eyoYFUa": {
    "title": "Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?",
    "volume": "spotlight",
    "abstract": "Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances large language models (LLMs) by providing structured and interpretable external knowledge. However, existing KG-based RAG methods struggle to retrieve accurate and diverse information from text-rich KGs for complex real-world queries. Process Reward Models (PRMs) offer a way to align the retrieval process of KG-based RAG with query-specific knowledge requirements, but they heavily rely on process-level supervision signals that are expensive and hard to obtain on KGs. To address this challenge, we propose GraphFlow, a framework that efficiently retrieves accurate and diverse knowledge required for real-world queries from text-rich KGs. GraphFlow employs a transition-based flow matching objective to jointly optimize a retrieval policy and a flow estimator. The flow estimator factorizes the reward of the retrieval outcome into the intermediate retrieval states. Such reward factorization guides the retrieval policy to retrieve candidates from KGs in proportion to their reward. This allows GraphFlow to explore high-quality regions of KGs that yield diverse and relevant results. We evaluate GraphFlow on the STaRK benchmark, which includes real-world queries from multiple domains over text-rich KGs. GraphFlow outperforms strong KG-RAG baselines, including GPT-4o, by 10\\% on average in hit rate and recall. It also shows strong generalization to unseen KGs, demonstrating its effectiveness and robustness",
    "checked": true,
    "id": "6d95d447814f7733da50d81d82b345aa40c6de2a",
    "semantic_title": "can knowledge-graph-based retrieval augmented generation really retrieve what you need?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yPnEvPq3kV": {
    "title": "Decomposing Interventional Causality into Synergistic, Redundant, and Unique Components",
    "volume": "spotlight",
    "abstract": "We introduce a novel framework for decomposing interventional causal effects into synergistic, redundant, and unique components, building on the intuition of Partial Information Decomposition (PID) and the principle of Möbius inversion. While recent work has explored a similar decomposition of an observational measure, we argue that a proper causal decomposition must be interventional in nature. We develop a mathematical approach that systematically quantifies how causal power is distributed among variables in a system, using a recently derived closed-form expression for the Möbius function of the redundancy lattice. The formalism is then illustrated by decomposing the causal power in logic gates, cellular automata, and chemical reaction networks. Our results reveal how the distribution of causal power can be context- and parameter-dependent. The decomposition provides new insights into complex systems by revealing how causal influences are shared and combined among multiple variables, with potential applications ranging from attribution of responsibility in legal or AI systems, to the analysis of biological networks or climate models",
    "checked": true,
    "id": "5a5b24841c42c6582bed4389832ad74e60df8388",
    "semantic_title": "decomposing interventional causality into synergistic, redundant, and unique components",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=UBsYf2lyNE": {
    "title": "Long-Tailed Recognition via Information-Preservable Two-Stage Learning",
    "volume": "spotlight",
    "abstract": "The imbalance (or long-tail) is the nature of many real-world data distributions, which often induces the undesirable bias of deep classification models toward frequent classes, resulting in poor performance for tail classes. In this paper, we propose a novel two-stage learning approach to mitigate such a majority-biased tendency while preserving valuable information within datasets. Specifically, the first stage proposes a new representation learning technique from the information theory perspective. This approach is theoretically equivalent to minimizing intra-class distance, yielding an effective and well-separated feature space. The second stage develops a novel sampling strategy that selects mathematically informative instances, able to rectify majority-biased decision boundaries without compromising a model's overall performance. As a result, our approach achieves the state-of-the-art performance across various long-tailed benchmark datasets, validated via extensive experiments. Our code is available at https://github.com/fudong03/BNS_IPDPP",
    "checked": true,
    "id": "c6a0ef1b7fc58ce1c0522187fdfd9ce0159259c2",
    "semantic_title": "long-tailed recognition via information-preservable two-stage learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zWHKKspghT": {
    "title": "Mozart: Modularized and Efficient MoE Training on 3.5D Wafer-Scale Chiplet Architectures",
    "volume": "spotlight",
    "abstract": "Mixture-of-Experts (MoE) architecture offers enhanced efficiency for Large Language Models (LLMs) with modularized computation, yet its inherent sparsity poses significant hardware deployment challenges, including memory locality issues, communication overhead, and inefficient computing resource utilization. Inspired by the modular organization of the human brain, we propose $\\texttt{Mozart}$, a novel algorithm-hardware co-design framework tailored for efficient training of MoE-based LLMs on 3.5D wafer-scale chiplet architectures. On the algorithm side, $\\texttt{Mozart}$ exploits the inherent modularity of chiplets and introduces: ($1$) an expert allocation strategy that enables efficient on-package all-to-all communication, and ($2$) a fine-grained scheduling mechanism that improves communication-computation overlap through streaming tokens and experts. On the architecture side, $\\texttt{Mozart}$ adaptively co-locates heterogeneous modules on specialized chiplets with a 2.5D NoP-Tree topology and hierarchical memory structure. Evaluation across three popular MoE models demonstrates significant efficiency gains, enabling more effective parallelization and resource utilization for large-scale modularized MoE-LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tQZK5frjVU": {
    "title": "Data Mixing Can Induce Phase Transitions in Knowledge Acquisition",
    "volume": "spotlight",
    "abstract": "Large Language Models (LLMs) are typically trained on data mixtures: most data come from web scrapes, while a small portion is curated from high-quality sources with dense domain-specific knowledge. In this paper, we show that when training LLMs on such data mixtures, knowledge acquisition from knowledge-dense datasets—unlike training exclusively on knowledge-dense data—does not always follow a smooth scaling law but can exhibit phase transitions with respect to the mixing ratio and model size. Through controlled experiments on a synthetic biography dataset mixed with web-scraped data, we demonstrate that: (1) as we increase the model size to a critical value, the model suddenly transitions from memorizing very few to most of the biographies; (2) below a critical mixing ratio, the model memorizes almost nothing even with extensive training, but beyond this threshold, it rapidly memorizes more biographies. We attribute these phase transitions to a capacity allocation phenomenon: a model with bounded capacity must act like a knapsack problem solver to minimize the overall test loss, and the optimal allocation across datasets can change discontinuously as the model size or mixing ratio varies. We formalize this intuition in an information-theoretic framework and reveal that these phase transitions are predictable, with the critical mixing ratio following a power-law relationship with the model size. Our findings highlight a concrete case where a good mixing recipe for large models may not be optimal for small models, and vice versa",
    "checked": true,
    "id": "ce637d7ea51aea6cad6fc3e368678f81340541bf",
    "semantic_title": "data mixing can induce phase transitions in knowledge acquisition",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Vj56Z9yNCr": {
    "title": "Shallow Diffuse: Robust and Invisible Watermarking through Low-Dim Subspaces in Diffusion Models",
    "volume": "spotlight",
    "abstract": "The widespread use of AI-generated content from diffusion models has raised significant concerns regarding misinformation and copyright infringement. Watermarking is a crucial technique for identifying these AI-generated images and preventing their misuse. In this paper, we introduce *Shallow Diffuse*, a new watermarking technique that embeds robust and invisible watermarks into diffusion model outputs. Unlike existing approaches that integrate watermarking throughout the entire diffusion sampling process, *Shallow Diffuse* decouples these steps by leveraging the presence of a low-dimensional subspace in the image generation process. This method ensures that a substantial portion of the watermark lies in the null space of this subspace, effectively separating it from the image generation process. Our theoretical and empirical analyses show that this decoupling strategy greatly enhances the consistency of data generation and the detectability of the watermark. Extensive experiments further validate that *Shallow Diffuse* outperforms existing watermarking methods in terms of consistency",
    "checked": false,
    "id": "eca4389780eec5257ed40df1011b2ff7d40e593a",
    "semantic_title": "shallow diffuse: robust and invisible watermarking through low-dimensional subspaces in diffusion models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=4P6Mployhf": {
    "title": "Offline Guarded Safe Reinforcement Learning for Medical Treatment Optimization Strategies",
    "volume": "spotlight",
    "abstract": "When applying offline reinforcement learning (RL) in healthcare scenarios, the out-of-distribution (OOD) issues pose significant risks, as inappropriate generalization beyond clinical expertise can result in potentially harmful recommendations. While existing methods like conservative Q-learning (CQL) attempt to address the OOD issue, their effectiveness is limited by only constraining action selection by suppressing uncertain actions. This action-only regularization imitates clinician actions that prioritize short-term rewards, but it fails to regulate downstream state trajectories, thereby limiting the discovery of improved long-term treatment strategies. To safely improve policy beyond clinician recommendations while ensuring that state-action trajectories remain in-distribution, we propose \\textit{Offline Guarded Safe Reinforcement Learning} ($\\mathsf{OGSRL}$), a theoretically grounded model-based offline RL framework. $\\mathsf{OGSRL}$ introduces a novel dual constraint mechanism for improving policy with reliability and safety. First, the OOD guardian is established to specify clinically validated regions for safe policy exploration. By constraining optimization within these regions, it enables the reliable exploration of treatment strategies that outperform clinician behavior by leveraging the full patient state history, without drifting into unsupported state-action trajectories. Second, we introduce a safety cost constraint that encodes medical knowledge about physiological safety boundaries, providing domain-specific safeguards even in areas where training data might contain potentially unsafe interventions. Notably, we provide theoretical guarantees on safety and near-optimality: policies that satisfy these constraints remain in safe and reliable regions and achieve performance close to the best possible policy supported by the data. When evaluated on the MIMIC-III sepsis treatment dataset, $\\mathsf{OGSRL}$ demonstrated significantly better OOD handling than baselines. $\\mathsf{OGSRL}$ achieved a 78\\% reduction in mortality estimates and a 51\\% increase in reward compared to clinician decisions",
    "checked": true,
    "id": "15002ad246a9b2562a6935a56568452ee62643b7",
    "semantic_title": "offline guarded safe reinforcement learning for medical treatment optimization strategies",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=6vcgsrK6pN": {
    "title": "From Counterfactuals to Trees: Competitive Analysis of Model Extraction Attacks",
    "volume": "spotlight",
    "abstract": "The advent of Machine Learning as a Service (MLaaS) has heightened the trade-off between model explainability and security. In particular, explainability techniques, such as counterfactual explanations, inadvertently increase the risk of model extraction attacks, enabling unauthorized replication of proprietary models. In this paper, we formalize and characterize the risks and inherent complexity of model reconstruction, focusing on the \"oracle'' queries required for faithfully inferring the underlying prediction function. We present the first formal analysis of model extraction attacks through the lens of competitive analysis, establishing a foundational framework to evaluate their efficiency. Focusing on models based on additive decision trees (e.g., decision trees, gradient boosting, and random forests), we introduce novel reconstruction algorithms that achieve provably perfect fidelity while demonstrating strong anytime performance. Our framework provides theoretical bounds on the query complexity for extracting tree-based model, offering new insights into the security vulnerabilities of their deployment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fZsd3KLMje": {
    "title": "Repo2Run: Automated Building Executable Environment for Code Repository at Scale",
    "volume": "spotlight",
    "abstract": "Scaling up executable code data is significant for improving language models' software engineering capability. The intricate nature of the process makes it labor-intensive, time-consuming and expert-knowledge-dependent to build a large number of executable code repositories, limiting the scalability of existing work based on running tests. The primary bottleneck lies in the automated building of test environments for different repositories, which is an essential yet underexplored task. To mitigate the gap, we introduce Repo2Run, the first LLM-based agent aiming at automating the building of executable test environments for any repositories at scale. Specifically, given a code repository, Repo2Run iteratively builds the Docker image, runs unit tests based on the feedback of the building, and synthesizes the Dockerfile until the entire pipeline is executed successfully. The resulting Dockerfile can then be used to create Docker container environments for running code and tests. We created a benchmark containing 420 Python repositories with unit tests for evaluation. The results illustrate that Repo2Run achieves an 86.0% success rate, outperforming SWE-agent by 77.0%. The resources of Repo2Run are available at https://github.com/bytedance/Repo2Run",
    "checked": true,
    "id": "0b8321b6b218e52ff2050d6f0c4ef39d7e75d7a2",
    "semantic_title": "repo2run: automated building executable environment for code repository at scale",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=pOJBw1YQgL": {
    "title": "Towards Dynamic 3D Reconstruction of Hand-Instrument Interaction in Ophthalmic Surgery",
    "volume": "spotlight",
    "abstract": "Accurate 3D reconstruction of hands and instruments is critical for vision-based analysis of ophthalmic microsurgery, yet progress has been hampered by the lack of realistic, large-scale datasets and reliable annotation tools. In this work, we introduce OphNet-3D, the first extensive RGB-D dynamic 3D reconstruction dataset for ophthalmic surgery, comprising 41 sequences from 40 surgeons and totaling 7.1 million frames, with fine-grained annotations of 12 surgical phases, 10 instrument categories, dense MANO hand meshes, and full 6-DoF instrument poses. To scalably produce high-fidelity labels, we design a multi-stage automatic annotation pipeline that integrates multi-view data observation, data-driven motion prior with cross-view geometric consistency and biomechanical constraints, along with a combination of collision-aware interaction constraints for instrument interactions. Building upon OphNet-3D, we establish two challenging benchmarks—bimanual hand pose estimation and hand–instrument interaction reconstruction—and propose two dedicated architectures: H-Net for dual-hand mesh recovery and OH-Net for joint reconstruction of two-hand–two-instrument interactions. These models leverage a novel spatial reasoning module with weak-perspective camera modeling and collision-aware center-based representation. Both architectures outperform existing methods by substantial margins, achieving improvements of over 2mm in Mean Per Joint Position Error (MPJPE) and up to 23\\% in ADD-S metrics for hand and instrument reconstruction, respectively",
    "checked": true,
    "id": "2dd1e9c77bfac163be258a32588555499fd9515c",
    "semantic_title": "towards dynamic 3d reconstruction of hand-instrument interaction in ophthalmic surgery",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c6O18DyBBx": {
    "title": "Unlocking Dataset Distillation with Diffusion Models",
    "volume": "spotlight",
    "abstract": "Dataset distillation seeks to condense datasets into smaller but highly representative synthetic samples. While diffusion models now lead all generative benchmarks, current distillation methods avoid them and rely instead on GANs or autoencoders, or, at best, sampling from a fixed diffusion prior. This trend arises because naive backpropagation through the long denoising chain leads to vanishing gradients, which prevents effective synthetic sample optimization. To address this limitation, we introduce Latent Dataset Distillation with Diffusion Models (LD3M), the first method to learn gradient-based distilled latents and class embeddings end-to-end through a pre-trained latent diffusion model. A linearly decaying skip connection, injected from the initial noisy state into every reverse step, preserves the gradient signal across dozens of timesteps without requiring diffusion weight fine-tuning. Across multiple ImageNet subsets at $128\\times128$ and $256\\times256$, LD3M improves downstream accuracy by up to 4.8 percentage points (1 IPC) and 4.2 points (10 IPC) over the prior state-of-the-art. The code for LD3M is provided at https://github.com/Brian-Moser/prune_and_distill",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YdggdEL41C": {
    "title": "Vision-centric Token Compression in Large Language Model",
    "volume": "spotlight",
    "abstract": "Real-world applications are stretching context windows to hundreds of thousand of tokens while Large Language Models (LLMs) swell from billions to trillions of parameters. This dual expansion send compute and memory costs skyrocketing, making $\\textit{token compression}$ indispensable. We introduce Vision Centric Token Compression ($\\textbf{Vist}$), a $\\textit{slow–fast}$ compression framework that mirrors human reading: the $\\textit{fast}$ path renders distant tokens into images, letting a $\\textbf{frozen, lightweight vision encoder}$ skim the low-salience context; the $\\textit{slow}$ path feeds the proximal window into the LLM for fine-grained reasoning. A Probability-Informed Visual Enhancement (PVE) objective masks high-frequency tokens during training, steering the Resampler to concentrate on semantically rich regions—just as skilled reader gloss over function words. On eleven in-context learning benchmarks, $\\textbf{Vist}$ achieves the same accuracy with 2.3$\\times$ fewer tokens, cutting FLOPs by 16\\% and memory by 50\\%. This method delivers remarkable results, outperforming the strongest text encoder-based compression method CEPE by $\\textbf{7.6}$\\% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI, and CLIN, setting a new standard for token efficiency in LLMs. The source code will be released",
    "checked": true,
    "id": "50b73ac7a0819180aba30d2059e341db5caa87fa",
    "semantic_title": "vision-centric token compression in large language model",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=8PUzLga3lU": {
    "title": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction",
    "volume": "spotlight",
    "abstract": "Recent Multimodal Large Language Models (MLLMs) have typically focused on integrating visual and textual modalities, with less emphasis placed on the role of speech in enhancing interaction. However, speech plays a crucial role in multimodal dialogue systems, and implementing high-performance in both vision and speech tasks remains a challenge due to the fundamental modality differences. In this paper, we propose a carefully designed multi-stage training methodology that progressively trains LLM to understand both visual and speech information, ultimately enabling fluent vision and speech interaction. Our approach not only preserves strong vision-language capacity, but also enables efficient speech-to-speech dialogue capabilities without separate ASR and TTS modules, significantly accelerating multimodal end-to-end response speed. By comparing against state-of-the-art counterparts across benchmarks for image, video, and speech, we demonstrate that our omni model is equipped with both strong visual and speech capabilities, making omni understanding and interaction",
    "checked": true,
    "id": "4f62ac743e2ed33e682a9c6f5f7875723d9a7025",
    "semantic_title": "vita-1.5: towards gpt-4o level real-time vision and speech interaction",
    "citation_count": 113,
    "authors": []
  },
  "https://openreview.net/forum?id=ph1V6n7BSv": {
    "title": "EDELINE: Enhancing Memory in Diffusion-based World Models via Linear-Time Sequence Modeling",
    "volume": "spotlight",
    "abstract": "World models represent a promising approach for training reinforcement learning agents with significantly improved sample efficiency. While most world model methods primarily rely on sequences of discrete latent variables to model environment dynamics, this compression often neglects critical visual details essential for reinforcement learning. Recent diffusion-based world models condition generation on a fixed context length of frames to predict the next observation, using separate recurrent neural networks to model rewards and termination signals. Although this architecture effectively enhances visual fidelity, the fixed context length approach inherently limits memory capacity. In this paper, we introduce EDELINE, a unified world model architecture that integrates state space models with diffusion models. Our approach outperforms existing baselines across visually challenging Atari 100k tasks, memory-demanding Crafter benchmark, and 3D first-person ViZDoom environments, demonstrating superior performance in all these diverse challenges. Code is available at https://github.com/LJH-coding/EDELINE",
    "checked": true,
    "id": "68ba4d760efd49d60935e8df75d0d8d6522f75c1",
    "semantic_title": "edeline: enhancing memory in diffusion-based world models via linear-time sequence modeling",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f6aOPkGE8L": {
    "title": "UniTok: a Unified Tokenizer for Visual Generation and Understanding",
    "volume": "spotlight",
    "abstract": "Visual generative and understanding models typically rely on distinct tokenizers to process images, presenting a key challenge for unifying them within a single framework. Recent studies attempt to address this by connecting the training of VQVAE (for autoregressive generation) and CLIP (for understanding) to build a unified tokenizer. However, directly combining these training objectives has been observed to cause severe loss conflicts. In this paper, we show that reconstruction and semantic supervision do not inherently conflict. Instead, the underlying bottleneck stems from limited representational capacity of discrete token space. Building on these insights, we introduce UniTok, a unified tokenizer featuring a novel multi-codebook quantization mechanism that effectively scales up the vocabulary size and bottleneck dimension. In terms of final performance, UniTok sets a new record of 0.38 rFID and 78.6\\% zero-shot accuracy on ImageNet. Besides, UniTok can be seamlessly integrated into MLLMs to unlock native visual generation capability, without compromising the understanding performance. Additionally, we show that UniTok favors cfg-free generation, reducing gFID from 14.6 to 2.5 on ImageNet 256$\\times$256 benchmark. All codes and models have been made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LP4Q7tPMbs": {
    "title": "NormFit: A Lightweight Solution for Few-Shot Federated Learning with Non-IID Data",
    "volume": "spotlight",
    "abstract": "Vision–Language Models (VLMs) have recently attracted considerable attention in Federated Learning (FL) due to their strong and robust performance. In particular, few-shot adaptation with pre-trained VLMs like CLIP enhances the performance of downstream tasks. However, existing methods still suffer from substantial communication overhead, high local computational demands, and suboptimal performance under non-IID user data. To simultaneously address all those limitations, we propose NormFit, a lightweight solution that selectively fine-tunes only a very small portion of the model parameters, specifically only the Pre-LayerNorm parameters of the vision encoder within a VLM. Overcoming the existing tradeoff between performance and communication/computation efficiency in few-shot FL, NormFit sets a new benchmark by simultaneously achieving superior accuracy and substantially reduced communication and computational demands. Theoretically, we show that NormFit yields a considerably smaller generalization gap compared to tuning all LayerNorm parameters. Importantly, NormFit can function effectively as a standalone solution or integrate seamlessly with existing few-shot fine-tuning methods to further enhance their performance. Notably, NormFit offers implementation simplicity, achieving these improvements without any algorithmic modifications, changes to the underlying model architecture, or the addition of external parameters",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m5wrqqcWbN": {
    "title": "Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs",
    "volume": "spotlight",
    "abstract": "Large vision-language models (LVLMs) are increasingly deployed in interactive applications such as virtual and augmented reality, where a first-person (egocentric) view captured by head-mounted cameras serves as key input. While this view offers fine-grained cues about user attention and hand-object interactions, its narrow field of view and lack of global context often lead to failures on spatially or contextually demanding queries. To address this, we introduce a framework that augments egocentric inputs with third-person (exocentric) views, providing complementary information such as global scene layout and object visibility to LVLMs. We present E3VQA, the first benchmark for multi-view question answering with 4K high-quality question-answer pairs grounded in synchronized ego-exo image pairs. Additionally, we propose M3CoT, a training-free prompting technique that constructs a unified scene representation by integrating scene graphs from three complementary perspectives. M3CoT enables LVLMs to reason more effectively across views, yielding consistent performance gains (4.84\\% for GPT-4o and 5.94\\% for Gemini 2.0 Flash) over a recent CoT baseline. Our extensive evaluation reveals key strengths and limitations of LVLMs in multi-view reasoning and highlights the value of leveraging both egocentric and exocentric inputs. The dataset and source code are available at [https://github.com/Leeinsu1/Towards-Comprehensive-Scene-Understanding](https://github.com/Leeinsu1/Towards-Comprehensive-Scene-Understanding)",
    "checked": true,
    "id": "dc9d2fcd04efb12f585135949b607e4663657b1a",
    "semantic_title": "towards comprehensive scene understanding: integrating first and third-person views for lvlms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4frj038M6W": {
    "title": "Accelerating Visual-Policy Learning through Parallel Differentiable Simulation",
    "volume": "spotlight",
    "abstract": "In this work, we propose a computationally efficient algorithm for visual policy learning that leverages differentiable simulation and first-order analytical policy gradients. Our approach decouple the rendering process from the computation graph, enabling seamless integration with existing differentiable simulation ecosystems without the need for specialized differentiable rendering software. This decoupling not only reduces computational and memory overhead but also effectively attenuates the policy gradient norm, leading to more stable and smoother optimization. We evaluate our method on standard visual control benchmarks using modern GPU-accelerated simulation. Experiments show that our approach significantly reduces wall-clock training time and consistently outperforms all baseline methods in terms of final returns. Notably, on complex tasks such as humanoid locomotion, our method achieves a $4\\times$ improvement in final return, and successfully learns a humanoid running policy within 4 hours on a single GPU. Videos and code are available on https://haoxiangyou.github.io/Dva_website",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HFT821Q83J": {
    "title": "Online Prediction with Limited Selectivity",
    "volume": "spotlight",
    "abstract": "Selective prediction [Dru13, QV19] models the scenario where a forecaster freely decides on the prediction window that their forecast spans. Many data statistics can be predicted to a non-trivial error rate *without any* distributional assumptions or expert advice, yet these results rely on that the forecaster may predict at any time. We introduce a model of Prediction with Limited Selectivity (PLS) where the forecaster can start the prediction only on a subset of the time horizon. We study the optimal prediction error both on an instance-by-instance basis and via an average-case analysis. We introduce a complexity measure that gives instance-dependent bounds on the optimal error. For a randomly-generated PLS instance, these bounds match with high probability",
    "checked": true,
    "id": "96d6aaaf08bc3b253868f5182321c2d202cc5154",
    "semantic_title": "online prediction with limited selectivity",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VCTt5DXiBe": {
    "title": "Generative Trajectory Stitching through Diffusion Composition",
    "volume": "spotlight",
    "abstract": "Effective trajectory stitching for long-horizon planning is a significant challenge in robotic decision-making. While diffusion models have shown promise in planning, they are limited to solving tasks similar to those seen in their training data. We propose CompDiffuser, a novel generative approach that can solve new tasks by learning to compositionally stitch together shorter trajectory chunks from previously seen tasks. Our key insight is modeling the trajectory distribution by subdividing it into overlapping chunks and learning their conditional relationships through a single bidirectional diffusion model. This allows information to propagate between segments during generation, ensuring physically consistent connections. We conduct experiments on benchmark tasks of various difficulties, covering different environment sizes, agent state dimension, trajectory types, training data quality, and show that CompDiffuser significantly outperforms existing methods",
    "checked": true,
    "id": "54f661403d21e8de34029d4e37a29c040af8c3b1",
    "semantic_title": "generative trajectory stitching through diffusion composition",
    "citation_count": 17,
    "authors": []
  },
  "https://openreview.net/forum?id=bRWkBD2BfK": {
    "title": "VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs",
    "volume": "spotlight",
    "abstract": "Vision Language Models (VLMs) excel at complex visual tasks such as VQA and chart understanding, yet recent work suggests they struggle with simple perceptual tests. We present an evaluation that tests vision-language models' capacity for \\emph{nonlocal visual reasoning}- reasoning that requires chaining evidence collected from multiple, possibly distant, regions of an image. We isolate three distinct forms of non‑local vision: \\emph{comparative perception}, which demands holding two images in working memory and comparing them; \\emph{saccadic search}, which requires making discrete, evidence‑driven jumps to locate successive targets; and \\emph{smooth visual search}, which involves searching smoothly along a continuous contour. Flagship models (e.g. GPT-5, Gemini 2.5 Pro, Claude Sonnet 4), even those that perform well on prior primitive‑vision benchmarks, fail these tests and barely exceed random accuracy on two variants of our tasks that are trivial for humans. Our structured evaluation suite allows us to test if VLMs can perform similar visual algorithms to humans. Our findings show that despite gains in raw visual acuity, current models lack core visual reasoning capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d4CZoiaXeC": {
    "title": "Learning to Factorize Spatio-Temporal Foundation Models",
    "volume": "spotlight",
    "abstract": "Spatio-Temporal Foundation Models (STFMs) promise zero/few-shot generalization across various datasets, yet joint spatio-temporal pretraining is computationally prohibitive and struggles with domain-specific spatial correlations. To this end, we introduce FactoST, a factorized STFM that decouples universal temporal pretraining from spatio-temporal adaptation. The first stage pretrains a space-agnostic backbone with multi-frequency reconstruction and domain-aware prompting, capturing cross-domain temporal regularities at low computational cost. The second stage freezes or further fine-tunes the backbone and attaches an adapter that fuses spatial metadata, sparsifies interactions, and aligns domains with continual memory replay. Extensive forecasting experiments reveal that, in few-shot setting, FactoST reduces MAE by up to 46.4% versus UniST, uses 46.2% fewer parameters, and achieves 68% faster inference than OpenCity, while remaining competitive with expert models. We believe this factorized view offers a practical and scalable path toward truly universal STFMs. The code will be released upon notification",
    "checked": false,
    "id": "bc2fdf293d590164f705231d09bc955fa22055ae",
    "semantic_title": "diffusion transformers as open-world spatiotemporal foundation models",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=KGt0F2yjBz": {
    "title": "Angles Don't Lie: Unlocking Training‑Efficient RL Through the Model's Own Signals",
    "volume": "spotlight",
    "abstract": "Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models (LLMs) suffer from sample inefficiency due to the redundant exposure of identical queries under uniform data sampling. While previous work has explored curriculum learning via heuristic difficulty metrics, these strategies exhibit limitations by neglecting the intrinsic learning signals generated by the model itself, thus leading to suboptimal training regimes. In this paper, we identify a model-inherent signal termed *angle concentration* that effectively reflects an LLM's capacity to learn from specific data. We theoretically and empirically demonstrate a correlation between the angular distribution of token hidden state vectors and the resulting gradient, revealing a learning preference for data exhibiting higher angle concentration. Inspired by this finding, we propose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. By leveraging the model's intrinsic angle concentration signal, GAIN-RL dynamically selects training data in each epoch, ensuring consistently impactful gradient updates and thus significantly enhancing overall training efficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5$\\times$ acceleration in training efficiency across diverse mathematical and coding tasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient sampling yields data-efficient training, achieving better performance with half the original data compared to vanilla GRPO with full training data",
    "checked": false,
    "id": "bd0eeba9ed341c56dd9dbc0d80585a53f77f4657",
    "semantic_title": "angles don't lie: unlocking training-efficient rl through the model's own signals",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=ROfYsQ2KNV": {
    "title": "Generalizable Insights for Graph Transformers in Theory and Practice",
    "volume": "spotlight",
    "abstract": "Graph Transformers (GTs) have shown strong empirical performance, yet current architectures vary widely in their use of attention mechanisms, positional embeddings (PEs), and expressivity. Existing expressivity results are often tied to specific design choices and lack comprehensive empirical validation on large-scale data. This leaves a gap between theory and practice, preventing generalizable insights that exceed particular application domains. Here, we propose the Generalized-Distance Transformer (GDT), a GT architecture using standard attention that incorporates many advancements for GTs from recent years, and develop a fine-grained understanding of the GDT's representation power in terms of attention and PEs. Through extensive experiments, we identify design choices that consistently perform well across various applications, tasks, and model scales, demonstrating strong performance in a few-shot transfer setting without fine-tuning. Our evaluation covers over eight million graphs with roughly 270M tokens across diverse domains, including image-based object detection, molecular property prediction, code summarization, and out-of-distribution algorithmic reasoning. We distill our theoretical and practical findings into several generalizable insights about effective GT design, training, and inference",
    "checked": true,
    "id": "09075974abd5db2547e4efa088d64af66fdbd339",
    "semantic_title": "generalizable insights for graph transformers in theory and practice",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n5NZqzAITL": {
    "title": "Principled Data Augmentation for Learning to Solve Quadratic Programming Problems",
    "volume": "spotlight",
    "abstract": "Linear and quadratic optimization are crucial in numerous real-world applications, ranging from training machine learning models to solving integer linear programs. Recently, learning-to-optimize methods (L2O) for linear (LPs) or quadratic programs (QPs) using message-passing graph neural networks (MPNNs) have gained traction, promising lightweight, data-driven proxies for solving such optimization problems. For example, they replace the costly computation of strong branching scores in branch-and-bound solvers, thereby reducing the need to solve many such optimization problems. However, robust L2O MPNNs remain challenging in data-scarce settings, especially when addressing complex optimization problems such as QPs. This work introduces a principled approach to data augmentation tailored for QPs via MPNNs. Our method leverages theoretically justified data augmentation techniques to generate diverse yet optimality-preserving instances. Furthermore, we integrate these augmentations into a self-supervised contrastive learning framework, thereby pretraining MPNNs for improved performance on L2O tasks. Extensive experiments demonstrate that our approach improves generalization in supervised scenarios and facilitates effective transfer learning to related optimization problems",
    "checked": true,
    "id": "6dccfdcac3d9fa0545e8f36a69d6fe6752a01e8c",
    "semantic_title": "principled data augmentation for learning to solve quadratic programming problems",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C39ShJwtD5": {
    "title": "Dual Data Alignment Makes AI-Generated Image Detector Easier Generalizable",
    "volume": "spotlight",
    "abstract": "The rapid increase in AI-generated images (AIGIs) underscores the need for detection methods. Existing detectors are often trained on biased datasets, leading to overfitting on spurious correlations between non-causal image attributes and real/synthetic labels. While these biased features enhance performance on the training data, they result in substantial performance degradation when tested on unbiased datasets. A common solution is to perform data alignment through generative reconstruction, matching the content between real and synthetic images. However, we find that pixel-level alignment alone is inadequate, as the reconstructed images still suffer from frequency-level misalignment, perpetuating spurious correlations. To illustrate, we observe that reconstruction models restore the high-frequency details lost in real images, inadvertently creating a frequency-level misalignment, where synthetic images appear to have richer high-frequency content than real ones. This misalignment leads to models associating high-frequency features with synthetic labels, further reinforcing biased cues. To resolve this, we propose Dual Data Alignment (DDA), which aligns both the pixel and frequency domains. DDA generates synthetic images that closely resemble real ones by fusing real and synthetic image pairs in both domains, enhancing the detector's ability to identify forgeries without relying on biased features. Moreover, we introduce two new test sets: DDA-COCO, containing DDA-aligned synthetic images, and EvalGEN, featuring the latest generative models. Our extensive evaluations demonstrate that a detector trained exclusively on DDA-aligned MSCOCO improves across diverse benchmarks. Code is available at https://github.com/roy-ch/Dual-Data-Alignment",
    "checked": true,
    "id": "f462846f364767d0f8eccd5957a9a34ed1363545",
    "semantic_title": "dual data alignment makes ai-generated image detector easier generalizable",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=agcXjEHmyW": {
    "title": "CSBrain: A Cross-scale Spatiotemporal Brain Foundation Model for EEG Decoding",
    "volume": "spotlight",
    "abstract": "Understanding and decoding human brain activity from electroencephalography (EEG) signals is a fundamental problem in neuroscience and artificial intelligence, with applications ranging from cognition and emotion recognition to clinical diagnosis and brain–computer interfaces. While recent EEG foundation models have made progress in generalized brain decoding by leveraging unified architectures and large-scale pretraining, they inherit a scale-agnostic dense modeling paradigm from NLP and vision. This design overlooks an intrinsic property of neural activity—cross-scale spatiotemporal structure. Different EEG task patterns span a broad range of temporal and spatial scales, from brief neural activations to slow-varying rhythms, and from localized cortical activations to large-scale distributed interactions. Ignoring this diversity may lead to suboptimal representations and weakened generalization ability. To address these limitations, we propose CSBrain, a Cross-scale Spatiotemporal Brain foundation model for generalized EEG decoding. CSBrain introduces two key components: (i) Cross-scale Spatiotemporal Tokenization (CST), which aggregates multi-scale features within localized temporal windows and anatomical brain regions into compact scale-aware token representations; and (ii) Structured Sparse Attention (SSA), which models cross-window and cross-region dependencies for diverse decoding tasks, further enriching scale diversities while eliminating the spurious dependencies. CST and SSA are alternately stacked to progressively integrate cross-scale spatiotemporal dependencies. Extensive experiments across 11 representative EEG tasks and 16 datasets demonstrate that CSBrain consistently outperforms both task-specific models and strong foundation baselines. These results establish cross-scale modeling as a key inductive bias for generalized EEG decoding and highlight CSBrain as a robust backbone for future brain–AI research",
    "checked": true,
    "id": "5b26f69b54dfcab7c78554b45725f42da5a589dc",
    "semantic_title": "csbrain: a cross-scale spatiotemporal brain foundation model for eeg decoding",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=KaD2Dw8Ahz": {
    "title": "Flattening Hierarchies with Policy Bootstrapping",
    "volume": "spotlight",
    "abstract": "Offline goal-conditioned reinforcement learning (GCRL) is a promising approach for pretraining generalist policies on large datasets of reward-free trajectories, akin to the self-supervised objectives used to train foundation models for computer vision and natural language processing. However, scaling GCRL to longer horizons remains challenging due to the combination of sparse rewards and discounting, which obscures the comparative advantages of primitive actions with respect to distant goals. Hierarchical RL methods achieve strong empirical results on long-horizon goal-reaching tasks, but their reliance on modular, timescale-specific policies and subgoal generation introduces significant additional complexity and hinders scaling to high-dimensional goal spaces. In this work, we introduce an algorithm to train a flat (non-hierarchical) goal-conditioned policy by bootstrapping on subgoal-conditioned policies with advantage-weighted importance sampling. Our approach eliminates the need for a generative model over the (sub)goal space, which we find is key for scaling to high-dimensional control in large state spaces. We further show that existing hierarchical and bootstrapping-based approaches correspond to specific design choices within our derivation. Across a comprehensive suite of state- and pixel-based locomotion and manipulation benchmarks, our method matches or surpasses state-of-the-art offline GCRL algorithms and scales to complex, long-horizon tasks where prior approaches fail. Project page: https://johnlyzhou.github.io/saw/",
    "checked": true,
    "id": "bfaefd0879622ac96619ed236d9ad11abfcc7fcf",
    "semantic_title": "flattening hierarchies with policy bootstrapping",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=xOqCKB8XIl": {
    "title": "HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios",
    "volume": "spotlight",
    "abstract": "Action segmentation is a core challenge in high-level video understanding, aiming to partition untrimmed videos into segments and assign each a label from a predefined action set. Existing methods primarily address single-person activities with fixed action sequences, overlooking multi-person scenarios. In this work, we pioneer textual reference-guided human action segmentation in multi-person settings, where a textual description specifies the target person for segmentation. We introduce the first dataset for Referring Human Action Segmentation, i.e., RHAS133, built from 133 movies and annotated with 137 fine-grained actions with 33h video data, together with textual descriptions for this new task. Benchmarking existing action segmentation methods on RHAS133 using VLM-based feature extractors reveals limited performance and poor aggregation of visual cues for the target person. To address this, we propose a holistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF, leveraging a novel cross-input gate attentional xLSTM to enhance holistic-partial long-range reasoning and a novel Fourier condition to introduce more fine-grained control to improve the action segmentation generation. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse evaluation settings. The dataset and code are available at https://github.com/KPeng9510/HopaDIFF",
    "checked": true,
    "id": "05e3b5dd8b2c4dc2fc0e347c35d2c629d403eaf7",
    "semantic_title": "hopadiff: holistic-partial aware fourier conditioned diffusion for referring human action segmentation in multi-person scenarios",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nwlX15Wnr9": {
    "title": "Geometry Meets Incentives: Sample-Efficient Incentivized Exploration with Linear Contexts",
    "volume": "spotlight",
    "abstract": "In the incentivized exploration model, a principal aims to explore and learn over time by interacting with a sequence of self-interested agents. It has been recently understood that the main challenge in designing incentive-compatible algorithms for this problem is to gather a moderate amount of initial data, after which one can obtain near-optimal regret via posterior sampling. With high-dimensional contexts, however, this \\emph{initial exploration} phase requires exponential sample complexity in some cases, which prevents efficient learning unless initial data can be acquired exogenously. We show that these barriers to exploration disappear under mild geometric conditions on the set of available actions, in which case incentive-compatibility does not preclude regret-optimality. Namely, we consider the linear bandit model with actions in the Euclidean unit ball, and give an incentive-compatible exploration algorithm with sample complexity that scales polynomially with the dimension and other parameters",
    "checked": true,
    "id": "2fcf281352d69e2dc6ceea7fd729a3056ddbabd0",
    "semantic_title": "geometry meets incentives: sample-efficient incentivized exploration with linear contexts",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RnfyqrkOxD": {
    "title": "GeoRemover: Removing Objects and Their Causal Visual Artifacts",
    "volume": "spotlight",
    "abstract": "Towards intelligent image editing, object removal should eliminate both the target object and its causal visual artifacts, such as shadows and reflections. However, existing image appearance-based methods either follow strictly mask-aligned training and fail to remove these casual effects which are not explicitly masked, or adopt loosely mask-aligned strategies that lack controllability and may unintentionally over-erase other objects. We identify that these limitations stem from ignoring the causal relationship between an object's geometry presence and its visual effects. To address this limitation, we propose a geometry-aware two-stage framework that decouples object removal into (1) geometry removal and (2) appearance rendering. In the first stage, we remove the object directly from the geometry (e.g., depth) using strictly mask-aligned supervision, enabling structure-aware editing with strong geometric constraints. In the second stage, we render a photorealistic RGB image conditioned on the updated geometry, where causal visual effects are considered implicitly as a result of the modified 3D geometry. To guide learning in the geometry removal stage, we introduce a preference-driven objective based on positive and negative sample pairs, encouraging the model to remove objects as well as their causal visual artifacts while avoiding new structural insertions. Extensive experiments demonstrate that our method achieves state-of-the-art performance in removing both objects and their associated artifacts on two popular benchmarks. The project page is available at https://buxiangzhiren.github.io/GeoRemover",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s4LnWgjacg": {
    "title": "The Primacy of Magnitude in Low-Rank Adaptation",
    "volume": "spotlight",
    "abstract": "Low-Rank Adaptation (LoRA) offers a parameter-efficient paradigm for tuning large models. While recent spectral initialization methods improve convergence and performance over the naive \"Noise \\& Zeros\" scheme, their extra computational and storage overhead undermines efficiency. In this paper, we establish update magnitude as the fundamental driver of LoRA performance and propose LoRAM, a magnitude-driven \"Basis \\& Basis\" initialization scheme that matches spectral methods without their inefficiencies. Our key contributions are threefold: (i) Magnitude of weight updates determines convergence. We prove low-rank structures intrinsically bound update magnitudes, unifying hyperparameter tuning in learning rate, scaling factor, and initialization as mechanisms to optimize magnitude regulation. (ii) Spectral initialization succeeds via magnitude amplification. We demystify that the presumed knowledge-driven benefit of spectral component essentially arises from the boost in the weight update magnitude. (iii) A novel and compact initialization strategy, LoRAM, scales deterministic orthogonal bases using pretrained weight magnitudes to simulate spectral gains. Extensive experiments show that LoRAM serves as a strong baseline, retaining the full efficiency of LoRA while matching or outperforming spectral initialization across benchmarks",
    "checked": true,
    "id": "bbd48872f30d48f4ab61d3c8909bd30468baee52",
    "semantic_title": "the primacy of magnitude in low-rank adaptation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z3PMVmzoya": {
    "title": "GeRaF: Neural Geometry Reconstruction from Radio Frequency Signals",
    "volume": "spotlight",
    "abstract": "GeRaF is the first method to use neural implicit learning for near-range 3D geometry reconstruction from radio frequency (RF) signals. Unlike RGB or LiDAR-based methods, RF sensing can see through occlusion but suffers from low resolution and noise due to its lens-less imaging nature. While lenses in RGB imaging constrain sampling to 1D rays, RF signals propagate through the entire space, introducing significant noise and leading to cubic complexity in volumetric rendering. Moreover, RF signals interact with surfaces via specular reflections requiring fundamentally different modeling. To address these challenges, GeRaF (1) introduces filter-based rendering to suppress irrelevant signals, (2) implements a physics-based RF volumetric rendering pipeline, and (3) proposes a novel lens-less sampling and lens-less alpha blending strategy that makes full-space sampling feasible during training. By learning signed distance functions, reflectiveness, and signal power through MLPs and trainable parameters, GeRaF takes the first step towards reconstructing millimeter-level geometry from RF signals in real-world settings",
    "checked": false,
    "id": "38e0bf31db7abedd0d32b1abced573ece9d9a06a",
    "semantic_title": "an snr-aware feature reconstruction method in radio frequency fingerprint identification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SBYCu5uJJf": {
    "title": "Spatial Understanding from Videos: Structured Prompts Meet Simulation Data",
    "volume": "spotlight",
    "abstract": "Visual-spatial understanding, the ability to infer object relationships and layouts from visual input, is fundamental to downstream tasks such as robotic navigation and embodied interaction. However, existing methods face spatial uncertainty and data scarcity, limiting the 3D spatial reasoning capability of pre-trained vision-language models (VLMs). To address these challenges, we present a unified framework for enhancing 3D spatial reasoning in pre-trained VLMs without modifying their architecture. This framework combines SpatialMind, a structured prompting strategy that decomposes complex scenes and questions into interpretable reasoning steps, with ScanForgeQA, a scalable question-answering dataset built from diverse 3D simulation scenes through an automated construction process designed for fine-tuning. Extensive experiments across multiple benchmarks demonstrate the individual and combined effectiveness of our prompting and fine-tuning strategies, and yield insights that may inspire future research on visual-spatial understanding",
    "checked": true,
    "id": "af5ccd4120946917a27a5c63b70e0ba3221f1abf",
    "semantic_title": "spatial understanding from videos: structured prompts meet simulation data",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Sct4sajCi6": {
    "title": "SATURN: SAT-based Reinforcement Learning to Unleash LLMs Reasoning",
    "volume": "spotlight",
    "abstract": "How to design reinforcement learning (RL) tasks that effectively unleash the reasoning capability of large language models (LLMs) remains an open question. Existing RL tasks (e.g., math, programming, and constructing reasoning tasks) suffer from three key limitations: (1) Scalability. They rely heavily on human annotation or expensive LLM synthesis to generate sufficient training data. (2) Verifiability. LLMs' outputs are hard to verify automatically and reliably. (3) Controllable Difficulty. Most tasks lack fine-grained difficulty control, making it hard to train LLMs to develop reasoning ability from easy to hard. To address these limitations, we propose Saturn, a SAT-based RL framework that uses Boolean Satisfiability (SAT) problems to train and evaluate LLMs reasoning. Saturn enables scalable task construction, rule-based verification, and precise difficulty control. Saturn designs a curriculum learning pipeline that continuously improves LLMs' reasoning capability by constructing SAT tasks of increasing difficulty and training LLMs from easy to hard. To ensure stable training, we design a principled mechanism to control difficulty transitions. We introduce Saturn-2.6k, a dataset of 2,660 SAT problems with varying difficulty. It supports the evaluation of how LLM reasoning changes with problem difficulty. We apply Saturn to DeepSeek-R1-Distill-Qwen and obtain Saturn-1.5B and Saturn-7B. We achieve several notable results: (1) On SAT problems, Saturn-1.5B and Saturn-7B achieve average pass@3 improvements of +14.0 and +28.1, respectively. (2) On math and programming tasks, Saturn-1.5B and Saturn-7B improve average scores by +4.9 and +1.8 on benchmarks (e.g., AIME, LiveCodeBench). (3) Compared to the state-of-the-art (SOTA) approach in constructing RL tasks, Saturn achieves further improvements of +8.8\\%. We release the source code, data, and models to support future research",
    "checked": false,
    "id": "6f3dc2dd868207a5c10f3a268641bb8e2bd9410a",
    "semantic_title": "saturn: sat-based reinforcement learning to unleash language model reasoning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=MZoOpD9NHV": {
    "title": "JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation",
    "volume": "spotlight",
    "abstract": "This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder–LLM–decoder architecture, featuring a SyncFusion module for spatio-temporal audio- video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. To support this, we further construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that span diverse and multi-level comprehension and generation scenarios. Extensive experiments on JAV comprehension and generation benchmarks show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t4aN2G7Ucc": {
    "title": "Dimension-adapted Momentum Outscales SGD",
    "volume": "spotlight",
    "abstract": "We investigate scaling laws for stochastic momentum algorithms on the power law random features model, parameterized by data complexity, target complexity, and model size. When trained with a stochastic momentum algorithm, our analysis reveals four distinct loss curve shapes determined by varying data-target complexities. While traditional stochastic gradient descent with momentum (SGD-M) yields identical scaling law exponents to SGD, dimension-adapted Nesterov acceleration (DANA) improves these exponents by scaling momentum hyperparameters based on model size and data complexity. This outscaling phenomenon, which also improves compute-optimal scaling behavior, is achieved by DANA across a broad range of data and target complexities, while traditional methods fall short. Extensive experiments on high-dimensional synthetic quadratics validate our theoretical predictions and large-scale text experiments with LSTMs show DANA's improved loss exponents over SGD hold in a practical setting",
    "checked": true,
    "id": "9e9f532646b0c53c4c3b76639ff49e8e2260d2b7",
    "semantic_title": "dimension-adapted momentum outscales sgd",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZBSkyMwdEB": {
    "title": "From Experts to a Generalist: Toward General Whole-Body Control for Humanoid Robots",
    "volume": "spotlight",
    "abstract": "Achieving general agile whole-body control on humanoid robots remains a major challenge due to diverse motion demands and data conflicts. While existing frameworks excel in training single motion-specific policies, they struggle to generalize across highly varied behaviors due to conflicting control requirements and mismatched data distributions. In this work, we propose BumbleBee (BB), an expert-generalist learning framework that combines motion clustering and sim-to-real adaptation to overcome these challenges. BB first leverages an autoencoder-based clustering method to group behaviorally similar motions using motion features and motion descriptions. Expert policies are then trained within each cluster and refined with real-world data through iterative delta action modeling to bridge the sim-to-real gap. Finally, these experts are distilled into a unified generalist controller that preserves agility and robustness across all motion types. Experiments on two simulations and a real humanoid robot demonstrate that BB achieves state-of-the-art general whole-body control, setting a new benchmark for agile, robust, and generalizable humanoid performance in the real world",
    "checked": true,
    "id": "b47b70c344624a1032cd8495d89bf9542cc81feb",
    "semantic_title": "from experts to a generalist: toward general whole-body control for humanoid robots",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=VlvtStQN34": {
    "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with Rectified Flow Transformers",
    "volume": "spotlight",
    "abstract": "We introduce LoRAShop, the first framework for multi-concept image generation and editing with LoRA models. LoRAShop builds on a key observation about the feature interaction patterns inside Flux-style diffusion transformers: concept-specific transformer features activate spatially coherent regions early in the denoising process. We harness this observation to derive a disentangled latent mask for each concept in a prior forward pass and blend the corresponding LoRA weights only within regions bounding the concepts to be personalized. The resulting edits seamlessly integrate multiple subjects or styles into the original scene while preserving global context, lighting, and fine details. Our experiments demonstrate that LoRAShop delivers better identity preservation compared to baselines. By eliminating retraining and external constraints, LoRAShop turns personalized diffusion models into a practical `photoshop-with-LoRAs' tool and opens new avenues for compositional visual storytelling and rapid creative iteration",
    "checked": true,
    "id": "cfd8851cc343fee867dfad3a9627a1a16890e9f0",
    "semantic_title": "lorashop: training-free multi-concept image generation and editing with rectified flow transformers",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=EtqwyqJrJO": {
    "title": "GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction",
    "volume": "spotlight",
    "abstract": "Reconstructing accurate surfaces with radiance fields has achieved remarkable progress in recent years. However, prevailing approaches, primarily based on Gaussian Splatting, are increasingly constrained by representational bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based framework that explores and extends the under-investigated potential of sparse voxels for achieving accurate, detailed, and complete surface reconstruction. As strengths, sparse voxels support preserving the coverage completeness and geometric clarity, while corresponding challenges also arise from absent scene constraints and locality in surface refinement. To ensure correct scene convergence, we first propose a Voxel-Uncertainty Depth Constraint that maximizes the effect of monocular depth cues while presenting a voxel-oriented uncertainty to avoid quality degradation, enabling effective and robust scene constraints yet preserving highly accurate geometries. Subsequently, Sparse Voxel Surface Regularization is designed to enhance geometric consistency for tiny voxels and facilitate the voxel-based formation of sharp and accurate surfaces. Extensive experiments demonstrate our superior performance compared to existing methods across diverse challenging scenarios, excelling in geometric accuracy, detail preservation, and reconstruction completeness while maintaining high efficiency. Code is available at https://github.com/Fictionarry/GeoSVR",
    "checked": true,
    "id": "eb7da64051b6cba8abe606319a885552113f54b0",
    "semantic_title": "geosvr: taming sparse voxels for geometrically accurate surface reconstruction",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=MRvxlTlkNQ": {
    "title": "Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning",
    "volume": "spotlight",
    "abstract": "Large language models are typically adapted to downstream tasks through supervised fine-tuning on domain-specific data. While standard fine-tuning focuses on minimizing generation loss to optimize model parameters, we take a deeper step by retaining and leveraging the model's own learning signals, analogous to how human learners reflect on past mistakes to improve future performance. We first introduce the concept of Mistake Log to systematically track the model's learning behavior and recurring errors throughout fine-tuning. Treating the original transformer-based model as the Pilot, we correspondingly design a Copilot model to refine the Pilot's inference performance via logits rectification. We name the overall Pilot-Copilot framework the Transformer Copilot, which introduces (i) a novel Copilot model design, (ii) a joint training paradigm where the Copilot continuously learns from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference paradigm where the Copilot rectifies the Pilot's logits for enhanced generation. We provide both theoretical and empirical analyses on our new learning framework. Experiments on 12 benchmarks spanning commonsense, arithmetic, and recommendation tasks demonstrate that Transformer Copilot consistently improves performance by up to 34.5%, while introducing marginal computational overhead to Pilot models and exhibiting strong scalability and transferability. Our code is released at https://github.com/jiaruzouu/TransformerCopilot",
    "checked": true,
    "id": "a5a360af27af5153fbdbeb685159b4adcc930d66",
    "semantic_title": "transformer copilot: learning from the mistake log in llm fine-tuning",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=tI04KmK27S": {
    "title": "scMRDR: A scalable and flexible framework for unpaired single-cell multi-omics data integration",
    "volume": "spotlight",
    "abstract": "Advances in single-cell sequencing have enabled high-resolution profiling of diverse molecular modalities, while integrating unpaired multi-omics single-cell data remains challenging. Existing approaches either rely on pair information or prior correspondences, or require computing a global pairwise coupling matrix, limiting their scalability and flexibility. In this paper, we introduce a scalable and flexible generative framework called single-cell Multi-omics Regularized Disentangled Representations (scMRDR) for unpaired multi-omics integration. Specifically, we disentangle each cell's latent representations into modality-shared and modality-specific components using a well-designed $\\beta$-VAE architecture, which are augmented with isometric regularization to preserve intra-omics biological heterogeneity, adversarial objective to encourage cross-modal alignment, and masked reconstruction loss strategy to address the issue of missing features across modalities. Our method achieves excellent performance on benchmark datasets in terms of batch correction, modality alignment, and biological signal preservation. Crucially, it scales effectively to large-level datasets and supports integration of more than two omics, offering a powerful and flexible solution for large-scale multi-omics data integration and downstream biological discovery",
    "checked": true,
    "id": "5b2b235ae3ce91f2c1853b4b78b2f1b1be05ca51",
    "semantic_title": "scmrdr: a scalable and flexible framework for unpaired single-cell multi-omics data integration",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YX5DHa9OfX": {
    "title": "Imitation Beyond Expectation Using Pluralistic Stochastic Dominance",
    "volume": "spotlight",
    "abstract": "Imitation learning seeks policies reflecting the values of demonstrated behaviors. Prevalent approaches learn to match or exceed the demonstrator's performance in expectation without knowing the demonstrator's reward function. Unfortunately, this does not induce pluralistic imitators that learn to support qualitatively distinct demonstrations. We reformulate imitation learning using stochastic dominance over the demonstrations' reward distribution across a range of reward functions as our foundational aim. Our approach matches imitator policy samples (or support) with demonstrations using optimal transport theory to define an imitation learning objective over trajectory pairs. We demonstrate the benefits of pluralistic stochastic dominance (PSD) for imitation in both theory and practice",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g2AAvmBwkS": {
    "title": "Cloud4D: Estimating Cloud Properties at a High Spatial and Temporal Resolution",
    "volume": "spotlight",
    "abstract": "There has been great progress in improving numerical weather prediction and climate models using machine learning. However, most global models act at a kilometer-scale, making it challenging to model individual clouds and factors such as extreme precipitation, wind gusts, turbulence, and surface irradiance. Therefore, there is a need to move towards higher-resolution models, which in turn require high-resolution real-world observations that current instruments struggle to obtain. We present Cloud4D, the first learning-based framework that reconstructs a physically consistent, four–dimensional cloud state using only synchronized ground‐based cameras. Leveraging a homography-guided 2D‐to‐3D transformer, Cloud4D infers the full 3D distribution of liquid water content at 25 m spatial and 5 s temporal resolution. By tracking the 3D liquid water content retrievals over time, Cloud4D additionally estimates horizontal wind vectors. Across a two-month deployment comprising six skyward cameras, our system delivers an order-of-magnitude improvement in space-time resolution relative to state-of-the-art satellite measurements, while retaining single-digit relative error ($<10\\\\%$) against collocated radar measurements",
    "checked": false,
    "id": "cbb8c7f09ce34d2a6d78742d51ab28a230363dc6",
    "semantic_title": "soil organic carbon stocks in native forest of argentina: a useful surrogate for mitigation and conservation planning under climate variability",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=MGJVhzWa2s": {
    "title": "CLiFT: Compressive Light-Field Tokens for Compute Efficient and Adaptive Neural Rendering",
    "volume": "spotlight",
    "abstract": "This paper proposes a neural rendering approach that represents a scene as \"compressed light-field tokens (CLiFTs)\", retaining rich appearance and geometric information of a scene. CLiFT enables compute-efficient rendering by compressed tokens, while being capable of changing the number of tokens to represent a scene or render a novel view with one trained network. Concretely, given a set of images, multi-view encoder tokenizes the images with the camera poses. Latent-space K-means selects a reduced set of rays as cluster centroids using the tokens. The multi-view ``condenser'' compresses the information of all the tokens into the centroid tokens to construct CLiFTs. At test time, given a target view and a compute budget (i.e., the number of CLiFTs), the system collects the specified number of nearby tokens and synthesizes a novel view using a compute-adaptive renderer. trained to handle a variable number of tokens. Extensive experiments on RealEstate10K and DL3DV datasets quantitatively and qualitatively validate our approach, achieving significant data reduction with comparable rendering quality and the highest overall rendering score, while providing trade-offs of data size, rendering quality, and rendering speed",
    "checked": false,
    "id": "48e6dc92cd6175a7af0d3bcf939ac05207d19683",
    "semantic_title": "clift: compressive light-field tokens for compute-efficient and adaptive neural rendering",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qgi5TfBXBw": {
    "title": "Adaptive Neighborhood-Constrained Q Learning for Offline Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "Offline reinforcement learning (RL) suffers from extrapolation errors induced by out-of-distribution (OOD) actions. To address this, offline RL algorithms typically impose constraints on action selection, which can be systematically categorized into density, support, and sample constraints. However, we show that each category has inherent limitations: density and sample constraints tend to be overly conservative in many scenarios, while the support constraint, though least restrictive, faces challenges in accurately modeling the behavior policy. To overcome these limitations, we propose a new neighborhood constraint that restricts action selection in the Bellman target to the union of neighborhoods of dataset actions. Theoretically, the constraint not only bounds extrapolation errors and distribution shift under certain conditions, but also approximates the support constraint without requiring behavior policy modeling. Moreover, it retains substantial flexibility and enables pointwise conservatism by adapting the neighborhood radius for each data point. In practice, we employ data quality as the adaptation criterion and design an adaptive neighborhood constraint. Building on an efficient bilevel optimization framework, we develop a simple yet effective algorithm, Adaptive Neighborhood-constrained Q learning (ANQ), to perform Q learning with target actions satisfying this constraint. Empirically, ANQ achieves state-of-the-art performance on standard offline RL benchmarks and exhibits strong robustness in scenarios with noisy or limited data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YexxvBGwQM": {
    "title": "To Think or Not To Think: A Study of Thinking in Rule-Based Visual Reinforcement Fine-Tuning",
    "volume": "spotlight",
    "abstract": "This paper investigates the role of explicit thinking process in rule-based reinforcement fine-tuning (RFT) for multi-modal large language models (MLLMs). We first extend \\textit{Thinking-RFT} to image classification task, using verifiable rewards for fine-tuning~(FT). Experiments show {Thinking-RFT} significantly outperforms supervised FT and yields a cross-dataset generalization effect. We then rethink and question whether explicit thinking in RFT is always necessary and beneficial. Challenging the convention that explicit thinking is crucial for the success of RFT, we introduce \\textit{No-Thinking-RFT}, exploring RFT without thinking by introducing a simple equality accuracy reward. We evaluate No-Thinking-RFT on six diverse tasks across different model sizes and types. Experiment results reveal four key findings: \\textbf{(1).} Visual perception tasks do not require thinking during RFT, as No-Thinking-RFT consistently outperforms or matches Thinking-RFT across model sizes and types. \\textbf{(2).} Models with limited capabilities struggle to generate high-quality CoT for RFT, making Thinking-RFT less effective than No-Thinking-RFT. \\textbf{(3).} There are inconsistencies between the answers in the thinking tags and answer tags for some responses of Thinking-RFT, which show lower average accuracy than the overall accuracy. \\textbf{(4).} The performance gain of No-Thinking-RFT mainly stems from improved learning during no thinking FT and the avoidance of inference overthinking, as evidenced by the partial gains from appending empty thinking tags at inference time of Thinking-RFT. We hypothesize that explicit thinking before verifiable answers may hinder reward convergence and reduce performance in certain scenarios. To test this, we propose \\textit{Think-After-Answer}, which places thinking after the answer to mitigate this effect for experimental verification. Lastly, we conduct a pilot study to explore whether MLLMs can learn when to think during RFT, introducing an \\textit{Adaptive-Thinking} method. Experiments show that model converges to either thinking or not depending on model capability, achieving comparable or better performance than both Thinking and No-Thinking-RFT. Our findings suggest MLLMs can adaptively decide to think or not based on their capabilities and task complexity, offering insights into the thinking process in RFT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZZ09oX2Xpo": {
    "title": "DexGarmentLab: Dexterous Garment Manipulation Environment with Generalizable Policy",
    "volume": "spotlight",
    "abstract": "Garment manipulation is a critical challenge due to the diversity in garment categories, geometries, and deformations. Despite this, humans can effortlessly handle garments, thanks to the dexterity of our hands. However, existing research in the field has struggled to replicate this level of dexterity, primarily hindered by the lack of realistic simulations of dexterous garment manipulation. Therefore, we propose DexGarmentLab, the first environment specifically designed for dexterous (especially bimanual) garment manipulation, which features large-scale high-quality 3D assets for 15 task scenarios, and refines simulation techniques tailored for garment modeling to reduce the sim-to-real gap. Previous data collection typically relies on teleoperation or training expert reinforcement learning (RL) policies, which are labor-intensive and inefficient. In this paper, we leverage garment structural correspondence to automatically generate a dataset with diverse trajectories using only a single expert demonstration, significantly reducing manual intervention. However, even extensive demonstrations cannot cover the infinite states of garments, which necessitates the exploration of new algorithms. To improve generalization across diverse garment shapes and deformations, we propose a Hierarchical gArment-manipuLation pOlicy (HALO). It first identifies transferable affordance points to accurately locate the manipulation area, then generates generalizable trajectories to complete the task. Through extensive experiments and detailed analysis of our method and baseline, we demonstrate that HALO consistently outperforms existing methods, successfully generalizing to previously unseen instances even with significant variations in shape and deformation where others fail. Our project page is available at: https://wayrise.github.io/DexGarmentLab/",
    "checked": true,
    "id": "6a56eb117475bc16b2c470b39ea31da4613e5149",
    "semantic_title": "dexgarmentlab: dexterous garment manipulation environment with generalizable policy",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=AghtKxDf7f": {
    "title": "STITCH-OPE: Trajectory Stitching with Guided Diffusion for Off-Policy Evaluation",
    "volume": "spotlight",
    "abstract": "Off-policy evaluation (OPE) estimates the performance of a target policy using offline data collected from a behavior policy, and is crucial in domains such as robotics or healthcare where direct interaction with the environment is costly or unsafe. Existing OPE methods are ineffective for high-dimensional, long-horizon problems, due to exponential blow-ups in variance from importance weighting or compounding errors from learned dynamics models. To address these challenges, we propose STITCH-OPE, a model-based generative framework that leverages denoising diffusion for long-horizon OPE in high-dimensional state and action spaces. Starting with a diffusion model pre-trained on the behavior data, STITCH-OPE generates synthetic trajectories from the target policy by guiding the denoising process using the score function of the target policy. STITCH-OPE proposes two technical innovations that make it advantageous for OPE: (1) prevents over-regularization by subtracting the score of the behavior policy during guidance, and (2) generates long-horizon trajectories by stitching partial trajectories together end-to-end. We provide a theoretical guarantee that under mild assumptions, these modifications result in an exponential reduction in variance versus long-horizon trajectory diffusion. Experiments on the D4RL and OpenAI Gym benchmarks show substantial improvement in mean squared error, correlation, and regret metrics compared to state-of-the-art OPE methods",
    "checked": true,
    "id": "a29b2144c7ad08a17322900f21ece1b9cecba098",
    "semantic_title": "stitch-ope: trajectory stitching with guided diffusion for off-policy evaluation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=pkuVonMwhT": {
    "title": "Neural Atlas Graphs for Dynamic Scene Decomposition and Editing",
    "volume": "spotlight",
    "abstract": "Learning editable high-resolution scene representations for dynamic scenes is an open problem with applications across the domains from autonomous driving to creative editing - the most successful approaches today make a trade-off between editability and supporting scene complexity: neural atlases represent dynamic scenes as two deforming image layers, foreground and background, which are editable in 2D, but break down when multiple objects occlude and interact. In contrast, scene graph models make use of annotated data such as masks and bounding boxes from autonomous-driving datasets to capture complex 3D spatial relationships, but their implicit volumetric node representations are challenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a hybrid high-resolution scene representation, where every graph node is a view-dependent neural atlas, facilitating both 2D appearance editing and 3D ordering and positioning of scene elements. Fit at test-time, NAGs achieve state-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR increase compared to existing methods - and make environmental editing possible in high resolution and visual quality - creating counterfactual driving scenarios with new backgrounds and edited vehicle appearance. We find that the method also generalizes beyond driving scenes and compares favorably - by more than 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS video dataset with a diverse set of human and animal-centric scenes. Project Page: https://princeton-computational-imaging.github.io/nag/",
    "checked": true,
    "id": "eb268780d72cd89e9f9517ff4e654816518891b4",
    "semantic_title": "neural atlas graphs for dynamic scene decomposition and editing",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=QIXdI207nq": {
    "title": "LeMiCa: Lexicographic Minimax Path Caching for Efficient Diffusion-Based Video Generation",
    "volume": "spotlight",
    "abstract": "We present LeMiCa, a training-free and efficient acceleration framework for diffusion-based video generation. While existing caching strategies primarily focus on reducing local heuristic errors, they often overlook the accumulation of global errors, leading to noticeable content degradation between accelerated and original videos. To address this issue, we formulate cache scheduling as a directed graph with error-weighted edges and introduce a Lexicographic Minimax Path Optimization strategy that explicitly bounds the worst-case path error. This approach substantially improves the consistency of global content and style across generated frames. Extensive experiments on multiple text-to-video benchmarks demonstrate that LeMiCa delivers dual improvements in both inference speed and generation quality. Notably, our method achieves a 2.9× speedup on the Latte model and reaches an LPIPS score of 0.05 on Open-Sora, outperforming prior caching techniques. Importantly, these gains come with minimal perceptual quality degradation, making LeMiCa a robust and generalizable paradigm for accelerating diffusion-based video generation. We believe this approach can serve as a strong foundation for future research on efficient and reliable video synthesis",
    "checked": true,
    "id": "00144beb6600d456d3de7eedc87a5b550e942e92",
    "semantic_title": "lemica: lexicographic minimax path caching for efficient diffusion-based video generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rOR5IZcwJx": {
    "title": "Robust SuperAlignment: Weak-to-Strong Robustness Generalization for Vision-Language Models",
    "volume": "spotlight",
    "abstract": "Numerous well-established studies have demonstrated the superhuman capabilities of modern Vision-Language Models (VLMs) across a wide range of tasks. However, growing is the doubt about the continuing availability of reliable high-quality labeling (supervision) from human annotators, leading to stagnation of the model's performance. To address this challenge, ``superalignment'' employs the so-called weak-to-strong generalization paradigm, where the supervision from a weak model can provide generalizable knowledge for a strong model. While effective in aligning knowledge for clean samples between the strong and weak models, the standard weak-to-strong approach typically fails to capture adversarial robustness, exposing strong VLMs to adversarial attacks. This inability to transfer adversarial robustness is because adversarial samples are normally missing in the superalignment stage. To this end, we are the first to propose the weak-to-strong (adversarial) robustness generalization method to elicit zero-shot robustness in large-scale models by an unsupervised scheme, mitigating the unreliable information source for alignment from two perspectives: alignment re-weighting and source guidance refinement. We analyze settings under which robustness generalization is possible. Extensive experiments across various vision-language benchmarks validate the effectiveness of our method in numerous scenarios, demonstrating its plug-and-play applicability to large-scale VLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tq9lyV9Cml": {
    "title": "Thought Communication in Multiagent Collaboration",
    "volume": "spotlight",
    "abstract": "Natural language has long enabled human cooperation, but its lossy, ambiguous, and indirect nature limits the potential of collective intelligence. While machines are not subject to these constraints, most LLM-based multi-agent systems still rely solely on natural language, exchanging tokens or their embeddings. To go beyond language, we introduce a new paradigm, *thought communication*, which enables agents to interact directly mind-to-mind, akin to telepathy. To uncover these latent thoughts in a principled way, we formalize the process as a general latent variable model, where agent states are generated by an unknown function of underlying thoughts. We prove that, in a nonparametric setting without auxiliary information, both shared and private latent thoughts between any pair of agents can be identified. Moreover, the global structure of thought sharing, including which agents share which thoughts and how these relationships are structured, can also be recovered with theoretical guarantees. Guided by the established theory, we develop a framework that extracts latent thoughts from all agents prior to communication and assigns each agent the relevant thoughts, along with their sharing patterns. This paradigm naturally extends beyond LLMs to all modalities, as most observational data arise from hidden generative processes. Experiments on both synthetic and real-world benchmarks validate the theory and demonstrate the collaborative advantages of thought communication. We hope this work illuminates the potential of leveraging the hidden world, as many challenges remain unsolvable through surface-level observation alone, regardless of compute or data scale",
    "checked": true,
    "id": "3ebf6fed91c3577c4f7dfe77f6f0444fb923f85a",
    "semantic_title": "thought communication in multiagent collaboration",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EkoAKNikAj": {
    "title": "Polyline Path Masked Attention for Vision Transformer",
    "volume": "spotlight",
    "abstract": "Global dependency modeling and spatial position modeling are two core issues of the foundational architecture design in current deep learning frameworks. Recently, Vision Transformers (ViTs) have achieved remarkable success in computer vision, leveraging the powerful global dependency modeling capability of the self-attention mechanism. Furthermore, Mamba2 has demonstrated its significant potential in natural language processing tasks by explicitly modeling the spatial adjacency prior through the structured mask. In this paper, we propose Polyline Path Masked Attention (PPMA) that integrates the self-attention mechanism of ViTs with an enhanced structured mask of Mamba2, harnessing the complementary strengths of both architectures. Specifically, we first ameliorate the traditional structured mask of Mamba2 by introducing a 2D polyline path scanning strategy and derive its corresponding structured mask, polyline path mask, which better preserves the adjacency relationships among image tokens. Notably, we conduct a thorough theoretical analysis on the structural characteristics of the proposed polyline path mask and design an efficient algorithm for the computation of the polyline path mask. Next, we embed the polyline path mask into the self-attention mechanism of ViTs, enabling explicit modeling of spatial adjacency prior. Extensive experiments on standard benchmarks, including image classification, object detection, and segmentation, demonstrate that our model outperforms previous state-of-the-art approaches based on both state-space models and Transformers. For example, our proposed PPMA-T/S/B models achieve 48.7%/51.1%/52.3% mIoU on the ADE20K semantic segmentation task, surpassing RMT-T/S/B by 0.7%/1.3%/0.3%, respectively. Code is available at https://github.com/zhongchenzhao/PPMA",
    "checked": true,
    "id": "9ee20eba108af74cf45bd86c6803085329053616",
    "semantic_title": "polyline path masked attention for vision transformer",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SJPq1xBPHX": {
    "title": "PLMTrajRec: A Scalable and Generalizable Trajectory Recovery Method with Pre-trained Language Models",
    "volume": "spotlight",
    "abstract": "Spatiotemporal trajectory data is crucial for various traffic-related applications. However, issues such as device malfunctions and network instability often result in sparse trajectories that lose detailed movement information compared to their dense counterparts. Recovering missing points in sparse trajectories is thus essential. Despite recent progress, three challenges remain. First, the lack of large-scale dense trajectory datasets hinders the training of a trajectory recovery model. Second, the varying spatiotemporal correlations in sparse trajectories make it hard to generalize across different sampling intervals. Third, extracting road conditions for missing points is non-trivial. To address these challenges, we propose $\\textit{PLMTrajRec}$, a novel trajectory recovery model. It leverages the scalability of a pre-trained language model (PLM) and can effectively recover trajectories by fine-tuning with small-scale dense trajectory datasets. To handle different sampling intervals in sparse trajectories, we first convert sampling intervals and movement features into prompts for the PLM to understand. We then introduce a trajectory encoder to unify trajectories of varying intervals into a single interval. To extract road conditions for missing points, we propose an area flow-guided implicit trajectory prompt that represents traffic conditions in each region, and a road condition passing mechanism that infers the road conditions of missing points using the observed ones. Experiments on four public trajectory datasets with three sampling intervals demonstrate the effectiveness, scalability, and generalization ability of PLMTrajRec. Code is available at https://github.com/wtl52656/PLMTrajRec",
    "checked": true,
    "id": "c1adff937ed9a1d973ef25cdd2812831232383d6",
    "semantic_title": "plmtrajrec: a scalable and generalizable trajectory recovery method with pre-trained language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rZ2nSt1X58": {
    "title": "Optimization Inspired Few-Shot Adaptation for Large Language Models",
    "volume": "spotlight",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in real-world applications. However, adapting LLMs to novel tasks via fine-tuning often requires substantial training data and computational resources that are impractical in few-shot scenarios. Existing approaches, such as In-context learning and Parameter-Efficient Fine-Tuning (PEFT), face key limitations: In-context learning introduces additional inference computational overhead with limited performance gains, while PEFT models are prone to overfitting on the few demonstration examples. In this work, we reinterpret the forward pass of LLMs as an optimization process, a sequence of preconditioned gradient descent steps refining internal representations. Based on this connection, we propose Optimization-Inspired Few-Shot Adaptation (OFA), integrating a parameterization that learns preconditioners without introducing additional trainable parameters, and an objective that improves optimization efficiency by learning preconditioners based on a convergence bound, while simultaneously steering the optimization path toward the flat local minimum. Our method overcomes both issues of ICL-based and PEFT-based methods, and demonstrates superior performance over the existing methods on a variety of few-shot adaptation tasks in experiments",
    "checked": false,
    "id": "7ac65fd681ccb3bce33d682ed99aa7ff9fe95f31",
    "semantic_title": "optimization-inspired few-shot adaptation for large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=26kUrQm4zw": {
    "title": "Scent of Knowledge: Optimizing Search-Enhanced Reasoning with Information Foraging",
    "volume": "spotlight",
    "abstract": "Augmenting large language models (LLMs) with external retrieval has become a standard method to address their inherent knowledge cutoff limitations. However, traditional retrieval-augmented generation methods employ static, pre-inference retrieval strategies, making them inadequate for complex tasks involving ambiguous, multi-step, or evolving information needs. Recent advances in test-time scaling techniques have demonstrated significant potential in enabling LLMs to dynamically interact with external tools, motivating the shift toward adaptive inference-time retrieval. Inspired by Information Foraging Theory (IFT), we propose InForage, a reinforcement learning framework that formalizes retrieval-augmented reasoning as a dynamic information-seeking process. Unlike existing approaches, InForage explicitly rewards intermediate retrieval quality, encouraging LLMs to iteratively gather and integrate information through adaptive search behaviors. To facilitate training, we construct a human-guided dataset capturing iterative search and reasoning trajectories for complex, real-world web tasks. Extensive evaluations across general question answering, multi-hop reasoning tasks, and a newly developed real-time web QA dataset demonstrate InForage's superior performance over baseline methods. These results highlight InForage's effectiveness in building robust, adaptive, and efficient reasoning agents. We provide all codes and datasets in the supplementary materials",
    "checked": true,
    "id": "c778fad9acef0847c4f4e0f4ac033b7c4f1c217b",
    "semantic_title": "scent of knowledge: optimizing search-enhanced reasoning with information foraging",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=PtGMadeONU": {
    "title": "Taccel: Scaling Up Vision-based Tactile Robotics via High-performance GPU Simulation",
    "volume": "spotlight",
    "abstract": "Tactile sensing is crucial for achieving human-level robotic capabilities in manipulation tasks. As a promising solution, Vision-based Tactile Sensors (VBTSs) offer high spatial resolution and cost-effectiveness, but present unique challenges in robotics for their complex physical characteristics and visual signal processing requirements. The lack of efficient and accurate simulation tools for VBTSs has significantly limited the scale and scope of tactile robotics research. We present Taccel, a high-performance simulation platform that integrates Incremental Potential Contact (IPC) and Affine Body Dynamics (ABD) to model robots, tactile sensors, and objects with both accuracy and unprecedented speed, achieving a total of 915 FPS with 4096 parallel environments. Unlike previous simulators that operate at sub-real-time speeds with limited parallelization, Taccel provides precise physics simulation and realistic tactile signals while supporting flexible robot-sensor configurations through user-friendly APIs. Through extensive validation in object recognition, robotic grasping, and articulated object manipulation, we demonstrate precise simulation and successful sim-to-real transfer. These capabilities position Taccel as a powerful tool for scaling up tactile robotics research and development, potentially transforming how robots interact with and understand their physical environment",
    "checked": true,
    "id": "42d81c1f54bec8efea9100d1a4e88e7b59381bdb",
    "semantic_title": "taccel: scaling up vision-based tactile robotics via high-performance gpu simulation",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=MDWJlTWZHH": {
    "title": "Toward Relative Positional Encoding in Spiking Transformers",
    "volume": "spotlight",
    "abstract": "Spiking neural networks (SNNs) are bio-inspired networks that mimic how neurons in the brain communicate through discrete spikes, which have great potential in various tasks due to their energy efficiency and temporal processing capabilities. SNNs with self-attention mechanisms (spiking Transformers) have recently shown great advancements in various tasks, and inspired by traditional Transformers, several studies have demonstrated that spiking absolute positional encoding can help capture sequential relationships for input data, enhancing the capabilities of spiking Transformers for tasks such as sequential modeling and image classification. However, how to incorporate relative positional information into SNNs remains a challenge. In this paper, we introduce several strategies to approximate relative positional encoding (RPE) in spiking Transformers while preserving the binary nature of spikes. Firstly, we formally prove that encoding relative distances with Gray Code ensures that the binary representations of positional indices maintain a constant Hamming distance whenever their decimal values differ by a power of two, and we propose **Gray-PE** based on this property. In addition, we propose another RPE method called **Log-PE**, which combines the logarithmic form of the relative distance matrix directly into the spiking attention map. Furthermore, we extend our RPE methods to a two-dimensional form, making them suitable for processing image patches. We evaluate our RPE methods on various tasks, including time series forecasting, text classification, and patch-based image classification, and the experimental results demonstrate a satisfying performance gain by incorporating our RPE methods across many architectures. Our results provide fresh perspectives on designing spiking Transformers to advance their sequential modeling capability, thereby expanding their applicability across various domains. Our code is available at https://github.com/microsoft/SeqSNN",
    "checked": true,
    "id": "cc010bb76e32cb6bd0b61f0d809c724829b8bf75",
    "semantic_title": "toward relative positional encoding in spiking transformers",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=TcVCu2PKb9": {
    "title": "TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility and Speedup",
    "volume": "spotlight",
    "abstract": "Modern large-language models often face communication bottlenecks on current hardware rather than computational limitations. *Multi-head latent attention (MLA)* addresses this by compressing the key-value cache using low-rank matrices, while the Absorb operation prevents the KV cache from reverting to its original size, significantly boosting both training and inference speed. Despite the success of DeepSeek V2/V3/R1, most model providers have heavily invested in optimizing GQA-based models and, therefore, lack strong incentives to retrain MLA-based models from scratch. This paper demonstrates that MLA provides superior expressive power compared to GQA with the same KV cache overhead, thereby offering a rationale for transitioning from GQA to MLA. In addition, we introduce TransMLA, a framework that seamlessly converts any GQA-based pre-trained model (e.g., LLaMA, Qwen, Gemma, Mistral/Mixtral) into an MLA-based model. For the first time, our method enables *direct conversion of these models into a format compatible with DeepSeek's codebase*, allowing them to fully leverage the existing, highly-optimized support for the DeepSeek architecture within inference engines like vLLM and SGlang. By compressing 93\\% of the KV cache in LLaMA-2-7B, we achieve a **10x speedup** with an 8K context length while maintaining meaningful output. Moreover, the model requires only **6B tokens** for fine-tuning to recover comparable performance across multiple benchmarks. TransMLA provides a practical path for migrating GQA-based models to the MLA structure, and when combined with DeepSeek's advanced optimizations—such as FP8 quantization and Multi-Token Prediction—further inference acceleration can be achieved",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bRAm7A02Qm": {
    "title": "Towards Understanding the Mechanisms of Classifier-Free Guidance",
    "volume": "spotlight",
    "abstract": "Classifier-free guidance (CFG) is a core technique powering state-of-the-art image generation systems, yet its underlying mechanisms remain poorly understood. In this work, we first analyze CFG in a simplified linear diffusion model, where we show its behavior closely resembles that observed in the nonlinear case. Our analysis reveals that linear CFG improves generation quality via three distinct components: (i) a mean-shift term that approximately steers samples in the direction of class means, (ii) a positive Contrastive Principal Components (CPC) term that amplifies class-specific features, and (iii) a negative CPC term that suppresses generic features prevalent in unconditional data. We then verify that these insights in real-world, nonlinear diffusion models: over a broad range of noise levels, linear CFG resembles the behavior of its nonlinear counterpart. Although the two eventually diverge at low noise levels, we discuss how the insights from the linear analysis still shed light on the CFG's mechanism within the nonlinear regime",
    "checked": true,
    "id": "8d4d5bd7a1af9ed578798701eca72f04d05af278",
    "semantic_title": "towards understanding the mechanisms of classifier-free guidance",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=hLJLP3CmHR": {
    "title": "PhysX-3D: Physical-Grounded 3D Asset Generation",
    "volume": "spotlight",
    "abstract": "3D modeling is moving from virtual to physical. Existing 3D generation primarily emphasizes geometries and textures while neglecting physical-grounded modeling. Consequently, despite the rapid development of 3D generative models, the synthesized 3D assets often overlook rich and important physical properties, hampering their real-world application in physical domains like simulation and embodied AI. As an initial attempt to address this challenge, we propose \\textbf{PhysX}, an end-to-end paradigm for physical-grounded 3D asset generation. \\textbf{1)} To bridge the critical gap in physics-annotated 3D datasets, we present \\textbf{\\ourname}\\ - the first physics-grounded 3D dataset systematically annotated across five foundational dimensions: \\textbf{\\textcolor{color2}{absolute scale}}, \\textbf{\\textcolor{color3}{material}}, \\textbf{\\textcolor{color1}{affordance}}, \\textbf{\\textcolor{color4}{kinematics}}, and \\textbf{\\textcolor{color5}{function description}}. In particular, we devise a scalable human-in-the-loop annotation pipeline based on vision-language models, which enables efficient creation of physics-first assets from raw 3D assets. \\textbf{2)} Furthermore, we propose \\textbf{PhysXGen}, a feed-forward framework for physics-grounded 3D asset generation, injecting physical knowledge into the pre-trained 3D structural space. Specifically, PhysXGen employs a dual-branch architecture to explicitly model the latent correlations between 3D structures and physical properties, thereby producing 3D assets with plausible physical predictions while preserving the native geometry quality. Extensive experiments validate the superior performance and promising generalization capability of our framework. All the code, data, and models will be released to facilitate future research in generative physical AI",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pEUBqS8nTk": {
    "title": "Towards Physics-informed Spatial Intelligence with Human Priors: An Autonomous Driving Pilot Study",
    "volume": "spotlight",
    "abstract": "How to integrate and verify spatial intelligence in foundation models remains an open challenge. Current practice often proxies Visual-Spatial Intelligence (VSI) with purely textual prompts and VQA-style scoring, which obscures geometry, invites linguistic shortcuts, and weakens attribution to genuinely spatial skills. We introduce Spatial Intelligence Grid (SIG): a structured, grid-based schema that explicitly encodes object layouts, inter-object relations, and physically grounded priors. As a complementary channel to text, SIG provides a faithful, compositional representation of scene structure for foundation-model reasoning. Building on SIG, we derive SIG-informed evaluation metrics that quantify a model's intrinsic VSI, which separates spatial capability from language priors. In few-shot in-context learning with state-of-the-art multimodal LLMs (e.g. GPT- and Gemini-family models), SIG yields consistently larger, more stable, and more comprehensive gains across all VSI metrics compared to VQA-only representations, indicating its promise as a data-labeling and training schema for learning VSI. We also release SIGBench, a benchmark of 1.4K driving frames annotated with ground-truth SIG labels and human gaze traces, supporting both grid-based machine VSI tasks and attention-driven, human-like VSI tasks in autonomous-driving scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9loSPaBwGO": {
    "title": "StreamForest: Efficient Online Video Understanding with Persistent Event Memory",
    "volume": "spotlight",
    "abstract": "Multimodal Large Language Models (MLLMs) have recently achieved remarkable progress in video understanding. However, their effectiveness in real-time streaming scenarios remains limited due to storage constraints of historical visual features and insufficient real-time spatiotemporal reasoning. To address these challenges, we propose StreamForest, a novel architecture specifically designed for streaming video understanding. Central to StreamForest is the Persistent Event Memory Forest, a memory mechanism that adaptively organizes video frames into multiple event-level tree structures. This process is guided by penalty functions based on temporal distance, content similarity, and merge frequency, enabling efficient long-term memory retention under limited computational resources. To enhance real-time perception, we introduce a Fine-grained Spatiotemporal Window, which captures detailed short-term visual cues to improve current scene perception. Additionally, we present OnlineIT, an instruction-tuning dataset tailored for streaming video tasks. OnlineIT significantly boosts MLLM performance in both real-time perception and future prediction. To evaluate generalization in practical applications, we introduce ODV-Bench, a new benchmark focused on real-time streaming video understanding in autonomous driving scenarios. Experimental results demonstrate that StreamForest achieves the state-of-the-art performance, with accuracies of 77.3% on StreamingBench, 60.5% on OVBench, and 55.6% on OVO-Bench. In particular, even under extreme visual token compression (limited to 1024 tokens), the model retains 96.8% of its average accuracy in eight benchmarks relative to the default setting. These results underscore the robustness, efficiency, and generalizability of StreamForest for streaming video understanding",
    "checked": true,
    "id": "336c9100566746b564761ea08d44ec10efdcd957",
    "semantic_title": "streamforest: efficient online video understanding with persistent event memory",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Ll29PmM3UH": {
    "title": "Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2",
    "volume": "spotlight",
    "abstract": "Recent studies reveal the vulnerability of the image segmentation foundation model SAM to adversarial examples. Its successor, SAM2, has attracted significant attention due to its strong generalization capability in video segmentation. However, its robustness remains unexplored, and it is unclear whether existing attacks on SAM can be directly transferred to SAM2. In this paper, we first analyze the performance gap of existing attacks between SAM and SAM2 and highlight two key challenges arising from their architectural differences: directional guidance from the prompt and semantic entanglement across consecutive frames. To address these issues, we propose UAP-SAM2, the first cross-prompt universal adversarial attack against SAM2 driven by dual semantic deviation. For cross-prompt transferability, we begin by designing a target-scanning strategy that divides each frame into k regions, each randomly assigned a prompt, to reduce prompt dependency during optimization. For effectiveness, we design a dual semantic deviation framework that optimizes a UAP by distorting the semantics within the current frame and disrupting the semantic consistency across consecutive frames. Extensive experiments on six datasets across two segmentation tasks demonstrate the effectiveness of the proposed method for SAM2. The comparative results show that UAP-SAM2 significantly outperforms state-of-the-art (SOTA) attacks by a large margin",
    "checked": true,
    "id": "876aecb484e519204d120479e6a6166f9dc33943",
    "semantic_title": "vanish into thin air: cross-prompt universal adversarial attacks for sam2",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=xgTxQe3CNl": {
    "title": "Evolutionary Multi-View Classification via Eliminating Individual Fitness Bias",
    "volume": "spotlight",
    "abstract": "Evolutionary multi-view classification (EMVC) methods have gained wide recognition due to their adaptive mechanisms. Fitness evaluation (FE), which aims to calculate the classification performance of each individual in the population and provide reliable performance ranking for subsequent operations, is a core step in such methods. Its accuracy directly determines the correctness of the evolutionary direction. However, when FE fails to correctly reflect the superiority-inferiority relationship among individuals, it will lead to confusion in individual performance ranking, which in turn misleads the evolutionary direction and results in trapping into local optima. This paper is the first to identify the aforementioned issue in the field of EMVC and call it as fitness evaluation bias (FEB). FEB may be caused by a variety of factors, and this paper approaches the issue from the perspective of view information content: existing methods generally adopt joint training strategies, which restrict the exploration of key information in views with low information content. This makes it difficult for multi-view model (MVM) to achieve optimal performance during convergence, which in turn leads to FE failing to accurately reflect individual performance rankings and ultimately triggering FEB. To address this issue, we propose an evolutionary multi-view classification via eliminating individual fitness bias (EFB-EMVC) method, which alleviates the FEB issue by introducing evolutionary navigators for each MVM, thereby providing more accurate individual ranking. Experimental results fully verify the effectiveness of the proposed method in alleviating the FEB problem, and the EMVC method equipped with this strategy exhibits more superior performance compared with the original EMVC method. (The code is available at https://github.com/LiShuailzn/Neurips-2025-EFB-EMVC)",
    "checked": false,
    "id": "db214b8f53ec04c2ad96ed04de9562baa11d6560",
    "semantic_title": "multi-population ensemble genetic programming via cooperative coevolution and multi-view learning for classification",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=k8Mim6RI5O": {
    "title": "Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization",
    "volume": "spotlight",
    "abstract": "Existing methods to enhance the reasoning capability of large language models predominantly rely on supervised fine-tuning (SFT) followed by reinforcement learning (RL) on reasoning-specific data. These approaches critically depend on external supervisions--such as labeled reasoning traces, verified golden answers, or pre-trained reward models. In this work, we propose Entropy Minimized Policy Optimization (EMPO), which makes an early attempt at fully unsupervised LLM reasoning incentivization. By minimizing the semantic entropy of LLMs on unlabeled questions, EMPO achieves competitive performance compared to supervised counterparts. Specifically, without any supervised signals, EMPO boosts the accuracy of Qwen2.5-Math-7B Base from 33.7\\% to 51.6\\% on math benchmarks and improves the accuracy of Qwen2.5-7B Base from 32.1\\% to 50.1\\% on MMLU-Pro. Primary analysis are also provided to interpret the effectiveness of EMPO. Code is available at https://github.com/QingyangZhang/EMPO",
    "checked": true,
    "id": "1738bfdda961ddc9d31b39f3dd38b1f425199e94",
    "semantic_title": "right question is already half the answer: fully unsupervised llm reasoning incentivization",
    "citation_count": 56,
    "authors": []
  },
  "https://openreview.net/forum?id=7jg26Fd1ra": {
    "title": "MDReID: Modality-Decoupled Learning for Any-to-Any Multi-Modal Object Re-Identification",
    "volume": "spotlight",
    "abstract": "The challenge of inconsistent modalities in real-world applications presents significant obstacles to effective object re-identification (ReID). However, most existing approaches assume modality-matched conditions, significantly limiting their effectiveness in modality-mismatched scenarios. To overcome this limitation and achieve a more flexible ReID, we introduce MDReID to allow any-to-any image-level ReID systems. MDReID is inspired by the widely recognized perspective that modality information comprises both modality-shared features, predictable across modalities, and unpredictable modality-specific features, which are inherently modality-dependent and consist of two key components: the Modality Decoupling Module (MDM) and Modality-aware Metric Learning (MML). Specifically, MDM explicitly decomposes modality features into modality-shared and modality-specific representations, enabling effective retrieval in both modality-aligned and mismatched scenarios. MML, a tailored metric learning strategy, further enhances feature discrimination and decoupling by exploiting distributional relationships between shared and specific modality features. Extensive experiments conducted on three challenging multi-modality ReID benchmarks (RGBNT201, RGBNT100, MSVR310) consistently demonstrate the superiority of MDL. MDReID achieves significant mAP improvements of 9.8\\%, 3.0\\%, and 11.5\\% in modality-matched scenarios, and average gains of 3.4\\%, 11.8\\%, and 10.9\\% in modality-mismatched scenarios, respectively",
    "checked": true,
    "id": "6f1fef230bfbe9a9024d53cfe0e693537fbb0a02",
    "semantic_title": "mdreid: modality-decoupled learning for any-to-any multi-modal object re-identification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U7RZ9cC73S": {
    "title": "RobustMerge: Parameter-Efficient Model Merging for MLLMs with Direction Robustness",
    "volume": "spotlight",
    "abstract": "Fine-tuning pre-trained models with custom data leads to numerous expert models on specific tasks. Merging models into one universal model to empower multi-task ability refraining from data leakage has gained popularity. With the expansion in data and model size, parameter-efficient tuning becomes the common practice for obtaining task-specific models efficiently. However, few methods are dedicated to efficient merging, and existing methods designed for full fine-tuning merging fail under efficient merging. To address the issue, we analyze from low-rank decomposition and reveal that direction robustness during merging is crucial for merging efficient modules. We furthermore uncover that compensating for the gap between stark singular values contributes to direction robustness. Therefore, we propose RobustMerge, a training-free parameter-efficient merging method with complementary parameter adaptation to maintain direction robustness. Specifically, we (1) prune parameters and scale coefficients from inter-parameter relations for singular values to maintain direction stability away from task interference, and (2) perform cross-task normalization to enhance unseen task generalization. We establish a benchmark consisting of diverse multimodal tasks, on which we conduct experiments to certify the outstanding performance and generalizability of our method. Additional studies and extensive analyses further showcase the effectiveness",
    "checked": true,
    "id": "031ce437fff40083db430b57695ae56609dce022",
    "semantic_title": "robustmerge: parameter-efficient model merging for mllms with direction robustness",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=I9F53Qlwur": {
    "title": "Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation",
    "volume": "spotlight",
    "abstract": "We present Stable Part Diffusion 4D (SP4D), a framework for generating paired RGB and kinematic part videos from monocular inputs. Unlike conventional part segmentation methods that rely on appearance-based semantic cues, SP4D learns to produce kinematic parts --- structural components aligned with object articulation and consistent across views and time. SP4D adopts a dual-branch diffusion model that jointly synthesizes RGB frames and corresponding part segmentation maps. To simplify architecture and flexibly enable different part counts, we introduce a spatial color encoding scheme that maps part masks to continuous RGB-like images. This encoding allows the segmentation branch to share the latents VAE from the RGB branch, while enabling part segmentation to be recovered via straightforward post-processing. A Bidirectional Diffusion Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a contrastive part consistency loss to promote spatial and temporal alignment of part predictions. We demonstrate that the generated 2D part maps can be lifted to 3D to derive skeletal structures and harmonic skinning weights with few manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K, a curated dataset of over 20K rigged objects selected and processed from Objaverse XL, each paired with multi-view RGB and part video sequences. Experiments show that SP4D generalizes strongly to diverse scenarios, including real-world videos, novel generated objects, and rare articulated poses, producing kinematic-aware outputs suitable for downstream animation and motion-related tasks",
    "checked": true,
    "id": "f21b19164e23128d286415ff13204c1cf4ba9942",
    "semantic_title": "stable part diffusion 4d: multi-view rgb and kinematic parts video generation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Y9AdTCCEgI": {
    "title": "OnlineSplatter: Pose-Free Online 3D Reconstruction for Free-Moving Objects",
    "volume": "spotlight",
    "abstract": "Free-moving object reconstruction from monocular video remains challenging, particularly without reliable pose or depth cues and under arbitrary object motion. We introduce OnlineSplatter, a novel online feed-forward framework generating high-quality, object-centric 3D Gaussians directly from RGB frames without requiring camera pose, depth priors, or bundle optimization. Our approach anchors reconstruction using the first frame and progressively refines the object representation through a dense Gaussian primitive field, maintaining constant computational cost regardless of video sequence length. Our core contribution is a dual-key memory module combining latent appearance-geometry keys with explicit directional keys, robustly fusing current frame features with temporally aggregated object states. This design enables effective handling of free-moving objects via spatial-guided memory readout and an efficient sparsification mechanism, ensuring comprehensive yet compact object coverage. Evaluations on real-world datasets demonstrate that OnlineSplatter significantly outperforms state-of-the-art pose-free reconstruction baselines, consistently improving with more observations while maintaining constant memory and runtime",
    "checked": true,
    "id": "676fd82cdf9bb7fa6f53f67b489d88a827c94c67",
    "semantic_title": "onlinesplatter: pose-free online 3d reconstruction for free-moving objects",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bNTezDPlFH": {
    "title": "Rectified Point Flow: Generic Point Cloud Pose Estimation",
    "volume": "spotlight",
    "abstract": "We present Rectified Point Flow, a unified parameterization that formulates pairwise point cloud registration and multi-part shape assembly as a single conditional generative problem. Given unposed point clouds, our method learns a continuous point-wise velocity field that transports noisy points toward their target positions, from which part poses are recovered. In contrast to prior work that regresses part-wise poses with ad-hoc symmetry handling, our method intrinsically learns assembly symmetries without symmetry labels. Together with an overlap-aware encoder focused on inter-part contacts, Rectified Point Flow achieves a new state-of-the-art performance on six benchmarks spanning pairwise registration and shape assembly. Notably, our unified formulation enables effective joint training on diverse datasets, facilitating the learning of shared geometric priors and consequently boosting accuracy. Our code and models are available at https://rectified-pointflow.github.io/",
    "checked": true,
    "id": "21b6dad36b5f73f865e7b2683be3e6f99c0e5bc8",
    "semantic_title": "rectified point flow: generic point cloud pose estimation",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=8a9bAZFeIu": {
    "title": "Cue3D: Quantifying the Role of Image Cues in Single-Image 3D Generation",
    "volume": "spotlight",
    "abstract": "Humans and traditional computer vision methods rely on a diverse set of monocular cues to infer 3D structure from a single image, such as shading, texture, silhouette, etc. While recent deep generative models have dramatically advanced single-image 3D generation, it remains unclear which image cues these methods actually exploit. We introduce Cue3D, the first comprehensive, model-agnostic framework for quantifying the influence of individual image cues in single-image 3D generation. Our unified benchmark evaluates seven state-of-the-art methods, spanning regression-based, multi-view, and native 3D generative paradigms. By systematically perturbing cues such as shading, texture, silhouette, perspective, edges, and local continuity, we measure their impact on 3D output quality. Our analysis reveals that shape meaningfulness, not texture, dictates generalization. Geometric cues, particularly shading, are crucial for 3D generation. We further identify over-reliance on provided silhouettes and diverse sensitivities to cues such as perspective and local continuity across model families. By dissecting these dependencies, Cue3D advances our understanding of how modern 3D networks leverage classical vision cues, and offers directions for developing more transparent, robust, and controllable single-image 3D generation models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yFasd68NyI": {
    "title": "SceneDesigner: Controllable Multi-Object Image Generation with 9-DoF Pose Manipulation",
    "volume": "spotlight",
    "abstract": "Controllable image generation has attracted increasing attention in recent years, enabling users to manipulate visual content such as identity and style. However, achieving simultaneous control over the 9D poses (location, size, and orientation) of multiple objects remains an open challenge. Despite recent progress, existing methods often suffer from limited controllability and degraded quality, falling short of comprehensive multi-object 9D pose control. To address these limitations, we propose ***SceneDesigner***, a method for accurate and flexible multi-object 9-DoF pose manipulation. ***SceneDesigner*** incorporates a branched network to the pre-trained base model and leverages a new representation, ***CNOCS map***, which encodes 9D pose information from the camera view. This representation exhibits strong geometric interpretation properties, leading to more efficient and stable training. To support training, we construct a new dataset, ***ObjectPose9D***, which aggregates images from diverse sources along with 9D pose annotations. To further address data imbalance issues, particularly performance degradation on low-frequency poses, we introduce a two-stage training strategy with reinforcement learning, where the second stage fine-tunes the model using a reward-based objective on rebalanced data. At inference time, we propose ***Disentangled Object Sampling***, a technique that mitigates insufficient object generation and concept confusion in complex multi-object scenes. Moreover, by integrating user-specific personalization weights, ***SceneDesigner*** enables customized pose control for reference subjects. Extensive qualitative and quantitative experiments demonstrate that ***SceneDesigner*** significantly outperforms existing approaches in both controllability and quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=55Lv1unlUL": {
    "title": "StelLA: Subspace Learning in Low-rank Adaptation using Stiefel Manifold",
    "volume": "spotlight",
    "abstract": "Low-rank adaptation (LoRA) has been widely adopted as a parameter-efficient technique for fine-tuning large-scale pre-trained models. However, it still lags behind full fine-tuning in performance, partly due to its insufficient exploitation of the geometric structure underlying low-rank manifolds. In this paper, we propose a geometry-aware extension of LoRA that uses a three-factor decomposition $USV^\\top$. Analogous to the structure of singular value decomposition (SVD), it separates the adapter's input and output subspaces, $V$ and $U$, from the scaling factor $S$. Our method constrains $U$ and $V$ to lie on the Stiefel manifold, ensuring their orthonormality throughout the training. To optimize on the Stiefel manifold, we employ a flexible and modular geometric optimization design that converts any Euclidean optimizer to a Riemannian one. It enables efficient subspace learning while remaining compatible with existing fine-tuning pipelines. Empirical results across a wide range of downstream tasks, including commonsense reasoning, math and code generation, image classification, and image generation, demonstrate the superior performance of our approach against the recent state-of-the-art variants of LoRA. Code is available at https://github.com/SonyResearch/stella",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gYbreatcV1": {
    "title": "Non-Clairvoyant Scheduling with Progress Bars",
    "volume": "spotlight",
    "abstract": "In non-clairvoyant scheduling, the goal is to minimize the total job completion time without prior knowledge of individual job processing times. This classical online optimization problem has recently gained attention through the framework of learning-augmented algorithms. We introduce a natural setting in which the scheduler receives continuous feedback in the form of progress bars—estimates of the fraction of each job completed over time. We design new algorithms for both adversarial and stochastic progress bars and prove strong competitive bounds. Our results in the adversarial case surprisingly induce improved guarantees for learning-augmented scheduling with job size predictions. We also introduce a general method for combining scheduling algorithms, yielding further insights in scheduling with predictions. Finally, we propose a stochastic model of progress bars as a more optimistic alternative to conventional worst-case models, and present an asymptotically optimal scheduling algorithm in this setting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P37GIj4wB7": {
    "title": "Jacobian-Based Interpretation of Nonlinear Neural Encoding Model",
    "volume": "spotlight",
    "abstract": "In recent years, the alignment between artificial neural network (ANN) embeddings and blood oxygenation level dependent (BOLD) responses in functional magnetic resonance imaging (fMRI) via neural encoding models has significantly advanced research on neural representation mechanisms and interpretability in the brain. However, these approaches remain limited in characterizing the brain's inherently nonlinear response properties. To address this, we propose the Jacobian-based Nonlinearity Evaluation (JNE), an interpretability metric for nonlinear neural encoding models. JNE quantifies nonlinearity by statistically measuring the dispersion of local linear mappings (Jacobians) from model representations to predicted BOLD responses, thereby approximating the nonlinearity of BOLD signals. Centered on proposing JNE as a novel interpretability metric, we validated its effectiveness through controlled simulation experiments on various activation functions and network architectures, and further verified it on real fMRI data, demonstrating a hierarchical progression of nonlinear characteristics from primary to higher-order visual cortices, consistent with established cortical organization. We further extended JNE with Sample-Specificity (JNE-SS), revealing stimulus-selective nonlinear response patterns in functionally specialized brain regions. As the first interpretability metric for quantifying nonlinear responses, JNE provides new insights into brain information processing. Code available at https://github.com/Gaitxh/JNE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=te2RsWcyQp": {
    "title": "Mesh-RFT: Enhancing Mesh Generation via Fine-grained Reinforcement Fine-Tuning",
    "volume": "spotlight",
    "abstract": "Existing pretrained models for 3D mesh generation often suffer from data biases and produce low-quality results, while global reinforcement learning (RL) methods rely on object-level rewards that struggle to capture local structure details. To address these challenges, we present $\\textbf{Mesh-RFT}$, a novel fine-grained reinforcement fine-tuning framework that employs Masked Direct Preference Optimization (M-DPO) to enable localized refinement via quality-aware face masking. To facilitate efficient quality evaluation, we introduce an objective topology-aware scoring system to evaluate geometric integrity and topological regularity at both object and face levels through two metrics: Boundary Edge Ratio (BER) and Topology Score (TS). By integrating these metrics into a fine-grained RL strategy, Mesh-RFT becomes the first method to optimize mesh quality at the granularity of individual faces, resolving localized errors while preserving global coherence. Experiment results show that our M-DPO approach reduces Hausdorff Distance (HD) by 24.6\\% and improves Topology Score (TS) by 3.8\\% over pre-trained models, while outperforming global DPO methods with a 17.4\\% HD reduction and 4.9\\% TS gain. These results demonstrate Mesh-RFT's ability to improve geometric integrity and topological regularity, achieving new state-of-the-art performance in production-ready mesh generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uL7lCOHtiZ": {
    "title": "VisualQuality-R1: Reasoning-Induced Image Quality Assessment via Reinforcement Learning to Rank",
    "volume": "spotlight",
    "abstract": "DeepSeek-R1 has demonstrated remarkable effectiveness in incentivizing reasoning and generalization capabilities of large language models (LLMs) through reinforcement learning. Nevertheless, the potential of reasoning-induced computation has not been thoroughly explored in the context of image quality assessment (IQA), a task depending critically on visual reasoning. In this paper, we introduce VisualQuality-R1, a reasoning-induced no-reference IQA (NR-IQA) model, and we train it with reinforcement learning to rank, a learning algorithm tailored to the intrinsically relative nature of visual quality. Specifically, for a pair of images, we employ group relative policy optimization to generate multiple quality scores for each image. These estimates are used to compute comparative probabilities of one image having higher quality than the other under the Thurstone model. Rewards for each quality estimate are defined using continuous fidelity measures rather than discretized binary labels. Extensive experiments show that the proposed VisualQuality-R1 consistently outperforms discriminative deep learning-based NR-IQA models as well as a recent reasoning-induced quality regression method. Moreover, VisualQuality-R1 is capable of generating contextually rich, human-aligned quality descriptions, and supports multi-dataset training without requiring perceptual scale realignment. These features make VisualQuality-R1 especially well-suited for reliably measuring progress in a wide range of image processing tasks like super-resolution and image generation",
    "checked": true,
    "id": "db17b20925eb3e96ebd40b45abf71ef8eedaf4c3",
    "semantic_title": "visualquality-r1: reasoning-induced image quality assessment via reinforcement learning to rank",
    "citation_count": 15,
    "authors": []
  },
  "https://openreview.net/forum?id=n8AvXKcCeR": {
    "title": "GenColor: Generative and Expressive Color Enhancement with Pixel-Perfect Texture Preservation",
    "volume": "spotlight",
    "abstract": "Color enhancement is a crucial yet challenging task in digital photography. It demands methods that are (i) expressive enough for fine-grained adjustments, (ii) adaptable to diverse inputs, and (iii) able to preserve texture. Existing approaches typically fall short in at least one of these aspects, yielding unsatisfactory results. We propose GenColor, a novel diffusion-based framework for sophisticated, texture-preserving color enhancement. GenColor reframes the task as conditional image generation. Leveraging ControlNet and a tailored training scheme, it learns advanced color transformations that adapt to diverse lighting and content. We train GenColor on ARTISAN, our newly collected large-scale dataset of 1.2M high-quality photographs specifically curated for enhancement tasks. To overcome texture preservation limitations inherent in diffusion models, we introduce a color-transfer network with a novel degradation scheme that simulates texture–color relationships. This network achieves pixel-perfect texture preservation while enabling fine-grained color matching with the diffusion-generated reference images. Extensive experiments show that GenColor produces visually compelling results comparable to those of expert colorists and surpasses state-of-the-art methods in both subjective and objective evaluations. We have released the code and dataset",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mmIAp3cVS0": {
    "title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems",
    "volume": "spotlight",
    "abstract": "Large language model (LLM)-powered multi-agent systems (MAS) have demonstrated cognitive and execution capabilities that far exceed those of single LLM agents, yet their capacity for self-evolution remains hampered by underdeveloped memory architectures. Upon close inspection, we are alarmed to discover that prevailing MAS memory mechanisms (1) are overly simplistic, completely disregarding the nuanced inter-agent collaboration trajectories, and (2) lack cross-trial and agent-specific customization, in stark contrast to the expressive memory developed for single agents. To bridge this gap, we introduce G-Memory, a hierarchical, agentic memory system for MAS inspired by organizational memory theory, which manages the lengthy MAS interaction via a three-tier graph hierarchy: insight, query, and interaction graphs. Upon receiving a new user query, G-Memory performs bi-directional memory traversal to retrieve both \\textit{high-level, generalizable insights} that enable the system to leverage cross-trial knowledge, and \\textit{fine-grained, condensed interaction trajectories} that compactly encode prior collaboration experiences. Upon task execution, the entire hierarchy evolves by assimilating new collaborative trajectories, nurturing the progressive evolution of agent teams. Extensive experiments across five benchmarks, three LLM backbones, and three popular MAS frameworks demonstrate that G-Memory improves success rates in embodied action and accuracy in knowledge QA by up to $20.89\\\\%$ and $10.12\\\\%$, respectively, without any modifications to the original frameworks",
    "checked": true,
    "id": "daa9bcf3ffefdfa8c0f25654aa0512a19ff08157",
    "semantic_title": "g-memory: tracing hierarchical memory for multi-agent systems",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=fCirUh6FRb": {
    "title": "FutureSightDrive: Thinking Visually with Spatio-Temporal CoT for Autonomous Driving",
    "volume": "spotlight",
    "abstract": "Vision–Language–Action (VLA) models are increasingly used for end-to-end driving due to their world knowledge and reasoning ability. Most prior work, however, inserts textual chains-of-thought (CoT) as intermediate steps tailored to the current scene. Such symbolic compressions can blur spatio-temporal relations and discard fine visual cues, creating a cross-modal gap between perception and planning. We propose FSDrive, a visual spatio-temporal CoT framework that enables VLAs to think in images. The model first acts as a world model to generate a unified future frame that overlays coarse but physically-plausible priors—future lane dividers and 3D boxes—on the predicted future image. This unified frame serves as the visual CoT, capturing both spatial structure and temporal evolution. The same VLA then functions as an inverse-dynamics model, planning trajectories from current observations and the visual CoT. To equip VLAs with image generation while preserving understanding, we introduce a unified pre-training paradigm that expands the vocabulary to include visual tokens and jointly optimizes VQA (for semantics) and future-frame prediction (for dynamics). A progressive easy-to-hard scheme first predicts lane/box priors to enforce physical constraints, then completes full future frames for fine details. On nuScenes and NAVSIM, FSDrive improves trajectory accuracy and reduces collisions under both ST-P3 and UniAD metrics, and attains competitive FID for future-frame generation despite using lightweight autoregression. It also advances scene understanding on DriveLM. Together, these results indicate that visual CoT narrows the cross-modal gap and yields safer, more anticipatory planning. Code is available at https://github.com/MIV-XJTU/FSDrive",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ls5L4IMEwt": {
    "title": "E2Former: An Efficient and Equivariant Transformer with Linear-Scaling Tensor Products",
    "volume": "spotlight",
    "abstract": "Equivariant Graph Neural Networks (EGNNs) have demonstrated significant success in modeling microscale systems, including those in chemistry, biology and materials science. However, EGNNs face substantial computational challenges due to the high cost of constructing edge features via spherical tensor products, making them almost impractical for large-scale systems. To address this limitation, we introduce E2Former, an equivariant and efficient transformer architecture that incorporates a Wigner $6j$ convolution (Wigner $6j$ Conv). By shifting the computational burden from edges to nodes, Wigner $6j$ Conv reduces the complexity from $O(| \\mathcal{E}|)$ to $O(| \\mathcal{V}|)$ while preserving both the model's expressive power and rotational equivariance. We show that this approach achieves a 7x–30x speedup compared to conventional $\\mathrm{SO}(3)$ convolutions. Furthermore, our empirical results demonstrate that the derived E2Former mitigates the computational challenges of existing approaches without compromising the ability to capture detailed geometric information. This development could suggest a promising direction for scalable molecular modeling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sm2e1SnMK4": {
    "title": "Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler",
    "volume": "spotlight",
    "abstract": "Harmful fine-tuning poses critical safety risks to fine-tuning-as-a-service for large language models. Existing defense strategies preemptively build robustness via attack simulation but suffer from fundamental limitations: (i) the infeasibility of extending attack simulations beyond bounded threat models due to the inherent difficulty of anticipating unknown attacks, and (ii) limited adaptability to varying attack settings, as simulation fails to capture their variability and complexity. To address these challenges, we propose Bayesian Data Scheduler (BDS), an adaptive tuning-stage defense strategy with no need for attack simulation. BDS formulates harmful fine-tuning defense as a Bayesian inference problem, learning the posterior distribution of each data point's safety attribute, conditioned on the fine-tuning and alignment datasets. The fine-tuning process is then constrained by weighting data with their safety attributes sampled from the posterior, thus mitigating the influence of harmful data. By leveraging the post hoc nature of Bayesian inference, the posterior is conditioned on the fine-tuning dataset, enabling BDS to tailor its defense to the specific dataset, thereby achieving adaptive defense. Furthermore, we introduce a neural scheduler based on amortized Bayesian learning, enabling efficient transfer to new data without retraining. Comprehensive results across diverse attack and defense settings demonstrate the state-of-the-art performance of our approach. Code is available at https://github.com/Egg-Hu/Bayesian-Data-Scheduler",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oIpRvQkrH9": {
    "title": "Neptune-X: Active X-to-Maritime Generation for Universal Maritime Object Detection",
    "volume": "spotlight",
    "abstract": "Maritime object detection is essential for navigation safety, surveillance, and autonomous operations, yet constrained by two key challenges: the scarcity of annotated maritime data and poor generalization across various maritime attributes (e.g., object category, viewpoint, location, and imaging environment). To address these challenges, we propose Neptune-X, a data-centric generative-selection framework that enhances training effectiveness by leveraging synthetic data generation with task-aware sample selection. From the generation perspective, we develop X-to-Maritime, a multi-modality-conditioned generative model that synthesizes diverse and realistic maritime scenes. A key component is the Bidirectional Object-Water Attention module, which captures boundary interactions between objects and their aquatic surroundings to improve visual fidelity. To further improve downstream tasking performance, we propose Attribute-correlated Active Sampling, which dynamically selects synthetic samples based on their task relevance. To support robust benchmarking, we construct the Maritime Generation Dataset, the first dataset tailored for generative maritime learning, encompassing a wide range of semantic conditions. Extensive experiments demonstrate that our approach sets a new benchmark in maritime scene synthesis, significantly improving detection accuracy, particularly in challenging and previously underrepresented settings. The code is available at https://github.com/gy65896/Neptune-X",
    "checked": true,
    "id": "f50357911c1fcceb140fa58cc0b94e16c547f1b5",
    "semantic_title": "neptune-x: active x-to-maritime generation for universal maritime object detection",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=2hgHyoyVWj": {
    "title": "AuroRA: Breaking Low-Rank Bottleneck of LoRA with Nonlinear Mapping",
    "volume": "spotlight",
    "abstract": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method validated across NLP and CV domains. However, LoRA faces an inherent low-rank bottleneck: narrowing its performance gap with full fine-tuning requires increasing the rank of its parameter matrix, resulting in significant parameter overhead. Recent linear LoRA variants have attempted to enhance expressiveness by introducing additional linear mappings; however, their composition remains inherently linear and fails to fundamentally improve LoRA's representational capacity. To address this limitation, we propose \\ourmethod, which incorporates an Adaptive Nonlinear Layer (ANL) between two linear projectors to capture \\emph{fixed} and \\emph{learnable} nonlinearities. This combination forms an {\\fontfamily{lmtt}\\selectfont \\textbf{MLP-like structure}} with a compressed rank, enabling flexible and precise approximation of diverse target functions while theoretically guaranteeing lower approximation errors and bounded gradients. Extensive experiments on 22 datasets and 6 pretrained models demonstrate that \\ourmethod: (\\textbf{I}) not only matches or surpasses full fine-tuning performance with only $6.18\\%\\sim25\\%$ of LoRA's parameters but also (\\textbf{II}) outperforms state-of-the-art PEFT methods by up to $10.88\\%$ in both NLP and CV tasks, and \\textbf{(III)} exhibits robust performance across various rank configurations",
    "checked": true,
    "id": "b9a1d7fbf92a3aa09f419cc6d1ab6cf101be108c",
    "semantic_title": "aurora: breaking low-rank bottleneck of lora with nonlinear mapping",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Wbc3PutCyQ": {
    "title": "Puppeteer: Rig and Animate Your 3D Models",
    "volume": "spotlight",
    "abstract": "Modern interactive applications increasingly demand dynamic 3D content, yet the transformation of static 3D models into animated assets constitutes a significant bottleneck in content creation pipelines. While recent advances in generative AI have revolutionized static 3D model creation, rigging and animation continue to depend heavily on expert intervention. We present \\textbf{Puppeteer}, a comprehensive framework that addresses both automatic rigging and animation for diverse 3D objects. Our system first predicts plausible skeletal structures via an auto-regressive transformer that introduces a joint-based tokenization strategy for compact representation and a hierarchical ordering methodology with stochastic perturbation that enhances bidirectional learning capabilities. It then infers skinning weights via an attention-based architecture incorporating topology-aware joint attention that explicitly encodes inter-joint relationships based on skeletal graph distances. Finally, we complement these rigging advances with a differentiable optimization-based animation pipeline that generates stable, high-fidelity animations while being computationally more efficient than existing approaches. Extensive evaluations across multiple benchmarks demonstrate that our method significantly outperforms state-of-the-art techniques in both skeletal prediction accuracy and skinning quality. The system robustly processes diverse 3D content, ranging from professionally designed game assets to AI-generated shapes, producing temporally coherent animations that eliminate the jittering issues common in existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fxERuSBpfQ": {
    "title": "Talk2Event: Grounded Understanding of Dynamic Scenes from Event Cameras",
    "volume": "spotlight",
    "abstract": "Event cameras offer microsecond-level latency and robustness to motion blur, making them ideal for understanding dynamic environments. Yet, connecting these asynchronous streams to human language remains an open challenge. We introduce Talk2Event, the first large-scale benchmark for language-driven object grounding in event-based perception. Built from real-world driving data, Talk2Event provides over 30,000 validated referring expressions, each enriched with four grounding attributes -- appearance, status, relation to viewer, and relation to other objects -- bridging spatial, temporal, and relational reasoning. To fully exploit these cues, we propose EventRefer, an attribute-aware grounding framework that dynamically fuses multi-attribute representations through a Mixture of Event-Attribute Experts (MoEE). Our method adapts to different modalities and scene dynamics, achieving consistent gains over state-of-the-art baselines in event-only, frame-only, and event-frame fusion settings. We hope our dataset and approach will establish a foundation for advancing multimodal, temporally-aware, and language-driven perception in real-world robotics and autonomy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JxBA9OJExP": {
    "title": "DNAEdit: Direct Noise Alignment for Text-Guided Rectified Flow Editing",
    "volume": "spotlight",
    "abstract": "Leveraging the powerful generation capability of large-scale pretrained text-to-image models, training-free methods have demonstrated impressive image editing results. Conventional diffusion-based methods, as well as recent rectified flow (RF)-based methods, typically reverse synthesis trajectories by gradually adding noise to clean images, during which the noisy latent at the current timestep is used to approximate that at the next timesteps, introducing accumulated drift and degrading reconstruction accuracy. Considering the fact that in RF the noisy latent is estimated through direct interpolation between Gaussian noises and clean images at each timestep, we propose Direct Noise Alignment (DNA), which directly refines the desired Gaussian noise in the noise domain, significantly reducing the error accumulation in previous methods. Specifically, DNA estimates the velocity field of the interpolated noised latent at each timestep and adjusts the Gaussian noise by computing the difference between the predicted and expected velocity field. We validate the effectiveness of DNA and reveal its relationship with existing RF-based inversion methods. Additionally, we introduce a Mobile Velocity Guidance (MVG) to control the target prompt-guided generation process, balancing image background preservation and target object editability. DNA and MVG collectively constitute our proposed method, namely DNAEdit. Finally, we introduce DNA-Bench, a long-prompt benchmark, to evaluate the performance of advanced image editing models. Experimental results demonstrate that our DNAEdit achieves superior performance to state-of-the-art text-guided editing methods. Our code, model, and benchmark will be made publicly available",
    "checked": true,
    "id": "071be58cede3eeb11bf289548695c9f422c8d5d9",
    "semantic_title": "dnaedit: direct noise alignment for text-guided rectified flow editing",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=yHJRI6rzaA": {
    "title": "Alligat0R: Pre-Training through Covisibility Segmentation for Relative Camera Pose Regression",
    "volume": "spotlight",
    "abstract": "Pre-training techniques have greatly advanced computer vision, with CroCo's cross-view completion approach yielding impressive results in tasks like 3D reconstruction and pose regression. However, cross-view completion is ill-posed in non-covisible regions, limiting its effectiveness. We introduce Alligat0R, a novel pre-training approach that replaces cross-view learning with a covisibility segmentation task. Our method predicts whether each pixel in one image is covisible in the second image, occluded, or outside the field of view, making the pre-training effective in both covisible and non-covisible regions, and provides interpretable predictions. To support this, we present Cub3, a large-scale dataset with 5M image pairs and dense covisibility annotations derived from the nuScenes and ScanNet datasets. Cub3 includes diverse scenarios with varying degrees of overlap. The experiments show that our novel pre-training method Alligat0R significantly outperforms CroCo in relative pose regression. Alligat0R and Cub3 will be made publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UnslcaZSnb": {
    "title": "DiCo: Revitalizing ConvNets for Scalable and Efficient Diffusion Modeling",
    "volume": "spotlight",
    "abstract": "Diffusion Transformer (DiT), a promising diffusion model for visual generation, demonstrates impressive performance but incurs significant computational overhead. Intriguingly, analysis of pre-trained DiT models reveals that global self-attention is often redundant, predominantly capturing local patterns—highlighting the potential for more efficient alternatives. In this paper, we revisit convolution as an alternative building block for constructing efficient and expressive diffusion models. However, naively replacing self-attention with convolution typically results in degraded performance. Our investigations attribute this performance gap to the higher channel redundancy in ConvNets compared to Transformers. To resolve this, we introduce a compact channel attention mechanism that promotes the activation of more diverse channels, thereby enhancing feature diversity. This leads to Diffusion ConvNet (DiCo), a family of diffusion models built entirely from standard ConvNet modules, offering strong generative performance with significant efficiency gains. On class-conditional ImageNet generation benchmarks, DiCo-XL achieves an FID of 2.05 at 256$\\times$256 resolution and 2.53 at 512$\\times$512, with a **2.7$\\times$** and **3.1$\\times$** speedup over DiT-XL/2, respectively. Furthermore, experimental results on MS-COCO demonstrate that the purely convolutional DiCo exhibits strong potential for text-to-image generation",
    "checked": true,
    "id": "9446ed5b848de8b9a24c9869991e9f4d144b74d1",
    "semantic_title": "dico: revitalizing convnets for scalable and efficient diffusion modeling",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=oaWpRaZ4jj": {
    "title": "Injecting Frame-Event Complementary Fusion into Diffusion for Optical Flow in Challenging Scenes",
    "volume": "spotlight",
    "abstract": "Optical flow estimation has achieved promising results in conventional scenes but faces challenges in high-speed and low-light scenes, which suffer from motion blur and insufficient illumination. These conditions lead to weakened texture and amplified noise and deteriorate the appearance saturation and boundary completeness of frame cameras, which are necessary for motion feature matching. In degraded scenes, the frame camera provides dense appearance saturation but sparse boundary completeness due to its long imaging time and low dynamic range. In contrast, the event camera offers sparse appearance saturation, while its short imaging time and high dynamic range gives rise to dense boundary completeness. Traditionally, existing methods utilize feature fusion or domain adaptation to introduce event to improve boundary completeness. However, the appearance features are still deteriorated, which severely affects the mostly adopted discriminative models that learn the mapping from visual features to motion fields and generative models that generate motion fields based on given visual features. So we introduce diffusion models that learn the mapping from noising flow to clear flow, which is not affected by the deteriorated visual features. Therefore, we propose a novel optical flow estimation framework Diff-ABFlow based on diffusion models with frame-event appearance-boundary fusion. Inspired by the appearance-boundary complementarity of frame and event, we propose an Attention-Guided Appearance-Boundary Fusion module to fuse frame and event. Based on diffusion models, we propose a Multi-Condition Iterative Denoising Decoder. Our proposed method can effectively utilize the respective advantages of frame and event, and shows great robustness to degraded input. In addition, we propose a dual-modal optical flow dataset for generalization experiments. Extensive experiments have verified the superiority of our proposed method. The code is released at <https://github.com/Haonan-Wang-aurora/Diff-ABFlow>",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9XCyUFsm1H": {
    "title": "OmniSync: Towards Universal Lip Synchronization via Diffusion Transformers",
    "volume": "spotlight",
    "abstract": "Lip synchronization is the task of aligning a speaker's lip movements in video with corresponding speech audio, and it is essential for creating realistic, expressive video content. However, existing methods often rely on reference frames and masked-frame inpainting, which limit their robustness to identity consistency, pose variations, facial occlusions, and stylized content. In addition, since audio signals provide weaker conditioning than visual cues, lip shape leakage from the original video will affect lip sync quality. In this paper, we present OmniSync, a universal lip synchronization framework for diverse visual scenarios. Our approach introduces a mask-free training paradigm using Diffusion Transformer models for direct frame editing without explicit masks, enabling unlimited-duration inference while maintaining natural facial dynamics and preserving character identity. During inference, we propose a flow-matching-based progressive noise initialization to ensure pose and identity consistency, while allowing precise mouth-region editing. To address the weak conditioning signal of audio, we develop a Dynamic Spatiotemporal Classifier-Free Guidance (DS-CFG) mechanism that adaptively adjusts guidance strength over time and space. We also establish the AIGC-LipSync Benchmark, the first evaluation suite for lip synchronization in diverse AI-generated videos. Extensive experiments demonstrate that OmniSync significantly outperforms prior methods in both visual quality and lip sync accuracy, achieving superior results in both real-world and AI-generated videos",
    "checked": true,
    "id": "9ab02289297e36c4d53863ec4a0b326a844aae92",
    "semantic_title": "omnisync: towards universal lip synchronization via diffusion transformers",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=U88JlpY0vR": {
    "title": "MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning",
    "volume": "spotlight",
    "abstract": "The ability of robots to interpret human instructions and execute manipulation tasks necessitates the availability of task-relevant tabletop scenes for training. However, traditional methods for creating these scenes rely on time-consuming manual layout design or purely randomized layouts, which are limited in terms of plausibility or alignment with the tasks. In this paper, we formulate a novel task, namely task-oriented tabletop scene generation, which poses significant challenges due to the substantial gap between high-level task instructions and the tabletop scenes. To support research on such a challenging task, we introduce \\textbf{MesaTask-10K}, a large-scale dataset comprising approximately 10,700 synthetic tabletop scenes with \\emph{manually crafted layouts} that ensure realistic layouts and intricate inter-object relations. To bridge the gap between tasks and scenes, we propose a \\textbf{Spatial Reasoning Chain} that decomposes the generation process into object inference, spatial interrelation reasoning, and scene graph construction for the final 3D layout. We present \\textbf{MesaTask}, an LLM-based framework that utilizes this reasoning chain and is further enhanced with DPO algorithms to generate physically plausible tabletop scenes that align well with given task descriptions. Exhaustive experiments demonstrate the superior performance of MesaTask compared to baselines in generating task-conforming tabletop scenes with realistic layouts",
    "checked": true,
    "id": "4bf87fe3052bf77e29c1d4028702c3975d8fd292",
    "semantic_title": "mesatask: towards task-driven tabletop scene generation via 3d spatial reasoning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mSiN7i0BYH": {
    "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion",
    "volume": "spotlight",
    "abstract": "We introduce Self Forcing, a novel training paradigm for autoregressive video diffusion models. It addresses the longstanding issue of exposure bias, where models trained on ground-truth context must generate sequences conditioned on their own imperfect outputs during inference. Unlike prior methods that denoise future frames based on ground-truth context frames, Self Forcing conditions each frame's generation on previously self-generated outputs by performing autoregressive rollout with key-value (KV) caching during training. This strategy enables supervision through a holistic loss at the video level that directly evaluates the quality of the entire generated sequence, rather than relying solely on traditional frame-wise objectives. To ensure training efficiency, we employ a few-step diffusion model along with a stochastic gradient truncation strategy, effectively balancing computational cost and performance. We further introduce a rolling KV cache mechanism that enables efficient autoregressive video extrapolation. Extensive experiments demonstrate that our approach achieves real-time streaming video generation with sub-second latency on a single GPU, while matching or even surpassing the generation quality of significantly slower and non-causal diffusion models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hQhAPGCtPo": {
    "title": "DICEPTION: A Generalist Diffusion Model for Visual Perceptual Tasks",
    "volume": "spotlight",
    "abstract": "This paper's primary objective is to develop a robust generalist perception model capable of addressing multiple tasks under constraints of computational resources and limited training data. We leverage text-to-image diffusion models pre-trained on billions of images and successfully introduce our DICEPTION, a visual generalist model. Exhaustive evaluations demonstrate that DICEPTION effectively tackles diverse perception tasks, even achieving performance comparable to SOTA single-task specialist models. Specifically, we achieve results on par with SAM-vit-h using only 0.06% of their data (e.g., 600K vs.\\ 1B pixel-level annotated images). We designed comprehensive experiments on architectures and input paradigms, demonstrating that the key to successfully re-purposing a single diffusion model for multiple perception tasks lies in maximizing the preservation of the pre-trained model's prior knowledge. Consequently, DICEPTION can be trained with substantially lower computational costs than conventional models requiring training from scratch. Furthermore, adapting DICEPTION to novel tasks is highly efficient, necessitating fine-tuning on as few as 50 images and approximately 1% of its parameters. Finally, we demonstrate that a subtle application of classifier-free guidance can improve the model's performance on depth and normal estimation. We also show that pixel-aligned training, as is characteristic of perception tasks, significantly enhances the model's ability to preserve fine details. DICEPTION offers valuable insights and presents a promising direction for the development of advanced diffusion-based visual generalist models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nIFFMrDQ5w": {
    "title": "Variational Learning Finds Flatter Solutions at the Edge of Stability",
    "volume": "spotlight",
    "abstract": "Variational Learning (VL) has recently gained popularity for training deep neural networks. Part of its empirical success can be explained by theories such as PAC-Bayes bounds, minimum description length and marginal likelihood, but little has been done to unravel the implicit regularization in play. Here, we analyze the implicit regularization of VL through the Edge of Stability (EoS) framework. EoS has previously been used to show that gradient descent can find flat solutions and we extend this result to show that VL can find even flatter solutions. This result is obtained by controlling the shape of the variational posterior as well as the number of posterior samples used during training. The derivation follows in a similar fashion as in the standard EoS literature for deep learning, by first deriving a result for a quadratic problem and then extending it to deep neural networks. We empirically validate these findings on a wide variety of large networks, such as ResNet and ViT, to find that the theoretical results closely match the empirical ones. Ours is the first work to analyze the EoS dynamics of~VL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PWdWmw9wh0": {
    "title": "Seeing Sound, Hearing Sight: Uncovering Modality Bias and Conflict of AI models in Sound Localization",
    "volume": "spotlight",
    "abstract": "Imagine hearing a dog bark and instinctively turning toward the sound—only to find a parked car, while a silent dog sits nearby. Such moments of sensory conflict challenge perception, yet humans flexibly resolve these discrepancies, prioritizing auditory cues over misleading visuals to accurately localize sounds. Despite the rapid advancement of multimodal AI models that integrate vision and sound, little is known about how these systems handle cross-modal conflicts or whether they favor one modality over another. Here, we systematically and quantitatively examine modality bias and conflict resolution in AI models for Sound Source Localization (SSL). We evaluate a wide range of state-of-the-art multimodal models and compare them against human performance in psychophysics experiments spanning six audiovisual conditions, including congruent, conflicting, and absent visual and audio cues. Our results reveal that humans consistently outperform AI in SSL and exhibit greater robustness to conflicting or absent visual information by effectively prioritizing auditory signals. In contrast, AI shows a pronounced bias toward vision, often failing to suppress irrelevant or conflicting visual input, leading to chance-level performance. To bridge this gap, we present EchoPin, a neuroscience-inspired multimodal model for SSL that emulates human auditory perception. The model is trained on our carefully curated AudioCOCO dataset, in which stereo audio signals are first rendered using a physics-based 3D simulator, then filtered with Head-Related Transfer Functions (HRTFs) to capture pinnae, head, and torso effects, and finally transformed into cochleagram representations that mimic cochlear processing. To eliminate existing biases in standard benchmark datasets, we carefully controlled the vocal object sizes, semantics, and spatial locations in the corresponding images of AudioCOCO. EchoPin outperforms existing models trained on standard audio-visual datasets. Remarkably, consistent with neuroscience findings, it exhibits a human-like localization bias, favoring horizontal (left–right) precision over vertical (up–down) precision. This asymmetry likely arises from HRTF-shaped and cochlear-modulated stereo audio and the lateral placement of human ears, highlighting how sensory input quality and physical structure jointly shape precision of multimodal representations. All code, data, and models are available \\href{https://github.com/CuriseJia/SSHS}{here}",
    "checked": true,
    "id": "3ce59e1062300b66833085420fa4319114e1807a",
    "semantic_title": "seeing sound, hearing sight: uncovering modality bias and conflict of ai models in sound localization",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=btm5Z5Vu8G": {
    "title": "ALINE: Joint Amortization for Bayesian Inference and Active Data Acquisition",
    "volume": "spotlight",
    "abstract": "Many critical applications, from autonomous scientific discovery to personalized medicine, demand systems that can both strategically acquire the most informative data and instantaneously perform inference based upon it. While amortized methods for Bayesian inference and experimental design offer part of the solution, neither approach is optimal in the most general and challenging task, where new data needs to be collected for instant inference. To tackle this issue, we introduce the Amortized Active Learning and Inference Engine (ALINE), a unified framework for amortized Bayesian inference and active data acquisition. ALINE leverages a transformer architecture trained via reinforcement learning with a reward based on self-estimated information gain provided by its own integrated inference component. This allows it to strategically query informative data points while simultaneously refining its predictions. Moreover, ALINE can selectively direct its querying strategy towards specific subsets of model parameters or designated predictive tasks, optimizing for posterior estimation, data prediction, or a mixture thereof. Empirical results on regression-based active learning, classical Bayesian experimental design benchmarks, and a psychometric model with selectively targeted parameters demonstrate that ALINE delivers both instant and accurate inference along with efficient selection of informative points",
    "checked": true,
    "id": "94aef748bf3e426e6be89ef9110ea5f76a3a730d",
    "semantic_title": "aline: joint amortization for bayesian inference and active data acquisition",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=11fe8wKkmk": {
    "title": "Fully Autonomous Neuromorphic Navigation and Dynamic Obstacle Avoidance",
    "volume": "spotlight",
    "abstract": "Unmanned aerial vehicles could accurately accomplish complex navigation and obstacle avoidance tasks under external control. However, enabling unmanned aerial vehicles (UAVs) to rely solely on onboard computation and sensing for real-time navigation and dynamic obstacle avoidance remains a significant challenge due to stringent latency and energy constraints. Inspired by the efficiency of biological systems, we propose a fully neuromorphic framework achieving end-to-end obstacle avoidance during navigation with an overall latency of just 2.3 milliseconds. Specifically, our bio-inspired approach enables accurate moving object detection and avoidance without requiring target recognition or trajectory computation. Additionally, we introduce the first monocular event-based pose correction dataset with over 50,000 paired and labeled event streams. We validate our system on an autonomous quadrotor using only onboard resources, demonstrating reliable navigation and avoidance of diverse obstacles moving at speeds up to 10 m/s",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RbdLnwEEjk": {
    "title": "Enhancing LLM Watermark Resilience Against Both Scrubbing and Spoofing Attacks",
    "volume": "spotlight",
    "abstract": "Watermarking is a promising defense against the misuse of large language models (LLMs), yet it remains vulnerable to scrubbing and spoofing attacks. This vulnerability stems from an inherent trade-off governed by watermark window size: smaller windows resist scrubbing better but are easier to reverse-engineer, enabling low-cost statistics-based spoofing attacks. This work expands the trade-off boundary by introducing a novel mechanism, equivalent texture keys, where multiple tokens within a watermark window can independently support the detection. Based on the redundancy, we propose a watermark scheme with **S**ub-vocabulary decomposed **E**quivalent t**E**xture **K**ey (**SEEK**). It achieves a Pareto improvement, increasing the resilience against scrubbing attacks without compromising robustness to spoofing. Our code will be available at [https://github.com/Hearum/SeekWM](https://github.com/Hearum/SeekWM)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lwOV2ACEK9": {
    "title": "Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via Collective Monte Carlo Tree Search",
    "volume": "spotlight",
    "abstract": "In this work, we aim to develop an MLLM that understands and solves questions by learning to create each intermediate step of the reasoning involved till the final answer. To this end, we propose Collective Monte Carlo Tree Search (CoMCTS), a new learning-to-reason method for MLLMs, which introduces the concept of collective learning into ``tree search'' for effective and efficient reasoning-path searching and learning. The core idea of CoMCTS is to leverage collective knowledge from multiple models to collaboratively conjecture, search and identify effective reasoning paths toward correct answers via four iterative operations including Expansion, Simulation and Error Positioning, Backpropagation, and Selection. Using CoMCTS, we construct Mulberry-260k, a multimodal dataset with a tree of rich, explicit and well-defined reasoning nodes for each question. With Mulberry-260k, we perform collective SFT to train our model, Mulberry, a series of MLLMs with o1-like step-by-step Reasoning and Reflection capabilities. Extensive experiments demonstrate the superiority of our proposed methods on various benchmarks. Code is available at https://github.com/HJYao00/Mulberry",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2uKVyGq5zK": {
    "title": "ProtInvTree: Deliberate Protein Inverse Folding with Reward-guided Tree Search",
    "volume": "spotlight",
    "abstract": "Designing protein sequences that fold into a target 3D structure—known as protein inverse folding—is a fundamental challenge in protein engineering. While recent deep learning methods have achieved impressive performance by recovering native sequences, they often overlook the one-to-many nature of the problem: multiple diverse sequences can fold into the same structure. This motivates the need for a generative model capable of designing diverse sequences while preserving structural consistency. To address this trade-off, we introduce ProtInvTree, the first reward-guided tree-search framework for protein inverse folding. ProtInvTree reformulates sequence generation as a deliberate, step-wise decision-making process, enabling the exploration of multiple design paths and exploitation of promising candidates through self-evaluation, lookahead, and backtracking. We propose a two-stage focus-and-grounding action mechanism that decouples position selection and residue generation. To efficiently evaluate intermediate states, we introduce a jumpy denoising strategy that avoids full rollouts. Built upon pretrained protein language models, ProtInvTree supports flexible test-time scaling by adjusting the search depth and breadth without retraining. Empirically, ProtInvTree outperforms state-of-the-art baselines across multiple benchmarks, generating structurally consistent yet diverse sequences, including those far from the native ground truth. The code is available at https://github.com/A4Bio/ProteinInvBench/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3KtPujOw5z": {
    "title": "On the Value of Cross-Modal Misalignment in Multimodal Representation Learning",
    "volume": "spotlight",
    "abstract": "Multimodal representation learning, exemplified by multimodal contrastive learning (MMCL) using image-text pairs, aims to learn powerful representations by aligning cues across modalities. This approach relies on the core assumption that the exemplar image-text pairs constitute two representations of an identical concept. However, recent research has revealed that real-world datasets often exhibit cross-modal misalignment. There are two distinct viewpoints on how to address this issue: one suggests mitigating the misalignment, and the other leveraging it. We seek here to reconcile these seemingly opposing perspectives, and to provide a practical guide for practitioners. Using latent variable models we thus formalize cross-modal misalignment by introducing two specific mechanisms: Selection bias, where some semantic variables are absent in the text, and perturbation bias, where semantic variables are altered—both leading to misalignment in data pairs. Our theoretical analysis demonstrates that, under mild assumptions, the representations learned by MMCL capture exactly the information related to the subset of the semantic variables invariant to selection and perturbation biases. This provides a unified perspective for understanding misalignment. Based on this, we further offer actionable insights into how misalignment should inform the design of real-world ML systems. We validate our theoretical findings via extensive empirical studies on both synthetic data and real image-text datasets, shedding light on the nuanced impact of cross-modal misalignment on multimodal representation learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nlQRra0OLH": {
    "title": "A Unified Solution to Video Fusion: From Multi-Frame Learning to Benchmarking",
    "volume": "spotlight",
    "abstract": "The real world is dynamic, yet most image fusion methods process static frames independently, ignoring temporal correlations in videos and leading to flickering and temporal inconsistency. To address this, we propose Unified Video Fusion (UniVF), a novel and unified framework for video fusion that leverages multi-frame learning and optical flow-based feature warping for informative, temporally coherent video fusion. To support its development, we also introduce Video Fusion Benchmark (VF-Bench), the first comprehensive benchmark covering four video fusion tasks: multi-exposure, multi-focus, infrared-visible, and medical fusion. VF-Bench provides high-quality, well-aligned video pairs obtained through synthetic data generation and rigorous curation from existing datasets, with a unified evaluation protocol that jointly assesses the spatial quality and temporal consistency of video fusion. Extensive experiments show that UniVF achieves state-of-the-art results across all tasks on VF-Bench. Project page: [vfbench.github.io](https://vfbench.github.io)",
    "checked": true,
    "id": "ee83d42a63bc5a7b0da189479a6b0ba11ecceb42",
    "semantic_title": "a unified solution to video fusion: from multi-frame learning to benchmarking",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=IupCqXiiOE": {
    "title": "Multi-agent Markov Entanglement",
    "volume": "spotlight",
    "abstract": "Value decomposition has long been a fundamental technique in multi-agent reinforcement learning and dynamic programming. Specifically, the value function of a global state $(s_1,s_2,\\ldots,s_N)$ is often approximated as the sum of local functions: $V(s_1,s_2,\\ldots,s_N)\\approx\\sum_{i=1}^N V_i(s_i)$. This approach has found various applications in modern RL systems. However, the theoretical justification for why this decomposition works so effectively remains underexplored. In this paper, we uncover the underlying mathematical structure that enables value decomposition. We demonstrate that a Markov decision process (MDP) permits value decomposition *if and only if* its transition matrix is not \"entangled\"—a concept analogous to quantum entanglement in quantum physics. Drawing inspiration from how physicists measure quantum entanglement, we introduce how to measure the \"Markov entanglement\" and show that this measure can be used to bound the decomposition error in general multi-agent MDPs. Using the concept of Markov entanglement, we proved that a widely-used class of policies, the index policy, is weakly-entangled and enjoys a sublinear $\\mathcal O(\\sqrt{N})$ scale of decomposition error for $N$-agent systems. Finally, we show Markov entanglement can be efficiently estimated, guiding practitioners on the feasibility of value decomposition",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=plpAecfkf4": {
    "title": "SQS: Enhancing Sparse Perception Models via Query-based Splatting in Autonomous Driving",
    "volume": "spotlight",
    "abstract": "Sparse Perception Models (SPMs) adopt a query-driven paradigm that forgoes explicit dense BEV or volumetric construction, enabling highly efficient computation and accelerated inference. In this paper, we introduce SQS, a novel query-based splatting pre-training specifically designed to advance SPMs in autonomous driving. SQS introduces a plug-in module that predicts 3D Gaussian representations from sparse queries during pre-training, leveraging self-supervised splatting to learn fine-grained contextual features through the reconstruction of multi-view images and depth maps. During fine-tuning, the pre-trained Gaussian queries are seamlessly integrated into downstream networks via query interaction mechanisms that explicitly connect pre-trained queries with task-specific queries, effectively accommodating the diverse requirements of occupancy prediction and 3D object detection. Extensive experiments on autonomous driving benchmarks demonstrate that SQS delivers considerable performance gains across multiple query-based 3D perception tasks, notably in occupancy prediction and 3D object detection, outperforming prior state-of-the-art pre-training approaches by a significant margin (i.e., +1.3 mIoU on occupancy prediction and +1.0 NDS on 3D detection)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5NkfjxMpWe": {
    "title": "PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding",
    "volume": "spotlight",
    "abstract": "Vision-language models are integral to computer vision research, yet many high-performing models remain closed-source, obscuring their data, design and training recipe. The research community has responded by using distillation from black-box models to label training data, achieving strong benchmark results, at the cost of measurable scientific progress. However, without knowing the details of the teacher model and its data sources, scientific progress remains difficult to measure. In this paper, we study building a Perception Language Model (PLM) in a fully open and reproducible framework for transparent research in image and video understanding. We analyze standard training pipelines without distillation from proprietary models and explore large-scale synthetic data to identify critical data gaps, particularly in detailed video understanding. To bridge these gaps, we release 2.8M human-labeled instances of fine-grained video question-answer pairs and spatio-temporally grounded video captions. Additionally, we introduce PLM–VideoBench, a suite for evaluating challenging video understanding tasks focusing on the ability to reason about ''what'', ''where'', ''when'', and ''how'' of a video. We make our work fully reproducible by providing data, training recipes, code & models",
    "checked": true,
    "id": "821247d1e96d89e5c8df1118412b235a9fda0577",
    "semantic_title": "perceptionlm: open-access data and models for detailed visual understanding",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=iUjGNJzrF1": {
    "title": "Debate or Vote: Which Yields Better Decisions in Multi-Agent Large Language Models?",
    "volume": "spotlight",
    "abstract": "Multi-Agent Debate (MAD) has emerged as a promising paradigm for improving the performance of large language models through collaborative reasoning. Despite recent advances, the key factors driving MAD's effectiveness remain unclear. In this work, we disentangle MAD into two key components–Majority Voting and inter-agent Debate–and assess their respective contributions. Through extensive experiments across seven NLP benchmarks, we find that Majority Voting alone accounts for most of the performance gains typically attributed to MAD. To explain this, we propose a theoretical framework that models debate as a stochastic process. We prove that it induces a martingale over agents' belief trajectories, implying that debate alone does not improve expected correctness. Guided by these insights, we demonstrate that targeted interventions, by biasing the belief update toward correction, can meaningfully enhance debate effectiveness. Overall, our findings suggest that while MAD has potential, simple ensembling methods remain strong and more reliable alternatives in many practical settings. Code is released in https://github.com/deeplearning-wisc/debate-or-vote",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lMhNrt0Bnm": {
    "title": "VoxDet: Rethinking 3D Semantic Scene Completion as Dense Object Detection",
    "volume": "spotlight",
    "abstract": "Semantic Scene Completion (SSC) aims to reconstruct the 3D geometry and semantics of the surrounding environment. With dense voxel labels, prior works typically formulate SSC as a *dense segmentation task*, independently classifying each voxel. However, this paradigm neglects critical instance-centric discriminability, leading to instance-level incompleteness and adjacent ambiguities. To address this, we highlight a \"free lunch\" of SSC labels: the voxel-level class label has implicitly told the instance-level insight, which is ever-overlooked by the community. Motivated by this observation, we first introduce a training-free **Voxel-to-Instance (VoxNT) trick**: a simple yet effective method that freely converts voxel-level class labels into instance-level offset labels. Building on this, we further propose **VoxDet**, an instance-centric framework that reformulates the voxel-level SSC as *dense object detection* by decoupling it into two sub-tasks: offset regression and semantic prediction. Specifically, based on the lifted 3D volume, VoxDet first uses (a) Spatially-decoupled Voxel Encoder to generate disentangled feature volumes for the two sub-tasks, which learn task-specific spatial deformation in the densely projected tri-perceptive space. Then, we deploy (b) Task-decoupled Dense Predictor to address SSC via dense detection. Here, we first regress a 4D offset field to estimate distances (6 directions) between voxels and the corresponding object boundaries in the voxel space. The regressed offsets are then used to guide the instance-level aggregation in the classification branch, achieving instance-aware scene completion. VoxDet can be deployed on both camera and LiDAR input and jointly achieves state-of-the-art results on both benchmarks, which gives 63.0 IoU on the SemanticKITTI test set, **ranking 1$^{st}$** on the online leaderboard",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lv4zLWzOi2": {
    "title": "Approximate Domain Unlearning for Vision-Language Models",
    "volume": "spotlight",
    "abstract": "Pre-trained Vision-Language Models (VLMs) exhibit strong generalization capabilities, enabling them to recognize a wide range of objects across diverse domains without additional training. However, they often retain irrelevant information beyond the requirements of specific target downstream tasks, raising concerns about computational efficiency and potential information leakage. This has motivated growing interest in approximate unlearning, which aims to selectively remove unnecessary knowledge while preserving overall model performance. Existing approaches to approximate unlearning have primarily focused on {\\em class unlearning}, where a VLM is retrained to fail to recognize specified object classes while maintaining accuracy for others. However, merely forgetting object classes is often insufficient in practical applications. For instance, an autonomous driving system should accurately recognize {\\em real} cars, while avoiding misrecognition of {\\em illustrated} cars depicted in roadside advertisements as {\\em real} cars, which could be hazardous. In this paper, we introduce {\\em Approximate Domain Unlearning (ADU)}, a novel problem setting that requires reducing recognition accuracy for images from specified domains (e.g., {\\em illustration}) while preserving accuracy for other domains (e.g., {\\em real}). ADU presents new technical challenges: due to the strong domain generalization capability of pre-trained VLMs, domain distributions are highly entangled in the feature space, making naive approaches based on penalizing target domains ineffective. To tackle this limitation, we propose a novel approach that explicitly disentangles domain distributions and adaptively captures instance-specific domain information. Extensive experiments on four multi-domain benchmark datasets demonstrate that our approach significantly outperforms strong baselines built upon state-of-the-art VLM tuning techniques, paving the way for practical and fine-grained unlearning in VLMs. Code : https://kodaikawamura.github.io/Domain_Unlearning/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yPC9zmkQgG": {
    "title": "BioCLIP 2: Emergent Properties from Scaling Hierarchical Contrastive Learning",
    "volume": "spotlight",
    "abstract": "Foundation models trained at scale exhibit remarkable emergent behaviors, learning new capabilities beyond their initial training objectives. We find such emergent behaviors in biological vision models via large-scale contrastive vision-language training. To achieve this, we first curate TreeOfLife-200M, comprising 214 million images of living organisms, the largest and most diverse biological organism image dataset to date. We then train BioCLIP 2 on TreeOfLife-200M to distinguish different species. Despite the narrow training objective, BioCLIP 2 yields extraordinary accuracy when applied to various biological visual tasks such as habitat classification and trait prediction. We identify emergent properties in the learned embedding space of BioCLIP 2. At the inter-species level, the embedding distribution of different species aligns closely with functional and ecological meanings (e.g., beak sizes and habitats). At the intra-species level, instead of being diminished, the intra-species variations (e.g., life stages and sexes) are preserved and better separated in subspaces orthogonal to inter-species distinctions. We provide formal proof and analyses to explain why hierarchical supervision and contrastive objectives encourage these emergent properties. Crucially, our results reveal that these properties become increasingly significant with larger-scale training data, leading to a biologically meaningful embedding space",
    "checked": true,
    "id": "af5f9b40cb20df4cf0f3c0c767e9855738918554",
    "semantic_title": "bioclip 2: emergent properties from scaling hierarchical contrastive learning",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=h3dbocj7po": {
    "title": "GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments",
    "volume": "spotlight",
    "abstract": "The era of foundation models has revolutionized AI research, yet Graph Foundation Models (GFMs) remain constrained by the scarcity of large-scale graph corpora. Traditional graph data synthesis techniques primarily focus on simplistic structural operations, lacking the capacity to generate semantically rich nodes with meaningful textual attributes—a critical limitation for real-world applications. While large language models (LLMs) demonstrate exceptional text generation capabilities, their direct application to graph synthesis is impeded by context window limitations, hallucination phenomena, and structural consistency challenges. To address these issues, we introduce \\textbf{GraphMaster}—the first multi-agent framework specifically designed for graph data synthesis in data-limited environments. GraphMaster orchestrates four specialized LLM agents (Manager, Perception, Enhancement, and Evaluation) that collaboratively optimize the synthesis process through iterative refinement, ensuring both semantic coherence and structural integrity. To rigorously evaluate our approach, we create new data-limited \"Sub\" variants of six standard graph benchmarks, specifically designed to test synthesis capabilities under realistic constraints. Additionally, we develop a novel interpretability assessment framework that combines human evaluation with a principled Grassmannian manifold-based analysis, providing both qualitative and quantitative measures of semantic coherence. Experimental results demonstrate that GraphMaster significantly outperforms traditional synthesis methods across multiple datasets, establishing a strong foundation for advancing GFMs in data-scarce environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bds54EfR9x": {
    "title": "Q-Insight: Understanding Image Quality via Visual Reinforcement Learning",
    "volume": "spotlight",
    "abstract": "Image quality assessment (IQA) focuses on the perceptual visual quality of images, playing a crucial role in downstream tasks such as image reconstruction, compression, and generation. The rapid advancement of multi-modal large language models (MLLMs) has significantly broadened the scope of IQA, moving toward comprehensive image quality understanding that incorporates content analysis, degradation perception, and comparison reasoning beyond mere numerical scoring. Previous MLLM-based methods typically either generate numerical scores lacking interpretability or heavily rely on supervised fine-tuning (SFT) using large-scale annotated datasets to provide descriptive assessments, limiting their flexibility and applicability. In this paper, we propose Q-Insight, a reinforcement learning-based model built upon group relative policy optimization (GRPO), which demonstrates strong visual reasoning capability for image quality understanding while requiring only a limited amount of rating scores and degradation labels. By jointly optimizing score regression and degradation perception tasks with carefully designed reward functions, our approach effectively exploits their mutual benefits for enhanced performance. Extensive experiments demonstrate that Q-Insight substantially outperforms existing state-of-the-art methods on both score regression and degradation perception tasks, while exhibiting impressive zero-shot generalization and superior comparison reasoning capability. The code and models are available at https://github.com/bytedance/Q-Insight",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7pufO0SJAC": {
    "title": "What Makes a Reward Model a Good Teacher? An Optimization Perspective",
    "volume": "spotlight",
    "abstract": "The success of Reinforcement Learning from Human Feedback (RLHF) critically depends on the quality of the reward model. However, while this quality is primarily evaluated through accuracy, it remains unclear whether accuracy fully captures what makes a reward model an effective teacher. We address this question from an optimization perspective. First, we prove that regardless of how accurate a reward model is, if it induces low reward variance, then the RLHF objective suffers from a flat landscape. Consequently, even a perfectly accurate reward model can lead to extremely slow optimization, underperforming less accurate models that induce higher reward variance. We additionally show that a reward model that works well for one language model can induce low reward variance, and thus a flat objective landscape, for another. These results establish a fundamental limitation of evaluating reward models solely based on accuracy or independently of the language model they guide. Experiments using models of up to 8B parameters corroborate our theory, demonstrating the interplay between reward variance, accuracy, and reward maximization rate. Overall, our findings highlight that beyond accuracy, a reward model needs to induce sufficient variance for efficient optimization",
    "checked": true,
    "id": "94385846fd17d7836f68f8f2f2a36214e298c0dc",
    "semantic_title": "what makes a reward model a good teacher? an optimization perspective",
    "citation_count": 40,
    "authors": []
  },
  "https://openreview.net/forum?id=slVqJAI5sT": {
    "title": "Ψ -Sampler: Initial Particle Sampling for SMC-Based Inference-Time Reward Alignment in Score Models",
    "volume": "spotlight",
    "abstract": "We introduce $\\Psi$-Sampler, an SMC-based framework incorporating pCNL-based initial particle sampling for effective inference-time reward alignment with a score-based model. Inference-time reward alignment with score-based generative models has recently gained significant traction, following a broader paradigm shift from pre-training to post-training optimization. At the core of this trend is the application of Sequential Monte Carlo (SMC) to the denoising process. However, existing methods typically initialize particles from the Gaussian prior, which inadequately captures reward-relevant regions and results in reduced sampling efficiency. We demonstrate that initializing from the reward-aware posterior significantly improves alignment performance. To enable posterior sampling in high-dimensional latent spaces, we introduce the preconditioned Crank–Nicolson Langevin (pCNL) algorithm, which combines dimension-robust proposals with gradient-informed dynamics. This approach enables efficient and scalable posterior sampling and consistently improves performance across various reward alignment tasks, including layout-to-image generation, quantity-aware generation, and aesthetic-preference generation, as demonstrated in our experiments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OaNbl9b56B": {
    "title": "Do-PFN: In-Context Learning for Causal Effect Estimation",
    "volume": "spotlight",
    "abstract": "Causal effect estimation is critical to a range of scientific disciplines. Existing methods for this task either require interventional data, knowledge about the ground-truth causal graph, or rely on assumptions such as unconfoundedness, restricting their applicability in real-world settings. In the domain of tabular machine learning, Prior-data fitted networks (PFNs) have achieved state-of-the-art predictive performance, having been pre-trained on synthetic causal data to solve tabular prediction problems via in-context learning. To assess whether this can be transferred to the problem of causal effect estimation, we pre-train PFNs on synthetic data drawn from a wide variety of causal structures, including interventions, to predict interventional outcomes given observational data. Through extensive experiments in synthetic and semi-synthetic settings, we show that our approach allows for the accurate estimation of causal effects without knowledge of the underlying causal graph",
    "checked": true,
    "id": "22de6bbc648fd723db7c12bb2ad7d9d95e70b7c0",
    "semantic_title": "do-pfn: in-context learning for causal effect estimation",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=T0CiI4gDFB": {
    "title": "ReSim: Reliable World Simulation for Autonomous Driving",
    "volume": "spotlight",
    "abstract": "How can we reliably simulate future driving scenarios under a wide range of ego driving behaviors? Recent driving world models, developed exclusively on real-world driving data composed mainly of safe expert trajectories, struggle to follow hazardous or non-expert behaviors, which are rare in such data. This limitation restricts their applicability to tasks such as policy evaluation. In this work, we address this challenge by enriching real-world human demonstrations with diverse non-expert data collected from a driving simulator (e.g., CARLA), and building a controllable world model trained on this heterogeneous corpus. Starting with a video generator featuring diffusion transformer architecture, we devise several strategies to effectively integrate conditioning signals and improve prediction controllability and fidelity. The resulting model, ReSim, enables Reliable Simulation of diverse open-world driving scenarios under various actions, including hazardous non-expert ones. To close the gap between high-fidelity simulation and applications that require reward signals to judge different actions, we introduce a Video2Reward module that estimates reward from ReSim's simulated future. Our ReSim paradigm achieves up to 44% higher visual fidelity, improves controllability for both expert and non-expert actions by over 50%, and boosts planning and policy selection performance on NAVSIM by 2% and 25%, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8OGTkEJrmb": {
    "title": "Differentiable Sparsity via D -Gating: Simple and Versatile Structured Penalization",
    "volume": "spotlight",
    "abstract": "Structured sparsity regularization offers a principled way to compact neural networks, but its non-differentiability breaks compatibility with conventional stochastic gradient descent and requires either specialized optimizers or additional post-hoc pruning without formal guarantees. In this work, we propose $D$-Gating, a fully differentiable structured overparameterization that splits each group of weights into a primary weight vector and multiple scalar gating factors. We prove that any local minimum under $D$-Gating is also a local minimum using non-smooth structured $L_{2,2/D}$ penalization, and further show that the $D$-Gating objective converges at least exponentially fast to the $L_{2,2/D}$–regularized loss in the gradient flow limit. Together, our results show that $D$-Gating is theoretically equivalent to solving the original group sparsity problem, yet induces distinct learning dynamics that evolve from a non-sparse regime into sparse optimization. We validate our theory across vision, language, and tabular tasks, where $D$-Gating consistently delivers strong performance–sparsity tradeoffs and outperforms both direct optimization of structured penalties and conventional pruning baselines",
    "checked": false,
    "id": "e44770fc5725e649c99a03bed63ae20423106d95",
    "semantic_title": "differentiable sparsity via d-gating: simple and versatile structured penalization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RxWILaXuhb": {
    "title": "Time-o1: Time-Series Forecasting Needs Transformed Label Alignment",
    "volume": "poster",
    "abstract": "Training time-series forecast models presents unique challenges in designing effective learning objectives. Existing methods predominantly utilize the temporal mean squared error, which faces two critical challenges: (1) label autocorrelation, which leads to bias from the label sequence likelihood; (2) excessive amount of tasks, which increases with the forecast horizon and complicates optimization. To address these challenges, we propose Time-o1, a transformation-augmented learning objective for training time-series forecasting models. The central idea is to transform the label sequence into decorrelated components with discriminated significance. Models are then trained to align the most significant components, thereby effectively mitigating label autocorrelation and reducing task amount. Extensive experiments demonstrate that Time-o1 achieves state-of-the-art performance and is compatible with various forecast models. Code is available at https://github.com/Master-PLC/Time-o1",
    "checked": true,
    "id": "dd3be5331e4eabb11ac6efb0c7be99aaeb34963a",
    "semantic_title": "time-o1: time-series forecasting needs transformed label alignment",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=ZeFMtRBy4Z": {
    "title": "REVE: A Foundation Model for EEG - Adapting to Any Setup with Large-Scale Pretraining on 25,000 Subjects",
    "volume": "poster",
    "abstract": "Foundation models have transformed AI by reducing reliance on task-specific data through large-scale pretraining. While successful in language and vision, their adoption in EEG has lagged due to the heterogeneity of public datasets, which are collected under varying protocols, devices, and electrode configurations. Existing EEG foundation models struggle to generalize across these variations, often restricting pretraining to a single setup, resulting in suboptimal performance, in particular under linear probing. We present REVE (Representation for EEG with Versatile Embeddings), a pretrained model explicitly designed to generalize across diverse EEG signals. REVE introduces a novel 4D positional encoding scheme that enables it to process signals of arbitrary length and electrode arrangement. Using a masked autoencoding objective, we pretrain REVE on over 60,000 hours of EEG data from 92 datasets spanning 25,000 subjects, representing the largest EEG pretraining effort to date. REVE achieves state-of-the-art results on 10 downstream EEG tasks, including motor imagery classification, seizure detection, sleep staging, cognitive load estimation, and emotion recognition. With little to no fine-tuning, it demonstrates strong generalization, and nuanced spatio-temporal modeling. We release code, pretrained weights, and tutorials to support standardized EEG research and accelerate progress in clinical neuroscience",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yrrU5YChQr": {
    "title": "Vulnerable Data-Aware Adversarial Training",
    "volume": "poster",
    "abstract": "Fast adversarial training (FAT) has been considered as one of the most effective alternatives to the computationally-intensive adversarial training. Generally, FAT methods pay equal attention to each sample of the target task. However, the distance between each sample and the decision boundary is different, learning samples which are far from the decision boundary (i.e., less important to adversarial robustness) brings additional training cost and leads to sub-optimal results. To tackle this issue, we present vulnerable data-aware adversarial training (VDAT) in this study. Specifically, we first propose a margin-based vulnerability calculation method to measure the vulnerability of data samples. Moreover, we propose a vulnerability-aware data filtering method to reduce the training data for adversarial training thus improve the training efficiency. The experiments are conducted in terms of adversarial training and robust neural architecture search on CIFAR-10, CIFAR-100, and ImageNet-1K. The results demonstrate that VDAT is up to 76% more efficient than state-of-the-art FAT methods, while achieving improvements regarding the natural accuracy and adversarial accuracy in both scenarios. Furthermore, the visualizations and ablation studies show the effectiveness of both core components designed in VDAT",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hlPk6Hi43e": {
    "title": "Tight analyses of first-order methods with error feedback",
    "volume": "poster",
    "abstract": "Communication between agents often constitutes a major computational bottleneck in distributed learning. One of the most common mitigation strategies is to compress the information exchanged, thereby reducing communication overhead. To counteract the degradation in convergence associated with compressed communication, error feedback schemes---most notably EF and EF21---were introduced. In this work, we provide a tight analysis of both of these methods. Specifically, we find the Lyapunov function that yields the best possible convergence rate for each method---with matching lower bounds. This principled approach yields sharp performance guarantees and enables a rigorous, apples-to-apples comparison between EF, EF21, and compressed gradient descent. Our analysis is carried out in the simplified single-agent setting, which allows for clean theoretical insights and fair comparison of the underlying mechanisms",
    "checked": true,
    "id": "e0caad62af49360080bcefa8ff222fea6e7d8623",
    "semantic_title": "tight analyses of first-order methods with error feedback",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ZUb4JpNoJe": {
    "title": "Cost-Sensitive Freeze-thaw Bayesian Optimization for Efficient Hyperparameter Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "b3d87bb7feb04c6a05d2927b0cfe5cca2061cdec",
    "semantic_title": "cost-sensitive freeze-thaw bayesian optimization for efficient hyperparameter tuning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yJS1eZSNUv": {
    "title": "Novel Exploration via Orthogonality",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "c35cd153d65b6a0f72f6fb398879344eb2150198",
    "semantic_title": "elden: exploration via local dependencies",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=NVDrWBwJTV": {
    "title": "The Good, the Bad and the Ugly: Meta-Analysis of Watermarks, Transferable Attacks and Adversarial Defenses",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F3DrgOZYc6": {
    "title": "Improved Algorithms for Overlapping and Robust Clustering of Edge-Colored Hypergraphs: An LP-Based Combinatorial Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BiowiwzQaO": {
    "title": "GoalLadder: Incremental Goal Discovery with Vision-Language Models",
    "volume": "poster",
    "abstract": "Natural language can offer a concise and human-interpretable means of specifying reinforcement learning (RL) tasks. The ability to extract rewards from a language instruction can enable the development of robotic systems that can learn from human guidance; however, it remains a challenging problem, especially in visual environments. Existing approaches that employ large, pretrained language models either rely on non‑visual environment representations, require prohibitively large amounts of feedback, or generate noisy, ill‑shaped reward functions. In this paper, we propose a novel method, GoalLadder, that leverages vision-language models (VLMs) to train RL agents from a single language instruction in visual environments. GoalLadder works by incrementally discovering states that bring the agent closer to completing a task specified in natural language. To do so, it queries a VLM to identify states that represent an improvement in agent's task progress and to rank them using pairwise comparisons. Unlike prior work, GoalLadder does not trust VLM's feedback completely; instead, it uses it to rank potential goal states using an ELO-based rating system, thus reducing the detrimental effects of noisy VLM feedback. Over the course of training, the agent is tasked with minimising the distance to the top-ranked goal in a learned embedding space, which is trained on unlabelled visual data. This key feature allows us to bypass the need for abundant and accurate feedback typically required to train a well-shaped reward function. We demonstrate that GoalLadder outperforms existing related methods on classic control and robotic manipulation environments with the average final success rate of $\\sim$95\\% compared to only $\\sim$45\\% of the best competitor",
    "checked": true,
    "id": "d8f8562ce63d9176d3e30db46e208c25ef6932a5",
    "semantic_title": "goalladder: incremental goal discovery with vision-language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=CB8jwNE2vV": {
    "title": "CADGrasp: Learning Contact and Collision Aware General Dexterous Grasping in Cluttered Scenes",
    "volume": "poster",
    "abstract": "Dexterous grasping in cluttered environments presents substantial challenges due to the high degrees of freedom of dexterous hands, occlusion, and potential collisions arising from diverse object geometries and complex layouts. To address these challenges, we propose CADGrasp, a two-stage algorithm for general dexterous grasping using single-view point cloud inputs. In the first stage, we predict a scene-decoupled, contact- and collision-aware representation—sparse IBS—as the optimization target. Sparse IBS compactly encodes the geometric and contact relationships between the dexterous hand and the scene, enabling stable and collision-free dexterous grasp pose optimization. To enhance the prediction of this high-dimensional representation, we introduce an occupancy-diffusion model with voxel-level conditional guidance and force closure score filtering. In the second stage, we develop several energy functions and ranking strategies for optimization based on sparse IBS to generate high-quality dexterous grasp poses. Extensive experiments in both simulated and real-world settings validate the effectiveness of our approach, demonstrating its capability to mitigate collisions while maintaining a high grasp success rate across diverse objects and complex scenes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DWf4vroKWJ": {
    "title": "Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning",
    "volume": "poster",
    "abstract": "The rapid emergence of diverse large language models (LLMs) has spurred the development of LLM routers that assign user queries to the most suitable model. However, existing LLM routers typically perform a single-round, one-to-one mapping (\\textit{i.e.}, assigning each query to a single model in isolation), which limits their capability to tackle complex tasks that demand the complementary strengths of multiple LLMs. In this paper, we present \\textbf{Router-R1}, a reinforcement learning (RL)-based framework that formulates multi-LLM routing and aggregation as a sequential decision process. Router-R1 instantiates the router itself as a capable LLM, leveraging its reasoning ability to interleave \"think\" actions (internal deliberation) with \"route\" actions (dynamic model invocation), and integrates each response into its evolving context. To facilitate learning, we employ a lightweight rule-based reward comprising format rewards, final outcome rewards, and a novel cost reward for optimizing the balance between performance and cost, opening a pathway toward enhancing performance-cost trade-offs via RL. Router-R1 also conditions only on simple model descriptors such as pricing, latency, and example performance, enabling strong generalization to unseen model selection. Experiments on seven general and multi-hop QA benchmarks show that Router-R1 outperforms several strong baselines, achieving superior performance while maintaining robust generalization and cost management",
    "checked": true,
    "id": "47ea295a79c58b04eb71278e1f48afa9650ebb2f",
    "semantic_title": "router-r1: teaching llms multi-round routing and aggregation via reinforcement learning",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=Z4eFqgYbha": {
    "title": "Structure-Aware Spectral Sparsification via Uniform Edge Sampling",
    "volume": "poster",
    "abstract": "Spectral clustering is a fundamental method for graph partitioning, but its reliance on eigenvector computation limits scalability to massive graphs. Classical sparsification methods preserve spectral properties by sampling edges proportionally to their effective resistances, but require expensive preprocessing to estimate these resistances. We study whether uniform edge sampling—a simple, structure-agnostic strategy—can suffice for spectral clustering. Our main result shows that for graphs admitting a well-separated $k$-clustering, characterized by a large structure ratio $\\Upsilon(k) = \\lambda_{k+1} / \\rho_G(k)$, uniform sampling preserves the spectral subspace used for clustering. Specifically, we prove that uniformly sampling $O(\\gamma^2 n \\log n / \\varepsilon^2)$ edges, where $\\gamma$ is the Laplacian condition number, yields a sparsifier whose top $(n-k)$-dimensional eigenspace is approximately orthogonal to the cluster indicators. This ensures that the spectral embedding remains faithful, and clustering quality is preserved. Our analysis introduces new resistance bounds for intra-cluster edges, a rank-$(n-k)$ effective resistance formulation, and a matrix Chernoff bound adapted to the dominant eigenspace. These tools allow us to bypass importance sampling entirely. Conceptually, our result connects recent coreset-based clustering theory to spectral sparsification, showing that under strong clusterability, even uniform sampling is structure-aware. This provides the first provable guarantee that uniform edge sampling suffices for structure-preserving spectral clustering",
    "checked": true,
    "id": "cdb6d5285b57c7f20628da6cd3da72138b6e3651",
    "semantic_title": "structure-aware spectral sparsification via uniform edge sampling",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B2iPEX5A9c": {
    "title": "Information-Theoretic Discrete Diffusion",
    "volume": "poster",
    "abstract": "We present an information-theoretic framework for discrete diffusion models that yields principled estimators of log-likelihood using score-matching losses. Inspired by the I-MMSE identity for the Gaussian setup, we derive analogous results for the discrete setting. Specifically, we introduce the Information–Minimum Denoising Score Entropy (I-MDSE) relation, which links mutual information between data and its diffused version to the minimum denoising score entropy (DSE) loss. We extend this theory to masked diffusion and establish the Information–Minimum Denoising Cross-Entropy (I-MDCE) relation, connecting cross-entropy losses to mutual information in discrete masked processes. These results provide a time-integral decomposition of the log-likelihood of the data in terms of optimal score-based losses, showing that commonly used losses such as DSE and DCE are not merely variational bounds but tight and principled estimators of log-likelihood. The I-MDCE decomposition further enables practical extensions, including time-free formula, conditional likelihood estimation in prompt–response tasks, and coupled Monte Carlo estimation of likelihood ratios. Experiments on synthetic and real-world data confirm the accuracy, variance stability, and utility of our estimators. The code is publicly available at https://github.com/Dongjae0324/infodis",
    "checked": true,
    "id": "687e686d4236cec1d6a8eaf5b632a2b1b9444159",
    "semantic_title": "information-theoretic discrete diffusion",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cNqMAmpZh4": {
    "title": "From Euler to AI: Unifying Formulas for Mathematical Constants",
    "volume": "poster",
    "abstract": "The constant $\\large \\pi$ has fascinated scholars throughout the centuries, inspiring numerous formulas for its evaluation, such as infinite sums and continued fractions. Despite their individual significance, many of the underlying connections among formulas remain unknown, missing unifying theories that could unveil deeper understanding. The absence of a unifying theory reflects a broader challenge across math and science: knowledge is typically accumulated through isolated discoveries, while deeper connections often remain hidden. In this work, we present an automated framework for the unification of mathematical formulas. Our system combines large language models (LLMs) for systematic formula harvesting, an LLM-code feedback loop for validation, and a novel symbolic algorithm for clustering and eventual unification. We demonstrate this methodology on the hallmark case of $\\large \\pi$, an ideal testing ground for symbolic unification. Applying this approach to 455,050 arXiv papers, we validate 385 distinct formulas for $\\large \\pi$ and prove relations between 360 (94\\%) of them, of which 166 (43\\%) can be derived from a single mathematical object—linking canonical formulas by Euler, Gauss, Brouncker, and newer ones from algorithmic discoveries by the Ramanujan Machine. Our method generalizes to other constants, including $e$, $\\zeta(3)$, and Catalan's constant, demonstrating the potential of AI-assisted mathematics to uncover hidden structures and unify knowledge across domains",
    "checked": true,
    "id": "acfccb9c5d60ed15016094671ac25cb48c2e1008",
    "semantic_title": "from euler to ai: unifying formulas for mathematical constants",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=1CqEAuRzHc": {
    "title": "CVGL: Causal Learning and Geometric Topology",
    "volume": "poster",
    "abstract": "Cross-view geo-localization (CVGL) aims to estimate the geographic location of a street image by matching it with a corresponding aerial image. This is critical for autonomous navigation and mapping in complex real-world scenarios. However, the task remains challenging due to significant viewpoint differences and the influence of confounding factors. To tackle these issues, we propose the Causal Learning and Geometric Topology (CLGT) framework, which integrates two key components: a Causal Feature Extractor (CFE) that mitigates the influence of confounding factors by leveraging causal intervention to encourage the model to focus on stable, task-relevant semantics; and a Geometric Topology Fusion (GT Fusion) module that injects Bird's Eye View (BEV) road topology into street features to alleviate cross-view inconsistencies caused by extreme perspective changes. Additionally, we introduce a Data-Adaptive Pooling (DA Pooling) module to enhance the representation of semantically rich regions. Extensive experiments on CVUSA, CVACT, and their robustness-enhanced variants (CVUSA-C-ALL and CVACT-C-ALL) demonstrate that CLGT achieves state-of-the-art performance, particularly under challenging real-world corruptions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l4F50jpiVH": {
    "title": "Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for Efficient LLM Deployment",
    "volume": "poster",
    "abstract": "We study weight-only post-training quantization (PTQ), which quantizes the weights of a large language model (LLM) without retraining, using little or no calibration data. Weight-only PTQ is crucial for reducing the memory footprint and latency of LLM inference, especially in memory-bound, small-batch inference scenarios, such as personalized inference on edge devices. Despite its importance, irregular weight distributions with heavy-tailed outliers in LLMs complicate quantization, recently motivating rotation-based methods that transform weights into near-Gaussian distributions, which are more regular with fewer outliers, thereby reducing quantization error. In this work, we first derive the information-theoretically optimal bit allocation for Gaussianized weights under given bit budgets, revealing that fine-grained fractional-bit quantizers approaching the Gaussian distortion-rate bound are essential to achieve near-optimal quantization performance. To bridge this theoretical insight and practical implementation, we introduce Q-Palette, a versatile collection of fractional-bit quantizers that range from trellis-coded quantizers offering near-optimal distortion to simpler vector and scalar quantizers optimized for faster inference, all efficiently implemented with optimized CUDA kernels across various bitwidths. Furthermore, leveraging Q-Palette as a foundational component, we propose a novel mixed-scheme quantization framework, jointly optimizing quantizer choices and layer fusion decisions given resource constraints. The code is available at https://github.com/snu-mllab/Q-Palette",
    "checked": true,
    "id": "db696a00207d020247d7a49e172ea55bf4e43613",
    "semantic_title": "q-palette: fractional-bit quantizers toward optimal bit allocation for efficient llm deployment",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pQ0D0vdjJv": {
    "title": "Neural Evolution Strategy for Black-box Pareto Set Learning",
    "volume": "poster",
    "abstract": "Multi-objective optimization problems (MOPs) are prevalent in numerous real-world applications. Recently, Pareto Set Learning (PSL) has emerged as a powerful paradigm for solving MOPs. PSL can produce a neural network for modeling the set of all Pareto optimal solutions. However, applying PSL to black-box objectives, particularly those exhibiting non-separability, high dimensionality, and/or other complex properties, remains very challenging. To address this issue, we propose leveraging evolution strategies (ESs), a class of specialized black-box optimization algorithms, within the PSL paradigm. Traditional ESs capture the complex dimensional dependencies less efficiently, which can significantly hinder their performance in PSL. To tackle this issue, we suggest encapsulating the dependencies within a neural network, which is then trained using a novel gradient estimation method. The proposed method, termed Neural-ES, is evaluated using a bespoke benchmark suite for black-box PSL. Experimental comparisons with other methods demonstrate the efficiency of Neural-ES, underscoring its ability to learn the Pareto sets of challenging black-box MOPs",
    "checked": false,
    "id": "b8940c25f5e07e842774a33cd1d6e2a8f6207133",
    "semantic_title": "neural-driven heuristic for strip packing trained with black-box optimization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=nZ4mFzCZIx": {
    "title": "Towards Generalizable 3D Human Pose Estimation via Ensembles on Flat Loss Landscapes",
    "volume": "poster",
    "abstract": "3D Human Pose Estimation (HPE) is a fundamental task in the computer vision. Generalization in 3D HPE task is crucial due to the need for robustness across diverse environments and datasets. Existing methods often focus on learning relationships between joints to enhance the generalization capability, but the role of the loss landscape, which is closely tied to generalization, remains underexplored. In this paper, we empirically visualize the loss landscape of the 3D HPE task, revealing its complexity and the challenges it poses for optimization. To address this, we first introduce a simple adaptive scaling mechanism that smooths the loss landscape. We further observe that different solutions on this smoothed loss landscape exhibit varying generalization behaviors. Based on this insight, we propose an efficient ensemble approach that combines diverse solutions on the smooth loss landscape induced by our adaptive scaling mechanism. Extensive experimental results demonstrate that our approach improves the generalization capability of 3D HPE models, and can be easily applied, regardless of model architecture, with consistent performance gains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gkcU26BOml": {
    "title": "Cross-modal Associations in Vision and Language Models: Revisiting the Bouba-Kiki Effect",
    "volume": "poster",
    "abstract": "Recent advances in multimodal models have raised questions about whether vision-and-language models (VLMs) integrate cross-modal information in ways that reflect human cognition. One well-studied test case in this domain is the bouba-kiki effect, where humans reliably associate pseudowords like ‘bouba' with round shapes and ‘kiki' with jagged ones. Given the mixed evidence found in prior studies for this effect in VLMs, we present a comprehensive re-evaluation focused on two variants of CLIP, ResNet and Vision Transformer (ViT), given their centrality in many state-of-the-art VLMs. We apply two complementary methods closely modelled after human experiments: a prompt-based evaluation that uses probabilities as a measure of model preference, and we use Grad-CAM as a novel approach to interpret visual attention in shape-word matching tasks. Our findings show that these model variants do not consistently exhibit the bouba-kiki effect. While ResNet shows a preference for round shapes, overall performance across both model variants lacks the expected associations. Moreover, direct comparison with prior human data on the same task shows that the models' responses fall markedly short of the robust, modality-integrated behaviour characteristic of human cognition. These results contribute to the ongoing debate about the extent to which VLMs truly understand cross-modal concepts, highlighting limitations in their internal representations and alignment with human intuitions",
    "checked": true,
    "id": "c240515f65866c6f8f792cb6084a689c0f89efd4",
    "semantic_title": "cross-modal associations in vision and language models: revisiting the bouba-kiki effect",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=615vk8hmeH": {
    "title": "How Data Mixing Shapes In-Context Learning: Asymptotic Equivalence for Transformers with MLPs",
    "volume": "poster",
    "abstract": "Pretrained Transformers demonstrate remarkable in-context learning (ICL) capabilities, enabling them to adapt to new tasks from demonstrations without parameter updates. However, theoretical studies often rely on simplified architectures (e.g., omitting MLPs), data models (e.g., linear regression with isotropic inputs), and single-source training—limiting their relevance to realistic settings. In this work, we study ICL in pretrained Transformers with nonlinear MLP heads on nonlinear tasks drawn from multiple data sources with heterogeneous input, task, and noise distributions. We analyze a model where the MLP comprises two layers, with the first layer trained via a single gradient step and the second layer fully optimized. Under high-dimensional asymptotics, we prove that such models are equivalent in ICL error to structured polynomial predictors, leveraging results from the theory of Gaussian universality and orthogonal polynomials. This equivalence reveals that nonlinear MLPs meaningfully enhance ICL performance—particularly on nonlinear tasks—compared to linear baselines. It also enables a precise analysis of data mixing effects: we identify key properties of high-quality data sources (low noise, structured covariances) and show that feature learning emerges only when the task covariance exhibits sufficient structure. These results are validated empirically across various activation functions, model sizes, and data distributions. Finally, we experiment with a real-world scenario involving multilingual sentiment analysis where each language is treated as a different source. Our experimental results for this case exemplify how our findings extend to real-world cases. Overall, our work advances the theoretical foundations of ICL in Transformers and provides actionable insight into the role of architecture and data in ICL",
    "checked": true,
    "id": "fa3e4782fced9d8fb97ab956109fdc17b5ffe109",
    "semantic_title": "how data mixing shapes in-context learning: asymptotic equivalence for transformers with mlps",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5GaDcRVgBw": {
    "title": "MASTER: Enhancing Large Language Model via Multi-Agent Simulated Teaching",
    "volume": "poster",
    "abstract": "Instruction fine-tuning is crucial in NLP tasks, enhancing pretrained models' instruction-following capabilities and task-specific performance. However, obtaining high-quality fine-tuning data for large models is challenging due to data collection difficulties and high production costs. To address this, we propose MASTER, a novel data augmentation method that enriches original data through interactions among multiple agents with varying cognitive levels. We simulate three pedagogically grounded teaching scenarios, leveraging multi-agent conversations to generate high-quality teacher-student interaction data. Utilizing MASTER, we construct BOOST-QA, a fine-tuning dataset augmented from existing datasets like Orca-Math-200k, ProcQA, and OpenHermes2.5. Experiments show that models fine-tuned with BOOST-QA perform excellently across multiple benchmarks, demonstrating strong multitask generalization. Notably, MASTER significantly improves models' reasoning abilities in complex tasks, providing valuable insights for future research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5bu1IOOvf0": {
    "title": "Self-Supervised Learning of Graph Representations for Network Intrusion Detection",
    "volume": "poster",
    "abstract": "Detecting intrusions in network traffic is a challenging task, particularly under limited supervision and constantly evolving attack patterns. While recent works have leveraged graph neural networks for network intrusion detection, they often decouple representation learning from anomaly detection, limiting the utility of the embeddings for identifying attacks. We propose GraphIDS, a self-supervised intrusion detection model that unifies these two stages by learning local graph representations of normal communication patterns through a masked autoencoder. An inductive graph neural network embeds each flow with its local topological context to capture typical network behavior, while a Transformer‑based encoder-decoder reconstructs these embeddings, implicitly learning global co-occurrence patterns via self-attention without requiring explicit positional information. During inference, flows with unusually high reconstruction errors are flagged as potential intrusions. This end-to-end framework ensures that embeddings are directly optimized for the downstream task, facilitating the recognition of malicious traffic. On diverse NetFlow benchmarks, GraphIDS achieves up to 99.98% PR‑AUC and 99.61% macro F1-score, outperforming baselines by 5–25 percentage points",
    "checked": true,
    "id": "1ec21b1e277049c330e2cb8c052de684f562a75b",
    "semantic_title": "self-supervised learning of graph representations for network intrusion detection",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hJKDwf32Xu": {
    "title": "Kernel conditional tests from learning-theoretic bounds",
    "volume": "poster",
    "abstract": "We propose a framework for hypothesis testing on conditional probability distributions, which we then use to construct *statistical tests of functionals of conditional distributions*. These tests identify the inputs where the functionals differ with high probability, and include tests of conditional moments or two-sample tests. Our key idea is to transform confidence bounds of a learning method into a test of conditional expectations. We instantiate this principle for kernel ridge regression (KRR) with subgaussian noise. An intermediate data embedding then enables more general tests — including *conditional two-sample tests* — via kernel mean embeddings of distributions. To have guarantees in this setting, we generalize existing pointwise-in-time or time-uniform confidence bounds for KRR to previously-inaccessible yet essential cases such as infinite-dimensional outputs with non-trace-class kernels. These bounds also circumvent the need for independent data, allowing for instance online sampling. To make our tests readily applicable in practice, we introduce bootstrapping schemes leveraging the parametric form of testing thresholds identified in theory to avoid tuning inaccessible parameters. We illustrate the tests on examples, including one in process monitoring and comparison of dynamical systems. Overall, our results establish a comprehensive foundation for conditional testing on functionals, from theoretical guarantees to an algorithmic implementation, and advance the state of the art on confidence bounds for vector-valued least squares estimation",
    "checked": true,
    "id": "152496cbbbbc5caae29b5f22765819cb184ff5de",
    "semantic_title": "kernel conditional tests from learning-theoretic bounds",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OR5WyyTESh": {
    "title": "SPOT: Scalable Policy Optimization with Trees for Markov Decision Processes",
    "volume": "poster",
    "abstract": "Interpretable reinforcement learning policies are essential for high-stakes decision-making, yet optimizing decision tree policies in Markov Decision Processes (MDPs) remains challenging. We propose SPOT, a novel method for computing decision tree policies, which formulates the optimization problem as a mixed-integer linear program (MILP). To enhance efficiency, we employ a reduced-space branch-and-bound approach that decouples the MDP dynamics from tree-structure constraints, enabling efficient parallel search. This significantly improves runtime and scalability compared to previous methods. Our approach ensures that each iteration yields the optimal decision tree. Experimental results on standard benchmarks demonstrate that SPOT achieves substantial speedup and scales to larger MDPs with a significantly higher number of states. The resulting decision tree policies are interpretable and compact, maintaining transparency without compromising performance. These results demonstrate that our approach simultaneously achieves interpretability and scalability, delivering high-quality policies an order of magnitude faster than existing approaches",
    "checked": true,
    "id": "ef4e848a1d7e75c226b0abb2779bcc0749bd30cd",
    "semantic_title": "spot: scalable policy optimization with trees for markov decision processes",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=geNdDlzKTG": {
    "title": "Thinking in Character: Advancing Role-Playing Agents with Role-Aware Reasoning",
    "volume": "poster",
    "abstract": "The advancement of Large Language Models (LLMs) has spurred significant interest in Role-Playing Agents (RPAs) for applications such as emotional companionship and virtual interaction. However, recent RPAs are often built on explicit dialogue data, lacking deep, human-like internal thought processes, resulting in superficial knowledge and style expression. While Large Reasoning Models (LRMs) can be employed to simulate character thought, their direct application is hindered by attention diversion (i.e., RPAs forget their role) and style drift (i.e., overly formal and rigid reasoning rather than character-consistent reasoning). To address these challenges, this paper introduces a novel Role-Aware Reasoning (RAR) method, which consists of two important stages: Role Identity Activation (RIA) and Reasoning Style Optimization (RSO). RIA explicitly guides the model with character profiles during reasoning to counteract attention diversion, and then RSO aligns reasoning style with the character and scene via LRM distillation to mitigate style drift. Extensive experiments demonstrate that the proposed RAR significantly enhances the performance of RPAs by effectively addressing attention diversion and style drift",
    "checked": true,
    "id": "0a646808002d3a629c60330178cb5b85fa6c77dd",
    "semantic_title": "thinking in character: advancing role-playing agents with role-aware reasoning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=erwwuMhTJX": {
    "title": "Cyclic Counterfactuals under Shift–Scale Interventions",
    "volume": "poster",
    "abstract": "Most counterfactual inference frameworks traditionally assume acyclic structural causal models (SCMs), i.e. directed acyclic graphs (DAGs). However, many real-world systems (e.g. biological systems) contain feedback loops or cyclic dependencies that violate acyclicity. In this work, we study counterfactual inference in cyclic SCMs under shift–scale interventions, i.e., soft, policy-style changes that rescale and/or shift a variable's mechanism",
    "checked": false,
    "id": "011bcefaac8616668eb1cfb161b85438da3aecab",
    "semantic_title": "cyclic counterfactuals under shift-scale interventions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VwsXmcMyg5": {
    "title": "SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification",
    "volume": "poster",
    "abstract": "Large language models with Mixture-of-Experts (MoE) architectures achieve efficiency and scalability, yet their routing mechanisms introduce safety alignment challenges insufficiently addressed by techniques developed for dense models. In this work, the MoE-specific safety risk of positional vulnerability—that safety-aligned behaviors rely on specific expert modules—is formalized and systematically analyzed. An analytical framework, SAFEx, is presented to robustly identify, characterize, and validate safety-critical experts via a stability-based expert selection procedure, and to decompose them into two functional groups: the Harmful Content Detection Group (HCDG), which specializes in identifying and recognizing harmful content within user inputs, and the Harmful Response Control Group (HRCG), which specializes in controlling and enforcing model behaviors to generate appropriate safety responses. Expert-level interventions are conducted to probe causality and to test mitigation. Targeted masking of SAFEx-selected experts reveals that safety behavior is highly concentrated. On Qwen3-30B-A3B, configured with 48 MoE-FFN layers and 128 experts per layer under top-8 routing (48×128=6,144 experts in total), disabling 12 selected experts reduces the refusal rate by 22%. In addition, lightweight adaptation is performed using LoRA under three configurations—the HRCG, the union of HCDG and HRCG, and all experts—and the resulting updates are composed through negative weight merging targeted at the HRCG, leading to improved refusal under adversarial prompts without full-model retraining. These results establish positional vulnerability as a distinct MoE-specific safety challenge and provide a practical, compute-efficient pathway for expert-level safety interventions within routed architectures",
    "checked": true,
    "id": "85991051bdbe5342a19e7beff050e51228574522",
    "semantic_title": "safex: analyzing vulnerabilities of moe-based llms via stable safety-critical expert identification",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=cCefuzQrjK": {
    "title": "A geometric framework for momentum-based optimizers for low-rank training",
    "volume": "poster",
    "abstract": "Low-rank pre-training and fine-tuning have recently emerged as promising techniques for reducing the computational and storage costs of large neural networks. Training low-rank parameterizations typically relies on conventional optimizers such as heavy ball momentum methods or Adam. In this work, we identify and analyze potential difficulties that these training methods encounter when used to train low-rank parameterizations of weights. In particular, we show that classical momentum methods can struggle to converge to a local optimum due to the geometry of the underlying optimization landscape. To address this, we introduce novel training strategies derived from dynamical low-rank approximation, which explicitly account for the underlying geometric structure. Our approach leverages and combines tools from dynamical low-rank approximation and momentum-based optimization to design optimizers that respect the intrinsic geometry of the parameter space. We validate our methods through numerical experiments, demonstrating faster convergence, and stronger validation metrics at given parameter budgets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iBFfb6bGOz": {
    "title": "Atomic Thinking of LLMs: Decoupling and Exploring Mathematical Reasoning Abilities",
    "volume": "poster",
    "abstract": "Large Language Models (LLMs) have demonstrated outstanding performance in mathematical reasoning capabilities. However, we argue that current large-scale reasoning models primarily rely on scaling up training datasets with diverse mathematical problems and long thinking chains, which raises questions about whether LLMs genuinely acquire mathematical concepts and reasoning principles or merely remember the training data. In contrast, humans tend to break down complex problems into multiple fundamental atomic capabilities. Inspired by this, we propose a new paradigm for evaluating mathematical atomic capabilities. Our work categorizes atomic abilities into two dimensions: (1) field-specific abilities across four major mathematical fields, algebra, geometry, analysis, and topology, and (2) logical abilities at different levels, including conceptual understanding, forward multi-step reasoning with formal math language, and counterexample-driven backward reasoning. We propose corresponding training and evaluation datasets for each atomic capability unit, and conduct extensive experiments about how different atomic capabilities influence others, to explore the strategies to elicit the required specific atomic capability. Evaluation and experimental results on advanced models show many interesting discoveries and inspirations about the different performances of models on various atomic capabilities and the interactions between atomic capabilities. Our findings highlight the importance of decoupling mathematical intelligence into atomic components, providing new insights into model cognition and guiding the development of training strategies toward a more efficient, transferable, and cognitively grounded paradigm of \"atomic thinking\"",
    "checked": true,
    "id": "05f13609d4b167a10ad10259721e99a3e72c3178",
    "semantic_title": "atomic thinking of llms: decoupling and exploring mathematical reasoning abilities",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=dVIx32Lq7J": {
    "title": "Unveiling Concept Attribution in Diffusion Models",
    "volume": "poster",
    "abstract": "Diffusion models have shown remarkable abilities in generating realistic and high-quality images from text prompts. However, a trained model remains largely black-box; little do we know about the roles of its components in exhibiting a concept such as objects or styles. Recent works employ causal tracing to localize knowledge-storing layers in generative models without showing how other layers contribute to the target concept. In this work, we approach diffusion models' interpretability problem from a more general perspective and pose a question: \\textit{``How do model components work jointly to demonstrate knowledge?''}. To answer this question, we decompose diffusion models using component attribution, systematically unveiling the importance of each component (specifically the model parameter) in generating a concept. The proposed framework, called \\textbf{C}omponent \\textbf{A}ttribution for \\textbf{D}iffusion Model (CAD), discovers the localization of concept-inducing (positive) components, while interestingly uncovers another type of components that contribute negatively to generating a concept, which is missing in the previous knowledge localization work. Based on this holistic understanding of diffusion models, we present and empirically evaluate one utility of component attribution in controlling the generation process. Specifically, we introduce two fast, inference-time model editing algorithms, CAD-Erase and CAD-Amplify; in particular, CAD-Erase enables erasure and CAD-Amplify allows amplification of a generated concept by ablating the positive and negative components, respectively, while retaining knowledge of other concepts. Extensive experimental results validate the significance of both positive and negative components pinpointed by our framework, demonstrating the potential of providing a complete view of interpreting generative models",
    "checked": true,
    "id": "9a765999784b8e7d3071b10c95fc517e6c08ed5f",
    "semantic_title": "unveiling concept attribution in diffusion models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=OM0Qkq9xtY": {
    "title": "Brain network science modelling of sparse neural networks enables Transformers and LLMs to perform as fully connected",
    "volume": "poster",
    "abstract": "This study aims to enlarge our current knowledge on the application of brain-inspired network science principles for training artificial neural networks (ANNs) with sparse connectivity. Dynamic sparse training (DST) emulates the synaptic turnover of real brain networks, reducing the computational demands of training and inference in ANNs. However, existing DST methods face difficulties in maintaining peak performance at high connectivity sparsity levels. The Cannistraci-Hebb training (CHT) is a brain-inspired method that is used in DST for growing synaptic connectivity in sparse neural networks. CHT leverages a gradient-free, topology-driven link regrowth mechanism, which has been shown to achieve ultra-sparse (1\\% connectivity or lower) advantage across various tasks compared to fully connected networks. Yet, CHT suffers two main drawbacks: (i) its time complexity is $\\mathcal{O}(N\\cdot d^3)$- N node network size, d node degree - hence it can be efficiently applied only to ultra-sparse networks. (ii) it rigidly selects top link prediction scores, which is inappropriate for the early training epochs, when the network topology presents many unreliable connections. Here, we design the first brain-inspired network model - termed bipartite receptive field (BRF) - to initialize the connectivity of sparse artificial neural networks. Then, we propose a matrix multiplication GPU-friendly approximation of the CH link predictor, which reduces the computational complexity to $\\mathcal{O}(N^3)$, enabling a fast implementation of link prediction in large-scale models. Moreover, we introduce the Cannistraci-Hebb training soft rule (CHTs), which adopts a flexible strategy for sampling connections in both link removal and regrowth, balancing the exploration and exploitation of network topology. Additionally, we propose a sigmoid-based gradual density decay strategy, leading to an advanced framework referred to as CHTss. Empirical results show that BRF offers performance advantages over previous network science models. Using 1\\% of connections, CHTs outperforms fully connected networks in MLP architectures on visual classification tasks, compressing some networks to less than 30\\% of the nodes. Using 5\\% of the connections, CHTss outperforms fully connected networks in two Transformer-based machine translation tasks. Finally, with only 30\\% of the connections, both CHTs and CHTss achieve superior performance over other dynamic sparse training methods, and perform on par with—or even surpass—their fully connected counterparts in language modeling across various sparsity levels within the LLaMA model family. The code is available at: https://github.com/biomedical-cybernetics/Cannistraci-Hebb-training",
    "checked": true,
    "id": "351a03eeeee64cba5815484eedbbc21eeddbe050",
    "semantic_title": "brain network science modelling of sparse neural networks enables transformers and llms to perform as fully connected",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=00Bwl1woOJ": {
    "title": "Uncertainty-Sensitive Privileged Learning",
    "volume": "poster",
    "abstract": "Privileged learning efficiently tackles high-dimensional, partially observable decision-making problems by first training a privileged policy (PP) on low-dimensional privileged observations, and then deriving a deployment policy (DP) either by imitating the PP or coupling it with an observation encoder. However, since the DP relies on local and partial observations, a behavioral divergence (BD) often emerges between the DP and the PP, ultimately degrading deployment performance. A promising strategy is to train a PP to learn the optimal behaviors attainable under the DP's observation space by applying reward penalties in regions with large BD. However, producing these behaviors is challenging for the PP because they rely on the DP's information-gathering progress, which is invisible to the PP. In this paper, we quantify the DP's information-gathering progress by estimating the prediction uncertainty of privileged observations reconstructed from partial observations, and accordingly propose the framework of Uncertainty-Sensitive Privileged Learning (USPL). USPL feeds this uncertainty estimation to the PP and combines reward transformation with privileged-observation blurring, driving the PP to choose actions that actively reduce uncertainty and thus gather the necessary information. Experiments across nine tasks demonstrate that USPL significantly reduces the behavioral discrepancies, achieving superior deployment performance compared to baselines. Additional visualization results show that the DP accurately quantifies its uncertainty, and the PP effectively adapts to uncertainty variations. Code is available at https://github.com/FanmingL/USPL",
    "checked": false,
    "id": "a776a0e164aa95b553ff0c24033d7b684e153da1",
    "semantic_title": "group fairness with uncertainty in sensitive attributes",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=qJLPlZSdkb": {
    "title": "Joint Modeling of fMRI and EEG Imaging Using Ordinary Differential Equation-Based Hypergraph Neural Networks",
    "volume": "poster",
    "abstract": "Fusing multimodal brain imaging has been a hot topic since different modalities of brain imaging can provide complementary information. However, due to the size of simultaneous recorded fMRI-EEG dataset being limited and the substantial discrepancy between hemodynamic responses of fMRI and neural oscillations of EEG, the joint modeling of fMRI and EEG images is a rarely explored area and has not yielded satisfactory results. Existing studies have also indicated that the relationships between region of interest (ROI) are not one-to-one when synchronizing fMRI and EEG. Current graph-based multimodal modeling methods overlook those information. Based on this, we propose a hypergraph based fMRI-EEG modeling framework for asynchronous fMRI-EEG data named FE-NET. To the best of our knowledge, this is the first attempt to jointly model asynchronous EEG and fMRI data as Neural ODEs based hypergraph. Extensive experiments have demonstrated that the proposed FE-NET outperforms many state-of-the-art brain imaging modeling methods. Meanwhile, compared to simultaneously recorded fMRI-EEG data, asynchronously acquired fMRI-EEG data is less costly, which demonstrates the practical applicability of our method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rew03VaNUJ": {
    "title": "Improving Target Sound Extraction via Disentangled Codec Representations with Privileged Knowledge Distillation",
    "volume": "poster",
    "abstract": "Target sound extraction aims to isolate target sound sources from an input mixture using a target clue to identify the sounds of interest. To address the challenge posed by the wide variety of sounds, recent work has introduced privileged knowledge distillation (PKD), which utilizes privileged information (PI) about the target sound, available only during training. While PKD has shown promise, existing approaches often suffer from overfitting of the teacher model for the overly rich PI and ineffective knowledge transfer to the student model. In this paper, we propose Disentangled Codec Knowledge Distillation (DCKD) to mitigate these issues by regulating the amount and the flow of target sound information within the teacher model. We begin by extracting a compressed representation of the target sound using a neural audio codec to regulate the amount of PI. Disentangled representation learning is then applied to remove class information and extract fine-grained temporal information as PI. Subsequently, an n-hot vector as the class information and the class-independent PI are used to condition the early and later layers of the teacher model, respectively, forming a regulated coarse-to-fine target information flow. The resulting representation is transferred to the student model through feature-level knowledge distillation. Experimental results show that DCKD consistently improves existing methods across model architectures under the multi-target selection condition",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bsska2ayiy": {
    "title": "MLEP: Multi-granularity Local Entropy Patterns for Generalized AI-generated Image Detection",
    "volume": "poster",
    "abstract": "Advances in image generation technologies have raised growing concerns about their potential misuse, particularly in producing misinformation and deepfakes. This creates an urgent demand for effective methods to detect AI-generated images (AIGIs). While progress has been made, achieving reliable performance across diverse generative models and scenarios remains challenging due to the absence of source-invariant features and the limited generalization of existing approaches. In this study, we investigate the potential of using image entropy as a discriminative cue for AIGI detection and propose Multi-granularity Local Entropy Patterns (MLEP), a set of feature maps computed based on Shannon entropy from shuffled small patches at multiple image scales. MLEP effectively captures pixel dependencies across scales and dimensions while disrupting semantic content, thereby reducing potential content bias. Based on MLEP, we can easily build a robust CNN-based classifier capable of detecting AIGIs with enhanced reliability. Extensive experiments in an open-world setting, involving images synthesized by 32 distinct generative models, demonstrate that our approach achieves substantial improvements over state-of-the-art methods in both accuracy and generalization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w1TjXJk846": {
    "title": "Reasoning Models Sometimes Output Illegible Chains of Thought",
    "volume": "poster",
    "abstract": "Language models trained via outcome-based reinforcement learning (RL) to reason using chain-of-thought (CoT) have shown remarkable performance. Monitoring such a model's CoT may allow us to understand its intentions and detect potential malicious behavior. However, to be effective, this requires that CoTs are legible and faithful. We evaluate the legibility of CoTs in state-of-the-art reasoning models. We find that R1, R1-Zero, and QwQ often produce illegible CoTs (mixing nonsensical phrases, random words, and non-English characters) before returning to perfect coherence in their final responses, while Claude models notably exhibit higher legibility. Across 14 models, we observe that larger models within the same training paradigm tend to produce more illegible reasoning. Prefill experiments show that truncating reasoning at a legibility threshold reduces accuracy by 53\\%, suggesting that illegible portions contribute to performance despite being difficult to monitor. Illegibility increases with question difficulty, suggesting that CoT monitoring may be less reliable precisely when most needed. We discuss potential hypotheses for these results, including steganography, vestigial tokens, and training artifacts. Our findings suggest that current approaches to CoT monitoring may be vulnerable to the emergence of outcome-based RL, particularly as models face increasingly complex tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9PL1DIIB7e": {
    "title": "JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model",
    "volume": "poster",
    "abstract": "Large language models (LLMs) have revolutionized natural language processing and are increasingly applied to other sequential data types, including genetic sequences. However, adapting LLMs to genetics presents significant challenges. Capturing complex genomic interactions requires modeling long-range global dependencies within DNA sequences, where interactions often span over 10,000 base pairs, even within a single gene. This poses substantial computational demands under conventional model architectures and training paradigms. Additionally, traditional LLM training approaches are suboptimal for DNA sequences: autoregressive training, while efficient for training, only supports unidirectional sequence understanding. However, DNA is inherently bidirectional. For instance, bidirectional promoters regulate gene expression in both directions and govern approximately 11% of human gene expression. Masked language models (MLMs) enable bidirectional understanding. However, they are inefficient since only masked tokens contribute to loss calculations at each training step. To address these limitations, we introduce JanusDNA, the first bidirectional DNA foundation model built upon a novel pretraining paradigm, integrating the optimization efficiency of autoregressive modeling with the bidirectional comprehension capability of masked modeling. JanusDNA's architecture leverages a Mamba-Attention Mixture-of-Experts (MoE) design, combining the global, high-resolution context awareness of attention mechanisms with the efficient sequential representation learning capabilities of Mamba. The MoE layers further enhance the model's capacity through sparse parameter scaling, while maintaining manageable computational costs. Notably, JanusDNA can process up to 1 million base pairs at single-nucleotide resolution on a single 80GB GPU using its hybrid architecture. Extensive experiments and ablation studies demonstrate that JanusDNA achieves new state-of-the-art performance on three genomic representation benchmarks. Remarkably, JanusDNA surpasses models with 250x more activated parameters, underscoring its efficiency and effectiveness. Code available at https://anonymous.4open.science/r/JanusDNA/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B5mEYUJi85": {
    "title": "PubSub-VFL: Towards Efficient Two-Party Split Learning in Heterogeneous Environments via Publisher/Subscriber Architecture",
    "volume": "poster",
    "abstract": "With the rapid advancement of the digital economy, data collaboration between organizations has become a well-established business model, driving the growth of various industries. However, privacy concerns make direct data sharing impractical. To address this, Two-Party Split Learning (a.k.a. Vertical Federated Learning (VFL)) has emerged as a promising solution for secure collaborative learning. Despite its advantages, this architecture still suffers from low computational resource utilization and training efficiency. Specifically, its synchronous dependency design increases training latency, while resource and data heterogeneity among participants further hinder efficient computation. To overcome these challenges, we propose \\texttt{PubSub-VFL}, a novel VFL paradigm with a Publisher/Subscriber architecture optimized for two-party collaborative learning with high computational efficiency. \\texttt{PubSub-VFL} leverages the decoupling capabilities of the Pub/Sub architecture and the data parallelism of the parameter server architecture to design a hierarchical asynchronous mechanism, reducing training latency and improving system efficiency. Additionally, to mitigate the training imbalance caused by resource and data heterogeneity, we formalize an optimization problem based on participants' system profiles, enabling the selection of optimal hyperparameters while preserving privacy. We conduct a theoretical analysis to demonstrate that \\texttt{PubSub-VFL} achieves stable convergence and is compatible with security protocols such as differential privacy. Extensive case studies on five benchmark datasets further validate its effectiveness, showing that \\texttt{PubSub-VFL} compared to state-of-the-art baselines not only accelerates training by $2 \\sim 7\\times$ without compromising accuracy but also achieves computational resource utilization by up to 91.07\\%",
    "checked": true,
    "id": "46a5464a2ce87cb5424bddea6e14a27a10b33fb8",
    "semantic_title": "pubsub-vfl: towards efficient two-party split learning in heterogeneous environments via publisher/subscriber architecture",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wVxIBvUAlj": {
    "title": "Bohdi: Heterogeneous LLM Fusion with Automatic Data Exploration",
    "volume": "poster",
    "abstract": "Heterogeneous Large Language Model (LLM) fusion integrates the strengths of multiple source LLMs with different architectures into a target LLM with low computational overhead. While promising, existing methods suffer from two major limitations: 1) **reliance on real data from limited domain** for knowledge fusion, preventing the target LLM from fully acquiring knowledge across diverse domains, and 2) **fixed data allocation proportions** across domains, failing to dynamically adjust according to the target LLM's varying capabilities across domains, leading to a capability imbalance. To overcome these limitations, we propose Bohdi, a synthetic-data-only heterogeneous LLM fusion framework. Through the organization of knowledge domains into a hierarchical tree structure, Bohdi enables automatic domain exploration and multi-domain data generation through multi-model collaboration, thereby comprehensively extracting knowledge from source LLMs. By formalizing domain expansion and data sampling proportion allocation on the knowledge tree as a Hierarchical Multi-Armed Bandit problem, Bohdi leverages the designed DynaBranches mechanism to adaptively adjust sampling proportions based on the target LLM's performance feedback across domains. Integrated with our proposed Introspection-Rebirth (IR) mechanism, DynaBranches dynamically tracks capability shifts during target LLM's updates via Sliding Window Binomial Likelihood Ratio Testing (SWBLRT), further enhancing its online adaptation capability. Comparative experimental results on a comprehensive suite of benchmarks demonstrate that Bohdi significantly outperforms existing baselines on multiple target LLMs, exhibits higher data efficiency, and virtually eliminates the imbalance in the target LLM's capabilities",
    "checked": true,
    "id": "206685eb3ad285d89ec9047890f5facde15db8c2",
    "semantic_title": "bohdi: heterogeneous llm fusion with automatic data exploration",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vMkJWaa02n": {
    "title": "How Does Sequence Modeling Architecture Influence Base Capabilities of Pre-trained Language Models? Exploring Key Architecture Design Principles to Avoid Base Capabilities Degradation",
    "volume": "poster",
    "abstract": "Pre-trained language models represented by the Transformer have been proven to possess strong base capabilities, and the representative self-attention mechanism in the Transformer has become a classic in sequence modeling architectures. Different from the work of proposing sequence modeling architecture to improve the efficiency of attention mechanism, this work focuses on the impact of sequence modeling architectures on base capabilities. Specifically, our concern is: How exactly do sequence modeling architectures affect the base capabilities of pre-trained language models? In this work, we first point out that the mixed domain pre-training setting commonly adopted in existing architecture design works fails to adequately reveal the differences in base capabilities among various architectures. To address this, we propose a limited domain pre-training setting with out-of-distribution testing, which successfully uncovers significant differences in base capabilities among architectures at an early stage. Next, we analyze the base capabilities of stateful sequence modeling architectures, and find that they exhibit significant degradation in base capabilities compared to the Transformer. Then, through a series of architecture component analysis, we summarize a key architecture design principle: A sequence modeling architecture need possess full-sequence arbitrary selection capability to avoid degradation in base capabilities. Finally, we empirically validate this principle using an extremely simple Top-1 element selection architecture and further generalize it to a more practical Top-1 chunk selection architecture. Experimental results demonstrate our proposed sequence modeling architecture design principle and suggest that our work can serve as a valuable reference for future architecture improvements and novel designs",
    "checked": true,
    "id": "0bea974897420b053a5a79f5710ad3724d2c70d4",
    "semantic_title": "how does sequence modeling architecture influence base capabilities of pre-trained language models? exploring key architecture design principles to avoid base capabilities degradation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L84DdFuvwV": {
    "title": "Inverse Methods for Missing Data Imputation",
    "volume": "poster",
    "abstract": "Iterative imputation is a prevalent method for completing missing data, which involves iteratively imputing each feature by treating it as a target variable and predicting its missing values using the remaining features. However, existing iterative imputation methods exhibit two critical defects: (1) model misspecification, where a uniform parametric form of model is applied across different features, conflicting with heterogeneous data generation processes; (2) underuse of oracle features, where all features are treated as potentially missing, neglecting the valuable information in fully observed features. In this work, we propose kernel point imputation (KPI), a bi-level optimization framework designed to address these issues. The inner-level optimization optimizes the model form for each feature in a reproducing kernel Hilbert space, mitigating model misspecification. The outer-level optimization leverages oracle features as supervision signals to refine imputations. Extensive experiments on real-world datasets demonstrate that KPI consistently outperforms state-of-the-art imputation methods. Code is available at https://github.com/FMLYD/kpi.git",
    "checked": false,
    "id": "0a91c0e93c41049e7a27b54aa481d83c2601325b",
    "semantic_title": "a comparison of different methods for rainfall imputation: a galician case study",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=14B5d6NEaH": {
    "title": "Reinforcement Learning with Backtracking Feedback",
    "volume": "poster",
    "abstract": "Addressing the critical need for robust safety in Large Language Models (LLMs), particularly against adversarial attacks and in-distribution errors, we introduce Reinforcement Learning with Backtracking Feedback (RLBF). This framework advances upon prior methods, such as BSAFE, by primarily leveraging a Reinforcement Learning (RL) stage where models learn to dynamically correct their own generation errors. Through RL with critic feedback on the model's live outputs, LLMs are trained to identify and recover from their actual, emergent safety violations by emitting an efficient \"backtrack by x tokens\" signal, then continuing generation autoregressively. This RL process is crucial for instilling resilience against sophisticated adversarial strategies, including middle filling, Greedy Coordinate Gradient (GCG) attacks, and decoding parameter manipulations. To further support the acquisition of this backtracking capability, we also propose an enhanced Supervised Fine-Tuning (SFT) data generation strategy (BSAFE+). This method improves upon previous data creation techniques by injecting violations into coherent, originally safe text, providing more effective initial training for the backtracking mechanism. Comprehensive empirical evaluations demonstrate that RLBF significantly reduces attack success rates across diverse benchmarks and model scales, achieving superior safety outcomes while critically preserving foundational model utility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XxRKqFsvoK": {
    "title": "Exploring Landscapes for Better Minima along Valleys",
    "volume": "poster",
    "abstract": "Finding lower and better-generalizing minima is crucial for deep learning. However, most existing optimizers stop searching the parameter space once they reach a local minimum. Given the complex geometric properties of the loss landscape, it is difficult to guarantee that such a point is the lowest or provides the best generalization. To address this, we propose an adaptor \"E\" for gradient-based optimizers. The adapted optimizer tends to continue exploring along landscape valleys (areas with low and nearly identical losses) in order to search for potentially better local minima even after reaching a local minimum. This approach increases the likelihood of finding a lower and flatter local minimum, which is often associated with better generalization. We also provide a proof of convergence for the adapted optimizers in both convex and non-convex scenarios for completeness. Finally, we demonstrate their effectiveness in an important but notoriously difficult training scenario, large-minibatch training, where Lamb is the benchmark optimizer. Our testing results show that the adapted Lamb, ALTO, increases the test accuracy (generalization) of the current state-of-the-art optimizer by an average of 2.5\\% across a variety of large-batch training tasks. This work potentially opens a new research direction in the design of optimization algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OIH3T5ZPBW": {
    "title": "Safe RLHF-V: Safe Reinforcement Learning from Multi-modal Human Feedback",
    "volume": "poster",
    "abstract": "Multimodal large language models (MLLMs) are essential for building general-purpose AI assistants; however, they pose increasing safety risks. How can we ensure safety alignment of MLLMs to prevent undesired behaviors? Going further, it is critical to explore how to fine-tune MLLMs to preserve capabilities while meeting safety constraints. Fundamentally, this challenge can be formulated as a min-max optimization problem. However, existing datasets have not yet disentangled single preference signals into explicit safety constraints, hindering systematic investigation in this direction. Moreover, it remains an open question whether such constraints can be effectively incorporated into the optimization process for multi-modal models. In this work, we present the first exploration of the Safe RLHF-V -- the first multimodal safety alignment framework. The framework consists of: (I) BeaverTails-V, the first open-source dataset featuring dual preference annotations for helpfulness and safety, supplemented with multi-level safety labels (minor, moderate, severe); (II) Beaver-Guard-V, a multi-level guardrail system to proactively defend against unsafe queries and adversarial attacks. Applying the guard model over five rounds of filtering and regeneration significantly enhances the precursor model's overall safety by an average of 40.9%. (II) Based on dual preference, we initiate the first exploration of multi-modal safety alignment within a constrained optimization. Experimental results demonstrate that Safe RLHF effectively improves both model helpfulness and safety. Specifically, Safe RLHF-V enhances model safety by 34.2% and helpfulness by 34.3%",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fggSyPPk0K": {
    "title": "World-aware Planning Narratives Enhance Large Vision-Language Model Planner",
    "volume": "poster",
    "abstract": "Large Vision-Language Models (LVLMs) show promise for embodied planning tasks but struggle with complex scenarios involving unfamiliar environments and multi-step goals. Current approaches rely on environment-agnostic imitation learning that disconnects instructions from environmental contexts, causing models to struggle with context-sensitive instructions and rely on supplementary cues rather than visual reasoning during long-horizon interactions. In this work, we propose World-Aware Planning Narrative Enhancement (WAP), a framework that infuses LVLMs with comprehensive environmental understanding through four cognitive capabilities (visual appearance modeling, spatial reasoning, functional abstraction, and syntactic grounding) while developing and evaluating models using only raw visual observations through curriculum learning. Evaluations on the EB-ALFRED benchmark demonstrate substantial improvements, with Qwen2.5-VL achieving a 60.7 absolute improvement in task success rates—particularly in commonsense reasoning (+60.0) and long-horizon planning (+70.0). Notably, our enhanced open-source models outperform proprietary systems like GPT-4o and Claude-3.5-Sonnet by a large margin",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0MXUkBmm09": {
    "title": "Embodied Cognition Augmented End2End Autonomous Driving",
    "volume": "poster",
    "abstract": "In recent years, vision-based end-to-end autonomous driving has emerged as a new paradigm. However, popular end-to-end approaches typically rely on visual feature extraction networks trained under label supervision. This limited supervision framework restricts the generality and applicability of driving models. In this paper, we propose a novel paradigm termed $E^{3}AD$, which advocates for comparative learning between visual feature extraction networks and the general EEG large model, in order to learn latent human driving cognition for enhancing end-to-end planning. In this work, we collected a cognitive dataset for the mentioned contrastive learning process. Subsequently, we investigated the methods and potential mechanisms for enhancing end-to-end planning with human driving cognition, using popular driving models as baselines on publicly available autonomous driving datasets. Both open-loop and closed-loop tests are conducted for a comprehensive evaluation of planning performance. Experimental results demonstrate that the $E^{3}AD$ paradigm significantly enhances the end-to-end planning performance of baseline models. Ablation studies further validate the contribution of driving cognition and the effectiveness of comparative learning process. To the best of our knowledge, this is the first work to integrate human driving cognition for improving end-to-end autonomous driving planning. It represents an initial attempt to incorporate embodied cognitive data into end-to-end autonomous driving, providing valuable insights for future brain-inspired autonomous driving systems. Our code will be made available at https://github.com/AIR-DISCOVER/E-cubed-AD",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=88MXvVn5dl": {
    "title": "Feature-aware Modulation for Learning from Temporal Tabular Data",
    "volume": "poster",
    "abstract": "While tabular machine learning has achieved remarkable success, temporal distribution shifts pose significant challenges in real-world deployment, as the relationships between features and labels continuously evolve. Static models assume fixed mappings to ensure generalization, whereas adaptive models may overfit to transient patterns, creating a dilemma between robustness and adaptability. In this paper, we analyze key factors essential for constructing an effective dynamic mapping for temporal tabular data. We discover that evolving feature semantics—particularly objective and subjective meanings—introduce concept drift over time. Crucially, we identify that feature transformation strategies are able to mitigate discrepancies in feature representations across temporal stages. Motivated by these insights, we propose a feature-aware temporal modulation mechanism that conditions feature representations on temporal context, modulating statistical properties such as scale and skewness. By aligning feature semantics across time, our approach achieves a lightweight yet powerful adaptation, effectively balancing generalizability and adaptability. Benchmark evaluations validate the effectiveness of our method in handling temporal shifts in tabular data",
    "checked": false,
    "id": "2db955b3bd0d72ef97627cd91b055fd5c5c68c20",
    "semantic_title": "gem-crap: a fusion architecture for focal seizure detection",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d6WUTRJqP3": {
    "title": "Seemingly Redundant Modules Enhance Robust Odor Learning in Fruit Flies",
    "volume": "poster",
    "abstract": "Biological circuits have evolved to incorporate multiple modules that perform similar functions. In the fly olfactory circuit, both lateral inhibition (LI) and neuronal spike frequency adaptation (SFA) are thought to enhance pattern separation for odor learning. However, it remains unclear whether these mechanisms play redundant or distinct roles in this process. In this study, we present a computational model of the fly olfactory circuit to investigate odor discrimination under varying noise conditions that simulate complex environments. Our results show that LI primarily enhances odor discrimination in low‑ and medium‑noise scenarios, but this benefit diminishes and may reverse under higher‑noise conditions. In contrast, SFA consistently improves discrimination across all noise levels. LI is preferentially engaged in low‑ and medium‑noise environments, whereas SFA dominates in high‑noise settings. When combined, these two sparsification mechanisms enable optimal discrimination performance. This work demonstrates that seemingly redundant modules in biological circuits can, in fact, be essential for achieving optimal learning in complex contexts",
    "checked": true,
    "id": "399ccf982caed1c8cc19650cc11e23b8e25d0cdd",
    "semantic_title": "seemingly redundant modules enhance robust odor learning in fruit flies",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=4jgsUhWWaF": {
    "title": "Brain-tuning Improves Generalizability and Efficiency of Brain Alignment in Speech Models",
    "volume": "poster",
    "abstract": "Pretrained language models are remarkably effective in aligning with human brain responses elicited by natural language stimuli, positioning them as promising model organisms for studying language processing in the brain. However, existing approaches for both estimating and improving this brain alignment are participant-dependent and highly affected by the amount of data available per participant, hindering both generalization to new participants and population-level analyses. In this work, we address these limitations by introducing a scalable, generalizable brain-tuning method, in which we fine-tune pretrained speech language models to jointly predict fMRI responses from multiple participants. We demonstrate that the resulting brain-tuned models exhibit strong individual brain alignment while generalizing across participants. Specifically, our method leads to 1) a 5-fold decrease in the amount of fMRI data needed to predict brain data from new participants, 2) up to a 50\\% increase in the overall brain alignment, and 3) strong generalization to new unseen datasets. Furthermore, this multi-participant brain-tuning additionally improves downstream performance on semantic tasks, suggesting that training using brain data from multiple participants leads to more generalizable semantic representations. Taken together, these findings demonstrate a bidirectional benefit between neuroscience and AI, helping bridge the gap between the two fields. We make our code and models publicly available at https://github.com/bridge-ai-neuro/multi-brain-tuning",
    "checked": true,
    "id": "c996fec36029317d49bdd90e91e1054ffe84521f",
    "semantic_title": "brain-tuning improves generalizability and efficiency of brain alignment in speech models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RbkHARGCcH": {
    "title": "Multi-Agent Imitation by Learning and Sampling from Factorized Soft Q-Function",
    "volume": "poster",
    "abstract": "Learning from multi-agent expert demonstrations, known as Multi-Agent Imitation Learning (MAIL), provides a promising approach to sequential decision-making. However, existing MAIL methods including Behavior Cloning (BC) and Adversarial Imitation Learning (AIL) face significant challenges: BC suffers from the compounding error issue, while the very nature of adversarial optimization makes AIL prone to instability. In this work, we propose \\textbf{M}ulti-\\textbf{A}gent imitation by learning and sampling from \\textbf{F}actor\\textbf{I}zed \\textbf{S}oft Q-function (MAFIS), a novel method that addresses these limitations for both online and offline MAIL settings. Built upon the single-agent IQ-Learn framework, MAFIS introduces the value decomposition network to factorize the imitation objective at agent level, thus enabling scalable training for multi-agent systems. Moreover, we observe that the soft Q-function implicitly defines the optimal policy as an energy-based model, from which we can sample actions via stochastic gradient Langevin dynamics. This allows us to estimate the gradient of the factorized optimization objective for continuous control tasks, avoiding the adversarial optimization between the soft Q-function and the policy required by prior work. By doing so, we obtain a tractable and \\emph{non-adversarial} objective for both discrete and continuous multi-agent control. Experiments on common benchmarks including the discrete control tasks StarCraft Multi-Agent Challenge v2 (SMACv2), Gold Miner, and Multi Particle Environments (MPE), as well as the continuous control task Multi-Agent MuJoCo (MaMuJoCo), demonstrate that MAFIS achieves superior performance compared with baselines. Our code is available at https://github.com/LAMDA-RL/MAFIS",
    "checked": false,
    "id": "8479620f72777af6ea33258afa56c7b4baf4c3d7",
    "semantic_title": "inverse factorized q-learning for cooperative multi-agent imitation learning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=7Nxq4RQApu": {
    "title": "Root Cause Analysis of Outliers with Missing Structural Knowledge",
    "volume": "poster",
    "abstract": "The goal of Root Cause Analysis (RCA) is to explain why an anomaly occurred by identifying where the fault originated. Several recent works model the anomalous event as resulting from a change in the causal mechanism at the root cause, i.e., as a soft intervention. RCA is then the task of identifying which causal mechanism changed. In real-world applications, one often has either few or only a single sample from the post-intervention distribution: a severe limitation for most methods, which assume one knows or can estimate the distribution. However, even those that do not are statistically ill-posed due to the need to probe regression models in regions of low probability density. In this paper, we propose simple, efficient methods to overcome both difficulties in the case where there is a single root cause and the causal graph is a polytree. When one knows the causal graph, we give guarantees for a traversal algorithm that requires only marginal anomaly scores and does not depend on specifying an arbitrary anomaly score cut-off. When one does not know the causal graph, we show that the heuristic of identifying root causes as the variables with the highest marginal anomaly scores is causally justified. To this end, we prove that anomalies with small scores are unlikely to cause those with larger scores in polytrees and give upper bounds for the likelihood of causal pathways with non-monotonic anomaly scores",
    "checked": true,
    "id": "b2335b90e225ad59deb1e29bcf18ef89c3402c35",
    "semantic_title": "root cause analysis of outliers with missing structural knowledge",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=aI3d897dgV": {
    "title": "Causal Mixture Models: Characterization and Discovery",
    "volume": "poster",
    "abstract": "Real-world datasets are often a combination of unobserved subpopulations that follow distinct causal generating processes. In an observational study, for example, participants may fall into unknown groups that either (a) respond effectively to a drug, or (b) show no response due to drug resistance. Not accounting for such heterogeneity then risks biased estimates of drug effectiveness. In this work, we formulate this setting through a causal mixture model, in which the data-generating process of each variable depends on latent group membership (a or b). Specifically, we model each variable as a mixture of structural causal equation models, where latent categorical (mixing) variables index assignment to subpopulations. Unlike prior work, the approach allows for multiple independent mixing variables, each affecting distinct sets of observed variables. To infer both the graph, mixing variables, and assignments jointly, we integrate mixture modeling into score-based causal discovery; show theoretically that the resulting scoring criterion is consistent; and demonstrate empirically that the resulting causal discovery approach discovers the causal model in synthetic and real-world evaluations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zrqn7ZshXG": {
    "title": "From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization",
    "volume": "poster",
    "abstract": "Recent unlearning methods for LLMs are vulnerable to relearning attacks: knowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of (even seemingly-unrelated) examples. We study this phenomenon in a controlled setting for example-level unlearning in vision classifiers. We make the surprising discovery that forget-set accuracy can recover from around 50\\% post-unlearning to nearly 100\\% with fine-tuning on just the *retain* set---i.e., zero examples of the forget set. We observe this effect across a wide variety of unlearning methods, whereas for a model retrained from scratch excluding the forget set (gold standard), the accuracy remains at 50\\%. We observe that resistance to relearning attacks can be predicted by weight-space properties, specifically, $L_2$-distance and linear mode connectivity between the original and the unlearned model. Leveraging this insight, we propose a new class of methods that achieve state-of-the-art resistance to relearning attacks",
    "checked": true,
    "id": "6b9dec8fb34bfb0791d41b4df3df0adce10ae9db",
    "semantic_title": "from dormant to deleted: tamper-resistant unlearning through weight-space regularization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=cWEssTIwG5": {
    "title": "TV-Rec: Time-Variant Convolutional Filter for Sequential Recommendation",
    "volume": "poster",
    "abstract": "Recently, convolutional filters have been increasingly adopted in sequential recommendation for their ability to capture local sequential patterns. However, most of these models complement convolutional filters with self-attention. This is because convolutional filters alone, generally fixed filters, struggle to capture global interactions necessary for accurate recommendation. We propose \\textbf{T}ime-\\textbf{V}ariant Convolutional Filters for Sequential \\textbf{Rec}ommendation (TV-Rec), a model inspired by graph signal processing, where time-variant graph filters capture position-dependent temporal variations in user sequences. By replacing both fixed kernels and self-attention with time-variant filters, TV-Rec achieves higher expressive power and better captures complex interaction patterns in user behavior. This design not only eliminates the need for self-attention but also reduces computation while accelerating inference. Extensive experiments on six public benchmarks show that TV-Rec outperforms state-of-the-art baselines by an average of 7.49\\%",
    "checked": true,
    "id": "a28d45d28019a3250fb73c301c55f95bf6d5ad2f",
    "semantic_title": "tv-rec: time-variant convolutional filter for sequential recommendation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vKyiv67VWa": {
    "title": "AutoSciDACT: Automated Scientific Discovery through Contrastive Embedding and Hypothesis Testing",
    "volume": "poster",
    "abstract": "Novelty detection in large scientific datasets faces two key challenges: the noisy and high-dimensional nature of experimental data, and the necessity of making *statistically robust* statements about any observed outliers. While there is a wealth of literature on anomaly detection via dimensionality reduction, most methods do not produce outputs compatible with quantifiable claims of scientific discovery. In this work we directly address these challenges, presenting the first step towards a unified pipeline for novelty detection adapted for the rigorous statistical demands of science. We introduce AutoSciDACT (Automated Scientific Discovery with Anomalous Contrastive Testing), a general-purpose pipeline for detecting novelty in scientific data. AutoSciDACT begins by creating expressive low-dimensional data representations using a contrastive pre-training, leveraging the abundance of high-quality simulated data in many scientific domains alongside expertise that can guide principled data augmentation strategies. These compact embeddings then enable an extremely sensitive machine learning-based two-sample test using the New Physics Learning Machine (NPLM) framework, which identifies and statistically quantifies deviations in observed data relative to a reference distribution (null hypothesis). We perform experiments across a range of astronomical, physical, biological, image, and synthetic datasets, demonstrating strong sensitivity to small injections of anomalous data across all domains",
    "checked": true,
    "id": "556fb390a4a718e16b4510cb46271e66c651a9d8",
    "semantic_title": "autoscidact: automated scientific discovery through contrastive embedding and hypothesis testing",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=nO8ShqG2ci": {
    "title": "Multivariate Latent Recalibration for Conditional Normalizing Flows",
    "volume": "poster",
    "abstract": "A reliable estimate of the full conditional distribution of a multivariate response given a set of covariates is essential in many decision-making applications. However, misspecified or miscalibrated models can lead to poor approximations of the joint distribution, resulting in unreliable predictions and suboptimal decisions. Standard recalibration methods are largely restricted to univariate settings, and while conformal prediction techniques yield multivariate regions with coverage guarantees, they do not provide an explicit form of the underlying probability distribution. We address this gap by first introducing a novel notion of latent calibration, which assesses probabilistic calibration in the latent space of conditional invertible generative models such as normalizing flows and flow matching. Second, we propose latent recalibration (LR), a post-hoc model recalibration method that learns a transformation of the latent space with finite-sample bounds on latent calibration. Unlike existing recalibration methods, LR produces a recalibrated distribution with an explicit multivariate density function while remaining computationally efficient. Extensive experiments on both tabular and image datasets show that LR consistently improves latent calibration error and the negative log-likelihood of the recalibrated models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kRZVz1qEqa": {
    "title": "Teaching Language Models to Reason with Tools",
    "volume": "poster",
    "abstract": "Large reasoning models (LRMs) like OpenAI-o1 have shown impressive capabilities in natural language reasoning. However, these models frequently demonstrate inefficiencies or inaccuracies when tackling complex mathematical operations. While integrating computational tools such as Code Interpreters (CIs) offers a promising solution, it introduces a critical challenge: a conflict between the model's internal, probabilistic reasoning and the external, deterministic knowledge provided by the CI, which often leads models to unproductive deliberation. To overcome this, we introduce CoRT (Code-Optimized Reasoning Training), a post-training framework designed to teach LRMs to effectively utilize CIs. We propose **Hint-Engineering**, a new data synthesis strategy that strategically injects diverse hints at optimal points within reasoning paths. This approach generates high-quality, code-integrated reasoning data specifically tailored to optimize LRM-CI interaction. Using this method, we have synthesized 30 high-quality samples to post-train models ranging from 1.5B to 32B parameters through supervised fine-tuning. CoRT further refines the multi-round interleaving of external CI usage and internal thinking by employing rejection sampling and reinforcement learning. Our experimental evaluations demonstrate CoRT's effectiveness, yielding absolute improvements of 4\\% and 8\\% on DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Qwen-1.5B, respectively, across five challenging mathematical reasoning datasets. Moreover, CoRT significantly enhances efficiency, reducing token usage by approximately 30\\% for the 32B model and 50\\% for the 1.5B model compared to pure natural language reasoning baselines. The models and code are available at: [this url](https://github.com/ChengpengLi1003/CoRT)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JPjMXgQQxk": {
    "title": "Generative Modeling of Full-Atom Protein Conformations using Latent Diffusion on Graph Embeddings",
    "volume": "poster",
    "abstract": "Generating diverse, all‐atom conformational ensembles of dynamic proteins such as G‐protein‐coupled receptors (GPCRs) is critical for understanding their function, yet most generative models simplify atomic detail or ignore conformational diversity altogether. We present latent diffusion for full protein generation (LD-FPG), a framework that constructs complete all‐atom protein structures, including every side‐chain heavy atom, directly from molecular dynamics (MD) trajectories. LD-FPG employs a Chebyshev graph neural network (ChebNet) to obtain low‐dimensional latent embeddings of protein conformations, which are processed using three pooling strategies: blind, sequential and residue‐based. A diffusion model trained on these latent representations generates new samples that a decoder, optionally regularized by dihedral‐angle losses, maps back to Cartesian coordinates. Using D2R-MD, a $2\\mu\\text{s}$ MD trajectory (12 000 frames) of the human dopamine D$2$ receptor in a membrane environment, the sequential and residue-based pooling strategies reproduce the reference ensemble with high structural fidelity (all‐atom lDDT \\~ $0.7$; $C\\alpha$-lDDT \\~ $0.8$) and recovers backbone and side‐chain dihedral‐angle distributions with a Jensen–Shannon divergence $<0.03$ compared to the MD data. LD-FPG thereby offers a practical route to system‐specific, all‐atom ensemble generation for large proteins, providing a promising tool for structure‐based therapeutic design on complex, dynamic targets. The D2R-MD dataset and our implementation are freely available to facilitate further research",
    "checked": true,
    "id": "9963e88a148d1f9f1886031070902898e299bb12",
    "semantic_title": "generative modeling of full-atom protein conformations using latent diffusion on graph embeddings",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=7bCPXHq8xV": {
    "title": "Price of Parsimony: Complexity of Fourier Sparsity Testing",
    "volume": "poster",
    "abstract": "A function \\( f : \\mathbb{F}_2^n \\to \\mathbb{R} \\) is said to be \\( s \\)-Fourier sparse if its Fourier expansion contains at most \\( s \\) nonzero coefficients. In general, the existence of a sparse representation in the Fourier basis serves as a key enabler for the design of efficient learning algorithms. However, most existing techniques assume prior knowledge of the function's Fourier sparsity, with algorithmic parameters carefully tuned to this value. This motivates the following decision problem: given \\( s > 0 \\), determine whether a function is \\( s \\)-Fourier sparse. In this work, we study the problem of tolerant testing of Fourier Sparsity for real-valued functions over \\( \\mathbb{F}_2^n \\), accessed via oracle queries. The goal is to decide whether a given function is close to being \\( s \\)-Fourier sparse or far from every \\( s \\)-Fourier sparse function. Our algorithm provides an estimator that, given oracle access to the function, estimates its distance to the nearest \\( s \\)-Fourier sparse function with query complexity \\( \\widetilde{O}(s) \\), for constant accuracy and confidence parameters. A key structural ingredient in our analysis is a new spectral concentration result for real-valued functions over \\( \\mathbb{F}_2^n \\) when restricted to small-dimensional random affine subspaces. We further complement our upper bound with a matching lower bound of \\( \\Omega(s) \\), establishing that our tester is optimal up to logarithmic factors. The lower bound exploits spectral properties of a class of cryptographically hard functions, namely, the Maiorana--McFarland family, in a novel way",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1NGlLNaVC": {
    "title": "CoCoA: A Minimum Bayes Risk Framework Bridging Confidence and Consistency for Uncertainty Quantification in LLMs",
    "volume": "poster",
    "abstract": "Uncertainty quantification for Large Language Models (LLMs) encompasses a diverse range of approaches, with two major families being particularly prominent: (i) information-based, which estimate model confidence from token-level probabilities, and (ii) consistency-based, which assess the semantic agreement among multiple outputs generated using repeated sampling. While several recent methods have sought to combine these two paradigms to improve uncertainty quantification performance, they often fail to consistently outperform simpler baselines. In this work, we revisit the foundations of uncertainty estimation through the lens of Minimum Bayes Risk decoding, establishing a direct link between uncertainty and the optimal decision-making process of LLMs. Building on these findings, we propose CoCoA, a unified framework that integrates model confidence with output consistency, yielding a family of efficient and robust uncertainty quantification methods. We evaluate CoCoA across diverse tasks, including question answering, abstractive text summarization, and machine translation, and demonstrate sizable improvements over state-of-the-art uncertainty quantification approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aq9Nc5NvNc": {
    "title": "On the Global Optimality of Policy Gradient Methods in General Utility Reinforcement Learning",
    "volume": "poster",
    "abstract": "Reinforcement learning with general utilities (RLGU) offers a unifying framework to capture several problems beyond standard expected returns, including imitation learning, pure exploration, and safe RL. Despite recent fundamental advances in the theoretical analysis of policy gradient (PG) methods for standard RL and recent efforts in RLGU, the understanding of these PG algorithms and their scope of application in RLGU still remain limited. In this work, we establish global optimality guarantees of PG methods for RLGU in which the objective is a general concave utility function of the state-action occupancy measure. In the tabular setting, we provide global optimality results using a new proof technique building on recent theoretical developments on the convergence of PG methods for standard RL using gradient domination. Our proof technique opens avenues for analyzing policy parameterizations beyond the direct policy parameterization for RLGU. In addition, we provide global optimality results for large state-action space settings beyond prior work which has mostly focused on the tabular setting. In this large scale setting, we adapt PG methods by approximating occupancy measures within a function approximation class using maximum likelihood estimation. Our sample complexity only scales with the dimension induced by our approximation class instead of the size of the state-action space",
    "checked": true,
    "id": "86760bd722c23b464971e4e933c432e6a14ecb17",
    "semantic_title": "on the global optimality of policy gradient methods in general utility reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hh8ebJYQs3": {
    "title": "Hybrid Latent Representations for PDE Emulation",
    "volume": "poster",
    "abstract": "For classical PDE solvers, adjusting the spatial resolution and time step offers a trade-off between speed and accuracy. Neural emulators often achieve better speed-accuracy trade-offs by operating accurately on a compact representation of the PDE system. Coarsened PDE fields are a simple and effective representation, but cannot exploit fine spatial scales in the high-fidelity numerical solutions. Alternatively, unstructured latent representations provide efficient autoregressive rollouts, but cannot enforce local interactions or physical laws as inductive biases. To overcome these limitations, we introduce hybrid representations that augment coarsened PDE fields with spatially structured latent variables extracted from high-resolution inputs. Hybrid representations provide efficient rollouts, can be trained on a simple loss defined on coarsened PDE fields, and support hard physical constraints. When predicting fine- and coarse-scale features across multiple PDE emulation tasks, they outperform or match the speed-accuracy trade-offs of the best convolutional, attentional, Fourier operator-based and autoencoding baselines",
    "checked": false,
    "id": "fff9ae60c7600865a3810c2122400ca9dbb3813d",
    "semantic_title": "dynamically meaningful latent representations of dynamical systems",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=AWU93F6Bup": {
    "title": "MoodAngels: A Retrieval-augmented Multi-agent Framework for Psychiatry Diagnosis",
    "volume": "poster",
    "abstract": "The application of AI in psychiatric diagnosis faces significant challenges, including the subjective nature of mental health assessments, symptom overlap across disorders, and privacy constraints limiting data availability. To address these issues, we present MoodAngels, the first specialized multi-agent framework for mood disorder diagnosis. Our approach combines granular-scale analysis of clinical assessments with a structured verification process, enabling more accurate interpretation of complex psychiatric data. Complementing this framework, we introduce MoodSyn, an open-source dataset of 1,173 synthetic psychiatric cases that preserves clinical validity while ensuring patient privacy. Experimental results demonstrate that MoodAngels outperforms conventional methods, with our baseline agent achieving 12.3\\% higher accuracy than GPT-4o on real-world cases, and our full multi-agent system delivering further improvements. Together, these contributions provide both an advanced diagnostic tool and a critical research resource for computational psychiatry, bridging important gaps in AI-assisted mental health assessment",
    "checked": true,
    "id": "53a723d546c1a7672c582b230e7059e8866cf783",
    "semantic_title": "moodangels: a retrieval-augmented multi-agent framework for psychiatry diagnosis",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cJlgdpEFx9": {
    "title": "Meta-learning how to Share Credit among Macro-Actions",
    "volume": "poster",
    "abstract": "One proposed mechanism to improve exploration in reinforcement learning is the use of macro-actions, a form of temporal abstractions over actions. Paradoxically though, in many scenarios the naive addition of macro-actions does not lead to better exploration, but rather the opposite. In this work, we argue that the difficulty stems from the trade-offs between reducing the average number of decisions per episode versus increasing the size of the action space. Namely, one typically treats each potential macro-action as independent and atomic, hence strictly increasing the search space and making typical exploration strategies inefficient. To address this problem we propose a novel regularization term that exploits the relationship between actions and macro-actions to improve the credit assignment mechanism reducing the effective dimension of the action space and therefore improving exploration. The term relies on a similarity matrix that is meta-learned jointly with learning the desired policy. We empirically validate our strategy looking at macro-actions in Atari games, and the StreetFighter II environment. Our results show significant improvements over the Rainbow-DQN baseline in all environments. Additionally, we show that the macro-action similarity is transferable to other environments with similar dynamics. We believe this work is a small but important step towards understanding how the similarity-imposed geometry on the action space can be exploited to improve credit assignment and exploration, therefore making learning more efficient",
    "checked": true,
    "id": "c995cc553f63d42aef0b420103e73de91d92ffea",
    "semantic_title": "meta-learning how to share credit among macro-actions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TPMsCus3r0": {
    "title": "Mitigating Spurious Features in Contrastive Learning with Spectral Regularization",
    "volume": "poster",
    "abstract": "Neural networks generally prefer simple and easy-to-learn features. When these features are spuriously correlated with the labels, the network's performance can suffer, particularly for underrepresented classes or concepts. Self-supervised representation learning methods, such as contrastive learning, are especially prone to this issue, often resulting in worse performance on downstream tasks. We identify a key spectral signature of this failure: early reliance on dominant singular modes of the learned feature matrix. To mitigate this, we propose a novel framework that promotes a uniform eigenspectrum of the feature covariance matrix, encouraging diverse and semantically rich representations. Our method operates in a fully self-supervised setting, without relying on ground-truth labels or any additional information. Empirical results on SimCLR and SimSiam demonstrate consistent gains in robustness and transfer performance, suggesting broad applicability across self-supervised learning paradigms. Code: https://github.com/NaghmehGh/SpuriousCorrelation_SSRL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6lCY5bLW8E": {
    "title": "FedFACT: A Provable Framework for Controllable Group-Fairness Calibration in Federated Learning",
    "volume": "poster",
    "abstract": "With emerging application of Federated Learning (FL) in decision-making scenarios, it is imperative to regulate model fairness to prevent disparities across sensitive groups (e.g., female, male). Current research predominantly focuses on two concepts of group fairness within FL: *Global Fairness* (overall model disparity across all clients) and *Local Fairness* (the disparity within each client). However, the non-decomposable, non-differentiable nature of fairness criteria pose two fundamental, unresolved challenges for fair FL: (i) *Harmonizing global and local fairness, especially in multi-class classification*; (ii) *Enabling a controllable, optimal accuracy-fairness trade-off*. To tackle the aforementioned challenges, we propose a novel controllable federated group-fairness calibration framework, named FedFACT. FedFACT identifies the Bayes-optimal classifiers under both global and local fairness constraints in multi-class case, yielding models with minimal performance decline while guaranteeing fairness. To effectively realize an adjustable, optimal accuracy-fairness balance, we derive specific characterizations of the Bayes-optimal fair classifiers for reformulating fair FL as personalized cost-sensitive learning problem for in-processing, and bi-level optimization for post-processing. Theoretically, we provide convergence and generalization guarantees for FedFACT to approach the near-optimal accuracy under given fairness levels. Extensive experiments on multiple datasets across various data heterogeneity demonstrate that FedFACT consistently outperforms baselines in balancing accuracy and global-local fairness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2TKEGTfQBd": {
    "title": "Transformer Key-Value Memories Are Nearly as Interpretable as Sparse Autoencoders",
    "volume": "poster",
    "abstract": "Recent interpretability work on large language models (LLMs) has been increasingly dominated by a feature-discovery approach with the help of proxy modules. Then, the quality of features learned by, e.g., sparse auto-encoders (SAEs), is evaluated. This paradigm naturally raises a critical question: do such learned features have better properties than those already represented within the original model parameters, and unfortunately, only a few studies have made such comparisons systematically so far. In this work, we revisit the interpretability of feature vectors stored in feed-forward (FF) layers, given the perspective of FF as key-value memories, with modern interpretability benchmarks. Our extensive evaluation revealed that SAE and FFs exhibits a similar range of interpretability, although SAEs displayed an observable but minimal improvement in some aspects. Furthermore, in certain aspects, surprisingly, even vanilla FFs yielded better interpretability than the SAEs, and features discovered in SAEs and FFs diverged. These bring questions about the advantage of SAEs from both perspectives of feature quality and faithfulness, compared to directly interpreting FF feature vectors, and FF key-value parameters serve as a strong baseline in modern interpretability research",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IQ513IX1G5": {
    "title": "Beyond Oracle: Verifier-Supervision for Instruction Hierarchy in Reasoning and Instruction-Tuned LLMs",
    "volume": "poster",
    "abstract": "Large language models (LLMs) are often prompted with multi-level directives, such as system instructions and user queries, that imply a hierarchy of authority. Yet models frequently fail to enforce this structure, especially in multi-step reasoning where errors propagate across intermediate steps. Existing methods rely on oracle completions but lack verifiable reward signals or intermediate traces, limiting their applicability. We introduce a unified supervision framework that embeds programmatically verifiable checkers into synthesized instruction-conflict instances. Each instance pairs a compliance directive with a conflicting one, along with an executable verifier that deterministically checks output adherence. This enables alignment without oracle labels or reasoning traces, supporting both instruction-tuned and reasoning models. The framework is instantiated via a synthesis pipeline that includes unit-test–based validation, LLM-assisted repair, and a probabilistic analysis of cleaning reliability. Fine-tuning on the resulting data improves instruction hierarchy adherence and boosts safety robustness, generalizing to adversarial safety benchmarks without task-specific supervision. This highlights verifiable supervision as a scalable foundation for robust alignment. All code, dataset, and verifier pipeline are publicly available at: https://github.com/cycraft-corp/BeyondOracle",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IIGiVRKJYa": {
    "title": "ASDSV: Multimodal Generation Made Efficient with Approximate Speculative Diffusion and Speculative Verification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gqoeQPhQcE": {
    "title": "Towards Accurate Time Series Forecasting via Implicit Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "e177ce13d5e3ca4673b2fff9481408cd98bba0d3",
    "semantic_title": "dam: towards a foundation model for time series forecasting",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=vIaNnnQxcl": {
    "title": "SAFEPATH: Preventing Harmful Reasoning in Chain-of-Thought via Early Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "2abf52108128003875ae80de94d3ef2a35fe65ca",
    "semantic_title": "safepath: preventing harmful reasoning in chain-of-thought via early alignment",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=66Z5tS8E45": {
    "title": "Diffusing DeBias: Synthetic Bias Amplification for Model Debiasing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5XoqKCmkS7": {
    "title": "Perturb a Model, Not an Image: Towards Robust Privacy Protection via Anti-Personalized Diffusion Models",
    "volume": "poster",
    "abstract": "Recent advances in diffusion models have enabled high-quality synthesis of specific subjects, such as identities or objects. This capability, while unlocking new possibilities in content creation, also introduces significant privacy risks, as personalization techniques can be misused by malicious users to generate unauthorized images. Although several studies have attempted to counter this by generating adversarially perturbed samples designed to disrupt personalization, they rely on unrealistic assumptions and become ineffective in the presence of even a few clean images or under simple image transformations. To address these challenges, we shift the protection target from the images to the diffusion model itself to hinder the personalization of specific subjects, through our novel framework called $\\textbf{A}$nti-$\\textbf{P}$ersonalized $\\textbf{D}$iffusion $\\textbf{M}$odels ($\\textbf{APDM}$). We first provide a theoretical analysis demonstrating that a naive approach of existing loss functions to diffusion models is inherently incapable of ensuring convergence for robust anti-personalization. Motivated by this finding, we introduce Direct Protective Optimization (DPO), a novel loss function that effectively disrupts subject personalization in the target model without compromising generative quality. Moreover, we propose a new dual-path optimization strategy, coined Learning to Protect (L2P). By alternating between personalization and protection paths, L2P simulates future personalization trajectories and adaptively reinforces protection at each step. Experimental results demonstrate that our framework outperforms existing methods, achieving state-of-the-art performance in preventing unauthorized personalization. The code is available at https://github.com/KU-VGI/APDM",
    "checked": true,
    "id": "f67d5d3cf3ed7a97bed0c2c9501fa4ce697f8adb",
    "semantic_title": "perturb a model, not an image: towards robust privacy protection via anti-personalized diffusion models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wWSVjaVZBu": {
    "title": "Why Playing Against Diverse and Challenging Opponents Speeds Up Coevolution: A Theoretical Analysis on Combinatorial Games",
    "volume": "poster",
    "abstract": "Competitive coevolutionary algorithms (CoEAs) have a natural application to problems that are adversarial or feature strategic interaction. However, there is currently limited theoretical insight into how to avoid pathological behaviour associated with CoEAs. In this paper we use impartial combinatorial games as a challenging domain for CoEAs and provide a corresponding runtime analysis. By analysing how individuals capitalise on the mistakes of their opponents, we prove that the Univariate Marginal Distribution Algorithm finds (with high probability) an optimal strategy for a game called Reciprocal LeadingOnes within $O(n^2\\log^3{n})$ game evaluations, a significant improvement over the best known bound of $O(n^5\\log^2{n})$. Critical to the analysis is the introduction of a novel stabilising operator, the impact of which we study both theoretically and empirically",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e5QEGDVsqn": {
    "title": "Theoretical Guarantees for the Retention of Strict Nash Equilibria by Coevolutionary Algorithms",
    "volume": "poster",
    "abstract": "Most methods for finding a Nash equilibrium rely on procedures that operate over the entire action space, making them infeasible for settings with too many actions to be searched exhaustively. Randomised search heuristics such as coevolutionary algorithms offer benefits in such settings, however they lack many of the theoretical guarantees established for exhaustive methods such as zero-regret learning. We address this by developing a method for proving necessary and sufficient conditions for a coevolutionary algorithm to be stable, in the sense that it reliably retains a Nash equilibrium following discovery. As the method provides bounds that are adapted to both application and algorithm instance, it can be used as a practical tool for parameter configuration. We additionally show how bounds on regret may be deduced from our results and undertake corresponding empirical analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u0rNHqMpFD": {
    "title": "Stochastic Principal-Agent Problems: Computing and Learning Optimal History-Dependent Policies",
    "volume": "poster",
    "abstract": "We study a stochastic principal-agent model. A principal and an agent interact in a stochastic environment, each privy to observations about the state not available to the other. The principal has the power of commitment, both to elicit information from the agent and to signal her own information. The players communicate with each other and then select actions independently. Both players are {\\em far-sighted}, aiming to maximize their total payoffs over the entire time horizon. We consider both the computation and learning of the principal's optimal policy. The key challenge lies in enabling {\\em history-dependent} policies, which are essential for achieving optimality in this model but difficult to cope with because of the exponential growth of possible histories as the size of the model increases; explicit representation of history-dependent policies is infeasible as a result. To address this challenge, we develop algorithmic techniques based on the concept of {\\em inducible value set}. The techniques yield an efficient algorithm that computes an $\\epsilon$-approximate optimal policy in time polynomial in $1/\\epsilon$. We also present an efficient learning algorithm for an episodic reinforcement learning setting with unknown transition probabilities. The algorithm achieves sublinear regret $\\widetilde{\\mathcal{O}}(T^{2/3})$ for both players over $T$ episodes",
    "checked": false,
    "id": "b5505e6fc94c39bd875b0bc347b789a1b041e7ef",
    "semantic_title": "stochastic principal-agent problems: efficient computation and learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=2g5cJqX15Y": {
    "title": "Large language models can learn and generalize steganographic chain-of-thought under process supervision",
    "volume": "poster",
    "abstract": "Chain-of-thought (CoT) reasoning not only enhances large language model performance but also provides critical insights into decision-making processes, marking it as a useful tool for monitoring model intent and planning. By proactively preventing models from acting on CoT indicating misaligned or harmful intent, CoT monitoring can be used to reduce risks associated with deploying models. However, developers may be incentivized to train away the appearance of harmful intent from CoT traces, by either customer preferences or regulatory requirements. However, recent works have shown that banning the mention of a specific example of reward hacking causes obfuscation of the undesired reasoning traces but the persistence of the undesired behavior, threatening the reliability of CoT monitoring. However, obfuscation of reasoning can be due to its internalization to latent space computation, or its encoding within the CoT. We provide an extension to these results with regard to the ability of models to learn a specific type of obfuscated reasoning: steganography. First, we show that penalizing the use of specific strings within load-bearing reasoning traces causes models to substitute alternative strings. Crucially, this does not alter the underlying method by which the model performs the task, demonstrating that the model can learn to steganographically encode its reasoning. This is an example of models learning to encode their reasoning. We further demonstrate that models can generalize an encoding scheme. When the penalized strings belong to an overarching class, the model learns not only to substitute strings seen in training, but also develops a general encoding scheme for all members of the class which it can apply to held-out testing strings",
    "checked": true,
    "id": "500cf190de9c3db87173535f6416b7e52921fe64",
    "semantic_title": "large language models can learn and generalize steganographic chain-of-thought under process supervision",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=QJSrgYcf4b": {
    "title": "PyraMotion: Attentional Pyramid-Structured Motion Integration for Co-Speech 3D Gesture Synthesis",
    "volume": "poster",
    "abstract": "Generating full-body human gestures encompassing face, body, hands, and global movements from audio is crucial yet challenging for virtual avatar creation. Existing systems tokenize gestures frame-wise, predicting tokens of each frame from the input audio. However, expressive human gestures consist of varied patterns with different frame lengths, and different body parts exhibit motion patterns of varying durations. Existing systems fail to capture motion patterns across body parts and temporal scales due to the fixed frame-count setting of their gesture tokens. Inspired by the success of the feature pyramid technique in the multi-scale visual information extraction, we propose a novel framework named PyraMotion and an adaptive multi-scale feature capturing model called Attentive Pyramidal VQ-VAE (APVQ-VAE). Objective and subjective experiments demonstrate that the PyraMotion outperforms state-of-the-art methods in terms of generating natural and expressive full-body human gestures. Extensive ablation experiments highlight that the self-adaptiveness integration through attention maps contributes to performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DO5LtJc80w": {
    "title": "Monoculture or Multiplicity: Which Is It?",
    "volume": "poster",
    "abstract": "Two narratives about machine learning ecosystems grew out of recent algorithmic fairness discourse. In one, dubbed \\emph{monoculture}, algorithmic ecosystems tend toward homogeneity akin to a single model making all decisions. Individuals then face the risk of systematic exclusion with no recourse. In the other, \\emph{model multiplicity}, many models solve the same task with similar accuracy, causing excessive variation in outcomes. Both narratives are compelling, yet, seemingly at odds: model multiplicity can't exist in a strict monoculture. In this work, we conduct a comprehensive empirical evaluation to test both claims. We work from the premise that increasingly decision makers will use large language models for consequential prediction tasks. We therefore examine 50 language models, open source models ranging in size from 1B to 141B parameters and state-of-the-art commercial models, under 4 different prompt variations, and across 6 different prediction tasks. Evaluating both new and old quantitative measures of monoculture and multiplicity, we find the empirical landscape sits between the two extremes. Each narrative finds some empirical support, but neither is dominant. Systematic exclusion with no recourse is rare, but model similarity is real. Even when starting from a single model, prompt variation induces some diversity in predictions. Our results contribute critical empirical grounding to ongoing debates and point toward a middle ground between monoculture and multiplicity as the most realistic outcome",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=heQsyrMDzm": {
    "title": "Scalable Evaluation and Neural Models for Compositional Generalization",
    "volume": "poster",
    "abstract": "Compositional generalization—a key open challenge in modern machine learning—requires models to predict unknown combinations of known concepts. However, assessing compositional generalization remains a fundamental challenge due to the lack of standardized evaluation protocols and the limitations of current benchmarks, which often favor efficiency over rigor. At the same time, general-purpose vision architectures lack the necessary inductive biases, and existing approaches to endow them compromise scalability. As a remedy, this paper introduces: 1) a rigorous evaluation framework that unifies and extends previous approaches while reducing computational requirements from combinatorial to constant; 2) an extensive and modern evaluation on the status of compositional generalization in supervised vision backbones, training more than 5000 models; 3) Attribute Invariant Networks, a class of models establishing a new Pareto frontier in compositional generalization, achieving a 23.43% accuracy improvement over baselines while reducing parameter overhead from 600% to 16% compared to fully disentangled counterparts",
    "checked": true,
    "id": "7c101be451a592a7b28d1b517fdf7832b1ced2ad",
    "semantic_title": "scalable evaluation and neural models for compositional generalization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jHWCeU39Ft": {
    "title": "MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining",
    "volume": "poster",
    "abstract": "Data quality is a critical driver of large language model performance, yet existing model-based selection methods focus almost exclusively on English, neglecting other languages that are essential in the training mix for multilingual LLMs. We introduce MuRating, a scalable framework that transfers high-quality English data-quality signals into a multilingual autorater, capable of handling 17 languages. MuRating aggregates multiple English autoraters via pairwise comparisons to learn unified document quality scores, then projects these judgments through translation to train a multilingual evaluator on monolingual, cross-lingual, and parallel text pairs. Applied to web data, MuRating selects balanced subsets of English and multilingual content to pretrain LLaMA-architecture models of 1.2B and 7B parameters. Compared to strong baselines, including QuRater, FineWeb2-HQ, AskLLM, DCLM, our approach increases average accuracy on both English benchmarks and multilingual evaluations. Extensive analyses further validate that pairwise training provides greater stability and robustness than pointwise scoring, underscoring the effectiveness of MuRating as a general multilingual data-selection framework",
    "checked": true,
    "id": "9898bbde43fa16e5e1b67d5ab651529a5915eb91",
    "semantic_title": "murating: a high quality data selecting approach to multilingual large language model pretraining",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cHi8QxGrZH": {
    "title": "Diffusion Adaptive Text Embedding for Text-to-Image Diffusion Models",
    "volume": "poster",
    "abstract": "Text-to-image diffusion models rely on text embeddings from a pre-trained text encoder, but these embeddings remain fixed across all diffusion timesteps, limiting their adaptability to the generative process. We propose Diffusion Adaptive Text Embedding (DATE), which dynamically updates text embeddings at each diffusion timestep based on intermediate perturbed data. We formulate an optimization problem and derive an update rule that refines the text embeddings at each sampling step to improve alignment and preference between the mean predicted image and the text. This allows DATE to dynamically adapts the text conditions to the reverse-diffused images throughout diffusion sampling without requiring additional model training. Through theoretical analysis and empirical results, we show that DATE maintains the generative capability of the model while providing superior text-image alignment over fixed text embeddings across various tasks, including multi-concept generation and text-guided image editing. Our code is available at https://github.com/aailab-kaist/DATE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EBONa3tT3K": {
    "title": "LinEAS: End-to-end Learning of Activation Steering with a Distributional Loss",
    "volume": "poster",
    "abstract": "The growing use of generative models in daily life calls for efficient mechanisms to control their generation, to e.g. produce safe content or provide users with tools to explore style changes. Ideally, such mechanisms should require low volume of unpaired data (\\ie without explicit preference), and should be cheap, both at train and inference time, while preserving output quality. Recent research has shown that such mechanisms can be obtained by intervening exclusively on model activations, with the goal of correcting distributional differences between activations seen when using prompts from a source vs. a target set (e.g. toxic and non-toxic sentences). While cheap, these fast methods are inherently crude: their maps are tuned locally, not accounting for their impact on downstream layers, resulting in interventions that cause unintended shifts when used out-of-sample. We propose in this work linear end-to-end activation steering (LinEAS), an approach trained with a global loss that accounts simultaneously for all layer-wise distributional shifts. In addition to being more robust, the loss used to train LinEAS can be regularized with sparsifying norms, which can automatically carry out neuron selection. LinEAS only requires a handful of unpaired samples to be effective, and beats similar baselines on toxicity mitigation in language models, becoming competitive with oracle-dependent methods that have access to strong supervision. LinEAS is modality-agnostic and we empirically find that it outperforms existing activation steering methods at mitigating and including new concepts at the output of single-step text-to-image generation models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NfBrMDF0Xi": {
    "title": "PDEfuncta: Spectrally-Aware Neural Representation for PDE Solution Modeling",
    "volume": "poster",
    "abstract": "Scientific machine learning often involves representing complex solution fields that exhibit high-frequency features such as sharp transitions, fine-scale oscillations, and localized structures. While implicit neural representations (INRs) have shown promise for continuous function modeling, capturing such high-frequency behavior remains a challenge—especially when modeling multiple solution fields with a shared network. Prior work addressing spectral bias in INRs has primarily focused on single-instance settings, limiting scalability and generalization. In this work, we propose Global Fourier Modulation (GFM), a novel modulation technique that injects high-frequency information at each layer of the INR through Fourier-based reparameterization. This enables compact and accurate representation of multiple solution fields using low-dimensional latent vectors. Building upon GFM, we introduce PDEfuncta, a meta-learning framework designed to learn multi-modal solution fields and support generalization to new tasks. Through empirical studies on diverse scientific problems, we demonstrate that our method not only improves representational quality but also shows potential for forward and inverse inference tasks without the need for retraining",
    "checked": true,
    "id": "dfa450b0e389b219272276d086fd60c2193672fc",
    "semantic_title": "pdefuncta: spectrally-aware neural representation for pde solution modeling",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ORrCEtiiVX": {
    "title": "Minimizing False-Positive Attributions in Explanations of Non-Linear Models",
    "volume": "poster",
    "abstract": "Suppressor variables can influence model predictions without being dependent on the target outcome, and they pose a significant challenge for Explainable AI (XAI) methods. These variables may cause false-positive feature attributions, undermining the utility of explanations. Although effective remedies exist for linear models, their extension to non-linear models and instance-based explanations has remained limited. We introduce PatternLocal, a novel XAI technique that addresses this gap. PatternLocal begins with a locally linear surrogate, e.g., LIME, KernelSHAP, or gradient-based methods, and transforms the resulting discriminative model weights into a generative representation, thereby suppressing the influence of suppressor variables while preserving local fidelity. In extensive hyperparameter optimization on the XAI-TRIS benchmark, PatternLocal consistently outperformed other XAI methods and reduced false-positive attributions when explaining non-linear tasks, thereby enabling more reliable and actionable insights. We further evaluate PatternLocal on an EEG motor imagery dataset, demonstrating physiologically plausible explanations",
    "checked": true,
    "id": "63e5e3138a33e5543b0faeeffdac1d1d39e53270",
    "semantic_title": "minimizing false-positive attributions in explanations of non-linear models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IFQBrEAuQ6": {
    "title": "Rethinking PCA Through Duality",
    "volume": "poster",
    "abstract": "Motivated by the recently shown connection between self-attention and (kernel) principal component analysis (PCA), we revisit the fundamentals of PCA. Using the difference-of-convex (DC) framework, we present several novel formulations and provide new theoretical insights. In particular, we show the kernelizability and out-of-sample applicability for a PCA-like family of problems. Moreover, we uncover that simultaneous iteration, which is connected to the classical QR algorithm, is an instance of the difference-of-convex algorithm (DCA), offering an optimization perspective on this longstanding method. Further, we describe new algorithms for PCA and empirically compare them with state-of-the-art methods. Lastly, we introduce a kernelizable dual formulation for a robust variant of PCA that minimizes the $l_1$-deviation of the reconstruction errors",
    "checked": true,
    "id": "cf6e2704ba838451811fdd590444e902d8b126c6",
    "semantic_title": "rethinking pca through duality",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sFyTsO2qO3": {
    "title": "Disentangled Cross-Modal Representation Learning with Enhanced Mutual Supervision",
    "volume": "poster",
    "abstract": "Cross-modal representation learning aims to extract semantically aligned representations from heterogeneous modalities such as images and text. Existing multimodal VAE-based models often suffer from limited capability to align heterogeneous modalities or lack sufficient structural constraints to clearly separate the modality-specific and shared factors. In this work, we propose a novel framework, termed **D**isentangled **C**ross-**M**odal Representation Learning with **E**nhanced **M**utual Supervision (DCMEM). Specifically, our model disentangles the common and distinct information across modalities and regularizes the shared representation learned from each modality in a mutually supervised manner. Moreover, we incorporate the information bottleneck principle into our model to ensure that the shared and modality-specific factors encode exclusive yet complementary information. Notably, our model is designed to be trainable on both complete and partial multimodal datasets with a valid Evidence Lower Bound. Extensive experimental results demonstrate significant improvements of our model over existing methods on various tasks including cross-modal generation, clustering, and classification",
    "checked": false,
    "id": "9b490ca9b9fd2d22891cfd7adf9f36a3d65331ac",
    "semantic_title": "causal disentanglement and cross-modal alignment for enhanced few-shot learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y1UVWWWGKB": {
    "title": "Bandit Guided Submodular Curriculum for Adaptive Subset Selection",
    "volume": "poster",
    "abstract": "Traditional curriculum learning proceeds from easy to hard samples, yet defining a reliable notion of difficulty remains elusive. Prior work has used submodular functions to induce difficulty scores in curriculum learning. We reinterpret adaptive subset selection and formulate it as a multi-armed bandit problem, where each arm corresponds to a submodular function guiding sample selection. We introduce OnlineSubmod, a novel online greedy policy that optimizes a utility-driven reward and provably achieves no-regret performance under various sampling regimes. Empirically, OnlineSubmod outperforms both traditional curriculum learning and bi-level optimization approaches across vision and language datasets, showing superior accuracy-efficiency tradeoffs. More broadly, we show that validation-driven reward metrics offer a principled way to guide the curriculum schedule. Our code is publicly available at GitHub : https://github.com/efficiency-learning/banditsubmod/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wN7aPpCfSx": {
    "title": "FastDINOv2: Frequency Based Curriculum Learning Improves Robustness and Training Speed",
    "volume": "poster",
    "abstract": "Large-scale vision foundation models such as DINOv2 boast impressive performances by leveraging massive architectures and training datasets. The expense of large-scale pre-training puts such research out of reach for many, hence limiting scientific advancements. We thus propose a novel pretraining strategy for DINOv2 that simultaneously accelerates convergence–and strengthens robustness to common corruptions as a by-product. Our approach involves a frequency filtering curriculum–low-frequency being seen first–and the Gaussian noise patching augmentation. Applied to a ViT-B/16 backbone trained on ImageNet-1K, while pre-training time is reduced by 1.6×–from 16.64 to 10.32 NVIDIA L40S days–and FLOPs by 2.25×, our method still achieves matching robustness in corruption benchmarks (ImageNet-C) and maintains competitive linear probing performance compared with the DINOv2 baseline. This dual benefit of efficiency and robustness makes large-scale self-supervised foundation modeling more attainable, while opening the door to novel exploration around data curriculum and augmentation as a means to improve self-supervised learning models robustness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fbDHv2LQZJ": {
    "title": "Training-Free Safe Text Embedding Guidance for Text-to-Image Diffusion Models",
    "volume": "poster",
    "abstract": "Text-to-image models have recently made significant advances in generating realistic and semantically coherent images, driven by advanced diffusion models and large-scale web-crawled datasets. However, these datasets often contain inappropriate or biased content, raising concerns about the generation of harmful outputs when provided with malicious text prompts. We propose Safe Text embedding Guidance (STG), a training-free approach to improve the safety of diffusion models by guiding the text embeddings during sampling. STG adjusts the text embeddings based on a safety function evaluated on the expected final denoised image, allowing the model to generate safer outputs without additional training. Theoretically, we show that STG aligns the underlying model distribution with safety constraints, thereby achieving safer outputs while minimally affecting generation quality. Experiments on various safety scenarios, including nudity, violence, and artist-style removal, show that STG consistently outperforms both training-based and training-free baselines in removing unsafe content while preserving the core semantic intent of input prompts. Our code is available at https://github.com/aailab-kaist/STG",
    "checked": true,
    "id": "abf3802443ba35a2fa672496e753ffff7841b503",
    "semantic_title": "training-free safe text embedding guidance for text-to-image diffusion models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1frqf6iY4v": {
    "title": "Learning conformational ensembles of proteins based on backbone geometry",
    "volume": "poster",
    "abstract": "Deep generative models have recently been proposed for sampling protein conformations from the Boltzmann distribution, as an alternative to often prohibitively expensive Molecular Dynamics simulations. However, current state-of-the-art approaches rely on fine-tuning pre-trained folding models and evolutionary sequence information, limiting their applicability and efficiency, and introducing potential biases. In this work, we propose a flow matching model for sampling protein conformations based solely on backbone geometry - BBFlow. We introduce a geometric encoding of the backbone equilibrium structure as input and propose to condition not only the flow but also the prior distribution on the respective equilibrium structure, eliminating the need for evolutionary information. The resulting model is orders of magnitudes faster than current state-of-the-art approaches at comparable accuracy, is transferable to multi-chain proteins, and can be trained from scratch in a few GPU days. In our experiments, we demonstrate that the proposed model achieves competitive performance with reduced inference time, across not only an established benchmark of naturally occurring proteins but also de novo proteins, for which evolutionary information is scarce or absent. BBFlow is available at https://github.com/graeter-group/bbflow",
    "checked": true,
    "id": "971f5956f35d037ee8afdde744d4c16ccdf13551",
    "semantic_title": "learning conformational ensembles of proteins based on backbone geometry",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=JTNvG0jTaJ": {
    "title": "Constrained Best Arm Identification",
    "volume": "poster",
    "abstract": "In real-world decision-making problems, one needs to pick among multiple policies the one that performs best while respecting economic constraints. This motivates the problem of constrained best-arm identification for bandit problems where every arm is a joint distribution of reward and cost. We investigate the general case where reward and cost are dependent. The goal is to accurately identify the arm with the highest mean reward among all arms whose mean cost is below a given threshold. We prove information-theoretic lower bounds on the sample complexity for three models: Gaussian with fixed covariance, Gaussian with unknown covariance, and non-parametric distributions of rectangular support. We propose a combination of a sampling and a stopping rule that correctly identifies the constrained best arm and matches the optimal sample complexities for each of the three models. Simulations demonstrate the performance of our algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5P5YgohyBZ": {
    "title": "Towards Visualization-of-Thought Jailbreak Attack against Large Visual Language Models",
    "volume": "poster",
    "abstract": "As Visual Language Models (VLMs) continue to evolve, they have demonstrated increasingly sophisticated logical reasoning capabilities and multimodal thought generation, opening doors to widespread applications. However, this advancement raises serious concerns about content security, particularly when these models process complex multimodal inputs requiring intricate reasoning. When faced with these safety challenges, the critical competition between logical reasoning and safety objectives of VLMs is often overlooked in previous works. In this paper, we introduce Visualization-of-Thought Attack (\\textbf{VoTA}), a novel and automated attack framework that strategically constructs chains of images with risky visual thoughts to challenge victim models. Our attack provokes the inherent conflict between the model's logical processing and safety protocols, ultimately leading to the generation of unsafe content. Through comprehensive experiments, VoTA achieves remarkable effectiveness, improving the average attack success rate (ASR) by 26.71\\% (from 63.70\\% to 90.41\\%) on 9 open-source and 6 commercial VLMs, compared to the state-of-the-art methods. These results expose a critical vulnerability: current VLMs struggle to maintain safety guarantees when processing insecure multimodal visualization-of-thought inputs, highlighting the urgency and necessity of enhancing safety alignment. Our code and dataset are available at https://github.com/Hongqiong12/VoTA. Content Warning: This paper contains harmful contents that may be offensive",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jsln9ZyMl4": {
    "title": "The Cost of Robustness: Tighter Bounds on Parameter Complexity for Robust Memorization in ReLU Nets",
    "volume": "poster",
    "abstract": "We study the parameter complexity of robust memorization for ReLU networks: the number of parameters required to interpolate any dataset with $\\epsilon$-separation between differently labeled points, while ensuring predictions remain consistent within a $\\mu$-ball around each training example. We establish upper and lower bounds on the parameter count as a function of the robustness ratio $\\rho = \\mu / \\epsilon$. Unlike prior work, we provide a fine-grained analysis across the entire range $\\rho \\in (0,1)$ and obtain tighter upper and lower bounds that improve upon existing results. Our findings reveal that the parameter complexity of robust memorization matches that of non-robust memorization when $\\rho$ is small, but grows with increasing $\\rho$. As a special case, when the input dimension is comparable to or exceeds the dataset size, our bounds become tight (up to logarithmic factors) across the entire range of $\\rho$",
    "checked": true,
    "id": "5741ac501f19981d40160f80ed3ab59388936747",
    "semantic_title": "the cost of robustness: tighter bounds on parameter complexity for robust memorization in relu nets",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=baNBqpzvMT": {
    "title": "Mixing Expert Knowledge: Bring Human Thoughts Back To the Game of Go",
    "volume": "poster",
    "abstract": "Large language models (LLMs) have demonstrated exceptional performance in reasoning tasks such as mathematics and coding, matching or surpassing human capabilities. However, these impressive reasoning abilities face significant challenges in specialized domains. Taking Go as an example, although AlphaGo has established the high performance ceiling of AI systems in Go, mainstream LLMs still struggle to reach even beginner-level proficiency, let alone perform natural language reasoning. This performance gap between general-purpose LLMs and domain experts is significantly limiting the application of LLMs on a wider range of domain-specific tasks. In this work, we aim to bridge the divide between LLMs' general reasoning capabilities and expert knowledge in domain-specific tasks. We perform mixed fine-tuning with structured Go expertise and general long Chain-of-Thought (CoT) reasoning data as a cold start, followed by reinforcement learning to integrate expert knowledge in Go with general reasoning capabilities. Through this methodology, we present LoGos, a powerful LLM that not only maintains outstanding general reasoning abilities, but also conducts Go gameplay in natural language, demonstrating effective strategic reasoning and accurate next-move prediction. LoGos achieves performance comparable to human professional players, substantially surpassing all existing LLMs. Through this work, we aim to contribute insights on applying general LLM reasoning capabilities to specialized domains. We will release the first large-scale Go dataset for LLM training, the first LLM Go evaluation benchmark, and the first general LLM that reaches human expert-level performance in Go",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BJ3z1hYuKx": {
    "title": "MetaKoopman: Bayesian Meta-Learning of Koopman Operators for Modeling Structured Dynamics under Distribution Shifts",
    "volume": "poster",
    "abstract": "Modeling and forecasting nonlinear dynamics under distribution shifts is essential for robust decision-making in real-world systems. In this work, we propose **MetaKoopman**, a Bayesian meta-learning framework for modeling nonlinear dynamics through linear latent representations. MetaKoopman learns a Matrix Normal-Inverse Wishart (*MNIW*) prior over the Koopman operator, enabling closed-form Bayesian updates conditioned on recent trajectory segments. Moreover, it provides a closed-form posterior predictive distribution over future state trajectories, capturing both epistemic and aleatoric uncertainty in the learned dynamics. We evaluate MetaKoopman on a full-scale autonomous truck and trailer system across a wide range of adverse winter scenarios—including snow, ice, and mixed-friction conditions—as well as in simulated control tasks with diverse distribution shifts. MetaKoopman consistently outperforms prior approaches in multi-step prediction accuracy, uncertainty calibration and robustness to distributional shifts. Field experiments further demonstrate its effectiveness in dynamically feasible motion planning, particularly during evasive maneuvers and operation at the limits of traction. Project website: [https://mahmoud-selim.github.io/MetaKoopman/](https://mahmoud-selim.github.io/MetaKoopman/)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8W8SRZIpJP": {
    "title": "Spike-RetinexFormer: Rethinking Low-light Image Enhancement with Spiking Neural Networks",
    "volume": "poster",
    "abstract": "Low-light image enhancement (LLIE) aims to improve the visibility and quality of images captured under poor illumination. However, existing deep enhancement methods often underemphasize computational efficiency, leading to high energy and memory costs. We propose \\textbf{Spike-RetinexFormer}, a novel LLIE architecture that synergistically integrates Retinex theory, spiking neural networks (SNNs) and a Transformer-based design. Leveraging sparse spike-driven computation, the model reduces theoretical compute energy and memory traffic relative to ANN counterparts. Across standard benchmarks, the method matches or surpasses strong ANNs (25.50 dB on LOL-v1; 30.37 dB on SDSD-out) with comparable parameters and lower theoretical energy. Our work pioneers the synergistic integration of SNNs into Transformer architectures for LLIE, establishing a compelling pathway toward powerful, energy-efficient low-level vision on resource-constrained platforms",
    "checked": false,
    "id": "39c3fbb5e9f7628725008c95b06255bd1599fb3d",
    "semantic_title": "sgm-transformer: rethinking gradient information loss and compensation in spiking neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3JoQTGhUzz": {
    "title": "IPAD: Inverse Prompt for AI Detection - A Robust and Interpretable LLM-Generated Text Detector",
    "volume": "poster",
    "abstract": "Large Language Models (LLMs) have attained human-level fluency in text generation, which complicates the distinguishing between human-written and LLM generated texts. This increases the risk of misuse and highlights the need for reliable detectors. Yet, existing detectors exhibit poor robustness on out-of-distribution (OOD) data and attacked data, which is critical for real-world scenarios. Also, they struggle to provide interpretable evidence to support their decisions, thus undermining reliability. In light of these challenges, we propose IPAD (Inverse Prompt for AI Detection), a novel framework consisting of a Prompt Inverter that identifies predicted prompts that could have generated the input text, and two Distinguishers that examine the probability that the input texts align with the predicted prompts. Empirical evaluations demonstrate that IPAD outperforms the strongest baselines by 9.05% (Average Recall) on in-distribution data, 12.93% (AUROC) on out-of-distribution (OOD) data, and 5.48% (AUROC) on attacked data. IPAD also performs robust on structured datasets. Furthermore, an interpretability assessment is conducted to illustrate that IPAD enhances the AI detection trustworthiness by allowing users to directly examine the decision-making evidence, which provides interpretable support for its state-of-the-art detection results",
    "checked": false,
    "id": "2bb989fc74118d77c27678749295ec9b7ef5a37e",
    "semantic_title": "ipad: inverse prompt for ai detection - a robust and explainable llm-generated text detector",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=IAGbhDARZd": {
    "title": "Making Classic GNNs Strong Baselines Across Varying Homophily: A Smoothness–Generalization Perspective",
    "volume": "poster",
    "abstract": "Graph Neural Networks (GNNs) have achieved great success but are often considered to be challenged by varying levels of homophily in graphs. Recent empirical studies have surprisingly shown that homophilic GNNs can perform well across datasets of different homophily levels with proper hyperparameter tuning, but the underlying theory and effective architectures remain unclear. To advance GNN universality across varying homophily, we theoretically revisit GNN message passing and uncover a novel \\textit{smoothness-generalization dilemma}, where increasing hops inevitably enhances smoothness at the cost of generalization. This dilemma hinders learning in high-order homophilic neighborhoods and all heterophilic ones, where generalization is critical due to complex neighborhood class distributions that are sensitive to shifts induced by noise or sparsity. To address this, we introduce the Inceptive Graph Neural Network (IGNN) built on three simple yet effective design principles, which alleviate the dilemma by enabling distinct hop-wise generalization alongside improved overall generalization with adaptive smoothness. Benchmarking against 30 baselines demonstrates IGNN's superiority and reveals notable universality in certain homophilic GNN variants. Our code and datasets are available at \\href{https://github.com/galogm/IGNN}{https://github.com/galogm/IGNN}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9QyNYxKeKr": {
    "title": "Unveiling the Uncertainty in Embodied and Operational Carbon of Large AI Models through a Probabilistic Carbon Accounting Model",
    "volume": "poster",
    "abstract": "The rapid growth of large AI models has raised significant environmental concerns due to their substantial carbon footprint. Existing carbon accounting methods for AI models are fundamentally deterministic and fail to account for inherent uncertainties in embodied and operational carbon emissions. Our work aims to investigate the effect of these uncertainties on embodied and operational carbon footprint estimates for large AI models. We propose a Probabilistic Carbon Accounting Model (PCAM), which quantifies uncertainties in the carbon accounting of large AI models. We develop parameter models to quantify key components (processors, memory, storage) in the carbon footprint of AI models. To characterize the distribution of the parameters, we develop a carbon dataset by aggregating related data from various sources. Then, we generate the probabilistic distribution of the parameters from the collected dataset. We compare the performance of PCAM with LLMCarbon, the state-of-the-art carbon accounting method for large AI models. PCAM achieves $\\leq7.44\\%$ error compared to LLMCarbon's $\\leq108.51\\%$",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tmtUA2X57D": {
    "title": "Graph-Theoretic Insights into Bayesian Personalized Ranking for Recommendation",
    "volume": "poster",
    "abstract": "Graph self-supervised learning (GSL) is essential for processing graph-structured data, reducing the need for manual labeling. Traditionally, this paradigm has extensively utilized Bayesian Personalized Ranking (BPR) as its primary loss function. Despite its widespread application, the theoretical analysis of its node relations evaluation have remained largely unexplored. This paper employs recent advancements in latent hyperbolic geometry to deepen our understanding of node relationships from a graph-theoretical perspective. We analyze BPR's limitations, particularly its reliance on local connectivity through 2-hop paths, which overlooks global connectivity and the broader topological structure. To address these shortcomings, we purpose a novel loss function, BPR+, designed to encompass even-hop paths and better capture global connectivity and topological nuances. This approach facilitates a more detailed measurement of user-item relationships and improves the granularity of relationship assessments. We validate BPR+ through extensive empirical testing across five real-world datasets and demonstrate its efficacy in refining graph self-supervised learning frameworks. Additionally, we explore the application of BPR+ in drug repositioning, highlighting its potential to support pharmaceutical research and development. Our findings not only illuminate the success factors of previous methodologies but also offer new theoretical insights into this learning paradigm",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xMiKDqxEE8": {
    "title": "From Linear to Nonlinear: Provable Weak-to-Strong Generalization through Feature Learning",
    "volume": "poster",
    "abstract": "Weak-to-strong generalization refers to the phenomenon where a stronger model trained under supervision from a weaker one can outperform its teacher. While prior studies aim to explain this effect, most theoretical insights are limited to abstract frameworks or linear/random feature models. In this paper, we provide a formal analysis of weak-to-strong generalization from a linear CNN (weak) to a two-layer ReLU CNN (strong). We consider structured data composed of label-dependent signals of varying difficulty and label-independent noise, and analyze gradient descent dynamics when the strong model is trained on data labeled by the pretrained weak model. Our analysis identifies two regimes—data-scarce and data-abundant—based on the signal-to-noise characteristics of the dataset, and reveals distinct mechanisms of weak-to-strong generalization. In the data-scarce regime, generalization occurs via benign overfitting or fails via harmful overfitting, depending on the amount of data, and we characterize the transition boundary. In the data-abundant regime, generalization emerges in the early phase through label correction, but we observe that overtraining can subsequently degrade performance",
    "checked": true,
    "id": "ba85cd62e7e3fec44589078d1192ebf445d39dd3",
    "semantic_title": "from linear to nonlinear: provable weak-to-strong generalization through feature learning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=E9EwDc45f8": {
    "title": "STAR: Efficient Preference-based Reinforcement Learning via Dual Regularization",
    "volume": "poster",
    "abstract": "Preference-based reinforcement learning (PbRL) bypasses complex reward engineering by learning from human feedback. However, due to the high cost of obtaining feedback, PbRL typically relies on a limited set of preference-labeled samples. This data scarcity introduces two key inefficiencies: (1) the reward model overfits to the limited feedback, leading to poor generalization to unseen samples, and (2) the agent exploits the learned reward model, exacerbating overestimation of action values in temporal difference (TD) learning. To address these issues, we propose STAR, an efficient PbRL method that integrates preference margin regularization and policy regularization. Preference margin regularization mitigates overfitting by introducing a bounded margin in reward optimization, preventing excessive bias toward specific feedback. Policy regularization bootstraps a conservative estimate $\\widehat{Q}$ from well-supported state-action pairs in the replay memory, reducing overestimation during policy learning. Experimental results show that STAR improves feedback efficiency, achieving 34.8\\% higher performance in online settings and 29.7\\% in offline settings compared to state-of-the-art methods. Ablation studies confirm that STAR facilitates more robust reward and value function learning. The videos of this project are released at https://sites.google.com/view/pbrl-star",
    "checked": false,
    "id": "8e4fb48e0882066cc44636fbd812607909e191a7",
    "semantic_title": "efficient preference-based reinforcement learning via aligned experience estimation",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=3BTqvtwZYY": {
    "title": "CURV: Coherent Uncertainty-Aware Reasoning in Vision-Language Models for X-Ray Report Generation",
    "volume": "poster",
    "abstract": "Vision-language models have been explored for radiology report generation with promising results. Yet, uncertainty elaborated in findings and the reasoning process for reaching clinical impressions are seldom explicitly modeled, reducing the clinical accuracy and trustworthiness of the generated reports. We present CURV, a novel framework that alleviates the limitations through integrated awareness of uncertainty and explicit reasoning capabilities. Our approach consists of three key components: (1) an uncertainty modeling mechanism that teaches the model to recognize and express appropriate levels of diagnostic confidence, (2) a structured reasoning framework that generates intermediate explanatory steps connecting visual findings to clinical impressions, and (3) a reasoning coherence reward that ensures logical consistency among findings, reasoning, and impressions. We implement CURV through a three-stage training pipeline that combines uncertainty-aware fine-tuning, reasoning initialization, and reinforcement learning. In particular, we adopt a comprehensive reward function addresses multiple aspects of report quality, incorporating medical term matching, uncertainty expression evaluation, and semantic coherence evaluation. Experimental results demonstrate that CURV generates clinically relevant reports with appropriate uncertainty expressions and transparent reasoning traces, significantly outperforming previous methods. CURV represents a substantial advancement toward interpretable and trustworthy AI-generated radiology reports, with broader implications for the deployment of vision-language models in high-stakes clinical environments where uncertainty awareness and reasoning transparency are essential",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rRHuBZdDfY": {
    "title": "EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving",
    "volume": "poster",
    "abstract": "We introduce EvaLearn, a pioneering benchmark designed to evaluate large language models (LLMs) on their learning capability and efficiency in challenging tasks, a critical, yet underexplored aspect of model potential. EvaLearn contains 648 challenging problems across six task types, grouped into 182 sequences, each sequence dedicated to one task type. Diverging from most existing benchmarks that evaluate models in parallel, EvaLearn requires models to solve problems sequentially, allowing them to leverage the experience gained from previous solutions. EvaLearn provides five comprehensive automated metrics to evaluate models and quantify their learning capability and efficiency. We extensively benchmark nine frontier models and observe varied performance profiles: some models, such as Claude-3.7-sonnet, start with moderate initial performance but exhibit strong learning ability, while some models struggle to benefit from experience and may even show negative transfer. Moreover, we investigate model performance under two learning settings and find that instance-level rubrics and teacher-model feedback further facilitate model learning. Importantly, we observe that current LLMs with stronger static abilities do not show a clear advantage in learning capability across all tasks, highlighting that EvaLearn evaluates a new dimension of model performance. We hope EvaLearn provides a novel evaluation perspective for assessing LLM potential and understanding the gap between models and human capabilities, promoting the development of deeper and more dynamic evaluation approaches. All datasets, the automatic evaluation framework, and the results studied in this paper are available in the supplementary materials",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FOkbHmaW0I": {
    "title": "Simple and Efficient Heterogeneous Temporal Graph Neural Network",
    "volume": "poster",
    "abstract": "Heterogeneous temporal graphs (HTGs) are ubiquitous data structures in the real world. Recently, to enhance representation learning on HTGs, numerous attention-based neural networks have been proposed. Despite these successes, existing methods rely on a decoupled temporal and spatial learning paradigm, which weakens interactions of spatio-temporal information and leads to a high model complexity. To bridge this gap, we propose a novel learning paradigm for HTGs called Simple and Efficient Heterogeneous Temporal Graph Neural Network (SE-HTGNN). Specifically, we innovatively integrate temporal modeling into spatial learning via a novel dynamic attention mechanism, which substantially reduces model complexity while enhancing discriminative representation learning on HTGs. Additionally, to comprehensively and adaptively understand HTGs, we leverage large language models to prompt SE-HTGNN, enabling the model to capture the implicit properties of node types as prior knowledge. Extensive experiments demonstrate that SE-HTGNN achieves up to 10× speed-up over the state-of-the-art and latest baseline while maintaining the best forecasting accuracy",
    "checked": true,
    "id": "fe902ace4c9c5eb21ca5096f709a126c25df7eb4",
    "semantic_title": "simple and efficient heterogeneous temporal graph neural network",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VD22PY0fZm": {
    "title": "Rethinking Out-of-Distribution Detection and Generalization with Collective Behavior Dynamics",
    "volume": "poster",
    "abstract": "Out-of-distribution (OOD) problems commonly occur when models process data with a distribution significantly deviates from the in-distribution (InD) training data. In this paper, we hypothesize that a $\\textit{field}$ or $\\textit{potential}$ more essential than features exists, and features are not the ultimate essence of the data but rather manifestations of them during training. we investigate OOD problems from the perspective of collective behavior dynamics. With this in mind, we first treat the output of the feature extractor as charged particles and investigate their collective behavior dynamics within a self-consistent electric field. Then, to characterize the relationship between OOD problems and dynamical equations, we introduce the $\\textit{basin of attraction}$ and prove that its boundary can be represented as the zero level set of a differentiable function of the potential, $\\textit{i.e.}$, the spatial integral of field. We further demonstrate that: $\\textit{i)}$ InD and OOD inputs can be effectively separated based on whether they are steady state solutions for specific field conditions, enabling robust OOD detection and outperforming prior methods over three benchmarks. $\\textit{ii)}$ the generalization capability correlates positively with the basin of attraction. By analyzing the dynamics of perturbations, we propose that the potential is well-characterized by a Fourier-domain form of the Poisson equation. Evaluated on six benchmark datasets, our method rivals the SoTA approaches for OOD generalization and can be seamlessly integrated with them to deliver additional gains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JSSvYZKvL8": {
    "title": "FracFace: Breaking The Visual Clues—Fractal-Based Privacy-Preserving Face Recognition",
    "volume": "poster",
    "abstract": "Face recognition is essential for identity authentication, but the rich visual clues in facial images pose significant privacy risks, highlighting the critical importance of privacy-preserving solutions. For instance, numerous studies have shown that generative models are capable of effectively performing reconstruction attacks that result in the restoration of original visual clues. To mitigate this threat, we introduce FracFace, a fractal-based privacy-preserving face recognition framework. This approach effectively weakens the visual clues that can be exploited by reconstruction attacks by disrupting the spatial structure in frequency domain features, while retaining the vital visual clues required for identity recognition. To achieve this, we craft a Frequency Channels Refining module that reduces sparsity in the frequency domain. It suppresses visual clues that could be exploited by reconstruction attacks, while preserving features indispensable for recognition, thus making these attacks more challenging. More significantly, we design a Frequency Fractal Mapping module that obfuscates deep representations by remapping refined frequency channels into a fractal-based privacy structure. By leveraging the self-similarity of fractals, this module preserves identity relevant features while enhancing defense capabilities, thereby improving the overall robustness of the protection scheme. Experiments conducted on multiple public face recognition benchmarks demonstrate that the proposed FracFace significantly reduces the visual recoverability of facial features, while maintaining high recognition accuracy, as well as the superiorities over state-of-the-art privacy protection approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=caSQXNsxXX": {
    "title": "An Adaptive Quantum Circuit of Dempster's Rule of Combination for Uncertain Pattern Classification",
    "volume": "poster",
    "abstract": "In pattern classification, efficient uncertainty reasoning plays a critical role, particularly in real-time applications involving noisy data, ambiguous class boundaries, or overlapping categories. Leveraging the advanced computational power of quantum computing, an Adaptive Quantum Circuit for Dempster's Rule of Combination (AQC-DRC) is proposed to address efficient classification under uncertain environments. The AQC-DRC is developed within the framework of quantum evidence theory (QET) and facilitates decision-making based on quantum basic probability and plausibility levels, which is a generalized Bayesian inference method. The AQC-DRC provides a deterministic computation of DRC, ensuring that quantum fusion outcomes in uncertain pattern classification are exactly aligned with those of the classical method, while simultaneously achieving exponential reductions in the computational complexity of evidence combination and significantly improving fusion efficiency. It is founded that the quantum basic probability amplitude function in QET, as a generalized quantum probability amplitude, can be naturally utilized to express the quantum amplitude encoding. In addition, the quantum basic probability in QET, as a generalized quantum probability, naturally forms a quantum basic probability distribution and can be used to represent quantum measurement outcomes for quantum basic probability level decision-making. Furthermore, the quantum plausibility function in QET also can be naturally used to express the quantum measurement outcomes for quantum plausibility level decision-making. These findings enrich the physical understanding of quantum amplitude encoding and quantum measurement outcomes, offering broad application prospects for representing and processing uncertain knowledge in pattern classification",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3hnqwOq7iT": {
    "title": "TARFVAE: Efficient One-Step Generative Time Series Forecasting via TARFLOW based VAE",
    "volume": "poster",
    "abstract": "Time series data is ubiquitous, with forecasting applications spanning from finance to healthcare. Beyond popular deterministic methods, generative models are gaining attention due to advancements in areas like image synthesis and video generation, as well as their inherent ability to provide probabilistic predictions. However, existing generative approaches mostly involve recurrent generative operations or repeated denoising steps, making the prediction laborious, particularly for long-term forecasting. Most of them only conduct experiments for relatively short-term forecasting, with limited comparison to deterministic methods in long-term forecasting, leaving their practical advantages unclear. This paper presents TARFVAE, a novel generative framework that combines the Transformer-based autoregressive flow (TARFLOW) and variational autoencoder (VAE) for efficient one-step generative time series forecasting. Inspired by the rethinking that complex architectures for extracting time series representations might not be necessary, we add a flow module, TARFLOW, to VAE to promote spontaneous learning of latent variables that benefit predictions. TARFLOW enhances VAE's posterior estimation by breaking the Gaussian assumption, thereby enabling a more informative latent space. TARFVAE uses only the forward process of TARFLOW, avoiding autoregressive inverse operations and thus ensuring fast generation. During generation, it samples from the prior latent space and directly generates full-horizon forecasts via the VAE decoder. With simple MLP modules, TARFVAE achieves superior performance over state-of-the-art deterministic and generative models across different forecast horizons on benchmark datasets while maintaining efficient prediction speed, demonstrating its effectiveness as an efficient and powerful solution for generative time series forecasting. Our code is available at https://github.com/Gavine77/TARFVAE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2jQJ7aNdT1": {
    "title": "FairDICE: Fairness-Driven Offline Multi-Objective Reinforcement Learning",
    "volume": "poster",
    "abstract": "Multi-objective reinforcement learning (MORL) aims to optimize policies in the presence of conflicting objectives, where linear scalarization is commonly used to reduce vector-valued returns into scalar signals. While effective for certain preferences, this approach cannot capture fairness-oriented goals such as Nash social welfare or max-min fairness, which require nonlinear and non-additive trade-offs. Although several online algorithms have been proposed for specific fairness objectives, a unified approach for optimizing nonlinear welfare criteria in the offline setting—where learning must proceed from a fixed dataset—remains unexplored. In this work, we present FairDICE, the first offline MORL framework that directly optimizes nonlinear welfare objective. FairDICE leverages distribution correction estimation to jointly account for welfare maximization and distributional regularization, enabling stable and sample-efficient learning without requiring explicit preference weights or exhaustive weight search. Across multiple offline benchmarks, FairDICE demonstrates strong fairness-aware performance compared to existing baselines",
    "checked": true,
    "id": "e78c31ae7033d95f6ffe2efbe34a8ad905c87e1f",
    "semantic_title": "fairdice: fairness-driven offline multi-objective reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o4lJrA0s3I": {
    "title": "One for All: Universal Topological Primitive Transfer for Graph Structure Learning",
    "volume": "poster",
    "abstract": "The non-Euclidean geometry inherent in graph structures fundamentally impedes cross-graph knowledge transfer. Drawing inspiration from texture transfer in computer vision, we pioneer topological primitives as transferable semantic units for graph structural knowledge. To address three critical barriers - the absence of specialized benchmarks, aligned semantic representations, and systematic transfer methodologies - we present G²SN-Transfer, a unified framework comprising: (i) TopoGraph-Mapping that transforms non-Euclidean graphs into transferable sequences via topological primitive distribution dictionaries; (ii) G²SN, a dual-stream architecture learning text-topology aligned representations through contrastive alignment; and (iii) AdaCross-Transfer, a data-adaptive knowledge transfer mechanism leveraging cross-attention for both full-parameter and parameter-frozen scenarios. Particularly, G²SN is a dual-stream sequence network driven by ordinary differential equations, and our theoretical analysis establishes the convergence guarantee of G²SN. We construct STA-18, the first large-scale benchmark with aligned topological primitive-text pairs across 18 diverse graph datasets. Comprehensive evaluations demonstrate that G²SN achieves state-of-the-art performance on four structural learning tasks (average 3.2\\% F1-score improvement), while our transfer method yields consistent enhancements across 13 downstream tasks (5.2\\% average gains) including 10 large-scale graph datasets. The datasets and code are available at https://anonymous.4open.science/r/UGSKT-C10E/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dbq6NZfi3c": {
    "title": "EVOREFUSE: Evolutionary Prompt Optimization for Evaluation and Mitigation of LLM Over-Refusal to Pseudo-Malicious Instructions",
    "volume": "poster",
    "abstract": "Large language models (LLMs) frequently refuse to respond to pseudo-malicious instructions: semantically harmless input queries triggering unnecessary LLM refusals due to conservative safety alignment, significantly impairing user experience. Collecting such instructions is crucial for evaluating and mitigating over-refusals, but existing instruction curation methods, like manual creation or instruction rewriting, either lack scalability or fail to produce sufficiently diverse and effective refusal-inducing prompts. To address these limitations, we introduce EVOREFUSE, a prompt optimization approach that generates diverse pseudo-malicious instructions consistently eliciting confident refusals across LLMs. EVOREFUSE employs an evolutionary algorithm exploring the instruction space in more diverse directions than existing methods via mutation strategies and recombination, and iteratively evolves seed instructions to maximize evidence lower bound on LLM refusal probability. Using EVOREFUSE, we create two novel datasets: EVOREFUSE-TEST, a benchmark of 582 pseudo-malicious instructions that outperforms the next-best benchmark with 85.34% higher average refusal triggering rate across 9 LLMs, 34.86% greater lexical diversity, and 40.03% improved LLM response confidence scores; and EVOREFUSE-ALIGN, which provides 3,000 pseudo-malicious instructions with responses for supervised and preference-based alignment training. LLAMA3.1-8B-INSTRUCT supervisedly fine-tuned on EVOREFUSE-ALIGN achieves up to 29.85% fewer over-refusals than models trained on the second-best alignment dataset, without compromising safety. Our analysis with EVOREFUSE-TEST reveals models trigger over-refusals by overly focusing on sensitive keywords while ignoring broader context. Our code and datasets are available at https://github.com/FishT0ucher/EVOREFUSE",
    "checked": true,
    "id": "19427f874ac5806a2640e5a8597dcdecb6491c17",
    "semantic_title": "evorefuse: evolutionary prompt optimization for evaluation and mitigation of llm over-refusal to pseudo-malicious instructions",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=4LSulRbbeL": {
    "title": "Dynamic Regret Reduces to Kernelized Static Regret",
    "volume": "poster",
    "abstract": "We study dynamic regret in online convex optimization, where the objective is to achieve low cumulative loss relative to an arbitrary benchmark sequence. By observing that competing with an arbitrary sequence of comparators $u_{1},\\ldots,u_{T}$ in $\\mathcal{W}\\subseteq\\mathbb{R}^{d}$ can be reframed as competing with a *fixed* comparator *function* $u:[1,T]\\to \\mathcal{W}$, we cast dynamic regret minimization as a *static regret* problem in a *function space*. By carefully constructing a suitable function space in the form of a Reproducing Kernel Hilbert Space (RKHS), our reduction enables us to recover the optimal $R_{T}(u_{1},\\ldots,u_{T}) = \\mathcal{O}(\\sqrt{\\sum_{t}\\\\|u_{t}-u_{t-1}\\\\|T})$ dynamic regret guarantee in the setting of linear losses, and yields new scale-free and directionally-adaptive dynamic regret guarantees. Moreover, unlike prior dynamic-to-static reductions---which are valid only for linear losses---our reduction holds for *any* sequence of losses, allowing us to recover $\\mathcal{O}\\big(\\\\|u\\\\|^2+d_{\\mathrm{eff}}(\\lambda)\\ln T\\big)$ bounds when the losses have meaningful curvature, where $d_{\\mathrm{eff}}(\\lambda)$ is a measure of complexity of the RKHS. Despite working in an infinite-dimensional space, the resulting reduction leads to algorithms that are computable in practice, due to the reproducing property of RKHSs",
    "checked": true,
    "id": "38865f9488b2ec41e35cb88c97598dd01b02615c",
    "semantic_title": "dynamic regret reduces to kernelized static regret",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xpVkYQofw9": {
    "title": "A Differential and Pointwise Control Approach to Reinforcement Learning",
    "volume": "poster",
    "abstract": "Reinforcement learning (RL) in continuous state-action spaces remains challenging in scientific computing due to poor sample efficiency and lack of pathwise physical consistency. We introduce Differential Reinforcement Learning (Differential RL), a novel framework that reformulates RL from a continuous-time control perspective via a differential dual formulation. This induces a Hamiltonian structure that embeds physics priors and ensures consistent trajectories without requiring explicit constraints. To implement Differential RL, we develop Differential Policy Optimization (dfPO), a pointwise, stage-wise algorithm that refines local movement operators along the trajectory for improved sample efficiency and dynamic alignment. We establish pointwise convergence guarantees, a property not available in standard RL, and derive a competitive theoretical regret bound of $\\mathcal{O}(K^{5/6})$. Empirically, dfPO outperforms standard RL baselines on representative scientific computing tasks, including surface modeling, grid control, and molecular dynamics, under low-data and physics-constrained conditions",
    "checked": false,
    "id": "6561107f44a69dd168b2c301df6bc639ce222d5e",
    "semantic_title": "dpo: a differential and pointwise control approach to reinforcement learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=lYnNzmFt7r": {
    "title": "The Quest for Universal Master Key Filters in DS-CNNs",
    "volume": "poster",
    "abstract": "A recent study has proposed the ``Master Key Filters Hypothesis\" for convolutional neural network filters. This paper extends this hypothesis by radically constraining its scope to a single set of just 8 universal filters that depthwise separable convolutional networks inherently converge to. While conventional DS-CNNs employ thousands of distinct trained filters, our analysis reveals these filters are predominantly linear shifts (ax+b) of our discovered universal set. Through systematic unsupervised search, we extracted these fundamental patterns across different architectures and datasets. Remarkably, networks initialized with these 8 unique frozen filters achieve over 80\\% ImageNet accuracy, and even outperform models with thousands of trainable parameters when applied to smaller datasets. The identified master key filters closely match Difference of Gaussians (DoGs), Gaussians, and their derivatives, structures that are not only fundamental to classical image processing but also strikingly similar to receptive fields in mammalian visual systems. Our findings provide compelling evidence that depthwise convolutional layers naturally gravitate toward this fundamental set of spatial operators regardless of task or architecture. This work offers new insights for understanding generalization and transfer learning through the universal language of these master key filters",
    "checked": true,
    "id": "b16bdf5075dc5fdd908e25cf4667ec6b6f9e3850",
    "semantic_title": "the quest for universal master key filters in ds-cnns",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=b7bOWd3kUL": {
    "title": "Scaling Code-Assisted Chain-of-Thoughts and Instructions for Model Reasoning",
    "volume": "poster",
    "abstract": "Reasoning capability is pivotal for Large Language Models (LLMs) to solve complex tasks, yet achieving reliable and scalable reasoning remains challenging. While Chain-of-Thought (CoT) prompting has become a mainstream approach, existing methods often suffer from uncontrolled generation, insufficient quality, and limited diversity in reasoning paths. Recent efforts leverage code to enhance CoT by grounding reasoning in executable steps, but such methods are typically constrained to predefined mathematical problems, hindering scalability and generalizability. In this work, we propose \\texttt{Caco} (Code-Assisted Chain-of-ThOught), a novel framework that automates the synthesis of high-quality, verifiable, and diverse instruction-CoT reasoning data through code-driven augmentation. Unlike prior work, \\texttt{Caco} first fine-tunes a code-based CoT generator on existing math and programming solutions in a unified code format, then scales the data generation to a large amount of diverse reasoning traces. Crucially, we introduce automated validation via code execution and rule-based filtering to ensure logical correctness and structural diversity, followed by reverse-engineering filtered outputs into natural language instructions and language CoTs to enrich task adaptability. This closed-loop process enables fully automated, scalable synthesis of reasoning data with guaranteed executability. Experiments on our created \\texttt{Caco}-1.3M dataset demonstrate that \\texttt{Caco}-trained models achieve strong competitive performance on mathematical reasoning benchmarks, outperforming existing strong baselines. Further analysis reveals that \\texttt{Caco}'s code-anchored verification and instruction diversity contribute to superior generalization across unseen tasks. Our work establishes a paradigm for building self-sustaining, trustworthy reasoning systems without human intervention",
    "checked": true,
    "id": "f41f93881800bce1893eb7ad6f6bbf2228101b8d",
    "semantic_title": "scaling code-assisted chain-of-thoughts and instructions for model reasoning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PJrvX7Jz2c": {
    "title": "Scalable Feature Learning on Huge Knowledge Graphs for Downstream Machine Learning",
    "volume": "poster",
    "abstract": "Many machine learning tasks can benefit from external knowledge. Large knowledge graphs store such knowledge, and embedding methods can be used to distill it into ready-to-use vector representations for downstream applications. For this purpose, current models have however two limitations: they are primarily optimized for link prediction, via local contrastive learning, and their application to the largest graphs requires significant engineering effort due to GPU memory limits. To address these, we introduce SEPAL: a Scalable Embedding Propagation ALgorithm for large knowledge graphs designed to produce high-quality embeddings for downstream tasks at scale. The key idea of SEPAL is to ensure global embedding consistency by optimizing embeddings only on a small core of entities, and then propagating them to the rest of the graph with message passing. We evaluate SEPAL on 7 large-scale knowledge graphs and 46 downstream machine learning tasks. Our results show that SEPAL significantly outperforms previous methods on downstream tasks. In addition, SEPAL scales up its base embedding model, enabling fitting huge knowledge graphs on commodity hardware. Our code is available at: <https://github.com/flefebv/sepal.git>",
    "checked": true,
    "id": "3295c2f58bf0e7a410cef557286c655c45da77df",
    "semantic_title": "scalable feature learning on huge knowledge graphs for downstream machine learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XougXwZAHI": {
    "title": "When Kernels Multiply, Clusters Unify: Fusing Embeddings with the Kronecker Product",
    "volume": "poster",
    "abstract": "State-of-the-art embeddings often capture distinct yet complementary discriminative features: For instance, one image embedding model may excel at distinguishing fine-grained textures, while another focuses on object-level structure. Motivated by this observation, we propose a principled approach to fuse such complementary representations through *kernel multiplication*. Multiplying the kernel similarity functions of two embeddings allows their discriminative structures to interact, producing a fused representation whose kernel encodes the union of the clusters identified by each parent embedding. This formulation also provides a natural way to construct *joint kernels* for paired multi-modal data (e.g., image–text tuples), where the product of modality-specific kernels inherits structure from both domains. We highlight that this kernel product is mathematically realized via the *Kronecker product* of the embedding feature maps, yielding our proposed *KrossFuse* framework for embedding fusion. To address the computational cost of the resulting high-dimensional Kronecker space, we further develop *RP-KrossFuse*, a scalable variant that leverages random projections for efficient approximation. As a key application, we use this framework to bridge the performance gap between cross-modal embeddings (e.g., CLIP, BLIP) and unimodal experts (e.g., DINOv2, E5). Experiments show that RP-KrossFuse effectively integrates these models, enhancing modality-specific performance while preserving cross-modal alignment. The project code is available at https://github.com/yokiwuuu/KrossFuse",
    "checked": true,
    "id": "2190633431e3e3dc2143b4b6f6086684a6073ac8",
    "semantic_title": "when kernels multiply, clusters unify: fusing embeddings with the kronecker product",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=lkmlNHuzY4": {
    "title": "ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts",
    "volume": "poster",
    "abstract": "Dataset bias, where data points are skewed to certain concepts, is ubiquitous in machine learning datasets. Yet, systematically identifying these biases is challenging without costly, fine-grained attribute annotations. We present ConceptScope, a scalable and automated framework for analyzing visual datasets by discovering and quantifying human-interpretable concepts using Sparse Autoencoders trained on representations from vision foundation models. ConceptScope categorizes concepts into target, context, and bias types based on their semantic relevance and statistical correlation to class labels, enabling class-level dataset characterization, bias identification, and robustness evaluation through concept-based subgrouping. We validate that ConceptScope captures a wide range of visual concepts, including objects, textures, backgrounds, facial attributes, emotions, and actions, through comparisons with annotated datasets. Furthermore, we show that concept activations produce spatial attributions that align with semantically meaningful image regions. ConceptScope reliably detects known biases (e.g., background bias in Waterbirds) and uncovers previously unannotated ones (e.g, co-occurring objects in ImageNet), offering a practical tool for dataset auditing and model diagnostics",
    "checked": true,
    "id": "ba1846d9d46da94d47264bb2e14b015d05c81fd1",
    "semantic_title": "conceptscope: characterizing dataset bias via disentangled visual concepts",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yiSoT2pHfk": {
    "title": "CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections",
    "volume": "poster",
    "abstract": "Recent advances in enhancing the reasoning ability of Large Language Models (LLMs) have been remarkably successful. LLMs trained with Reinforcement Learning (RL) for reasoning demonstrate strong performance in challenging tasks such as mathematics and coding, even with relatively small model sizes. However, despite these impressive improvements in task accuracy, the assessment of creativity in LLM generations has been largely overlooked in reasoning tasks, in contrast to writing tasks. The lack of research on creativity assessment in reasoning primarily stems from two challenges: (1) the difficulty of defining the range of creativity, and (2) the necessity of human evaluation in the assessment process. To address these challenges, we propose CLAWS, a novel method that defines and classifies mathematical solutions into Typical, Creative, and Hallucinated categories without human evaluation, by leveraging attention weights across prompt sections and output. CLAWS outperforms five existing white-box detection methods—Perplexity, Logit Entropy, Window Entropy, Hidden Score, and Attention Score—on five 7–8B math RL models (DeepSeek, Qwen, Mathstral, OpenMath2, and Oreal). We validate CLAWS on 4,545 math problems collected from 181 math contests (A(J)HSME, AMC, AIME). Our code is available at https://github.com/kkt94/CLAWS",
    "checked": true,
    "id": "7b2e7234c7c983f8c04ebd211e8df57bdcd0ee01",
    "semantic_title": "claws:creativity detection for llm-generated solutions using attention window of sections",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N2bLuwofZ0": {
    "title": "Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics",
    "volume": "poster",
    "abstract": "Large Vision-Language Models (LVLMs) have recently shown great promise in advancing robotics by combining embodied reasoning with robot control. A common approach involves training on embodied reasoning tasks related to robot control using Supervised Fine-Tuning (SFT). However, SFT datasets are often heuristically constructed and not explicitly optimized for improving robot control. Furthermore, SFT often leads to issues such as catastrophic forgetting and reduced generalization performance. To address these limitations, we introduce Robot-R1, a novel framework that leverages reinforcement learning to enhance embodied reasoning specifically for robot control. Robot-R1 learns to predict the next keypoint state required for task completion, conditioned on the current scene image and environment metadata derived from expert demonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples reasoning-based responses and reinforces those that lead to more accurate predictions. Our experiments show that models trained with Robot-R1 outperform SFT methods on embodied reasoning tasks. Despite having only 7B parameters, Robot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action control, such as spatial and movement reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z3OtNSwuXX": {
    "title": "Joint Relational Database Generation via Graph-Conditional Diffusion Models",
    "volume": "poster",
    "abstract": "Building generative models for relational databases (RDBs) is important for many applications, such as privacy-preserving data release and augmenting real datasets. However, most prior works either focus on single-table generation or adapt single-table models to the multi-table setting by relying on autoregressive factorizations and sequential generation. These approaches limit parallelism, restrict flexibility in downstream applications, and compound errors due to commonly made conditional independence assumptions. In this paper, we propose a fundamentally different approach: jointly modeling all tables in an RDB without imposing any table order. By using a natural graph representation of RDBs, we propose the Graph-Conditional Relational Diffusion Model (GRDM), which leverages a graph neural network to jointly denoise row attributes and capture complex inter-table dependencies. Extensive experiments on six real-world RDBs demonstrate that our approach substantially outperforms autoregressive baselines in modeling multi-hop inter-table correlations and achieves state-of-the-art performance on single-table fidelity metrics. Our code is available at https://github.com/ketatam/rdb-diffusion",
    "checked": true,
    "id": "b09830fe95f29e52ef2200a6c2f54dafd9b783cc",
    "semantic_title": "joint relational database generation via graph-conditional diffusion models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=a7bisLzORM": {
    "title": "Wasserstein Convergence of Critically Damped Langevin Diffusions",
    "volume": "poster",
    "abstract": "Score-based Generative Models (SGMs) have achieved impressive performance in data generation across a wide range of applications and benefit from strong theoretical guarantees. Recently, methods inspired by statistical mechanics, in particular, Hamiltonian dynamics, have introduced Critically-damped Langevin Diffusions (CLDs), which define diffusion processes on extended spaces by coupling the data with auxiliary variables. These approaches, along with their associated score-matching and sampling procedures, have been shown to outperform standard diffusion-based samplers numerically. In this paper, we analyze a generalized dynamic that extends classical CLDs by introducing an additional hyperparameter controlling the noise applied to the data coordinate, thereby better exploiting the extended space. We further derive a novel upper bound on the sampling error of CLD-based generative models in the Wasserstein metric. This additional hyperparameter influences the smoothness of sample paths, and our discretization error analysis provides practical guidance for its tuning, leading to improved sampling performance",
    "checked": true,
    "id": "6ed021e4ab42358cf4a24c38303fb59573ae8e3b",
    "semantic_title": "wasserstein convergence of critically damped langevin diffusions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mc0eJHZhW5": {
    "title": "Efficient Low Rank Attention for Long-Context Inference in Large Language Models",
    "volume": "poster",
    "abstract": "As the length of input text grows, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long‐context inference on resource‐constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. We introduce Low Rank Query and Key attention (LRQK), a two‐stage framework that jointly decomposes the full‐precision query and key matrices into compact rank-\\(r\\) factors during the prefill stage, and then uses these low-dimensional projections to compute proxy attention scores in \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the top-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU–CPU cache with a hit-and-miss mechanism that transfers only missing full-precision KV pairs, thereby preserving exact attention outputs while reducing CPU–GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal loss in accuracy. Our code is available at \\url{https://github.com/tenghuilee/LRQK}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sdlLycSeZl": {
    "title": "Learning Memory-Enhanced Improvement Heuristics for Flexible Job Shop Scheduling",
    "volume": "poster",
    "abstract": "The rise of smart manufacturing under Industry 4.0 introduces mass customization and dynamic production, demanding more advanced and flexible scheduling techniques. The flexible job-shop scheduling problem (FJSP) has attracted significant attention due to its complex constraints and strong alignment with real-world production scenarios. Current deep reinforcement learning (DRL)-based approaches to FJSP predominantly employ constructive methods. While effective, they often fall short of reaching (near-)optimal solutions. In contrast, improvement-based methods iteratively explore the neighborhood of initial solutions and are more effective in approaching optimality. However, the flexible machine allocation in FJSP poses significant challenges to the application of this framework, including accurate state representation, effective policy learning, and efficient search strategies. To address these challenges, this paper proposes a $\\textbf{M}$emory-enhanced $\\textbf{I}$mprovement $\\textbf{S}$earch framework with he$\\textbf{t}$erogeneous gr$\\textbf{a}$ph $\\textbf{r}$epresentation—$\\textit{MIStar}$. It employs a novel heterogeneous disjunctive graph that explicitly models the operation sequences on machines to accurately represent scheduling solutions. Moreover, a memory-enhanced heterogeneous graph neural network (MHGNN) is designed for feature extraction, leveraging historical trajectories to enhance the decision-making capability of the policy network. Finally, a parallel greedy search strategy is adopted to explore the solution space, enabling superior solutions with fewer iterations. Extensive experiments on synthetic data and public benchmarks demonstrate that $\\textit{MIStar}$ significantly outperforms both traditional handcrafted improvement heuristics and state-of-the-art DRL-based constructive methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3XuUnUEI7e": {
    "title": "Diversifying Parallel Ergodic Search: A Signature Kernel Evolution Strategy",
    "volume": "poster",
    "abstract": "Effective robotic exploration in continuous domains requires planning trajectories that maximize coverage over a predefined region. A recent development, Stein Variational Ergodic Search (SVES), proposed parallel ergodic exploration (a key approach within the field of robotic exploration), via Stein variational inference that computes a set of candidate trajectories approximating the posterior distribution over the solution space trajectories. While this approach leverages GPU parallelism well, the trajectories in the set might not be distinct enough, leading to a suboptimal set. In this paper, we propose two key methods to diversify the solution set of this approach. First, we leverage the signature kernel within the SVES framework, introducing a pathwise, sequence-sensitive interaction that preserves the Markovian structure of the trajectories and naturally spreads paths across distinct regions of the search space. Second, we propose a derivative-free evolution-strategy interpretation of SVES that exploits batched, GPU-friendly fitness evaluations and can be paired with approximate gradients whenever analytic gradients of the kernel are unavailable or computationally intractable. The resulting method both retains SVES's advantages while diversifying the solution set and extending its reach to black-box objectives. Across planar forest search, 3D quadrotor coverage, and model-predictive control benchmarks, our approach consistently reduces ergodic cost and produces markedly richer trajectory sets than SVES without significant extra tuning effort",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OU6FXkSIe0": {
    "title": "Elastic ViTs from Pretrained Models without Retraining",
    "volume": "poster",
    "abstract": "Vision foundation models achieve remarkable performance but are only available in a limited set of pre-determined sizes, forcing sub-optimal deployment choices under real-world constraints. We introduce SnapViT: single-shot network approximation for pruned Vision Transformers, a new post-pretraining structured pruning method that enables elastic inference across a continuum of compute budgets. Our approach efficiently combines gradient information with cross-network structure correlations, approximated via an evolutionary algorithm, does not require labeled data, generalizes to models without a classification head, and is retraining-free. Experiments on DINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over state-of-the-art methods across various sparsities, requiring less than five minutes on a single A100 GPU to generate elastic models that can be adjusted to any computational budget. Our key contributions include an efficient pruning strategy for pretrained Vision Transformers, a novel evolutionary approximation of Hessian off-diagonal structures, and a self-supervised importance scoring mechanism that maintains strong performance without requiring retraining or labels. Code and pruned models are available at: https://elastic.ashita.nl/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y6tRVadmgo": {
    "title": "S-Crescendo: A Nested Transformer Weaving Framework for Scalable Nonlinear System in S-Domain Representation",
    "volume": "poster",
    "abstract": "Simulation of high-order nonlinear system requires extensive computational resources, especially in modern VLSI backend design where bifurcation-induced instability and chaos-like transient behaviors pose challenges. We present S-Crescendo - a nested transformer weaving framework that synergizes S-domain with neural operators for scalable time-domain prediction in high-order nonlinear networks, alleviating the computational bottlenecks of conventional solvers via Newton-Raphson method. By leveraging the partial-fraction decomposition of an n-th order transfer function into first-order modal terms with repeated poles and residues, our method bypasses the conventional Jacobian matrix-based iterations and efficiently reduces computational complexity from cubic $O(n^3)$ to linear $O(n)$.The proposed architecture seamlessly integrates an S-domain encoder with an attention-based correction operator to simultaneously isolate dominant response and adaptively capture higher-order non-linearities. Validated on order-1 to order-10 networks, our method achieves up to 0.99 test-set \\(R^2\\) accuracy against HSPICE golden waveforms and accelerates simulation by up to 18\\(\\times\\), providing a scalable, physics-aware framework for high-dimensional nonlinear modeling",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l0kR6m9NDT": {
    "title": "Adaptive Time Encoding for Irregular Multivariate Time-Series Classification",
    "volume": "poster",
    "abstract": "Time series are often irregularly sampled with uneven time intervals. In multivariate cases, such irregularities may lead to misaligned observations across variables and varying observation counts, making it difficult to extract intrinsic patterns and degrading the classification performance of deep learning models. In this study, we propose an adaptive time encoding approach to address the challenge of irregular sampling in multivariate time-series classification. Our approach generates latent representations at learnable reference points that capture missingness patterns in irregular sequences, enhancing classification performance. We also introduce consistency regularization techniques to incorporate intricate temporal and intervariable information into the learned representations. Extensive experiments demonstrate that our method achieves state-of-the-art performance with high computational efficiency in irregular multivariate time-series classification tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p37Kd7EQhy": {
    "title": "Prior Forgetting and In-Context Overfitting",
    "volume": "poster",
    "abstract": "In-context learning (ICL) is one of the key capabilities contributing to the great success of LLMs. At test time, ICL is known to operate in the two modes: task recognition and task learning. In this paper, we investigate the emergence and dynamics of the two modes of ICL during pretraining. To provide an analytical understanding of the learning dynamics of the ICL abilities, we investigate the in-context random linear regression problem with a simple linear-attention-based transformer, and define and disentangle the strengths of the task recognition and task learning abilities stored in the transformer model's parameters. We show that, during the pretraining phase, the model first learns the task learning and the task recognition abilities together in the beginning, but it (a) gradually forgets the task recognition ability to recall the priorly learned tasks and (b) relies more on the given context in the later phase, which we call (a) \\textit{prior forgetting} and (b) \\textit{in-context overfitting}, respectively",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=seCBUZYs5c": {
    "title": "Boosting Adversarial Transferability with Spatial Adversarial Alignment",
    "volume": "poster",
    "abstract": "Deep neural networks are vulnerable to adversarial examples that exhibit transferability across various models. Numerous approaches are proposed to enhance the transferability of adversarial examples, including advanced optimization, data augmentation, and model modifications. However, these methods still show limited transferability, partiovovocularly in cross-architecture scenarios, such as from CNN to ViT. To achieve high transferability, we propose a technique termed Spatial Adversarial Alignment (SAA), which employs an alignment loss and leverages a witness model to fine-tune the surrogate model. Specifically, SAA consists of two key parts: spatial-aware alignment and adversarial-aware alignment. First, we minimize the divergences of features between the two models in both global and local regions, facilitating spatial alignment. Second, we introduce a self-adversarial strategy that leverages adversarial examples to impose further constraints, aligning features from an adversarial perspective. Through this alignment, the surrogate model is trained to concentrate on the common features extracted by the witness model. This facilitates adversarial attacks on these shared features, thereby yielding perturbations that exhibit enhanced transferability. Extensive experiments on various architectures on ImageNet show that aligned surrogate models based on SAA can provide higher transferable adversarial examples, especially in cross-architecture attacks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZYHzcZFEGD": {
    "title": "Direct3D-S2: Gigascale 3D Generation Made Easy with Spatial Sparse Attention",
    "volume": "poster",
    "abstract": "Generating high-resolution 3D shapes using volumetric representations such as Signed Distance Functions (SDFs) presents substantial computational and memory challenges. We introduce Direct3D-S2, a scalable 3D generation framework based on sparse volumes that achieves superior output quality with dramatically reduced training costs. Our key innovation is the Spatial Sparse Attention (SSA) mechanism, which greatly enhances the efficiency of Diffusion Transformer (DiT) computations on sparse volumetric data. SSA allows the model to effectively process large token sets within sparse volumes, significantly reducing computational overhead and achieving a 3.9$\\times$ speedup in the forward pass and a 9.6$\\times$ speedup in the backward pass. Our framework also includes a variational autoencoder (VAE) that maintains a consistent sparse volumetric format across input, latent, and output stages. Compared to previous methods with heterogeneous representations in 3D VAE, this unified design significantly improves training efficiency and stability. Our model is trained on public datasets, and experiments demonstrate that Direct3D-S2 not only surpasses state-of-the-art methods in generation quality and efficiency, but also enables training at 1024³ resolution using only 8 GPUs—a task typically requiring at least 32 GPUs for volumetric representations at $256^3$ resolution, thus making gigascale 3D generation both practical and accessible. Project page: https://www.neural4d.com/research-page/direct3d-s2",
    "checked": true,
    "id": "03ebe0fb80d3253a5231d0f1cbac7f3f20fa3aec",
    "semantic_title": "direct3d-s2: gigascale 3d generation made easy with spatial sparse attention",
    "citation_count": 14,
    "authors": []
  },
  "https://openreview.net/forum?id=lUtNvMiW3C": {
    "title": "Addressing Mark Imbalance in Integration-free Marked Temporal Point Processes",
    "volume": "poster",
    "abstract": "Marked Temporal Point Process (MTPP) has been well studied to model the event distribution in marked event streams, which can be used to predict the mark and arrival time of the next event. However, existing studies overlook that the distribution of event marks is highly imbalanced in many real-world applications, with some marks being frequent but others rare. The imbalance poses a significant challenge to the performance of the next event prediction, especially for events of rare marks. To address this issue, we propose a thresholding method, which learns thresholds to tune the mark probability normalized by the mark's prior probability to optimize mark prediction, rather than predicting the mark directly based on the mark probability as in existing studies. In conjunction with this method, we predict the mark first and then the time. In particular, we develop a novel neural Marked Temporal Point Process (MTPP) model to support effective time sampling and estimation of mark probability without computationally expensive numerical improper integration. Extensive experiments on real-world datasets demonstrate the superior performance of our solution against various baselines for the next event mark and time prediction. The code is available at https://github.com/undes1red/IFNMTPP",
    "checked": false,
    "id": "8156a7ac3e2bb31c710173bd484b63021e69cb43",
    "semantic_title": "addressing mark imbalance in integration-free neural marked temporal point processes",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0Zri6HSYaK": {
    "title": "More Than Just Functional: LLM-as-a-Critique for Efficient Code Generation",
    "volume": "poster",
    "abstract": "Large language models (LLMs) have demonstrated remarkable progress in generating functional code, leading to numerous AI-based coding program tools. However, their reliance on the perplexity objective during both training and inference primarily emphasizes functionality, often at the expense of efficiency—an essential consideration for real-world coding tasks. Perhaps interestingly, we observed that well-trained LLMs inherently possess knowledge about code efficiency, but this potential remains underutilized with standard decoding approaches. To address this, we design strategic prompts to activate the model's embedded efficiency understanding, effectively using LLMs as \\textit{efficiency critiques} to guide code generation toward higher efficiency without sacrificing—and sometimes even improving—functionality, all without the need for costly real code execution. Extensive experiments on benchmark datasets (EffiBench, HumanEval+) across multiple representative code models demonstrate up to a 70.6\\% reduction in average execution time and a 13.6\\% decrease in maximum memory usage, highlighting the computational efficiency and practicality of our approach compared to existing alternatives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WCenI6RU9s": {
    "title": "A Circular Argument: Does RoPE need to be Equivariant for Vision?",
    "volume": "poster",
    "abstract": "Rotary Positional Encodings (RoPE) have emerged as a highly effective technique for one-dimensional sequences in Natural Language Processing spurring recent progress towards generalizing RoPE to higher-dimensional data such as images and videos. The success of RoPE has been thought to be due to its positional equivariance, i.e. its status as a \\textit{relative} positional encoding. In this paper, we mathematically show RoPE to be one of the most general solutions for equivariant positional embedding in one-dimensional data. Moreover, we show Mixed RoPE to be the analogously general solution for $M$-dimensional data, if we require commutative generators -- a property necessary for RoPE's equivariance. However, we question the necessity of equivariance. We propose Spherical RoPE, a method analogous to Mixed RoPE, but with the assumption of anti-commutative generators -- relaxing the equivariant condition. Empirically, we find Spherical RoPE to have the equivalent learning behavior as its equivariant analogues. This strongly suggests that relative positional embeddings are not as important as is commonly believed. We expect this discovery to facilitate future work in positional encodings for vision that are faster and generalize better by removing the preconception that they must be relative",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wGZPNMEHjP": {
    "title": "UnCLe: Towards Scalable Dynamic Causal Discovery in Non-linear Temporal Systems",
    "volume": "poster",
    "abstract": "Uncovering cause-effect relationships from observational time series is fundamental to understanding complex systems. While many methods infer static causal graphs, real-world systems often exhibit *dynamic causality*—where relationships evolve over time. Accurately capturing these temporal dynamics requires time-resolved causal graphs. We propose UnCLe, a novel deep learning method for scalable dynamic causal discovery. UnCLe employs a pair of Uncoupler and Recoupler networks to disentangle input time series into semantic representations and learns inter-variable dependencies via auto-regressive Dependency Matrices. It estimates dynamic causal influences by analyzing datapoint-wise prediction errors induced by temporal perturbations. Extensive experiments demonstrate that UnCLe not only outperforms state-of-the-art baselines on static causal discovery benchmarks but, more importantly, exhibits a unique capability to accurately capture and represent evolving temporal causality in both synthetic and real-world dynamic systems (e.g., human motion). UnCLe offers a promising approach for revealing the underlying, time-varying mechanisms of complex phenomena",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2T2zMiqcY6": {
    "title": "Beyond Least Squares: Uniform Approximation and the Hidden Cost of Misspecification",
    "volume": "poster",
    "abstract": "We study the problem of controlling worst-case errors in misspecified linear regression under the random design setting, where the regression function is estimated via (penalized) least-squares. This setting arises naturally in value function approximation for bandit algorithms and reinforcement learning (RL). Our first main contribution is the observation that the amplification of the misspecification error when using least-squares is governed by the \\emph{Lebesgue constant}, a classical quantity from approximation theory that depends on the choice of the feature subspace and the covariate distribution. We also show that this dependence on the misspecification error is tight for least-squares regression: in general, no method minimizing the empirical squared loss, including regularized least-squares, can improve it substantially. We argue this explains the empirical observation that some feature-maps (e.g., those derived from the Fourier bases) ``work better in RL'' than others (e.g., polynomials): given some covariate distribution, the Lebesgue constant is known to be highly sensitive to choice of the feature-map. As a second contribution, we propose a method that augments the original feature set with auxiliary features designed to reduce the error amplification. We then prove that the method successfully competes with an \"oracle'' that knows the best way of using the auxiliary features to reduce this amplification. For example, when the domain is a real interval and the features are monomials, our method reduces the amplification factor to $O(1)$ as $d\\to\\infty$, while without our method, least-squares with the monomials (and in fact polynomials) will suffer a worst-case error amplification of order $\\Omega(d)$. It follows that there are functions and feature maps for which our method is consistent, while least-squares is inconsistent",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=60I6TzuHOb": {
    "title": "Preference-driven Knowledge Distillation for Few-shot Node Classification",
    "volume": "poster",
    "abstract": "Graph neural networks (GNNs) can efficiently process text-attributed graphs (TAGs) due to their message-passing mechanisms, but their training heavily relies on the human-annotated labels. Moreover, the complex and diverse local topologies of nodes of real-world TAGs make it challenging for a single mechanism to handle. Large language models (LLMs) perform well in zero-/few-shot learning on TAGs but suffer from a scalability challenge. Therefore, we propose a preference-driven knowledge distillation (PKD) framework to synergize the complementary strengths of LLMs and various GNNs for few-shot node classification. Specifically, we develop a GNN-preference-driven node selector that effectively promotes prediction distillation from LLMs to teacher GNNs. To further tackle nodes' intricate local topologies, we develop a node-preference-driven GNN selector that identifies the most suitable teacher GNN for each node, thereby facilitating tailored knowledge distillation from teacher GNNs to the student GNN. Extensive experiments validate the efficacy of our proposed framework in few-shot node classification on real-world TAGs. Our code can be available at <https://github.com/GEEX-Weixing/PKD>",
    "checked": true,
    "id": "ef7b70a90c493fe9e63aff9ee5ccd5f64c32a17f",
    "semantic_title": "preference-driven knowledge distillation for few-shot node classification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pozsP0ZcZN": {
    "title": "AudSemThinker: Enhancing Audio-Language Models Through Reasoning over Semantics of Sound",
    "volume": "poster",
    "abstract": "Audio-language models have shown promising results in various sound understanding tasks, yet they remain limited in their ability to reason over the fine-grained semantics of sound. In this paper, we present AudSemThinker, a model whose reasoning is structured around a framework of auditory semantics inspired by human cognition. To support this, we introduce AudSem, a novel dataset specifically curated for semantic descriptor reasoning in audio-language models. AudSem addresses the persistent challenge of data contamination in zero-shot evaluations by providing a carefully filtered collection of audio samples paired with captions generated through a robust multi-stage pipeline. Our experiments demonstrate that AudSemThinker outperforms state-of-the-art models across multiple training settings, highlighting its strength in semantic audio reasoning. Both AudSemThinker and the AudSem dataset are released publicly",
    "checked": true,
    "id": "5d2771dad48f5f66e6247cca9d4cbd2ae8948149",
    "semantic_title": "audsemthinker: enhancing audio-language models through reasoning over semantics of sound",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=l2Wl77TSYY": {
    "title": "On Evaluating Policies for Robust POMDPs",
    "volume": "poster",
    "abstract": "Robust partially observable Markov decision processes (RPOMDPs) model sequential decision-making problems under partial observability, where an agent must be robust against a range of dynamics. RPOMDPs can be viewed as a two-player game between an agent, who selects actions, and nature, who adversarially selects the dynamics. Evaluating an agent policy requires finding an adversarial nature policy, which is computationally challenging. In this paper, we advance the evaluation of agent policies for RPOMDPs in three ways. First, we discuss suitable benchmarks. We observe that for some RPOMDPs, an optimal agent policy can be found by considering only subsets of nature policies, making them easier to solve. We formalize this concept of solvability and construct three benchmarks that are only solvable for expressive sets of nature policies. Second, we describe a new method to evaluate agent policies for RPOMDPs by solving an equivalent MDP. Third, we lift two well-known upper bounds from POMDPs to RPOMDPs, which can be used to efficiently approximate the optimality gap of a policy and serve as baselines. Our experimental evaluation shows that (1) our proposed benchmarks cannot be solved by assuming naive nature policies, (2) our method of evaluating policies is accurate, and (3) the upper bounds provide solid baselines for evaluation",
    "checked": false,
    "id": "c4059c6004a7ab94b46835e1b86e9b821146a990",
    "semantic_title": "pessimistic iterative planning for robust pomdps",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=oBf5eZSeBT": {
    "title": "Neural Rule Lists: Learning Discretizations, Rules, and Order in One Go",
    "volume": "poster",
    "abstract": "Interpretable machine learning is essential in high-stakes domains like healthcare. Rule lists are a popular choice due to their transparency and accuracy, but learning them effectively remains a challenge. Existing methods require feature pre-discretization, constrain rule complexity or ordering, or struggle to scale. We present NeuRules, a novel end-to-end framework that overcomes these limitations. At its core, NeuRules transforms the inherently combinatorial task of rule list learning into a differentiable optimization problem, enabling gradient-based learning. It simultaneously discovers feature conditions, assembles them into conjunctive rules, and determines their order—without pre-processing or manual constraints. A key contribution here is a gradient shaping technique that steers learning toward sparse rules with strong predictive performance. To produce ordered lists, we introduce a differentiable relaxation that, through simulated annealing, converges to a strict rule list. Extensive experiments show that NeuRules consistently outperforms combinatorial and neural baselines on binary as well as multi-class classification tasks across a wide range of datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qEfgajdKea": {
    "title": "Efficient Preference-Based Reinforcement Learning: Randomized Exploration meets Experimental Design",
    "volume": "poster",
    "abstract": "We study reinforcement learning from human feedback in general Markov decision processes, where agents learn from trajectory-level preference comparisons. A central challenge in this setting is to design algorithms that select informative preference queries to identify the underlying reward while ensuring theoretical guarantees. We propose a meta-algorithm based on randomized exploration, which avoids the computational challenges associated with optimistic approaches and remains tractable. We establish both regret and last-iterate guarantees under mild reinforcement learning oracle assumptions. To improve query complexity, we introduce and analyze an improved algorithm that collects batches of trajectory pairs and applies optimal experimental design to select informative comparison queries. The batch structure also enables parallelization of preference queries, which is relevant in practical deployment as feedback can be gathered concurrently. Empirical evaluation confirms that the proposed method is competitive with reward-based reinforcement learning while requiring a small number of preference queries",
    "checked": true,
    "id": "4c63722f98a282f3e055f33993ecdd46ad7f36aa",
    "semantic_title": "efficient preference-based reinforcement learning: randomized exploration meets experimental design",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=DsGrLE9gqv": {
    "title": "Statistical Analysis of the Sinkhorn Iterations for Two-Sample Schr\\\"{o}dinger Bridge Estimation",
    "volume": "poster",
    "abstract": "The Schrödinger bridge problem seeks the optimal stochastic process that connects two given probability distributions with minimal energy modification. While the Sinkhorn algorithm is widely used to solve the static optimal transport problem, a recent work (Pooladian and Niles-Weed, 2024) proposed the *Sinkhorn bridge*, which estimates Schrödinger bridges by plugging optimal transport into the time-dependent drifts of SDEs, with statistical guarantees in the one-sample estimation setting where the true source distribution is fully accessible. In this work, to further justify this method, we study the statistical performance of intermediate Sinkhorn iterations in the two-sample estimation setting, where only finite samples from both source and target distributions are available. Specifically, we establish a statistical bound on the squared total variation error of Sinkhorn bridge iterations: $\\mathcal{O}(1/m+1/n + r^{2k})~(r \\in (0,1))$, where $m$ and $n$ are the sample sizes from the source and target distributions, respectively, and $k$ is the number of Sinkhorn iterations. This result provides a theoretical guarantee for the finite-sample performance of the Schrödinger bridge estimator and offers practical guidance for selecting sample sizes and the number of Sinkhorn iterations. Notably, our theoretical results apply to several representative methods such as [SF]$^2$M, DSBM-IMF, BM2, and lightSB(-M) under specific settings, through the previously unnoticed connection between these estimators",
    "checked": false,
    "id": "f369df4257a82f4ad9ce634afa5cf782206ba62f",
    "semantic_title": "statistical analysis of the sinkhorn iterations for two-sample schr\\\"odinger bridge estimation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zFGdHL9pcD": {
    "title": "Test-Time Adaptation by Causal Trimming",
    "volume": "poster",
    "abstract": "Test-time adaptation aims to improve model robustness under distribution shifts by adapting models with access to unlabeled target samples. A primary cause of performance degradation under such shifts is the model's reliance on features that lack a direct causal relationship with the prediction target. We introduce Test-time Adaptation by Causal Trimming (TACT), a method that identifies and removes non-causal components from representations for test distributions. TACT applies data augmentations that preserve causal features while varying non-causal ones. By analyzing the changes in the representations using Principal Component Analysis, TACT identifies the highest variance directions associated with non-causal features. It trims the representations by removing their projections on the identified directions, and uses the trimmed representations for the predictions. During adaptation, TACT continuously tracks and refines these directions to get a better estimate of non-causal features. We theoretically analyze the effectiveness of this approach and empirically validate TACT on real-world out-of-distribution benchmarks. TACT consistently outperforms state-of-the-art methods by a significant margin",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SXr3Dynctm": {
    "title": "Learning to Insert for Constructive Neural Vehicle Routing Solver",
    "volume": "poster",
    "abstract": "Neural Combinatorial Optimisation (NCO) is a promising learning-based approach for solving Vehicle Routing Problems (VRPs) without extensive manual design. While existing constructive NCO methods typically follow an appending-based paradigm that sequentially adds unvisited nodes to partial solutions, this rigid approach often leads to suboptimal results. To overcome this limitation, we explore the idea of the insertion-based paradigm and propose Learning to Construct with Insertion-based Paradigm (L2C-Insert), a novel learning-based method for constructive NCO. Unlike traditional approaches, L2C-Insert builds solutions by strategically inserting unvisited nodes at any valid position in the current partial solution, which can significantly enhance the flexibility and solution quality. The proposed framework introduces three key components: a novel model architecture for precise insertion position prediction, an efficient training scheme for model optimization, and an advanced inference technique that fully exploits the insertion paradigm's flexibility. Extensive experiments on both synthetic and real-world instances of the Travelling Salesman Problem (TSP) and Capacitated Vehicle Routing Problem (CVRP) demonstrate that L2C-Insert consistently achieves superior performance across various problem sizes. The code is available at [https://github.com/CIAM-Group/L2C\\_Insert](https://github.com/CIAM-Group/L2C\\_Insert)",
    "checked": true,
    "id": "df9f5b4e75e52c53c6fdb29d8b97ee5b9d1ea79b",
    "semantic_title": "learning to insert for constructive neural vehicle routing solver",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=CmY6DzEG7Z": {
    "title": "NOBLE - Neural Operator with Biologically-informed Latent Embeddings to Capture Experimental Variability in Biological Neuron Models",
    "volume": "poster",
    "abstract": "Characterizing the cellular properties of neurons is fundamental to understanding their function in the brain. In this quest, the generation of bio-realistic models is central towards integrating multimodal cellular data sets and establishing causal relationships. However, current modeling approaches remain constrained by the limited availability and intrinsic variability of experimental neuronal data. The deterministic formalism of bio-realistic models currently precludes accounting for the natural variability observed experimentally. While deep learning is becoming increasingly relevant in this space, it fails to capture the full biophysical complexity of neurons, their nonlinear voltage dynamics, and variability. To address these shortcomings, we introduce $\\texttt{NOBLE}$, a neural operator framework that learns a mapping from a continuous frequency-modulated embedding of interpretable neuron features to the somatic voltage response induced by current injection. Trained on synthetic data generated from bio-realistic neuron models, $\\texttt{NOBLE}$ predicts distributions of neural dynamics accounting for the intrinsic experimental variability. Unlike conventional bio-realistic neuron models, interpolating within the embedding space offers models whose dynamics are consistent with experimentally observed responses. $\\texttt{NOBLE}$ enables the efficient generation of synthetic neurons that closely resemble experimental data and exhibit trial-to-trial variability, offering a $4200$× speedup over the numerical solver. $\\texttt{NOBLE}$ is the first scaled-up deep learning framework that validates its generalization with real experimental data. To this end, $\\texttt{NOBLE}$ captures fundamental neural properties in a unique and emergent manner that opens the door to a better understanding of cellular composition and computations, neuromorphic architectures, large-scale brain circuits, and general neuroAI applications",
    "checked": true,
    "id": "387fd0baf7a7a48739bd948fc543306e46d0e73d",
    "semantic_title": "noble - neural operator with biologically-informed latent embeddings to capture experimental variability in biological neuron models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=bzjlMvUKDv": {
    "title": "CSPCL: Category Semantic Prior Contrastive Learning for Deformable DETR-Based Prohibited Item Detectors",
    "volume": "poster",
    "abstract": "Prohibited item detection based on X-ray images is one of the most effective security inspection methods. However, the foreground-background feature coupling caused by the overlapping phenomenon specific to X-ray images makes general detectors designed for natural images perform poorly. To address this issue, we propose a Category Semantic Prior Contrastive Learning (CSPCL) mechanism, which aligns the class prototypes perceived by the classifier with the content queries to correct and supplement the missing semantic information responsible for classification, thereby enhancing the model sensitivity to foreground features. To achieve this alignment, we design a specific contrastive loss, CSP loss, which comprises the Intra-Class Truncated Attraction (ITA) loss and the Inter-Class Adaptive Repulsion (IAR) loss, and outperforms classic contrastive losses. Specifically, the ITA loss leverages class prototypes to attract intra-class content queries and preserves essential intra-class diversity via a gradient truncation function. The IAR loss employs class prototypes to adaptively repel inter-class content queries, with the repulsion strength scaled by prototype-prototype similarity, thereby improving inter-class discriminability, especially among similar categories. CSPCL is general and can be easily integrated into Deformable DETR-based models. Extensive experiments on the PIXray, OPIXray, PIDray, and CLCXray datasets demonstrate that CSPCL significantly enhances the performance of various state-of-the-art models without increasing inference complexity. The code is publicly available at https://github.com/Limingyuan001/CSPCL",
    "checked": true,
    "id": "f13efd7a8c9ff9bbf56e795c097927032bf212fe",
    "semantic_title": "cspcl: category semantic prior contrastive learning for deformable detr-based prohibited item detectors",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=QvsDTpf4yF": {
    "title": "A Difference-of-Convex Functions Approach to Energy-Based Iterative Reasoning",
    "volume": "poster",
    "abstract": "While energy-based models have recently proven to be a powerful framework for learning to reason with neural networks, their practical application is still limited by computational cost. That is, existing methods for energy-based iterative reasoning suffer from computational bottlenecks by relying on expensive optimization routines during training and especially during inference. Furthermore, these routines may not always converge to minimal energy states, potentially leading to suboptimal reasoning. To address these limitations, we propose a novel and efficient algorithm for energy-based iterative reasoning based on a difference-of-convex (DC) functions approach. Our algorithm achieves a significant speedup compared to prior methods while offering theoretical convergence guarantees ensuring locally minimal energy states. In addition, we achieve state-of-the-art or superior performance on continuous reasoning tasks, as demonstrated by our experiments on multiple benchmark datasets from continuous algorithmic reasoning. As such, our method offers a leap in computational efficiency, enabling faster inference with theoretical guarantees, and hence unlocking the potential of energy-based models for iterative reasoning applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F4LhOqhxkk": {
    "title": "Non-Stationary Structural Causal Bandits",
    "volume": "poster",
    "abstract": "We study the problem of sequential decision-making in environments governed by evolving causal mechanisms. Prior work on structural causal bandits--formulations that integrate causal graphs into multi-armed bandit problems to guide intervention selection--has shown that leveraging the causal structure can reduce unnecessary interventions by identifying possibly-optimal minimal intervention sets (POMISs). However, such formulations fall short in dynamic settings where reward distributions may vary over time, as their static, hence myopic, nature focuses on immediate rewards and overlooks the long-term effects of interventions. In this work, we propose a non-stationary structural causal bandit framework that leverages temporal structural causal models to capture evolving dynamics over time. We characterize how interventions propagate over time by developing graphical tools and assumptions, which form the basis for identifying non-myopic intervention strategies. Within this framework, we devise POMIS$^+$, which captures the existence of variables that contribute to maximizing both immediate and long-term rewards. Our framework provides a principled way to reason about temporally-aware interventions by explicitly modeling information propagation across time. Empirical results validate the effectiveness of our approach, demonstrating improved performance over myopic baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S82Afyfbj3": {
    "title": "TS-MOF: Two-Stage Multi-Objective Fine-tuning for Long-Tailed Recognition",
    "volume": "poster",
    "abstract": "Long-Tailed Recognition (LTR) presents a significant challenge due to extreme class imbalance, where existing methods often struggle to balance performance across head and tail classes. Directly applying multi-objective optimization (MOO) to leverage multiple LTR strategies can be complex and unstable. To address this, we propose TS-MOF (Two-Stage Multi-Objective Fine-tuning), a novel framework that strategically decouples feature learning from classifier adaptation. After standard pre-training, TS-MOF freezes the feature backbone and focuses on an efficient multi-objective fine-tuning of specialized classifier heads. The core of TS-MOF's second stage lies in two innovations: Refined Performance Level Agreement for adaptive task weighting based on real-time per-class performance, and Robust Deterministic Projective Conflict Gradient for stable gradient conflict resolution and constructive fusion. This approach enables effective synergy between diverse LTR strategies, leading to significant and balanced performance improvements. Extensive experiments on CIFAR100-LT, ImageNet-LT, and iNaturalist 2018 demonstrate that TS-MOF achieves state-of-the-art results, particularly enhancing tail class accuracy (e.g., +3.3\\% on CIFAR100-LT IR=100 tail) while improving head class performance, all within a remarkably short fine-tuning period of 20 epochs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KL9yKasAcZ": {
    "title": "Bi-Directional Communication-Efficient Stochastic FL via Remote Source Generation",
    "volume": "poster",
    "abstract": "Federated Learning (FL) incurs high communication costs in both uplink and downlink. The literature largely focuses on lossy compression of model updates in deterministic FL. In contrast, stochastic (Bayesian) FL considers distributions over parameters, enabling uncertainty quantification, better generalization, and, crucially, inherent communication-regularized training through a mirror-descent structure. In this paper, we consider both uplink and downlink communication in stochastic FL, and propose a communication framework based on remote source generation. Employing Minimal Random Coding (MRC) for remote generation, we allow the server and the clients to sample from local and global posteriors (sources), respectively, rather than transmitting locally sampled updates. The framework encompasses communication-regularized local optimization and principled compression of model updates, leveraging gradually updated prior distributions as side information. Through extensive simulations, we show that our method achieves $5-32\\times$ reduction in total communication cost while preserving accuracy. We further analyze the communication cost, refining existing MRC bounds and enabling precise quantification of uplink and downlink trade-offs. We also extend our method to conventional FL via stochastic quantization and prove a contraction property for the biased MRC compressor to facilitate convergence analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jy4bBsr1Jc": {
    "title": "Less is More: Unlocking Specialization of Time Series Foundation Models via Structured Pruning",
    "volume": "poster",
    "abstract": "Scaling laws motivate the development of Time Series Foundation Models (TSFMs) that pre-train vast parameters and achieve remarkable zero-shot forecasting performance. Surprisingly, even after fine-tuning, TSFMs cannot consistently outperform smaller, specialized models trained on full-shot downstream data. A key question is how to realize effective adaptation of TSFMs for a target forecasting task. Through empirical studies on various TSFMs, the pre-trained models often exhibit inherent sparsity and redundancy in computation, suggesting that TSFMs have learned to activate task-relevant network substructures to accommodate diverse forecasting tasks. To preserve this valuable prior knowledge, we propose a structured pruning method to regularize the subsequent fine-tuning process by focusing it on a more relevant and compact parameter space. Extensive experiments on seven TSFMs and six benchmarks demonstrate that fine-tuning a smaller, pruned TSFM significantly improves forecasting performance compared to fine-tuning original models. This ``prune-then-finetune'' paradigm often enables TSFMs to achieve state-of-the-art performance and surpass strong specialized baselines. Source code is made publicly available at \\url{https://github.com/SJTU-DMTai/Prune-then-Finetune}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eK31JidsTN": {
    "title": "OmniTalker: One-shot Real-time Text-Driven Talking Audio-Video Generation With Multimodal Style Mimicking",
    "volume": "poster",
    "abstract": "Although significant progress has been made in audio-driven talking head generation, text-driven methods remain underexplored. In this work, we present OmniTalker, a unified framework that jointly generates synchronized talking audio-video content from input text while emulating the target identity's speaking and facial movement styles, including speech characteristics, head motion, and facial dynamics. Our framework adopts a dual-branch diffusion transformer (DiT) architecture, with one branch dedicated to audio generation and the other to video synthesis. At the shallow layers, cross-modal fusion modules are introduced to integrate information between the two modalities. In deeper layers, each modality is processed independently, with the generated audio decoded by a vocoder and the video rendered using a GAN-based high-quality visual renderer. Leveraging DiT's in-context learning capability through a masked-infilling strategy, our model can simultaneously capture both audio and visual styles without requiring explicit style extraction modules. Thanks to the efficiency of the DiT backbone and the optimized visual renderer, OmniTalker achieves real-time inference at 25 FPS. To the best of our knowledge, OmniTalker is the first one-shot framework capable of jointly modeling speech and facial styles in real time. Extensive experiments demonstrate its superiority over existing methods in terms of generation quality, particularly in preserving style consistency and ensuring precise audio-video synchronization, all while maintaining efficient inference",
    "checked": true,
    "id": "355c82cd7b064219e80a3cc9901403d97c943f36",
    "semantic_title": "omnitalker: one-shot real-time text-driven talking audio-video generation with multimodal style mimicking",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=0M2M2EVreG": {
    "title": "DUET: Dual-Perspective Pseudo Labeling and Uncertainty-aware Exploration & Exploitation Training for Source-Free Domain Adaptation",
    "volume": "poster",
    "abstract": "Source-free domain adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain without requiring labeled source data. In a self supervised setting, relying on pseudo labels on target domain samples facilitates the domain adaptation performance providing strong supervision. However, a critical problem of this approach is the inherent instability of the pre-trained source model in the target domain, leading to unreliable pseudo labels for the target domain data. To tackle this, we propose a novel Dual-perspective pseudo labeling strategy that jointly leverages a task-specific perspective and a domain-invariant perspective, assigning pseudo labels only to target samples on which the target model's predictions and CLIP's predictions agree. To further enhance representation learning without introducing noisy supervision, we apply consistency training to uncertain samples. Additionally, we introduce a Tsallis mutual information(TMI)-based vision optimization strategy guided by an Uncertainty-based adaptation index (UAI), which dynamically modulates entropy sensitivity based on the model's adaptation uncertainty. The UAI-based training paradigm enables stable and adaptive domain alignment by effectively balancing exploration and exploitation processes during the optimization process. Our proposed method achieves state-of-the-art performance on domain adaptation benchmark datasets, improving adaptation accuracy by 1.6% on Office-Home, 1.4% on VisDA-C, and 2.9% on DomainNet-126, demonstrating its effectiveness in SFDA. The code is publicly available at https://github.com/l3umblee/duet-sfda",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Huw15LqglI": {
    "title": "Born a Transformer -- Always a Transformer? On the Effect of Pretraining on Architectural Abilities",
    "volume": "poster",
    "abstract": "Transformers have theoretical limitations in modeling certain sequence-to-sequence tasks, yet it remains largely unclear if these limitations play a role in large-scale pretrained LLMs, or whether LLMs might effectively overcome these constraints in practice due to the scale of both the models themselves and their pretraining data. We explore how these architectural constraints manifest after pretraining by studying a family of *retrieval* and *copying* tasks inspired by Liu et al. [2024a]. We use a recently proposed framework for studying length generalization [Huang et al., 2025] to provide guarantees for each of our settings. Empirically, we observe an *induction-versus-anti-induction asymmetry*, where pretrained models are better at retrieving tokens to the right (induction) rather than the left (anti-induction) of a query token. This asymmetry disappears upon targeted fine-tuning if length-generalization is guaranteed by theory. Mechanistic analysis reveals that this asymmetry is connected to the differences in the strength of induction versus anti-induction circuits within pretrained transformers. We validate our findings through practical experiments on real-world tasks demonstrating reliability risks. Our results highlight that pretraining selectively enhances certain transformer capabilities, but does not overcome fundamental length-generalization limits",
    "checked": true,
    "id": "dc6f419ee59c999c9283c26c4954b0696bb09451",
    "semantic_title": "born a transformer -- always a transformer? on the effect of pretraining on architectural abilities",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=fyp34w19N2": {
    "title": "Dimensionality Mismatch Between Brains and Artificial Neural Networks",
    "volume": "poster",
    "abstract": "Biological and artificial vision systems both rely on hierarchical architectures, yet it remains unclear how their representational geometry evolves across processing stages, and what functional consequences may arise from potential differences. In this work, we systematically quantify and compare the linear and non-linear dimensionality of human brain activity (fMRI) and artificial neural networks (ANNs) during natural image viewing. In the human ventral visual stream, both dimensionality measures increase along the visual hierarchy, supporting the emergence of semantic and abstract representations. For linear dimensionality, most ANNs show a similar increase, but only for pooled features, emphasizing the importance of appropriate feature readouts in brain–model comparisons. In contrast, nonlinear dimensionality shows a collapse in the later layers of ANNs, pointing at a mismatch in representational geometry between the human and artificial visual systems. This mismatch may have functional consequences: while high-dimensional brain representations support flexible generalization to abstract features, ANNs appear to lose this capacity in later layers, where their representations become overly compressed. Overall, our findings propose dimensionality alignment as a benchmark for building more flexible and biologically grounded vision models",
    "checked": false,
    "id": "32124a079b5eb7da3a666e72bc5e7da50d6857b8",
    "semantic_title": "automated interpretation of myocardial spect perfusion images using artificial neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dgQRSJdo6a": {
    "title": "Uniform Wrappers: Bridging Concave to Quadratizable Functions in Online Optimization",
    "volume": "poster",
    "abstract": "This paper presents novel contributions to the field of online optimization, particularly focusing on the adaptation of algorithms from concave optimization to more challenging classes of functions. Key contributions include the introduction of uniform wrappers, a class of meta-algorithms that could be used for algorithmic conversions such as converting algorithms for convex optimization into those for quadratizable optimization. Moreover, we propose a guideline that, given a base algorithm $\\mathcal{A}$ for concave optimization and a uniform wrapper $\\mathcal{W}$, describes how to convert a proof of the regret bound of $\\mathcal{A}$ in the concave setting into a proof of the regret bound of $\\mathcal{W}(\\mathcal{A})$ for quadratizable setting. Through this framework, the paper demonstrates improved regret guarantees for various classes of DR-submodular functions under zeroth-order feedback. Furthermore, the paper extends zeroth-order online algorithms to bandit feedback and offline counterparts, achieving notable improvements in regret/sample complexity compared to existing approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J5XXBS6wPz": {
    "title": "Rethinking Gradient Step Denoiser: Towards Truly Pseudo-Contractive Operator",
    "volume": "poster",
    "abstract": "Learning pseudo-contractive denoisers is a fundamental challenge in the theoretical analysis of Plug-and-Play (PnP) methods and the Regularization by Denoising (RED) framework. While spectral methods attempt to address this challenge using the power iteration method, they fail to guarantee the truly pseudo-contractive property and suffer from high computational complexity. In this work, we rethink gradient step (GS) denoisers and establish a theoretical connection between GS denoisers and pseudo-contractive operators. We show that GS denoisers, with the gradients of convex potential functions parameterized by input convex neural networks (ICNNs), can achieve truly pseudo-contractive properties. Furthermore, we integrate the learned truly pseudo-contractive denoiser into the RED-PRO (RED via fixed-point projection) model, definitely ensuring convergence in terms of both iterative sequences and objective functions. Extensive numerical experiments confirm that the learned GS denoiser satisfies the truly pseudo-contractive property and, when integrated into RED-PRO, provides a favorable trade-off between interpretability and empirical performance on inverse problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xMcKyUGTt1": {
    "title": "Solver-Free Decision-Focused Learning for Linear Optimization Problems",
    "volume": "poster",
    "abstract": "Mathematical optimization is a fundamental tool for decision-making in a wide range of applications. However, in many real-world scenarios, the parameters of the optimization problem are not known a priori and must be predicted from contextual features. This gives rise to predict-then-optimize problems, where a machine learning model predicts problem parameters that are then used to make decisions via optimization. A growing body of work on decision-focused learning (DFL) addresses this setting by training models specifically to produce predictions that maximize downstream decision quality, rather than accuracy. While effective, DFL is computationally expensive, because it requires solving the optimization problem with the predicted parameters at each loss evaluation. In this work, we address this computational bottleneck for linear optimization problems, a common class of problems in both DFL literature and real-world applications. We propose a solver-free training method that exploits the geometric structure of linear optimization to enable efficient training with minimal degradation in solution quality. Our method is based on the insight that a solution is optimal if and only if it achieves an objective value that is at least as good as that of its adjacent vertices on the feasible polytope. Building on this, our method compares the estimated quality of the ground-truth optimal solution with that of its precomputed adjacent vertices, and uses this as loss function. Experiments demonstrate that our method significantly reduces computational cost while maintaining high decision quality",
    "checked": true,
    "id": "765534848d032c603605611c71433930887f58a9",
    "semantic_title": "solver-free decision-focused learning for linear optimization problems",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=D2XdJf1tXW": {
    "title": "Cooperative Retrieval-Augmented Generation for Question Answering: Mutual Information Exchange and Ranking by Contrasting Layers",
    "volume": "poster",
    "abstract": "Since large language models (LLMs) have a tendency to generate factually inaccurate output, retrieval-augmented generation (RAG) has gained significant attention as a key means to mitigate this downside of harnessing only LLMs. However, existing RAG methods for simple and multi-hop question answering (QA) are still prone to incorrect retrievals and hallucinations. To address these limitations, we propose CoopRAG, a novel RAG framework for the question answering task in which a retriever and an LLM work cooperatively with each other by exchanging informative knowledge, and the earlier and later layers of the retriever model work cooperatively with each other to accurately rank the retrieved documents relevant to a given query. In this framework, we (i) unroll a question into sub-questions and a reasoning chain in which uncertain positions are masked, (ii) retrieve the documents relevant to the question augmented with the sub-questions and the reasoning chain, (iii) rerank the documents by contrasting layers of the retriever, and (iv) reconstruct the reasoning chain by filling the masked positions via the LLM. Our experiments demonstrate that CoopRAG consistently outperforms state-of-the-art QA methods on three multi-hop QA datasets as well as a simple QA dataset in terms of both the retrieval and QA performances. Our code is available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ngBOb9wSYN": {
    "title": "Quantifying Uncertainty in Error Consistency: Towards Reliable Behavioral Comparison of Classifiers",
    "volume": "poster",
    "abstract": "Benchmarking models is a key factor for the rapid progress in machine learning (ML) research. Thus, further progress depends on improving benchmarking metrics. A standard metric to measure the behavioral alignment between ML models and human observers is error consistency (EC). EC allows for more fine-grained comparisons of behavior than other metrics such as accuracy, and has been used in the influential Brain-Score benchmark to rank different DNNs by their behavioral consistency with humans. Previously, EC values have been reported without confidence intervals. However, empirically measured EC values are typically noisy - thus, without confidence intervals, valid benchmarking conclusions are problematic. Here we improve on standard EC in two ways: First, we show how to obtain confidence intervals for EC using a bootstrapping technique, allowing us to derive significance tests for EC. Second, we propose a new computational model relating the EC between two classifiers to the implicit probability that one of them copies responses from the other. This view of EC allows us to give practical guidance to scientists regarding the number of trials required for sufficiently powerful, conclusive experiments. Finally, we use our methodology to revisit popular NeuroAI-results. We find that while the general trend of behavioral differences between humans and machines holds up to scrutiny, many reported differences between deep vision models are statistically insignificant. Our methodology enables researchers to design adequately powered experiments that can reliably detect behavioral differences between models, providing a foundation for more rigorous benchmarking of behavioral alignment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uUWb5eawL9": {
    "title": "Noise Injection Reveals Hidden Capabilities of Sandbagging Language Models",
    "volume": "poster",
    "abstract": "Capability evaluations play a crucial role in assessing and regulating frontier AI systems. The effectiveness of these evaluations faces a significant challenge: strategic underperformance, or ``sandbagging'', where models deliberately underperform during evaluation. Sandbagging can manifest either through explicit developer intervention or through unintended model behavior, presenting a fundamental obstacle to accurate capability assessment. We introduce a novel sandbagging detection method based on injecting noise of varying magnitudes into model weights. While non-sandbagging models show predictable performance degradation with increasing noise, we demonstrate that sandbagging models exhibit anomalous performance improvements, likely due to disruption of underperformance mechanisms while core capabilities remain partially intact. Through experiments across various model architectures, sizes, and sandbagging techniques, we establish this distinctive response pattern as a reliable, model-agnostic signal for detecting sandbagging behavior. Importantly, we find noise-injection is capable of eliciting the full performance of Mistral Large 120B in a setting where the model underperforms without being instructed to do so. Our findings provide a practical tool for AI evaluation and oversight, addressing a challenge in ensuring accurate capability assessment of frontier AI systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XsNi2STaj0": {
    "title": "Overcoming Long Context Limitations of State Space Models via Context Dependent Sparse Attention",
    "volume": "poster",
    "abstract": "Efficient long-context modeling remains a critical challenge for natural language processing (NLP), as the time complexity of the predominant Transformer architecture scales quadratically with the sequence length. While state-space models (SSMs) offer alternative sub-quadratic solutions, they struggle to capture long-range dependencies effectively. In this work, we focus on analyzing and improving the long-context modeling capabilities of SSMs. We show that the widely used synthetic task, associative recall, which requires a model to recall a value associated with a single key without context, insufficiently represents the complexities of real-world long-context modeling. To address this limitation, we extend the associative recall to a novel synthetic task, joint recall, which requires a model to recall the value associated with a key given in a specified context. Theoretically, we prove that SSMs do not have the expressiveness to solve multi-query joint recall in sub-quadratic time complexity. To resolve this issue, we propose a solution based on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which has the expressiveness to solve multi-query joint recall with sub-quadratic computation. To bridge the gap between theoretical analysis and real-world applications, we propose locality-sensitive Hashing Attention with sparse Key Selection (HAX), which instantiates the theoretical solution and is further tailored to natural language domains. Extensive experiments on both synthetic and real-world long-context benchmarks show that HAX consistently outperforms SSM baselines and SSMs integrated with context-independent sparse attention (CISA). Our code is available at: https://github.com/DeepGraphLearning/HAX",
    "checked": false,
    "id": "32b8ce7a3ea6ec6880dfc16648b1fb1cd32a34eb",
    "semantic_title": "overcoming long-context limitations of state-space models via context-dependent sparse attention",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=BGwZsFLJFU": {
    "title": "Contextual Tokenization for Graph Inverted Indices",
    "volume": "poster",
    "abstract": "Retrieving graphs from a large corpus, that contain a subgraph isomorphic to a given query graph, is a core operation in many real-world applications. While recent multi-vector graph representations and scores based on set alignment and containment can provide accurate subgraph isomorphism tests, their use in retrieval remains limited by their need to score corpus graphs exhaustively. We introduce CoRGII (COntextual Representation of Graphs for Inverted Indexing), a graph indexing framework in which, starting with a contextual dense graph representation, a differentiable discretization module computes sparse binary codes over a learned latent vocabulary. This text document-like representation allows us to leverage classic, highly optimized inverted indexes, while supporting soft (vector) set containment scores. Improving on this paradigm further, we replace the classical impact score of a `word' on a graph (such as defined by TFIDF or BM25) with a data-driven, trainable impact score. Crucially, CoRGII is trained end-to-end using only binary relevance labels, without fine-grained supervision of query-to-document set alignments. Extensive experiments show that CoRGII provides better trade-offs between efficiency and accuracy, compared to several baselines",
    "checked": true,
    "id": "48c4becf7742cd6d3bb746061061b9b86d224615",
    "semantic_title": "contextual tokenization for graph inverted indices",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pjTbFuv9ET": {
    "title": "Neural Collapse in Cumulative Link Models for Ordinal Regression: An Analysis with Unconstrained Feature Model",
    "volume": "poster",
    "abstract": "A phenomenon known as ``Neural Collapse (NC)'' in deep classification tasks, in which the penultimate-layer features and the final classifiers exhibit an extremely simple geometric structure, has recently attracted considerable attention, with the expectation that it can deepen our understanding of how deep neural networks behave. The Unconstrained Feature Model (UFM) has been proposed to explain NC theoretically, and there emerges a growing body of work that extends NC to tasks other than classification and leverages it for practical applications. In this study, we investigate whether a similar phenomenon arises in deep Ordinal Regression (OR) tasks, via combining the cumulative link model for OR and UFM. We show that a phenomenon we call Ordinal Neural Collapse (ONC) indeed emerges and is characterized by the following three properties: (ONC1) all optimal features in the same class collapse to their within-class mean when regularization is applied; (ONC2) these class means align with the classifier, meaning that they collapse onto a one-dimensional subspace; (ONC3) the optimal latent variables (corresponding to logits or preactivations in classification tasks) are aligned according to the class order, and in particular, in the zero-regularization limit, a highly local and simple geometric relationship emerges between the latent variables and the threshold values. We prove these properties analytically within the UFM framework with fixed threshold values and corroborate them empirically across a variety of datasets. We also discuss how these insights can be leveraged in OR, highlighting the use of fixed thresholds",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GSE3oaiDL2": {
    "title": "DeltaFormer: Unlock the state space of Transformer",
    "volume": "poster",
    "abstract": "In recent years, large language models with Transformer architecture as the core have made breakthrough progress in many fields. At the same time, there are also some weaknesses in the large language model that have prompted people to reflect, among which the most fundamental one is the reflection on the Transformer architecture. The Transformer architecture has high parallelism and can fully utilize the computing power of GPUs, thus replacing models such as LSTM in the past few years. However, high parallelism is not a free lunch, as it fundamentally limits the performance of models. Especially, the problems that logarithmic precision Transformer architecture can solve are strictly limited to the $TC^0$. And there are many important issues that are usually considered out of $TC^0$, such as Python code evaluation, entity tracking, chess, and other state tracking tasks. Meanwhile, some recent state space methods based on Delta Rule have been able to break through the $TC^0$ architecture, but they are limited by fixed size state spaces and perform poorly on many tasks. To this end, we have re-examined the Transformer from the perspective of a state space with kernel functions and propose an improved Transformer called DeltaFormer. We have theoretically and practically demonstrated that the proposed new architecture can break through the limitation of the inherent $TC^0$ expressivity of Transformers and verified that it is not weaker than standard Transformer in language modeling tasks. We hope our work can provide inspiration for designing more expressive models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BPSU46emit": {
    "title": "Periodic Skill Discovery",
    "volume": "poster",
    "abstract": "Unsupervised skill discovery in reinforcement learning (RL) aims to learn diverse behaviors without relying on external rewards. However, current methods often overlook the periodic nature of learned skills, focusing instead on increasing the mutual dependency between states and skills or maximizing the distance traveled in latent space. Considering that many robotic tasks—particularly those involving locomotion—require periodic behaviors across varying timescales, the ability to discover diverse periodic skills is essential. Motivated by this, we propose Periodic Skill Discovery (PSD), a framework that discovers periodic behaviors in an unsupervised manner. The key idea of PSD is to train an encoder that maps states to a circular latent space, thereby naturally encoding periodicity in the latent representation. By capturing temporal distance, PSD can effectively learn skills with diverse periods in complex robotic tasks, even with pixel-based observations. We further show that these learned skills achieve high performance on downstream tasks such as hurdling. Moreover, integrating PSD with an existing skill discovery method offers more diverse behaviors, thus broadening the agent's repertoire. Our code and demos are available at https://jonghaepark.github.io/psd",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XUks1Y96NR": {
    "title": "Reinforcement Learning with Action Chunking",
    "volume": "poster",
    "abstract": "We present Q-chunking, a simple yet effective recipe for improving reinforcement learning (RL) algorithms for long-horizon, sparse-reward tasks. Our recipe is designed for the offline-to-online RL setting, where the goal is to leverage an offline prior dataset to maximize the sample-efficiency of online learning. Effective exploration and sample-efficient learning remain central challenges in this setting, as it is not obvious how the offline data should be utilized to acquire a good exploratory policy. Our key insight is that action chunking, a technique popularized in imitation learning where sequences of future actions are predicted rather than a single action at each timestep, can be applied to temporal difference (TD)-based RL methods to mitigate the exploration challenge. Q-chunking adopts action chunking by directly running RL in a *chunked* action space, enabling the agent to (1) leverage temporally consistent behaviors from offline data for more effective online exploration and (2) use unbiased $n$-step backups for more stable and efficient TD learning. Our experimental results demonstrate that Q-chunking exhibits strong offline performance and online sample efficiency, outperforming prior best offline-to-online methods on a range of long-horizon, sparse-reward manipulation tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CwXyUdqFqW": {
    "title": "MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks",
    "volume": "poster",
    "abstract": "Large multimodal models (LMMs) have shown remarkable progress in audiovisual understanding, yet they struggle with real-world scenarios that require complex reasoning across extensive video collections. Existing benchmarks for video question answering remain limited in scope, typically involving one clip per query, which falls short of representing the challenges of large-scale, audiovisual retrieval and reasoning encountered in practical applications. To bridge this gap, we introduce a novel task named AVHaystacksQA, where the goal is to identify salient segments across different videos in response to a query and link them together to generate the most informative answer. To this end, we present AVHaystacks, an audio-visual benchmark comprising 3100 annotated QA pairs designed to assess the capabilities of LMMs in multi-video retrieval and temporal grounding task. Additionally, we propose a model-agnostic, multi-agent framework MAGNET to address this challenge, achieving up to 89% and 65% relative improvements over baseline methods on BLEU@4 and GPT evaluation scores in QA task on our proposed AVHaystacks. To enable robust evaluation of multi-video retrieval and temporal grounding for optimal response generation, we introduce two new metrics, STEM, which captures alignment errors between a ground truth and a predicted step sequence and MTGS, to facilitate balanced and interpretable evaluation of segment-level grounding performance",
    "checked": true,
    "id": "f8aed7062dd4689d6c489d884c989a86b7ef5bee",
    "semantic_title": "magnet: a multi-agent framework for finding audio-visual needles by reasoning over multi-video haystacks",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=T52hZeT7rn": {
    "title": "Visual Structures Help Visual Reasoning: Addressing the Binding Problem in LVLMs",
    "volume": "poster",
    "abstract": "Despite progress in Large Vision-Language Models (LVLMs), their capacity for visual reasoning is often limited by the binding problem: the failure to reliably associate perceptual features with their correct visual referents. This limitation underlies persistent errors in tasks such as counting, visual search, scene description, and spatial relationship understanding. A key factor is that current LVLMs process visual features largely in parallel, lacking mechanisms for spatially grounded, serial attention. This paper introduces Visual Input Structure for Enhanced Reasoning (VISER), a simple, effective method that augments visual inputs with low-level spatial structures and pairs them with a textual prompt that encourages sequential, spatially-aware parsing. We empirically demonstrate substantial performance improvements across core visual reasoning tasks, using only a single-query inference. Specifically, VISER improves GPT-4o performance on visual search, counting, and spatial relationship tasks by 25.0%, 26.8%, and 9.5%, respectively, and reduces edit distance error in scene description by 0.32 on 2D datasets. Furthermore, we find that the visual modification is essential for these gains; purely textual strategies, including Chain-of-Thought prompting, are insufficient and can even degrade performance. VISER underscores the importance of visual input design over purely linguistically based reasoning strategies and suggests that visual structuring is a powerful and general approach for enhancing compositional and spatial reasoning in LVLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByQdHPGKgU": {
    "title": "Soft Thinking: Unlocking the Reasoning Potential of LLMs in Continuous Concept Space",
    "volume": "poster",
    "abstract": "Human cognition typically involves thinking through abstract, fluid concepts rather than strictly using discrete linguistic tokens. Current Large Language Models (LLMs), however, are constrained to reasoning within the boundaries of human language, processing discrete token embeddings that represent fixed points in semantic space. This discrete constraint restricts the expressive power and upper potential of such reasoning models, often causing incomplete exploration of reasoning paths, as standard Chain-of-Thought (CoT) methods rely on sampling one token per step. In this work, we introduce Soft Thinking, a training-free method that emulates human-like ``soft'' reasoning by generating abstract concept tokens in a continuous concept space. These concept tokens are created by the probability-weighted mixture of token embeddings, which span the continuous concept space, enabling smooth transitions and richer representations that transcend traditional discrete boundaries. In essence, each generated concept token encapsulates multiple meanings from related discrete tokens, implicitly exploring various reasoning paths to converge effectively toward the correct answer. Empirical evaluations on diverse mathematical and coding benchmarks consistently demonstrate the effectiveness and efficiency of Soft Thinking, improving pass@1 accuracy by up to 2.48 points while simultaneously reducing token usage by up to 22.4\\% compared to standard CoT. Qualitative analysis further reveals that Soft Thinking outputs remain highly interpretable and readable, highlighting the potential of Soft Thinking to break the inherent limits of discrete language-based reasoning",
    "checked": true,
    "id": "5a96bfbae6ea33b0f108604436bc33b144f36ad0",
    "semantic_title": "soft thinking: unlocking the reasoning potential of llms in continuous concept space",
    "citation_count": 25,
    "authors": []
  },
  "https://openreview.net/forum?id=OuGAwwAT8G": {
    "title": "ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning",
    "volume": "poster",
    "abstract": "Large Language Models (LLMs) have shown remarkable capabilities in reasoning, exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integrating reasoning with external search processes remains challenging, especially for complex multi-hop questions requiring multiple retrieval steps. We propose ReSearch, a novel framework that trains LLMs to Reason with Search via reinforcement learning without using any supervised data on reasoning steps. Our approach treats search operations as integral components of the reasoning chain, where when and how to perform searches is guided by text-based thinking, and search results subsequently influence further reasoning. We train ReSearch on Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conduct extensive experiments. Despite being trained on only one dataset, our models demonstrate strong generalizability across various benchmarks. Analysis reveals that ReSearch naturally elicits advanced reasoning capabilities such as reflection and self-correction during the reinforcement learning process",
    "checked": true,
    "id": "fb970ce4383a78ad52c641bc38815d78ad737aad",
    "semantic_title": "research: learning to reason with search for llms via reinforcement learning",
    "citation_count": 52,
    "authors": []
  },
  "https://openreview.net/forum?id=Igq7Dyc3OL": {
    "title": "Reasoning Models Hallucinate More: Factuality-Aware Reinforcement Learning for Large Reasoning Models",
    "volume": "poster",
    "abstract": "Large language models (LLMs) have significantly advanced in reasoning tasks through reinforcement learning (RL) optimization, achieving impressive capabilities across various challenging benchmarks. However, our empirical analysis reveals a critical drawback: reasoning-oriented RL fine-tuning significantly increases the prevalence of hallucinations. We theoretically analyze the RL training dynamics, identifying high-variance gradient, entropy-induced randomness, and susceptibility to spurious local optima as key factors leading to hallucinations. To address this drawback, we propose Factuality-aware Step-wise Policy Optimization (FSPO), an innovative RL fine-tuning algorithm incorporating explicit factuality verification at each reasoning step. FSPO leverages automated verification against given evidence to dynamically adjust token-level advantage values, incentivizing factual correctness throughout the reasoning process. Experiments across mathematical reasoning and hallucination benchmarks using Qwen2.5 and Llama models demonstrate that FSPO effectively reduces hallucinations while enhancing reasoning accuracy, substantially improving both reliability and performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MR7Fn23hSE": {
    "title": "Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows",
    "volume": "poster",
    "abstract": "Autoregressive models have driven remarkable progress in language modeling. Their foundational reliance on discrete tokens, unidirectional context, and single-pass decoding, while central to their success, also inspires the exploration of a design space that could offer new axes of modeling flexibility. In this work, we explore an alternative paradigm, shifting language modeling from a discrete token space to a continuous latent space. We propose a novel framework that employs transformer-based autoregressive normalizing flows to model these continuous representations. This approach unlocks substantial flexibility, enabling the construction of models that can capture global bi-directional context through stacked, alternating-direction autoregressive transformations, support block-wise generation with flexible token patch sizes, and facilitate a hierarchical multi-pass generation process. We further propose new mixture-based coupling transformations designed to capture complex dependencies within the latent space shaped by discrete data, and demonstrate theoretical connections to conventional discrete autoregressive models. Extensive experiments on language modeling benchmarks demonstrate strong likelihood performance and highlight the flexible modeling capabilities inherent in our framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cjHQj0tCy6": {
    "title": "BrainOmni: A Brain Foundation Model for Unified EEG and MEG Signals",
    "volume": "poster",
    "abstract": "Electroencephalography (EEG) and magnetoencephalography (MEG) measure neural activity non-invasively by capturing electromagnetic fields generated by dendritic currents. Although rooted in the same biophysics, EEG and MEG exhibit distinct signal patterns, further complicated by variations in sensor configurations across modalities and recording devices. Existing approaches typically rely on separate, modality- and dataset-specific models, which limits the performance and cross-domain scalability. This paper proposes BrainOmni, the first brain foundation model that generalises across heterogeneous EEG and MEG recordings. To unify diverse data sources, we introduce BrainTokenizer, the first tokeniser that quantises spatiotemporal brain activity into discrete representations. Central to BrainTokenizer is a novel Sensor Encoder that encodes sensor properties such as spatial layout, orientation, and type, enabling compatibility across devices and modalities. Building upon the discrete representations, BrainOmni learns unified semantic embeddings of brain signals by self-supervised pretraining. To the best of our knowledge, it is the first foundation model to support both EEG and MEG signals, as well as the first to incorporate large-scale MEG pretraining. A total of 1,997 hours of EEG and 656 hours of MEG data are curated and standardised from publicly available sources for pretraining. Experiments show that BrainOmni outperforms both existing foundation models and state-of-the-art task-specific models on a range of downstream tasks. It also demonstrates strong generalisation to unseen EEG and MEG devices. Further analysis reveals that joint EEG-MEG (EMEG) training yields consistent improvements across both modalities. Code and checkpoints are publicly available at https://github.com/OpenTSLab/BrainOmni",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OmtKcee8NA": {
    "title": "EventMG: Efficient Multilevel Mamba-Graph Learning for Spatiotemporal Event Representation",
    "volume": "poster",
    "abstract": "Event cameras offer unique advantages in scenarios involving high speed, low light, and high dynamic range, yet their asynchronous and sparse nature poses significant challenges to efficient spatiotemporal representation learning. Specifically, despite notable progress in the field, effectively modeling the full spatiotemporal context, selectively attending to salient dynamic regions, and robustly adapting to the variable density and dynamic nature of event data remain key challenges. Motivated by these challenges, this paper proposes EventMG, a lightweight, efficient, multilevel Mamba-Graph architecture designed for learning high-quality spatiotemporal event representations. EventMG employs a multilevel approach, jointly modeling information at the micro (single event) and macro (event cluster) levels to comprehensively capture the multi-scale characteristics of event data. At the micro-level, it focuses on spatiotemporal details, employing State Space Model (SSM) based Mamba, to precisely capture long-range dependencies among numerous event nodes. Concurrently, at the macro-level, Component Graphs are introduced to efficiently encode the local semantics and global topology of dense event regions. Furthermore, to better accommodate the dynamic and sparse characteristics of data, we propose the Spatiotemporal-aware Event Scanning Technology (SEST), integrating the Adaptive Perturbation Network (APN) and Multidirectional Scanning Module (MSM), which substantially enhances the model's ability to perceive and focus on key spatiotemporal patterns. By employing this novel collaborative paradigm, EventMG demonstrates the ability to effectively capture multi-level spatiotemporal characteristics of event data while maintaining a low parameter count and linear computational complexity, suggesting a promising direction for event representation learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hwEhsFLPh1": {
    "title": "Memory-Integrated Reconfigurable Adapters: A Unified Framework for Settings with Multiple Tasks",
    "volume": "poster",
    "abstract": "Organisms constantly pivot between tasks such as evading predators, foraging, traversing rugged terrain, and socializing, often within milliseconds. Remarkably, they preserve knowledge of once-learned environments sans catastrophic forgetting, a phenomenon neuroscientists hypothesize, is due to a singular neural circuitry dynamically overlayed by neuromodulatory agents such as dopamine and acetylcholine. In parallel, deep learning research addresses analogous challenges via domain generalization ($\\textbf{DG}$) and continual learning ($\\textbf{CL}$), yet these methods remain siloed, despite the brain's ability to perform them seamlessly. In particular, prior work has not explored architectures involving associative memories ($\\textbf{AM}$s), which are an integral part of biological systems, to jointly address these tasks. We propose Memory-Integrated Reconfigurable Adapters ($\\textbf{MIRA}$), a unified framework that integrates Hopfield-style associative memory modules atop a shared backbone. These memory modules store adapter-weight updates as values and retrieve them via learned keys. Associative memory keys are learned post-hoc to index and retrieve an affine combination of stored adapter updates for any given task or domain on a per-sample basis. By varying only the task-specific objectives, we demonstrate that $\\textbf{MIRA}$ seamlessly accommodates domain shifts and sequential task exposures under one roof. Empirical evaluations on standard benchmarks confirm that our $\\textbf{AM}$-augmented architecture significantly enhances adaptability and retention: in $\\textbf{DG}$, $\\textbf{MIRA}$ achieves SoTA out-of-distribution accuracy, and in incremental learning settings, it outperforms architectures explicitly designed to handle catastrophic forgetting using generic $\\textbf{CL}$ algorithms. Extensive ablation studies validate the necessity of both associative memory storage and post-hoc key learning for robust interpolated retrieval of adapters. By unifying adapter-based modulation with biologically inspired associative memory, $\\textbf{MIRA}$ delivers rapid task switching and enduring knowledge retention in a single extensible architecture, charting a path toward more versatile and memory-augmented AI systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sH0ZwzDJZn": {
    "title": "UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection",
    "volume": "poster",
    "abstract": "A primary impediment to scaling reinforcement learning (RL) for large language model (LLM) training is the substantial computational cost, predominantly arising from the necessity of multi-sampling for policy optimization and evaluation. This underscores the critical yet challenging nature of efficient training data selection. Drawing inspiration from the Zone of Proximal Development (ZPD) theory, which posits that learners acquire knowledge more effectively from tasks of intermediate difficulty, we hypothesize that LLMs exhibit optimal learning from data they have not yet mastered but demonstrate the potential to comprehend. Conventional methodologies for assessing data difficulty or informativeness typically rely on computationally intensive multi-sampling or iterative procedures. To address this limitation, we introduce UFO-RL (**U**ncertainty-**F**ocused **O**ptimization for **R**einforcement **L**earning), a novel framework that employs a computationally efficient single-pass uncertainty estimation technique to identify informative training instances. This method, requiring only a single forward pass and obviating the need for iterative next-token computation, achieves a significant acceleration (up to 185$\\times$) in data evaluation compared to multi-sampling approaches. UFO-RL leverages this efficient metric to select data within the model's estimated ZPD for training. Extensive experimentation across diverse LLMs and mathematical benchmarks demonstrates that training with a mere 10\\% of the data, carefully selected by UFO-RL, yields performance comparable to or even surpassing that of full-data training. Furthermore, this targeted data selection results in up to a 16$\\times$ reduction in overall training time, concurrently enhancing training stability and improving generalization capabilities. Thus, UFO-RL presents a practical and highly efficient strategy for scaling RL fine-tuning of LLMs by focusing learning efforts on the most informative and valuable data, thereby mitigating the computational bottlenecks associated with traditional RL training",
    "checked": true,
    "id": "da51d8c8d2c8c5d767836335990f086e5df0aac0",
    "semantic_title": "ufo-rl: uncertainty-focused optimization for efficient reinforcement learning data selection",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AJJ5fBB7JY": {
    "title": "Diversity as a Reward: Fine-Tuning LLMs on a Mixture of Domain-Undetermined Data",
    "volume": "poster",
    "abstract": "Fine-tuning large language models (LLMs) using diverse datasets is crucial for enhancing their overall performance across various domains. In practical scenarios, existing methods based on modeling the mixture proportions of data composition often struggle with data whose domain labels are missing, imprecise or non-normalized, while methods based on data selection usually encounter difficulties in balancing multi-domain performance. To address these challenges, in this work, we investigate the role of data diversity in enhancing the overall abilities of LLMs by empirically constructing contrastive data pools and theoretically deriving explanations. Building upon the insights gained, we propose a new method that gives the LLM a dual identity: an output model to cognitively probe and select data based on diversity reward, as well as an input model to be tuned with the selected data. Extensive experiments show that the proposed method notably boosts performance across domain-undetermined data and a series of foundational downstream tasks when applied to various advanced LLMs. We release our code and hope this study can shed light on the understanding of data diversity and advance feedback-driven data-model co-design for LLMs",
    "checked": true,
    "id": "3d467e6c54409a72add156406feeaf6f55333351",
    "semantic_title": "diversity as a reward: fine-tuning llms on a mixture of domain-undetermined data",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=QwY1vk67T3": {
    "title": "RoMA: Scaling up Mamba-based Foundation Models for Remote Sensing",
    "volume": "poster",
    "abstract": "Recent advances in self-supervised learning for Vision Transformers (ViTs) have fueled breakthroughs in remote sensing (RS) foundation models. However, the quadratic complexity of self-attention poses a significant barrier to scalability, particularly for large models and high-resolution images. While the linear-complexity Mamba architecture offers a promising alternative, existing RS applications of Mamba remain limited to supervised tasks on small, domain-specific datasets. To address these challenges, we propose RoMA, a framework that enables scalable self-supervised pretraining of Mamba-based RS foundation models using large-scale, diverse, unlabeled data. RoMA enhances scalability for high-resolution images through a tailored auto-regressive learning strategy, incorporating two key innovations: 1) a rotation-aware pretraining mechanism combining adaptive cropping with angular embeddings to handle sparsely distributed objects with arbitrary orientations, and 2) multi-scale token prediction objectives that address the extreme variations in object scales inherent to RS imagery. Systematic empirical studies validate that Mamba adheres to RS data and parameter scaling laws, with performance scaling reliably as model and data size increase. Furthermore, experiments across scene classification, object detection, and semantic segmentation tasks demonstrate that RoMA-pretrained Mamba models consistently outperform ViT-based counterparts in both accuracy and computational efficiency. The source code and pretrained models have be released at https://github.com/MiliLab/RoMA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dkgx2pS4Ww": {
    "title": "Quantifying Elicitation of Latent Capabilities in Language Models",
    "volume": "poster",
    "abstract": "Large language models often possess latent capabilities that lie dormant unless explicitly elicited, or surfaced, through fine-tuning or prompt engineering. Predicting, assessing, and understanding these latent capabilities pose significant challenges in the development of effective, safe AI systems. In this work, we recast elicitation as an information-constrained fine-tuning problem and empirically characterize upper bounds on the minimal number of parameters needed to achieve specific task performances. We find that training as few as 10–100 randomly chosen parameters—several orders of magnitude fewer than state-of-the-art parameter-efficient methods—can recover up to 50\\% of the performance gap between pretrained-only and full fine-tuned models, and 1,000s to 10,000s of parameters can recover 95\\% of this performance gap. We show that a logistic curve fits the relationship between the number of trained parameters and model performance gap recovery. This scaling generalizes across task formats and domains, as well as model sizes and families, extending to reasoning models and remaining robust to increases in inference compute. To help explain this behavior, we consider a simplified picture of elicitation via fine-tuning where each trainable parameter serves as an encoding mechanism for accessing task-specific knowledge. We observe a relationship between the number of trained parameters and how efficiently relevant model capabilities can be accessed and elicited, offering a potential route to distinguish elicitation from teaching",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HPmDgKOlM3": {
    "title": "Stable Matching with Ties: Approximation Ratios and Learning",
    "volume": "poster",
    "abstract": "We study matching markets with ties, where workers on one side of the market may have tied preferences over jobs, determined by their matching utilities. Unlike classical two-sided markets with strict preferences, no single stable matching exists that is utility-maximizing for all workers. To address this challenge, we introduce the \\emph{Optimal Stable Share} (OSS)-ratio, which measures the ratio of a worker's maximum achievable utility in any stable matching to their utility in a given matching. We prove that distributions over only stable matchings can incur linear utility losses, i.e., an $\\Omega (N)$ OSS-ratio, where $N$ is the number of workers. To overcome this, we design an algorithm that efficiently computes a distribution over (possibly non-stable) matchings, achieving an asymptotically tight $O (\\log N)$ OSS-ratio. When exact utilities are unknown, our second algorithm guarantees workers a logarithmic approximation of their optimal utility under bounded instability. Finally, we extend our offline approximation results to a bandit learning setting where utilities are only observed for matched pairs. In this setting, we consider worker-optimal stable regret, design an adaptive algorithm that smoothly interpolates between markets with strict preferences and those with statistical ties, and establish a lower bound revealing the fundamental trade-off between strict and tied preference regimes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tdl89SZItB": {
    "title": "Accurate KV Cache Eviction via Anchor Direction Projection for Efficient LLM Inference",
    "volume": "poster",
    "abstract": "Key-Value (KV) cache eviction---which retains the KV pairs of the most important tokens while discarding less important ones---is a critical technique for optimizing both memory usage and inference latency in large language models (LLMs). However, existing approaches often rely on simple heuristics---such as attention weights---to measure token importance, overlooking the spatial relationships between token value states in the vector space. This often leads to suboptimal token selections and thus performance degradation. To tackle this problem, we propose a novel method, namely **AnDPro** (**An**chor **D**irection **Pro**jection), which introduces a projection-based scoring function to more accurately measure token importance. Specifically, AnDPro operates in the space of value vectors and leverages the projections of these vectors onto an *``Anchor Direction''*---the direction of the pre-eviction output---to measure token importance and guide more accurate token selection. Experiments on $16$ datasets from the LongBench benchmark demonstrate that AnDPro can maintain $96.07\\\\%$ of the full cache accuracy using only $3.44\\\\%$ KV cache budget, reducing KV cache budget size by $46.0\\\\%$ without compromising quality compared to previous state-of-the-arts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KDKddNgeKo": {
    "title": "Reinforcement learning for one-shot DAG scheduling with comparability identification and dense reward",
    "volume": "poster",
    "abstract": "In recent years, many studies proposed to generate solutions for Directed Acyclic Graph (DAG) scheduling problem in one shot by combining reinforcement learning and list scheduling heuristic. However, these existing methods suffer from biased estimation of sampling probabilities and inefficient guidance in training, due to redundant comparisons among node priorities and the sparse reward challenge. To address these issues, we analyze of the limitations of these existing methods, and propose a novel one-shot DAG scheduling method with comparability identification and dense reward signal, based on the policy gradient framework. In our method, a comparable antichain identification mechanism is proposed to eliminate the problem of redundant nodewise priority comparison. We also propose a dense reward signal for node level decision-making optimization in training, effectively addressing the sparse reward challenge. The experimental results show that the proposed method can yield superior results of scheduling objectives compared to other learning-based DAG scheduling methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3cYcUmcDhU": {
    "title": "Tortoise and Hare Guidance: Accelerating Diffusion Model Inference with Multirate Integration",
    "volume": "poster",
    "abstract": "In this paper, we propose Tortoise and Hare Guidance (THG), a training-free strategy that accelerates diffusion sampling while maintaining high-fidelity generation. We demonstrate that the noise estimate and the additional guidance term exhibit markedly different sensitivity to numerical error by reformulating the classifier-free guidance (CFG) ODE as a multirate system of ODEs. Our error-bound analysis shows that the additional guidance branch is more robust to approximation, revealing substantial redundancy that conventional solvers fail to exploit. Building on this insight, THG significantly reduces the computation of the additional guidance: the noise estimate is integrated with the tortoise equation on the original, fine-grained timestep grid, while the additional guidance is integrated with the hare equation only on a coarse grid. We also introduce (i) an error-bound-aware timestep sampler that adaptively selects step sizes and (ii) a guidance-scale scheduler that stabilizes large extrapolation spans. THG reduces the number of function evaluations (NFE) by up to 30% with virtually no loss in generation fidelity (∆ImageReward ≤ 0.032) and outperforms state-of-the-art CFG-based training-free accelerators under identical computation budgets. Our findings highlight the potential of multirate formulations for diffusion solvers, paving the way for real-time high-quality image synthesis without any model retraining. The source code is available at https://github.com/yhlee-add/THG",
    "checked": true,
    "id": "50acba84b214c1b90beac868389c249eb4b8d328",
    "semantic_title": "tortoise and hare guidance: accelerating diffusion model inference with multirate integration",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3CbwwCpsSk": {
    "title": "Tropical Attention: Neural Algorithmic Reasoning for Combinatorial Algorithms",
    "volume": "poster",
    "abstract": "*Can algebraic geometry enhance the sharpness, robustness, and interpretability of modern neural reasoning models by equipping them with a mathematically grounded inductive bias?* To answer this, we introduce Tropical Attention, an attention mechanism grounded in tropical geometry that lifts the attention kernel into tropical projective space, where reasoning is piecewise-linear and 1-Lipschitz, thus preserving the polyhedral decision structure inherent to combinatorial reasoning. We prove that multi-head Tropical Attention (MHTA) stacks universally approximate tropical circuits and realize tropical transitive closure through composition, achieving polynomial resource bounds without invoking recurrent mechanisms. These guarantees explain why the induced polyhedral decision boundaries remain sharp and scale-invariant, rather than smoothed by Softmax. Empirically, we show that Tropical Attention delivers stronger out-of-distribution generalization in both length and value, with high robustness against perturbative noise, and substantially faster inference with fewer parameters compared to Softmax-based and recurrent attention baselines, respectively. For the first time, we push the domain of neural algorithmic reasoning beyond **PTIME** problems to **NP-hard/complete** problems, paving the way toward sharper and more expressive Large Reasoning Models (LRMs) capable of tackling complex combinatorial challenges in Phylogenetics, Cryptography, Particle Physics, and Mathematical Discovery. The code is available at https://github.com/Baran-phys/Tropical-Attention/",
    "checked": true,
    "id": "7ac1a05ff199ca71cabe79aef4d0bc91b3172535",
    "semantic_title": "tropical attention: neural algorithmic reasoning for combinatorial algorithms",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=NUDaln2vCe": {
    "title": "BREAD: Branched Rollouts from Expert Anchors Bridge SFT & RL for Reasoning",
    "volume": "poster",
    "abstract": "Small language models (SLMs) struggle to learn complex reasoning behaviors, especially when high-quality traces are scarce or difficult to learn from. A typical approach for training such models combines a supervised fine-tuning (SFT) stage, often to distill reasoning capabilities from a larger model, followed by a reinforcement learning (RL) stage such as Group Relative Policy Optimization (GRPO). In this paper, we investigate the fundamental limitations of this SFT + RL paradigm and propose methods to overcome them. Using a toy student-expert model over Markov chains, we demonstrate that the SFT + RL strategy can fail completely when (1) the expert's traces are too difficult for the small model to express, or (2) the small model's initialization achieves exponentially sparse rewards as task complexity grows. To address these, we introduce BREAD, a GRPO variant that bridges SFT and RL via partial expert guidance and branch rollouts. When self-generated traces fail, BREAD adaptively inserts short expert prefixes/hints, allowing the small model to complete the rest of the reasoning path, and ensuring that each update includes at least one successful trace. This mechanism both densifies the reward signal and induces a natural learning curriculum. BREAD requires fewer than 40\\% of ground-truth traces, consistently outperforming standard GRPO while speeding up the training by about 3$\\times$. Importantly, we find that BREAD helps the model solve problems that are otherwise unsolvable by the SFT + RL strategy, highlighting how branch rollouts and expert guidance can aid SLM reasoning",
    "checked": true,
    "id": "c7015e293bd768f023c4e5b3458ffceff5b4a242",
    "semantic_title": "bread: branched rollouts from expert anchors bridge sft & rl for reasoning",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=PBy1Ew1ihV": {
    "title": "Improving Task-Specific Multimodal Sentiment Analysis with General MLLMs via Prompting",
    "volume": "poster",
    "abstract": "Multimodal Sentiment Analysis (MSA) aims to predict sentiment from diverse data types, such as video, audio, and language. Recent progress in Multimodal Large Language Models (MLLMs) have demonstrated impressive performance across various tasks. However, in MSA, the increase in computational costs does not always correspond to a significant improvement in performance, raising concerns about the cost-effectiveness of applying MLLMs to MSA. This paper introduces the MLLM-Guided Multimodal Sentiment Learning Framework (MMSLF). It improves the performance of task-specific MSA models by leveraging the generalized knowledge of MLLMs through a teacher-student framework, rather than directly using MLLMs for sentiment prediction. First, the proposed teacher built upon a powerful MLLM (e.g., GPT-4o-mini), guides the student model to align multimodal representations through MLLM-generated context-aware prompts. Then, knowledge distillation enables the student to mimic the teacher's predictions, thus allowing it to predict sentiment independently without relying on the context-aware prompts. Extensive experiments on the SIMS, MOSI, and MOSEI datasets demonstrate that our framework enables task-specific models to achieve state-of-the-art performance across most metrics. This also provides new insights into the application of general MLLMs for improving MSA",
    "checked": false,
    "id": "6dc3280af7c93b891dd6b3be1278803d47b8d077",
    "semantic_title": "enhancing multimodal sentiment analysis via learning from large language model",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=9osvTOYbT4": {
    "title": "Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models",
    "volume": "poster",
    "abstract": "Enhancing the reasoning capabilities of large language models effectively using reinforcement learning (RL) remains a crucial challenge. Existing approaches primarily adopt two contrasting advantage estimation granularities: token-level methods (e.g., PPO) aim to provide fine-grained advantage signals but suffer from inaccurate estimation due to difficulties in training an accurate critic model. On the other extreme, trajectory-level methods (e.g., GRPO) solely rely on a coarse-grained advantage signal from the final reward, leading to imprecise credit assignment. To address these limitations, we propose Segment Policy Optimization (SPO), a novel RL framework that leverages segment-level advantage estimation at an intermediate granularity, achieving a better balance by offering more precise credit assignment than trajectory-level methods and requiring fewer estimation points than token-level methods, enabling accurate advantage estimation based on Monte Carlo (MC) without a critic model. SPO features three components with novel strategies: (1) flexible segment partition; (2) accurate segment advantage estimation; and (3) policy optimization using segment advantages, including a novel probability-mask strategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain for short chain-of-thought (CoT), featuring novel cutpoint-based partition and chain-based advantage estimation, achieving $6$-$12$ percentage point improvements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT, featuring novel tree-based advantage estimation, which significantly reduces the cost of MC estimation, achieving $7$-$11$ percentage point improvements over GRPO on MATH500 under 2K and 4K context evaluation. We make our code publicly available at https://github.com/AIFrameResearch/SPO",
    "checked": true,
    "id": "ac8cbc7d2c43bceebcb07ec7a1bc04c1edd809cb",
    "semantic_title": "segment policy optimization: effective segment-level credit assignment in rl for large language models",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=4JnZvkVssS": {
    "title": "Connecting Neural Models Latent Geometries with Relative Geodesic Representations",
    "volume": "poster",
    "abstract": "Neural models learn representations of high-dimensional data on low-dimensional manifolds. Multiple factors, including stochasticities in the training process, model architectures, and additional inductive biases, may induce different representations, even when learning the same task on the same data. However, it has recently been shown that when a latent structure is shared between distinct latent spaces, relative distances between representations can be preserved, up to distortions. Building on this idea, we demonstrate that exploiting the differential-geometric structure of latent spaces of neural models, it is possible to capture *precisely* the transformations between representational spaces trained on similar data distributions. Specifically, we assume that distinct neural models parametrize approximately the same underlying manifold, and introduce a representation based on the *pullback metric* that captures the intrinsic structure of the latent space, while scaling efficiently to large models. We validate experimentally our method on model stitching and retrieval tasks, covering autoencoders and vision foundation discriminative models, across diverse architectures, datasets, pretraining schemes and modalities. Code is available at the following [link](https://github.com/marc0git/RelativeGeodesics)",
    "checked": true,
    "id": "dd2e1d576a5fcab2dac38a7f75c1a25dc9c65397",
    "semantic_title": "connecting neural models latent geometries with relative geodesic representations",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=ZTfqehfcEJ": {
    "title": "MRO: Enhancing Reasoning in Diffusion Language Models via Multi-Reward Optimization",
    "volume": "poster",
    "abstract": "Recent advances in diffusion language models (DLMs) have presented a promising alternative to traditional autoregressive large language models (LLMs). However, DLMs still lag behind LLMs in reasoning performance, especially as the number of denoising steps decreases. Our analysis reveals that this shortcoming arises primarily from the independent generation of masked tokens across denoising steps, which fails to capture the token correlation. In this paper, we define two types of token correlation: intra-sequence correlation and inter-sequence correlation, and demonstrate that enhancing these correlations improves reasoning performance. To this end, we propose a Multi-Reward Optimization (MRO) approach, which encourages DLMs to consider the token correlation during the denoising process. More specifically, our MRO approach leverages test-time scaling, reject sampling, and reinforcement learning to directly optimize the token correlation with multiple elaborate rewards. Additionally, we introduce group step and importance sampling strategies to mitigate reward variance and enhance sampling efficiency. Through extensive experiments, we demonstrate that MRO not only improves reasoning performance but also achieves significant sampling speedups while maintaining high performance on reasoning benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UGaAXvav8S": {
    "title": "Improving Regret Approximation for Unsupervised Dynamic Environment Generation",
    "volume": "poster",
    "abstract": "Unsupervised Environment Design (UED) seeks to automatically generate training curricula for reinforcement learning (RL) agents, with the goal of improving generalisation and zero-shot performance. However, designing effective curricula remains a difficult problem, particularly in settings where small subsets of environment parameterisations result in significant increases in the complexity of the required policy. Current methods struggle with a difficult credit assignment problem and rely on regret approximations that fail to identify challenging levels, both of which are compounded as the size of the environment grows. We propose Dynamic Environment Generation for UED (DEGen) to enable a denser level generator reward signal, reducing the difficulty of credit assignment and allowing for UED to scale to larger environment sizes. We also introduce a new regret approximation, Maximised Negative Advantage (MNA), as a significantly improved metric to optimise for, that better identifies more challenging levels. We show empirically that MNA outperforms current regret approximations and when combined with DEGen, consistently outperforms existing methods, especially as the size of the environment grows. We have made all our code available here: \\url{https://github.com/HarryMJMead/Dynamic-Environment-Generation-for-UED}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jRjvcqtdtA": {
    "title": "LaRes: Evolutionary Reinforcement Learning with LLM-based Adaptive Reward Search",
    "volume": "poster",
    "abstract": "The integration of evolutionary algorithms (EAs) with reinforcement learning (RL) has shown superior performance compared to standalone methods. However, previous research focuses on exploration in policy parameter space, while overlooking the reward function search. To bridge this gap, we propose **LaRes**, a novel hybrid framework that achieves efficient policy learning through reward function search. LaRes leverages large language models (LLMs) to generate the reward function population, guiding RL in policy learning. The reward functions are evaluated by the policy performance and improved through LLMs. To improve sample efficiency, LaRes employs a shared experience buffer that collects experiences from all policies, with each experience containing rewards from all reward functions. Upon reward function updates, the rewards of experiences are relabeled, enabling efficient use of historical data. Furthermore, we introduce a Thompson sampling-based selection mechanism that enables more efficient elite interaction. To prevent policy collapse when improving reward functions, we propose the reward scaling and parameter constraint mechanisms to efficiently coordinate reward search with policy learning. Across both initialized and non-initialized settings, LaRes consistently achieves state-of-the-art performance, outperforming strong baselines in both sample efficiency and final performance. The code is available at https://github.com/yeshenpy/LaRes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zuHs6RHQwT": {
    "title": "Uncovering a Universal Abstract Algorithm for Modular Addition in Neural Networks",
    "volume": "poster",
    "abstract": "We propose a testable universality hypothesis, asserting that seemingly disparate neural network solutions observed in the simple task of modular addition actually reflect a common abstract algorithm. While prior work interpreted variations in neuron-level representations as evidence for distinct algorithms, we demonstrate---through multi-level analyses spanning neurons, neuron clusters, and entire networks---that multilayer perceptrons and transformers universally implement the abstract algorithm we call the approximate Chinese Remainder Theorem. Crucially, we introduce approximate cosets and show that neurons activate exclusively on them. Furthermore, our theory works for deep neural networks (DNNs). It predicts that universally learned solutions in DNNs with trainable embeddings or more than one hidden layer require only $\\mathcal{O}(\\log n)$ features, a result we empirically confirm. This work thus provides the first theory‑backed interpretation of \\textit{multilayer} networks solving modular addition. It advances generalizable interpretability and opens a testable universality hypothesis for group multiplication beyond modular addition",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aokiuaM7Lt": {
    "title": "Inference-time Alignment in Continuous Space",
    "volume": "poster",
    "abstract": "Aligning large language models with human feedback at inference time has received increasing attention due to its flexibility. Existing methods rely on generating multiple responses from the base policy for search using a reward model, which can be considered as searching in a discrete response space. However, these methods struggle to explore informative candidates when the base policy is weak or the candidate set is small, resulting in limited effectiveness. In this paper, to address this problem, we propose Simple Energy Adaptation ($\\textbf{SEA}$), a simple yet effective algorithm for inference-time alignment. In contrast to expensive search over the discrete space, SEA directly adapts original responses from the base policy toward the optimal one via gradient-based sampling in continuous latent space. Specifically, SEA formulates inference as an iterative optimization procedure on an energy function over actions in the continuous space defined by the optimal policy, enabling simple and effective alignment. For instance, despite its simplicity, SEA outperforms the second-best baseline with a relative improvement of up to $ \\textbf{77.51\\%}$ on AdvBench and $\\textbf{16.36\\%}$ on MATH. Code is publicly available at [this link](https://github.com/yuanyige/sea)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v31jzDdDts": {
    "title": "Towards Irreversible Attack: Fooling Scene Text Recognition via Multi-Population Coevolution Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nhIJe0dgRs": {
    "title": "An Effective Levelling Paradigm for Unlabeled Scenarios",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "651eda16dcce7ed06fa30588dc1f74a08053cc63",
    "semantic_title": "infer from what you have seen before: temporally-dependent classifier for semi-supervised video segmentation",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=lYOOHqfM46": {
    "title": "Filter Like You Test: Data-Driven Data Filtering for CLIP Pretraining",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "67160ae8e81487ae578a599c630e30c1d88d3bd4",
    "semantic_title": "filter like you test: data-driven data filtering for clip pretraining",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d6lIOnvOX2": {
    "title": "A Gradient Guidance Perspective on Stepwise Preference Optimization for Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cldpn7H3NN": {
    "title": "COLA: Towards Efficient Multi-Objective Reinforcement Learning with Conflict Objective Regularization in Latent Space",
    "volume": "poster",
    "abstract": "Many real-world control problems require continual policy adjustments to balance multiple objectives, which requires the acquisition of high-quality policies to cover diverse preferences. Multi-Objective Reinforcement Learning (MORL) provides a general framework to solve such problems. However, current MORL methods suffer from high sample complexity, primarily due to the neglect of efficient knowledge sharing and conflicts in optimization with different preferences. To this end, this paper introduces a novel framework, Conflict Objective Regularization in Latent Space (**COLA**). To enable efficient knowledge sharing, COLA establishes a shared latent representation space for common knowledge, which can avoid redundant learning under different preferences. Besides, COLA introduces a regularization term for the value function to mitigate the negative effects of conflicting preferences on the value function approximation, thereby improving the accuracy of value estimation. The experimental results across various multi-objective continuous control tasks demonstrate the significant superiority of COLA over the state-of-the-art MORL baselines. Code is available at https://github.com/yeshenpy/COLA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=86IvZmY26S": {
    "title": "CORE: Collaborative Optimization with Reinforcement Learning and Evolutionary Algorithm for Floorplanning",
    "volume": "poster",
    "abstract": "Floorplanning is the initial step in the physical design process of Electronic Design Automation (EDA), directly influencing subsequent placement, routing, and final power of the chip. However, the solution space in floorplanning is vast, and current algorithms often struggle to explore it sufficiently, making them prone to getting trapped in local optima. To achieve efficient floorplanning, we propose **CORE**, a general and effective solution optimization framework that synergizes Evolutionary Algorithms (EAs) and Reinforcement Learning (RL) for high-quality layout search and optimization. Specifically, we propose the Clustering-based Diversified Evolutionary Search that directly perturbs layouts and evolves them based on novelty and performance. Additionally, we model the floorplanning problem as a sequential decision problem with B*-Tree representation and employ RL for efficient learning. To efficiently coordinate EAs and RL, we propose the reinforcement-driven mechanism and evolution-guided mechanism. The former accelerates population evolution through RL, while the latter guides RL learning through EAs. The experimental results on the MCNC and GSRC benchmarks demonstrate that CORE outperforms other strong baselines in terms of wirelength and area utilization metrics, achieving a 12.9\\% improvement in wirelength. CORE represents the first evolutionary reinforcement learning (ERL) algorithm for floorplanning, surpassing existing RL-based methods. The code is available at https://github.com/yeshenpy/CORE",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S9Y89poypx": {
    "title": "Volume Transmission Implements Context Factorization to Target Online Credit Assignment and Enable Compositional Generalization",
    "volume": "poster",
    "abstract": "The modern connectivist framing of neural computation emphasizes the primacy of synaptic communication at the risk of neglecting the influence of the surrounding neuromodulatory environment --- a neuron's 'biophysical context.' Decades of experimental work has established two views of neuromodulatory (NMs) influence: 1) NMs significantly alter circuit dynamics and 2) NMs gate synaptic plasticity, acting as a 'third factor' in learning. Here, we unify these perspectives, proposing that neuromodulation via volume transmission implements a powerful computational principle: context factorization. We derive an endogenously neuromodulated Recurrent Neural Network (e-nmRNN) from a rate reduction of NM release, showing how NM concentrations dynamically factorize network connectivity. This framework reveals how multiplicative NM gating distinctly influences dynamical regimes compared to additive input. Crucially, this context factorization enables targeted online credit assignment: learning rules derived for the e-nmRNN are naturally gated by NM concentrations, localizing updates to relevant contexts. We demonstrate that e-nmRNN dynamics can learn to approximate gradient descent, facilitating rapid in-context adaptation akin to meta-learning. Empirically, e-nmRNNs achieve strong compositional generalization in sequence-to-sequence tasks, outperforming baselines and exhibiting greater hyperparameter robustness. Furthermore, when trained on complex multitasking benchmarks, e-nmRNNs develop emergent properties mirroring biological observations, including modularity, cell-type specialization based on NM release, and distinct neuromodulatory timescales encoding task context. The model's interpretability allows us to reverse engineer these emergent structures. Notably, in reinforcement learning tasks, the e-nmRNN learns to encode context and signals like Reward Prediction Error (RPE) within its neuromodulator dynamics, demonstrating a mechanism for RPE-gated online credit assignment essential for learning how to learn. By bridging biophysical mechanisms with computational principles and empirical validation, our work presents e-nmRNNs as a performant, interpretable model for understanding the computational role of neuromodulation in flexible and compositional learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z7eXOBcxE9": {
    "title": "DiffBreak: Is Diffusion-Based Purification Robust?",
    "volume": "poster",
    "abstract": "Diffusion-based purification (DBP) has become a cornerstone defense against adversarial examples (AEs), regarded as robust due to its use of diffusion models (DMs) that project AEs onto the natural data manifold. We refute this core claim, theoretically proving that gradient-based attacks effectively target the DM rather than the classifier, causing DBP's outputs to align with adversarial distributions. This prompts a reassessment of DBP's robustness, accrediting it two critical factors: inaccurate gradients and improper evaluation protocols that test only a single random purification of the AE. We show that when accounting for stochasticity and resubmission risk, DBP collapses. To support this, we introduce DiffBreak, the first reliable toolkit for differentiation through DBP, eliminating gradient mismatches that previously further inflated robustness estimates. We also analyze the current defense scheme used for DBP where classification relies on a single purification, pinpointing its inherent invalidity. We provide a statistically grounded majority-vote (MV) alternative that aggregates predictions across multiple purified copies, showing partial but meaningful robustness gain. We then propose a novel adaptation of an optimization method against deepfake watermarking, crafting systemic perturbations that defeat DBP even under MV, challenging DBP's viability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FGliQVcrDZ": {
    "title": "Sinusoidal Initialization, Time for a New Start",
    "volume": "poster",
    "abstract": "Initialization plays a critical role in Deep Neural Network training, directly influencing convergence, stability, and generalization. Common approaches such as Glorot and He initializations rely on randomness, which can produce uneven weight distributions across layer connections. In this paper, we introduce the Sinusoidal initialization, a novel deterministic method that employs sinusoidal functions to construct structured weight matrices expressly to improve the spread and balance of weights throughout the network while simultaneously fostering a more uniform, well‑conditioned distribution of neuron activation states from the very first forward pass. Because Sinusoidal initialization begins with weights and activations that are already evenly and efficiently utilized, it delivers consistently faster convergence, greater training stability, and higher final accuracy across a wide range of models, including convolutional neural networks, vision transformers, and large language models. On average, our experiments show an increase of 4.8 % in final validation accuracy and 20.9 % in convergence speed. By replacing randomness with structure, this initialization provides a stronger and more reliable foundation for Deep Learning systems",
    "checked": true,
    "id": "8a84dfb337adc8538a3cb908928aae3095a64673",
    "semantic_title": "sinusoidal initialization, time for a new start",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y38oEwTLTQ": {
    "title": "Metropolis Adjusted Microcanonical Hamiltonian Monte Carlo",
    "volume": "poster",
    "abstract": "Sampling from high dimensional distributions is a computational bottleneck in many scientific applications. Hamiltonian Monte Carlo (HMC), and in particular the No-U-Turn Sampler (NUTS), are widely used, yet they struggle on problems with a very large number of parameters or a complicated geometry. Microcanonical Langevin Monte Carlo (MCLMC) has been recently proposed as an alternative which shows striking gains in efficiency over NUTS, especially for high-dimensional problems. However, it produces biased samples, with a bias that is hard to control in general. We introduce the Metropolis-Adjusted Microcanonical sampler (MAMS), which relies on the same dynamics as MCLMC, but introduces a Metropolis-Hastings step and thus produces asymptotically unbiased samples. We develop an automated tuning scheme for the hyperparameters of the algorithm, making it applicable out of the box. We demonstrate that MAMS outperforms NUTS across the board on benchmark problems of varying complexity and dimensionality, achieving up to a factor of seven speedup",
    "checked": true,
    "id": "323fd6d752ed95551ea59fa01323536284a9a8a3",
    "semantic_title": "metropolis adjusted microcanonical hamiltonian monte carlo",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=1m177EsP3V": {
    "title": "Multi-Agent Reinforcement Learning with Communication-Constrained Priors",
    "volume": "poster",
    "abstract": "Communication is one of the effective means to improve the learning of cooperative policy in multi-agent systems. However, in most real-world scenarios, lossy communication is a prevalent issue. Existing multi-agent reinforcement learning with communication, due to their limited scalability and robustness, struggles to apply to complex and dynamic real-world environments. To address these challenges, we propose a generalized communication-constrained model to uniformly characterize communication conditions across different scenarios. Based on this, we utilize it as a learning prior to distinguish between lossy and lossless messages for specific scenarios. Additionally, we decouple the impact of lossy and lossless messages on distributed decision-making, drawing on a dual mutual information estimatior, and introduce a communication-constrained multi-agent reinforcement learning framework, quantifying the impact of communication messages into the global reward. Finally, we validate the effectiveness of our approach across several communication-constrained benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YaQnKRtTdh": {
    "title": "Towards General Continuous Memory for Vision-Language Models",
    "volume": "poster",
    "abstract": "Language models (LMs) and their extension, vision-language models (VLMs), have achieved remarkable performance across various tasks. However, they still struggle with complex reasoning tasks that require multimodal or multilingual real world knowledge. To support such capabilities, an external memory system that can efficiently provide relevant multimodal information is essential. Existing approaches generally concatenate image and text tokens into a long sequence as memory, which, however, may drastically increase context length and even degrade performance. In contrast, we propose using continuous memory-a compact set of dense embeddings-to more effectively and efficiently represent multimodal and multilingual knowledge. Our key insight is that a VLM can serve as its own continuous memory encoder. We empirically show that this design improves performance on complex multimodal reasoning tasks. Building on this, we introduce a data-efficient and parameter-efficient method to fine-tune the VLM into a memory encoder, requiring only 1.2\\% of the model's parameters and a small corpus of 15.6K self-synthesized samples. Our approach CoMEM utilizes VLM's original capabilities to encode arbitrary multimodal and multilingual knowledge into just 8 continuous embeddings. Since the inference-time VLM remains frozen, our memory module is plug-and-play and can be flexibly integrated as needed. Extensive experiments across eight multimodal reasoning benchmarks demonstrate the effectiveness of our approach. Code and data is publicly released here https://github.com/WenyiWU0111/CoMEM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hDDe38BNsT": {
    "title": "Consistency of the k n -nearest neighbor rule under adaptive sampling",
    "volume": "poster",
    "abstract": "In the adaptive sampling model of online learning, future prediction tasks can be arbitrarily dependent on the past. Every round, an adversary selects an instance to test the learner. After the learner makes a prediction, a noisy label is drawn from an underlying conditional label distribution and is revealed to both learner and adversary. A learner is consistent if it eventually performs no worse than the Bayes predictor. We study the $k_n$-nearest neighbor learner within this setting. In the worst-case, the learner will fail because an adaptive process can generate spurious patterns out of noise. However, under the mild smoothing assumption that the process generating the instances is uniformly absolutely continuous and that choice of $(k_n)_n$ is reasonable, the $k_n$-nearest neighbor rule is online consistent",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aZooxbW63m": {
    "title": "A Semantic Parsing Framework for End-to-End Time Normalization",
    "volume": "poster",
    "abstract": "Time normalization is the task of converting natural language temporal expressions into machine-readable representations. It underpins many downstream applications in information retrieval, question answering, and clinical decision-making. Traditional systems based on the ISO-TimeML schema limit expressivity and struggle with complex constructs such as compositional, event-relative, and multi-span time expressions. In this work, we introduce a novel formulation of time normalization as a code generation task grounded in the SCATE framework, which defines temporal semantics through symbolic and compositional operators. We implement a fully executable SCATE Python library and demonstrate that large language models (LLMs) can generate executable SCATE code. Leveraging this capability, we develop an automatic data augmentation pipeline using LLMs to synthesize large-scale annotated data with code-level validation. Our experiments show that small, locally deployable models trained on this augmented data can achieve strong performance, outperforming even their LLM parents and enabling practical, accurate, and interpretable time normalization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MBDWO29Qq6": {
    "title": "Incentivizing LLMs to Self-Verify Their Answers",
    "volume": "poster",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable progress in complex reasoning tasks through both post-training and test-time scaling laws. While prevalent test-time scaling approaches are often realized by using external reward models to guide the model generation process, we find that only marginal gains can be acquired when scaling a model post-trained on specific reasoning tasks. We identify that the limited improvement stems from distribution discrepancies between the specific post-trained generator and the general reward model. To address this, we propose a framework that incentivizes LLMs to self-verify their own answers. By unifying answer generation and verification within a single reinforcement learning (RL) process, we train models that can effectively assess the correctness of their own solutions. The trained model can further scale its performance at inference time by verifying its generations, without the need for external verifiers. We train our self-verification models based on Qwen2.5-Math-7B and DeepSeek-R1-Distill-Qwen-1.5B, demonstrating their capabilities across varying reasoning context lengths. Experiments on multiple mathematical reasoning benchmarks show that our models can not only improve post-training performance but also enable effective test-time scaling. Our code is available at https://github.com/mansicer/self-verification",
    "checked": true,
    "id": "af6fc4138e4a4a2f7224dd39dd9340c0d441e934",
    "semantic_title": "incentivizing llms to self-verify their answers",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=eUXVvpdFzZ": {
    "title": "HPSERec: A Hierarchical Partitioning and Stepwise Enhancement Framework for Long-tailed Sequential Recommendation",
    "volume": "poster",
    "abstract": "The long-tail problem in sequential recommender systems stems from imbalanced interaction data, resulting in suboptimal model performance for tail users and items. Recent studies have leveraged head data to enhance tail data for diminish the impact of the long-tail problem. However, these methods often adopt ad-hoc strategies to distinguish between head and tail data, which fails to capture the underlying distributional characteristics and structural properties of each category. Moreover, due to a substantial representational gap exists between head and tail data, head-to-tail enhancement strategies are susceptible to negative transfer, often leading to a decline in overall model performance. To address these issues, we propose a hierarchical partitioning and stepwise enhancement framework, called HPSERec, for long-tailed sequential recommendation. HPSERec partitions the item set into subsets based on a data imbalance metric, assigning an expert network to each subset to capture user-specific local features. Subsequently, we apply knowledge distillation to progressively improve long-tail interest representation, followed by a Sinkhorn optimal transport-based feedback module, which aligns user representations across expert levels through a globally optimal and softly matched mapping. Extensive experiments on three real-world datasets demonstrate that HPSERec consistently outperforms all baseline methods. The implementation code is available at https://anonymous.4open.science/r/HPSERec-2404",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nGQLYn13Xf": {
    "title": "FlyLoRA: Boosting Task Decoupling and Parameter Efficiency via Implicit Rank-Wise Mixture-of-Experts",
    "volume": "poster",
    "abstract": "Low-Rank Adaptation (LoRA) is a widely used parameter-efficient fine-tuning method for foundation models, but it suffers from parameter interference, resulting in suboptimal performance. Although Mixture-of-Experts (MoE)-based LoRA variants show promise in mitigating intra-task correlations in single-task instruction tuning, they introduce additional router parameters and remain ineffective in multi-task model merging where inter-task interference arises. Inspired by the fly olfactory circuit, we propose FlyLoRA, an implicit MoE-based LoRA variant that introduces: (1) rank-wise expert activation in the up-projection matrix, and (2) an implicit router that unifies expert routing and down-projection, where a frozen sparse random projection matrix replaces the traditional dense trainable version. This design resolves the trade-off between intra-task decorrelation and computational efficiency by eliminating the need for an explicit router, while inherently mitigating inter-task interference due to the orthogonality property of random matrices. Extensive experiments across four domains---general knowledge understanding, scientific question answering, mathematical reasoning, and code generation---demonstrate consistent performance improvements over existing methods. Beyond empirical gains, FlyLoRA highlights how biological structures can inspire innovations in AI technologies. Code is available at https://github.com/gfyddha/FlyLoRA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m0bR0sxhfL": {
    "title": "CAS-Spec: Cascade Adaptive Self-Speculative Decoding for On-the-Fly Lossless Inference Acceleration of LLMs",
    "volume": "poster",
    "abstract": "Speculative decoding has become a widely adopted as an effective technique for lossless inference acceleration when deploying large language models (LLMs). While on-the-fly self-speculative methods offer seamless integration and broad utility, they often fall short of the speed gains achieved by methods relying on specialized training. Cascading a hierarchy of draft models promises further acceleration and flexibility, but the high cost of training multiple models has limited its practical application. In this paper, we propose a novel Cascade Adaptive Self-Speculative Decoding (CAS-Spec) method which constructs speculative draft models by leveraging dynamically switchable inference acceleration (DSIA) strategies, including layer sparsity and activation quantization. Furthermore, traditional vertical and horizontal cascade algorithms are inefficient when applied to self-speculative decoding methods. We introduce a Dynamic Tree Cascade (DyTC) algorithm that adaptively routes the multi-level draft models and assigns the draft lengths, based on the heuristics of acceptance rates and latency prediction. Our CAS-Spec method achieves state-of-the-art acceleration compared to existing on-the-fly speculative decoding methods, with an average speedup from $1.1\\times$ to $2.3\\times$ over autoregressive decoding across various LLMs and datasets. DyTC improves the average speedup by $47$\\% and $48$\\% over cascade-based baseline and tree-based baseline algorithms, respectively. CAS-Spec can be easily integrated into most existing LLMs and holds promising potential for further acceleration as self-speculative decoding techniques continue to evolve",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u8SXX5ITE6": {
    "title": "Doodle to Detect: A Goofy but Powerful Approach to Skeleton-based Hand Gesture Recognition",
    "volume": "poster",
    "abstract": "Skeleton-based hand gesture recognition plays a crucial role in enabling intuitive human–computer interaction. Traditional methods have primarily relied on hand-crafted features—such as distances between joints or positional changes across frames—to alleviate issues from viewpoint variation or body proportion differences. However, these hand-crafted features often fail to capture the full spatio-temporal information in raw skeleton data, exhibit poor interpretability, and depend heavily on dataset-specific preprocessing, limiting generalization. In addition, normalization strategies in traditional methods, which rely on training data, can introduce domain gaps between training and testing environments, further hindering robustness in diverse real-world settings. To overcome these challenges, we exclude traditional hand-crafted features and propose Skeleton Kinematics Extraction Through Coordinated grapH (SKETCH), a novel framework that directly utilizes raw four-dimensional (time, x, y, and z) skeleton sequences and transforms them into intuitive visual graph representations. The proposed framework incorporates a novel learnable Dynamic Range Embedding (DRE) to preserve axis-wise motion magnitudes lost during normalization and visual graph representations, enabling richer and more discriminative feature learning. This approach produces a graph image that richly captures the raw data's inherent information and provides interpretable visual attention cues. Furthermore, SKETCH applies independent min–max normalization on fixed-length temporal windows in real time, mitigating degradation from absolute coordinate fluctuations caused by varying sensor viewpoints or differences in individual body proportions. Through these designs, our approach becomes inherently topology-agnostic, avoiding fragile dependencies on dataset- or sensor-specific skeleton definitions. By leveraging pre-trained vision backbones, SKETCH achieves efficient convergence and superior recognition accuracy. Experimental results on SHREC'19 and SHREC'22 benchmarks show that it outperforms state-of-the-art methods in both robustness and generalization, establishing a new paradigm for skeleton-based hand gesture recognition. The code is available at https://github.com/capableofanything/SKETCH",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kJqTkj2HhF": {
    "title": "AutoDiscovery: Open-ended Scientific Discovery via Bayesian Surprise",
    "volume": "poster",
    "abstract": "The promise of autonomous scientific discovery (ASD) hinges not only on answering questions, but also on knowing which questions to ask. Most recent works in ASD explore the use of large language models (LLMs) in goal-driven settings, relying on human-specified research questions to guide hypothesis generation. However, scientific discovery may be accelerated further by allowing the AI system to drive exploration by its own criteria. The few existing approaches in open-ended ASD select hypotheses based on diversity heuristics or subjective proxies for human interestingness, but the former struggles to meaningfully navigate the typically vast hypothesis space, and the latter suffers from imprecise definitions. This paper presents AutoDiscovery—a method for open-ended ASD that instead drives scientific exploration using Bayesian surprise. Here, we quantify the epistemic shift from the LLM's prior beliefs about a hypothesis to its posterior beliefs after gathering experimental results. To efficiently explore the space of nested hypotheses, our method employs a Monte Carlo tree search (MCTS) strategy with progressive widening using surprisal as the reward function. We evaluate AutoDiscovery in the setting of data-driven discovery across 21 real-world datasets spanning domains such as biology, economics, finance, and behavioral science. Our results demonstrate that under a fixed budget, AutoDiscovery substantially outperforms competitors by producing 5-29% more discoveries deemed surprising by the LLM. Our human evaluation further reveals that two-thirds of discoveries made by our system are surprising to domain experts as well, suggesting this is an important step towards building open-ended ASD systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kurEZdWU9G": {
    "title": "Table as a Modality for Large Language Models",
    "volume": "poster",
    "abstract": "To migrate the remarkable successes of Large Language Models (LLMs), the community has made numerous efforts to generalize them to the table reasoning tasks for the widely deployed tabular data. Despite that, in this work, by showing a probing experiment on our proposed StructQA benchmark, we postulate that even the most advanced LLMs (such as GPTs) may still fall short of coping with tabular data. More specifically, the current scheme often simply relies on serializing the tabular data, together with the meta information, then inputting them through the LLMs. We argue that the loss of structural information is the root of this shortcoming. In this work, we further propose TAMO, which bears an ideology to treat the tables as an independent modality integrated with the text tokens. The resulting model in TAMO is a multimodal framework consisting of a hypergraph neural network as the global table encoder seamlessly integrated with the mainstream LLM. Empirical results on various benchmarking datasets, including HiTab, WikiTQ, WikiSQL, FeTaQA, and StructQA, have demonstrated significant improvements on generalization with an average relative gain of **42.65%**",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wlqoUpuQrv": {
    "title": "Sequential Attention-based Sampling for Histopathological Analysis",
    "volume": "poster",
    "abstract": "Deep neural networks are increasingly applied in automated histopathology. Yet, whole-slide images (WSIs) are often acquired at gigapixel sizes, rendering them computationally infeasible to analyze entirely at high resolution. Diagnostic labels are largely available only at the slide-level, because expert annotation of images at a finer (patch) level is both laborious and expensive. Moreover, regions with diagnostic information typically occupy only a small fraction of the WSI, making it inefficient to examine the entire slide at full resolution. Here, we propose SASHA -- Sequential Attention-based Sampling for Histopathological Analysis -- a deep reinforcement learning approach for efficient analysis of histopathological images. First, SASHA learns informative features with a lightweight hierarchical, attention-based multiple instance learning (MIL) model. Second, SASHA samples intelligently and zooms selectively into a small fraction (10-20\\%) of high-resolution patches to achieve reliable diagnoses. We show that SASHA matches state-of-the-art methods that analyze the WSI fully at high resolution, albeit at a fraction of their computational and memory costs. In addition, it significantly outperforms competing, sparse sampling methods. We propose SASHA as an intelligent sampling model for medical imaging challenges that involve automated diagnosis with exceptionally large images containing sparsely informative features. Model implementation is available at: https://github.com/coglabiisc/SASHA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=guZBnsKPsw": {
    "title": "DISCOVER: Automated Curricula for Sparse-Reward Reinforcement Learning",
    "volume": "poster",
    "abstract": "Sparse-reward reinforcement learning (RL) can model a wide range of highly complex tasks. Solving sparse-reward tasks is RL's core premise — requiring efficient exploration coupled with long-horizon credit assignment — and overcoming these challenges is key for building self-improving agents with superhuman ability. We argue that solving complex and high-dimensional tasks requires solving simpler tasks that are *relevant* to the target task. In contrast, most prior work designs strategies for selecting exploratory tasks with the objective of solving *any* task, making exploration of challenging high-dimensional, long-horizon tasks intractable. We find that the sense of direction, necessary for effective exploration, can be extracted from existing reinforcement learning algorithms, without needing any prior information. Based on this finding, we propose a method for _**di**rected **s**parse-reward goal-**co**nditioned **ve**ry long-horizon **R**L_ (DISCOVER), which selects exploratory goals in the direction of the target task. We connect DISCOVER to principled exploration in bandits, formally bounding the time until the target task becomes achievable in terms of the agent's initial distance to the target, but independent of the volume of the space of all tasks. Empirically, we perform a thorough evaluation in high-dimensional simulated environments. We find that the directed goal selection of DISCOVER solves exploration problems that are beyond the reach of prior state-of-the-art exploration methods in RL",
    "checked": true,
    "id": "760389d05f01f9acc7258b8ba00797d835f25044",
    "semantic_title": "discover: automated curricula for sparse-reward reinforcement learning",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=igtjRQfght": {
    "title": "EnerVerse: Envisioning Embodied Future Space for Robotics Manipulation",
    "volume": "poster",
    "abstract": "We introduce EnerVerse, a generative robotics foundation model that constructs and interprets embodied spaces. EnerVerse employs a chunk-wise autoregressive video diffusion framework to predict future embodied spaces from instructions, enhanced by a sparse context memory for long-term reasoning. To model the 3D robotics world, we adopt a multi-view video representation, providing rich perspectives to address challenges like motion ambiguity and 3D grounding. Additionally, EnerVerse-D, a data engine pipeline combining generative modeling with 4D Gaussian Splatting, forms a self-reinforcing data loop to reduce the sim-to-real gap. Leveraging these innovations, EnerVerse translates 4D world representations into physical actions via a policy head (EnerVerse-A), achieving state-of-the-art performance in both simulation and real-world tasks. For efficiency, EnerVerse-A reuses features from the first denoising step and predicts action chunks, achieving about 280 ms per 8-step action chunk on a single RTX 4090. Further video demos, dataset samples could be found in our project page",
    "checked": true,
    "id": "700acb7505c578438b13a2ea046010308c17bfb8",
    "semantic_title": "enerverse: envisioning embodied future space for robotics manipulation",
    "citation_count": 26,
    "authors": []
  },
  "https://openreview.net/forum?id=MuhYHqLDZT": {
    "title": "Gradient-Guided Epsilon Constraint Method for Online Continual Learning",
    "volume": "poster",
    "abstract": "Online Continual Learning (OCL) requires models to learn sequentially from data streams with limited memory. Rehearsal-based methods, particularly Experience Replay (ER), are commonly used in OCL scenarios. This paper revisits ER through the lens of $\\epsilon$-constraint optimization, revealing that ER implicitly employs a soft constraint on past task performance, with its weighting parameter post-hoc defining a slack variable. While effective, ER's implicit and fixed slack strategy has limitations: it can inadvertently lead to updates that negatively impact generalization, and its fixed trade-off between plasticity and stability may not optimally balance current streaming with memory retention, potentially overfitting to the memory buffer. To address these shortcomings, we propose the \\textbf{G}radient-Guided \\textbf{E}psilon \\textbf{C}onstraint (\\textbf{GEC}) method for online continual learning. GEC explicitly formulates the OCL update as an $\\epsilon$-constraint optimization problem, which minimize the loss on the current task data and transform the stability objective as constraints and propose a gradient-guided method to dynamically adjusts the update direction based on whether the performance on memory samples violates a predefined slack tolerance $\\bar{\\varepsilon}$: if forgetting exceeds this tolerance, GEC prioritizes constraint satisfaction; otherwise, it focuses on the current task while controlling the rate of increase in memory loss. Empirical evaluations on standard OCL benchmarks demonstrate GEC's ability to achieve a superior trade-off, leading to improved overall performance",
    "checked": false,
    "id": "8a61cc75f65e0ab0c6fa0e92938887ecb5f30a52",
    "semantic_title": "progressive prototype evolving for dual-forgetting mitigation in non-exemplar online continual learning",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=Qvj8s2rRUs": {
    "title": "MemEIC: A Step Toward Continual and Compositional Knowledge Editing",
    "volume": "poster",
    "abstract": "The dynamic nature of information necessitates continuously updating large vision-language models (LVLMs). While recent knowledge editing techniques hint at promising directions, they often focus on editing a single modality (vision or language) in isolation. This prevalent practice neglects the inherent multimodality of LVLMs and the continuous nature of knowledge updates, potentially leading to suboptimal editing outcomes when considering the interplay between modalities and the need for ongoing knowledge refinement. To address these limitations, we propose MemEIC, a novel method for Continual and Compositional Knowledge Editing (CCKE) in LVLMs. MemEIC enables compositional editing of both visual and textual knowledge sequentially. Our approach employs a hybrid external-internal editor featuring a dual external memory for cross-modal evidence retrieval and dual LoRA adapters that facilitate disentangled parameter updates for each modality. A key component is a brain-inspired knowledge connector, activated selectively for compositional reasoning, that integrates information across different modalities. Experiments demonstrate that MemEIC significantly improves performance on complex multimodal questions and effectively preserves prior edits, setting a new benchmark for CCKE in LVLMs. Our project is available at https://github.com/MemEIC/MemEIC",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tPqBnGwTwa": {
    "title": "Far from the Shallow: Brain-Predictive Reasoning Embedding through Residual Disentanglement",
    "volume": "poster",
    "abstract": "Understanding how the human brain progresses from processing simple linguistic inputs to performing high-level reasoning is a fundamental challenge in neuroscience. While modern large language models (LLMs) are increasingly used to model neural responses to language, their internal representations are highly \"entangled,\" mixing information about lexicon, syntax, meaning, and reasoning. This entanglement biases conventional brain encoding analyses toward linguistically shallow features (e.g., lexicon and syntax), making it difficult to isolate the neural substrates of cognitively deeper processes. Here, we introduce a residual disentanglement method that computationally isolates these components. By first probing an LM to identify feature-specific layers, our method iteratively regresses out lower-level representations to produce four nearly orthogonal embeddings for lexicon, syntax, meaning, and, critically, reasoning. We used these disentangled embeddings to model intracranial (ECoG) brain recordings from neurosurgical patients listening to natural speech. We show that: 1) This isolated reasoning embedding exhibits unique predictive power, accounting for variance in neural activity not explained by other linguistic features and even extending to the recruitment of visual regions beyond classical language areas. 2) The neural signature for reasoning is temporally distinct, peaking later (~350-400ms) than signals related to lexicon, syntax, and meaning, consistent with its position atop a processing hierarchy. 3) Standard, non-disentangled LLM embeddings can be misleading, as their predictive success is primarily attributable to linguistically shallow features, masking the more subtle contributions of deeper cognitive processing. Our work provides compelling neural evidence for an abstract reasoning computation during language comprehension and offers a robust framework for mapping distinct cognitive functions from artificial models to the human brain",
    "checked": true,
    "id": "8b4d44d616817668ec4dc512b834248044a0cb08",
    "semantic_title": "far from the shallow: brain-predictive reasoning embedding through residual disentanglement",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=41bIzD5sit": {
    "title": "Dynamics-Aligned Latent Imagination in Contextual World Models for Zero-Shot Generalization",
    "volume": "poster",
    "abstract": "Real-world reinforcement learning demands adaptation to unseen environmental conditions without costly retraining. Contextual Markov Decision Processes (cMDP) model this challenge, but existing methods often require explicit context variables (e.g., friction, gravity), limiting their use when contexts are latent or hard to measure. We introduce Dynamics-Aligned Latent Imagination (DALI), a framework integrated within the Dreamer architecture that infers latent context representations from agent-environment interactions. By training a self-supervised encoder to predict forward dynamics, DALI generates actionable representations conditioning the world model and policy, bridging perception and control. We theoretically prove this encoder is essential for efficient context inference and robust generalization. DALI's latent space enables counterfactual consistency: Perturbing a gravity-encoding dimension alters imagined rollouts in physically plausible ways. On challenging cMDP benchmarks, DALI achieves significant gains over context-unaware baselines, often surpassing context-aware baselines in extrapolation tasks, enabling zero-shot generalization to unseen contextual variations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AAXMcAyNF6": {
    "title": "Towards Understanding Safety Alignment: A Mechanistic Perspective from Safety Neurons",
    "volume": "poster",
    "abstract": "Large language models (LLMs) excel in various capabilities but pose safety risks such as generating harmful content and misinformation, even after safety alignment. In this paper, we explore the inner mechanisms of safety alignment through the lens of mechanistic interpretability, focusing on identifying and analyzing safety neurons within LLMs that are responsible for safety behaviors. We propose inference-time activation contrasting to locate these neurons and dynamic activation patching to evaluate their causal effects on model safety. Experiments on multiple prevalent LLMs demonstrate that we can consistently identify about 5% safety neurons, and by only patching their activations we can restore over 90% of the safety performance across various red-teaming benchmarks without influencing general ability. The finding of safety neurons also helps explain the ''alignment tax'' phenomenon by revealing that the key neurons for model safety and helpfulness significantly overlap, yet they require different activation patterns for the same neurons. Furthermore, we demonstrate an application of our findings in safeguarding LLMs by detecting unsafe outputs before generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TYGDG9zEML": {
    "title": "Linearly Constrained Diffusion Implicit Models",
    "volume": "poster",
    "abstract": "We introduce Linearly Constrained Diffusion Implicit Models (CDIM), a fast and accurate approach to solving noisy linear inverse problems using diffusion models. Traditional diffusion-based inverse methods rely on numerous projection steps to enforce measurement consistency in addition to unconditional denoising steps. CDIM achieves a 10–50× reduction in projection steps by dynamically adjusting the number and size of projection steps to align a residual measurement energy with its theoretical distribution under the forward diffusion process. This adaptive alignment preserves measurement consistency while substantially accelerating constrained inference. For noise-free linear inverse problems, CDIM exactly satisfies the measurement constraints with few projection steps, even when existing methods fail. We demonstrate CDIM's effectiveness across a range of applications, including super-resolution, denoising, inpainting, deblurring, and 3D point cloud reprojection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fxxMReBRhi": {
    "title": "Adaptive Cannistraci-Hebb Network Automata Modelling of Complex Networks for Path-based Link Prediction",
    "volume": "poster",
    "abstract": "Many complex networks have partially observed or evolving connectivity, making link prediction a fundamental task. Topological link prediction infers missing links using only network topology, with applications in social, biological, and technological systems. The Cannistraci-Hebb (CH) theory provides a topological formulation of Hebbian learning, grounded on two pillars: (1) the **minimization of external links** within local communities, and (2) the **path-based definition of local communities** that capture homophilic (similarity-driven) interactions via paths of length 2 and synergetic (diversity-driven) interactions via paths of length 3. Building on this, we introduce the Cannistraci-Hebb Adaptive (CHA) network automata, an adaptive learning machine that automatically selects the optimal CH rule and path length to model each network. CHA unifies theoretical interpretability and data-driven adaptivity, bridging physics-inspired network science and machine intelligence. Across 1,269 networks from 14 domains, CHA consistently surpasses state-of-the-art methods—including SPM, SBM, graph embedding methods, and message-passing graph neural networks—while revealing the mechanistic principles governing link formation. Our code is available at https://github.com/biomedical-cybernetics/Cannistraci_Hebb_network_automata",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BNpIO5iYGc": {
    "title": "SceneForge: Enhancing 3D-text alignment with Structured Scene Compositions",
    "volume": "poster",
    "abstract": "The whole is greater than the sum of its parts, even in 3D-text contrastive learning. We introduce SceneForge, a novel framework that enhances contrastive alignment between 3D point clouds and text through structured multi-object scene compositions. SceneForge leverages individual 3D shapes to construct multi-object scenes with explicit spatial relations, pairing them with coherent multi-object descriptions refined by a large language model. By augmenting contrastive training with these structured, compositional samples, SceneForge effectively addresses the scarcity of large-scale 3D-text datasets, significantly enriching data complexity and diversity. We systematically investigate critical design elements, such as the optimal number of objects per scene, the proportion of compositional samples in training batches, and scene construction strategies. Extensive experiments demonstrate that SceneForge delivers substantial performance gains across multiple tasks, including zero-shot classification on ModelNet, ScanObjNN, Objaverse-LVIS, and ScanNet, as well as few-shot part segmentation on ShapeNetPart. SceneForge's compositional augmentations are model-agnostic, consistently improving performance across multiple encoder architectures. Moreover, SceneForge improves 3D visual question answering on ScanQA, generalizes robustly to retrieval scenarios with increasing scene complexity, and showcases spatial reasoning capabilities by adapting spatial configurations to align precisely with textual instructions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F54u4NkFvS": {
    "title": "Thompson Sampling in Function Spaces via Neural Operators",
    "volume": "poster",
    "abstract": "We propose an extension of Thompson sampling to optimization problems over function spaces where the objective is a known functional of an unknown operator's output. We assume that queries to the operator (such as running a high-fidelity simulator or physical experiment) are costly, while functional evaluations on the operator's output are inexpensive. Our algorithm employs a sample-then-optimize approach using neural operator surrogates. This strategy avoids explicit uncertainty quantification by treating trained neural operators as approximate samples from a Gaussian process (GP) posterior. We derive regret bounds and theoretical results connecting neural operators with GPs in infinite-dimensional settings. Experiments benchmark our method against other Bayesian optimization baselines on functional optimization tasks involving partial differential equations of physical systems, demonstrating better sample efficiency and significant performance gains",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i9RDDi2SZC": {
    "title": "Investigating and Mitigating Catastrophic Forgetting in Medical Knowledge Injection through Internal Knowledge Augmentation Learning",
    "volume": "poster",
    "abstract": "Large Language Models (LLMs) are expected to possess comprehensive medical knowledge to support real-world clinical applications. While domain-specific fine-tuning effectively injects medical knowledge into LLMs, it often causes catastrophic forgetting of previously acquired knowledge and instruction-following capabilities. In this paper, we investigate this issue and reveal a pattern of proximity-dependent forgetting: knowledge that is semantically or topically close to the injected content is more likely to be forgotten, while unrelated knowledge shows minimal degradation. Moreover, we observe that existing mitigation techniques fail to address this type of forgetting effectively. Motivated by this observation and inspired by human learning mechanisms, we proposeInternAL (\\Internal Knowledge Augmentation Learning), a novel approach that leverages LLMs' own internal knowledge to mitigate forgetting. InternAL first probes internal knowledge closely related to the injection by prompting the model with questions derived from the injected knowledge. This knowledge is then used to augment the original injection dataset, guiding the model to retain related prior knowledge during training. Experimental results on multiple LLMs (LLaMA, Qwen) demonstrate that InternAL significantly mitigates proximity-related forgetting while maintaining strong knowledge injection performance. Our findings provide new insights into the nature of catastrophic forgetting in medical knowledge injection and highlight a promising direction for robust domain adaptation in LLMs. Code and datasets are available at https://github.com/THUMLP/InternAL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tQgFLIkvKH": {
    "title": "Estimation of Stochastic Optimal Transport Maps",
    "volume": "poster",
    "abstract": "The optimal transport (OT) map is a geometry-driven transformation between high-dimensional probability distributions which underpins a wide range of tasks in statistics, applied probability, and machine learning. However, existing statistical theory for OT map estimation is quite restricted, hinging on Brenier's theorem (quadratic cost, absolutely continuous source) to guarantee existence and uniqueness of a deterministic OT map, on which various additional regularity assumptions are imposed to obtain quantitative error bounds. In many real‐world problems these conditions fail or cannot be certified, in which case optimal transportation is possible only via stochastic maps that can split mass. To broaden the scope of map estimation theory to such settings, this work introduces a novel metric for evaluating the transportation quality of stochastic maps. Under this metric, we develop computationally efficient map estimators with near-optimal finite-sample risk bounds, subject to easy-to-verify minimal assumptions. Our analysis further accommodates common forms of adversarial sample contamination, yielding estimators with robust estimation guarantees. Empirical experiments are provided which validate our theory and demonstrate the utility of the proposed framework in settings where existing theory fails. These contributions constitute the first general-purpose theory for map estimation, compatible with a wide spectrum of real-world applications where optimal transport may be intrinsically stochastic",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rtp3yLWAjI": {
    "title": "A Latent Multilayer Graphical Model For Complex, Interdependent Systems",
    "volume": "poster",
    "abstract": "Networks have been extensively used and have provided novel insights across a wide variety of research areas. However, many real-world systems are, in fact, a ``network of networks'', or a multilayer network, which interact as components of a larger multimodal system. A major difficulty in this multilayer framework is the estimation of interlayer edges or connections. In this work, we propose a new estimation method, called multilayer sparse + low-rank inverse covariance estimation (multiSLICE), which estimates the interlayer edges. multiSLICE bridges latent variable Gaussian graphical methods with multilayer networks, offering a flexible framework for modeling processes with irregular sampling and heterogeneous graph structures. We develop an effective algorithm to compute the estimator. We also establish theoretical conditions for the recoverability of the joint space, analyze how inter-layer interactions influence joint parameter estimation, and provide theoretical bounds on their relationships. Finally, we rigorously evaluate our method on both simulated and multimodal neuroimaging data, demonstrating improvements over state-of-the-art approaches. Finally, all the relevant R code implementing the method in the article is available on [GitHub](https://github.com/mondrus96/multiSLICE)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0U7D9AFiZ0": {
    "title": "Time-Masked Transformers with Lightweight Test-Time Adaptation for Neural Speech Decoding",
    "volume": "poster",
    "abstract": "Speech neuroprostheses aim to restore communication for people with severe paralysis by decoding speech directly from neural activity. To accelerate algorithmic progress, a recent benchmark released intracranial recordings from a paralyzed participant attempting to speak, along with a baseline decoding algorithm. Prior work on the benchmark showed impressive accuracy gains. However, these gains increased computational costs and were not demonstrated in a real-time decoding setting. Here, we make three contributions that pave the way towards accurate, efficient, and real-time neural speech decoding. First, we incorporate large amounts of time-masking during training. On average, over $50\\%$ of each trial is masked. Second, we replace the gated recurrent unit (GRU) architecture used in the baseline algorithm with a compact Transformer. The Transformer architecture uses $83\\%$ fewer parameters, cuts peak GPU memory usage by $52\\%$ relative, and is significantly faster to calibrate relative to the GRU. Third, we design a lightweight variant of an existing test-time adaptation method developed for decoding handwriting from neural activity. Our variant adapts the model using multiple time-masked augmentations of a single trial and requires only one gradient step per trial. Together, these contributions reduce word error rate by $20\\%$ and effectively mitigate performance degradations across held-out days in a real-time decoding setting while substantially lowering computational costs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DcFoi71Fgf": {
    "title": "Time-Embedded Algorithm Unrolling for Computational MRI",
    "volume": "poster",
    "abstract": "Algorithm unrolling methods have proven powerful for solving the regularized least squares problem in computational magnetic resonance imaging (MRI). These approaches unfold an iterative algorithm with a fixed number of iterations, typically alternating between a neural network-based proximal operator for regularization, a data fidelity operation and auxiliary updates with learnable parameters. While the connection to optimization methods dictate that the proximal operator network should be shared across unrolls, this can introduce artifacts or blurring. Heuristically, practitioners have shown that using distinct networks may be beneficial, but this significantly increases the number of learnable parameters, making it challenging to prevent overfitting. To address these shortcomings, by taking inspirations from proximal operators with varying thresholds in approximate message passing (AMP) and the success of time-embedding in diffusion models, we propose a time-embedded algorithm unrolling scheme for inverse problems. Specifically, we introduce a novel perspective on the iteration-dependent proximal operation in vector AMP (VAMP) and the subsequent Onsager correction in the context of algorithm unrolling, framing them as a time-embedded neural network. Similarly, the scalar weights in the data fidelity operation and its associated Onsager correction are cast as time-dependent learnable parameters. Our extensive experiments on the fastMRI dataset, spanning various acceleration rates and datasets, demonstrate that our method effectively reduces aliasing artifacts and mitigates noise amplification, achieving state-of-the-art performance. Furthermore, we show that our time-embedding strategy extends to existing algorithm unrolling approaches, enhancing reconstruction quality without increasing the computational complexity significantly. Code available at https://github.com/JN-Yun/TE-Unrolling-MRI",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MQVyuWBfSH": {
    "title": "Flow Matching-Based Autonomous Driving Planning with Advanced Interactive Behavior Modeling",
    "volume": "poster",
    "abstract": "Modeling interactive driving behaviors in complex scenarios remains a fundamental challenge for autonomous driving planning. Learning-based approaches attempt to address this challenge with advanced generative models, removing the dependency on over-engineered architectures for representation fusion. However, brute-force implementation by simply stacking transformer blocks lacks a dedicated mechanism for modeling interactive behaviors that is common in real driving scenarios. The scarcity of interactive driving data further exacerbates this problem, leaving conventional imitation learning methods ill-equipped to capture high-value interactive behaviors. We propose Flow Planner, which tackles these problems through coordinated innovations in data modeling, model architecture, and learning scheme. Specifically, we first introduce fine-grained trajectory tokenization, which decomposes the trajectory into overlapping segments to decrease the complexity of whole trajectory modeling. With a sophisticatedly designed architecture, we achieve efficient temporal and spatial fusion of planning and scene information, to better capture interactive behaviors. In addition, the framework incorporates flow matching with classifier-free guidance for multi-modal behavior generation, which dynamically reweights agent interactions during inference to maintain coherent response strategies, providing a critical boost for interactive scenario understanding. Experimental results on the large-scale nuPlan dataset demonstrate that Flow Planner achieves state-of-the-art performance among learning-based approaches while effectively modeling interactive behaviors in complex driving scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4OLbpaTKJe": {
    "title": "Model-Based Policy Adaptation for Closed-Loop End-to-end Autonomous Driving",
    "volume": "poster",
    "abstract": "End-to-end (E2E) autonomous driving models have demonstrated strong performance in open-loop evaluations but often suffer from cascading errors and poor generalization in closed-loop settings. To address this gap, we propose Model-based Policy Adaptation (MPA), a general framework that enhances the robustness and safety of pretrained E2E driving agents during deployment. MPA first generates diverse counterfactual trajectories using a geometry-consistent simulation engine, exposing the agent to scenarios beyond the original dataset. Based on this generated data, MPA trains a diffusion-based policy adapter to refine the base policy's predictions and a multi-step Q value model to evaluate long-term outcomes. At inference time, the adapter proposes multiple trajectory candidates, and the Q value model selects the one with the highest expected utility. Experiments on the nuScenes benchmark using a photorealistic closed-loop simulator demonstrate that MPA significantly improves performance across in-domain, out-of-domain, and safety-critical scenarios. We further investigate how the scale of counterfactual data and inference-time guidance strategies affect overall effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sce9lLS3Gn": {
    "title": "Procurement Auctions with Predictions: Improved Frugality for Facility Location",
    "volume": "poster",
    "abstract": "We study the problem of designing procurement auctions for the strategic uncapacitated facility location problem: a company needs to procure a set of facility locations in order to serve its customers and each facility location is owned by a strategic agent. Each owner has a private cost for providing access to their facility (e.g., renting it or selling it to the company) and needs to be compensated accordingly. The goal is to design truthful auctions that decide which facilities the company should procure and how much to pay the corresponding owners, aiming to minimize the total cost, i.e., the monetary cost paid to the owners and the connection cost suffered by the customers (their distance to the nearest facility). We evaluate the performance of these auctions using the \\emph{frugality ratio}. We first analyze the performance of the classic VCG auction in this context and prove that its frugality ratio is exactly $3$. We then leverage the learning-augmented framework and design auctions that are augmented with predictions regarding the owners' private costs. Specifically, we propose a family of learning-augmented auctions that achieve significant payment reductions when the predictions are accurate, leading to much better frugality ratios. At the same time, we demonstrate that these auctions remain robust even if the predictions are arbitrarily inaccurate, and maintain reasonable frugality ratios even under adversarially chosen predictions. We finally provide a family of ``error-tolerant'' auctions that maintain improved frugality ratios even if the predictions are only approximately accurate, and we provide upper bounds on their frugality ratio as a function of the prediction error",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sT1U2enBh0": {
    "title": "Interactive and Hybrid Imitation Learning: Provably Beating Behavior Cloning",
    "volume": "poster",
    "abstract": "Imitation learning (IL) is a paradigm for learning sequential decision-making policies from experts, leveraging offline demonstrations, interactive annotations, or both. Recent advances show that when annotation cost is tallied per trajectory, Behavior Cloning (BC)—which relies solely on offline demonstrations—cannot be improved in general, leaving limited conditions for interactive methods such as DAgger to help. We revisit this conclusion and prove that when the annotation cost is measured per state, algorithms using interactive annotations can provably outperform BC. Specifically: (1) we show that Stagger, a one‑sample‑per‑round variant of DAgger, provably beats BC under low-recovery-cost settings; (2) we initiate the study of hybrid IL where the agent learns from offline demonstrations and interactive annotations. We propose Warm-Stagger whose learning guarantee is not much worse than using either data source alone. Furthermore, motivated by compounding error and cold‑start problem in imitation learning practice, we give an MDP example in which Warm-Stagger has significant better annotation cost; (3) experiments on MuJoCo continuous‑control tasks confirm that, with modest cost ratio between interactive and offline annotations, interactive and hybrid approaches consistently outperform BC. To the best of our knowledge, our work is the first to highlight the benefit of state‑wise interactive annotation and hybrid feedback in imitation learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=73guIWG7fk": {
    "title": "InvFusion: Bridging Supervised and Zero-shot Diffusion for Inverse Problems",
    "volume": "poster",
    "abstract": "Diffusion Models have demonstrated remarkable capabilities in handling inverse problems, offering high-quality posterior-sampling-based solutions. Despite significant advances, a fundamental trade-off persists regarding the way the conditioned synthesis is employed: Zero-shot approaches can accommodate any linear degradation but rely on approximations that reduce accuracy. In contrast, training-based methods model the posterior correctly, but cannot adapt to the degradation at test-time. Here we introduce InvFusion, the first training-based degradation-aware posterior sampler. InvFusion combines the best of both worlds - the strong performance of supervised approaches and the flexibility of zero-shot methods. This is achieved through a novel architectural design that seamlessly integrates the degradation operator directly into the diffusion denoiser. We compare InvFusion against existing general-purpose posterior samplers, both degradation-aware zero-shot techniques and blind training-based methods. Experiments on the FFHQ and ImageNet datasets demonstrate state-of-the-art performance. Beyond posterior sampling, we further demonstrate the applicability of our architecture, operating as a general Minimum Mean Square Error predictor, and as a Neural Posterior Principal Component estimator",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xQH4lDLIC0": {
    "title": "AutoRedTeamer: Autonomous Red Teaming with Lifelong Attack Integration",
    "volume": "poster",
    "abstract": "As large language models (LLMs) become increasingly capable, security and safety evaluation are crucial. While current red teaming approaches have made strides in assessing LLM vulnerabilities, they often rely heavily on human input and lack comprehensive coverage of emerging attack vectors. This paper introduces AutoRedTeamer, a novel framework for fully automated, end-to-end red teaming against LLMs. AutoRedTeamer combines a multi-agent architecture with a memory-guided attack selection mechanism to enable continuous discovery and integration of new attack vectors. The dual-agent framework consists of a red teaming agent that can operate from high-level risk categories alone to generate and execute test cases, and a strategy proposer agent that autonomously discovers and implements new attacks by analyzing recent research. This modular design allows AutoRedTeamer to adapt to emerging threats while maintaining strong performance on existing attack vectors. We demonstrate AutoRedTeamer's effectiveness across diverse evaluation settings, achieving 20% higher attack success rates on HarmBench against Llama-3.1-70B while reducing computational costs by 46% compared to existing approaches. AutoRedTeamer also matches the diversity of human-curated benchmarks in generating test cases, providing a comprehensive, scalable, and continuously evolving framework for evaluating the security of AI systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kUzBGEuu7w": {
    "title": "Next Semantic Scale Prediction via Hierarchical Diffusion Language Models",
    "volume": "poster",
    "abstract": "In this paper we introduce Hierarchical Diffusion Language Models (HDLM) -- a novel family of discrete diffusion models for language modeling. HDLM builds on a hierarchical vocabulary where low-level tokens with detailed semantics are surjectively mapped to high-level tokens with coarse-grained meanings. In the forward process, each token is independently perturbed to its higher-level ancestor with more abstract semantics according to the scheduler, while in the reverse process the model progressively predicts the next, more detailed semantics. Taken together, HDLM provides a general time-varying next semantic scale prediction process for language modeling. We derive closed-form expressions for the diffusion Evidence Lower Bound (ELBO), and show that HDLM can be implemented in a flexible manner while including the existing MDLM as a special case. We also propose practical training techniques based on the insights. Extensive text generation experiments validate the effectiveness of HDLM, which demonstrates consistently lower validation and generative perplexity than baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=56Y2HRjPIp": {
    "title": "Discretization-free Multicalibration through Loss Minimization over Tree Ensembles",
    "volume": "poster",
    "abstract": "In recent years, multicalibration has emerged as a desirable learning objective for ensuring that a predictor is calibrated across a rich collection of overlapping subpopulations. Existing approaches typically achieve multicalibration by discretizing the predictor's output space and iteratively adjusting its output values. However, this discretization approach departs from the standard empirical risk minimization (ERM) pipeline, introduces rounding error and an additional sensitive hyperparameter, and may distort the predictor's outputs in ways that hinder downstream decision-making. In this work, we propose a discretization-free multicalibration method that directly optimizes an empirical risk objective over an ensemble of depth-two decision trees. Our ERM approach can be implemented using off-the-shelf tree ensemble learning methods such as LightGBM. Our algorithm provably achieves multicalibration, provided that the data distribution satisfies a technical condition we term as loss saturation. Across multiple datasets, our empirical evaluation shows that this condition is always met in practice. Our discretization-free algorithm consistently matches or outperforms existing multicalibration approaches—even when evaluated using a discretization-based multicalibration metric that shares its discretization granularity with the baselines. Code to replicate the results in this work is available at https://github.com/hjenryin/Discretization-free-MC",
    "checked": true,
    "id": "1d29ab84c638270868e511fe7240a2eba29b4e19",
    "semantic_title": "discretization-free multicalibration through loss minimization over tree ensembles",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Zvo2HBlviF": {
    "title": "Offline Actor-Critic for Average Reward MDPs",
    "volume": "poster",
    "abstract": "We study offline policy optimization for infinite-horizon average-reward Markov decision processes (MDPs) with large or infinite state spaces. Specifically, we propose a pessimistic actor-critic algorithm that uses a computationally efficient linear function class for value function estimation. At the core of our method is a critic that computes a pessimistic estimate of the average reward under the current policy, as well as the corresponding policy gradient, by solving a fixed-point Bellman equation, rather than solving a successive sequence of regression problems as in finite horizon settings. This procedure reduces to solving a second-order cone program, which is computationally tractable. Our theoretical analysis is based on a weak partial data coverage assumption, which requires only that the offline data aligns well with the expected feature vector of a comparator policy. Under this condition, we show that our algorithm achieves the optimal sample complexity of O(\\varepsilon^{-2}) for learning a near-optimal policy, up to model misspecification errors",
    "checked": false,
    "id": "e2711249c7f74199cbd72d42b7d0d0daf6a160cd",
    "semantic_title": "efficient q-learning and actor-critic methods for robust average reward reinforcement learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=NCzNOKjKnD": {
    "title": "Consensus-Robust Transfer Attacks via Parameter and Representation Perturbations",
    "volume": "poster",
    "abstract": "Adversarial examples crafted on one model often exhibit poor transferability to others, hindering their effectiveness in black-box settings. This limitation arises from two key factors: (i) \\emph{decision-boundary variation} across models and (ii) \\emph{representation drift} in feature space. We address these challenges through a new perspective that frames transferability for \\emph{untargeted attacks} as a \\emph{consensus-robust optimization} problem: adversarial perturbations should remain effective across a neighborhood of plausible target models. To model this uncertainty, we introduce two complementary perturbation channels: a \\emph{parameter channel}, capturing boundary shifts via weight perturbations, and a \\emph{representation channel}, addressing feature drift via stochastic blending of clean and adversarial activations. We then propose \\emph{CORTA} (COnsensus--Robust Transfer Attack), a lightweight attack instantiated from this robust formulation using two first-order strategies: (i) sensitivity regularization based on the squared Frobenius norm of logits' Jacobian with respect to weights, and (ii) Monte Carlo sampling for blended feature representations. Our theoretical analysis provides a certified lower bound linking these approximations to the robust objective. Extensive experiments on CIFAR-100 and ImageNet show that CORTA significantly outperforms state-of-the-art transfer-based methods---including ensemble approaches---across CNN and Vision Transformer targets. Notably, CORTA achieves a \\emph{19.1 percentage-point gain in transfer success rate over the best prior method} while using only a single surrogate model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FooiwsnEH9": {
    "title": "Fully Spiking Neural Networks for Unified Frame-Event Object Tracking",
    "volume": "poster",
    "abstract": "The integration of image and event streams offers a promising approach for achieving robust visual object tracking in complex environments. However, current fusion methods achieve high performance at the cost of significant computational overhead and struggle to efficiently extract the sparse, asynchronous information from event streams, failing to leverage the energy-efficient advantages of event-driven spiking paradigms. To address this challenge, we propose the first fully Spiking Frame-Event Tracking framework called SpikeFET. This network achieves synergistic integration of convolutional local feature extraction and Transformer-based global modeling within the spiking paradigm, effectively fusing frame and event data. To overcome the degradation of translation invariance caused by convolutional padding, we introduce a Random Patchwork Module (RPM) that eliminates positional bias through randomized spatial reorganization and learnable type encoding while preserving residual structures. Furthermore, we propose a Spatial-Temporal Regularization (STR) strategy that overcomes similarity metric degradation from asymmetric features by enforcing spatio-temporal consistency among temporal template features in latent space. Extensive experiments across multiple benchmarks demonstrate that the proposed framework achieves superior tracking accuracy over existing methods while significantly reducing power consumption, attaining an optimal balance between performance and efficiency",
    "checked": true,
    "id": "b795d27606f99de1ad83c90fc73f5bb3157b8c52",
    "semantic_title": "fully spiking neural networks for unified frame-event object tracking",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X5B2yTT97A": {
    "title": "SimpleStrat: Diversifying Language Model Generation with Stratification",
    "volume": "poster",
    "abstract": "Generating diverse responses from large language models (LLMs) is crucial for applications such as adversarial testing, search, and synthetic data generation, where diversity provides distinct answers across generations. Previous approaches rely solely on increasing the temperature, sacrificing quality. Furthermore, the model's next-token probabilities may not be representative of the true answer distribution. To combat these challenges, we propose SimpleStrat, an alternative that uses the language model itself to partition the solution space into strata from which to sample. To measure resampling diversity, we introduce CoverageQA, a dataset of underspecified questions with multiple equally plausible answers. We propose measuring resampling diversity as the KL Divergence between the output distribution and the uniform distribution over valid ground truth answers and use recall as an alternative when assessing proprietary models. On CoverageQA, SimpleStrat improves diversity across all temperatures, showing orthogonal benefits. Quantifiably, we achieve as much as 2X better recall when applied to GPT-4o, and an average reduction in KL divergence by 0.36 when applied to Llama 3. Furthermore, we show that SimpleStrat achieves more resampling diversity at temperature T=0 than scaling temperature to T=1 on creative writing, an open-ended domain. Implementation and dataset available at https://github.com/jwong8314/simplestrat",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sYk6ZMmrOz": {
    "title": "Let's Revise Step-by-Step: A Unified Local Search Framework for Code Generation with LLMs",
    "volume": "poster",
    "abstract": "Large Language Models (LLMs) with inference-time scaling techniques show promise for code generation, yet face notable efficiency and scalability challenges. Construction-based tree-search methods suffer from rapid growth in tree size, high token consumption, and lack of anytime property. In contrast, improvement-based methods offer better performance but often struggle with uninformative reward signals and inefficient search strategies. In this work, we propose $\\textbf{ReLoc}$, a unified local search framework which effectively performs step-by-step code revision. Specifically, ReLoc explores a series of local revisions through four key algorithmic components: initial code drafting, neighborhood code generation, candidate evaluation, and incumbent code updating, each of which can be instantiated with specific decision rules to realize different local search algorithms such as Hill Climbing (HC) or Genetic Algorithm (GA). Furthermore, we develop a specialized revision reward model that evaluates code quality based on revision distance to produce fine-grained preferences that guide the local search toward more promising candidates. Finally, our extensive experimental results demonstrate that our approach achieves superior performance across diverse code generation tasks, significantly outperforming both construction-based tree search as well as the state-of-the-art improvement-based code generation methods",
    "checked": true,
    "id": "f96489f6b8db9c6a6a8575bcf593a343520174cd",
    "semantic_title": "let's revise step-by-step: a unified local search framework for code generation with llms",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=cf4etwjY7n": {
    "title": "Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints",
    "volume": "poster",
    "abstract": "Deep generative models have recently been applied to physical systems governed by partial differential equations (PDEs), offering scalable simulation and uncertainty-aware inference. However, enforcing physical constraints, such as conservation laws (linear and nonlinear) and physical consistencies, remains challenging. Existing methods often rely on soft penalties or architectural biases that fail to guarantee hard constraints. In this work, we propose Physics-Constrained Flow Matching (PCFM), a zero-shot inference framework that enforces arbitrary nonlinear constraints in pretrained flow-based generative models. PCFM continuously guides the sampling process through physics-based corrections applied to intermediate solution states, while remaining aligned with the learned flow and satisfying physical constraints. Empirically, PCFM outperforms both unconstrained and constrained baselines on a range of PDEs, including those with shocks, discontinuities, and sharp features, while ensuring exact constraint satisfaction at the final solution. Our method provides a flexible framework for enforcing hard constraints in both scientific and general-purpose generative models, especially in applications where constraint satisfaction is essential",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1SCMFCGliM": {
    "title": "The First Few Tokens Are All You Need: An Efficient and Effective Unsupervised Prefix Fine-Tuning Method for Reasoning Models",
    "volume": "poster",
    "abstract": "Improving the reasoning capabilities of large language models (LLMs) typically requires supervised fine-tuning with labeled data or computationally expensive sampling. We introduce Unsupervised Prefix Fine-Tuning (UPFT), which leverages the observation of Prefix Self-Consistency -- the shared initial reasoning steps across diverse solution trajectories -- to enhance LLM reasoning efficiency. By training exclusively on the initial prefix substrings (as few as 8 tokens), UPFT removes the need for labeled data or exhaustive sampling. Experiments on reasoning benchmarks show that UPFT matches the performance of supervised methods such as Rejection Sampling Fine-Tuning, while reducing training time by 75\\% and sampling cost by 99\\%. Further analysis reveals that errors tend to appear in later stages of the reasoning process and that prefix-based training preserves the model's structural knowledge. This work demonstrates how minimal unsupervised fine-tuning can unlock substantial reasoning gains in LLMs, offering a scalable and resource-efficient alternative to conventional approaches",
    "checked": true,
    "id": "485ac73320040449bdd224f2d89732e69c585c2d",
    "semantic_title": "the first few tokens are all you need: an efficient and effective unsupervised prefix fine-tuning method for reasoning models",
    "citation_count": 24,
    "authors": []
  },
  "https://openreview.net/forum?id=IBhxvINfxv": {
    "title": "Adaptive Variance Inflation in Thompson Sampling: Efficiency, Safety, Robustness, and Beyond",
    "volume": "poster",
    "abstract": "Thompson Sampling (TS) has emerged as a powerful algorithm for sequential decision-making, with strong empirical success and theoretical guarantees. However, it has been shown that its behavior under stringent safety and robustness criteria --- such as safety of cumulative regret distribution and robustness to model mis-specification --- can sometimes perform poorly. In this work, we try to address these aspects through the lens of adaptive variance inflation for Gaussian Thompson Sampling. Our one-line change introduces a time- and arm-dependent inflation factor into the sampling variance, and yields several compelling benefits. The resulting policy achieves provably worst-case optimal expected regret and worst-case optimal fast-decaying regret tail bounds, even in the presence of heavy-tailed (sub-exponential) noise or mis-specified environments. The policy is also robust to mis-specified noise variances. Beyond cumulative regret, we further demonstrate that our method ensures strong post-experiment guarantees: simple regret and estimation error per arm exhibit fast-decaying tail probabilities, contributing to more reliable and robust downstream decisions. Finally, we extend our policy to incorporate settings with unknown arm-specific variances and empirically validate the consistent performance of our approach across a range of environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JnNJq81qDl": {
    "title": "Quantum Visual Fields with Neural Amplitude Encoding",
    "volume": "poster",
    "abstract": "Quantum Implicit Neural Representations (QINRs) have emerged as a promising paradigm that leverages parametrised quantum circuits to encode and process classical information. However, significant challenges remain in areas such as ansatz architecture design, the effective utility of quantum-mechanical properties, training efficiency, and the integration with classical modules. This paper advances the field by introducing a novel QINR architecture for 2D image and 3D geometric field learning, which we collectively refer to as Quantum Visual Field (QVF). QVF encodes classical data into quantum statevectors using neural amplitude encoding grounded in a learnable energy manifold, ensuring meaningful Hilbert space embeddings. Our ansatz follows a fully entangled design of learnable parametrised quantum circuits, with quantum (unitary) operations performed in the real Hilbert space, resulting in numerically stable training with fast convergence. QVF does not rely on classical post-processing---in contrast to the previous QINR learning approach---and directly employs measurements to extract learned signals encoded in the ansatz. Experiments on a quantum hardware simulator demonstrate that QVF outperforms existing quantum approach and competes widely used classical foundational baselines in terms of visual representation accuracy across various metrics and model characteristics. We also show applications of QVF in 2D and 3D field completion and 3D shape interpolation, highlighting its practical potential. Project page: \\url{https://4dqv.mpi-inf.mpg.de/QVF/}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nf8yfPDFTl": {
    "title": "SeerAttention: Self-distilled Attention Gating for Efficient Long-context Prefilling",
    "volume": "poster",
    "abstract": "Attention is the cornerstone of modern Large Language Models (LLMs). Yet its quadratic complexity hinders efficiency and scalability, especially for long-context processing. A promising approach is to leverage sparsity in attention. However, existing sparsity-based solutions predominantly rely on predefined patterns or heuristics at the attention head level, struggling to adapt dynamically to different contexts efficiently. We propose SeerAttention, a simple yet effective attention mechanism that directly learns the block-level attention sparsity from the LLM itself. Inspired by the gating mechanism in Mixture of Experts (MoE), SeerAttention augments the conventional attention with a **learnable gate** that **selectively activates important blocks** within the attention map. Specifically, the gate first pools the query (Q) and key (K) tensors along the sequence dimension and processes them through learnable linear layers. The resulting matrices are then multiplied together to produce the gating scores, which are used to predict block-level attention sparsity. Combined with our block-sparse FlashAttention kernel, SeerAttention can achieve significant speedup on GPUs. When applied to pre-trained LLMs, SeerAttention only requires training the gate parameters in a lightweight self-distillation manner, allowing rapid convergence. Our evaluation results demonstrate that SeerAttention achieves better model accuracy and lower latency for long-context pre-filling compared to prior methods. Code is available at: https://github.com/microsoft/SeerAttention",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=quJdphBcdP": {
    "title": "WebDancer: Towards Autonomous Information Seeking Agency",
    "volume": "poster",
    "abstract": "Addressing intricate real-world problems necessitates in-depth information seeking and multi-step reasoning. Recent progress in agentic systems, exemplified by Deep Research, underscores the potential for autonomous multi-step research. In this work, we present a cohesive paradigm for building end-to-end agentic information seeking agents from a data-centric and training-stage perspective. Our approach consists of four key stages: (1) browsing data construction, (2) trajectories sampling, (3) supervised fine-tuning for effective cold start, and (4) reinforcement learning for enhanced generalisation. We instantiate this framework in a web agent based on the ReAct format, WebDancer. Empirical evaluations on the challenging GAIA and WebWalkerQA benchmarks demonstrate the strong performance of WebDancer, achieving considerable results and highlighting the efficacy of our training paradigm. Further analysis of agent training provides valuable insights and actionable, systematic pathways for developing more capable agentic models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PuljHhCYxX": {
    "title": "QuadEnhancer: Leveraging Quadratic Transformations to Enhance Deep Neural Networks",
    "volume": "poster",
    "abstract": "The combination of linear transformations and nonlinear activation functions forms the foundation of most modern deep neural networks, enabling them to approximate highly complex functions. This paper explores the introduction of quadratic transformations to further increase the nonlinearity of the model, with the aim of enhancing the performance of existing architectures. To minimize the additional parameters and computational burden, we propose a lightweight quadratic enhancer that leverages matrix decomposition, weight sharing, and sparsification techniques. This approach introduces only a minimal and negligible increase in parameters and forward computation, while still yielding substantial improvements in model performance. We evaluate the effectiveness of the proposed method across three tasks: text classification, image classification, and fine-tuning large language models (LLMs). In all tasks, our approach demonstrates significant performance gains",
    "checked": true,
    "id": "be573091e13038bf6e453e331abe00f15cd61d38",
    "semantic_title": "quadenhancer: leveraging quadratic transformations to enhance deep neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aTBM5j3oyA": {
    "title": "Sample-Conditional Coverage in Split-Conformal Prediction",
    "volume": "poster",
    "abstract": "We revisit the problem of constructing predictive confidence sets for which we wish to obtain some type of conditional validity. We provide new arguments showing how ``split conformal'' methods achieve near desired coverage levels with high probability, a guarantee conditional on the validation data rather than marginal over it. In addition, we directly consider (approximate) conditional coverage, where, e.g., conditional on a covariate $X$ belonging to some group of interest, we seek a guarantee that a predictive set covers the true outcome $Y$. We show that the natural method of performing quantile regression on a held-out (validation) dataset yields minimax optimal guarantees of coverage in these cases. Complementing these positive results, we also provide experimental evidence highlighting work that remains to develop computationally efficient valid predictive inference methods",
    "checked": false,
    "id": "4ae06922dcc4e295c6381c111fa3bf8cad74996d",
    "semantic_title": "a few observations on sample-conditional coverage in conformal prediction",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=dsp8dUlZFq": {
    "title": "Where Graph Meets Heterogeneity: Multi-View Collaborative Graph Experts",
    "volume": "poster",
    "abstract": "The convergence of graph learning and multi-view learning has propelled the emergence of multi-view graph neural networks (MGNNs), offering unprecedented capabilities to address complex real-world data characterized by heterogeneous yet interconnected information. While existing MGNNs exploit the potential of multi-view graphs, they often fail to harmonize the dual inductive biases critical to multi-view learning: consistency (inherent inter-view agreement) and complementarity (view-specific distinctiveness). To bridge this gap, we propose Multi-view Collaborative Graph Experts (MvCGE), a novel framework grounded in the Mixture-of-Experts (MoE) paradigm. MvCGE establishes architectural consistency through shared parameters while preserving complementarity via layer-wise collaborative graph experts, which are dynamically activated by a graph-aware routing mechanism that adapts to the structural nuances of each view. This dual-level design is further reinforced by two novel components: a load equilibrium loss to prevent expert collapse and ensure balanced specialization, and a graph discrepancy loss based on distributional divergence to enhance inter-view complementarity. Extensive experiments on diverse datasets demonstrate MvCGE's superiority",
    "checked": false,
    "id": "f57b9e8919106334e8f87b09f39eab00b696bc7a",
    "semantic_title": "towards long-tailed recognition for graph classification via collaborative experts",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=EBHZNmygTT": {
    "title": "SAINT: Sequence-Aware Integration for Spatial Transcriptomics Multi-View Clustering",
    "volume": "poster",
    "abstract": "Spatial transcriptomics (ST) technologies provide gene expression measurements with spatial resolution, enabling the dissection of tissue structure and function. A fundamental challenge in ST analysis is clustering spatial spots into coherent functional regions. While existing models effectively integrate expression and spatial signals, they largely overlook sequence-level biological priors encoded in the DNA sequences of expressed genes. To bridge this gap, we propose SAINT (Sequence-Aware Integration for Nucleotide-informed Transcriptomics), a unified framework that augments spatial representation learning with nucleotide-derived features. We construct sequence-augmented datasets across 14 tissue sections from three widely used ST benchmarks (DLPFC, HBC, and MBA), retrieving reference DNA sequences for each expressed gene and encoding them using a pretrained Nucleotide Transformer. For each spot, gene-level embeddings are aggregated via expression-weighted and attention-based pooling, then fused with spatial-expression representations through a late fusion module. Extensive experiments demonstrate that SAINT consistently improves clustering performance across multiple datasets. Experiments validate the superiority, effectiveness, sensitivity, and transferability of our framework, confirming the complementary value of incorporating sequence-level priors into spatial transcriptomics clustering",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zlRvBwWFII": {
    "title": "Towards Syn-to-Real IQA: A Novel Perspective on Reshaping Synthetic Data Distributions",
    "volume": "poster",
    "abstract": "Blind Image Quality Assessment (BIQA) has advanced significantly through deep learning, but the scarcity of large-scale labeled datasets remains a challenge. While synthetic data offers a promising solution, models trained on existing synthetic datasets often show limited generalization ability. In this work, we make a key observation that representations learned from synthetic datasets often exhibit a discrete and clustered pattern that hinders regression performance: features of high-quality images cluster around reference images, while those of low-quality images cluster based on distortion types. Our analysis reveals that this issue stems from the distribution of synthetic data rather than model architecture. Consequently, we introduce a novel framework SynDR-IQA, which reshapes synthetic data distribution to enhance BIQA generalization. Based on theoretical derivations of sample diversity and redundancy's impact on generalization error, SynDR-IQA employs two strategies: distribution-aware diverse content upsampling, which enhances visual diversity while preserving content distribution, and density-aware redundant cluster downsampling, which balances samples by reducing the density of densely clustered areas. Extensive experiments across three cross-dataset settings (synthetic-to-authentic, synthetic-to-algorithmic, and synthetic-to-synthetic) demonstrate the effectiveness of our method. The code is available at https://github.com/Li-aobo/SynDR-IQA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=krG6UHAvYr": {
    "title": "MisoDICE: Multi-Agent Imitation from Mixed-Quality Demonstrations",
    "volume": "poster",
    "abstract": "We study offline imitation learning (IL) in cooperative multi-agent settings, where demonstrations have unlabeled mixed quality - containing both expert and suboptimal trajectories. Our proposed solution is structured in two stages: trajectory labeling and multi-agent imitation learning, designed jointly to enable effective learning from heterogeneous, unlabeled data. In the first stage, we combine advances in large language models and preference-based reinforcement learning to construct a progressive labeling pipeline that distinguishes expert-quality trajectories. In the second stage, we introduce MisoDICE, a novel multi-agent IL algorithm that leverages these labels to learn robust policies while addressing the computational complexity of large joint state-action spaces. By extending the popular single-agent DICE framework to multi-agent settings with a new value decomposition and mixing architecture, our method yields a convex policy optimization objective and ensures consistency between global and local policies. We evaluate MisoDICE on multiple standard multi-agent RL benchmarks and demonstrate superior performance, especially when expert data is scarce",
    "checked": false,
    "id": "8c376a7d9bad8122afaf4f841db39ae4c1bf2162",
    "semantic_title": "misodice: multi-agent imitation from unlabeled mixed-quality demonstrations",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=6nBg0uUxhO": {
    "title": "Can NeRFs \"See\" without Cameras?",
    "volume": "poster",
    "abstract": "Neural Radiance Fields (NeRFs) have been remarkably successful at synthesizing novel views of 3D scenes by optimizing a volumetric scene function. This scene function models how optical rays bring color information from a 3D object to the camera pixels. Radio frequency (RF) or audio signals can also be viewed as a vehicle for delivering information about the environment to a sensor. However, unlike camera pixels, an RF/audio sensor receives a mixture of signals that contain many environmental reflections (also called \"multipath\"). Is it still possible to infer the environment using such multipath signals? We show that with redesign, NeRFs can be taught to learn from multipath signals, and thereby \"see\" the environment. As a grounding application, we aim to infer the indoor floorplan of a home from sparse WiFi measurements made at multiple locations inside the home. Although a difficult inverse problem, our implicitly learnt floorplans look promising, and enables forward applications, such as indoor signal prediction and basic ray tracing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JGkZgEEjiM": {
    "title": "Off-policy Reinforcement Learning with Model-based Exploration Augmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kgmyjyDFrx": {
    "title": "Causal Head Gating: A Framework for Interpreting Roles of Attention Heads in Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rWW5wdECl8": {
    "title": "On the Edge of Memorization in Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "c7c1966fe3e90aaa6ae4ba8e8c78821897adf6fb",
    "semantic_title": "on the edge of memorization in diffusion models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=kCUDzyKQ7G": {
    "title": "Adam Reduces a Unique Form of Sharpness: Theoretical Insights Near the Minimizer Manifold",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "542a9c2ebca2dddba0493eaba5697ff2a8bad6cd",
    "semantic_title": "adam reduces a unique form of sharpness: theoretical insights near the minimizer manifold",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s14llhrkjA": {
    "title": "Zero-Shot Context Generalization in Reinforcement Learning from Few Training Contexts",
    "volume": "poster",
    "abstract": "Deep reinforcement learning (DRL) has achieved remarkable success across multiple domains, including competitive games, natural language processing, and robotics. Despite these advancements, policies trained via DRL often struggle to generalize to evaluation environments with different parameters. This challenge is typically addressed by training with multiple contexts and/or by leveraging additional structure in the problem. However, obtaining sufficient training data across diverse contexts can be impractical in real-world applications. In this work, we consider contextual Markov decision processes (CMDPs) with transition and reward functions that exhibit regularity in context parameters. We introduce the context-enhanced Bellman equation (CEBE) to improve generalization when training on a single context. We prove both analytically and empirically that the CEBE yields a first-order approximation to the Q function trained across multiple contexts. We then derive context sample enhancement (CSE) as an efficient data augmentation method for approximating the CEBE in deterministic control environments. We numerically validate the performance of CSE in simulation environments, showcasing its potential to improve generalization in DRL",
    "checked": true,
    "id": "30c3606f7472aae170195b6cb5ce139dcf8dd43f",
    "semantic_title": "zero-shot context generalization in reinforcement learning from few training contexts",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7dJfwHG3GN": {
    "title": "Democratizing Clinical Risk Prediction with Cross-Cohort Cross-Modal Knowledge Transfer",
    "volume": "poster",
    "abstract": "Clinical risk prediction plays a crucial role in early disease detection and personalized intervention. While recent models increasingly incorporate multimodal data, their development typically assumes access to large-scale, multimodal datasets and substantial computational resources. In practice, however, most clinical sites operate under resource constraints, with access limited to EHR data alone and insufficient capacity to train complicated models. This gap highlights the urgent need to democratize clinical risk prediction by enabling effective deployment in data- and resource-limited local clinical settings. In this work, we propose a cross-cohort cross-modal knowledge transfer framework that leverages the multimodal model trained on a nationwide cohort and adapts it to local cohorts with only EHR data. We focus on EHR and genetic data as representative multimodal inputs and address two key challenges. First, to mitigate the influence of noisy or less informative biological signals, we propose a novel mixture-of-aggregations design to enhance the modeling of informative and relevant genetic features. Second, to support rapid model adaptation in low-resource sites, we develop a lightweight graph-guided fine-tuning method that adapts pretrained phenotypical EHR representations to target cohorts using limited patient data. Extensive experiments on real-world clinical data validate the effectiveness of our proposed model",
    "checked": false,
    "id": "31b105e7f7995d9faac6a044f118c16d251dbd66",
    "semantic_title": "tmbclaw: tumor clone-aware graph learning improves immunotherapy response prediction across heterogeneous cohorts",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u1QFeoxnhW": {
    "title": "Revisiting Consensus Error: A Fine-grained Analysis of Local SGD under Second-order Data Heterogeneity",
    "volume": "poster",
    "abstract": "Local SGD, or Federated Averaging, is one of the most widely used algorithms for distributed optimization. Although it often outperforms alternatives such as mini-batch SGD, existing theory has not fully explained this advantage under realistic assumptions about data heterogeneity. Recent work has suggested that a second-order heterogeneity assumption may suffice to justify the empirical gains of local SGD. We confirm this conjecture by establishing new upper and lower bounds on the convergence of local SGD. These bounds demonstrate how a low second-order heterogeneity, combined with third-order smoothness, enables local SGD to interpolate between heterogeneous and homogeneous regimes while maintaining communication efficiency. Our main technical contribution is a refined analysis of the consensus error, a central quantity in such results. We validate our theory with experiments on a distributed linear regression task",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SpSru9SRyp": {
    "title": "ScaleDiff: Higher-Resolution Image Synthesis via Efficient and Model-Agnostic Diffusion",
    "volume": "poster",
    "abstract": "Text-to-image diffusion models often exhibit degraded performance when generating images beyond their training resolution. Recent training-free methods can mitigate this limitation, but they often require substantial computation or are incompatible with recent Diffusion Transformer models. In this paper, we propose ScaleDiff, a model-agnostic and highly efficient framework for extending the resolution of pretrained diffusion models without any additional training. A core component of our framework is Neighborhood Patch Attention (NPA), an efficient mechanism that reduces computational redundancy in the self-attention layer with non-overlapping patches. We integrate NPA into an SDEdit pipeline and introduce Latent Frequency Mixing (LFM) to better generate fine details. Furthermore, we apply Structure Guidance to enhance global structure during the denoising process. Experimental results demonstrate that ScaleDiff achieves state-of-the-art performance among training-free methods in terms of both image quality and inference speed on both U-Net and Diffusion Transformer architectures",
    "checked": true,
    "id": "6a11e5f8951dcd0046ad36d9c6ff4a5c9dab7526",
    "semantic_title": "scalediff: higher-resolution image synthesis via efficient and model-agnostic diffusion",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5ODlY0KYCx": {
    "title": "On Learning Verifiers and Implications to Chain-of-Thought Reasoning",
    "volume": "poster",
    "abstract": "Chain-of-Thought reasoning has emerged as a powerful approach for solving complex math- ematical and logical problems. However, it can often veer off track through incorrect or unsubstantiated inferences. Formal mathematical reasoning, which can be checked with a formal verifier, is one approach to addressing this issue. However, currently LLMs are simply not good enough to solve complex problems in a formal way, and even just formalizing an informal problem statement can be challenging. Motivated by this fact, in this work we consider the problem of learning reliable verifiers for sequential reasoning, including natural language Chain-of-Thought reasoning. That is, given a problem statement and step-by-step solution in natural language, the aim of the verifier is to output [Yes] if the reasoning steps in the solution are all valid, and [No] otherwise. In this work we give a formal PAC-learning framework for studying this problem. We propose and analyze several natural verification goals, at different levels of strength, in this framework. We provide sample complexity upper-bounds for learning verifiers satisfying these goals, as well as lower-bound and impossibility results for learning other natural verification objectives without additional assumptions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l0tTXno7c2": {
    "title": "Score-Based Diffusion Modeling for Nonparametric Empirical Bayes in Heteroscedastic Gaussian Mixtures",
    "volume": "poster",
    "abstract": "We propose a generalized score-based diffusion framework for learning multivariate Gaussian mixture models with homoscedastic or heteroscedastic noise. Our goal is to nonparametrically estimate the latent location distribution and denoise the observations. Departing from the conventional maximum likelihood approach, we reinterpret each observation as a temporal slice of a family of stochastic diffusion processes. This modeling choice enables a principled characterization of the additive noise structure and supports a multi-step denoising procedure grounded in reverse-time dynamics. We introduce a score-based objective that explicitly models the latent distribution and accommodates observation-specific noise covariances. Theoretically, we establish that the score estimation error with $n$ independent observations achieves a near-parametric error rate of $\\frac{\\mathrm{polylog}(n)}{n}$, improving upon existing results in the diffusion literature. Empirically, our method outperforms the nonparametric maximum likelihood estimator in both density estimation and denoising fidelity, especially in high-dimensional settings. These findings suggest a promising direction for integrating nonparametric empirical Bayes with diffusion-based generative modeling for latent structure recovery",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FxV7Fvlm2T": {
    "title": "CCL: Causal-aware In-context Learning for Out-of-Distribution Generalization",
    "volume": "poster",
    "abstract": "In-context learning (ICL), a nonparametric learning method based on the knowledge of demonstration sets, has become a de facto standard for large language models (LLMs). The primary goal of ICL is to select valuable demonstration sets to enhance the performance of LLMs. Traditional ICL methods choose demonstration sets that share similar features with a given query. However, we have found that the performance of these traditional ICL approaches is limited on out-of-distribution (OOD) datasets, where the demonstration set and the query originate from different distributions. To ensure robust performance in OOD datasets, it is essential to learn causal representations that remain invariant between the source and target datasets. Inspired by causal representation learning, we propose causal-aware in-context learning (CCL). CCL captures the causal representations of a given dataset and selects demonstration sets that share similar causal features with the query. To achieve this, CCL employs a novel VAE-based causal representation learning technique. We demonstrate that CCL improves the OOD generalization performance of LLMs both theoretically and empirically. Code is available at: \\url{https://github.com/MLAI-Yonsei/causal-context-learning}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lscdYmLJ5v": {
    "title": "On the creation of narrow AI: hierarchy and nonlocality of neural network skills",
    "volume": "poster",
    "abstract": "We study the problem of creating strong, yet narrow, AI systems. While recent AI progress has been driven by the training of large general-purpose foundation models, the creation of smaller models specialized for narrow domains could be valuable for both efficiency and safety. In this work, we explore two challenges involved in creating such systems, having to do with basic properties of how neural networks learn and structure their representations. The first challenge regards when it is possible to train narrow models from scratch. Through experiments on a synthetic task, we find that it is sometimes necessary to train networks on a wide distribution of data to learn certain narrow skills within that distribution. This effect arises when skills depend on each other hierarchically, and training on a broad distribution introduces a curriculum which substantially accelerates learning. The second challenge regards how to transfer particular skills from large general models into small specialized models. We find that model skills are often not perfectly localized to a particular set of prunable components. However, we find that methods based on pruning can still outperform distillation. We investigate the use of a regularization objective to align desired skills with prunable components while unlearning unnecessary skills",
    "checked": true,
    "id": "55e695f516e4873fb33d15a7f3abdf02dd2e0a8f",
    "semantic_title": "on the creation of narrow ai: hierarchy and nonlocality of neural network skills",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ZtzWvNKOCr": {
    "title": "Q3R: Quadratic Reweighted Rank Regularizer for Effective Low-Rank Training",
    "volume": "poster",
    "abstract": "Parameter-efficient training, based on low-rank optimization, has become a highly successful tool for fine-tuning large deep-learning models. However, these methods fail at low-rank pre-training tasks where maintaining the low-rank structure and the objective remains a challenging task. We propose the Quadratic Reweighted Rank Regularizer dubbed Q3R, which leads to a novel low-rank inducing training strategy inspired by the iteratively reweighted least squares (IRLS) framework. Q3R is based on a quadratic regularizer term which majorizes a smoothed log determinant serving as rank surrogate objective. Unlike other low-rank training techniques, Q3R is able to train weight matrices with prescribed, low target ranks of models that achieve comparable predictive performance as dense models, with small computational overhead, while remaining fully compatible with existing architectures. In experiments, we are able to truncate 60% of the parameters of a ViT-Tiny parameters with marginal loss in CIFAR-10 performance and up to 80% with only 4% accuracy drop. The efficacy of Q3R is confirmed on Transformers across both image and language tasks, including for low-rank fine-tuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rMptAK0Xm8": {
    "title": "Unveiling m-Sharpness Through the Structure of Stochastic Gradient Noise",
    "volume": "poster",
    "abstract": "Sharpness‐aware minimization (SAM) has emerged as a highly effective technique for improving model generalization, but its underlying principles are not fully understood. We investigated the phenomenon known as m-sharpness, where the performance of SAM improves monotonically as the micro-batch size for computing perturbations decreases. In practice, the empirical m-sharpness effect underpins the deployment of SAM in distributed training, yet a rigorous theoretical account has remained lacking. To provide a theoretical explanation for m-sharpness, we leverage an extended Stochastic Differential Equation (SDE) framework and analyze the structure of stochastic gradient noise (SGN) to characterize the dynamics of various SAM variants, including n-SAM and m-SAM. Our findings reveal that the stochastic noise introduced during SAM perturbations inherently induces a variance-based sharpness regularization effect. Motivated by our theoretical insights, we introduce Reweighted SAM (RW-SAM), which employs sharpness-weighted sampling to mimic the generalization benefits of m-SAM while remaining parallelizable. Comprehensive experiments validate the effectiveness of our theoretical analysis and proposed method",
    "checked": true,
    "id": "a3728ca9cbb622b67f35e7650549158dd4f14f19",
    "semantic_title": "unveiling m-sharpness through the structure of stochastic gradient noise",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0SRGbRbngJ": {
    "title": "Distributional LLM-as-a-Judge",
    "volume": "poster",
    "abstract": "LLMs have emerged as powerful evaluators in the LLM-as-a-Judge paradigm, offering significant efficiency and flexibility compared to human judgments. However, previous methods primarily rely on single-point evaluations, overlooking the inherent diversity and uncertainty in human evaluations. This approach leads to information loss and decreases the reliability of evaluations. To address this limitation, we propose a novel training framework that explicitly aligns the LLM-generated judgment distribution with human evaluation distributions. Specifically, we propose a distributional alignment objective based on KL divergence, combined with an auxiliary cross-entropy regularization to stabilize the training process. Furthermore, due to limited human annotations, empirical human distributions are merely noisy estimates of the true underlying distribution. We therefore incorporate adversarial training to ensure a robust alignment with this true distribution, rather than overfitting to its imperfect approximation. Extensive experiments across various LLM backbones and evaluation tasks demonstrate that our framework significantly outperforms existing closed-source LLMs and conventional single-point alignment methods, with superior alignment quality, strong robustness, and competitive evaluation accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GiqeRe1NsY": {
    "title": "Understanding Softmax Attention Layers:\\ Exact Mean-Field Analysis on a Toy Problem",
    "volume": "poster",
    "abstract": "Self-attention has emerged as a fundamental component driving the success of modern transformer architectures, which power large language models and various applications. However, a theoretical understanding of how such models actually work is still under active development. The recent work of (Marion et al., 2025) introduced the so-called \"single-location regression\" problem, which can provably be solved by a simplified self-attention layer but not by linear models, thereby demonstrating a striking functional separation. A rigorous analysis of self-attention with softmax for this problem is challenging due to the coupled nature of the model. In the present work, we use ideas from the classical random energy model in statistical physics to analyze softmax self-attention on the single-location problem. Our analysis yields exact analytic expressions for the population risk in terms of the overlaps between the learned model parameters and those of an oracle. Moreover, we derive a detailed description of the gradient descent dynamics for these overlaps and prove that, under broad conditions, the dynamics converge to the unique oracle attractor. Our work not only advances our understanding of self-attention but also provides key theoretical ideas that are likely to find use in further analyses of even more complex transformer architectures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vyxhmosU6E": {
    "title": "Differentially Private High-dimensional Variable Selection via Integer Programming",
    "volume": "poster",
    "abstract": "Sparse variable selection improves interpretability and generalization in high-dimensional learning by selecting a small subset of informative features. Recent advances in Mixed Integer Programming (MIP) have enabled solving large-scale non-private sparse regression—known as Best Subset Selection (BSS)—with millions of variables in minutes. However, extending these algorithmic advances to the setting of Differential Privacy (DP) has remained largely unexplored. In this paper, we introduce two new differentially private estimators for sparse variable selection, levering modern MIP techniques. Our framework is general and applies broadly to problems like sparse regression or classification, and we provide theoretical support recovery guarantees in the case of BSS. Inspired by the exponential mechanism, we develop structured sampling procedures that efficiently explore the non-convex objective landscape, avoiding the exhaustive combinatorial search in the exponential mechanism. We complement our theoretical findings with extensive numerical experiments, using both least squares and hinge loss for our objective function, and demonstrate that our methods achieve state-of-the-art empirical support recovery, outperforming competing algorithms in settings with up to $p=10^4$",
    "checked": true,
    "id": "10f9d8f28bd2793fe0e8e2f9b9b115ffe6f326aa",
    "semantic_title": "differentially private high-dimensional variable selection via integer programming",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qO7j8ymv5I": {
    "title": "SSIMBaD: Sigma Scaling with SSIM-Guided Balanced Diffusion for AnimeFace Colorization",
    "volume": "poster",
    "abstract": "We propose a novel diffusion-based framework for automatic colorization of Anime-style facial sketches, which preserves the structural fidelity of the input sketch while effectively transferring stylistic attributes from a reference image. Our approach builds upon recent continuous-time diffusion models, but departs from traditional methods that rely on predefined noise schedules, which often fail to maintain perceptual consistency across the generative trajectory. To address this, we introduce SSIMBaD (Sigma Scaling with SSIM-Guided Balanced Diffusion), a sigma-space transformation that ensures linear alignment of perceptual degradation, as measured by structural similarity. This perceptual scaling enforces uniform visual difficulty across timesteps, enabling more balanced and faithful reconstructions. On a large-scale Anime face dataset, SSIMBaD attains state-of-the-art structural fidelity and strong perceptual quality, with robust generalization to diverse styles and structural variations",
    "checked": true,
    "id": "8f129b78ca9adfee9c5cbd6668dc545f32ecd88c",
    "semantic_title": "ssimbad: sigma scaling with ssim-guided balanced diffusion for animeface colorization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TE63KPCXWt": {
    "title": "Sharp Analysis for KL-Regularized Contextual Bandits and RLHF",
    "volume": "poster",
    "abstract": "Reverse-Kullback-Leibler (KL) regularization has emerged to be a predominant technique to enhance policy optimization in reinforcement learning (RL) and reinforcement learning from human feedback (RLHF), which forces the learned policy to stay close to a reference policy. While the effectiveness of KL-regularization has been empirically demonstrated in various practical scenarios, current theoretical analyses of KL-regularized RLHF still yield the same $\\mathcal{O}(1 / \\epsilon^2)$ sample complexity as ones without KL-regularization. To understand the fundamental distinction between objectives with KL-regularization and ones without KL-regularization, we are the first to theoretically demonstrate the power of KL-regularization by providing a sharp analysis for KL-regularized contextual bandits and RLHF, revealing an $\\mathcal{O}(1 / \\epsilon)$ sample complexity when $\\epsilon$ is sufficiently small. We also prove matching lower bounds for both settings. More specifically, we study how the coverage of the reference policy affects the sample complexity of KL-regularized online contextual bandits and RLHF. We show that with sufficient coverage from the reference policy, a simple two-stage mixed sampling algorithm can achieve an $\\mathcal{O}(1 / \\epsilon)$ sample complexity with only an additive dependence on the coverage coefficient, thus proving the benefits of online data even without explicit exploration. Our results provide a comprehensive understanding of the roles of KL-regularization and data coverage in online decision making, shedding light on the design of more efficient algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FJkDPf4n7j": {
    "title": "Efficient and Near-Optimal Algorithm for Contextual Dueling Bandits with Offline Regression Oracles",
    "volume": "poster",
    "abstract": "The problem of contextual dueling bandits is central to reinforcement learning with human feedback (RLHF), a widely used approach in AI alignment for incorporating human preferences into learning systems. Despite its importance, existing methods are constrained either by strong preference modeling assumptions or by applicability only to finite action spaces. Moreover, prior algorithms typically rely on online optimization oracles, which are computationally infeasible for complex function classes, limiting their practical effectiveness. In this work, we present the first fundamental theoretical study of general contextual dueling bandits over continuous action spaces. Our key contribution is a novel algorithm based on a regularized min-max optimization framework that achieves a regret bound of $\\tilde{O}(\\sqrt{dT})$—the first such guarantee for this general setting. By leveraging offline oracles instead of online ones, our method further improves computational efficiency. Empirical evaluations validate our theoretical findings, with our approach significantly outperforming existing baselines in terms of regret",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BlcTbgxr4Z": {
    "title": "Gaussian Processes for Shuffled Regression",
    "volume": "poster",
    "abstract": "Shuffled regression is the problem of learning regression functions from shuffled data where the correspondence between the input features and target response is unknown. This paper proposes a probabilistic model for shuffled regression called Gaussian Process Shuffled Regression (GPSR). By introducing Gaussian processes as a prior of regression functions in function space via the kernel function, GPSR can express a wide variety of functions in a nonparametric manner while quantifying the uncertainty of the prediction. By adopting the Bayesian evidence maximization framework and a theoretical analysis of the connection between the marginal likelihood/predictive distribution of GPSR and that of standard Gaussian process regression (GPR), we derive an easy-to-implement inference algorithm for GPSR that iteratively applies GPR and updates the input-output correspondence. To reduce computation costs and obtain closed-form solutions for correspondence updates, we also develop a sparse approximate variant of GPSR using its weight space formulation, which can be seen as Bayesian shuffled linear regression with random Fourier features. Experiments on benchmark datasets confirm the effectiveness of our GPSR proposal",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aHAMUEsQLc": {
    "title": "Data-Adaptive Exposure Thresholds under Network Interference",
    "volume": "poster",
    "abstract": "Randomized controlled trials often suffer from interference, a violation of the Stable Unit Treatment Value Assumption (SUTVA), where a unit's outcome is influenced by its neighbors' treatment assignments. This interference biases naive estimators of the average treatment effect (ATE). A popular method to achieve unbiasedness pairs the Horvitz-Thompson estimator of the ATE with a known exposure mapping, a function that identifies units in a given randomization unaffected by interference. For example, an exposure mapping may stipulate that a unit experiences no further interference if at least an $h$-fraction of its neighbors share its treatment status. However, selecting this threshold $h$ is challenging, requiring domain expertise; in its absence, fixed thresholds such as $h = 1$ are often used. In this work, we propose a data-adaptive method to select the $h$-fractional threshold that minimizes the mean-squared-error (MSE) of the Horvitz-Thompson estimator. Our approach estimates the bias and variance of the Horvitz-Thompson estimator paired with candidate thresholds by leveraging a first-order approximation, specifically, linear regression of potential outcomes on exposures. We present simulations illustrating that our method improves upon non-adaptive threshold choices, and an adapted Lepski's method. We further illustrate the performance of our estimator by running experiments with synthetic outcomes on a real village network dataset, and on a publicly-available Amazon product similarity graph. Furthermore, we demonstrate that our method remains robust to deviations from the linear potential outcomes model",
    "checked": false,
    "id": "54f2451ef55131de922ee5bac4313999c1e2b22b",
    "semantic_title": "data-adaptive exposure thresholds for the horvitz-thompson estimator of the average treatment effect in experiments with network interference",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pe8Dr5jtjV": {
    "title": "Versatile Transferable Unlearnable Example Generator",
    "volume": "poster",
    "abstract": "The rapid growth of publicly available data has fueled deep learning advancements but also raises concerns about unauthorized data usage. Unlearnable Examples (UEs) have emerged as a data protection strategy that introduces imperceptible perturbations to prevent unauthorized learning. However, most existing UE methods produce perturbations strongly tied to specific training sets, leading to a significant drop in unlearnability when applied to unseen data or tasks. In this paper, we argue that for broad applicability, UEs should maintain their effectiveness across diverse application scenarios. To this end, we conduct the first comprehensive study on the transferability of UEs across diverse and practical yet demanding settings. Specifically, we identify key scenarios that pose significant challenges for existing UE methods, including varying styles, out-of-distribution classes, resolutions, and architectures. Moreover, we propose $\\textbf{Versatile Transferable Generator}$ (VTG), a transferable generator designed to safeguard data across various conditions. Specifically, VTG integrates Adversarial Domain Augmentation (ADA) into the generator's training process to synthesize out-of-distribution samples, thereby improving its generalizability to unseen scenarios. Furthermore, we propose a Perturbation-Label Coupling (PLC) mechanism that leverages contrastive learning to directly align perturbations with class labels. This approach reduces the generator's reliance on data semantics, allowing VTG to produce unlearnable perturbations in a distribution-agnostic manner. Extensive experiments demonstrate the effectiveness and broad applicability of our approach. Code is available at https://github.com/zhli-cs/VTG",
    "checked": false,
    "id": "3910479c41b15e484ff0ee7378a14417fa124f2c",
    "semantic_title": "mtl-ue: learning to learn nothing for multi-task learning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=UKFg5yeZeX": {
    "title": "A Temporal Difference Method for Stochastic Continuous Dynamics",
    "volume": "poster",
    "abstract": "For continuous systems modeled by dynamical equations such as ODEs and SDEs, Bellman's principle of optimality takes the form of the Hamilton-Jacobi-Bellman (HJB) equation, which provides the theoretical target of reinforcement learning (RL). Although recent advances in RL successfully leverage this formulation, the existing methods typically assume the underlying dynamics are known a priori because they need explicit access to the coefficient functions of dynamical equations to update the value function following the HJB equation. We address this inherent limitation of HJB-based RL; we propose a model-free approach still targeting the HJB equation and propose the corresponding temporal difference method. We establish exponential convergence of the idealized continuous-time dynamics and empirically demonstrate its potential advantages over transition–kernel–based formulations. The proposed formulation paves the way toward bridging stochastic control and model-free reinforcement learning",
    "checked": true,
    "id": "ea6edcb7e1bd0a7fe844de5239d5e19f8c88bc8b",
    "semantic_title": "a temporal difference method for stochastic continuous dynamics",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NXkUWAxSEH": {
    "title": "What Data Enables Optimal Decisions? An Exact Characterization for Linear Optimization",
    "volume": "poster",
    "abstract": "We study the fundamental question of how informative a dataset is for solving a given decision-making task. In our setting, the dataset provides partial information about unknown parameters that influence task outcomes. Focusing on linear programs, we characterize when a dataset is sufficient to recover an optimal decision, given an uncertainty set on the cost vector. Our main contribution is a sharp geometric characterization that identifies the directions of the cost vector that matter for optimality, relative to the task constraints and uncertainty set. We further develop a practical algorithm that, for a given task, constructs a minimal or least-costly sufficient dataset. Our results reveal that small, well-chosen datasets can often fully determine optimal decisions---offering a principled foundation for task-aware data selection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uIIxKnFmeY": {
    "title": "When Additive Noise Meets Unobserved Mediators: Bivariate Denoising Diffusion for Causal Discovery",
    "volume": "poster",
    "abstract": "Distinguishing cause and effect from bivariate observational data is a foundational problem in many disciplines, but challenging without additional assumptions. Additive noise models (ANMs) are widely used to enable sample-efficient bivariate causal discovery. However, conventional ANM-based methods fail when unobserved mediators corrupt the causal relationship between variables. This paper makes three key contributions: first, we rigorously characterize why standard ANM approaches break down in the presence of unmeasured mediators. Second, we demonstrate that prior solutions for hidden mediation are brittle in finite sample settings, limiting their practical utility. To address these gaps, we propose Bivariate Denoising Diffusion (BiDD) for causal discovery, a method designed to handle latent noise introduced by unmeasured mediators. Unlike prior methods that infer directionality through mean squared error loss comparisons, our approach introduces a novel independence test statistic: during the noising and denoising processes for each variable, we condition on the other variable as input and evaluate the independence of the predicted noise relative to this input. We prove asymptotic consistency of BiDD under the ANM, and conjecture that it performs well under hidden mediation. Experiments on synthetic and real-world data demonstrate consistent performance, outperforming existing methods in mediator-corrupted settings while maintaining strong performance in mediator-free settings",
    "checked": true,
    "id": "b448a58eace9b474f6d28be05ac5d4e58cdef2aa",
    "semantic_title": "when additive noise meets unobserved mediators: bivariate denoising diffusion for causal discovery",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zPKeJAEo27": {
    "title": "What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence Functions",
    "volume": "poster",
    "abstract": "Large language models (LLMs) are trained on a vast amount of human-written data, but data providers often remain uncredited. In response to this issue, data valuation (or data attribution), which quantifies the contribution or value of each data to the model output, has been discussed as a potential solution. Nevertheless, applying existing data valuation methods to recent LLMs and their vast training datasets has been largely limited by prohibitive compute and memory costs. In this work, we focus on influence functions, a popular gradient-based data valuation method, and significantly improve its scalability with an efficient gradient projection strategy called LoGra that leverages the gradient structure in backpropagation. We then provide a theoretical motivation of gradient projection approaches to influence functions to promote trust in the data valuation process. Lastly, we lower the barrier to implementing data valuation systems by introducing LogIX, a software package that can transform existing training code into data valuation code with minimal effort. In our data valuation experiments, LoGra achieves competitive accuracy against more expensive baselines while showing up to 6,500x improvement in throughput and 5x reduction in GPU memory usage when applied to Llama3-8B-Instruct and the 1B-token dataset",
    "checked": true,
    "id": "f33f3dece9f34c1ec5417dccf9e0acf592d8e8cb",
    "semantic_title": "what is your data worth to gpt? llm-scale data valuation with influence functions",
    "citation_count": 66,
    "authors": []
  },
  "https://openreview.net/forum?id=GCwJd0JH5k": {
    "title": "State Size Independent Statistical Error Bound for Discrete Diffusion Models",
    "volume": "poster",
    "abstract": "Diffusion models operating in discrete state spaces have emerged as powerful approaches, demonstrating remarkable efficacy across diverse domains, including reasoning tasks and molecular design. Despite their promising applications, the theoretical foundations of these models remain substantially underdeveloped, with the existing literature predominantly focusing on continuous-state diffusion models. A critical gap persists in the theoretical understanding of discrete diffusion modeling: the absence of a rigorous framework for quantifying estimation error with finite data. Consequently, the fundamental question of how precisely one can reconstruct the true underlying distribution from a limited training set remains unresolved. In this work, we analyze the estimation error induced by a score estimation of the discrete diffusion models. One of the main difficulties in the analysis stems from the fact that the cardinality of the state space can be exponentially large with respect to its dimension, which results in an intractable error bound by a naive approach. To overcome this difficulty, we make use of a property that the state space can be smoothly embedded in a continuous Euclidean space that enables us to derive a cardinality independent bound, which is more practical in real applications. In particular, we consider a setting where the state space is structured as a hypercube graph, and another where the induced graph Laplacian can be asymptotically well approximated by the ordinary Laplacian defined on the continuous space, and then derive state space size independent bounds",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j38Cb5LlaY": {
    "title": "User-Instructed Disparity-aware Defocus Control",
    "volume": "poster",
    "abstract": "In photography, an All-in-Focus (AiF) image may not always effectively convey the creator's intent. Professional photographers manipulate Depth of Field (DoF) to control which regions appear sharp or blurred, achieving compelling artistic effects. For general users, the ability to flexibly adjust DoF enhances creative expression and image quality. In this paper, we propose UiD, a User-Instructed DoF control framework, that allows users to specify refocusing regions using text, box, or point prompts, and our UiD automatically simulates in-focus and out-of-focus (OoF) regions in the given images. However, controlling defocus blur in a single-lens camera remains challenging due to the difficulty in estimating depth-aware aberrations and the suboptimal quality of reconstructed AiF images. To address this, we leverage dual-pixel (DP) sensors, commonly found in DSLR-style and mobile cameras. DP sensors provide a small-baseline stereo pair in a single snapshot, enabling depth-aware aberration estimation. Our approach first establishes an invertible mapping between OoF and AiF images to learn spatially varying defocus kernels and the disparity features. These depth-aware kernels enable bidirectional image transformation—deblurring out-of-focus (OoF) images into all-in-focus (AiF) representations, and conversely reblurring AiF images into OoF outputs—by seamlessly switching between the kernel and its inverse form. These depth-aware kernels enable both deblurring of OoF images into AiF representations and reblurring AiF images into OoF representations by flexibly switching its original form to its inverse one. For user-guided refocusing, we first generate masks based on user prompts using SAM, which modulates disparity features in closed form, allowing dynamic kernel re-estimation for reblurring. This achieves user-controlled refocusing effects. Extensive experiments on both common datasets and the self-collected dataset demonstrate that UiD offers superior flexibility and quality in DoF manipulation imaging",
    "checked": false,
    "id": "f02907a4e51b66bfb303991191425ff01b7e4e89",
    "semantic_title": "superrs: multi scenario reciprocal-aware dual moe for unified recommendation-search ranking",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zOCENGh1Jg": {
    "title": "Private Evolution Converges",
    "volume": "poster",
    "abstract": "Private Evolution (PE) is a promising training-free method for differentially private (DP) synthetic data generation. While it achieves strong performance in some domains (e.g., images and text), its behavior in others (e.g., tabular data) is less consistent. To date, the only theoretical analysis of the convergence of PE depends on unrealistic assumptions about both the algorithm's behavior and the structure of the sensitive dataset. In this work, we develop a new theoretical framework to understand PE's practical behavior and identify sufficient conditions for its convergence. For $d$-dimensional sensitive datasets with $n$ data points from a convex and compact domain, we prove that under the right hyperparameter settings and given access to the Gaussian variation API proposed in \\cite{PE23}, PE produces an $(\\varepsilon, \\delta)$-DP synthetic dataset with expected 1-Wasserstein distance $\\tilde{O}(d(n\\varepsilon)^{-1/d})$ from the original; this establishes worst-case convergence of the algorithm as $n \\to \\infty$. Our analysis extends to general Banach spaces as well. We also connect PE to the Private Signed Measure Mechanism, a method for DP synthetic data generation that has thus far not seen much practical adoption. We demonstrate the practical relevance of our theoretical findings in experiments",
    "checked": true,
    "id": "c76813d263b62e1a4c31dc9769516e1cd131ba0c",
    "semantic_title": "private evolution converges",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tdwRIP6NG2": {
    "title": "Linearization Explains Fine-Tuning in Large Language Models",
    "volume": "poster",
    "abstract": "Parameter-Efficient Fine-Tuning (PEFT) is a popular class of techniques that strive to adapt large models in a scalable and resource-efficient manner. Yet, the mechanisms underlying their training performance and generalization remain underexplored. In this paper, we provide several insights into such fine-tuning through the lens of linearization. Fine-tuned models are often implicitly encouraged to remain close to the pretrained model. By making this explicit, using an $\\ell_2$-distance inductive bias in parameter space, we show that fine-tuning dynamics become equivalent to learning with the positive-definite neural tangent kernel (NTK). We specifically analyze how close the fully linear and the linearized fine-tuning optimizations are, based on the strength of the regularization. This allows us to be pragmatic about how good a model linearization is when fine-tuning large language models (LLMs). When linearization is a good model, our findings reveal a strong correlation between the eigenvalue spectrum of the NTK and the performance of model adaptation. Motivated by this, we give spectral perturbation bounds on the NTK induced by the choice of layers selected for fine-tuning. We empirically validate our theory on Low Rank Adaptation (LoRA) on LLMs. These insights not only characterize fine-tuning but also have the potential to enhance PEFT techniques, paving the way to better informed and more nimble adaptation in LLMs",
    "checked": false,
    "id": "f78144a912048c9e35428386d7b885f29fcf68c7",
    "semantic_title": "user rating prediction method based on fine-tuning of large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eyH8QLn2Qx": {
    "title": "RLZero: Direct Policy Inference from Language Without In-Domain Supervision",
    "volume": "poster",
    "abstract": "The reward hypothesis states that all goals and purposes can be understood as the maximization of a received scalar reward signal. However, in practice, defining such a reward signal is notoriously difficult, as humans are often unable to predict the optimal behavior corresponding to a reward function. Natural language offers an intuitive alternative for instructing reinforcement learning (RL) agents, yet previous language-conditioned approaches either require costly supervision or test-time training given a language instruction. In this work, we present a new approach that uses a pretrained RL agent trained using only unlabeled, offline interactions—without task-specific supervision or labeled trajectories—to get zero-shot test-time policy inference from arbitrary natural language instructions. We introduce a framework comprising three steps: *imagine*, *project*, and *imitate*. First, the agent imagines a sequence of observations corresponding to the provided language description using video generative models. Next, these imagined observations are projected into the target environment domain. Finally, an agent pretrained in the target environment with unsupervised RL instantly imitates the projected observation sequence through a closed-form solution. To the best of our knowledge, our method, RLZero, is the first approach to show direct language-to-behavior generation abilities on a variety of tasks and environments without any in-domain supervision. We further show that components of RLZero can be used to generate policies zero-shot from cross-embodied videos, such as those available on YouTube, even for complex embodiments like humanoids",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MYe8FiahWi": {
    "title": "No Experts, No Problem: Avoidance Learning from Bad Demonstrations",
    "volume": "poster",
    "abstract": "This paper addresses the problem of learning avoidance behavior within the context of offline imitation learning. In contrast to conventional methodologies that prioritize the replication of expert or near-expert demonstrations, our work investigates a setting where expert (or desirable) data is absent, and the objective is to learn to eschew undesirable actions by leveraging demonstrations of such behavior (i.e., learning from negative examples). To address this challenge, we propose a novel training objective grounded in the maximum entropy principle. We further characterize the fundamental properties of this objective function, reformulating the learning process as a cooperative inverse Q-learning task. Moreover, we introduce an efficient strategy for the integration of unlabeled data (i.e., data of indeterminate quality) to facilitate unbiased and practical offline training. The efficacy of our method is evaluated across standard benchmark environments, where it consistently outperforms state-of-the-art baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OEp1J4V2fN": {
    "title": "IOSTOM: Offline Imitation Learning from Observations via State Transition Occupancy Matching",
    "volume": "poster",
    "abstract": "Offline Learning from Observations (LfO) focuses on enabling agents to imitate expert behavior using datasets that contain only expert state trajectories and separate transition data with suboptimal actions. This setting is both practical and critical in real-world scenarios where direct environment interaction or access to expert action labels is costly, risky, or infeasible. Most existing LfO methods attempt to solve this problem through state or state-action occupancy matching. They typically rely on pretraining a discriminator to differentiate between expert and non-expert states, which could introduce errors and instability—especially when the discriminator is poorly trained. While recent discriminator-free methods have emerged, they generally require substantially more data, limiting their practicality in low-data regimes. In this paper, we propose IOSTOM ($\\textit{Imitation from Observation via State Transition Occupancy Matching}$), a novel offline LfO algorithm designed to overcome these limitations. Our approach formulates a learning objective based on the joint state visitation distribution. A key distinction of IOSTOM is that it first excludes actions entirely from the training objective. Instead, we learn an $\\textit{implicit policy}$ that models transition probabilities between states, resulting in a more compact and stable optimization problem. To recover the expert policy, we introduce an efficient action inference mechanism that $\\textit{avoids training an inverse dynamics model}$. Extensive empirical evaluations across diverse offline LfO benchmarks show that IOSTOM substantially outperforms state-of-the-art methods, demonstrating both improved performance and data efficiency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rud3M6wlxH": {
    "title": "ARGenSeg: Image Segmentation with Autoregressive Image Generation Model",
    "volume": "poster",
    "abstract": "We propose a novel AutoRegressive Generation-based paradigm for image Segmentation (ARGenSeg), achieving multimodal understanding and pixel-level perception within a unified framework. Prior works integrating image segmentation into multimodal large language models (MLLMs) typically employ either boundary points representation or dedicated segmentation heads. These methods rely on discrete representations or semantic prompts fed into task-specific decoders, which limits the ability of the MLLM to capture fine-grained visual details. To address these challenges, we introduce a segmentation framework for MLLM based on image generation, which naturally produces dense masks for target objects. We leverage MLLM to output visual tokens and detokenize them into images using an universal VQ-VAE, making the segmentation fully dependent on the pixel-level understanding of the MLLM. To reduce inference latency, we employ a next-scale-prediction strategy to generate required visual tokens in parallel. Extensive experiments demonstrate that our method surpasses prior state-of-the-art approaches on multiple segmentation datasets with a remarkable boost in inference speed, while maintaining strong understanding capabilities",
    "checked": true,
    "id": "0fb25541f070f6445b9194f793a1d20c293f41b7",
    "semantic_title": "argenseg: image segmentation with autoregressive image generation model",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f6AoMP75dy": {
    "title": "From Kolmogorov to Cauchy: Shallow XNet Surpasses KANs",
    "volume": "poster",
    "abstract": "We study a shallow variant of XNet, a neural architecture whose activation functions are derived from the Cauchy integral formula. While prior work focused on deep variants, we show that even a single-layer XNet exhibits near-exponential approximation rates—exceeding the polynomial bounds of MLPs and spline-based networks such as Kolmogorov–Arnold Networks (KANs). Empirically, XNet reduces approximation error by over 600× on discontinuous functions, achieves up to 20,000× lower residuals in physics-informed PDEs, and improves policy accuracy and sample efficiency in PPO-based reinforcement learning—while maintaining comparable or better computational efficiency than KAN baselines. These results demonstrate that expressive approximation can stem from principled activation design rather than depth alone, offering a compact, theoretically grounded alternative for function approximation, scientific computing, and control",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=msaLqsSTyF": {
    "title": "Vertical Federated Feature Screening",
    "volume": "poster",
    "abstract": "With the rapid development of the big data era, Vertical Federated Learning (VFL) has been widely applied to enable data collaboration while ensuring privacy protection. However, the ultrahigh dimensionality of features and the sparse data structures inherent in large-scale datasets introduce significant computational complexity. In this paper, we propose the Vertical Federated Feature Screening (VFS) algorithm, which effectively reduces computational, communication, and encryption costs. VFS is a two-stage feature screening procedure that proceeds from coarse to fine: the first stage quickly filters out irrelevant feature groups, followed by a more refined screening of individual features. It significantly reduces the resource demands of downstream tasks such as secure joint modeling or federated feature selection. This efficiency is particularly beneficial in scenarios with ultrahigh feature dimensionality or severe class imbalance in the response variable. The statistical and computational properties of VFS are rigorously established. Numerical simulations and real-world applications demonstrate its superior performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9AHkbALT2t": {
    "title": "Task-Specific Data Selection for Instruction Tuning via Monosemantic Neuronal Activations",
    "volume": "poster",
    "abstract": "Instruction tuning improves the ability of large language models (LLMs) to follow diverse human instructions, but achieving strong performance on specific target tasks remains challenging. A critical bottleneck is selecting the most relevant data to maximize task-specific performance. Existing data selection approaches include unstable influence-based methods and more stable distribution alignment methods, the latter of which critically rely on the underlying sample representation. In practice, most distribution alignment methods, from shallow features (e.g., BM25) to neural embeddings (e.g., BGE, LLM2Vec), may fail to capture how the model internally processes samples. To bridge this gap, we adopt a model-centric strategy in which each sample is represented by its neuronal activation pattern in the model, directly reflecting internal computation. However, directly using raw neuron activations leads to spurious similarity between unrelated samples due to neuron polysemanticity, where a single neuron may respond to multiple, unrelated concepts. To address this, we employ sparse autoencoders to disentangle polysemantic activations into sparse, monosemantic representations, and introduce a dedicated similarity metric for this space to better identify task-relevant data. Comprehensive experiments across multiple instruction datasets, models, tasks, and selection ratios show that our approach consistently outperforms existing data selection baselines in both stability and task-specific performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fvho95EtPu": {
    "title": "SD-KDE: Score-Debiased Kernel Density Estimation",
    "volume": "poster",
    "abstract": "We propose a method for density estimation that leverages an estimated score function to debias kernel density estimation (SD-KDE). In our approach, each data point is adjusted by taking a single step along the score function with a specific choice of step size, followed by standard KDE with a modified bandwidth. The step size and modified bandwidth are chosen to remove the leading order bias in the KDE, improving the asymptotic convergence rate. Our experiments on synthetic tasks in 1D, 2D and on MNIST, demonstrate that our proposed SD-KDE method significantly reduces the mean integrated squared error compared to the standard Silverman KDE, even with noisy estimates in the score function. These results underscore the potential of integrating score-based corrections into nonparametric density estimation",
    "checked": false,
    "id": "8d6304b4104b63cce2a5cd321e1d814c9f829651",
    "semantic_title": "score-debiased kernel density estimation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ifavjJaKQa": {
    "title": "Temporal Logic-Based Multi-Vehicle Backdoor Attacks against Offline RL Agents in End-to-end Autonomous Driving",
    "volume": "poster",
    "abstract": "Assessing the safety of autonomous driving (AD) systems against security threats, particularly backdoor attacks, is a stepping stone for real-world deployment. However, existing works mainly focus on pixel-level triggers which are impractical to deploy in the real world. We address this gap by introducing a novel backdoor attack against the end-to-end AD systems that leverage one or more other vehicles' trajectories as triggers. To generate precise trigger trajectories, we first use temporal logic (TL) specifications to define the behaviors of attacker vehicles. Configurable behavior models are then used to generate these trajectories, which are quantitatively evaluated and iteratively refined based on the TL specifications. We further develop a negative training strategy by incorporating patch trajectories that are similar to triggers but are designated not to activate the backdoor. It enhances the stealthiness of the attack and refines the system's responses to trigger scenarios. Through extensive experiments on 5 offline reinforcement learning (RL) driving agents with 6 trigger patterns and target actions combinations, we demonstrate the flexibility and effectiveness of our proposed attack, showing the under-exploration of existing end-to-end AD systems' vulnerabilities to such trajectory-based backdoor attacks. Videos of our attack are available at: https://sites.google.com/view/tlbackdoor/home",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TUh4GDposM": {
    "title": "AANet: Virtual Screening under Structural Uncertainty via Alignment and Aggregation",
    "volume": "poster",
    "abstract": "Virtual screening (VS) is a critical component of modern drug discovery, yet most existing methods—whether physics-based or deep learning-based—are developed around *holo* protein structures with known ligand-bound pockets. Consequently, their performance degrades significantly on *apo* or predicted structures such as those from AlphaFold2, which are more representative of real-world early-stage drug discovery, where pocket information is often missing. In this paper, we introduce an alignment-and-aggregation framework to enable accurate virtual screening under structural uncertainty. Our method comprises two core components: (1) a tri-modal contrastive learning module that aligns representations of the ligand, the *holo* pocket, and cavities detected from structures, thereby enhancing robustness to pocket localization error; and (2) a cross-attention based adapter for dynamically aggregating candidate binding sites, enabling the model to learn from activity data even without precise pocket annotations. We evaluated our method on a newly curated benchmark of *apo* structures, where it significantly outperforms state-of-the-art methods in blind apo setting, improving the early enrichment factor (EF1\\%) from 11.75 to 37.19. Notably, it also maintains strong performance on *holo* structures. These results demonstrate the promise of our approach in advancing first-in-class drug discovery, particularly in scenarios lacking experimentally resolved protein-ligand complexes. Our implementation is publicly available at [https://github.com/Wiley-Z/AANet](https://github.com/Wiley-Z/AANet)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PdRf0O7baQ": {
    "title": "Scaffolding Dexterous Manipulation with Vision-Language Models",
    "volume": "poster",
    "abstract": "Dexterous robotic hands are essential for performing complex manipulation tasks, yet remain difficult to train due to the challenges of demonstration collection and high-dimensional control. While reinforcement learning (RL) can alleviate the data bottleneck by generating experience in simulation, it typically relies on carefully designed, task-specific reward functions, which hinder scalability and generalization. Thus, contemporary works in dexterous manipulation have often bootstrapped from reference trajectories. These trajectories specify target hand poses that guide the exploration of RL policies and object poses that enable dense, task-agnostic rewards. However, sourcing suitable trajectories---particularly for dexterous hands---remains a significant challenge. Yet, the precise details in explicit reference trajectories are often unnecessary, as RL ultimately refines the motion. Our key insight is that modern vision-language models (VLMs) already encode the commonsense spatial and semantic knowledge needed to specify tasks and guide exploration effectively. Given a task description (e.g., \"open the cabinet\") and a visual scene, our method uses an off-the-shelf VLM to first identify task-relevant keypoints (e.g., handles, buttons) and then synthesize 3D trajectories for hand motion and object motion. Subsequently, we train a low-level residual RL policy in simulation to track these coarse trajectories or ``scaffolds'' with high fidelity. Across a number of simulated tasks involving articulated objects and semantic understanding, we demonstrate that our method is able to learn robust dexterous manipulation policies. Moreover, we showcase that our method transfers to real-world robotic hands without any human demonstrations or handcrafted rewards",
    "checked": true,
    "id": "507dd245030099afc1f5a416ac5a9547837e4cbe",
    "semantic_title": "scaffolding dexterous manipulation with vision-language models",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=I1C0a01BZu": {
    "title": "Tree-Guided Diffusion Planner",
    "volume": "poster",
    "abstract": "Planning with pretrained diffusion models has emerged as a promising approach for solving test-time guided control problems. Standard gradient guidance typically performs optimally under convex, differentiable reward landscapes. However, it shows substantially reduced effectiveness in real-world scenarios with non-convex objectives, non-differentiable constraints, and multi-reward structures. Furthermore, recent supervised planning approaches require task-specific training or value estimators, which limits test-time flexibility and zero-shot generalization. We propose a Tree-guided Diffusion Planner (TDP), a zero-shot test-time planning framework that balances exploration and exploitation through structured trajectory generation. We frame test-time planning as a tree search problem using a bi-level sampling process: (1) diverse parent trajectories are produced via training-free particle guidance to encourage broad exploration, and (2) sub-trajectories are refined through fast conditional denoising guided by task objectives. TDP addresses the limitations of gradient guidance by exploring diverse trajectory regions and harnessing gradient information across this expanded solution space using only pretrained models and test-time reward signals. We evaluate TDP on three diverse tasks: maze gold-picking, robot arm block manipulation, and AntMaze multi-goal exploration. TDP consistently outperforms state-of-the-art approaches on all tasks. The project page can be found at: https://tree-diffusion-planner.github.io",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EZfDHprhZM": {
    "title": "FlashMoE: Fast Distributed MoE in a Single Kernel",
    "volume": "poster",
    "abstract": "The computational sparsity of Mixture-of-Experts (MoE) models enables sub-linear growth in compute cost as model size increases, thus offering a scalable path to training massive neural networks. However, existing implementations suffer from low GPU utilization, significant latency overhead, and a fundamental inability to leverage task locality, primarily due to CPU-managed scheduling, host-initiated communication, and frequent kernel launches. To overcome these limitations, we develop FlashMoE, a fully GPU-resident MoE operator that fuses expert computation and inter-GPU communication into a single persistent GPU kernel. FlashMoE enables fine-grained pipelining of dispatch, compute, and combine phases, eliminating launch overheads and reducing idle gaps. Unlike existing work, FlashMoE obviates bulk-synchronous collectives for one-sided, device-initiated, inter-GPU (R)DMA transfers, thus unlocking payload efficiency, where we eliminate bloated or redundant network payloads in sparsely activated layers. When evaluated on an 8-H100 GPU node with MoE models having up to 128 experts and 16K token sequences, FlashMoE achieves up to 9× higher GPU utilization, 6× lower latency, 5.7× higher throughput, and 4× better overlap efficiency compared to state-of-the-art baselines—despite using FP32 while baselines use FP16. FlashMoE shows that principled GPU kernel-hardware co-design is key to unlocking the performance ceiling of large-scale distributed ML. We provide code at https://github.com/osayamenja/FlashMoE",
    "checked": true,
    "id": "28d52f59250c6707f4e49c747083a23cf8b9e252",
    "semantic_title": "flashmoe: fast distributed moe in a single kernel",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=XjayhGBraW": {
    "title": "SPINT: Spatial Permutation-Invariant Neural Transformer for Consistent Intracortical Motor Decoding",
    "volume": "poster",
    "abstract": "Intracortical Brain-Computer Interfaces (iBCI) decode behavior from neural population activity to restore motor functions and communication abilities in individuals with motor impairments. A central challenge for long-term iBCI deployment is the nonstationarity of neural recordings, where the composition and tuning profiles of the recorded populations are unstable across recording sessions. Existing approaches attempt to address this issue by explicit alignment techniques; however, they rely on fixed neural identities and require test-time labels or parameter updates, limiting their generalization across sessions and imposing additional computational burden during deployment. In this work, we address the problem of cross-session nonstationarity in long-term iBCI systems and introduce SPINT - a Spatial Permutation-Invariant Neural Transformer framework for behavioral decoding that operates directly on unordered sets of neural units. Central to our approach is a novel context-dependent positional embedding scheme that dynamically infers unit-specific identities, enabling flexible generalization across recording sessions. SPINT supports inference on variable-size populations and allows few-shot, gradient-free adaptation using a small amount of unlabeled data from the test session. We evaluate SPINT on three multi-session datasets from the FALCON Benchmark, covering continuous motor decoding tasks in human and non-human primates. SPINT demonstrates robust cross-session generalization, outperforming existing zero-shot and few-shot unsupervised baselines while eliminating the need for test-time alignment and fine-tuning. Our work contributes an initial step toward a robust and scalable neural decoding framework for long-term iBCI applications",
    "checked": true,
    "id": "0aae2c60cb6c332ae96cd0dca0afb9bc8df52049",
    "semantic_title": "spint: spatial permutation-invariant neural transformer for consistent intracortical motor decoding",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OBaK9JSbHk": {
    "title": "On Evaluating LLM Alignment by Evaluating LLMs as Judges",
    "volume": "poster",
    "abstract": "Alignment with human preferences is an important evaluation aspect of LLMs, requiring them to be helpful, honest, safe, and to precisely follow human instructions. Evaluating large language models' (LLMs) alignment typically involves directly assessing their open-ended responses, requiring human annotators or strong LLM judges. Conversely, LLMs themselves have also been extensively evaluated as judges for assessing alignment. In this work, we examine the relationship between LLMs' generation and evaluation capabilities in aligning with human preferences. To this end, we first conduct a comprehensive analysis of the generation-evaluation consistency (GE-consistency) among various LLMs, revealing a strong correlation between their generation and evaluation capabilities when evaluated by a strong LLM preference oracle (GPT-4o). Utilizing this finding, we propose a benchmarking paradigm that measures LLM alignment with human preferences without directly evaluating their generated outputs, instead assessing LLMs in their role as evaluators. Our evaluation shows that our proposed benchmark, AlignEval, matches or surpasses widely used automatic LLM evaluation benchmarks, such as AlpacaEval and Arena-Hard, in capturing human preferences when ranking LLMs. Our study offers valuable insights into the connection between LLMs' generation and evaluation capabilities, and introduces a benchmark that assesses alignment without directly evaluating model outputs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5jneOToPou": {
    "title": "Encoder-Decoder Diffusion Language Models for Efficient Training and Inference",
    "volume": "poster",
    "abstract": "Discrete diffusion models enable parallel token sampling for faster inference than autoregressive approaches. However, prior diffusion models use a decoder-only architecture, which requires sampling algorithms that invoke the full network at every denoising step and incur high computational cost. Our key insight is that discrete diffusion models perform two types of computation: 1) representing clean tokens and 2) denoising corrupted tokens, which enables us to use separate modules for each task. We propose an encoder-decoder architecture to accelerate discrete diffusion inference, which relies on an encoder to represent clean tokens and a lightweight decoder to iteratively refine a noised sequence. We also show that this architecture enables faster training of block diffusion models, which partition sequences into blocks for better quality and are commonly used in diffusion language model inference. We introduce a framework for **E**fficient **E**ncoder-**D**ecoder **D**iffusion (E2D2), consisting of an architecture with specialized training and sampling algorithms, and we show that E2D2 achieves superior trade-offs between generation quality and inference throughput on summarization, translation, and mathematical reasoning tasks",
    "checked": true,
    "id": "e78156d2ceaa43586f86cb2ba8558255d900ae1b",
    "semantic_title": "encoder-decoder diffusion language models for efficient training and inference",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z9xyREqxzq": {
    "title": "Robust Egocentric Referring Video Object Segmentation via Dual-Modal Causal Intervention",
    "volume": "poster",
    "abstract": "Egocentric Referring Video Object Segmentation (Ego-RVOS) aims to segment the specific object actively involved in a human action, as described by a language query, within first-person videos. This task is critical for understanding egocentric human behavior. However, achieving such segmentation robustly is challenging due to ambiguities inherent in egocentric videos and biases present in training data. Consequently, existing methods often struggle, learning spurious correlations from skewed object-action pairings in datasets and fundamental visual confounding factors of the egocentric perspective, such as rapid motion and frequent occlusions. To address these limitations, we introduce Causal Ego-REferring Segmentation (CERES), a plug-in causal framework that adapts strong, pre-trained RVOS backbones to the egocentric domain. CERES implements dual-modal causal intervention: applying backdoor adjustment principles to counteract language representation biases learned from dataset statistics, and leveraging front-door adjustment concepts to address visual confounding by intelligently integrating semantic visual features with geometric depth information guided by causal principles, creating representations more robust to egocentric distortions. Extensive experiments demonstrate that CERES achieves state-of-the-art performance on Ego-RVOS benchmarks, highlighting the potential of applying causal reasoning to build more reliable models for broader egocentric video understanding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pQ8DeHXKMh": {
    "title": "Are Large Language Models Sensitive to the Motives Behind Communication?",
    "volume": "poster",
    "abstract": "Human communication is $\\textit{motivated}$: people speak, write, and create content with a particular communicative intent in mind. As a result, information that large language models (LLMs) and AI agents process is inherently framed by humans' intentions and incentives. People are adept at navigating such nuanced information: we routinely identify benevolent or self-serving motives in order to decide what statements to trust. For LLMs to be effective in the real world, they too must critically evaluate content by factoring in the motivations of the source---for instance, weighing the credibility of claims made in a sales pitch. In this paper, we undertake a comprehensive study of whether LLMs have this capacity for $\\textit{motivational vigilance}$. We first employ controlled experiments from cognitive science to verify that LLMs' behavior is consistent with rational models of learning from motivated testimony, and find they successfully discount information from biased sources in a human-like manner. We then extend our evaluation to sponsored online adverts, a more naturalistic reflection of LLM agents' information ecosystems. In these settings, we find that LLMs' inferences do not track the rational models' predictions nearly as closely---partly due to additional information that distracts them from vigilance-relevant considerations. However, a simple steering intervention that boosts the salience of intentions and incentives substantially increases the correspondence between LLMs and the rational model. These results suggest that LLMs possess a basic sensitivity to the motivations of others, but generalizing to novel real-world settings will require further improvements to these models",
    "checked": true,
    "id": "35258b001814b39b53a79607844425510e39824a",
    "semantic_title": "are large language models sensitive to the motives behind communication?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iQoZv77o3g": {
    "title": "Predicting Functional Brain Connectivity with Context-Aware Deep Neural Networks",
    "volume": "poster",
    "abstract": "Spatial location and molecular interactions have long been linked to the connectivity patterns of neural circuits. Yet, at the macroscale of human brain networks, the interplay between spatial position, gene expression, and connectivity remains incompletely understood. Recent efforts to map the human transcriptome and connectome have yielded spatially resolved brain atlases, however modeling the relationship between high-dimensional transcriptomic data and connectivity while accounting for inherent spatial confounds presents a significant challenge. In this paper, we present the first deep learning approaches for predicting whole-brain functional connectivity from gene expression and regional spatial coordinates, including our proposed Spatiomolecular Transformer (SMT). SMT explicitly models biological context by tokenizing genes based on their transcription start site (TSS) order to capture multi-scale genomic organization, and incorporating regional 3D spatial location via a dedicated context [CLS] token within its multi-head self-attention mechanism. We rigorously benchmark context-aware neural networks, including SMT and a single-gene resolution Multilayer-Perceptron (MLP), to established rules-based and bilinear methods. Crucially, to ensure that learned relationships in any model are not mere artifacts of spatial proximity, we introduce novel spatiomolecular null maps, preserving both spatial and transcriptomic autocorrelation. Context-aware neural networks outperform linear methods, significantly exceed our stringent null shuffle models, and generalize across diverse connectomic datasets and parcellation resolutions. Together, these findings demonstrate a strong, predictable link between the spatial distributions of gene expression and functional brain network architecture, and establish a rigorously validated deep learning framework for decoding this relationship. Code to reproduce our results is available at: github.com/neuroinfolab/GeneEx2Conn",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UkR2zO5uww": {
    "title": "Real-Time Execution of Action Chunking Flow Policies",
    "volume": "poster",
    "abstract": "Modern AI systems, especially those interacting with the physical world, increasingly require real-time performance. However, the high latency of state-of-the-art generalist models, including recent vision-language-action models (VLAs), poses a significant challenge. While action chunking has enabled temporal consistency in high-frequency control tasks, it does not fully address the latency problem, leading to pauses or out-of-distribution jerky movements at chunk boundaries. This paper presents a novel inference-time algorithm that enables smooth asynchronous execution of action chunking policies. Our method, real-time chunking (RTC), is applicable to any diffusion- or flow-based VLA out of the box with no retraining. It generates the next action chunk while executing the current one, \"freezing\" actions guaranteed to execute and \"inpainting\" the rest. To test RTC, we introduce a new benchmark of 12 highly dynamic tasks in the Kinetix simulator, as well as evaluate 6 challenging real-world bimanual manipulation tasks. Results demonstrate that RTC is fast, performant, and uniquely robust to inference delay, significantly improving task throughput and enabling success in precise tasks --- such as lighting a match --- even in the presence of extreme latency",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ARJpQtLXfe": {
    "title": "Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models",
    "volume": "poster",
    "abstract": "Large Language Models (LLMs) have shown strong abilities in general language tasks, yet adapting them to specific domains remains a challenge. Current method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter training and suffers from catastrophic forgetting. Meanwhile, Retrieval-Augmented Generation (RAG) introduces substantial inference latency due to expensive nearest-neighbor searches and longer context. This paper introduces \\textit{Memory Decoder}, a plug-and-play pretrained memory that enables efficient domain adaptation without changing the original model's parameters. Memory Decoder employs a small transformer decoder that learns to imitate the behavior of an external non-parametric retriever. Once trained, Memory Decoder can be seamlessly integrated with any pretrained language model that shares the same tokenizer, requiring no model-specific modifications. Experimental results demonstrate that Memory Decoder enables effective adaptation of various Qwen and Llama models to three distinct specialized domains: biomedicine, finance, and law, reducing perplexity by an average of 6.17 points. Overall, Memory Decoder introduces a novel paradigm centered on a specially pretrained memory component designed for domain-specific adaptation. This memory architecture can be integrated in a plug-and-play manner, consistently enhancing performance across multiple models within the target domain",
    "checked": true,
    "id": "78ee2f9ecf1774d1777e54a3b86e1ac94ade06c7",
    "semantic_title": "memory decoder: a pretrained, plug-and-play memory for large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=95plu1Mo20": {
    "title": "Prompted Policy Search: Reinforcement Learning through Linguistic and Numerical Reasoning in LLMs",
    "volume": "poster",
    "abstract": "Reinforcement Learning (RL) traditionally relies on scalar reward signals, limiting its ability to leverage the rich semantic knowledge often available in real-world tasks. In contrast, humans learn efficiently by combining numerical feedback with language, prior knowledge, and common sense. We introduce Prompted Policy Search (ProPS), a novel RL method that unifies numerical and linguistic reasoning within a single framework. Unlike prior work that augments existing RL components with language, ProPS places a large language model (LLM) at the center of the policy optimization loop—directly proposing policy updates based on both reward feedback and natural language input. We show that LLMs can perform numerical optimization in-context, and that incorporating semantic signals, such as goals, constraints, and strategy hints can lead to more informed exploration and sample-efficient learning. ProPS is evaluated across 15 Gymnasium tasks, spanning classic control, Atari games, and MuJoCo environments, and compared to seven widely-adopted RL algorithms (e.g., PPO, SAC, TRPO). It outperforms all baselines on 8 out of 15 tasks and demonstrates substantial gains when provided with domain knowledge. These results highlight the potential of unifying semantics and numerics for transparent, generalizable, and human-aligned reinforcement learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2neULQUK3n": {
    "title": "Adversarial Robustness of Nonparametric Regression",
    "volume": "poster",
    "abstract": "In this paper, we investigate the adversarial robustness of nonparametric regression, a fundamental problem in machine learning, under the setting where an adversary can arbitrarily corrupt a subset of the input data. While the robustness of parametric regression has been extensively studied, its nonparametric counterpart remains largely unexplored. We characterize the adversarial robustness in nonparametric regression, assuming the regression function belongs to the second-order Sobolev space (i.e., it is square integrable up to its second derivative). The contribution of this paper is two-fold: (i) we establish a minimax lower bound on the estimation error, revealing a fundamental limit that no estimator can overcome, and (ii) we show that, perhaps surprisingly, the classical smoothing spline estimator, when properly regularized, exhibits robustness against adversarial corruption. These results imply that if $o(n)$ out of $n$ samples are corrupted, the estimation error of the smoothing spline vanishes as $n \\to \\infty$. On the other hand, when a constant fraction of the data is corrupted, no estimator can guarantee vanishing estimation error, implying the optimality of the smoothing spline in terms of maximum tolerable number of corrupted samples",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xk5d55Eu1i": {
    "title": "Training Robust Graph Neural Networks by Modeling Noise Dependencies",
    "volume": "poster",
    "abstract": "In real-world applications, node features in graphs often contain noise from various sources, leading to significant performance degradation in GNNs. Although several methods have been developed to enhance robustness, they rely on the unrealistic assumption that noise in node features is independent of the graph structure and node labels, thereby limiting their applicability. To this end, we introduce a more realistic noise scenario, dependency-aware noise on graphs (DANG), where noise in node features create a chain of noise dependencies that propagates to the graph structure and node labels. We propose a novel robust GNN, DA-GNN, which captures the causal relationships among variables in the data generating process (DGP) of DANG using variational inference. In addition, we present new benchmark datasets that simulate DANG in real-world applications, enabling more practical research on robust GNNs. Extensive experiments demonstrate that DA-GNN consistently outperforms existing baselines across various noise scenarios, including both DANG and conventional noise models commonly considered in this field",
    "checked": true,
    "id": "fb53f7623eea68eba8c8f6bac92d9fb5b86ddd26",
    "semantic_title": "training robust graph neural networks by modeling noise dependencies",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RBiyFxwGJm": {
    "title": "Learning-Augmented Streaming Algorithms for Correlation Clustering",
    "volume": "poster",
    "abstract": "We study streaming algorithms for Correlation Clustering. Given a graph as an arbitrary-order stream of edges, with each edge labeled as positive or negative, the goal is to partition the vertices into disjoint clusters, such that the number of disagreements is minimized. In this paper, we give the first learning-augmented streaming algorithms for the problem on both complete and general graphs, improving the best-known space-approximation tradeoffs. Based on the works of Cambus et al. (SODA'24) and Ahn et al. (ICML'15), our algorithms use the predictions of pairwise distances between vertices provided by a predictor. For complete graphs, our algorithm achieves a better-than-$3$ approximation under good prediction quality, while using $\\tilde{O}(n)$ total space. For general graphs, our algorithm achieves an $O(\\log |E^-|)$ approximation under good prediction quality using $\\tilde{O}(n)$ total space, improving the best-known non-learning algorithm in terms of space efficiency. Experimental results on synthetic and real-world datasets demonstrate the superiority of our proposed algorithms over their non-learning counterparts",
    "checked": true,
    "id": "b7ce88da6d7f43dc0927aa68fccc5e1cc21c8bdf",
    "semantic_title": "learning-augmented streaming algorithms for correlation clustering",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Qe7AGO3Eq": {
    "title": "Multipole Attention for Efficient Long Context Reasoning",
    "volume": "poster",
    "abstract": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on complex problem-solving tasks. While these models have attained high accuracy by leveraging additional computation at test time, they need to generate long chain-of-thought reasoning in order to think before answering, which requires generating thousands of tokens. While sparse attention methods can help reduce the KV cache pressure induced by this long autoregressive reasoning, these methods can introduce errors which disrupt the reasoning process. Our work addresses these challenges by introducing Multipole Attention, which accelerates autoregressive reasoning by only computing exact attention for the most important tokens, while maintaining approximate representations for the remaining tokens. Our method first performs clustering to group together semantically similar key vectors, and then uses the cluster centroids both to identify important key vectors and to approximate the remaining key vectors in order to retain high accuracy. Additionally, in order to accelerate long generation tasks, we design a fast cluster update process to quickly re-cluster the input and previously generated tokens, thereby allowing for accelerating attention to the previous output tokens. We evaluate our method using emerging LRMs such as Qwen-8B and Deepseek-R1-Distil-Qwen2.5-14B, demonstrating that our approach can maintain accuracy on complex reasoning tasks even with aggressive attention sparsity settings. We also provide kernel implementations to demonstrate the practical efficiency gains from our method, achieving up to 4.5$\\times$ speedup for attention in long-context reasoning applications",
    "checked": true,
    "id": "ffffd30d055f84dc27678fbf389e9375b22f7a76",
    "semantic_title": "multipole attention for efficient long context reasoning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=qwwPhjDea0": {
    "title": "Transforming Generic Coder LLMs to Effective Binary Code Embedding Models for Similarity Detection",
    "volume": "poster",
    "abstract": "Cybersecurity and software research have crossed paths with modern deep learning research for a few years. The power of large language models (LLMs) in particular has intrigued us to apply them to understanding binary code. In this paper, we investigate some of the many ways LLMs can be applied to binary code similarity detection, as it is a significantly more difficult task compared to source code similarity detection due to the sparsity of information and less meaningful syntax. It also has great practical implications, such as vulnerability and malware detection. We find that pretrained LLMs are mostly capable of detecting similar binary code, even with a zero-shot setting. Our main contributions and findings are to provide several supervised fine-tuning methods that, when combined, significantly surpass zero-shot LLMs and state-of-the-art binary code similarity detection methods. Specifically, we up-train the model through data augmentation, translation-style causal learning, LLM2Vec, and cumulative GTE loss. With a complete ablation study, we show that our training method can transform a generic language model into a powerful binary similarity expert, and is also robust and general enough for cross-optimization, cross-architecture, and cross-obfuscation detection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qPQUrjiA0q": {
    "title": "Rollout Roulette: A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods",
    "volume": "poster",
    "abstract": "Large language models (LLMs) have achieved significant performance gains via scaling up model sizes and/or data. However, recent evidence suggests diminishing returns from such approaches, motivating a pivot to scaling test-time compute. Existing deterministic inference-time scaling methods, usually with reward models, cast the task as a search problem, but suffer from a key limitation: early pruning. Due to inherently imperfect reward models, promising trajectories may be discarded prematurely, leading to suboptimal performance. We propose a novel inference-time scaling approach by adapting particle-based Monte Carlo methods. Our method maintains a diverse set of candidates and robustly balances exploration and exploitation. Our empirical evaluation demonstrates that our particle filtering methods have a 4--16x better scaling rate over deterministic search counterparts on both various challenging mathematical and more general reasoning tasks. Using our approach, we show that Qwen2.5-Math-1.5B-Instruct surpasses GPT-4o accuracy in only 4 rollouts, while Qwen2.5-Math-7B-Instruct scales to o1 level accuracy in only 32 rollouts. Our work not only presents an effective method to inference-time scaling, but also connects rich literature in probabilistic inference with inference-time scaling of LLMs to develop more robust algorithms in future work",
    "checked": true,
    "id": "38af83f87c61177aa96eeba8a07a1e0922241532",
    "semantic_title": "rollout roulette: a probabilistic inference approach to inference-time scaling of llms using particle-based monte carlo methods",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=3oQDkmW72a": {
    "title": "Creativity or Brute Force? Using Brainteasers as a Window into the Problem-Solving Abilities of Large Language Models",
    "volume": "poster",
    "abstract": "Accuracy remains a standard metric for evaluating AI systems, but it offers limited insight into how models arrive at their solutions. In this work, we introduce a benchmark based on brainteasers written in long narrative form to probe more deeply into the types of reasoning strategies that models use. Brainteasers are well-suited for this goal because they can be solved with multiple approaches, such as a few-step solution that uses a creative insight or a longer solution that uses more brute force. We investigate large language models (LLMs) across multiple layers of reasoning, focusing not only on correctness but also on the quality and creativity of their solutions. We investigate many aspects of the reasoning process: (1) semantic parsing of the brainteasers into precise mathematical competition style formats; (2) self-correcting solutions based on gold solutions; (3) producing step-by-step sketches of solutions; and (4) making use of hints. We find that LLMs are in many cases able to find creative, insightful solutions to brainteasers, suggesting that they capture some of the capacities needed to solve novel problems in creative ways. Nonetheless, there also remain situations where they rely on brute force despite the availability of more efficient, creative solutions, highlighting a potential direction for improvement in the reasoning abilities of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RIL1vOuZOC": {
    "title": "‌Navigating the MIL Trade-Off: Flexible Pooling for Whole Slide Image Classification",
    "volume": "poster",
    "abstract": "Multiple Instance Learning (MIL) is a standard weakly supervised approach for Whole Slide Image (WSI) classification, where performance hinges on both feature representation and MIL pooling strategies. Recent research has predominantly focused on Transformer-based architectures adapted for WSIs. However, we argue that this trend faces a fundamental limitation: data scarcity. In typical settings, Transformer models yield only marginal gains without access to large-scale datasets—resources that are virtually inaccessible to all but a few well-funded research labs. Motivated by this, we revisit simple, non-attention MIL with unsupervised slide features and analyze temperature-$\\beta$-controlled log-sum-exp (LSE) pooling. For slides partitioned into $N$ patches, we theoretically show that LSE has a smooth transition at a critical $\\beta_{\\mathrm{crit}}=\\mathcal{O}(\\log N)$ threshold, interpolating between mean-like aggregation (stable, better generalization but less sensitive) and max-like aggregation (more sensitive but looser generalization bounds). Grounded in this analysis, we introduce Maxsoft—a novel MIL pooling function that enables flexible control over this trade-off, allowing adaptation to specific tasks and datasets. To further tackle real-world deployment challenges such as specimen heterogeneity, we propose PerPatch augmentation—a simple yet effective technique that enhances model robustness. Empirically, Maxsoft achieves state-of-the-art performance in low-data regimes across four major benchmarks (CAMELYON16, CAMELYON17, TCGA-Lung, and SICAP-MIL), often matching or surpassing large-scale foundation models. When combined with PerPatch augmentation, this performance is further improved through increased robustness. Code is available at \\href{https://github.com/jafarinia/maxsoft}{\\texttt{https://github.com/jafarinia/maxsoft}}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rgajBQxbC7": {
    "title": "ESCORT: Efficient Stein-variational and Sliced Consistency-Optimized Temporal Belief Representation for POMDPs",
    "volume": "poster",
    "abstract": "In Partially Observable Markov Decision Processes (POMDPs), maintaining and updating belief distributions over possible underlying states provides a principled way to summarize action-observation history for effective decision-making under uncertainty. As environments grow more realistic, belief distributions develop complexity that standard mathematical models cannot accurately capture, creating a fundamental challenge in maintaining representational accuracy. Despite advances in deep learning and probabilistic modeling, existing POMDP belief approximation methods fail to accurately represent complex uncertainty structures such as high-dimensional, multi-modal belief distributions, resulting in estimation errors that lead to suboptimal agent behaviors. To address this challenge, we present ESCORT (Efficient Stein-variational and sliced Consistency-Optimized Representation for Temporal beliefs), a particle-based framework for capturing complex, multi-modal distributions in high-dimensional belief spaces. ESCORT extends SVGD with two key innovations: correlation-aware projections that model dependencies between state dimensions, and temporal consistency constraints that stabilize updates while preserving correlation structures. This approach retains SVGD's attractive-repulsive particle dynamics while enabling accurate modeling of intricate correlation patterns. Unlike particle filters prone to degeneracy or parametric methods with fixed representational capacity, ESCORT dynamically adapts to belief landscape complexity without resampling or restrictive distributional assumptions. We demonstrate ESCORT's effectiveness through extensive evaluations on both POMDP domains and synthetic multi-modal distributions of varying dimensionality, where it consistently outperforms state-of-the-art methods in terms of belief approximation accuracy and downstream decision quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3aFwsZxM5H": {
    "title": "Structural Causal Bandits under Markov Equivalence",
    "volume": "poster",
    "abstract": "In decision-making processes, an intelligent agent with causal knowledge can optimize action spaces to avoid unnecessary exploration. A *structural causal bandit* framework provides guidance on how to prune actions that are unable to maximize reward by leveraging prior knowledge of the underlying causal structure among actions. A key assumption of this framework is that the agent has access to a fully-specified causal diagram representing the target system. In this paper, we extend the structural causal bandits to scenarios where the agent leverages a Markov equivalence class. In such cases, the causal structure is provided to the agent in the form of a *partial ancestral graph* (PAG). We propose a generalized framework for identifying potentially optimal actions within this graph structure, thereby broadening the applicability of structural causal bandits",
    "checked": false,
    "id": "06dd2f1ee45857c1d3f39ab710fb191593809b8b",
    "semantic_title": "towards bounding causal effects under markov equivalence",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=avZ01E4aYt": {
    "title": "Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation",
    "volume": "poster",
    "abstract": "Scenario-based testing is essential for validating the performance of autonomous driving (AD) systems. However, such testing is limited by the scarcity of long-tailed, safety-critical scenarios in existing datasets collected in the real world. To tackle the data issue, we propose the Adv-BMT framework, which augments real-world scenarios with diverse and realistic adversarial interactions. The core component of Adv-BMT is a bidirectional motion transformer (BMT) model to perform inverse traffic motion predictions, which takes the last frame of the scenario as input and reconstruct the traffic in the inverse of chronological order, till the initial time step. The Adv-BMT framework is a two-stage pipeline: it first conducts adversarial initializations and then inverse motion predictions. Different from previous work, we do not need any collision data for pretraining and are still able to generate realistic and diverse collision interactions. Our experimental results validate the quality of generated collision scenarios by Adv-BMT: training in our augmented dataset would reduce episode collision rates by 20\\% compared to previous work. The code will be made available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hNjSvgFTbp": {
    "title": "Statistical Analysis of an Adversarial Bayesian Weak Supervision Method",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FGMBxzpgis": {
    "title": "EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric Human Data",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "bc47eb8fa36ca097a76ab4d2c566aa003617bc99",
    "semantic_title": "egobridge: domain adaptation for generalizable imitation from egocentric human data",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Wic0OgYsgy": {
    "title": "Improving Monte Carlo Tree Search for Symbolic Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=35Rum4a6tC": {
    "title": "Learn2Mix: Training Neural Networks Using Adaptive Data Integration",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "26a93d5c0b21f04c18ed0127c1adde5ad1281e68",
    "semantic_title": "learn2mix: training neural networks using adaptive data integration",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=1si0Vq4O91": {
    "title": "Identifying multi-compartment Hodgkin-Huxley models with high-density extracellular voltage recordings",
    "volume": "poster",
    "abstract": "Multi-compartment Hodgkin-Huxley models are biophysical models of how electrical signals propagate throughout a neuron, and they form the basis of our knowledge of neural computation at the cellular level. However, these models have many free parameters that must be estimated for each cell, and existing fitting methods rely on intracellular voltage measurements that are highly challenging to obtain in-vivo. Recent advances in neural recording technology with high-density probes and arrays enable dense sampling of extracellular voltage from many sites surrounding a neuron, allowing indirect measurement of many compartments of a cell simultaneously. Here, we propose a method for inferring the underlying membrane voltage, biophysical parameters, and the neuron's position relative to the probe, using extracellular measurements alone. We use an Extended Kalman Filter to infer membrane voltage and channel states using efficient, differentiable simulators. Then, we learn the model parameters by maximizing the marginal likelihood using gradient-based methods. We demonstrate the performance of this approach using simulated data and real neuron morphologies",
    "checked": true,
    "id": "7b7e3188125d7ac3f11bb0efce2e1bd18ca4827f",
    "semantic_title": "identifying multi-compartment hodgkin-huxley models with high-density extracellular voltage recordings",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mzlwDAQkgJ": {
    "title": "Behavior Injection: Preparing Language Models for Reinforcement Learning",
    "volume": "poster",
    "abstract": "Reinforcement learning (RL) has emerged as a powerful post-training technique to incentivize the reasoning ability of large language models (LLMs). However, LLMs can respond very inconsistently to RL finetuning: some show substantial performance gains, while others plateau or even degrade. To understand this divergence, we analyze the per-step influence of the RL objective and identify two key conditions for effective post-training: (1) RL-informative rollout accuracy, and (2) strong data co-influence, which quantifies how much the training data affects performance on other samples. Guided by these insights, we propose behavior injection, a task-agnostic data augmentation scheme applied prior to RL. Behavior injection enriches the supervised finetuning (SFT) data by seeding exploratory and exploitative behaviors, effectively making the model more RL-ready. We evaluate our method across two reasoning benchmarks with multiple base models. The results demonstrate that our theoretically motivated augmentation can significantly increase the performance gain from RL over the pre-RL model",
    "checked": true,
    "id": "7cab61a4d24a6e10abaadca758824e078e790187",
    "semantic_title": "behavior injection: preparing language models for reinforcement learning",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=lQYNmlTwtc": {
    "title": "A Closer Look at TabPFN v2: Understanding Its Strengths and Extending Its Capabilities",
    "volume": "poster",
    "abstract": "Tabular datasets are inherently heterogeneous, presenting significant challenges for developing pre-trained foundation models. The recently introduced transformer-based Tabular Prior-data Fitted Network v2 (TabPFN v2) achieves unprecedented *in-context learning* performance across diverse downstream datasets, marking a pivotal advancement in tabular foundation models. In this paper, we take a closer look at TabPFN v2 to examine how it effectively handles heterogeneity and achieves high predictive accuracy, and to explore how its limitations in high-dimensional, many-category, and large-scale tasks can be mitigated. We find that TabPFN v2 can infer attribute relationships even when provided with randomized attribute token inputs, eliminating the need to explicitly learn dataset-specific attribute embeddings to address heterogeneity. We further show that TabPFN v2 can be transformed into a feature extractor, revealing its ability to construct a highly separable feature space for accurate predictions. Lastly, we demonstrate that TabPFN v2's limitations can be addressed through a test-time divide-and-conquer strategy, enabling scalable inference without requiring re-training. By uncovering the mechanisms behind TabPFN v2's success and introducing strategies to extend its applicability, this study offers key insights into the design of future tabular foundation models",
    "checked": true,
    "id": "00e90ef5fff477eed81a2b2742e9b96f57d58cf1",
    "semantic_title": "a closer look at tabpfn v2: understanding its strengths and extending its capabilities",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=fnkkY7UGRl": {
    "title": "Test-Time Scaling of Diffusion Models via Noise Trajectory Search",
    "volume": "poster",
    "abstract": "The iterative and stochastic nature of diffusion models enables *test-time scaling*, whereby spending additional compute during denoising generates higher-fidelity samples. Increasing the number of denoising steps is the primary scaling axis, but this yields quickly diminishing returns. Instead optimizing the *noise trajectory*—the sequence of injected noise vectors—is promising, as the specific noise realizations critically affect sample quality; but this is challenging due to a high-dimensional search space, complex noise-outcome interactions, and costly trajectory evaluations. We address this by first casting diffusion as a Markov Decision Process (MDP) with a terminal reward, showing tree-search methods such as Monte Carlo tree search (MCTS) to be meaningful but impractical. To balance performance and efficiency, we then resort to a relaxation of MDP, where we view denoising as a sequence of independent *contextual bandits*. This allows us to introduce an $\\epsilon$-greedy search algorithm that *globally explores* at extreme timesteps and *locally exploits* during the intermediate steps where de-mixing occurs. Experiments on EDM and Stable Diffusion reveal state-of-the-art scores for class-conditioned/text-to-image generation, exceeding baselines by up to $164$% and matching/exceeding MCTS performance. To our knowledge, this is the first practical method for test-time noise *trajectory* optimization of *arbitrary (non-differentiable)* rewards",
    "checked": true,
    "id": "a0129f7b3f16d92f30f53811a57dc0c61dcfd281",
    "semantic_title": "test-time scaling of diffusion models via noise trajectory search",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=5UMRGlcatB": {
    "title": "Consistency Conditions for Differentiable Surrogate Losses",
    "volume": "poster",
    "abstract": "The statistical consistency of surrogate losses for discrete prediction tasks is often checked using the condition of calibration. However, directly verifying calibration can be arduous. Recent work shows that for polyhedral surrogates, a less arduous condition, indirect elicitation (IE), is still equivalent to calibration. We give the first results of this type for non-polyhedral surrogates, specifically the class of convex differentiable losses. We first prove that under mild conditions, IE and calibration are equivalent for one-dimensional losses in this class. We construct a counter-example that shows that this equivalence fails in higher dimensions. This motivates the introduction of strong IE, a strengthened form of IE that is equally easy to verify. We establish that strong IE implies calibration for differentiable surrogates and is both necessary and sufficient for strongly convex, differentiable surrogates. Finally, we apply these results to a range of problems to demonstrate the power of IE and strong IE for designing and analyzing consistent differentiable surrogates",
    "checked": true,
    "id": "1798baeb0bc0eb584bdef3af1d6a6f9d8f581f42",
    "semantic_title": "consistency conditions for differentiable surrogate losses",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YFZOIra4OE": {
    "title": "Meta-D2AG: Causal Graph Learning with Interventional Dynamic Data",
    "volume": "poster",
    "abstract": "Causal discovery in the form of a directed acyclic graph (DAG) for dynamic time series data has been widely studied in various applications. Much of the existing work has focused on observational, offline, and/or stationary settings. In this work, we propose a dynamic DAG discovery algorithm, Meta-D$^2$AG, based on online meta-learning. Meta-D$^2$AG is designed to learn dynamic DAG structures from potentially nonlinear and non-stationary times series datasets, accounting for changes in both parameters and graph structures. Notably, Meta-D$^2$AG explicitly treats data collected at different time points with distribution shifts as distinct domains, which is assumed to occur as a result of external interventions. Moreover, Meta-D$^2$AG contains a new online meta-learning framework to take advantage of the temporal transition among existing domains such that it can quickly adapt to new domains with few measurements. A first-order optimization approach is utilized to efficiently solve the meta-learning framework, and theoretical analysis establishes the identifiability conditions and the convergence of the learning process. We demonstrate the promising performance of our method through better accuracy and sample efficiency on benchmark datasets against state-of-the-art baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D1PeGJtVEu": {
    "title": "ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning",
    "volume": "poster",
    "abstract": "Recent advances in large language models have been driven by reinforcement learning (RL)-style post-training, which improves reasoning by optimizing model outputs based on reward or preference signals. GRPO-style approaches implement this by using self-generated samples labeled by an outcome-based verifier. However, these methods depend heavily on the model's initial ability to produce positive samples. They primarily refine what the model already knows (distribution sharpening) rather than enabling the model to solve problems where it initially fails. This limitation is especially problematic in early-stage RL training and on challenging reasoning tasks, where positive samples are unlikely to be generated. To unlock reasoning ability in such settings, the model must explore new reasoning trajectories beyond its current output distribution. Such exploration requires access to sufficiently good positive samples to guide the learning. While expert demonstrations seem like a natural solution, we find that they are often ineffective in RL post-training. Instead, we identify two key properties of effective positive samples: they should (1) be likely under the current policy, and (2) increase the model's likelihood of predicting the correct answer. Based on these insights, we propose \\textbf{Self-Explanation Policy Optimization (ExPO)}—a simple and modular framework that generates such samples by conditioning on the ground-truth answer. ExPO enables efficient exploration and guides the model to produce reasoning trajectories more aligned with its policy than expert-written CoTs, while ensuring higher quality than its own (incorrect) samples. Experiments show that ExPO improves both learning efficiency and final performance on reasoning benchmarks, surpassing expert-demonstration-based methods in challenging settings such as MATH level-5, where the model initially struggles the most",
    "checked": true,
    "id": "3a0bde838c114f61fd8081c3c34a258eeb09c015",
    "semantic_title": "expo: unlocking hard reasoning with self-explanation-guided reinforcement learning",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=IUYAqr7qDA": {
    "title": "When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions",
    "volume": "poster",
    "abstract": "Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval (SMR). However, one query can correspond to multiple relevant moments in real-world applications. This makes the existing datasets and methods insufficient for video temporal grounding. By revisiting the gap between current MR tasks and real-world applications, we introduce a high-quality datasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new evaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists of 2,212 annotations covering 6,384 video segments. Building on existing efforts in MMR, we propose a framework called FlashMMR. Specifically, we propose a Multi-moment Post-verification module to refine the moment boundaries. We introduce constrained temporal adjustment and subsequently leverage a verification module to re-evaluate the candidate segments. Through this sophisticated filtering pipeline, low-confidence proposals are pruned, and robust multi-moment alignment is achieved. We retrain and evaluate 6 existing MR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings. Results show that QV-M$^2$ serves as an effective benchmark for training and evaluating MMR models, while FlashMMR provides a strong baseline. Specifically, on QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP, 2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method establish a foundation for advancing research in more realistic and challenging video temporal grounding scenarios. Code is released at https://github.com/Zhuo-Cao/QV-M2",
    "checked": true,
    "id": "f0fd83a03cf5a55f9d70f2d38afaf49a209bbf0b",
    "semantic_title": "when one moment isn't enough: multi-moment retrieval with cross-moment interactions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oPRNsj8AMk": {
    "title": "Generalizing Single-Frame Supervision to Event-Level Understanding for Video Anomaly Detection",
    "volume": "poster",
    "abstract": "Video Anomaly Detection (VAD) aims to identify abnormal frames from discrete events within video sequences. Existing VAD methods suffer from heavy annotation burdens in fully-supervised paradigm, insensitivity to subtle anomalies in semi-supervised paradigm, and vulnerability to noise in weakly-supervised paradigm. To address these limitations, we propose a novel paradigm: Single-Frame supervised VAD (SF-VAD), which uses a single annotated abnormal frame per abnormal video. SF-VAD ensures annotation efficiency while offering precise anomaly reference, facilitating robust anomaly modeling, and enhancing the detection of subtle anomalies in complex visual contexts. To validate its effectiveness, we construct three SF-VAD benchmarks by manually re-annotating the ShanghaiTech, UCF-Crime, and XD-Violence datasets in a practical procedure. Further, we devise Frame-guided Progressive Learning (FPL), to generalize sparse frame supervision to event-level anomaly understanding. FPL first leverages evidential learning to estimate anomaly relevance guided by annotated frames. Then it extends anomaly supervision by mining discrete abnormal events based on anomaly relevance and feature similarity. Meanwhile, FPL decouples normal patterns by isolating distinct normal frames outside abnormal events, reducing false alarms. Extensive experiments show SF-VAD achieves state-of-the-art detection results while offering a favorable trade-off between performance and annotation cost",
    "checked": false,
    "id": "3d190aa44f1cc3a501e241cff20801c3de33463a",
    "semantic_title": "an advanced autoencoder-based approach to anomaly detection for video surveillance systems",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AuBSUgFVgq": {
    "title": "Multimodal Tabular Reasoning with Privileged Structured Information",
    "volume": "poster",
    "abstract": "Tabular reasoning requires complex, multi-step information extraction and logical inference, such as aggregation, comparison, or calculation over tabular data. While recent advances have leveraged large language models (LLMs) for reasoning over structured text tables, such high-quality textual representations are often unavailable in real-world settings, where tables typically appear as images. In this paper, we tackle the task of tabular reasoning directly from table images. Our core strategy is to leverage privileged structured information---specifically, the ground-truth structured table data available during training but inaccessible at test time---to enhance multimodal large language models (MLLMs). The key challenges lie in: accurately aligning visual representations with the structured information, particularly mapping the visual evidence to logical steps; and effectively transferring the reasoning skills learned during training to the MLLM for visual inference. To address these, we introduce {\\sc Turbo} (TabUlar Reasoning with Bridged infOrmation), a new framework for multimodal tabular reasoning using privileged information. {\\sc Turbo} benefits from a structure-aware reasoning trace generator based on DeepSeek-R1, which contributes to high-quality modality-bridged information. On this basis, {\\sc Turbo} repeatedly generates and selects advantageous reasoning traces, further enhancing the model's tabular reasoning ability. Experimental results demonstrate that, with limited (9k) data, {\\sc Turbo} achieves state-of-the-art performance ($+7.2\\%$ vs. previous SOTA) across multiple datasets",
    "checked": true,
    "id": "4a3683125a20d4e0f75856abcc7b6cb53b3892bb",
    "semantic_title": "multimodal tabular reasoning with privileged structured information",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=6l3llg5ZeO": {
    "title": "Look-Ahead Reasoning on Learning Platforms",
    "volume": "poster",
    "abstract": "Predictive models are often designed to minimize risk for the learner, yet their objectives do not always align with the interests of the users they affect. Thus, as a way to contest predictive systems, users might act strategically in order to achieve favorable outcomes. While past work has studied strategic user behavior on learning platforms, the focus has largely been on strategic responses to the deployed model, without considering the behavior of other users, or implications thereof for the deployed model. In contrast, *look-ahead reasoning* takes into account that user actions are coupled, and---at scale---impact future predictions. Within this framework, we first formalize level-$k$ thinking, a concept from behavioral economics, where users aim to outsmart their peers by looking one step ahead. We show that, while convergence to an equilibrium is accelerated, the equilibrium remains the same, providing no benefit of higher-level reasoning for individuals in the long run. Then, we focus on collective reasoning, where users take coordinated actions by optimizing through their impact on the model. By contrasting collective with selfish behavior, we characterize the benefits and limits of coordination; a new notion of alignment between the learner's and the users' utilities emerges as a key concept. We discuss connections to several related mathematical frameworks, including strategic classification, performative prediction, and algorithmic collective action",
    "checked": false,
    "id": "e531beabea7bad32c842058201f274103b2d8dea",
    "semantic_title": "look-ahead reasoning with a learned model in imperfect information games",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CkAZ5sgDQX": {
    "title": "SplashNet: Split‑and‑Share Encoders for Accurate and Efficient Typing with Surface Electromyography",
    "volume": "poster",
    "abstract": "Surface electromyography (sEMG) at the wrists could enable natural, keyboard‑free text entry, yet the state‑of‑the‑art emg2qwerty baseline still misrecognizes 51.8\\% of characters zero‑shot on unseen users and 7.0\\% after user‑specific fine‑tuning. We trace much of these errors to mismatched cross‑user signal statistics, fragile reliance on high‑order feature dependencies, and the absence of architectural inductive biases aligned with the bilateral nature of typing. To address these issues, we introduce three simple modifications: (i) Rolling Time Normalization which adaptively aligns input distributions across users; (ii) Aggressive Channel Masking, which encourages reliance on low‑order feature combinations more likely to generalize across users; and (iii) a Split‑and‑Share encoder that processes each hand independently with weight‑shared streams to reflect the bilateral symmetry of the neuromuscular system. Combined with a five‑fold reduction in spectral resolution (33$\\rightarrow$6 frequency bands), these components yield a compact Split-and-Share model, SplashNet‑mini, which uses only ¼ the parameters and 0.6× the FLOPs of the baseline while reducing character error rate (CER) to 36.4\\% zero‑shot and 5.9\\% after fine‑tuning. An upscaled variant, SplashNet (½ parameters, 1.15× FLOPs of the baseline), further lowers error to 35.7\\% and 5.5\\%, representing 31\\% and 21\\% relative improvements in the zero-shot and finetuned settings, respectively. SplashNet therefore establishes a new state-of-the-art without requiring additional data",
    "checked": false,
    "id": "e1e11419744f5da030bf01131711c586f52b519d",
    "semantic_title": "splashnet: split-and-share encoders for accurate and efficient typing with surface electromyography",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6hidr8PJ8F": {
    "title": "Spectral Convolutional Conditional Neural Process",
    "volume": "poster",
    "abstract": "Neural processes (NPs) are probabilistic meta-learning models that map sets of observations to posterior predictive distributions, enabling inference at arbitrary domain points. Their capacity to handle variable-sized collections of unstructured observations, combined with simple maximum-likelihood training and uncertainty-aware predictions, makes them well-suited for modeling data over continuous domains. Since their introduction, several variants have been proposed. Early approaches typically represented observed data using finite-dimensional summary embeddings obtained through aggregation schemes such as mean pooling. However, this strategy fundamentally mismatches the infinite-dimensional nature of the generative processes that NPs aim to capture. Convolutional conditional neural processes (ConvCNPs) address this limitation by constructing infinite-dimensional functional embeddings processed through convolutional neural networks (CNNs) to enforce translation equivariance. Yet CNNs with local spatial kernels struggle to capture long-range dependencies without resorting to large kernels, which impose significant computational costs. To overcome this limitation, we propose the Spectral ConvCNP (SConvCNP), which performs global convolution in the frequency domain. Inspired by Fourier neural operators (FNOs) for learning solution operators of partial differential equations (PDEs), our approach directly parameterizes convolution kernels in the frequency domain, leveraging the relatively compact yet global Fourier representation of many natural signals. We validate the effectiveness of SConvCNP on both synthetic and real-world datasets, demonstrating how ideas from operator learning can advance the capabilities of NPs",
    "checked": false,
    "id": "ae0c3108fca655bb8d7d7f248bb2dc1b1644f6d5",
    "semantic_title": "spectral convolutional conditional neural processes",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=pGRjDetCDM": {
    "title": "JADE: Joint Alignment and Deep Embedding for Multi-Slice Spatial Transcriptomics",
    "volume": "poster",
    "abstract": "As spatial transcriptomics (ST) datasets increasingly span multiple adjacent or replicated slices, effective joint analysis across slices is needed to reconstruct tissue structures and identify consistent spatial gene expression patterns. This requires resolving spatial correspondences between slices while capturing shared transcriptomic features, two tasks that are typically addressed in isolation. Multi-slice analysis remains challenging due to physical distortions, technical variability, and batch effects. To address these challenges, we introduce Joint Alignment and Deep Embedding for multi-slice ST (JADE), a unified computational framework that simultaneously learns spot-wise alignments and shared low-dimensional embeddings across tissue slices. Unlike existing methods, JADE adopts a roundtrip framework in which each iteration alternates between alignment and embedding refinement. To infer alignment, we employ attention mechanisms that dynamically assess and weight the importance of different embedding dimensions, allowing the model to focus on the most alignment-relevant features while suppressing noise. To the best of our knowledge, JADE is the first method that jointly optimizes alignment and representation learning in a shared latent space, enabling robust multi-slice integration. We demonstrate that JADE outperforms existing alignment and embedding methods across multiple evaluation metrics in the 10x Visium human dorsolateral prefrontal cortex (DLPFC) and Stereo-seq axolotl brain datasets. By bridging spatial alignment and feature integration, JADE provides a scalable and accurate solution for cross-slice analysis of ST data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jut5q3UYRz": {
    "title": "Reverse-Annealed Sequential Monte Carlo for Efficient Bayesian Optimal Experiment Design",
    "volume": "poster",
    "abstract": "Expected information gain (EIG) is a crucial quantity in Bayesian optimal experimental design (BOED), quantifying how useful an experiment is by the amount we expect the posterior to differ from the prior. However, evaluating the EIG can be computationally expensive since it generally requires estimating the posterior normalizing constant. In this work, we leverage two idiosyncrasies of BOED to improve efficiency of EIG estimation via sequential Monte Carlo (SMC). First, in BOED we simulate the data and thus know the true underlying parameters. Second, we ultimately care about the EIG, not the individual normalizing constants. Often we observe that the Monte Carlo variance of standard SMC estimators for the normalizing constant of a single dataset are significantly lower than the variance of the normalizing constants across datasets; the latter thus contributes the majority of the variance for EIG estimates. This suggests the potential to slightly increase variance while drastically decreasing computation time by reducing the SMC population size, which leads us to an EIG-specific SMC estimator that starts with a only a single sample from the posterior and tempers \\textit{backwards} towards the prior. Using this single-sample estimator, which we call reverse-annealed SMC (RA-SMC), we show that it is possible to estimate EIG with orders of magnitude fewer likelihood evaluations in three models: a four-dimensional spring-mass, a six-dimensional Johnson-Cook model and a four-dimensional source-finding problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YyhRJXxbpi": {
    "title": "KGGen: Extracting Knowledge Graphs from Plain Text with Language Models",
    "volume": "poster",
    "abstract": "Recent interest in building foundation models for knowledge graphs has highlighted a fundamental challenge: knowledge graph data is scarce. The best-known knowl- edge graphs are primarily human-labeled, created by pattern-matching, or extracted using early NLP techniques. While human-generated knowledge graphs are in short supply, automatically extracted ones are of questionable quality. We present KGGen, a novel text-to-knowledge-graph generator that uses language models to extract high-quality graphs from plain text with a novel entity resolution approach that clusters related entities, significantly reducing the sparsity problem that plagues existing extractors. Unlike other KG generators, KGGen clusters and de-duplicates related entities to reduce sparsity in extracted KGs. Along with KGGen, we release Measure of Information in Nodes and Edges (MINE), the first benchmark to test an extractor's ability to produce a useful KG from plain text. We benchmark our new tool against leading existing generators such as Microsoft's GraphRAG; we achieve comparable retrieval accuracy on the generated graphs and better information re- tention. Moreover, our graphs exhibit more concise and generalizable entities and relations. Our code is open-sourced at https://github.com/stair-lab/kg-gen/",
    "checked": true,
    "id": "98def1a1f6a3eb10475c1c40164a1ebc02c5502a",
    "semantic_title": "kggen: extracting knowledge graphs from plain text with language models",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=VoFXUNc9Zh": {
    "title": "Coarse-to-fine Q-Network with Action Sequence for Data-Efficient Reinforcement Learning",
    "volume": "poster",
    "abstract": "Predicting a sequence of actions has been crucial in the success of recent behavior cloning algorithms in robotics. Can similar ideas improve reinforcement learning (RL)? We answer affirmatively by observing that incorporating action sequences when predicting ground-truth return-to-go leads to lower validation loss. Motivated by this, we introduce Coarse-to-fine Q-Network with Action Sequence (CQN-AS), a novel value-based RL algorithm that learns a critic network that outputs Q-values over a sequence of actions, i.e., explicitly training the value function to learn the consequence of executing action sequences. Our experiments show that CQN-AS outperforms several baselines on a variety of sparse-reward humanoid control and tabletop manipulation tasks from BiGym and RLBench",
    "checked": false,
    "id": "145308b3c7aa04c5d3483bfd68eff50fb38a0c58",
    "semantic_title": "coarse-to-fine q-network with action sequence for data-efficient robot learning",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=rVEzTCYfD7": {
    "title": "Online Learning of Pure States is as Hard as Mixed States",
    "volume": "poster",
    "abstract": "Quantum state tomography, the task of learning an unknown quantum state, is a fundamental problem in quantum information. In standard settings, the complexity of this problem depends significantly on the type of quantum state that one is trying to learn, with pure states being substantially easier to learn than general mixed states. A natural question is whether this separation holds for any quantum state learning setting. In this work, we consider the online learning framework and prove the surprising result that learning pure states in this setting is as hard as learning mixed states. More specifically, we show that both classes share almost the same sequential fat-shattering dimension, leading to identical regret scaling. We also generalize previous results on full quantum state tomography in the online setting to (i) the $\\epsilon$-realizable setting and (ii) learning the density matrix only partially, using smoothed analysis",
    "checked": true,
    "id": "27e70da1b9432fc4a732e7b5721d906496afc0b8",
    "semantic_title": "online learning of pure states is as hard as mixed states",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=pG1Y63MqHm": {
    "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
    "volume": "poster",
    "abstract": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning capabilities through its rule-based reward system. While it's a ''perfect'' reward system that effectively mitigates reward hacking, such reward functions are often discrete. Our experimental observations suggest that discrete rewards can lead to gradient anomaly, unstable optimization, and slow convergence. To address this issue, we propose ReDit (Reward Dithering), a method that dithers the discrete reward signal by adding simple random noise. With this perturbed reward, exploratory gradients are continuously provided throughout the learning process, enabling smoother gradient updates and accelerating convergence. The injected noise also introduces stochasticity into flat reward regions, encouraging the model to explore novel policies and escape local optima. Experiments across diverse tasks demonstrate the effectiveness and efficiency of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO with only approximately 10% the training steps, and furthermore, still exhibits a 4% performance improvement over vanilla GRPO when trained for a similar duration. Visualizations confirm significant mitigation of gradient issues with ReDit. Moreover, theoretical analyses are provided to further validate these advantages",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ETgPUJfQE1": {
    "title": "Understanding the Generalization of Stochastic Gradient Adam in Learning Neural Networks",
    "volume": "poster",
    "abstract": "Adam is a popular and widely used adaptive gradient method in deep learning, which has also received tremendous focus in theoretical research. However, most existing theoretical work primarily analyzes its full-batch version, which differs fundamentally from the stochastic variant used in practice. Unlike SGD, stochastic Adam does not converge to its full-batch counterpart even with infinitesimal learning rates. We present the first theoretical characterization of how batch size affects Adam's generalization, analyzing two-layer over-parameterized CNNs on image data. Our results reveal that while both Adam and AdamW with proper weight decay $\\lambda$ converge to poor test error solutions, their mini-batch variants can achieve near-zero test error. We further prove Adam has a strictly smaller effective weight decay bound than AdamW, theoretically explaining why Adam requires more sensitive $\\lambda$ tuning. Extensive experiments validate our findings, demonstrating the critical role of batch size and weight decay in Adam's generalization performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3D88hCO0Gd": {
    "title": "Diffusion Tree Sampling: Scalable inference‑time alignment of diffusion models",
    "volume": "poster",
    "abstract": "Adapting a pretrained diffusion model to new objectives at inference time remains an open problem in generative modeling. Existing steering methods suffer from inaccurate value estimation, especially at high noise levels, which biases guidance. Moreover, information from past runs is not reused to improve sample quality, leading to inefficient use of compute. Inspired by the success of Monte Carlo Tree Search, we address these limitations by casting inference-time alignment as a search problem that reuses past computations. We introduce a tree-based approach that _samples_ from the reward-aligned target density by propagating terminal rewards back through the diffusion chain and iteratively refining value estimates with each additional generation. Our proposed method, Diffusion Tree Sampling (DTS), produces asymptotically exact samples from the target distribution in the limit of infinite rollouts, and its greedy variant Diffusion Tree Search (DTS*) performs a robust search for high reward samples. On MNIST and CIFAR-10 class-conditional generation, DTS matches the FID of the best-performing baseline with up to $5\\times$ less compute. In text-to-image generation and language completion tasks, DTS* effectively searches for high reward samples that match best-of-N with $2\\times$ less compute. By reusing information from previous generations, we get an _anytime algorithm_ that turns additional compute budget into steadily better samples, providing a scalable approach for inference-time alignment of diffusion models",
    "checked": false,
    "id": "aabd199f5094c726021f9a4175491549f6feb593",
    "semantic_title": "diffusion tree sampling: scalable inference-time alignment of diffusion models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=yFfWVr2TmZ": {
    "title": "Learning \"Partner-Aware\" Collaborators in Multi-Party Collaboration",
    "volume": "poster",
    "abstract": "Large Language Models (LLMs) are increasingly bring deployed in agentic settings where they act as collaborators with humans. Therefore, it is increasingly important to be able to evaluate their abilities to collaborate effectively in multi-turn, multi-party tasks. In this paper, we build on the AI alignment and \"safe interruptability\" literature to offer novel theoretical insights on collaborative behavior between LLM-driven *collaborator agents* and an *intervention agent*. Our goal is to learn an ideal \"partner-aware\" collaborator that increases the group's common-ground (CG)—alignment on task-relevant propositions—by intelligently collecting information provided in *interventions* by a partner agent.We show how LLM agents trained using standard RLHF and related approaches are naturally inclined to ignore possibly well-meaning interventions, which makes increasing group common ground non-trivial in this setting. We employ a two-player Modified-Action MDP to examine this suboptimal behavior of standard AI agents, and propose **Interruptible Collaborative Roleplayer (ICR)**—a novel \"partner-aware\" learning algorithm to train CG-optimal collaborators. Experiments on multiple collaborative task environments show that ICR, on average, is more capable of promoting successful CG convergence and exploring more diverse solutions in such tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kKPP6q5vBs": {
    "title": "Generalization vs Specialization under Concept Shift",
    "volume": "poster",
    "abstract": "Machine learning models are often brittle under distribution shift, i.e., when data distributions at test time differ from those during training. Understanding this failure mode is central to identifying and mitigating safety risks of mass adoption of machine learning. Here we analyze ridge regression under concept shift—a form of distribution shift in which the input-label relationship changes at test time. We derive an exact expression for prediction risk in the thermodynamic limit. Our results reveal nontrivial effects of concept shift on generalization performance, including a phase transition between weak and strong concept shift regimes and nonmonotonic data dependence of test performance even when double descent is absent. Our theoretical results are in good agreement with experiments based on transformers pretrained to solve linear regression; under concept shift, too long context length can be detrimental to generalization performance of next token prediction. Finally, experiments on MNIST and FashionMNIST further validate our theoretical predictions, suggesting these phenomena represent a fundamental aspect of learning under distribution shift",
    "checked": false,
    "id": "55ef4e72ff990632e534d25bb50dd79439e71292",
    "semantic_title": "generalization vs. specialization under concept shift",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XkgkMFn4Ja": {
    "title": "Track3R: Joint Point Map and Trajectory Prior for Spatiotemporal 3D Understanding",
    "volume": "poster",
    "abstract": "Understanding the 3D world from 2D monocular videos is a crucial ability for AI. Recently, to tackle this underdetermined task, end-to-end 3D geometry priors have been sought after, such as pre-trained point map models at scale. These models enable robust 3D understanding from casually taken videos, providing accurate object shapes disentangled from uncertain camera parameters. However, they still struggle when affected by object deformation and dynamics, failing to establish consistent correspondence over the frames. Furthermore, their architectures are typically limited to pairwise frame processing, which is insufficient for capturing complex motion dynamics over extended sequences. To address these limitations, we introduce Track3R, a novel framework that integrates a new architecture and task to jointly predict point map and motion trajectories across multiple frames from video input. Specifically, our key idea is modeling two disentangled trajectories for each point: one representing object motion and the other camera poses. This design not only can enable understanding of the 3D object dynamics, but also facilitates the learning of more robust priors for 3D shapes in dynamic scenes. In our experiments, Track3R demonstrates significant improvements in a joint point mapping and 3D motion estimation task for dynamic scenes, such as 25.8% improvements in the motion estimation, and 15.7% in the point mapping accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RBWnyDEBKf": {
    "title": "Constant Bit-size Transformers Are Turing Complete",
    "volume": "poster",
    "abstract": "We prove that any Turing machine running on inputs of arbitrary length can be simulated by a constant bit-size transformer, as long as the context window is sufficiently long. This improves previous works, which require scaling up either the model's precision or the number of parameters on longer inputs. Furthermore, we prove that the complexity class SPACE$[s(n)]$ exactly characterizes the expressive power of a constant bit-size transformer with a context window of length $s(n)$. Our approach relies on simulating Post machines, a Turing-complete computational model. Post machines can be modeled as automata equipped with a queue, exhibiting computational behaviors naturally aligned with those of transformers. The behavioral similarity between transformers and Post machines may offer new insights into the mechanisms underlying the reasoning abilities of transformers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KaYMGsnZ4R": {
    "title": "DINGO: Constrained Inference for Diffusion LLMs",
    "volume": "poster",
    "abstract": "Diffusion LLMs have emerged as a promising alternative to conventional autoregressive LLMs, offering substantial potential for improving runtime efficiency. However, existing diffusion models fail to provably enforce user-specified formal constraints, such as regular expressions, which makes them unreliable for tasks that require structured outputs, such as fixed-schema JSON generation. Unlike autoregressive models, which generate tokens sequentially, diffusion LLMs predict a block of tokens in parallel. This parallelism makes traditional constrained decoding algorithms, designed to enforce constraints with sequential token prediction, ineffective at preserving the true output distribution. To address this limitation, we propose DINGO, a dynamic programming-based constrained decoding strategy that is both efficient and provably distribution-preserving. DINGO enables sampling of output strings with the highest probability under the model's predicted distribution while strictly adhering to any user-specified regular expression. On standard symbolic math and JSON generation benchmarks, DINGO achieves up to a $68$\\% points of improvement over unconstrained inference. The code is available at [**DINGO**](https://github.com/uiuc-focal-lab/DINGO)",
    "checked": true,
    "id": "3886f4dc7bbca578b73d2231c138690d2c01b94d",
    "semantic_title": "dingo: constrained inference for diffusion llms",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=WlVBCT5pbB": {
    "title": "FORLA: Federated Object-centric Representation Learning with Slot Attention",
    "volume": "poster",
    "abstract": "Learning efficient visual representations across heterogeneous unlabeled datasets remains a central challenge in federated learning. Effective federated representations require features that are jointly informative across clients while disentangling domain-specific factors without supervision. We introduce FORLA, a novel framework for federated object-centric representation learning and feature adaptation across clients using unsupervised slot attention. At the core of our method is a shared feature adapter, trained collaboratively across clients to adapt features from foundation models, and a shared slot attention module that learns to reconstruct the adapted features. To optimize this adapter, we design a two-branch student–teacher architecture. In each client, a student decoder learns to reconstruct full features from foundation models, while a teacher decoder reconstructs their adapted, low-dimensional counterpart. The shared slot attention module bridges cross-domain learning by aligning object-level representations across clients. Experiments in multiple real-world datasets show that our framework not only outperforms centralized baselines on object discovery but also learns a compact, universal representation that generalizes well across domains. This work highlights federated slot attention as an effective tool for scalable, unsupervised visual representation learning from cross-domain data with distributed concepts",
    "checked": false,
    "id": "8043fd020e615c7672d7b39f7d39231a44cb5b89",
    "semantic_title": "forla:federated object-centric representation learning with slot attention",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=6ZkcC9NmGU": {
    "title": "ItDPDM: Information-Theoretic Discrete Poisson Diffusion Model",
    "volume": "poster",
    "abstract": "Generative modeling of non-negative, discrete data, such as symbolic music, remains challenging due to two persistent limitations in existing methods. Firstly, many approaches rely on modeling continuous embeddings, which is suboptimal for inherently discrete data distributions. Secondly, most models optimize variational bounds rather than exact data likelihood, resulting in inaccurate likelihood estimates and degraded sampling quality. While recent diffusion-based models have addressed these issues separately, we tackle them jointly. In this work, we introduce the Information-Theoretic Discrete Poisson Diffusion Model (ItDPDM), inspired by photon arrival process, which combines exact likelihood estimation with fully discrete-state modeling. Central to our approach is an information-theoretic Poisson Reconstruction Loss (PRL) that has a provable exact relationship with the true data likelihood. ItDPDM achieves improved likelihood and sampling performance over prior discrete and continuous diffusion models on a variety of synthetic discrete datasets. Furthermore, on real-world datasets such as symbolic music and images, ItDPDM attains superior likelihood estimates and competitive generation quality—demonstrating a proof of concept for distribution-robust discrete generative modeling",
    "checked": true,
    "id": "7f79f46355efb58caff7e4b31e07349760434576",
    "semantic_title": "itdpdm: information-theoretic discrete poisson diffusion model",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=DGtHOkJAU2": {
    "title": "UMAMI: Unifying Masked Autoregressive Models and Deterministic Rendering for View Synthesis",
    "volume": "poster",
    "abstract": "Novel view synthesis (NVS) seeks to render photorealistic, 3D‑consistent images of a scene from unseen camera poses given only a sparse set of posed views. Existing deterministic networks render observed regions quickly but blur unobserved areas, whereas stochastic diffusion‑based methods hallucinate plausible content yet incur heavy training‑ and inference‑time costs. In this paper, we propose a hybrid framework that unifies the strengths of both paradigms. A bidirectional transformer encodes multi‑view image tokens and Plücker‑ray embeddings, producing a shared latent representation. Two lightweight heads then act on this representation: (i) a feed‑forward regression head that renders pixels where geometry is well constrained, and (ii) a masked autoregressive diffusion head that completes occluded or unseen regions. The entire model is trained end‑to‑end with joint photometric and diffusion losses, without handcrafted 3D inductive biases, enabling scalability across diverse scenes. Experiments demonstrate that our method attains state‑of‑the‑art image quality while reducing rendering time by an order of magnitude compared with fully generative baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GuvQJGgbLm": {
    "title": "Let Me Think! A Long Chain of Thought Can Be Worth Exponentially Many Short Ones",
    "volume": "poster",
    "abstract": "Inference-time computation has emerged as a promising scaling axis for improving large language model reasoning. However, despite yielding impressive performance, the optimal allocation of inference-time computation remains poorly understood. A central question is whether to prioritize sequential scaling (e.g., longer chains of thought) or parallel scaling (e.g., majority voting across multiple short chains of thought). In this work, we seek to illuminate the landscape of test-time scaling by demonstrating the existence of reasoning settings where sequential scaling offers an exponential advantage over parallel scaling. These settings are based on graph connectivity problems in challenging distributions of graphs. We validate our theoretical findings with comprehensive experiments across a range of language models, including models trained from scratch for graph connectivity with different chain of thought strategies as well as large reasoning models",
    "checked": false,
    "id": "54c7427571f2b26ca96fa164a4ba11e46a18f07c",
    "semantic_title": "let me think! a long chain-of-thought can be worth exponentially many short ones",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=ZE9cxnEBpy": {
    "title": "The Emergence of Abstract Thought in Large Language Models Beyond Any Language",
    "volume": "poster",
    "abstract": "As large language models (LLMs) continue to advance, their capacity to function effectively across a diverse range of languages has shown marked improvement. Preliminary studies observe that the hidden activations of LLMs often resemble English, even when responding to non-English prompts. This has led to the widespread assumption that LLMs may ``think'' in English. However, more recent results showing strong multilingual performance, even surpassing English performance on specific tasks in other languages, challenge this view. In this work, we find that LLMs progressively develop a core language-agnostic parameter space—a remarkably small subset of parameters whose deactivation results in significant performance degradation across all languages. This compact yet critical set of parameters underlies the model's ability to generalize beyond individual languages, supporting the emergence of abstract thought that is not tied to any specific linguistic system. Specifically, we identify language-related neurons—those are consistently activated during the processing of particular languages, and categorize them as either shared (active across multiple languages) or exclusive (specific to one). As LLMs undergo continued development over time, we observe a marked increase in both the proportion and functional importance of shared neurons, while exclusive neurons progressively diminish in influence. These shared neurons constitute the backbone of the core language-agnostic parameter space, supporting the emergence of abstract thought. Motivated by these insights, we propose neuron-specific training strategies tailored to LLMs' language-agnostic levels at different development stages. Experiments across diverse LLM families support our approach. Our codes are available at https://anonymous.4open.science/status/S-C393",
    "checked": true,
    "id": "922f847c9b207545a5def2b8130a2c8ad9458393",
    "semantic_title": "the emergence of abstract thought in large language models beyond any language",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=LNsGnfXS2V": {
    "title": "An Ellipsoid Algorithm for Online Convex Optimization",
    "volume": "poster",
    "abstract": "We study the problem of Online Convex Optimization (OCO) over a convex set $\\mathcal{K} \\subset \\mathbb{R}^d$, accessed via a separation oracle. While classical projection-based algorithms such as projected Online Gradient Descent (OGD) achieve the optimal $O(\\sqrt{T})$ regret, they require computing Euclidean projections onto $\\mathcal{K}$ whenever an iterate falls outside the feasible set. These projections can be computationally expensive, especially for complex or high-dimensional sets. Projection-free algorithms address this by replacing projections with alternative oracle-based procedures, such as separation or linear optimization oracles. However, the regret bounds of existing separation-based methods scale poorly with the set's \\emph{asphericity} $\\kappa$, defined as the ratio between the radii of the smallest enclosing ball and the largest inscribed ball in $\\mathcal{K}$; for ill-conditioned sets, $\\kappa$ can be arbitrarily large. We introduce a new separation-based algorithm for OCO that achieves a regret bound of $\\tilde{O}(\\sqrt{dT} + d^2)$, with only logarithmic dependence on $\\kappa$. This removes a key limitation of prior work and eliminates the need for costly geometric pre-processing, such as transforming $\\mathcal{K}$ into isotropic position. Our algorithm is based on a novel reduction to online optimization over a sequence of dynamically updated ellipsoids, inspired by the classical ellipsoid method for convex optimization. It requires only $\\tilde{O}(1)$ separation oracle calls per round, on par with existing separation-based approaches. These advances make our method particularly well suited for online optimization over geometrically complex feasible sets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LPAqolHp9R": {
    "title": "Reverse Diffusion Sequential Monte Carlo Samplers",
    "volume": "poster",
    "abstract": "We propose a novel sequential Monte Carlo (SMC) method for sampling from unnormalized target distributions based on a reverse denoising diffusion process. While recent diffusion-based samplers simulate the reverse diffusion using approximate score functions, they can suffer from accumulating errors due to time discretization and imperfect score estimation. In this work, we introduce a principled SMC framework that formalizes diffusion-based samplers as proposals while systematically correcting for their biases. The core idea is to construct informative intermediate target distributions that progressively steer the sampling trajectory toward the final target distribution. Although ideal intermediate targets are intractable, we develop \\emph{exact approximations} using quantities from the score estimation-based proposal, without requiring additional model training or inference overhead. The resulting sampler, termed \\textit{\\ourmethodfull}, enables consistent sampling and unbiased estimation of the target's normalization constant under mild conditions. We demonstrate the effectiveness of our method on a range of synthetic targets and real-world Bayesian inference problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eKeQB9mpuT": {
    "title": "Fast exact recovery of noisy matrix from few entries: the infinity norm approach",
    "volume": "poster",
    "abstract": "The matrix recovery (completion) problem, a central problem in data science, involves recovering a matrix $A$ from a relatively small random set of entries. While such a task is generally impossible, it has been shown that one can recover $A$ exactly in polynomial time, with high probability, under three basic and necessary assumptions: (1) the rank of $A$ is very small compared to its dimensions (low rank), (2) $A$ has delocalized singular vectors (incoherence), and (3) the sample size is sufficiently large. Various algorithms address this task, including convex optimization by Candes, Recht, and Tao (2009, 2010), alternating projection by Hardt and Wooters (2014), and low-rank approximation with gradient descent by Keshavan, Montanari, and Oh (2009, 2010). In applications, Candes and Plan (2009) noted that it is more realistic to assume noisy observations. In such cases, the above approaches provide approximate recovery with small root mean square error, which is difficult to convert into exact recovery. Recently, results by Abbe et al. (2017) and Bhardwaj et al. (2023) on approximation in the infinity norm showed that one can recover $A$ even in the noisy case, provided $A$ has bounded precision. However, beyond the three basic assumptions, they either required that the condition number of $A$ be small (2017) or that the gaps between consecutive singular values be large (2023). These additional assumptions conflict, with one requiring singular values to be close together and the other suggesting they should be far apart. It is thus natural to conjecture that neither is necessary. In this paper, we demonstrate that this is indeed the case. We propose a simple algorithm for exact recovery of noisy data, relying solely on the three basic assumptions. The core step of the algorithm is a straightforward truncated singular value decomposition, which is highly efficient. To analyze the algorithm, we prove a new infinity norm version of the classical Davis-Kahan perturbation theorem, improving an earlier result in (2023). Our proof employs a combinatorial contour integration argument and is entirely distinct from all previous approaches",
    "checked": true,
    "id": "0cbdffb4b11aa54177ac71e478bee585b7a9afef",
    "semantic_title": "fast exact recovery of noisy matrix from few entries: the infinity norm approach",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=MDUt8t5b34": {
    "title": "Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time Markers",
    "volume": "poster",
    "abstract": "One of the most profound challenges of modern machine learning is performing well on the long-tail of rare and underrepresented features. Large general-purpose models are trained for many tasks, but work best on high-frequency use cases. After training, it is hard to adapt a model to perform well on specific use cases underrepresented in the training corpus. Relying on prompt engineering or few-shot examples to maximize the output quality on a particular test case can be frustrating, as models can be highly sensitive to small changes, react in unpredicted ways or rely on a fixed system prompt for maintaining performance. In this work, we ask: Can we optimize our training protocols to both improve controllability and performance on underrepresented use cases at inference time? We revisit the divide between training and inference techniques to improve long-tail performance while providing users with a set of control levers the model is trained to be responsive to. We create a detailed taxonomy of data characteristics and task provenance to explicitly control generation attributes and implicitly condition generations at inference time. We fine-tune a base model to infer these markers automatically, which makes them optional at inference time. This principled and flexible approach yields pronounced improvements in performance on examples from the long tail of the training distribution. Overall, we observe lifts of 5.7% across all tasks. However, treasure markers are particularly effective at finding difficult to obtain gains in the long-tail. We observe relative lifts of up to 14.1% on underrepresented tasks like CodeRepair and absolute improvements of 35.3% on length instruction following evaluations",
    "checked": true,
    "id": "88e48dd7717ce14cf6d6837fe09aadedce42c23c",
    "semantic_title": "treasure hunt: real-time targeting of the long tail using training-time markers",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Guar1tumDr": {
    "title": "AttentionPredictor: Temporal Patterns Matter for KV Cache Compression",
    "volume": "poster",
    "abstract": "With the development of large language models (LLMs), efficient inference through Key-Value (KV) cache compression has attracted considerable attention, especially for long-context generation. To compress the KV cache, recent methods identify critical KV tokens through static modeling of attention scores. However, these methods often struggle to accurately determine critical tokens as they neglect the *temporal patterns* in attention scores, resulting in a noticeable degradation in LLM performance. To address this challenge, we propose **AttentionPredictor**, which is the **first learning-based method to directly predict attention patterns for KV cache compression and critical token identification**. Specifically, AttentionPredictor learns a lightweight, unified convolution model to dynamically capture spatiotemporal patterns and predict the next-token attention scores. An appealing feature of AttentionPredictor is that it accurately predicts the attention score and shares the unified prediction model, which consumes negligible memory, among all transformer layers. Moreover, we propose a cross-token critical cache prefetching framework that hides the token estimation time overhead to accelerate the decoding stage. By retaining most of the attention information, AttentionPredictor achieves **13$\\times$** KV cache compression and **5.6$\\times$** speedup in a cache offloading scenario with comparable LLM performance, significantly outperforming the state-of-the-arts. The code is available at https://github.com/MIRALab-USTC/LLM-AttentionPredictor",
    "checked": true,
    "id": "239a5644c64520f22b6e2073677aeeef96b62f07",
    "semantic_title": "attentionpredictor: temporal patterns matter for kv cache compression",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CCbXvvcdF9": {
    "title": "Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery",
    "volume": "poster",
    "abstract": "Cryo-EM is a transformational paradigm in molecular biology where computational methods are used to infer 3D molecular structure at atomic resolution from extremely noisy 2D electron microscope images. At the forefront of research is how to model the structure when the imaged particles exhibit non-rigid conformational flexibility and compositional variation where parts are sometimes missing. We introduce a novel 3D reconstruction framework with a hierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for 4D scene reconstruction. In particular, the structure of the model is grounded in an initial process that infers a part-based segmentation of the particle, providing essential inductive bias in order to handle both conformational and compositional variability. The framework, called \\methodName, is shown to reveal biologically meaningful structures on complex experimental datasets, and establishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM heterogeneity methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CGReBdzOdP": {
    "title": "QSCA: Quantization with Self-Compensating Auxiliary for Monocular Depth Estimation",
    "volume": "poster",
    "abstract": "Monocular depth estimation has advanced significantly with foundation models like Depth Anything, leveraging large-scale transformer architectures for the superior generalization. However, the deployment on resource-constrained devices remains challenging due to the high computation and memory requirement. Existing quantization methods, such as post-training quantization and quantization-aware training, often face trade-offs between efficiency and accuracy, or require extensive labeled data for retraining. To address these limitations, we propose Quantization with Self-Compensating Auxiliary for Monocular Depth Estimation (QSCA), a novel framework for 4-bit post-training quantization of Monocular depth estimation models. Our method integrates a lightweight Self-Compensating Auxiliary (SCA) module into both transformer encoder and decoder blocks, enabling the quantized model to recover from performance degradation without requiring ground truth. This design enables fast adaptation while preserving structural and spatial consistency in predicted depth maps. To our knowledge, this is the first framework to successfully apply 4-bit quantization across all layers of large-scale monocular depth estimation models. Experimental results demonstrate that QSCA significantly improves quantized depth estimation performance. On the NYUv2 dataset, it achieves an 11\\% improvement in $\\delta_1$ accuracy over existing post-training quantization methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7GwcxPIkip": {
    "title": "New Perspectives on the Polyak Stepsize: Surrogate Functions and Negative Results",
    "volume": "poster",
    "abstract": "The Polyak stepsize has been proven to be a fundamental stepsize in convex optimization, giving near optimal gradient descent rates across a wide range of assumptions. The universality of the Polyak stepsize has also inspired many stochastic variants, with theoretical guarantees and strong empirical performance. Despite the many theoretical results, our understanding of the convergence properties and shortcomings of the Polyak stepsize or its variants is both incomplete and fractured across different analyses. We propose a new, unified, and simple perspective for the Polyak stepsize and its variants as gradient descent on a surrogate loss. We show that each variant is equivalent to minimize a surrogate function with stepsizes that adapt to a guaranteed local curvature. Our general surrogate loss perspective is then used to provide a unified analysis of existing variants across different assumptions. Moreover, we show a number of negative results proving that the non-convergence results in some of the upper bounds is indeed real",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PdUdzvYzoH": {
    "title": "Hadamard Test is Sufficient for Efficient Quantum Gradient Estimation with Lie Algebraic Symmetries",
    "volume": "poster",
    "abstract": "Gradient estimation is a central challenge in training parameterized quantum circuits ( PQCs) for hybrid quantum-classical optimization and learning problems. This difficulty arises from several factors, including the exponential dimensionality of the Hilbert spaces and the information loss in quantum measurements. Existing estimators, such as finite difference and the parameter shift rule, often fail to adequately address these challenges for certain classes of PQCs. In this work, we propose a novel gradient estimation framework that leverages the underlying Lie algebraic structure of PQCs, combined with the Hadamard test. By analyzing the differential of the matrix exponential in Lie algebras, we derive an expression for the gradient as a linear combination of expectation values obtained via Hadamard tests. The coefficients in this decomposition depend solely on the circuit's parameterization and can be computed efficiently. Also, these expectation values can be estimated using state-of-the-art shadow tomography techniques. Our approach enables efficient gradient estimation, requiring a number of measurement shots that scales logarithmically with the number of parameters, and with polynomial classical and quantum time. This is an exponential reduction in the measurement cost and a polynomial speed-up in time compared to existing works",
    "checked": true,
    "id": "2c0a778f2e33046c72fc11591fcbc1f372971ae3",
    "semantic_title": "hadamard test is sufficient for efficient quantum gradient estimation with lie algebraic symmetries",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x0q4EXkbc2": {
    "title": "Parameter-Free Hypergraph Neural Network for Few-Shot Node Classification",
    "volume": "poster",
    "abstract": "Few-shot node classification on hypergraphs requires models that generalize from scarce labels while capturing high-order structures. Existing hypergraph neural networks (HNNs) effectively encode such structures but often suffer from overfitting and scalability issues due to complex, black-box architectures. In this work, we propose ZEN (Zero-Parameter Hypergraph Neural Network), a fully linear and parameter-free model that achieves both expressiveness and efficiency. Built upon a unified formulation of linearized HNNs, ZEN introduces a tractable closed-form solution for the weight matrix and a redundancy-aware propagation scheme to avoid iterative training and to eliminate redundant self-information. On 11 real-world hypergraph benchmarks, ZEN consistently outperforms eight baseline models in classification accuracy while achieving up to 696x speedups over the fastest competitor. Moreover, the decision process of ZEN is fully interpretable, providing insights into the characteristic of a dataset. Our code and datasets are fully available at https://github.com/chaewoonbae/ZEN",
    "checked": true,
    "id": "a4d73efb7f74a3d6a58e60a5ea898780d673a992",
    "semantic_title": "parameter-free hypergraph neural network for few-shot node classification",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=inEpyClGV2": {
    "title": "Grasp2Grasp: Vision-Based Dexterous Grasp Translation via Schrödinger Bridges",
    "volume": "poster",
    "abstract": "We propose a new approach to vision-based dexterous grasp translation, which aims to transfer grasp intent across robotic hands with differing morphologies. Given a visual observation of a source hand grasping an object, our goal is to synthesize a functionally equivalent grasp for a target hand without requiring paired demonstrations or hand-specific simulations. We frame this problem as a stochastic transport between grasp distributions using the Schrödinger Bridge formalism. Our method learns to map between source and target latent grasp spaces via score and flow matching, conditioned on visual observations. To guide this translation, we introduce physics-informed cost functions that encode alignment in base pose, contact maps, wrench space, and manipulability. Experiments across diverse hand-object pairs demonstrate that our approach generates stable, physically grounded grasps with strong generalization. This work enables semantic grasp transfer for heterogeneous manipulators and bridges vision-based grasping with probabilistic generative modeling. Additional details at https://grasp2grasp.github.io/",
    "checked": true,
    "id": "a98c33d9690109699091bacd33829dd23d06890f",
    "semantic_title": "grasp2grasp: vision-based dexterous grasp translation via schrödinger bridges",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=9twwDW60Bw": {
    "title": "Self-Guided Hierarchical Exploration for Generalist Foundation Model Web Agents",
    "volume": "poster",
    "abstract": "Foundation models have recently shown strong potential as web agents, capable of interpreting high-level instructions and interacting with complex web interfaces. However, existing training paradigms for these agents often rely on predefined task datasets and curated demonstrations, limiting their scalability, adaptability, and capacity for self-improvement. In this work, we introduce *Self-guided hierArchical exploration for Generalist wEb agents* (SAGE), a new training framework designed to support autonomous skill acquisition through self-guided hierarchical exploration. Our method introduces a three-tier exploration strategy: a pre-exploration phase to build structural understanding of web environments, a top-level exploration strategy to generate a self-evolving curriculum of tasks from easy to hard, and a low-level exploration mechanism that combines planning-based rollouts with step-wise learning to improve policy efficiency. Together, these components form a scalable, supervision-free framework for web agent training. Experimental results on WebVoyager and WebArena demonstrate that our method significantly outperforms prior approaches, enabling foundation model agents to learn complex web tasks with greater generalization and robustness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H8fscnm6Xx": {
    "title": "Unextractable Protocol Models: Collaborative Training and Inference without Weight Materialization",
    "volume": "poster",
    "abstract": "We consider a decentralized setup in which the participants collaboratively train and serve a large neural network, and where each participant only processes a subset of the model. In this setup, we explore the possibility of unmaterializable weights, where a full weight set is never available to any one participant. We introduce Unextractable Protocol Models (UPMs): a training and inference framework that leverages the sharded model setup to ensure model shards (i.e.,, subsets) held by participants are incompatible at different time steps. UPMs periodically inject time-varying, random, invertible transforms at participant boundaries; preserving the overall network function yet rendering cross-time assemblies incoherent. On Qwen-2.5-0.5B and Llama-3.2-1B, 10 000 transforms leave FP32 perplexity unchanged ($\\Delta$PPL$< 0.01$; Jensen–Shannon drift $<4 \\times 10^{-5}$), and we show how to control growth for lower precision datatypes. Applying a transform every 30s adds 3% latency, 0.1% bandwidth, and 10% GPU-memory overhead at inference, while training overhead falls to 1.6% time and < 1% memory. We consider several attacks, showing that the requirements of direct attacks are impractical and easy to defend against, and that gradient-based fine-tuning of stitched partitions consumes $\\geq 60\\%$ of the tokens required to train from scratch. By enabling models to be collaboratively trained yet not extracted, UPMs make it practical to embed programmatic incentive mechanisms in community-driven decentralized training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dX2BTCD02T": {
    "title": "Conformal Arbitrage: Risk-Controlled Balancing of Competing Objectives in Language Models",
    "volume": "poster",
    "abstract": "Modern language‑model deployments must often balance competing objectives—for example, helpfulness versus harmlessness, cost versus accuracy, and reward versus safety. We introduce Conformal Arbitrage, a post‑hoc framework that learns a data‑driven threshold to mediate between a Primary model optimized for a primary objective and a more conservative Guardian—which could be another model or a human domain expert—aligned with a guardrail objective. The threshold is calibrated with conformal risk control, yielding finite‑sample, distribution‑free guarantees that the long‑run frequency of undesirable events (such as factual errors or safety violations) does not exceed a user‑specified quota. Because Conformal Arbitrage operates wholly at the API level—without requiring access to model logits or updating model weights—it complements weight‑based alignment techniques and integrates seamlessly with existing cost‑aware cascades. Empirically, Conformal Arbitrage traces an efficient frontier, allowing users to define an acceptable performance level for one objective while maximizing utility in another. We observe that our method outperforms (in terms of accuracy) cost-matched random routing between models. These properties make Conformal Arbitrage a practical, theoretically grounded tool for trustworthy and economical deployment of large language models across a broad range of potentially competing objectives",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0IyP6PnlfG": {
    "title": "Differentiable Constraint-Based Causal Discovery",
    "volume": "poster",
    "abstract": "Causal discovery from observational data is a fundamental task in artificial intelligence, with far-reaching implications for decision-making, predictions, and interventions. Despite significant advances, existing methods can be broadly categorized as constraint-based or score-based approaches. Constraint-based methods offer rigorous causal discovery but are often hindered by small sample sizes, while score-based methods provide flexible optimization but typically forgo explicit conditional independence testing. This work explores a third avenue: developing differentiable $d$-separation scores, obtained through a percolation theory using soft logic. This enables the implementation of a new type of causal discovery method: gradient-based optimization of conditional independence constraints. Empirical evaluations demonstrate the robust performance of our approach in low-sample regimes, surpassing traditional constraint-based and score-based baselines on a real-world dataset. Code implementing the proposed method is publicly available at [https://github.com/PurdueMINDS/DAGPA](https://github.com/PurdueMINDS/DAGPA)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sYfIpZzojf": {
    "title": "Outcome-Based Online Reinforcement Learning: Algorithms and Fundamental Limits",
    "volume": "poster",
    "abstract": "Reinforcement learning with outcome-based feedback faces a fundamental challenge: when rewards are only observed at trajectory endpoints, how do we assign credit to the right actions? This paper provides the first comprehensive analysis of this problem in online RL with general function approximation. We develop a provably sample-efficient algorithm achieving $\\widetilde{O}({C_{\\rm cov} H^3}/{\\varepsilon^2})$ sample complexity, where $C_{\\rm cov}$ is the coverability coefficient of the underlying MDP. By leveraging general function approximation, our approach works effectively in large or infinite state spaces where tabular methods fail, requiring only that value functions and reward functions can be represented by appropriate function classes. Our results also characterize when outcome-based feedback is statistically separated from per-step rewards, revealing an unavoidable exponential separation for certain MDPs. For deterministic MDPs, we show how to eliminate the completeness assumption, dramatically simplifying the algorithm. We further extend our approach to preference-based feedback settings, proving that equivalent statistical efficiency can be achieved even under more limited information. Together, these results constitute a theoretical foundation for understanding the statistical properties of outcome-based reinforcement learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=42VDMzV7qm": {
    "title": "NEP: Autoregressive Image Editing via Next Editing Token Prediction",
    "volume": "poster",
    "abstract": "Text-guided image editing involves modifying a source image based on a language instruction and, typically, requires changes to only small local regions. However, existing approaches generate the entire target image rather than selectively regenerate only the intended editing areas. This results in (1) unnecessary computational costs and (2) a bias toward reconstructing non-editing regions, which compromises the quality of the intended edits. To resolve these limitations, we propose to formulate image editing as $\\textbf{N}$ext $\\textbf{E}$diting-token $\\textbf{P}$rediction (NEP) based on autoregressive image generation, where only regions that need to be edited are regenerated, thus avoiding unintended modification to the non-editing areas. To enable any-region editing, we propose to pre-train an any-order autoregressive text-to-image (T2I) model. Once trained, it is capable of zero-shot image editing and can be easily adapted to NEP for image editing, which achieves a new state-of-the-art on widely used image editing benchmarks. Moreover, our model naturally supports test-time scaling (TTS) through iteratively refining its generation in a zero-shot manner",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RCeZ063p33": {
    "title": "TimePerceiver: An Encoder-Decoder Framework for Generalized Time-Series Forecasting",
    "volume": "poster",
    "abstract": "In machine learning, effective modeling requires a holistic consideration of how to encode inputs, make predictions (i.e., decoding), and train the model. However, in time-series forecasting, prior work has predominantly focused on encoder design, often treating prediction and training as separate or secondary concerns. In this paper, we propose TimePerceiver, a unified encoder-decoder forecasting framework that is tightly aligned with an effective training strategy. To be specific, we first generalize the forecasting task to include diverse temporal prediction objectives such as extrapolation, interpolation, and imputation. Since this generalization requires handling input and target segments that are arbitrarily positioned along the temporal axis, we design a novel encoder-decoder architecture that can flexibly perceive and adapt to these varying positions. For encoding, we introduce a set of latent bottleneck representations that can interact with all input segments to jointly capture temporal and cross-channel dependencies. For decoding, we leverage learnable queries corresponding to target timestamps to effectively retrieve relevant information. Extensive experiments demonstrate that our framework consistently and significantly outperforms prior state-of-the-art baselines across a wide range of benchmark datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G8WnkCYEZ6": {
    "title": "Private Continual Counting of Unbounded Streams",
    "volume": "poster",
    "abstract": "We study the problem of differentially private continual counting in the unbounded setting where the input size $n$ is not known in advance. Current state-of-the-art algorithms based on optimal instantiations of the matrix mechanism cannot be directly applied here because their privacy guarantees only hold when key parameters are tuned to $n$. Using the common `doubling trick' avoids knowledge of $n$ but leads to suboptimal and non-smooth error. We solve this problem by introducing novel matrix factorizations based on logarithmic perturbations of the function $\\frac{1}{\\sqrt{1-z}}$ studied in prior works, which may be of independent interest. The resulting algorithm has smooth error, and for any $\\alpha > 0$ and $t\\leq n$ it is able to privately estimate the sum of the first $t$ data points with $O(\\log^{2+2\\alpha}(t))$ variance. It requires $O(t)$ space and amortized $O(\\log t)$ time per round, compared to $O(\\log(n)\\log(t))$ variance, $O(n)$ space and $O(n \\log n)$ pre-processing time for the nearly-optimal bounded-input algorithm of Henzinger et al. Empirically, we find that our algorithm's performance is also comparable to theirs in absolute terms: our variance is less than $1.5\\times$ theirs for $t$ as large as $2^{24}$",
    "checked": true,
    "id": "c408885252c9d7541a55736a5d2e10e4efcc8988",
    "semantic_title": "private continual counting of unbounded streams",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lqm1qJ43Sw": {
    "title": "Train with Perturbation, Infer after Merging: A Two-Stage Framework for Continual Learning",
    "volume": "poster",
    "abstract": "Continual Learning (CL) aims to enable models to continuously acquire new knowledge from a sequence of tasks with avoiding the forgetting of learned information. However, existing CL methods only rely on the parameters of the most recent task for inference, which makes them susceptible to catastrophic forgetting. Inspired by the recent success of model merging techniques, we propose Perturb-and-Merge (P\\&M), a novel continual learning framework that integrates model merging into the CL paradigm to mitigate forgetting. Specifically, after training on each task, P\\&M constructs a new model by forming a convex combination of the previous model and the newly trained task-specific model. Through theoretical analysis, We minimize the total loss increase across all tasks and derive a closed-form solution for the merging coefficient under mild assumptions. To further improve the performance of the merged model, we observe that the degradation introduced during merging can be alleviated by a regularization term composed of the task vector and the Hessian matrix of the loss function. Interestingly, we show that this term can be efficiently approximated using second-order symmetric finite differences, and a stochastic perturbation strategy along the task vector direction is accordingly devised which incurs no additional forward or backward passes while providing an effective approximation of the regularization term. Finally, we combine P\\&M with LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead. Our proposed approach achieves state-of-the-art performance on several continual learning benchmark datasets. The code is available at \\url{https://github.com/qhmiao/P-M-for-Continual-Learning}",
    "checked": true,
    "id": "311662d276caf5645637637c58bedb691c87e1b2",
    "semantic_title": "train with perturbation, infer after merging: a two-stage framework for continual learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zLkpt30ngy": {
    "title": "LLMs Encode Harmfulness and Refusal Separately",
    "volume": "poster",
    "abstract": "LLMs are trained to refuse harmful instructions, but do they truly understand harmfulness beyond just refusing? Prior work has shown that LLMs' refusal behaviors can be mediated by a one-dimensional subspace, i.e., a refusal direction. In this work, we identify a new dimension to analyze safety mechanisms in LLMs, i.e., harmfulness, which is encoded internally as a separate concept from refusal. And there exists a harmfulness direction that is distinct from the refusal direction. As causal evidence, steering along the harmfulness direction can lead LLMs to interpret harmless instructions as harmful, but steering along the refusal direction tends to elicit refusal responses directly without reversing the model's judgment on harmfulness. Furthermore, using our identified harmfulness concept, we find that certain jailbreak methods work by reducing the refusal signals without suppressing the model's internal belief of harmfulness. We also find that adversarially fine- tuning models to accept harmful instructions has minimal impact on the model's internal belief of harmfulness. These insights lead to a practical safety application: The model's latent harmfulness representation can serve as an intrinsic safeguard (Latent Guard) for detecting unsafe inputs and reducing over-refusals that is robust to finetuning attacks. For instance, our Latent Guard achieves performance comparable to or better than Llama Guard 3 8B, a dedicated finetuned safeguard model, across different jailbreak methods. Our findings suggest that LLMs' internal understanding of harmfulness is more robust than their refusal decision to diverse input instructions, offering a new perspective to study AI safety",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3kmbucBZPA": {
    "title": "MARS: A Malignity-Aware Backdoor Defense in Federated Learning",
    "volume": "poster",
    "abstract": "Federated Learning (FL) is a distributed paradigm aimed at protecting participant data privacy by exchanging model parameters to achieve high-quality model training. However, this distributed nature also makes FL highly vulnerable to backdoor attacks. Notably, the recently proposed state-of-the-art (SOTA) attack, 3DFed (SP2023), uses an indicator mechanism to determine whether the backdoor models have been accepted by the defender and adaptively optimizes backdoor models, rendering existing defenses ineffective. In this paper, we first reveal that the failure of existing defenses lies in the employment of empirical statistical measures that are loosely coupled with backdoor attacks. Motivated by this, we propose a Malignity-Aware backdooR defenSe (MARS) that leverages backdoor energy (BE) to indicate the malicious extent of each neuron. To amplify malignity, we further extract the most prominent BE values from each model to form a concentrated backdoor energy (CBE). Finally, a novel Wasserstein distance-based clustering method is introduced to effectively identify backdoor models. Extensive experiments demonstrate that MARS can defend against SOTA backdoor attacks and significantly outperforms existing defenses",
    "checked": true,
    "id": "c63b794f7600514433570e5acae08cd8c6eeddca",
    "semantic_title": "mars: a malignity-aware backdoor defense in federated learning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=zftxlb1AOo": {
    "title": "Structure-Aware Cooperative Ensemble Evolutionary Optimization on Combinatorial Problems with Multimodal Large Language Models",
    "volume": "poster",
    "abstract": "Evolutionary algorithms (EAs) have proven effective in exploring the vast solution spaces typical of graph-structured combinatorial problems. However, traditional encoding schemes, such as binary or numerical representations, often fail to straightforwardly capture the intricate structural properties of networks. Through employing the image-based encoding to preserve topological context, this study utilizes multimodal large language models (MLLMs) as evolutionary operators to facilitate structure-aware optimization over graph data. To address the visual clutter inherent in large-scale network visualizations, we leverage graph sparsification techniques to simplify structures while maintaining essential structural features. To further improve robustness and mitigate bias from different sparsification views, we propose a cooperative evolutionary optimization framework that facilitates cross-domain knowledge transfer and unifies multiple sparsified variants of diverse structures. Additionally, recognizing the sensitivity of MLLMs to network layout, we introduce an ensemble strategy that aggregates outputs from various layout configurations through consensus voting. Finally, experiments on real-world networks through various tasks demonstrate that our approach improves both the quality and reliability of solutions in MLLM-driven evolutionary optimization",
    "checked": true,
    "id": "1df1ce32a833106f49de0777cbe3feaa3f05ea11",
    "semantic_title": "structure-aware cooperative ensemble evolutionary optimization on combinatorial problems with multimodal large language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HUUQmnwUIx": {
    "title": "FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design",
    "volume": "poster",
    "abstract": "Designing analog circuits from performance specifications is a complex, multi-stage process encompassing topology selection, parameter inference, and layout feasibility. We introduce FALCON, a unified machine learning framework that enables fully automated, specification-driven analog circuit synthesis through topology selection and layout-constrained optimization. Given a target performance, FALCON first selects an appropriate circuit topology using a performance-driven classifier guided by human design heuristics. Next, it employs a custom, edge-centric graph neural network trained to map circuit topology and parameters to performance, enabling gradient-based parameter inference through the learned forward model. This inference is guided by a differentiable layout cost, derived from analytical equations capturing parasitic and frequency-dependent effects, and constrained by design rules. We train and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave circuits, generated and simulated using Cadence Spectre across 20 expert-designed topologies. Through this evaluation, FALCON demonstrates >99\\% accuracy in topology inference, <10\\% relative error in performance prediction, and efficient layout-aware design that completes in under 1 second per instance. Together, these results position FALCON as a practical and extensible foundation model for end-to-end analog circuit design automation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1i4wNFgHDd": {
    "title": "Generalizable, real-time neural decoding with hybrid state-space models",
    "volume": "poster",
    "abstract": "Real-time decoding of neural activity is central to neuroscience and neurotechnology applications, from closed-loop experiments to brain-computer interfaces, where models are subject to strict latency constraints. Traditional methods, including simple recurrent neural networks, are fast and lightweight but often struggle to generalize to unseen data. In contrast, recent Transformer-based approaches leverage large-scale pretraining for strong generalization performance, but typically have much larger computational requirements and are not always suitable for low-resource or real-time settings. To address these shortcomings, we present POSSM, a novel hybrid architecture that combines individual spike tokenization via a cross-attention module with a recurrent state-space model (SSM) backbone to enable (1) fast and causal online prediction on neural activity and (2) efficient generalization to new sessions, individuals, and tasks through multi-dataset pretraining. We evaluate POSSM's decoding performance and inference speed on intracortical decoding of monkey motor tasks, and show that it extends to clinical applications, namely handwriting and speech decoding in human subjects. Notably, we demonstrate that pretraining on monkey motor-cortical recordings improves decoding performance on the human handwriting task, highlighting the exciting potential for cross-species transfer. In all of these tasks, we find that POSSM achieves decoding accuracy comparable to state-of-the-art Transformers, at a fraction of the inference cost (up to 9x faster on GPU). These results suggest that hybrid SSMs are a promising approach to bridging the gap between accuracy, inference speed, and generalization when training neural decoders for real-time, closed-loop applications",
    "checked": true,
    "id": "5b5c3c42337516d457336458887cce8fcc6547c5",
    "semantic_title": "generalizable, real-time neural decoding with hybrid state-space models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=FWWZgavvBe": {
    "title": "Beyond the Average: Distributional Causal Inference under Imperfect Compliance",
    "volume": "poster",
    "abstract": "We study the estimation of distributional treatment effects in randomized experiments with imperfect compliance. When participants do not adhere to their assigned treatments, we leverage treatment assignment as an instrumental variable to identify the local distributional treatment effect—the difference in outcome distributions between treatment and control groups for the subpopulation of compliers. We propose a regression-adjusted estimator based on a distribution regression framework with Neyman-orthogonal moment conditions, enabling robustness and flexibility with high-dimensional covariates. Our approach accommodates continuous, discrete, and mixed discrete-continuous outcomes, and applies under a broad class of covariate-adaptive randomization schemes, including stratified block designs and simple random sampling. We derive the estimator's asymptotic distribution and show that it achieves the semiparametric efficiency bound. Simulation results demonstrate favorable finite-sample performance, and we demonstrate the method's practical relevance in an application to the Oregon Health Insurance Experiment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xeFZUrJSH7": {
    "title": "Fairshare Data Pricing via Data Valuation for Large Language Models",
    "volume": "poster",
    "abstract": "Training data is the backbone of large language models (LLMs), yet today's data markets often operate under exploitative pricing -- sourcing data from marginalized groups with little pay or recognition. This paper introduces a theoretical framework for LLM data markets, modeling the strategic interactions between buyers (LLM builders) and sellers (human annotators). We begin with theoretical and empirical analysis showing how exploitative pricing drives high-quality sellers out of the market, degrading data quality and long-term model performance. Then we introduce fairshare, a pricing mechanism grounded in data valuation that quantifies each data's contribution. It aligns incentives by sustaining seller participation and optimizing utility for both buyers and sellers. Theoretically, we show that fairshare yields mutually optimal outcomes: maximizing long-term buyer utility and seller profit while sustaining market participation. Empirically when training open-source LLMs on complex NLP tasks, including math problems, medical diagnosis, and physical reasoning, fairshare boosts seller earnings and ensures a stable supply of high-quality data, while improving buyers' performance-per-dollar and long-term welfare. Our findings offer a concrete path toward fair, transparent, and economically sustainable data markets for LLM. Our code will be open sourced",
    "checked": true,
    "id": "e4358a5e67634d4a7a573f515020023d454c30c6",
    "semantic_title": "fairshare data pricing via data valuation for large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=2rupjsmVpp": {
    "title": "Prediction with expert advice under additive noise",
    "volume": "poster",
    "abstract": "Prediction with expert advice serves as a fundamental model in online learning and sequential decision-making. However, in many real-world settings, this classical model proves insufficient as the feedback available to the decision-maker is often subject to noise, errors, or communication constraints. This paper provides fundamental limits on performance, quantified by the regret, in the case when the feedback is corrupted by an additive noise. Our general analysis achieves sharp regret bounds for canonical examples of such additive noise as the Gaussian distribution, the uniform distribution, and a general noise with a log-concave density. This analysis demonstrates how different noise characteristics affect regret bounds and identifies how the regret fundamentally scales as a function of the properties of the noise distribution",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bEYzeItoOH": {
    "title": "BLEUBERI: BLEU is a surprisingly effective reward for instruction following",
    "volume": "poster",
    "abstract": "Reward models are central to aligning LLMs with human preferences, but they are costly to train, requiring large-scale human-labeled preference data and powerful pretrained LLM backbones. Meanwhile, the increasing availability of high-quality synthetic instruction-following datasets raises the question: can simpler, reference-based metrics serve as viable alternatives to reward models during RL-based alignment? In this paper, we show first that BLEU, a basic string-matching metric, surprisingly matches strong reward models in agreement with human preferences on general instruction-following datasets. Based on this insight, we develop BLEUBERI, a method that first identifies challenging instructions and then applies Group Relative Policy Optimization (GRPO) using BLEU directly as the reward function. We demonstrate that BLEUBERI-trained models are competitive with models trained via reward model-guided RL across four challenging instruction-following benchmarks and three different base language models. A human evaluation further supports that the quality of BLEUBERI model outputs is on par with those from reward model-aligned models. Moreover, BLEUBERI models generate outputs that are more factually grounded than competing methods. Overall, we show that given access to high-quality reference outputs (easily obtained via existing instruction-following datasets or synthetic data generation), string matching-based metrics are cheap yet effective proxies for reward models during alignment. We release our code and data at https://github.com/lilakk/BLEUBERI",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hFyIIqmcqf": {
    "title": "CymbaDiff: Structured Spatial Diffusion for Sketch-based 3D Semantic Urban Scene Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ugaepulZyA": {
    "title": "Influence Guided Context Selection for Effective Retrieval-Augmented Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=05cVmYJJnb": {
    "title": "BrainMoE: Cognition Joint Embedding via Mixture-of-Expert Towards Robust Brain Foundation Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pilivzwU7n": {
    "title": "The Price of Sparsity: Sufficient Conditions for Sparse Recovery using Sparse and Sparsified Measurements",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "8c414feee63109f5ae662b491b6a9bece1f9ce86",
    "semantic_title": "the price of sparsity: sufficient conditions for sparse recovery using sparse and sparsified measurements",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M1DA33qucy": {
    "title": "Adaptively Coordinating with Novel Partners via Learned Latent Strategies",
    "volume": "poster",
    "abstract": "Adaptation is the cornerstone of effective collaboration among heterogeneous team members. In human-agent teams, artificial agents need to adapt to their human partners in real time, as individuals often have unique preferences and policies that may change dynamically throughout interactions. This becomes particularly challenging in tasks with time pressure and complex strategic spaces, where identifying partner behaviors and selecting suitable responses is difficult. In this work, we introduce a strategy-conditioned cooperator framework that learns to represent, categorize, and adapt to a broad range of potential partner strategies in real-time. Our approach encodes strategies with a variational autoencoder to learn a latent strategy space from agent trajectory data, identifies distinct strategy types through clustering, and trains a cooperator agent conditioned on these clusters by generating partners of each strategy type. For online adaptation to novel partners, we leverage a fixed-share regret minimization algorithm that dynamically infers and adjusts the partner's strategy estimation during interaction. We evaluate our method in a modified version of the Overcooked domain, a complex collaborative cooking environment that requires effective coordination among two players with a diverse potential strategy space. Through these experiments and an online user study, we demonstrate that our proposed agent achieves state of the art performance compared to existing baselines when paired with novel human, and agent teammates",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wn3VBRz5GK": {
    "title": "Many LLMs Are More Utilitarian Than One",
    "volume": "poster",
    "abstract": "Moral judgment is integral to large language models' (LLMs) social reasoning. As multi-agent systems gain prominence, it becomes crucial to understand how LLMs function when collaborating compared to operating as individual agents. In human moral judgment, group deliberation leads to a Utilitarian Boost: a tendency to endorse norm violations that inflict harm but maximize benefits for the greatest number of people. We study whether a similar dynamic emerges in multi-agent LLM systems. We test six models on well-established sets of moral dilemmas across two conditions: (1) Solo, where models reason independently, and (2) Group, where they engage in multi-turn discussions in pairs or triads. In personal dilemmas, where agents decide whether to directly harm an individual for the benefit of others, all models rated moral violations as more acceptable when part of a group, demonstrating a Utilitarian Boost similar to that observed in humans. However, the mechanism for the boost in LLMs differed: While humans in groups become more utilitarian due to heightened sensitivity to decision outcomes, LLM groups showed diverse profiles, for example, reduced sensitivity to norms or enhanced impartiality. We report model differences in when and how strongly the boost manifests. We also discuss prompt and agent compositions that enhance or mitigate the effect. We end with a discussion of the implications for AI alignment, multi-agent design, and artificial moral reasoning. Code available at: https://github.com/baltaci-r/MoralAgents",
    "checked": true,
    "id": "b12cf922d16f8e9e442b8055e8d4d1208566948e",
    "semantic_title": "many llms are more utilitarian than one",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=pKg5zKIEuV": {
    "title": "Quantifying Statistical Significance of Deep Nearest Neighbor Anomaly Detection via Selective Inference",
    "volume": "poster",
    "abstract": "In real-world applications, anomaly detection (AD) often operates without access to anomalous data, necessitating semi-supervised methods that rely solely on normal data. Among these methods, deep $k$-nearest neighbor (deep $k$NN) AD stands out for its interpretability and flexibility, leveraging distance-based scoring in deep latent spaces. Despite its strong performance, deep $k$NN lacks a mechanism to quantify uncertainty—an essential feature for critical applications such as industrial inspection. To address this limitation, we propose a statistical framework that quantifies the significance of detected anomalies in the form of $p$-values, thereby enabling control over false positive rates at a user-specified significance level (e.g.,0.05). A central challenge lies in managing selection bias, which we tackle using Selective Inference—a principled method for conducting inference conditioned on data-driven selections. We evaluate our method on diverse datasets and demonstrate that it provides reliable AD well-suited for industrial use cases",
    "checked": false,
    "id": "b8230d94de5cc40b157354744194b17252bc2fd9",
    "semantic_title": "statistically significant $k$nnad by selective inference",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=hfOgywCQ8j": {
    "title": "Taming generative video models for zero-shot optical flow extraction",
    "volume": "poster",
    "abstract": "Extracting optical flow from videos remains a core computer vision problem. Motivated by the recent success of large general-purpose models, we ask whether frozen self-supervised video models trained only to predict future frames can be prompted, without fine-tuning, to output flow. Prior attempts to read out depth or illumination from video generators required fine-tuning; that strategy is ill-suited for flow, where labeled data is scarce and synthetic datasets suffer from a sim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm, which can obtain point-wise correspondences by injecting a small tracer perturbation into a next-frame predictor and tracking its propagation, we extend this idea to generative video models for zero-shot flow extraction. We explore several popular architectures and find that successful zero-shot flow extraction in this manner is aided by three model properties: (1) distributional prediction of future frames (avoiding blurry or noisy outputs); (2) factorized latents that treat each spatio-temporal patch independently; and (3) random-access decoding that can condition on any subset of future pixels. These properties are uniquely present in the recently introduced Local Random Access Sequence (LRAS) architecture. Building on LRAS, we propose KL-tracing: a novel test-time inference procedure that injects a localized perturbation into the first frame, rolls out the model one step, and computes the Kullback–Leibler divergence between perturbed and unperturbed predictive distributions. Without any flow-specific fine-tuning, our method is competitive with state-of-the-art, task-specific models on the real-world TAP-Vid DAVIS benchmark and the synthetic TAP-Vid Kubric. Our results show that counterfactual prompting of controllable generative video models is an effective alternative to supervised or photometric-loss methods for high-quality flow",
    "checked": true,
    "id": "9dace6cb5bff355b852c027970ed11cf16d17d6d",
    "semantic_title": "taming generative video models for zero-shot optical flow extraction",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=JgtCg08aZk": {
    "title": "LayerIF: Estimating Layer Quality for Large Language Models using Influence Functions",
    "volume": "poster",
    "abstract": "Pretrained Large Language Models (LLMs) achieve strong performance across a wide range of tasks, yet exhibit substantial variability in the various layers' training quality with respect to specific downstream applications, limiting their downstream performance. It is therefore critical to estimate layer-wise training quality in a manner that accounts for both model architecture and training data. However, existing approaches predominantly rely on model-centric heuristics (such as spectral statistics, outlier detection, or uniform allocation) while overlooking the influence of data. To address these limitations, we propose **LayerIF**, a data-driven framework that leverages *Influence Functions* to quantify the training quality of individual layers in a principled and task-sensitive manner. By isolating each layer's gradients and measuring the sensitivity of the validation loss to training examples by computing layer-wise influences, we derive data-driven estimates of layer importance. Notably, our method produces *task-specific* layer importance estimates for the *same* LLM, revealing how layers specialize for different test-time evaluation tasks. We demonstrate the utility of our scores by leveraging them for two downstream applications: (a) expert allocation in LoRA-MoE architectures and (b) layer-wise sparsity distribution for LLM pruning. Experiments across multiple LLM architectures demonstrate that our model-agnostic, influence-guided allocation leads to consistent gains in task performance",
    "checked": true,
    "id": "5c210f016c49e442faf45c6404ba166a2340348a",
    "semantic_title": "layerif: estimating layer quality for large language models using influence functions",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=nR9S0Iet8O": {
    "title": "Evolutionary Reasoning Does Not Arise in Standard Usage of Protein Language Models",
    "volume": "poster",
    "abstract": "Protein language models (PLMs) are often assumed to capture evolutionary information by training on large protein sequence datasets. Yet it remains unclear whether PLMs can reason about evolution—that is, infer evolutionary relationships between sequences. We test this capability by evaluating whether standard PLM usage, frozen or fine-tuned embeddings with distance-based comparison, supports evolutionary reasoning. Existing PLMs consistently fail to recover phylogenetic structure, despite strong performance on sequence-level tasks such as masked-token and contact prediction. We present Phyla, a hybrid state-space and transformer model that jointly processes multiple sequences and is trained using a tree-based objective across 3,000 phylogenies spanning diverse protein families. Phyla outperforms the next-best PLM by 9\\% on tree reconstruction and 23\\% on taxonomic clustering while remaining alignment- and guide-tree-free. Although classical alignment pipelines achieve higher absolute accuracy, Phyla narrows the gap and achieves markedly lower end-to-end runtime. Applied to real data, Phyla reconstructs biologically accurate clades in the tree of life and resolves genome-scale relationships among Mycobacterium tuberculosis isolates. These findings suggest that, under standard usage, evolutionary reasoning does not reliably emerge from large-scale sequence modeling. Instead, Phyla shows that models trained with phylogenetic supervision can reason about evolution more effectively, offering a biologically grounded path toward evolutionary foundation models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KXfIoUMlzA": {
    "title": "Flat Channels to Infinity in Neural Loss Landscapes",
    "volume": "poster",
    "abstract": "The loss landscapes of neural networks contain minima and saddle points that may be connected in flat regions or appear in isolation. We identify and characterize a special structure in the loss landscape: channels along which the loss decreases extremely slowly, while the output weights of at least two neurons, $a_i$ and $a_j$, diverge to $\\pm$infinity, and their input weight vectors, $\\mathbf{w_i}$ and $\\mathbf{w_j}$, become equal to each other. At convergence, the two neurons implement a gated linear unit: $a_i\\sigma(\\mathbf{w_i} \\cdot \\mathbf{x}) + a_j\\sigma(\\mathbf{w_j} \\cdot \\mathbf{x}) \\rightarrow c \\sigma(\\mathbf{w} \\cdot \\mathbf{x}) + (\\mathbf{v} \\cdot \\mathbf{x}) \\sigma'(\\mathbf{w} \\cdot \\mathbf{x})$. Geometrically, these channels to infinity are asymptotically parallel to symmetry-induced lines of critical points. Gradient flow solvers, and related optimization methods like SGD or ADAM, reach the channels with high probability in diverse regression settings, but without careful inspection they look like flat local minima with finite parameter values. Our characterization provides a comprehensive picture of this quasi-flat region in terms of gradient dynamics, geometry, and functional interpretation. The emergence of gated linear units at the end of the channels highlights a surprising aspect of the computational capabilities of fully connected layers",
    "checked": true,
    "id": "6eab2a84d3f9ab7e9dfe04d568b93a0045b09b6f",
    "semantic_title": "flat channels to infinity in neural loss landscapes",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J76cCYTJub": {
    "title": "Unveiling the Compositional Ability Gap in Vision-Language Reasoning Model",
    "volume": "poster",
    "abstract": "While large language models (LLMs) demonstrate strong reasoning capabilities utilizing reinforcement learning (RL) with verifiable reward, whether large vision-language models (VLMs) can directly inherit such capabilities through similar post-training strategies remains underexplored. In this work, we conduct a systematic compositional probing study to evaluate whether current VLMs trained with RL or other post-training strategies can compose capabilities across modalities or tasks under out-of-distribution conditions. We design a suite of diagnostic tasks that train models on unimodal tasks or isolated reasoning skills, and evaluate them on multimodal, compositional variants requiring skill integration. Through comparisons between supervised fine-tuning (SFT) and RL-trained models, we identify three key findings: (1) RL-trained models consistently outperform SFT on compositional generalization, demonstrating better integration of learned skills; (2) although VLMs achieve strong performance on individual tasks, they struggle to generalize compositionally under cross-modal and cross-task scenarios, revealing a significant gap in current training strategies; (3) enforcing models to explicitly describe visual content before reasoning (e.g., caption-before-thinking), along with rewarding progressive vision-to-text grounding, yields notable gains. It highlights two essential ingredients for improving compositionality in VLMs: visual-to-text alignment and accurate visual grounding. Our findings shed light on the current limitations of RL-based reasoning VLM training and provide actionable insights toward building models that reason compositionally across modalities and tasks",
    "checked": true,
    "id": "d624ac7d5cacc53ff21eb8bb94988165b1ddc197",
    "semantic_title": "unveiling the compositional ability gap in vision-language reasoning model",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=e46NRNunFp": {
    "title": "Fast Non-Log-Concave Sampling under Nonconvex Equality and Inequality Constraints with Landing",
    "volume": "poster",
    "abstract": "Sampling from constrained statistical distributions is a fundamental task in various fields including Bayesian statistics, computational chemistry, and statistical physics. This article considers the cases where the constrained distribution is described by an unconstrained density, as well as additional equality and/or inequality constraints, which often make the constraint set nonconvex. Existing methods for nonconvex constraint set $\\Sigma \\subset \\mathbb{R}^d$ defined by equality or inequality constraints commonly rely on costly projection steps. Moreover, they cannot handle equality and inequality constraints simultaneously as each method only specialized in one case. In addition, rigorous and quantitative convergence guarantee is often lacking. In this paper, we introduce Overdamped Langevin with LAnding (OLLA), a new framework that can design overdamped Langevin dynamics accommodating both equality and inequality constraints. The proposed dynamics also deterministically corrects trajectories along the normal direction of the constraint surface, thus obviating the need for explicit projections. We show that, under suitable regularity conditions on the target density and $\\Sigma$, OLLA converges exponentially fast in $W_2$ distance to the constrained target density $\\rho_\\Sigma(x) \\propto \\exp(-f(x))d\\sigma_\\Sigma$. Lastly, through experiments, we demonstrate the efficiency of OLLA compared to projection-based constrained Langevin algorithms and their slack variable variants, highlighting its favorable computational cost and reasonable empirical mixing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jPduiyxyfw": {
    "title": "Knee-Deep in C-RASP: A Transformer Depth Hierarchy",
    "volume": "poster",
    "abstract": "It has been observed that transformers with greater depth (that is, more layers) have more capabilities, but can we establish formally which capabilities are gained? We answer this question with a theoretical proof followed by an empirical study. First, we consider transformers that round to fixed precision except inside attention. We show that this subclass of transformers is expressively equivalent to the programming language $\\textsf{C}$-$\\textsf{RASP}$ and this equivalence preserves depth. Second, we prove that deeper $\\textsf{C}$-$\\textsf{RASP}$ programs are more expressive than shallower $\\textsf{C}$-$\\textsf{RASP}$ programs, implying that deeper transformers are more expressive than shallower transformers (within the subclass mentioned above). The same is also proven for transformers with positional encodings (like RoPE and ALiBi). These results are established by studying a temporal logic with counting operators equivalent to $\\textsf{C}$-$\\textsf{RASP}$. Finally, we provide empirical evidence that our theory predicts the depth required for transformers without positional encodings to length-generalize on a family of sequential dependency tasks",
    "checked": true,
    "id": "2ad49013277354e95df6396be036a5b50341dd22",
    "semantic_title": "knee-deep in c-rasp: a transformer depth hierarchy",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=7Eh2SK6Mo7": {
    "title": "Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning",
    "volume": "poster",
    "abstract": "Policy-based methods currently dominate reinforcement learning (RL) pipelines for large language model (LLM) reasoning, leaving value-based approaches largely unexplored. We revisit the classical paradigm of Bellman Residual Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an algorithm that naturally adapts this idea to LLMs, yielding a simple yet effective off-policy algorithm that optimizes a single trajectory-level Bellman objective using the model's own logits as $Q$-values. TBRM removes the need for critics, importance-sampling ratios, or clipping, and can operate with only one rollout per prompt. We prove convergence to the near-optimal KL-regularized policy from arbitrary off-policy data via an improved change-of-trajectory-measure analysis. Experiments on standard mathematical-reasoning benchmarks show that TBRM matches or surpasses policy-based baselines, like PPO and GRPO, with comparable or lower computational and memory overhead. Our results indicate that value-based RL might be a principled and efficient alternative for enhancing reasoning capabilities in LLMs. The codebase for TBRM is publicly available at [https://github.com/rlx-lab/TBRM](https://github.com/rlx-lab/TBRM)",
    "checked": true,
    "id": "cc5b6a71b2598cb147c757caf84ac9f26354305b",
    "semantic_title": "trajectory bellman residual minimization: a simple value-based method for llm reasoning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Ej20yjWMCj": {
    "title": "OptiTree: Hierarchical Thoughts Generation with Tree Search for LLM Optimization Modeling",
    "volume": "poster",
    "abstract": "Optimization modeling is one of the most crucial but technical parts of operations research (OR). To automate the modeling process, existing works have leveraged large language models (LLMs), prompting them to break down tasks into steps for generating variables, constraints, and objectives. However, due to the highly complex mathematical structures inherent in OR problems, standard fixed-step decomposition often fails to achieve high performance. To address this challenge, we introduce OptiTree, a novel tree search approach designed to enhance modeling capabilities for complex problems through adaptive problem decomposition into simpler subproblems. Specifically, we develop a modeling tree that organizes a wide range of OR problems based on their hierarchical problem taxonomy and complexity, with each node representing a problem category and containing relevant high-level modeling thoughts. Given a problem to model, we recurrently search the tree to identify a series of simpler subproblems and synthesize the global modeling thoughts by adaptively integrating the hierarchical thoughts. Experiments show that OptiTree significantly improves the modeling accuracy compared to the state-of-the-art, achieving over 10% improvements on the challenging benchmarks",
    "checked": true,
    "id": "5698cfe6f8fa7bff847a3302ee0d8c777c06fdd3",
    "semantic_title": "optitree: hierarchical thoughts generation with tree search for llm optimization modeling",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=Vk2sfKAdeu": {
    "title": "Tabula: A Tabular Self-Supervised Foundation Model for Single-Cell Transcriptomics",
    "volume": "poster",
    "abstract": "Foundation models (FMs) have shown great promise in single-cell genomics, yet current approaches, such as scGPT, Geneformer, and scFoundation, rely on centralized training and language modeling objectives that overlook the tabular nature of single-cell data and raise significant privacy concerns. We present TABULA, a foundation model designed for single-cell transcriptomics, which integrates a novel tabular modeling objective and federated learning framework to enable privacy-preserving pretraining across decentralized datasets. TABULA directly models the cell-by-gene expression matrix through column-wise gene reconstruction and row-wise cell contrastive learning, capturing both gene-level relationships and cell-level heterogeneity without imposing artificial gene sequence order. Extensive experiments demonstrate the effectiveness of TABULA: despite using only half the pretraining data, TABULA achieves state-of-the-art performance across key tasks, including gene imputation, perturbation prediction, cell type annotation, and multi-omics integration. It is important to note that as public single-cell datasets continue to grow, TABULA provides a scalable and privacy-aware foundation that not only validates the feasibility of federated tabular modeling but also establishes a generalizable framework for training future models under similar privacy-preserving settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lVV7F0piDK": {
    "title": "Gated Integration of Low-Rank Adaptation for Continual Learning of Large Language Models",
    "volume": "poster",
    "abstract": "Continual learning (CL), which requires the model to learn multiple tasks sequentially, is crucial for large language models (LLMs). Recently, low-rank adaptation (LoRA), one of the most representative parameter-efficient fine-tuning (PEFT) methods, has gained increasing attention in CL of LLMs. However, most existing CL methods based on LoRA typically expand a new LoRA branch to learn each new task and force the new and old LoRA branches to influence old tasks equally, potentially leading to forgetting. In this work, we propose a new method, called gated integration of low-rank adaptation (GainLoRA), for CL of LLMs. GainLoRA expands a new LoRA branch for each new task and introduces gating modules to integrate the new and old LoRA branches. Furthermore, GainLoRA leverages the new gating module to minimize the influence from the new LoRA branch to old tasks, effectively mitigating forgetting and improving the model's overall performance. Experimental results on CL benchmarks demonstrate that GainLoRA outperforms existing state-of-the-art methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TkHcdBLsJJ": {
    "title": "Exploiting Vocabulary Frequency Imbalance in Language Model Pre-training",
    "volume": "poster",
    "abstract": "Large language models are trained with tokenizers that map text to a fixed vocabulary, yet the resulting token distribution is highly imbalanced: a few words dominate the stream while most occur rarely. Recent practice favours ever-larger vocabularies, but it is unclear whether the benefit comes from better word segmentation or from amplifying this frequency skew. To this end, we perform a controlled study that scales the vocabulary of a constant-size Transformer from 24K to 196K symbols while holding data, compute and optimisation unchanged. Above 24K every common word is already a single token, so further growth only increases imbalance. Word-level loss decomposition shows that larger vocabularies reduce cross-entropy almost exclusively by lowering uncertainty on the ~$2,500$ most frequent words, even though loss on the rare tail rises. Same frequent words cover roughly $80\\%$ of tokens in downstream benchmarks, this training advantage transfers intact. We further show that enlarging model parameters with a fixed tokenizer yields the same frequent-word benefit, revealing a shared mechanism behind vocabulary and model scaling. Our results recast \"bigger vocabularies help\" as \"sharper frequency imbalance helps,\" offering a simple, principled knob for tokenizer–model co-design and clarifying the loss dynamics that govern language-model scaling in pre-training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mt5knCQyq0": {
    "title": "Balancing Performance and Costs in Best Arm Identification",
    "volume": "poster",
    "abstract": "We consider the problem of identifying the best arm in a multi-armed bandit model. Despite a wealth of literature in the traditional fixed budget and fixed confidence regimes of the best arm identification problem, it still remains a mystery to most practitioners as to how to choose an approach and corresponding budget or confidence parameter. We propose a new formalism to avoid this dilemma altogether by minimizing a risk functional which explicitly balances the performance of the recommended arm and the cost incurred by learning this arm. In this framework, a cost is incurred for each observation during the sampling phase, and upon recommending an arm, a performance penalty is incurred for identifying a suboptimal arm. The learner's goal is to minimize the sum of the penalty and cost. This new regime mirrors the priorities of many practitioners, e.g. maximizing profit in an A/B testing framework, better than classical fixed budget or confidence settings. We derive theoretical lower bounds for the risk of each of two choices for the performance penalty, the probability of misidentification and the simple regret, and propose an algorithm called DBCARE to match these lower bounds up to polylog factors on nearly all problem instances. We then demonstrate the performance of DBCARE on a number of simulated models, comparing to fixed budget and confidence algorithms to show the shortfalls of existing BAI paradigms on this problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dqBkZ9rmSF": {
    "title": "Fading to Grow: Growing Preference Ratios via Preference Fading Discrete Diffusion for Recommendation",
    "volume": "poster",
    "abstract": "Recommenders aim to rank items from a discrete item corpus in line with user interests, yet suffer from extremely sparse user preference data. Recent advances in diffusion models have inspired diffusion-based recommenders, which alleviate sparsity by injecting noise during a forward process to prevent collapse of perturbed preference distributions. However, current diffusion‑based recommenders predominantly rely on continuous Gaussian noise, which is intrinsically mismatched with the discrete nature of user preference data in recommendation. In this paper, building upon recent advances in discrete diffusion, we propose \\textbf{PreferGrow}, a discrete diffusion-based recommender modeling preference ratios by fading and growing user preferences over the discrete item corpus. PreferGrow differs from existing diffusion-based recommenders in three core aspects: (1) Discrete modeling of preference ratios: PreferGrow models relative preference ratios between two items, where a positive value indicates a more preferred one over another less preferred. This formulation aligns naturally with the discrete and ranking-oriented nature of recommendation tasks. (2) Perturbing via preference fading: Instead of injecting continuous noise, PreferGrow fades user preferences by replacing the preferred item with alternatives---physically akin to negative sampling---thereby eliminating the need for any prior noise assumption. (3) Preference reconstruction via growing: PreferGrow reconstructs user preferences by iteratively growing the preference signal from the estimated ratios. We further provide theoretical analysis showing that PreferGrow preserves key properties of discrete diffusion processes. PreferGrow provides a well-defined matrix‑based formulation for discrete diffusion-based recommendation and empirically outperforms existing diffusion‑based recommenders across five benchmark datasets, underscoring its superior effectiveness. Our codes are available at \\url{https://anonymous.4open.science/r/PreferGrow_Commit-2259/}",
    "checked": true,
    "id": "6cd762a0856b904dc27cc450711ae48fbc93e784",
    "semantic_title": "fading to grow: growing preference ratios via preference fading discrete diffusion for recommendation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1ix6MGV2fz": {
    "title": "On the Existence and Complexity of Core-Stable Data Exchanges",
    "volume": "poster",
    "abstract": "The rapid growth of data-driven technologies and the emergence of various data-sharing paradigms have underscored the need for efficient and stable data exchange protocols. In any such exchange, agents must carefully balance the benefit of acquiring valuable data against the cost of sharing their own. Ensuring stability in these exchanges is essential to prevent agents—or groups of agents—from departing and conducting local (and potentially more favorable) exchanges among themselves. To address this, we study a model where $n$ agents participate in a data exchange. Each agent has an associated payoff for the data acquired from other agents and a cost incurred during sharing its own data. The net utility of an agent is payoff minus the cost. We adapt the classical notion of *core-stability* from cooperative game theory to data exchange. A data exchange is core-stable if no subset of agents has any incentive to deviate to a different exchange. We show that a core-stable data exchange is guaranteed to exist when agents have concave payoff functions and convex cost functions-- a setting typical in domains like PAC learning and random discovery models. We show that relaxing either of the foregoing conditions may result in the nonexistence of core-stable data exchanges. Then, we prove that finding a core-stable exchange is *PPAD-hard*, even when the potential blocking coalitions are restricted to constant size. To the best of our knowledge, this provides the first known PPAD-hardness result for core-like guarantees in data economics. Finally, we show that data exchange can be modelled as a *balanced* $n$-person game. This immediately gives a pivoting algorithm via Scarf's theorem [Scarf1967core]. We show that the pivoting algorithm works well in practice through our empirical results",
    "checked": true,
    "id": "86147f78bcbb5115c3cf5825ce8d5c7e60e6558f",
    "semantic_title": "on the existence and complexity of core-stable data exchanges",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=22O1ejTxj3": {
    "title": "Modality-Aware SAM: Sharpness-Aware-Minimization Driven Gradient Modulation for Harmonized Multimodal Learning",
    "volume": "poster",
    "abstract": "In multimodal learning, dominant modalities often overshadow others, limiting generalization. We propose Modality-Aware Sharpness-Aware Minimization (M-SAM), a model-agnostic framework that applies to many modalities and supports early and late fusion scenarios. In every iteration, M-SAM in three steps optimizes learning. \\textbf{First, it identifies the dominant modality} based on modalities' contribution in the accuracy using Shapley. \\textbf{Second, it decomposes the loss landscape}, or in another language, it modulates the loss to prioritize the robustness of the model in favor of the dominant modality, and \\textbf{third, M-SAM updates the weights} by backpropagation of modulated gradients. This ensures robust learning for the dominant modality while enhancing contributions from others, allowing the model to explore and exploit complementary features that strengthen overall performance. Extensive experiments on four diverse datasets show that M-SAM outperforms the latest state-of-the-art optimization and gradient manipulation methods and significantly balances and improves multimodal learning. The code will be released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dpvX74dPWl": {
    "title": "Efficient Training of Minimal and Maximal Low-Rank Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "Low-rank recurrent neural networks (RNNs) provide a powerful framework for characterizing how neural systems solve complex cognitive tasks. However, fitting and interpreting these networks remains an important open problem. In this paper, we develop new methods for efficiently fitting low-rank RNNs in ''teacher-training'' settings. In particular, we build upon the neural engineering framework (NEF), in which RNNs are viewed as approximating an ordinary differential equation (ODE) of interest using a set of random nonlinear basis functions. This view provides geometric insight into how the choice of neural nonlinearity (e.g. tanh, ReLU) and the distribution of model parameters affects an RNN's representational capacity. We adapt this framework for online training and demonstrate better performance with significantly smaller networks compared to FORCE. Additionally, we outperform backpropagation-trained networks of similar size, while requiring substantially less training time. Next, we ask: how many neurons---and what distribution over their parameters---are needed to approximate a given dynamical system? To address this, we introduce methods for finding the smallest low-rank RNN to approximate a given dynamical system using an extension of orthogonal matching pursuit (OMP). We then consider infinite unit low-rank RNNs, which converge to a Gaussian Process (GP) over ODEs. In particular, we show that we can optimize the distribution over RNN parameters using the marginal likelihood under the equivalent GP covariance function---which can be computed in closed form for particular choices of nonlinearity. This results in substantially better training performance even for finite low-rank RNNs. Finally, we describe active learning methods for low-rank RNNs, which speed up training through the selection of maximally informative activity patterns",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DRIRD9ELMb": {
    "title": "Lookahead Routing for Large Language Models",
    "volume": "poster",
    "abstract": "Large language model (LLM) routers improve the efficiency of multi-model systems by directing each query to the most appropriate model while leveraging the diverse strengths of heterogeneous LLMs. Most existing approaches frame routing as a classification problem based solely on the input query. While this reduces overhead by avoiding inference across all models, it overlooks valuable information that could be gleaned from potential outputs and fails to capture implicit intent or contextual nuances that often emerge only during response generation. These limitations can result in suboptimal routing decisions, particularly for complex or ambiguous queries that require deeper semantic understanding. To address this challenge, we propose Lookahead, a routing framework that \"foresees\" potential model outputs by predicting their latent representations and uses these predictions to guide model selection, thus enabling more informed routing without full inference. Within this framework, we implement two approaches based on causal and masked language models. Empirical evaluations across seven public benchmarks—spanning instruction following, mathematical reasoning, and code generation—show that Lookahead consistently outperforms existing routing baselines, achieving an average performance gain of 7.7\\% over the state-of-the-art. Our code is available at https://github.com/huangcb01/lookahead-routing",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dye9w8IOV0": {
    "title": "Collaborative Reasoner: Self-Improving Social Agents with Synthetic Conversations",
    "volume": "poster",
    "abstract": "With increasingly powerful large language models (LLMs) and LLM-based agents tackling an ever-growing list of tasks, we envision a future where numerous LLM agents work seamlessly with other AI agents and humans to solve complex problems and enhance daily life. To achieve these goals, LLM agents must develop collaborative skills such as effective persuasion, assertion and disagreement, which are often overlooked in the prevalent single-turn training and evaluation of LLMs. In this work, we present Collaborative Reasoner (Coral), a framework to evaluate and improve the collaborative reasoning abilities of language models. In particular, tasks and metrics in Coral necessitate agents to disagree with incorrect solutions, convince their partners of a correct solution, and ultimately agree as a team to commit to a final solution, all through a natural multi-turn conversation. Through comprehensive evaluation on six collaborative reasoning tasks covering domains of coding, math, scientific QA and social reasoning, we show that current models cannot effectively collaborate due to undesirable social behaviors, collapsing even on problems that they can solve singlehandedly. To improve the collaborative reasoning capabilities of LLMs, we propose a self-play method to generate synthetic multi-turn preference data and further train the language models to be better collaborators. Experiments with Llama-3.1, Ministral and Qwen-2.5 models show that our proposed self-improvement approach consistently outperforms finetuned chain-of-thought performance of the same base model, yielding gains up to 16.7% absolute. Human evaluations show that the models exhibit more effective disagreement and produce more natural conversations after training on our synthetic interaction data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jusQeCotOF": {
    "title": "Uncertainty-Calibrated Prediction of Randomly-Timed Biomarker Trajectories with Conformal Bands",
    "volume": "poster",
    "abstract": "We introduce a novel conformal prediction framework for constructing conformal prediction bands with high probability around biomarker trajectories observed at subject-specific, randomly-timed follow-up visits. Existing conformal methods typically assume fixed time grids, limiting their applicability in longitudinal clinical studies. Our approach addresses this limitation by defining a time-varying nonconformity score that normalizes prediction errors using model-derived uncertainty estimates, enabling conformal inference at arbitrary time points. We evaluate our method on two well-established brain biomarkers—hippocampal and ventricular volume—using a range of standard and state-of-the-art predictors. Across models, our conformalized predictors consistently achieve nominal coverage with tighter prediction intervals compared to baseline uncertainty estimates. To further account for population heterogeneity, we develop group-conditional conformal bands with formal coverage guarantees across clinically relevant and high-risk subgroups. Finally, we demonstrate the clinical utility of our approach in identifying subjects at risk of progression to Alzheimer's disease. We introduce an uncertainty-aware progression metric based on the lower conformal bound and show that it enables the identification of 17.5\\% more high-risk subjects compared to standard slope-based methods, highlighting the value of uncertainty calibration in real-world clinical decision making. We make the code available at \\href{https://github.com/vatass/ConformalBiomarkerTrajectories}{\\texttt{github.com/vatass/ConformalBiomarkerTrajectories}}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5iAtDpkSZQ": {
    "title": "Smart Surrogate Losses for Contextual Stochastic Linear Optimization with Robust Constraints",
    "volume": "poster",
    "abstract": "We study an extension of contextual stochastic linear optimization (CSLO) that, in contrast to most of the existing literature, involves inequality constraints that depend on uncertain parameters predicted by a machine learning model. To handle the constraint uncertainty, we use contextual uncertainty sets constructed via methods like conformal prediction. Given a contextual uncertainty set method, we introduce the \"Smart Predict-then-Optimize with Robust Constraints\" (SPO-RC) loss, a feasibility-sensitive adaptation of the SPO loss that measures decision error of predicted objective parameters. We also introduce a convex surrogate, SPO-RC+, and prove Fisher consistency with SPO-RC. To enhance performance, we train on truncated datasets where true constraint parameters lie within the uncertainty sets, and we correct the induced sample selection bias using importance reweighting techniques. Through experiments on fractional knapsack and alloy production problem instances, we demonstrate that SPO-RC+ effectively handles uncertainty in constraints and that combining truncation with importance reweighting can further improve performance",
    "checked": true,
    "id": "197952d85f7dc4722ad1149151bcad0449595d91",
    "semantic_title": "smart surrogate losses for contextual stochastic linear optimization with robust constraints",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vHTkg57tPW": {
    "title": "Efficient Bayesian Experiment Design with Equivariant Networks",
    "volume": "poster",
    "abstract": "Recent work in Bayesian Experiment Design (BED) has shown the value of using Deep Learning (DL) to obtain highly efficient adaptive experiment designs. In this paper, we argue that a central bottleneck of DL training for BED is belief explosion. Specifically, as an agent progresses deeper into an experiment, the effective number of realisable beliefs grows enormously, placing significant sampling burdens on offline training schemes in an effort to gather experience from all regions of belief space. We argue that choosing an appropriate inductive bias for actor/critic networks is a critical component in mitigating the effects of belief explosion and has so far been overlooked in the BED literature. We show how Graph Neural Networks are particularly well-suited for BED DL training due to their domain permutation equivariance properties, resulting in multiple orders of magnitude improvement to sample efficiency compared to naive parameterizations",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2XelsPmKV1": {
    "title": "Path-specific effects for pulse-oximetry guided decisions in critical care",
    "volume": "poster",
    "abstract": "Identifying and measuring biases associated with sensitive attributes is a crucial consideration in healthcare to prevent treatment disparities. One prominent issue is inaccurate pulse oximeter readings, which tend to overestimate oxygen saturation for dark-skinned patients and misrepresent supplemental oxygen needs. Most existing research has revealed *statistical disparities* linking device measurement errors to patient outcomes in intensive care units (ICUs) without causal formalization. This study *causally* investigates how racial discrepancies in oximetry measurements affect invasive ventilation in ICU settings. We employ a causal inference-based approach using *path-specific effects* to isolate the impact of bias by race on clinical decision-making. To estimate these effects, we leverage a doubly robust estimator, propose its self-normalized variant for improved sample efficiency, and provide novel finite-sample guarantees. Our methodology is validated on semi-synthetic data and applied to two large real-world health datasets: MIMIC-IV and eICU. Contrary to prior work, our analysis reveals minimal impact of racial discrepancies on invasive ventilation rates. However, path-specific effects mediated by oxygen saturation disparity are more pronounced on ventilation duration, and the severity differs by dataset. Our work provides a novel pipeline for investigating potential disparities in clinical decision-making and, more importantly, highlights the necessity of causal methods to robustly assess fairness in healthcare",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I7y7MgsEgO": {
    "title": "Learning Source-Free Domain Adaptation for Visible-Infrared Person Re-Identification",
    "volume": "poster",
    "abstract": "In this paper, we investigate source-free domain adaptation (SFDA) for visible-infrared person re-identification (VI-ReID), aiming to adapt a pre-trained source model to an unlabeled target domain without access to source data. To address this challenging setting, we propose a novel learning paradigm, termed Source-Free Visible-Infrared Person Re-Identification (SVIP), which fully exploits the prior knowledge embedded in the source model to guide target domain adaptation. The proposed framework comprises three key components specifically designed for the source-free scenario: 1) a Source-Guided Contrastive Learning (SGCL) module, which leverages the discriminative feature space of the frozen source model as a reference to perform contrastive learning on the unlabeled target data, thereby preserving discrimination without requiring source samples; 2) a Residual Transfer Learning (RTL) module, which learns residual mappings to adapt the target model's representations while maintaining the knowledge from the source model; and 3) a Structural Consistency-Guided Cross-modal Alignment (SCCA) module, which enforces reciprocal structural constraints between visible and infrared modalities to identify reliable cross-modal pairs and achieve robust modality alignment without source supervision. Extensive experiments on benchmark datasets demonstrate that SVIP substantially enhances target domain performance and outperforms existing unsupervised VI-ReID methods under source-free settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1GCWcrZTX8": {
    "title": "Diffusion Federated Dataset",
    "volume": "poster",
    "abstract": "Diffusion models have demonstrated decent generation quality, yet their deployment in federated learning scenarios remains challenging. Due to data heterogeneity and a large number of parameters, conventional parameter averaging schemes often fail to achieve stable collaborative training of diffusion models. We reframe collaborative synthetic data generation as a cooperative sampling procedure from a mixture of decentralized distributions, each encoded by a pre-trained local diffusion model. This leverages the connection between diffusion and energy-based models, which readily supports compositional generation thereof. Consequently, we can directly obtain refined synthetic dataset, optionally with differential privacy guarantee, even without exchanging diffusion model parameters. Our framework reduces communication overhead while maintaining the generation quality, realized through an unadjusted Langevin algorithm with a convergence guarantee",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jc1wIiBtNk": {
    "title": "Non-Uniform Multiclass Learning with Bandit Feedback",
    "volume": "poster",
    "abstract": "We study the problem of multiclass learning with bandit feedback in both the i.i.d. batch and adversarial online models. In the *uniform* learning framework, it is well known that no hypothesis class $\\mathcal{H}$ is learnable in either model when the effective number of labels is unbounded. In contrast, within the *universal* learning framework, recent works by (Hanneke et al., 2025b) and (Hanneke et al., 2025a) have established surprising exact equivalences between learnability under bandit feedback and full supervision in both the i.i.d. batch and adversarial online models, respectively. This raises a natural question: What happens in the *non-uniform* learning framework, which lies between the uniform and universal learning frameworks? Our contributions are twofold: (1) We provide a combinatorial characterization of learnable hypothesis classes in both models, in the realizable and agnostic settings, within the non-uniform learning framework. Notably, this includes elementary and natural hypothesis classes, such as a countably infinite collection of constant functions over some domain that is learnable in both models. (2) We construct a hypothesis class that is non-uniformly learnable under full supervision in the adversarial online model (and thus also in the i.i.d. batch model), but not non-uniformly learnable under bandit feedback in the i.i.d. batch model (and thus also not in the adversarial online model). This serves as our main novel technical contribution that reveals a fundamental distinction between the non-uniform and universal learning frameworks",
    "checked": false,
    "id": "657ef6d8813f204522944d1bf8f7d96dbb41567a",
    "semantic_title": "for universal multiclass online learning, bandit feedback and full supervision are equivalent",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=SXxlb1miXS": {
    "title": "Aligning Transformers with Continuous Feedback via Energy Rank Alignment",
    "volume": "poster",
    "abstract": "Searching through chemical space is an exceptionally challenging problem because the number of possible molecules grows combinatorially with the number of atoms. Large, autoregressive models trained on databases of chemical compounds have yielded powerful generators, but we still lack robust strategies for generating molecules with desired properties. This molecular search problem closely resembles the \"alignment\" problem for large language models, though for many chemical tasks we have a specific and easily evaluable reward function. Here, we introduce an algorithm called energy rank alignment (ERA) that leverages an explicit reward function to produce a gradient-based objective that we use to optimize autoregressive policies. We show theoretically that this algorithm is closely related to proximal policy optimization (PPO) and direct preference optimization (DPO), but has a minimizer that converges to an ideal Gibbs-Boltzmann distribution with the reward playing the role of an energy function. Furthermore, this algorithm is highly scalable, does not require reinforcement learning, and performs well relative to DPO when the number of preference observations per pairing is small. We deploy this approach to align molecular transformers and protein language models to generate molecules and protein sequences, respectively, with externally specified properties and find that it does so robustly, searching through diverse parts of chemical space",
    "checked": true,
    "id": "2e1ebfc663a436659992c5c00a2b4164e39ea94f",
    "semantic_title": "aligning transformers with continuous feedback via energy rank alignment",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=lJ5WCJZfQn": {
    "title": "Robust Hyperbolic Learning with Curvature-Aware Optimization",
    "volume": "poster",
    "abstract": "Hyperbolic deep learning has become a growing research direction in computer vision due to the unique properties afforded by the alternate embedding space. The negative curvature and exponentially growing distance metric provide a natural framework for capturing hierarchical relationships between datapoints and allowing for finer separability between their embeddings. However, current hyperbolic learning approaches are still prone to overfitting, computationally expensive, and prone to instability, especially when attempting to learn the manifold curvature to adapt to tasks and different datasets. To address these issues, our paper presents a derivation for Riemannian AdamW that helps increase hyperbolic generalization ability. For improved stability, we introduce a novel fine-tunable hyperbolic scaling approach to constrain hyperbolic embeddings and reduce approximation errors. Using this along with our curvature-aware learning schema for Riemannian Optimizers enables the combination of curvature and non-trivialized hyperbolic parameter learning. Our approach demonstrates consistent performance improvements across Computer Vision, EEG classification, and hierarchical metric learning tasks while greatly reducing runtime",
    "checked": true,
    "id": "4fc84bc5c271ec90976f9642a187d8d3b469416b",
    "semantic_title": "robust hyperbolic learning with curvature-aware optimization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XBD4OjCcIA": {
    "title": "VERA: Variational Inference Framework for Jailbreaking Large Language Models",
    "volume": "poster",
    "abstract": "The rise of API-only access to state-of-the-art LLMs highlights the need for effective black-box jailbreak methods to identify model vulnerabilities in real-world settings. Without a principled objective for gradient-based optimization, most existing approaches rely on genetic algorithms, which are limited by their initialization and dependence on manually curated prompt pools. Furthermore, these methods require individual optimization for each prompt, failing to provide a comprehensive characterization of model vulnerabilities. To address this gap, we introduce VERA: Variational infErence fRamework for jAilbreaking. VERA casts black-box jailbreak prompting as a variational inference problem, training a small attacker LLM to approximate the target LLM's posterior over adversarial prompts. Once trained, the attacker can generate diverse, fluent jailbreak prompts for a target query without re-optimization. Experimental results show that VERA achieves strong performance across a range of target LLMs, highlighting the value of probabilistic inference for adversarial prompt generation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XTHI90js2e": {
    "title": "Neurons as Detectors of Coherent Sets in Sensory Dynamics",
    "volume": "poster",
    "abstract": "We model sensory streams as observations from high-dimensional stochastic dynamical systems and conceptualize sensory neurons as self-supervised learners of compact representations of such dynamics. From prior experience, neurons learn {\\it coherent sets}—regions of stimulus state space whose trajectories evolve cohesively over finite times—and assign membership indices to new stimuli. Coherent sets are identified via spectral clustering of the {\\it stochastic Koopman operator (SKO)}, where the sign pattern of a subdominant singular function partitions the state space into minimally coupled regions. For multivariate Ornstein–Uhlenbeck processes, this singular function reduces to a linear projection onto the dominant singular vector of the whitened state-transition matrix. Encoding this singular vector as a receptive field enables neurons to compute membership indices via the projection sign in a biologically plausible manner. Each neuron detects either a {\\it predictive} coherent set (stimuli with common futures) or a {\\it retrospective} coherent set (stimuli with common pasts), suggesting a functional dichotomy among neurons. Since neurons lack access to explicit dynamical equations, the requisite singular vectors must be estimated directly from data, for example, via past–future canonical correlation analysis on lag-vector representations—an approach that naturally extends to nonlinear dynamics. This framework provides a novel account of neuronal temporal filtering, the ubiquity of rectification in neural responses, and known functional dichotomies. Coherent-set clustering thus emerges as a fundamental computation underlying sensory processing and transferable to bio-inspired artificial systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A0T3piHiis": {
    "title": "Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning",
    "volume": "poster",
    "abstract": "Large Language Models (LLMs) are increasingly used to simulate human users in interactive settings such as therapy, education, and social role-play. While these simulations enable scalable training and evaluation of AI agents, off-the-shelf LLMs often drift from their assigned personas, contradict earlier statements, or abandon role-appropriate behavior. We introduce a unified framework for evaluating and improving persona consistency in LLM-generated dialogue. We define three automatic metrics—prompt-to-line consistency, line-to-line consistency, and Q\\&A consistency—that capture different types of persona drift and validate each against human annotations. Using these metrics as reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs for three user roles: a patient, a student, and a social chat partner. Our method reduces inconsistency by over 55%, resulting in more coherent, faithful, and trustworthy simulated users",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I1hgynwxGZ": {
    "title": "Reward-oriented Causal Representation Learning",
    "volume": "poster",
    "abstract": "Causal representation learning (CRL) is the process of disentangling the *latent* low-dimensional causally-related generating factors underlying high-dimensional observable data. Extensive recent studies have characterized CRL identifiability and *perfect* recovery of the latent variables and their attendant causal graph. This paper introduces the notion of *reward-oriented* CRL, the purpose of which is to move away from perfectly learning the latent representation and instead learning it to the extent needed for optimizing a desired downstream task (reward). In reward-oriented CRL, perfectly learning the latent representation can be excessive; instead, it must be learned at the *coarsest* level sufficient for optimizing the desired task. Reward-oriented CRL is formalized as the optimization of a desired function of the observable data over the space of all possible interventions and focuses on linear causal and transformation models. To sequentially identify the optimal subset of interventions, an adaptive exploration algorithm is designed that learns the latent causal graph and the variables needed to identify the best intervention. It is shown that for an $n$-dimensional latent space and a $d$-dimensional observation space, over a horizon $T$ the algorithm's regret scales as $\\tilde O(d^{\\frac{1}{3}}n^{\\frac{1}{3}}u^{\\frac{2}{3}}T^{\\frac{2}{3}} + u\\sqrt{T})$, where $u$ measures total uncertainty in the graph estimates. Furthermore, an almost-matching lower bound is shown to scale as $\\Omega(d^{\\frac{1}{3}}n^{\\frac{1}{3}}p^{\\frac{2}{3}}T^{\\frac{2}{3}} + p\\sqrt{T})$, in which $u$ is replaced by $p$ that counts the number of causal paths in the graph",
    "checked": false,
    "id": "8a88d832d4677c73d171a9910143fa659707dd0c",
    "semantic_title": "sirl: similarity-based implicit representation learning",
    "citation_count": 21,
    "authors": []
  },
  "https://openreview.net/forum?id=IjqTJELKU1": {
    "title": "The Gaussian Mixing Mechanism: Renyi Differential Privacy via Gaussian Sketches",
    "volume": "poster",
    "abstract": "Gaussian sketching, which consists of pre-multiplying the data with a random Gaussian matrix, is a widely used technique in data science and machine learning. Beyond computational benefits, this operation also provides differential privacy guarantees due to its inherent randomness. In this work, we revisit this operation through the lens of \\Renyi Differential Privacy (RDP), providing a refined privacy analysis that yields significantly tighter bounds than prior results. We then demonstrate how this improved analysis leads to performance improvement in different linear regression settings, establishing theoretical utility guarantees. Empirically, our methods improve performance across multiple datasets and, in several cases, reduce runtime",
    "checked": true,
    "id": "4e98a6520e0e0b1d807bd1f75baf034350724514",
    "semantic_title": "the gaussian mixing mechanism: renyi differential privacy via gaussian sketches",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cIGfKdfy3N": {
    "title": "Learning Diffusion Models with Flexible Representation Guidance",
    "volume": "poster",
    "abstract": "Diffusion models can be improved with additional guidance towards more effective representations of input. Indeed, prior empirical work has already shown that aligning internal representations of the diffusion model with those of pre-trained models improves generation quality. In this paper, we present a systematic framework for incorporating representation guidance into diffusion models. We provide alternative decompositions of denoising models along with their associated training criteria, where the decompositions determine when and how the auxiliary representations are incorporated. Guided by our theoretical insights, we introduce two new strategies for enhancing representation alignment in diffusion models. First, we pair examples with target representations either derived from themselves or arisen from different synthetic modalities, and subsequently learn a joint model over the multimodal pairs. Second, we design an optimal training curriculum that balances representation learning and data generation. Our experiments across image, protein sequence, and molecule generation tasks demonstrate superior performance as well as accelerated training. In particular, on the class-conditional ImageNet $256\\times 256$ benchmark, our guidance results in $23.3$ times faster training than the original SiT-XL as well as four times speedup over the state-of-the-art method REPA",
    "checked": true,
    "id": "9bf59c85fda0a99fbba1583a8aac93b7285695a0",
    "semantic_title": "learning diffusion models with flexible representation guidance",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=yqQVRNdmKJ": {
    "title": "KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning",
    "volume": "poster",
    "abstract": "Recent advances have demonstrated that integrating reinforcement learning with rule-based rewards can significantly enhance the reasoning capabilities of large language models (LLMs), even without supervised fine-tuning (SFT). However, prevalent reinforcement learning algorithms such as GRPO and its variants like DAPO, suffer from a coarse granularity issue when computing the advantage. Specifically, they compute rollout-level advantages that assign identical values to every token within a sequence, failing to capture token-specific contributions. To address this limitation, we propose Key-token Advantage Estimation (KTAE)—a novel algorithm that estimates fine-grained, token-level advantages without introducing additional models. KTAE leverages the correctness of sampled rollouts and applies statistical analysis to quantify the importance of individual tokens within a sequence to the final outcome. This quantified token-level importance is then combined with the rollout-level advantage to obtain a more fine-grained token-level advantage estimation. Empirical results show that models trained with GRPO+KTAE and DAPO+KTAE outperform baseline methods across five mathematical reasoning benchmarks. Notably, they achieve higher accuracy with shorter responses and even surpass R1-Distill-Qwen-1.5B using the same base model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=036C670YK6": {
    "title": "Parsimonious Predictions for Strategyproof Scheduling",
    "volume": "poster",
    "abstract": "We consider the problem of scheduling $m$ jobs on $n$ unrelated strategic machines to minimize the maximum load of any machine, but the machines are strategic and may misreport processing times to minimize their own load. The pioneering work of Nisan and Ronen gave an $n$-approximate deterministic strategyproof mechanism for this setting, and this was recently shown to be best possible by the breakthrough results of Christodoulou et al. This large approxation guarantee begs the question: how can we avoid these large worst-case results. In this work, we use the powerful framework of algorithms with (machine-learned) predictions to bypass these strong impossibility results. We show how we can predict $O(m+n)$ values to obtain a deterministic strategyproof algorithm whose makespan is within a constant factor of the optimal makespan when the predictions are correct, and $O(n)$ times the optimum no matter how poor the predictions are",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MHGViOjZ27": {
    "title": "Generative Caching for Structurally Similar Prompts and Responses",
    "volume": "poster",
    "abstract": "Large Language Models (LLMs) are increasingly being used to plan, reason, and execute tasks across diverse scenarios. In use cases like repeatable workflows and agentic settings, prompts are often reused with minor variations while having a similar structure for recurring tasks. This opens up opportunities for caching. However, exact prompt matching fails on such structurally similar prompts, while semantic caching may produce incorrect responses by ignoring critical differences. To address this, we introduce GenCache, a generative cache that produces variation-aware responses for structurally similar prompts. GenCache identifies reusable response patterns across similar prompt structures and synthesizes customized outputs for new requests. We show that GenCache achieves 83\\% cache hit rate, while having minimal incorrect hits on datasets without prompt repetition. In agentic workflows, it improves cache hit rate by $\\sim$20\\% and reduces end-to-end execution latency by $\\sim$34\\% compared to standard prompt matching",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QAVpe6a3rp": {
    "title": "Retrospective In-Context Learning for Temporal Credit Assignment with Large Language Models",
    "volume": "poster",
    "abstract": "Learning from self-sampled data and sparse environmental feedback remains a fundamental challenge in training self-evolving agents. Temporal credit assignment mitigates this issue by transforming sparse feedback into dense supervision signals. However, previous approaches typically depend on domain-specific value functions for credit assignment, which suffer from poor sample efficiency and limited generalization. In this work, we propose to leverage pre-trained knowledge from large language models (LLMs) to transform sparse rewards into dense training signals (i.e., the advantage function) through retrospective in-context learning (RICL). We further propose an online learning framework, RICOL, which iteratively refines the policy based on the credit assignment results from RICL. We empirically demonstrate that RICL can accurately estimate the advantage function with limited samples and effectively identify critical states for temporal credit assignment. Extended evaluation on the BabyAI benchmark shows that RICOL significantly improves sample efficiency compared to traditional online RL algorithms while achieving performance comparable to imitation learning from expert demonstartions. Our findings highlight the potential of leveraging LLMs for temporal credit assignment, paving the way for more sample-efficient and generalizable RL paradigms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=shMjc5zvqL": {
    "title": "Scalable inference of functional neural connectivity at submillisecond timescales",
    "volume": "poster",
    "abstract": "The Poisson Generalized Linear Model (GLM) is a foundational tool for analyzing neural spike train data. However, standard implementations rely on discretizing spike times into binned count data, limiting temporal resolution and scalability. Here, we develop stochastic optimization methods and polynomial approximations to the continuous-time analog of these models, and show them to be advantageous over their discrete-time counterparts. Further, we propose using a set of exponentially scaled Laguerre polynomials as an orthogonal temporal basis, which improves filter identification and yields closed-form integral solutions under the polynomial approximation. Applied to both synthetic and real spike-time data from rodent hippocampus, our methods demonstrate superior accuracy and scalability compared to traditional binned GLMs, enabling functional connectivity inference in large-scale neural recordings that are temporally precise on the order of synaptic dynamical timescales. We provide open-source implementations of both MC and PA estimators, optimized for GPU acceleration, to facilitate adoption in the neuroscience community",
    "checked": true,
    "id": "048c3966a8cb68f38978d6bdb711cb2c75bdbe95",
    "semantic_title": "scalable inference of functional neural connectivity at submillisecond timescales",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fKerD2AQai": {
    "title": "Understanding protein function with a multimodal retrieval-augmented foundation model",
    "volume": "poster",
    "abstract": "Protein language models (PLMs) learn probability distributions over natural protein sequences. By learning from hundreds of millions of natural protein sequences, protein understanding and design capabilities emerge. Recent works have shown that scaling these models improves structure prediction, but does not seem to improve mutation understanding and representation quality for protein function prediction. We introduce PoET-2, a multimodal, retrieval-augmented protein foundation model that incorporates in-context learning of family-specific evolutionary constraints with optional structure conditioning to learn generative distributions over protein sequences. PoET-2 uses a hierarchical transformer encoder that is equivariant to sequence context ordering and a dual decoder architecture with both causal and masked language modeling objectives, allowing PoET-2 to operate in both fully generative and bidirectional representation learning modes. PoET-2 achieves state-of-the-art performance on zero-shot variant effect prediction, excelling at scoring variants with multiple mutations and challenging indel mutations. In supervised settings, PoET-2 embeddings outperform previous methods for learning sequence-function relationships, especially with small datasets. This work highlights the benefits of combining retrieval augmentation with multimodal, family-centric modeling for advancing protein foundation models",
    "checked": true,
    "id": "2a8c3420b964d758d520b8bc6ab70547b05e515a",
    "semantic_title": "understanding protein function with a multimodal retrieval-augmented foundation model",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=V82wLePv0o": {
    "title": "IF-Guide: Influence Function-Guided Detoxification of LLMs",
    "volume": "poster",
    "abstract": "We study how training data contributes to the emergence of toxic behaviors in large language models. Most prior work on reducing model toxicity adopts *reactive* approaches, such as fine-tuning pre-trained (and potentially toxic) models to align them with human values. In contrast, we propose a *proactive* approach—IF-Guide—that leverages influence functions to identify and suppress harmful tokens in the training data. To this end, we first show that standard influence functions are ineffective at discovering harmful training records. We then present a novel adaptation that measures token-level attributions from training data to model toxicity, along with techniques for selecting toxic training documents and a learning objective that can be integrated into both pre-training and fine-tuning. Moreover, IF-Guide does not rely on human-preference data, which is typically required by existing alignment methods. In our evaluation, we demonstrate that IF-Guide substantially reduces both explicit and implicit toxicity—by up to 10$\\times$ compared to uncensored models, and up to 3$\\times$ compared to baseline alignment methods such as DPO and RAD—across both pre-training and fine-tuning scenarios. IF-Guide is computationally efficient: a billion-parameter model is *not necessary* for computing influence scores; a million-parameter model—with 7.5$\\times$ fewer parameters—can effectively serve as a proxy for identifying harmful data",
    "checked": true,
    "id": "0d91913396469ecd016caaebc86ef70fd11fd3c8",
    "semantic_title": "if-guide: influence function-guided detoxification of llms",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=U5w9l0yQdo": {
    "title": "Score-informed Neural Operator for Enhancing Ordering-based Causal Discovery",
    "volume": "poster",
    "abstract": "Ordering-based approaches to causal discovery identify topological orders of causal graphs, providing scalable alternatives to combinatorial search methods. Under the Additive Noise Models (ANMs) assumption, recent causal ordering methods based on score matching require an accurate estimation of the Hessian diagonal of the log-densities. However, previous approaches mainly use Stein gradient estimators, which are computationally expensive and memory-intensive. Although DiffAN addresses these limitations by substituting kernel-based estimates with diffusion models, it remains numerically unstable due to the second-order derivatives of score models. To alleviate these problems, we propose Score-informed Neural Operator (SciNO), a probabilistic generative model in smooth function spaces designed to stably approximate the Hessian diagonal and to preserve structural information during the score modeling. Empirical results show that SciNO reduces order divergence by 42.7% on synthetic graphs and by 31.5% in real-world datasets on average compared to DiffAN, while maintaining memory efficiency and scalability. Furthermore, we propose a probabilistic control algorithm for causal reasoning with autoregressive models that integrates SciNO's probability estimates with autoregressive model priors, enabling reliable data-driven causal ordering informed by semantic information. Consequently, the proposed method enhances causal reasoning abilities of LLMs without additional fine-tuning or prompt engineering",
    "checked": true,
    "id": "4b5b36aa95d58ca367d6caa4da7655994cf06a09",
    "semantic_title": "score-informed neural operator for enhancing ordering-based causal discovery",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eGa9S2OSca": {
    "title": "Global Prompt Refinement with Non-Interfering Attention Masking for One-Shot Federated Learning",
    "volume": "poster",
    "abstract": "Federated Prompt Learning (FPL) enables communication-efficient adaptation by tuning lightweight prompts on top of frozen pre-trained models. Existing FPL methods typically rely on global information, which is only available after the second training round, to facilitate collaboration among client models. Therefore, they are inherently dependent on multi-round communication to fully exhibit their strengths. Moreover, existing one-shot federated learning methods typically focus on fitting seen tasks, but lack cross-task generalization. To bridge this gap, we propose the global prompt refinement with non-interfering attention masking (GPR-NIAM) method for one-shot FPL. The core idea is to design a masking mechanism that restricts excessive interaction between the original text embeddings and the learnable prompt embeddings. GPR-NIAM achieves this through the collaboration of two key modules. Firstly, the attention isolation module suppresses attention from the learnable prompt tokens to the original text tokens, and reweights the reverse attention which preserves generalization across tasks. Secondly, the cross-silo collaborative refinement module integrates decentralized visual knowledge into a unified base and calibrates the global prompt through multi-source cross-modal knowledge alignment, further mitigating the inconsistency caused by data heterogeneity. Extensive experiments conducted on ten benchmark datasets under two tasks show that GPR-NIAM outperforms eight state-of-the-art methods in both class-level and domain-level generalization",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HMVQ00vabY": {
    "title": "Probabilistic Reasoning with LLMs for Privacy Risk Estimation",
    "volume": "poster",
    "abstract": "Probabilistic reasoning is a key aspect of both human and artificial intelligence that allows for handling uncertainty and ambiguity in decision-making. In this paper, we introduce a new numerical reasoning task under uncertainty for large language models, focusing on estimating the privacy risk of user-generated documents containing privacy-sensitive information. We propose BRANCH, a new LLM methodology that estimates the $k$-privacy value of a text—the size of the population matching the given information. BRANCH factorizes a joint probability distribution of personal information as random variables. The probability of each factor in a population is estimated separately using a Bayesian network and combined to compute the final $k$-value. Our experiments show that this method successfully estimates the $k$-value 73% of the time, a 13% increase compared to o3-mini with chain-of-thought reasoning. We also find that LLM uncertainty is a good indicator for accuracy, as high variance predictions are 37.47% less accurate on average",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gwBqVheRiD": {
    "title": "Curvature Tuning: Provable Training-free Model Steering From a Single Parameter",
    "volume": "poster",
    "abstract": "The scaling of model and data sizes has reshaped the AI landscape, establishing finetuning pretrained models as the standard paradigm for solving downstream tasks. However, dominant finetuning methods typically rely on weight adaptation, often lack interpretability, and depend on heuristically chosen hyperparameters. In this paper, we take a different perspective and shift the focus from weights to activation functions, viewing them through the lens of spline operators. We propose Curvature Tuning (CT), an interpretable and principled steering method that modulates a model's decision boundary by injecting a single hyperparameter into its activation functions. We show that CT provably adjusts model decision boundary curvature and, more fundamentally, projects a model onto a space of smooth functions—thereby complementing current finetuning methods, whose effect lies primarily in feature adaptation. Making this hyperparameter trainable gives rise to a novel and highly parameter-efficient finetuning method. Empirically, CT improves both generalization and robustness. For example, it boosts downstream accuracy of ResNet-50/152 by 8.59\\%/8.34\\% over linear probing and 4.64\\%/1.70\\% over LoRA across 12 datasets, and improves robust accuracy on the $\\ell_{\\infty}$ benchmark from RobustBench by 1032.64\\%/1494.46\\%. Our code is available at https://github.com/Leon-Leyang/curvature-tuning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ykDUVoelgj": {
    "title": "Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability",
    "volume": "poster",
    "abstract": "Learning a compact representation of history is critical for planning and generalization in partially observable environments. While meta-reinforcement learning (RL) agents can attain near Bayes-optimal policies, they often fail to learn the compact, interpretable Bayes-optimal belief states. This representational inefficiency potentially limits the agent's adaptability and generalization capacity. Inspired by predictive coding in neuroscience---which suggests that the brain predicts sensory inputs as a neural implementation of Bayesian inference---and by auxiliary predictive objectives in deep RL, we investigate whether integrating self-supervised predictive coding modules into meta-RL can facilitate learning of Bayes-optimal representations. Through state machine simulation, we show that meta-RL with predictive modules consistently generates more interpretable representations that better approximate Bayes-optimal belief states compared to conventional meta-RL across a wide variety of tasks, even when both achieve optimal policies. In challenging tasks requiring active information seeking, only meta-RL with predictive modules successfully learns optimal representations and policies, whereas conventional meta-RL struggles with inadequate representation learning. Finally, we demonstrate that better representation learning leads to improved generalization. Our results strongly suggest the role of predictive learning as a guiding principle for effective representation learning in agents navigating partial observability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jFaFCc5978": {
    "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization",
    "volume": "poster",
    "abstract": "Scaling test-time compute is crucial for enhancing the reasoning capabilities of large language models (LLMs). Existing approaches typically employ reinforcement learning (RL) to maximize a verifiable reward obtained at the end of reasoning traces. However, such methods optimize only the final performance under a large and fixed token budget, which hinders efficiency in both training and deployment. In this work, we present **AnytimeReasoner**, a novel framework for optimizing reasoning performance under varying thinking budget constraints. To achieve this, we truncate the complete thinking process to fit within sampled token budgets from a prior distribution, compelling the model to summarize the optimal answer for each truncated thinking for verification. This introduces verifiable dense rewards into the reasoning process, facilitating more effective credit assignment in RL optimization. We then optimize the thinking and summary policies in a decoupled manner to maximize the cumulative reward. Additionally, we introduce a novel variance reduction technique, **Budget Relative Policy Optimization (BRPO)**, to enhance the robustness and efficiency of the learning process when reinforcing the thinking policy. Empirical results in mathematical reasoning tasks demonstrate that our method consistently outperforms GRPO across all thinking budgets under various prior distributions, enhancing both training and token efficiency",
    "checked": true,
    "id": "9102b58c727577172e6c0cf53c9b928b752a33aa",
    "semantic_title": "optimizing anytime reasoning via budget relative policy optimization",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=X51kYnijag": {
    "title": "TRAP: Targeted Redirecting of Agentic Preferences",
    "volume": "poster",
    "abstract": "Autonomous agentic AI systems powered by vision-language models (VLMs) are rapidly advancing toward real-world deployment, yet their cross-modal reasoning capabilities introduce new attack surfaces for adversarial manipulation that exploit semantic reasoning across modalities. Existing adversarial attacks typically rely on visible pixel perturbations or require privileged model or environment access, making them impractical for stealthy, real-world exploitation. We introduce TRAP, a novel generative adversarial framework that manipulates the agent's decision-making using diffusion-based semantic injections into the vision-language embedding space. Our method combines negative prompt–based degradation with positive semantic optimization, guided by a Siamese semantic network and layout-aware spatial masking. Without requiring access to model internals, TRAP produces visually natural images yet induces consistent selection biases in agentic AI systems. We evaluate TRAP on the Microsoft Common Objects in Context (COCO) dataset, building multi-candidate decision scenarios. Across these scenarios, TRAP consistently induces decision-level preference redirection on leading models, including LLaVA-34B, Gemma3, GPT-4o, and Mistral-3.2, significantly outperforming existing baselines such as SPSA, Bandit, and standard diffusion approaches. These findings expose a critical, generalized vulnerability: autonomous agents can be consistently misled through visually subtle, semantically-guided cross-modal manipulations. Overall, our results show the need for defense strategies beyond pixel-level robustness to address semantic vulnerabilities in cross-modal decision-making. The code for TRAP is accessible on GitHub at https://github.com/uiuc-focal-lab/TRAP",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oo9IrVWcFL": {
    "title": "A Single-Swap Local Search Algorithm for k-Means of Lines",
    "volume": "poster",
    "abstract": "Clustering is a fundamental problem that has been extensively studied over past few decades, with most research focusing on point-based clustering such as $k$-means, $k$-median, and $k$-center. However, numerous real-world applications, such as motion analysis, computer vision, and missing data analysis, require clustering over structured data, including lines, time series and affine subspaces (flats), where traditional point-based clustering algorithms often fall short. In this paper, we study the $k$-means of lines problem, where the input is a set $L$ of lines in $\\mathbb{R}^d$, and the goal is to find $k$ centers $C$ in $\\mathbb{R}^d$ such that the sum of squared distances from each line in $L$ to its nearest center in $C$ is minimized. The local search algorithm is a well-established strategy for point-based $k$-means clustering, known for its efficiency and provable approximation guarantees. However, extending local search algorithm to the $k$-means of lines problem is nontrivial, as the capture relation used in point-based clustering does not generalize to the line setting. This is because that the point-to-line distance function lack the triangle inequality property that supports geometric analysis in point-based clustering. Moreover, since lines extend infinitely in space, it is difficult to identify effective swap points that can significantly reduce the clustering cost. To overcome above obstacles, we introduce a *proportional capture relation* that links optimal and current centers based the assignment proportions of lines, enabling a refined analysis that bypasses the triangle inequality barrier. We also introduce a *CrossLine* structure, which provides a principled discretization of the geometric space around line pairs, and ensures coverage of high-quality swap points essential for local search, thereby enabling effective execution of the local search process. Consequently, based on the proposed components, we develop the first single-swap local search algorithm for the $k$-means of lines problem, achieving a $(500+\\varepsilon)$-approximation in polynomial time for low-dimensional Euclidean space",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KR2zKdlEJ2": {
    "title": "Preference Learning with Response Time: Robust Losses and Guarantees",
    "volume": "poster",
    "abstract": "This paper investigates the integration of response time data into human preference learning frameworks for more effective reward model elicitation. While binary preference data has become fundamental in fine-tuning foundation models, generative AI systems, and other large-scale models, the valuable temporal information inherent in user decision-making remains largely unexploited. We propose novel methodologies to incorporate response time information alongside binary choice data, leveraging the Evidence Accumulation Drift Diffusion (EZ) model, under which response time is informative of the preference strength. We develop Neyman-orthogonal loss functions that achieve oracle convergence rates for reward model learning, matching the theoretical optimal rates that would be attained if the expected response times for each query were known a priori. Our theoretical analysis demonstrates that for linear reward functions, conventional preference learning suffers from error rates that scale exponentially with reward magnitude. In contrast, our response time-augmented approach reduces this to polynomial scaling, representing a significant improvement in sample efficiency. We extend these guarantees to non-parametric reward function spaces, establishing convergence properties for more complex, realistic reward models. Our extensive experiments validate our theoretical findings in the context of preference learning over images",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8C8F4NmHfz": {
    "title": "Tail-Optimized Caching for LLM Inference",
    "volume": "poster",
    "abstract": "Prompt caching is critical for reducing latency and cost in LLM inference---OpenAI and Anthropic report up to 50–90\\% cost savings through prompt reuse. Despite its widespread success, little is known about what constitutes an optimal prompt caching policy, particularly when optimizing tail latency—a metric of central importance to practitioners. The widely used Least Recently Used (LRU) policy can perform arbitrarily poor on this metric, as it is oblivious to the heterogeneity of conversation lengths. To address this gap, we propose Tail-Optimized LRU, a simple two-line modification that reallocates KV cache capacity to prioritize high-latency conversations by evicting cache entries that are unlikely to affect future turns. Though the implementation is simple, we prove its optimality under a natural stochastic model of conversation dynamics, providing the first theoretical justification for LRU in this setting---a result that may be of independent interest to the caching community. Experimentally, on real conversation data WildChat~\\citep{zhao2024wildchat}, Tail-Optimized LRU achieves up to 27.5\\% reduction in P90 tail Time to First Token latency and 23.9\\% in P95 tail latency compared to LRU, along with up to 38.9\\% decrease in SLO violations of 200ms. We believe this provides a practical and theoretically grounded option for practitioners seeking to optimize tail latency in real-world LLM deployments",
    "checked": true,
    "id": "6eb9ffb3144ede9f37a0245abb6fd21de091841c",
    "semantic_title": "tail-optimized caching for llm inference",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o4zN34ahEK": {
    "title": "Spark Transformer: Reactivating Sparsity in Transformer FFN and Attention",
    "volume": "poster",
    "abstract": "The discovery of the *lazy neuron phenomenon* (Li et al., 2022), where fewer than 10% of the feedforward networks (FFN) parameters in trained Transformers are activated per token, has spurred significant interests in *activation sparsity* for enhancing large model efficiency. While notable progress has been made in translating such sparsity to wall-time benefits across CPUs, GPUs, and TPUs, modern Transformers have moved away from the ReLU activation function crucial to this phenomenon. Existing efforts on re-introducing activation sparsity, e.g., by reverting to ReLU or applying top-k masking, often degrade model quality, increase parameter count, or complicate training. Sparse attention, the application of sparse activation to the attention mechanism, often face similar challenges. This paper introduces the Spark Transformer, a novel architecture that achieves high activation sparsity in both FFN and the attention mechanism while maintaining model quality, parameter count, and standard training procedures. Our method realizes sparsity via top-$k$ masking for explicit control over sparsity level. Crucially, we introduce *statistical top-k*, a hardware-accelerator-friendly, linear-time approximate algorithm that avoids costly sorting and mitigates significant training slowdown from standard top-k operators. Furthermore, Spark Transformer reallocates existing FFN parameters and attention key embeddings to form a low-cost predictor for identifying activated entries. This design not only mitigates quality loss from enforced sparsity, but also enhances wall-time benefit. Pretrained with the Gemma-2 recipe, Spark Transformer demonstrates competitive performance on standard benchmarks while exhibiting significant sparsity: only 8\\% of FFN neurons are activated, and each token attends to a maximum of 256 tokens. This translates to a 2.5x reduction in FLOPs, leading to decoding wall-time speedups of up to 1.79x on CPU and 1.40xon GPU",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Di5apl8HSH": {
    "title": "How to build a consistency model: Learning flow maps via self-distillation",
    "volume": "poster",
    "abstract": "Flow-based generative models achieve state-of-the-art sample quality, but require the expensive solution of a differential equation at inference time. Flow map models, commonly known as consistency models, encompass many recent efforts to improve inference-time efficiency by learning the solution operator of this differential equation. Yet despite their promise, these models lack a unified description that clearly explains how to learn them efficiently in practice. Here, building on the methodology proposed in Boffi et. al. (2024), we present a systematic algorithmic framework for directly learning the flow map associated with a flow or diffusion model. By exploiting a relationship between the velocity field underlying a continuous-time flow and the instantaneous rate of change of the flow map, we show how to convert any distillation scheme into a direct training algorithm via self-distillation, eliminating the need for pre-trained teachers. We introduce three algorithmic families based on different mathematical characterizations of the flow map: Eulerian, Lagrangian, and Progressive methods, which we show encompass and extend all known distillation and direct training schemes for consistency models. We find that the novel class of Lagrangian methods, which avoid both spatial derivatives and bootstrapping from small steps by design, achieve significantly more stable training and higher performance than more standard Eulerian and Progressive schemes. Our methodology unifies existing training schemes under a single common framework and reveals new design principles for accelerated generative modeling. Associated code is available at https://github.com/nmboffi/flow-maps",
    "checked": true,
    "id": "648495b844531dce7c581662bf90c339422bf16c",
    "semantic_title": "how to build a consistency model: learning flow maps via self-distillation",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=aUXiqhLh0S": {
    "title": "Balanced Active Inference",
    "volume": "poster",
    "abstract": "Limited labeling budget severely impedes data-driven research, such as medical analysis, remote sensing and population census, and active inference is a solution to this problem. Prior works utilizing independent sampling have achieved improvements over uniform sampling, but its insufficient usage of available information undermines its statistical efficiency. In this paper, we propose balanced active inference, a novel algorithm that incorporates balanced constraints based on model uncertainty utilizing the cube method for label selection. Under regularity conditions, we establish its asymptotic properties and also prove that the statistical efficiency of the proposed algorithm is higher than its alternatives. Various numerical experiments, including regression and classification in both synthetic setups and real data analysis, demonstrate that the proposed algorithm outperforms its alternatives while guaranteeing nominal coverage",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RSQgfaX4Qh": {
    "title": "Online Multi-Class Selection with Group Fairness Guarantee",
    "volume": "poster",
    "abstract": "We study the online multi-class selection problem with group fairness guarantees, where limited resources must be allocated to sequentially arriving agents. Our work addresses two key limitations in the existing literature. First, we introduce a novel lossless rounding scheme that ensures the integral algorithm achieves the same expected performance as any fractional solution. Second, we explicitly address the challenges introduced by agents who belong to multiple classes. To this end, we develop a randomized algorithm based on a relax-and-round framework. The algorithm first computes a fractional solution using a resource reservation approach---referred to as the *set-aside* mechanism---to enforce fairness across classes. The subsequent rounding step preserves these fairness guarantees without degrading performance. Additionally, we propose a learning-augmented variant that incorporates untrusted machine-learned predictions to better balance fairness and efficiency in practical settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zdRW39Tc3C": {
    "title": "Architectural and Inferential Inductive Biases for Exchangeable Sequence Modeling",
    "volume": "poster",
    "abstract": "Autoregressive models have emerged as a powerful framework for modeling exchangeable sequences---i.i.d. observations when conditioned on some latent factor---enabling direct modeling of uncertainty from missing data (rather than a latent). Motivated by the critical role posterior inference plays as a subroutine in decision-making (e.g., active learning, bandits), we study the inferential and architectural inductive biases that are most effective for exchangeable sequence modeling. For the inference stage, we highlight a fundamental limitation of the prevalent single-step generation approach: its inability to distinguish between epistemic and aleatoric uncertainty. Instead, a long line of works in Bayesian statistics advocates for multi-step autoregressive generation; we demonstrate this \"correct approach\" enables superior uncertainty quantification that translates into better performance on downstream decision-making tasks. This naturally leads to the next question: which architectures are best suited for multi-step inference? We identify a subtle yet important gap between recently proposed Transformer architectures for exchangeable sequences (Müller et al., 2022; Nguyen & Grover, 2022; Ye & Namkoong, 2024), and prove that they in fact cannot guarantee exchangeability despite introducing significant computational overhead. Through empirical evaluation, we find that these custom architectures can significantly underperform compared to standard causal masking, highlighting the need for new architectural innovations in Transformer-based modeling of exchangeable sequences",
    "checked": true,
    "id": "18edbad30b7258f212876fec20dbc560451bb874",
    "semantic_title": "architectural and inferential inductive biases for exchangeable sequence modeling",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=XdCglv7Um7": {
    "title": "Unlabeled Data Can Provably Enhance In-Context Learning of Transformers",
    "volume": "poster",
    "abstract": "Large language models (LLMs) exhibit impressive in‑context learning (ICL) capabilities, yet the quality of their predictions is fundamentally limited by the few costly labeled demonstrations that can fit into a prompt. Meanwhile, there exist vast and continuously growing amounts of unlabeled data that may be closely related to the ICL task. How to utilize such unlabeled data to provably enhance the performance of ICL thus becomes an emerging fundamental question. In this work, we propose a novel augmented ICL framework, in which the prompt includes a small set of labeled examples alongside a block of unlabeled inputs. We focus on the multi-class linear classification setting and demonstrate that, with chain-of-thought (CoT) prompting, a multi-layer transformer can effectively emulate an expectation–maximization (EM) algorithm. This enables the transformer to implicitly extract useful information from both labeled and unlabeled data, leading to provable improvements in ICL accuracy. Moreover, we show that such a transformer can be trained via teacher forcing, with its parameters converging to the desired solution at a linear rate. Experiments demonstrate that the augmented ICL framework consistently outperforms conventional few-shot ICL, providing empirical support for our theoretical findings. To the best of our knowledge, this is the first theoretical study on the impact of unlabeled data on the ICL performance of transformers",
    "checked": false,
    "id": "dcc9925aca8faad35e0a91c9d2f81cd52150c0d3",
    "semantic_title": "when and how unlabeled data provably improve in-context learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ktk4mxNzFe": {
    "title": "Hierarchical Retrieval: The Geometry and a Pretrain-Finetune Recipe",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CNxrp7u5gV": {
    "title": "Solving the Asymmetric Traveling Salesman Problem via Trace-Guided Cost Augmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6X5NGHe35l": {
    "title": "Efficiently Verifiable Proofs of Data Attribution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jAoqtT58G4": {
    "title": "Contribution of task-irrelevant stimuli to drift of neural representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zhFEO67s5w": {
    "title": "Co-PatcheR: Collaborative Software Patching with Component-specific Small Reasoning Models",
    "volume": "poster",
    "abstract": "Motivated by the success of general‑purpose large language models (LLMs) in software patching, recent works started to train specialized patching models. Most works trained one model to handle the end‑to‑end patching pipeline (including issue localization, patch generation, and patch validation). However, it is hard for a small model to handle all tasks, as different sub-tasks have different workflows and require different expertise. As such, by using a 70 billion model, SOTA methods can only reach up to 41% resolved rate on SWE-bench-Verified. Motivated by the collaborative nature, we propose Co-PatcheR, the first collaborative patching system with small and specialized reasoning models for individual components. Our key technique novelties are the specific task designs and training recipes. First, we train a model for localization and patch generation. Our localization pinpoints the suspicious lines through a two-step procedure, and our generation combines patch generation and critique. We then propose a hybrid patch validation that includes two models for crafting issue-reproducing test cases with and without assertions and judging patch correctness, followed by a majority vote-based patch selection. Through extensive evaluation, we show that Co-PatcheR achieves 46% resolved rate on SWE-bench-Verified with only 3 x 14B models. This makes Co-PatcheR the best patcher with specialized models, requiring the least training resources and the smallest models. We conduct a comprehensive ablation study to validate our recipes, as well as our choice of training data number, model size, and testing-phase scaling strategy",
    "checked": false,
    "id": "fff3bec525f79f2dcc433a275b234e3550534d21",
    "semantic_title": "co-patcher: collaborative software patching with component(s)-specific small reasoning models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=ZkGHzGIaMB": {
    "title": "Generalization or Hallucination? Understanding Out-of-Context Reasoning in Transformers",
    "volume": "poster",
    "abstract": "Large language models (LLMs) can acquire new knowledge through fine-tuning, but this process exhibits a puzzling duality: models can generalize remarkably from new facts, yet are also prone to hallucinating incorrect information. However, the reasons for this phenomenon remain poorly understood. In this work, we argue that both behaviors stem from a single mechanism known as out-of-context reasoning (OCR): the ability to deduce implications by associating concepts, even those without a causal link. Our experiments across five prominent LLMs confirm that OCR indeed drives both generalization and hallucination, depending on whether the associated concepts are causally related. To build a rigorous theoretical understanding of this phenomenon, we then formalize OCR as a synthetic factual recall task. We empirically show that a one-layer single-head attention-only transformer with factorized output and value matrices can learn to solve this task, while a model with combined weights cannot, highlighting the crucial role of matrix factorization. Our theoretical analysis shows that the OCR capability can be attributed to the implicit bias of gradient descent, which favors solutions that minimize the nuclear norm of the combined output-value matrix. This structure explains why the model learns to associate facts and implications with high sample efficiency, regardless of whether the correlation is causal or merely spurious. Ultimately, our work provides a theoretical foundation for understanding the OCR phenomenon, offering a new lens for analyzing and mitigating undesirable behaviors from knowledge injection",
    "checked": true,
    "id": "99bed165d7d47de906bbd7c536d615983c3dc986",
    "semantic_title": "generalization or hallucination? understanding out-of-context reasoning in transformers",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=Grpqzwpwv8": {
    "title": "Knowledge Distillation of Uncertainty using Deep Latent Factor Model",
    "volume": "poster",
    "abstract": "Deep ensembles deliver state-of-the-art, reliable uncertainty quantification, but their heavy computational and memory requirements hinder their practical deployments to real applications such as on-device AI. Knowledge distillation compresses an ensemble into small student models, but existing techniques struggle to preserve uncertainty partly because reducing the size of DNNs typically results in variation reduction. To resolve this limitation, we introduce a new method of distribution distillation (i.e. compressing a teacher ensemble into a student distribution instead of a student ensemble) called Gaussian distillation, which estimates the distribution of a teacher ensemble through a special Gaussian process called the deep latent factor model (DLF) by treating each member of the teacher ensemble as a realization of a certain stochastic process. The mean and covariance functions in the DLF model are estimated stably by using the expectation-maximization (EM) algorithm. By using multiple benchmark datasets, we demonstrate that the proposed Gaussian distillation outperforms existing baselines. In addition, we illustrate that Gaussian distillation works well for fine-tuning of language models and distribution shift problems",
    "checked": true,
    "id": "dddbff68c6c35ea9b4482779642683b0be7fc900",
    "semantic_title": "knowledge distillation of uncertainty using deep latent factor model",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bK3s3n0vPA": {
    "title": "Learning from positive and unlabeled examples -Finite size sample bounds",
    "volume": "poster",
    "abstract": "PU (Positive Unlabeled) learning is a variant of supervised classification learning in which the only labels revealed to the learner are of positively labeled instances. PU learning arises in many real-world applications. Most existing work relies on the simplifying assumption that the positively labeled training data is drawn from the restriction of the data generating distribution to positively labeled instances and/or that the proportion of positively labeled points (a.k.a. the class prior) is known apriori to the learner. This paper provides a theoretical analysis of the statistical complexity of PU learning under a wider range of setups. Unlike most prior work, our study does not assume that the class prior is known to the learner. We prove upper and lower bounds on the required sample sizes (of both the positively labeled and the unlabeled samples)",
    "checked": true,
    "id": "647fd55b7de39ac95078da7e03f6ca4d028e1264",
    "semantic_title": "learning from positive and unlabeled examples -finite size sample bounds",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IMamKWkS1s": {
    "title": "On the Sample Complexity of Differentially Private Policy Optimization",
    "volume": "poster",
    "abstract": "Policy optimization (PO) is a cornerstone of modern reinforcement learning (RL), with diverse applications spanning robotics, healthcare, and large language model training. The increasing deployment of PO in sensitive domains, however, raises significant privacy concerns. In this paper, we initiate a theoretical study of differentially private policy optimization, focusing explicitly on its sample complexity. We first formalize an appropriate definition of differential privacy (DP) tailored to PO, addressing the inherent challenges arising from on-policy learning dynamics and the subtlety involved in defining the unit of privacy. We then systematically analyze the sample complexity of widely-used PO algorithms, including policy gradient (PG), natural policy gradient (NPG) and more, under DP constraints and various settings, via a unified framework. Our theoretical results demonstrate that privacy costs can often manifest as lower-order terms in the sample complexity, while also highlighting subtle yet important observations in private PO settings. These offer valuable practical insights for privacy-preserving PO algorithms",
    "checked": true,
    "id": "fbabf258d66c00d1046643fac0e346353f65575b",
    "semantic_title": "on the sample complexity of differentially private policy optimization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6UAeCPQPwP": {
    "title": "Inference of Whole Brain Electrophysiological Networks Through Multimodal Integration of Simultaneous Scalp and Intracranial EEG",
    "volume": "poster",
    "abstract": "In the past decades, brain imaging research underwent a shift from mapping tasked evoked brain regions of activations towards identifying and characterizing the dynamic brain networks of multiple coordinating brain regions. Electrophysiological signals are the direct manifestation of brain activities, thus, characterizing the whole brain electrophysiological networks (WBEN) can serve as a fundamental tool for neuroscience studies and clinical applications. In this work, we introduce the first framework for the integration of scalp EEG and intracranial EEG (iEEG) for the WBEN estimation with a principled estimation framework based on state-space models, where an Expectation-Maximization (EM) algorithm is designed to infer the state variables and brain connectivity simultaneously. We validated the proposed method on synthetic data, and the results revealed improved performance compared to traditional two-step methods using scalp EEG only, demonstrating the importance of including iEEG signal for WBEN estimation. For real data with simultaneous EEG and iEEG, we applied the developed framework to understand the information flows of the encoding and maintenance phases during the working memory task. The information flows between the subcortical and cortical regions are delineated, highlighting more significant information flows from cortical to subcortical regions than maintenance phases. The results are consistent with previous research findings but with the view of the whole brain scope, which underscores the unique utility of the proposed framework",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TFidSatsOC": {
    "title": "Probing Hidden Knowledge Holes in Unlearned LLMs",
    "volume": "poster",
    "abstract": "Machine unlearning has emerged as a prevalent technical solution for selectively removing unwanted knowledge absorbed during pre-training, without requiring full retraining. While recent unlearning techniques can effectively remove undesirable content without severely compromising performance on standard benchmarks, we find that they may inadvertently create ``knowledge holes''---unintended losses of benign knowledge that standard benchmarks fail to capture. To probe where unlearned models reveal knowledge holes, we propose a test case generation framework that explores both immediate neighbors of unlearned content and broader areas of potential failures. Our evaluation demonstrates significant hidden costs of unlearning: up to 98.7\\% of the test cases yield irrelevant or nonsensical responses from unlearned models, despite being answerable by the pretrained model. These findings necessitate rethinking the conventional approach to evaluating knowledge preservation in unlearning, moving beyond standard, static benchmarks",
    "checked": false,
    "id": "cf4d55600234a294425eabfc9d62bf2331a0aa1d",
    "semantic_title": "probing knowledge holes in unlearned llms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SkZUo6Xg61": {
    "title": "Improved Balanced Classification with Theoretically Grounded Loss Functions",
    "volume": "poster",
    "abstract": "The *balanced loss* is a widely adopted objective for multi-class classification under class imbalance. By assigning equal importance to all classes, regardless of their frequency, it promotes fairness and ensures that minority classes are not overlooked. However, directly minimizing the balanced classification loss is typically intractable, which makes the design of effective surrogate losses a central question. This paper introduces and studies two advanced surrogate loss families: Generalized Logit-Adjusted (GLA) loss functions and Generalized Class-Aware weighted (GCA) losses. GLA losses generalize Logit-Adjusted losses, which shift logits based on class priors, to the broader general cross-entropy loss family. GCA loss functions extend the standard class-weighted losses, which scale losses inversely by class frequency, by incorporating class-dependent confidence margins and extending them to the general cross-entropy family. We present a comprehensive theoretical analysis of consistency for both loss families. We show that GLA losses are Bayes-consistent, but only $H$-consistent for complete (i.e., unbounded) hypothesis sets. Moreover, their $H$-consistency bounds depend inversely on the minimum class probability, scaling at least as $1/\\mathsf p _{\\min}$. In contrast, GCA losses are $H$-consistent for any hypothesis set that is bounded or complete, with $H$-consistency bounds that scale more favorably as $1/\\sqrt{\\mathsf p _{\\min}}$, offering significantly stronger theoretical guarantees in imbalanced settings. We report the results of experiments demonstrating that, empirically, both the GCA losses with calibrated class-dependent confidence margins and GLA losses can greatly outperform straightforward class-weighted losses as well as the LA losses. GLA generally performs slightly better in common benchmarks, whereas GCA exhibits a slight edge in highly imbalanced settings. Thus, we advocate for both GLA and GCA losses as principled, theoretically sound, and state-of-the-art surrogates for balanced classification under class imbalance",
    "checked": true,
    "id": "8001be6ce77296fb7083e887cf1eebb6fbb82079",
    "semantic_title": "improved balanced classification with theoretically grounded loss functions",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=qTUZJUXt0J": {
    "title": "3D-Prover: Diversity Driven Theorem Proving With Determinantal Point Processes",
    "volume": "poster",
    "abstract": "A key challenge in automated formal reasoning is the intractable search space, which grows exponentially with the depth of the proof. This branching is caused by the large number of candidate proof tactics which can be applied to a given goal. Nonetheless, many of these tactics are semantically similar or lead to an execution error, wasting valuable resources in both cases. We address the problem of effectively pruning this search, using only synthetic data generated from previous proof attempts. We first demonstrate that it is possible to generate semantically aware tactic representations which capture the effect on the proving environment, likelihood of success, and execution time. We then propose a novel filtering mechanism which leverages these representations to select semantically diverse and high quality tactics, using Determinantal Point Processes. Our approach, 3D-Prover, is designed to be general, and to augment any underlying tactic generator. We demonstrate the effectiveness of 3D-Prover on the miniF2F and LeanDojo benchmarks by augmenting popular open source proving LLMs. We show that our approach leads to an increase in the overall proof rate, as well as a significant improvement in the tactic success rate, execution time and diversity. We make our code available at https://github.com/sean-lamont/3D-Prover",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6Q5WG84uOD": {
    "title": "THD-BAR: Topology Hierarchical Derived Brain Autoregressive Modeling for EEG Generic Representations",
    "volume": "poster",
    "abstract": "Large-scale pre-trained models hold significant potential for learning universal EEG representations. However, most existing methods, particularly autoregressive (AR) frameworks, primarily rely on straightforward temporal sequencing of multi-channel EEG data, which fails to capture the rich physiological characteristics inherent to EEG signals. Moreover, their time-centered modeling approach also limits the effective representation of the dynamic spatial topology of brain activity. To address these challenges and fully exploit the potential of large-scale EEG models, we propose a novel Topology Hierarchical Derived Brain Autoregressive Modeling (THD-BAR) for EEG generic representations. The core innovation of THD-BAR lies in the introduction of the Brain Topology Hierarchy (BTH), which establishes a multi-scale spatial order for EEG channels. This hierarchical structure enables a redefinition of autoregressive learning as a \"next-scale-time prediction\" problem, effectively capturing both spatial and temporal dynamics. Based on BTH, we design a Topology-Hierarchical Vector Quantized-Variational Autoencoder (THVQ-VAE) for multi-scale tokenization and develop an enhanced Brain Autoregressive (BAR) module with specialized masking strategies for prediction. Through extensive large-scale pre-training on 17 datasets, followed by rigorous validation on 10 downstream datasets spanning 5 distinct tasks, THD-BAR consistently outperforms existing methods. These results highlight the superior generalization and modeling capabilities of our proposed approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jvq8nzOUp8": {
    "title": "Safe + Safe = Unsafe? Exploring How Safe Images Can Be Exploited to Jailbreak Large Vision-Language Models",
    "volume": "poster",
    "abstract": "Recent advances in Large Vision-Language Models (LVLMs) have showcased strong reasoning abilities across multiple modalities, achieving significant breakthroughs in various real-world applications. Despite this great success, the safety guardrail of LVLMs may not cover the unforeseen domains introduced by the visual modality. Existing studies primarily focus on eliciting LVLMs to generate harmful responses via carefully crafted image-based jailbreaks designed to bypass alignment defenses. In this study, we reveal that a safe image can be exploited to achieve the same jailbreak consequence when combined with additional safe images and prompts. This stems from two fundamental properties of LVLMs: universal reasoning capabilities and safety snowball effect. Building on these insights, we propose Safety Snowball Agent (SSA), a novel agent-based framework leveraging agents' autonomous and tool-using abilities to jailbreak LVLMs. SSA operates through two principal stages: (1) initial response generation, where tools generate or retrieve jailbreak images based on potential harmful intents, and (2) harmful snowballing, where refined subsequent prompts induce progressively harmful outputs. Our experiments demonstrate that SSA can use nearly any image to induce LVLMs to produce unsafe content, achieving high success jailbreaking rates against the latest LVLMs. Unlike prior works that exploit alignment flaws, SSA leverages the inherent properties of LVLMs, presenting a profound challenge for enforcing safety in generative multimodal systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wW2qRfhT9d": {
    "title": "Globally Optimal Policy Gradient Algorithms for Reinforcement Learning with PID Control Policies",
    "volume": "poster",
    "abstract": "We develop policy gradient algorithms with global optimality and convergence guarantees for reinforcement learning (RL) with proportional-integral-derivative (PID) parameterized control policies. RL enables learning control policies through direct interaction with a system, without explicit model knowledge that is typically assumed in classical control. The PID policy architecture offers built-in structural advantages, such as superior tracking performance, elimination of steady-state errors, and robustness to model error that have made it a widely adopted paradigm in practice. Despite these advantages, the PID parameterization has received limited attention in the RL literature, and PID control designs continue to rely on heuristic tuning rules without theoretical guarantees. We address this gap by rigorously integrating PID control with RL, offering theoretical guarantees while maintaining the practical advantages that have made PID control ubiquitous in practice. Specifically, we first formulate PID control design as an optimization problem with a control policy that is parameterized by proportional, integral, and derivative components. We derive exact expressions for policy gradients in these parameters, and leverage them to develop both model-based and model-free policy gradient algorithms for PID policies. We then establish gradient dominance properties of the PID policy optimization problem, and provide theoretical guarantees on convergence and global optimality in this setting. Finally, we benchmark the performance of our algorithms on the controlgym suite of environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1ULQvdbZ3X": {
    "title": "Conformal Prediction under Lévy-Prokhorov Distribution Shifts: Robustness to Local and Global Perturbations",
    "volume": "poster",
    "abstract": "Conformal prediction provides a powerful framework for constructing prediction intervals with finite-sample guarantees, yet its robustness under distribution shifts remains a significant challenge. This paper addresses this limitation by modeling distribution shifts using Lévy-Prokhorov (LP) ambiguity sets, which capture both local and global perturbations. We provide a self-contained overview of LP ambiguity sets and their connections to popular metrics such as Wasserstein and Total Variation. We show that the link between conformal prediction and LP ambiguity sets is a natural one: by propagating the LP ambiguity set through the scoring function, we reduce complex high-dimensional distribution shifts to manageable one-dimensional distribution shifts, enabling exact quantification of worst-case quantiles and coverage. Building on this analysis, we construct robust conformal prediction intervals that remain valid under distribution shifts, explicitly linking LP parameters to interval width and confidence levels. Experimental results on real-world datasets demonstrate the effectiveness of the proposed approach",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uB2afsXcdY": {
    "title": "Nyström-Accelerated Primal LS-SVMs: Breaking the O ( a n 3 ) Complexity Bottleneck for Scalable ODEs Learning",
    "volume": "poster",
    "abstract": "A major problem of kernel-based methods (e.g., least squares support vector machines, LS-SVMs) for solving linear/nonlinear ordinary differential equations (ODEs) is the prohibitive $O(an^3)$ ($a=1$ for linear ODEs and 27 for nonlinear ODEs) part of their computational complexity with increasing temporal discretization points $n$. We propose a novel Nyström-accelerated LS-SVMs framework that breaks this bottleneck by reformulating ODEs as primal-space constraints. Specifically, we derive for the first time an explicit Nyström-based mapping and its derivatives from one-dimensional temporal discretization points to a higher $m$-dimensional feature space ($1< m\\le n$), enabling the learning process to solve linear/nonlinear equation systems with $m$-dependent complexity. Numerical experiments on sixteen benchmark ODEs demonstrate: 1) $10-6000$ times faster computation than classical LS-SVMs and physics-informed neural networks (PINNs), 2) comparable accuracy to LS-SVMs ($<0.13\\%$ relative MAE, RMSE, and $\\left \\| y-\\hat{y} \\right \\| _{\\infty } $difference) while maximum surpassing PINNs by 72\\% in RMSE, and 3) scalability to $n=10^4$ time steps with $m=50$ features. This work establishes a new paradigm for efficient kernel-based ODEs learning without significantly sacrificing the accuracy of the solution",
    "checked": false,
    "id": "9946c0bc4b7f5861f586c32d3dc76471f3b4707a",
    "semantic_title": "nyström-accelerated primal ls-svms: breaking the o(an3) complexity bottleneck for scalable odes learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2NLHoWE0eS": {
    "title": "Reasoning as an Adaptive Defense for Safety",
    "volume": "poster",
    "abstract": "Reasoning methods that adaptively allocate test-time compute have advanced LLM performance on easy to verify domains such as math and code. In this work, we study how to utilize this approach to train models that exhibit a degree of robustness to safety vulnerabilities, and show that doing so can provide benefits. We build a recipe called $\\textit{\\textbf{TARS}}$ (Training Adaptive Reasoners for Safety), a reinforcement learning (RL) approach that trains models to reason about safety using chain-of-thought traces and a reward signal that balances safety with task completion. To build TARS, we identify three critical design choices: (1) a ``lightweight'' warmstart SFT stage, (2) a mix of harmful, harmless, and ambiguous prompts to prevent shortcut behaviors such as too many refusals, and (3) a reward function to prevent degeneration of reasoning capabilities during training. Models trained with TARS exhibit adaptive behaviors by spending more compute on ambiguous queries, leading to better safety-refusal trade-offs. They also internally learn to better distinguish between safe and unsafe prompts and attain greater robustness to both white-box (e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our work provides an effective, open recipe for training LLMs against jailbreaks and harmful requests by reasoning per prompt",
    "checked": true,
    "id": "e65b647faf4d6f912873856888297b183bf6dfbf",
    "semantic_title": "reasoning as an adaptive defense for safety",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=WydxWM2xNb": {
    "title": "Partner Modelling Emerges in Recurrent Agents (But Only When It Matters)",
    "volume": "poster",
    "abstract": "Humans are remarkably adept at collaboration, able to infer the strengths and weaknesses of new partners in order to work successfully towards shared goals. To build AI systems with this capability, we must first understand its building blocks: does such flexibility require explicit, dedicated mechanisms for modelling others—or can it emerge spontaneously from the pressures of open-ended cooperative interaction? To investigate this question, we train simple model-free RNN agents to collaborate with a population of diverse partners. Using the 'Overcooked-AI' environment, we collect data from thousands of collaborative teams, and analyse agents' internal hidden states. Despite a lack of additional architectural features, inductive biases, or auxiliary objectives, the agents nevertheless develop structured internal representations of their partners' task abilities, enabling rapid adaptation and generalisation to novel collaborators. We investigated these internal models through probing techniques, and large-scale behavioural analysis. Notably, we find that structured partner modelling emerges when agents can influence partner behaviour by controlling task allocation. Our results show that partner modelling can arise spontaneously in model-free agents—but only under environmental conditions that impose the right kind of social pressure",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q8m0TkIpJZ": {
    "title": "Blending Complementary Memory Systems in Hybrid Quadratic-Linear Transformers",
    "volume": "poster",
    "abstract": "We develop hybrid memory architectures for general-purpose sequence processing neural networks, that combine key-value memory using softmax attention (KV-memory) with fast weight memory through dynamic synaptic modulation (FW-memory)---the core principles of quadratic and linear transformers, respectively. These two memory systems have complementary but individually limited properties: KV-memory offers precise retrieval but is constrained by quadratic complexity in sequence length, while FW-memory supports arbitrarily long sequences and enables more expressive computation but sacrifices precise recall. We propose and compare three methods to blend these two systems into a single memory system, differing in how and when input information is delivered to each system, to leverage the strengths of both. We conduct experiments on general language modeling and retrieval tasks by training 340M- and 1.3B-parameter models from scratch, as well as on synthetic algorithmic tasks designed to precisely illustrate the benefits of certain hybrid methods over others. We also evaluate our hybrid memory systems on reinforcement learning in partially observable environments. Overall, we demonstrate how a well-designed hybrid can overcome the limitations of its individual components, offering new insights into the design principle of neural memory systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=09BPzrMcKg": {
    "title": "Graph Neural Network Based Action Ranking for Planning",
    "volume": "poster",
    "abstract": "We propose a novel approach to learn relational policies for classical planning based on learning to rank actions. We introduce a new graph representation that explicitly captures action information and propose a Graph Neural Network (GNN) architecture augmented with Gated Recurrent Units (GRUs) to learn action rankings. Unlike value-function based approaches that must learn a globally consistent function, our action ranking method only needs to learn locally consistent ranking. Our model is trained on data generated from small problem instances that are easily solved by planners and is applied to significantly larger instances where planning is computationally prohibitive. Experimental results across standard planning benchmarks demonstrate that our action-ranking approach not only achieves better generalization to larger problems than those used in training but also outperforms multiple baselines (value function and action ranking) methods in terms of success rate and plan quality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QL3J1fyAFv": {
    "title": "Length Generalization via Auxiliary Tasks",
    "volume": "poster",
    "abstract": "_Length generalization_, the ability of sequence models to generalize to sequences longer than those encountered during training, remains a key challenge for transformers, especially in tasks requiring algorithmic reasoning. Existing theoretical understanding of length generalization is limited, often providing only asymptotic results or focusing on specific problem classes or architectural variants, while empirical approaches frequently rely on ad hoc and often fragile techniques. In this work we introduce a novel framework for analyzing and proving length generalization bounds under specified, verifiable assumptions. A key outcome of the theory is the identification of a natural set of _auxiliary_ tasks, intricately related to the primary task structure, such that strong performance on these auxiliary tasks, alongside the primary task, provably guarantees length generalization within the framework. This motivates a multi-task training procedure that explicitly optimizes performance on both the primary and the identified auxiliary tasks. Empirical evaluations on a variety of synthetic benchmarks known to be challenging for length generalization, including sequence sorting, and reversal, demonstrate that our proposed method yields significant improvements in generalization to substantially longer sequences",
    "checked": false,
    "id": "ffd97533ac66b7d02ca58c1d5951d5427da0ffd6",
    "semantic_title": "improving length-generalization in transformers via task hinting",
    "citation_count": 12,
    "authors": []
  },
  "https://openreview.net/forum?id=4JihzQXNJn": {
    "title": "Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is Geometry Adaptive",
    "volume": "poster",
    "abstract": "Diffusion models have achieved state-of-the-art performance, demonstrating remarkable generalisation capabilities across diverse domains. However, the mechanisms underpinning these strong capabilities remain only partially understood. A leading conjecture, based on the manifold hypothesis, attributes this success to their ability to adapt to low-dimensional geometric structure within the data. This work provides evidence for this conjecture, focusing on how such phenomena could result from the formulation of the learning problem through score matching. We inspect the role of implicit regularisation by investigating the effect of smoothing minimisers of the empirical score matching objective. Our theoretical and empirical results confirm that smoothing the score function—or equivalently, smoothing in the log-density domain—produces smoothing tangential to the data manifold. In addition, we show that the manifold along which the diffusion model generalises can be controlled by choosing an appropriate smoothing",
    "checked": true,
    "id": "ce5a9c9ddcf1494ef8e93b6c6d1f5b1174c45363",
    "semantic_title": "diffusion models and the manifold hypothesis: log-domain smoothing is geometry adaptive",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=tIW29IpCwG": {
    "title": "Mitigating Hallucination Through Theory-Consistent Symmetric Multimodal Preference Optimization",
    "volume": "poster",
    "abstract": "Direct Preference Optimization (DPO) has emerged as an effective approach for mitigating hallucination in Multimodal Large Language Models (MLLMs). Although existing methods have achieved significant progress by utilizing vision-oriented contrastive objectives for enhancing MLLMs' attention to visual inputs and hence reducing hallucination, they suffer from non-rigorous optimization objective function and indirect preference supervision. To address these limitations, we propose a Symmetric Multimodal Preference Optimization (SymMPO), which conducts symmetric preference learning with direct preference supervision (i.e., response pairs) for visual understanding enhancement, while maintaining rigorous theoretical alignment with standard DPO. In addition to conventional ordinal preference learning, SymMPO introduces a preference margin consistency loss to quantitatively regulate the preference gap between symmetric preference pairs. Comprehensive evaluation across five benchmarks demonstrate SymMPO's superior performance, validating its effectiveness in hallucination mitigation of MLLMs",
    "checked": true,
    "id": "b4ed1e2749108b8d16b7765d2986423391d15909",
    "semantic_title": "mitigating hallucination through theory-consistent symmetric multimodal preference optimization",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=FUBaZDMOFj": {
    "title": "Efficient Kernelized Learning in Polyhedral Games beyond Full Information: From Colonel Blotto to Congestion Games",
    "volume": "poster",
    "abstract": "We examine the problem of efficiently learning coarse correlated equilibria (CCE) in polyhedral games, that is, normal-form games with an exponentially large number of actions per player and an underlying combinatorial structure—such as the classic Colonel Blotto game or congestion games. Achieving computational efficiency in this setting requires learning algorithms whose regret and per-iteration complexity scale at most polylogarithmically with the size of the players' action sets. This challenge has recently been addressed in the full-information setting, primarily through the use of kernelization; however, in the more realistic partial information setting, the situation is much more challenging, and existing approaches result in suboptimal and impractical runtime complexity to learn CCE. We address this gap via a novel kernelization-based framework for payoff-based learning in polyhedral games, which we then apply to certain key classes of polyhedral games—namely Colonel Blotto, graphic matroid and network congestion games. In so doing, we obtain a range of computationally efficient payoff-based learning algorithms which significantly improve upon prior work in terms of the runtime for learning CCE",
    "checked": false,
    "id": "29d7cc3168cc213dc3ad586a754821666065f0bf",
    "semantic_title": "efficient kernelized learning in polyhedral games beyond full-information: from colonel blotto to congestion games",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BumyAsRGGm": {
    "title": "Adjusted Count Quantification Learning on Graphs",
    "volume": "poster",
    "abstract": "*Quantification learning* is the task of predicting the label distribution of a set of instances. We study this problem in the context of graph-structured data, where the instances are vertices. Previously, this problem has only been addressed via node clustering methods. In this paper, we extend the popular *Adjusted Classify & Count* (ACC) method to graphs. We show that the prior probability shift assumption upon which ACC relies is often not applicable to graph quantification problems. To address this issue, we propose structural importance sampling (SIS), the first graph quantification method that is applicable under (structural) covariate shift. Additionally, we propose Neighborhood-aware ACC, which improves quantification in the presence of non-homophilic edges. We show the effectiveness of our techniques on multiple graph quantification tasks",
    "checked": true,
    "id": "d567502ad24a1ca103e8179072611d32b0ad8de5",
    "semantic_title": "adjusted count quantification learning on graphs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LtIk57nS8v": {
    "title": "Controlling The Spread of Epidemics on Networks with Differential Privacy",
    "volume": "poster",
    "abstract": "Designing effective strategies for controlling epidemic spread by vaccination is an important question in epidemiology, especially in the early stages when vaccines are limited. This is a challenging question when the contact network is very heterogeneous, and strategies based on controlling network properties, such as the degree and spectral radius, have been shown to be effective. Implementation of such strategies requires detailed information on the contact structure, which might be sensitive in many applications. Our focus here is on choosing effective vaccination strategies when the edges are sensitive and differential privacy guarantees are needed. Our main contributions are $(\\varepsilon,\\delta)$-differentially private algorithms for designing vaccination strategies by reducing the maximum degree and spectral radius. Our key technique is a private algorithm for the multi-set multi-cover problem, which we use for controlling network properties. We evaluate privacy-utility tradeoffs of our algorithms on multiple synthetic and real-world networks, and show their effectiveness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QD06Qv7O0P": {
    "title": "Deep RL Needs Deep Behavior Analysis: Exploring Implicit Planning by Model-Free Agents in Open-Ended Environments",
    "volume": "poster",
    "abstract": "Understanding the behavior of deep reinforcement learning (DRL) agents—particularly as task and agent sophistication increase—requires more than simple comparison of reward curves, yet standard methods for behavioral analysis remain underdeveloped in DRL. We apply tools from neuroscience and ethology to study DRL agents in a novel, complex, partially observable environment, ForageWorld, designed to capture key aspects of real-world animal foraging—including sparse, depleting resource patches, predator threats, and spatially extended arenas. We use this environment as a platform for applying joint behavioral and neural analysis to agents, revealing detailed, quantitatively grounded insights into agent strategies, memory, and planning. Contrary to common assumptions, we find that model-free RNN-based DRL agents can exhibit structured, planning-like behavior purely through emergent dynamics—without requiring explicit memory modules or world models. Our results show that studying DRL agents like animals—analyzing them with neuroethology-inspired tools that reveal structure in both behavior and neural dynamics—uncovers rich structure in their learning dynamics that would otherwise remain invisible. We distill these tools into a general analysis framework linking core behavioral and representational features to diagnostic methods, which can be reused for a wide range of tasks and agents. As agents grow more complex and autonomous, bridging neuroscience, cognitive science, and AI will be essential—not just for understanding their behavior, but for ensuring safe alignment and maximizing desirable behaviors that are hard to measure via reward. We show how this can be done by drawing on lessons from how biological intelligence is studied",
    "checked": true,
    "id": "5d3865a0d524da972d36a22b240938e14e0292f4",
    "semantic_title": "deep rl needs deep behavior analysis: exploring implicit planning by model-free agents in open-ended environments",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=YghiOusmvw": {
    "title": "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity",
    "volume": "poster",
    "abstract": "Recent generations of frontier language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood. Current evaluations primarily focus on established mathematical and coding benchmarks, emphasizing final answer accuracy. However, this evaluation paradigm often suffers from data contamination and does not provide insights into the reasoning traces' structure and quality. In this work, we systematically investigate these gaps with the help of controllable puzzle environments that allow precise manipulation of compositional complexity while maintaining consistent logical structures. This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into how LRMs ``think''. Through extensive experimentation across diverse puzzles, we show that frontier LRMs face a complete accuracy collapse beyond certain complexities. Moreover, they exhibit a counterintuitive scaling limit: their reasoning effort increases with problem complexity up to a point, then declines despite having an adequate token budget. By comparing LRMs with their standard LLM counterparts under equivalent inference compute, we identify three performance regimes: (1) low-complexity tasks where standard models surprisingly outperform LRMs, (2) medium-complexity tasks where additional thinking in LRMs demonstrates advantage, and (3) high-complexity tasks where both models experience complete collapse. We found that LRMs have limitations in exact computation: they fail to use explicit algorithms and reason inconsistently across scales and problems. We also investigate the reasoning traces in more depth, studying the patterns of explored solutions and analyzing the models' computational behavior, shedding light on their strengths, limitations, and ultimately raising questions about the nature for their reasoning capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IQlcfc40Ja": {
    "title": "Estimating Interventional Distributions with Uncertain Causal Graphs through Meta-Learning",
    "volume": "poster",
    "abstract": "In scientific domains---from biology to the social sciences---many questions boil down to \\textit{What effect will we observe if we intervene on a particular variable?} If the causal relationships (e.g.~a causal graph) are known, its possible to estimate the intervention distributions. In the absence of this domain knowledge, the causal structure must be discovered from the available observational data. However, observational data are often compatible with multiple causal graphs, making methods that commit to a single structure prone to overconfidence. A principled way to manage this structural uncertainty is via Bayesian inference, which averages over a posterior distribution on possible causal structures and functional mechanisms. Unfortunately, the number of causal structures grows super-exponentially with the number of nodes in the graph, making computations intractable. We propose to circumvent these challenges by using meta-learning to create an end-to-end model: the Model-Averaged Causal Estimation Transformer Neural Process (MACE-TNP). The model is trained to predict the Bayesian model-averaged interventional posterior distribution, and its end-to-end nature bypasses the need for expensive calculations. Empirically, we demonstrate that MACE-TNP outperforms strong Bayesian baselines. Our work established meta-learning as a flexible and scalable paradigm for approximating complex Bayesian causal inference, that can be scaled to increasingly challenging settings in the future",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R5EBtNE2Y9": {
    "title": "HeavyWater and SimplexWater: Distortion-free LLM Watermarks for Low-Entropy Distributions",
    "volume": "poster",
    "abstract": "Large language model (LLM) watermarks enable authentication of text provenance, curb misuse of machine-generated text, and promote trust in AI systems. Current watermarks operate by changing the next-token predictions output by an LLM. The updated (i.e., watermarked) predictions depend on random side information produced, for example, by hashing previously generated tokens. LLM watermarking is particularly challenging in low-entropy generation tasks -- such as coding -- where next-token predictions are near-deterministic. In this paper, we propose an optimization framework for watermark design. Our goal is to understand how to most effectively use random side information in order to maximize the likelihood of watermark detection and minimize the distortion of generated text. Our analysis informs the design of two new watermarks: HeavyWater and SimplexWater. Both watermarks are tunable, gracefully trading-off between detection accuracy and text distortion. They can also be applied to any LLM and are agnostic to side information generation. We examine the performance of HeavyWater and SimplexWater through several benchmarks, demonstrating that they can achieve high watermark detection accuracy with minimal compromise of text generation quality, particularly in the low-entropy regime. Our theoretical analysis also reveals surprising new connections between LLM watermarking and coding theory",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pzAeKALSfX": {
    "title": "Self Iterative Label Refinement via Robust Unlabeled Learning",
    "volume": "poster",
    "abstract": "Recent advances in large language models (LLMs) have yielded impressive performance on various tasks, yet they often depend on high-quality feedback that can be costly. Self-refinement methods attempt to leverage LLMs' internal evaluation mechanisms with minimal human supervision; however, these approaches frequently suffer from inherent biases and overconfidence, especially in domains where the models lack sufficient internal knowledge, resulting in performance degradation. As an initial step toward enhancing self-refinement for broader applications, we introduce an iterative refinement pipeline that employs the Unlabeled-Unlabeled learning framework to improve LLM-generated pseudo-labels for classification tasks. By exploiting two unlabeled datasets with differing positive class ratios, our approach iteratively denoises and refines the initial pseudo-labels, thereby mitigating the adverse effects of internal biases with minimal human supervision. Evaluations on diverse datasets, including low-resource language corpora, patent classifications, and protein structure categorizations, demonstrate that our method consistently outperforms both initial LLM's classification performance and the self-refinement approaches by cutting-edge models (e.g., GPT-4o and DeepSeek-R1). Moreover, we experimentally confirm that our refined classifier facilitates effective post-training alignment for safety in LLMs and demonstrate successful self-refinement in generative tasks as well. Our code is available at https://github.com/HikaruAsano/self-iterative-label-refinement",
    "checked": true,
    "id": "99b049973f0cb477e50570b699f313e102ebd89f",
    "semantic_title": "self iterative label refinement via robust unlabeled learning",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=1UL4dxvfcJ": {
    "title": "Measuring the Faithfulness of Thinking Drafts in Large Reasoning Models",
    "volume": "poster",
    "abstract": "Large Reasoning Models (LRMs) have significantly enhanced their capabilities in complex problem-solving by introducing a thinking draft that enables multi-path Chain-of-Thought explorations before producing final answers. Ensuring the faithfulness of these intermediate reasoning processes is crucial for reliable monitoring, interpretation, and effective control. In this paper, we propose a systematic counterfactual intervention framework to rigorously evaluate *thinking draft faithfulness*. Our approach focuses on two complementary dimensions: **(1) Intra-Draft Faithfulness**, which assesses whether individual reasoning steps causally influence subsequent steps and the final draft conclusion through counterfactual step insertions; and **(2) Draft-to-Answer Faithfulness**, which evaluates whether final answers are logically consistent with and dependent on the thinking draft, by perturbing the draft's concluding logic. We conduct extensive experiments across six state-of-the-art LRMs. Our findings show that current LRMs demonstrate selective faithfulness to intermediate reasoning steps and frequently fail to faithfully align with the draft conclusions. These results underscore the need for more faithful and interpretable reasoning in advanced LRMs",
    "checked": true,
    "id": "5ccbcfc99976fc6d0509fcaba2072c3b1faaf3fd",
    "semantic_title": "measuring the faithfulness of thinking drafts in large reasoning models",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=7AGXSlXcK6": {
    "title": "Predictability Enables Parallelization of Nonlinear State Space Models",
    "volume": "poster",
    "abstract": "The rise of parallel computing hardware has made it increasingly important to understand which nonlinear state space models can be efficiently parallelized. Recent advances have shown that evaluating a state space model can be recast as solving a parallelizable optimization problem, and sometimes this approach yields dramatic speed-ups in evaluation time. However, the factors that govern the difficulty of these optimization problems remain unclear, limiting the larger adoption of the technique. In this work, we establish a precise relationship between the dynamics of a nonlinear system and the conditioning of its corresponding optimization formulation. We show that the predictability of a system, defined as the degree to which small perturbations in state influence future behavior, directly governs the number of optimization steps required for evaluation. In predictable systems, the state trajectory can be computed in $\\mathcal{O}((\\log T)^2)$ time, where $T$ is the sequence length, a major improvement over the conventional sequential approach. In contrast, chaotic or unpredictable systems exhibit poor conditioning, with the consequence that parallel evaluation converges too slowly to be useful. Importantly, our theoretical analysis demonstrates that for predictable systems, the optimization problem is always well-conditioned, whereas for unpredictable systems, the conditioning degrades exponentially as a function of the sequence length. We validate our claims through extensive experiments, providing practical guidance on when nonlinear dynamical systems can be efficiently parallelized, and highlighting predictability as a key design principle for parallelizable models",
    "checked": true,
    "id": "9c51e133dade2a86b86297a168f156c608b79a8a",
    "semantic_title": "predictability enables parallelization of nonlinear state space models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=ipEqdzO2IT": {
    "title": "Robust and Diverse Multi-Agent Learning via Rational Policy Gradient",
    "volume": "poster",
    "abstract": "Adversarial optimization algorithms that explicitly search for flaws in agents' policies have been successfully applied to finding robust and diverse policies in the context of multi-agent learning. However, the success of adversarial optimization has been largely limited to zero-sum settings because its naive application in cooperative settings leads to a critical failure mode: agents are irrationally incentivized to *self-sabotage*, blocking the completion of tasks and halting further learning. To address this, we introduce *Rationality-preserving Policy Optimization (RPO)*, a formalism for adversarial optimization that avoids self-sabotage by ensuring agents remain *rational*—that is, their policies are optimal with respect to some possible partner policy. To solve RPO, we develop *Rational Policy Gradient (RPG)*, which trains agents to maximize their own reward in a modified version of the original game in which we use *opponent shaping* techniques to optimize the adversarial objective. RPG enables us to extend a variety of existing adversarial optimization algorithms that, no longer subject to the limitations of self-sabotage, can find adversarial examples, improve robustness and adaptability, and learn diverse policies. We empirically validate that our approach achieves strong performance in several popular cooperative and general-sum environments. Our project page can be found at https://rational-policy-gradient.github.io",
    "checked": true,
    "id": "05f71f2564d7f8c2861145186fbb92ca4abc253b",
    "semantic_title": "robust and diverse multi-agent learning via rational policy gradient",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OyX9cC9WaV": {
    "title": "Towards foundational LiDAR world models with efficient latent flow matching",
    "volume": "poster",
    "abstract": "LiDAR-based world models offer more structured and geometry-aware representations than their image-based counterparts. However, existing LiDAR world models are narrowly trained; each model excels only in the domain for which it was built. This raises a critical question: can we develop LiDAR world models that exhibit strong transferability across multiple domains? To answer this, we conduct the first systematic domain transfer study across three demanding scenarios: (i) outdoor to indoor generalization, (ii) sparse- to dense-beam adaptation, and (iii) non-semantic to semantic transfer. Given different amounts of fine-tuning data, our experiments show that a single pretrained model can achieve up to 11\\% absolute improvement (83\\% relative) over training from scratch and outperforms training from scratch in 30/36 of our comparisons. This transferability significantly reduces the reliance on manually annotated data for semantic occupancy forecasting: our method exceeds previous baselines with only 5\\% of the labeled training data of prior work. We also observed inefficiencies of current generative-model-based LiDAR world models, mainly through their under-compression of LiDAR data and inefficient training objectives. To address these issues, we propose a latent conditional flow matching (CFM)-based framework that achieves state-of-the-art reconstruction accuracy using only half the training data and a compression ratio 6 times higher than that of prior methods. Our model also achieves SOTA performance on semantic occupancy forecasting while being 1.98x-23x more computationally efficient (a 1.1x-3.9x FPS speedup) than previous methods",
    "checked": true,
    "id": "bf92d7803a4fd2a20a46c46fef619803c832f6eb",
    "semantic_title": "towards foundational lidar world models with efficient latent flow matching",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=ysgt21bQAM": {
    "title": "Transformers Learn Faster with Semantic Focus",
    "volume": "poster",
    "abstract": "Various forms of sparse attention have been explored to mitigate the quadratic computational and memory cost of the attention mechanism in transformers. We study sparse transformers not through a lens of efficiency but rather in terms of learnability and generalization. Empirically studying a range of attention mechanisms, we find that input-dependent sparse attention models appear to converge faster and generalize better than standard attention models, while input-agnostic sparse attention models show no such benefits -- a phenomenon that is robust across architectural and optimization hyperparameter choices. This can be interpreted as demonstrating that concentrating a model's \"semantic focus\" with respect to the tokens currently being considered (in the form of input-dependent sparse attention) accelerates learning. We develop a theoretical characterization of the conditions that explain this behavior. We establish a connection between the stability of the standard softmax and the loss function's Lipschitz properties, then show how sparsity affects the stability of the softmax and the subsequent convergence and generalization guarantees resulting from the attention mechanism. This allows us to theoretically establish that input-agnostic sparse attention does not provide any benefits. We also characterize conditions when semantic focus (input-dependent sparse attention) can provide improved guarantees, and we validate that these conditions are in fact met in our empirical evaluations",
    "checked": true,
    "id": "9e99db7707fea49d9f1b9a248a5681dbefc0b3b4",
    "semantic_title": "transformers learn faster with semantic focus",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1fycT4ZRf1": {
    "title": "Scalable Signature Kernel Computations via Local Neumann Series Expansions",
    "volume": "poster",
    "abstract": "The signature kernel is a recent state-of-the-art tool for analyzing high-dimensional sequential data, valued for its theoretical guarantees and strong empirical performance. In this paper, we present a novel method for efficiently computing the signature kernel of long, high-dimensional time series via adaptively truncated recursive local power series expansions. Building on the characterization of the signature kernel as the solution of a Goursat PDE, our approach employs tilewise Neumann‐series expansions to derive rapidly converging power series approximations of the signature kernel that are locally defined on subdomains and propagated iteratively across the entire domain of the Goursat solution by exploiting the geometry of the time series. Algorithmically, this involves solving a system of interdependent Goursat PDEs via adaptively truncated local power series expansions and recursive propagation of boundary conditions along a directed graph in a topological ordering. This method strikes an effective balance between computational cost and accuracy, achieving substantial performance improvements over state-of-the-art approaches for computing the signature kernel. It offers (a) adjustable and superior accuracy, even for time series with very high roughness; (b) drastically reduced memory requirements; and (c) scalability to efficiently handle very long time series (e.g., with up to one million data points or more) on a single GPU. As demonstrated in our benchmarks, these advantages make our method particularly well-suited for rough-path-assisted machine learning, financial modeling, and signal processing applications involving very long and highly volatile sequential data",
    "checked": false,
    "id": "c21ddf7831fc8b188d6b059468d525ad500813fb",
    "semantic_title": "scalable signature kernel computations for long time series via local neumann series expansions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T1V8BJO0iG": {
    "title": "Accelerating RL for LLM Reasoning with Optimal Advantage Regression",
    "volume": "poster",
    "abstract": "Reinforcement learning (RL) has emerged as a powerful tool for fine-tuning large language models (LLMs) to improve complex reasoning abilities. However, state-of-the-art policy optimization methods often suffer from high computational overhead and memory consumption, primarily due to the need for multiple generations per prompt and the reliance on critic networks or advantage estimates of the current policy. In this paper, we propose $A^\\star$-PO, a novel two-stage policy optimization framework that directly approximates the optimal advantage function and enables efficient training of LLMs for reasoning tasks. In the first stage, we leverage offline sampling from a reference policy to estimate the optimal value function $V^\\star$, eliminating the need for costly online value estimation. In the second stage, we perform on-policy updates using a simple least-squares regression loss with only a single generation per prompt. Theoretically, we establish performance guarantees and prove that the KL-regularized RL objective can be optimized without requiring complex exploration strategies. Empirically, $A^\\star$-PO achieves competitive performance across a wide range of mathematical reasoning benchmarks, while reducing training time by up to 2$\\times$ and peak memory usage by over 30\\% compared to PPO, GRPO, and REBEL. Implementation of $A^\\star$-PO can be found at https://github.com/ZhaolinGao/A-PO",
    "checked": true,
    "id": "03e385b1a6fdb7bef5612feea44e44d5efd45bcc",
    "semantic_title": "accelerating rl for llm reasoning with optimal advantage regression",
    "citation_count": 7,
    "authors": []
  },
  "https://openreview.net/forum?id=Cl45XdnCoK": {
    "title": "SpecMAS: A Multi-Agent System for Self-Verifying System Generation via Formal Model Checking",
    "volume": "poster",
    "abstract": "We present SpecMAS, a novel multi-agent system that autonomously constructs and formally verifies executable system models from natural language specifications. Given a Standard Operating Procedure (SOP) describing a target system, SpecMAS parses the specification, identifies relevant operational modes, variables, transitions, and properties, and generates a formal model in NuSMV code syntax, an industry-standard symbolic model checker. A dedicated reasoning agent extracts both explicit and implicit properties from the SOP, and verification is performed via temporal logic model checking. If any properties fail to verify, an autonomous debugging agent analyzes counterexamples and iteratively corrects the model until all properties are satisfied. This closed-loop system design guarantees provable correctness by construction and advances the state of the art in automated, interpretable, and deployable verification pipelines. We demonstrate the generality, correctness, and practical feasibility of SpecMAS across a set of representative case studies and propose a new benchmark dataset for the evaluation and comparison of model checking performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yokmj4lfwp": {
    "title": "Learning-Augmented Facility Location Mechanisms for the Envy Ratio Objective",
    "volume": "poster",
    "abstract": "The augmentation of algorithms with predictions of the optimal solution, such as from a machine-learning algorithm, has garnered significant attention in recent years, particularly in facility location problems. Moving beyond the traditional focus on utilitarian and egalitarian objectives, we design learning-augmented facility location mechanisms for the envy ratio objective, a fairness metric defined as the maximum ratio between the utilities of any two agents. For the deterministic setting, we propose a mechanism which utilizes predictions to achieve $\\alpha$-consistency and $\\frac{\\alpha}{\\alpha - 1}$-robustness for a selected parameter $\\alpha \\in [1,2]$, and prove its optimality. We also resolve open questions raised by Ding et al. [2020], devising a randomized mechanism without predictions to improve upon the best-known approximation ratio from $2$ to $1.8944$. Building upon these advancements, we construct a novel randomized mechanism which incorporates predictions to achieve improved performance guarantees",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9RelLjiaHw": {
    "title": "Distributionally Robust Feature Selection",
    "volume": "poster",
    "abstract": "We study the problem of selecting limited features to observe such that models trained on them can perform well simultaneously across multiple subpopulations. This problem has applications in settings where collecting each feature is costly, e.g. requiring adding survey questions or physical sensors, and we must be able to use the selected features to create high-quality downstream models for different populations. Our method frames the problem as a continuous relaxation of traditional variable selection using a noising mechanism, without requiring backpropagation through model training processes. By optimizing over the variance of a Bayes-optimal predictor, we develop a model-agnostic framework that balances overall performance of downstream prediction across populations. We validate our approach through experiments on both synthetic datasets and real-world data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EgH5WYB6my": {
    "title": "Seeds of Structure: Patch PCA Reveals Universal Compositional Cues in Diffusion Models",
    "volume": "poster",
    "abstract": "Diffusion models transform random noise into images of remarkable fidelity, yet the structure of this noise-to-image map remains largely unexplored. We investigate this relationship using patch-wise Principal Component Analysis (PCA) and empirically demonstrate that low-frequency components of the initial noise predominantly influence the compositional structure of generated images. Our analyses reveal that noise seeds inherently contain universal compositional cues, evident when identical seeds produce images with similar structural attributes across different datasets and model architectures. Leveraging these insights, we develop and theoretically justify a simple yet effective Patch PCA denoiser that extracts underlying structure from noise using only generic natural image statistics. The robustness of these structural cues is observed to persist across both pixel-space models and latent diffusion models, highlighting their fundamental nature. Finally, we introduce a zero-shot editing method that enables injecting compositional control over generated images, providing an intuitive approach to guided generation without requiring model fine-tuning or additional training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8gr3laSo3P": {
    "title": "Heterogeneous Diffusion Structure Inference for Network Cascade",
    "volume": "poster",
    "abstract": "A cascade over a network refers to the diffusion process where behavior changes occurring in one part of an interconnected population lead to a series of sequential changes throughout the entire population. In recent years, there has been a surge in interest and efforts to understand and model cascade mechanisms since they motivate many significant research topics across different disciplines. The propagation structure of cascades is governed by underlying diffusion networks that are often hidden. Inferring diffusion networks thus enables interventions in cascading process to maximize information propagation and provides insights into the Granger causality of interaction mechanisms among individuals. In this project, we propose a novel double network mixture model for inferring latent diffusion network in presence of strong cascade heterogeneity. The new model represents cascade pathways as a distributional mixture over diffusion networks that capture different cascading patterns at the population level. We develop a data-driven optimization method to infer diffusion networks using only visible temporal cascade records, avoiding the need to model complex and heterogeneous individual states. Both statistical and computational guarantees are established for the proposed method. We apply the proposed model to analyze research topic cascades in social sciences across U.S. universities and uncover the latent research topic diffusion network among top U.S. social science programs",
    "checked": false,
    "id": "90c6a16507de6be52350b8c57d307eae6ac63e09",
    "semantic_title": "inferring diffusion structures of heterogeneous network cascade",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eV2Y8Gt6JY": {
    "title": "Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models",
    "volume": "poster",
    "abstract": "Vision–language models (VLMs) excel at zero-shot inference but often degrade under test-time domain shifts. For this reason, episodic test-time adaptation strategies have recently emerged as powerful techniques for adapting VLMs to a single unlabeled image. However, existing adaptation strategies, such as test-time prompt tuning, typically require backpropagating through large encoder weights or altering core model components. In this work, we introduce \\textbf{S}pectrum-Aware \\textbf{T}est-Time \\textbf{S}teering (\\textbf{STS}), a \\textit{lightweight adaptation framework} that extracts a spectral subspace from the textual embeddings to define principal semantic directions, and learns to steer latent representations in a spectrum-aware manner by adapting a small number of per-sample shift parameters to minimize entropy across augmented views. STS operates entirely at inference in the latent space, without backpropagation through or modification of the frozen encoders. Building on standard evaluation protocols, our comprehensive experiments demonstrate that STS largely surpasses or compares favorably against state-of-the-art test-time adaptation methods, while introducing only a handful of additional parameters and achieving inference speeds up to 8× faster with a 12× smaller memory footprint than conventional test-time prompt tuning. The code is available at \\url{https://github.com/kdafnis/STS}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oLJMsGMfqr": {
    "title": "Data Selection Matters: Towards Robust Instruction Tuning of Large Multimodal Models",
    "volume": "poster",
    "abstract": "Selecting a compact subset of visual instruction–following data has emerged as an effective way to align large multimodal models with human intentions while avoiding the high cost of full-dataset training. Yet we observe that both full-data training and existing state-of-the-art data selection methods tend to inherit underlying dataset biases such as position bias and spurious correlations, leading to biased model behaviors. To address this issue, we introduce ARDS, a robustness-aware targeted visual instruction-selection framework that explicitly mitigates these weaknesses, sidestepping the need for access to downstream data or time-consuming gradient computation. Specifically, we first identify the worst-case evaluation subgroups through visual and textual task-specific perturbations. The robust training mixture is then constructed by prioritizing samples that are semantically closer to these subgroups in a rich multimodal embedding space. Extensive experiments demonstrate that ARDS substantially boosts both robustness and data efficiency for visual instruction tuning. We also showcase that the robust mixtures produced with a smaller model transfer effectively to larger architectures. Our code and selected datasets that have been demonstrated transferable across models are available at https://github.com/xyang583/ARDS",
    "checked": false,
    "id": "2ad5e1e36563882b7acc5df0a1d703a8c42dc5be",
    "semantic_title": "towards robust instruction tuning on multimodal large language models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=uC3DI4YPRv": {
    "title": "Let the LLM Stick to Its Strengths: Learning to Route Economical LLM",
    "volume": "poster",
    "abstract": "Recently, test-time scaling of Large Language Models (LLMs) has emerged as a practical alternative to parameter and data scaling. Reasoning tasks often require large-scale, RLVR-based LLMs, while more economical LLMs can handle simpler tasks. Routing an LLM tailored to *suitability* (*i.e.*, capability and cost) ensures usability and efficiency. We introduce LLMRec, which routes the most suitable LLM to the user query without pre-inference on the candidate LLM zoo. It pioneeringly reframes the LLM routing problem as a comprehensive recommendation system (RecSys) task. Our core insight is that an LLM's suitability for a query is a complex, latent signal equal to user-item preference. LLMRec systematically engineers features for candidate LLMs (intrinsic attributes and capability distributions), queries (general semantics and meta-dimensional info), and context (inference type, cost budgets). It also incorporates behavioral features to learn high-order interactions. LLMRec is designed to generalize to out-of-domain datasets and adapt to new LLMs as the model zoo evolves. We define the metric with the Pareto frontier under user-specified cost budgets. Across six datasets, LLMRec achieves an average cost reduction of over 38% while maintaining accuracy and consistently outperforming baselines in converging toward the Pareto frontier",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uVarpp7fhU": {
    "title": "Scaling Offline RL via Efficient and Expressive Shortcut Models",
    "volume": "poster",
    "abstract": "Diffusion and flow models have emerged as powerful generative approaches capable of modeling diverse and multimodal behavior. However, applying these models to offline RL remains challenging due to the iterative nature of their noise sampling processes, making policy optimization difficult. In this paper, we introduce Scalable Offline Reinforcement Learning (SORL), a new offline RL algorithm that leverages shortcut models – a novel class of generative models – to scale both training and inference. SORL's policy can capture complex data distributions and can be trained simply and efficiently in a one-stage training procedure. At test time, SORL supports both sequential and parallel inference scaling by using the learned Q-function as a verifier. We demonstrate that SORL achieves strong performance across a range of offline RL tasks and exhibits positive scaling behavior with increased test-time compute",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eYVFs6TfQ0": {
    "title": "Efficient Allocation of Working Memory Resource for Utility Maximization in Humans and Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "Working memory (WM) supports the temporary retention of task-relevant information. It is limited in capacity and inherently noisy. The ability to flexibly allocate WM resources is a hallmark of adaptive behavior. While it is well established that WM resources can be prioritized via selective attention, whether they can be allocated based on reward incentive alone remains under debate—raising open questions about whether humans can efficiently allocate WM resources based on utility. To address this, we conducted behavioral experiments using orientations as stimuli. Participants first learned stimulus–reward associations and then performed a delayed estimate WM task. We found that WM precision, indexed by the variability of memory reports, reflected both natural stimulus priors and utility-based allocation. The effects from reward and prior on memory variability both grew over time, indicating their effects in stabilizing memory representations. In contrast, memory bias was largely unaffected by time or reward. To interpret these findings, we extended efficient coding theory by incorporating time and reformulating the objective from minimizing estimation loss to maximizing expected utility. We showed that the behavioral results were consistent with an observer that efficiently allocates WM resources over time to maximize utility. Lastly, we trained recurrent neural networks (RNNs) to perform the same WM task under a 2×2 design: prior (uniform vs. natural) × reward policy (baseline vs. reward context). Human-like behaviors emerged in RNNs: memory was more stable (lower variability) for stimuli associated with higher probability or rewards, and these effects increased over time. Transfer learning showed that recurrent dynamics were crucial for adapting to different priors and reward policies. Together, these results provide converging behavioral and computational evidence that WM resource allocation is shaped by environmental statistics and rewards, offering insight into how intelligent systems can dynamically optimize memory for utility under resource constraints",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X9SNL203cV": {
    "title": "Evaluating LLMs in Open-Source Games",
    "volume": "poster",
    "abstract": "Large Language Models' (LLMs) programming capabilities enable their participation in \\textit{open-source games}: a game-theoretic setting in which players submit computer programs in lieu of actions. These programs offer numerous advantages, including interpretability, inter-agent transparency, and formal verifiability; additionally, they enable \\textit{program equilibria}, solutions that leverage the transparency of code and are inaccessible within normal-form settings. We evaluate the capabilities of leading open- and closed-weight LLMs to predict and classify program strategies and evaluate features of the approximate program equilibria reached by LLM agents in dyadic and evolutionary settings. We identify the emergence of payoff-maximizing, cooperative, and deceptive strategies, characterize the adaptation of mechanisms within these programs over repeated open-source games, and analyze their comparative evolutionary fitness. We find that open-source games serve as a viable environment to study and steer the emergence of cooperative strategy in multi-agent dilemmas",
    "checked": false,
    "id": "7d884f1ff991eb9fff7bf31fa006196e58934b8a",
    "semantic_title": "deploying and evaluating llms to program service mobile robots",
    "citation_count": 51,
    "authors": []
  },
  "https://openreview.net/forum?id=Es4s9dtCjR": {
    "title": "Constrained Discrete Diffusion",
    "volume": "poster",
    "abstract": "Discrete diffusion models are a class of generative models that construct sequences by progressively denoising samples from a categorical noise distribution. Beyond their rapidly growing ability to generate coherent natural language, these models present a new and important opportunity to enforce sequence-level constraints, a capability that current autoregressive models cannot natively provide. This paper capitalizes on this opportunity by introducing $\\textit{Constrained Discrete Diffusion}$ (CDD), a novel integration of differentiable constraint optimization within the diffusion process to ensure adherence to constraints, logic rules, or safety requirements for generated sequences. Unlike conventional text generators that often rely on post-hoc filtering or model retraining for controllable generation, CDD directly imposes constraints into the discrete diffusion sampling process, resulting in a training-free and effective approach. Experiments in toxicity-controlled text generation, property-constrained molecule design, and instruction-constrained text completion demonstrate that CDD achieves $\\textit{zero constraint violations}$ in a diverse array of tasks while preserving fluency, novelty, and coherence, and outperforming autoregressive and existing discrete diffusion approaches",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OOQMahom6P": {
    "title": "Stable Coresets via Posterior Sampling: Aligning Induced and Full Loss Landscapes",
    "volume": "poster",
    "abstract": "As deep learning models continue to scale, the growing computational demands have amplified the need for effective coreset selection techniques. Coreset selection aims to accelerate training by identifying small, representative subsets of data that approximate the performance of the full dataset. Among various approaches, gradient-based methods stand out due to their strong theoretical underpinnings and practical benefits, particularly under limited data budgets. However, these methods face challenges such as naïve stochastic gradient descent (SGD) acting as a surprisingly strong baseline and the breakdown of representativeness due to loss curvature mismatches over time. In this work, we propose a novel framework that addresses these limitations. First, we establish a connection between posterior sampling and loss landscapes, enabling robust coreset selection even in high-data-corruption scenarios. Second, we introduce a smoothed loss function based on posterior sampling onto the model weights, enhancing stability and generalization while maintaining computational efficiency. We also present a novel convergence analysis for our sampling-based coreset selection method. Finally, through extensive experiments, we demonstrate how our approach achieves faster training and enhanced generalization across diverse datasets than the current state of the art",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9GzyCtlngK": {
    "title": "Compute-Optimal Scaling for Value-Based Deep RL",
    "volume": "poster",
    "abstract": "As models grow larger and training them becomes expensive, it becomes increasingly important to scale training recipes not just to larger models and more data, but to do so in a compute-optimal manner that extracts maximal performance per unit of compute. While such scaling has been well studied for language modeling, reinforcement learning (RL) has received less attention in this regard. In this paper, we investigate compute scaling for online, value-based deep RL. These methods present two primary axes for compute allocation: model capacity and the update-to-data (UTD) ratio. Given a fixed compute budget, we ask: how should resources be partitioned across these axes to maximize data efficiency? Our analysis reveals a nuanced interplay between model size, batch size, and UTD. In particular, we identify a phenomenon we call TD-overfitting: increasing the batch quickly harms Q-function accuracy for small models, but this effect is absent in large models, enabling effective use of large batch size at scale. We provide a mental model for understanding this phenomenon and build guidelines for choosing batch size and UTD to optimize compute usage. Our findings provide a grounded starting point for compute-optimal scaling in deep RL, mirroring studies in supervised learning but adapted to TD learning. Project page: https://value-scaling.github.io/",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5RIop1E1ga": {
    "title": "Distributional Autoencoders Know the Score",
    "volume": "poster",
    "abstract": "The Distributional Principal Autoencoder (DPA) combines distributionally correct reconstruction with principal-component-like interpretability of the encodings. In this work, we provide exact theoretical guarantees on both fronts. First, we derive a closed-form relation linking each optimal level-set geometry to the data-distribution score. This result explains DPA's empirical ability to disentangle factors of variation of the data, as well as allows the score to be recovered directly from samples. When the data follows the Boltzmann distribution, we demonstrate that this relation yields an approximation of the minimum free-energy path for the Müller–Brown potential in a single fit. Second, we prove that if the data lies on a manifold that can be approximated by the encoder, latent components beyond the manifold dimension are conditionally independent of the data distribution - carrying no additional information - and thus reveal the intrinsic dimension. Together, these results show that a single model can learn the data distribution and its intrinsic dimension with exact guarantees simultaneously, unifying two longstanding goals of unsupervised learning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eG5oh8l1WZ": {
    "title": "Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers",
    "volume": "poster",
    "abstract": "Pause tokens, simple filler symbols such as \"...\", consistently improve Transformer performance on both language and mathematical tasks, yet their theoretical effect remains unexplained. We provide the first formal separation result, proving that adding pause tokens to constant-depth, logarithmic-width Transformers strictly increases their computational expressivity. With bounded-precision activations, Transformers without pause tokens compute only a strict subset of $\\mathsf{AC}^0$ functions, while adding a polynomial number of pause tokens enables expressing the complete class. For logarithmic-precision Transformers, we show that adding pause tokens achieves expressivity equivalent to $\\mathsf{TC}^0$, matching known upper bounds. Empirically, we demonstrate that two‑layer causally masked Transformers can learn parity when supplied with pause tokens, a function that they appear unable to learn without them. Our results provide a rigorous theoretical explanation for prior empirical findings, clarify how pause tokens interact with width, depth, and numeric precision, and position them as a distinct mechanism, complementary to chain-of-thought prompting, for enhancing Transformer reasoning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OJAW2mHVND": {
    "title": "Revising and Falsifying Sparse Autoencoder Feature Explanations",
    "volume": "poster",
    "abstract": "Mechanistic interpretability research seeks to reverse-engineer large language models (LLMs) by uncovering the internal representations of concepts within their activations. Sparse Autoencoders (SAEs) have emerged as a valuable tool for disentangling polysemantic neurons into more monosemantic, interpretable features. However, recent work on automatic explanation generation for these features has faced challenges: explanations tend to be overly broad and fail to take polysemanticity into consideration. This work addresses these limitations by introducing a similarity-based strategy for sourcing close negative sentences that more effectively falsify generated explanations. Additionally, we propose a structured, component-based format for feature explanations and a tree-based, iterative explanation method that refines explanations. We demonstrate that our structured format and tree-based explainer improve explanation quality, while our similarity-based evaluation strategy exposes biases in existing interpretability methods. We also analyze the evolution of feature complexity and polysemanticity across LLM layers, offering new insights into information content within LLMs' residual streams",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HdY8CCHife": {
    "title": "A Unified Stability Analysis of SAM vs SGD: Role of Data Coherence and Emergence of Simplicity Bias",
    "volume": "poster",
    "abstract": "Understanding the dynamics of optimization algorithms in deep learning has become increasingly critical, especially as models grow in scale and complexity. Despite the empirical success of stochastic gradient descent (SGD) and its variants in finding solutions that generalize well, the precise mechanisms underlying this generalization remain poorly understood. A particularly intriguing aspect of this phenomenon is the bias of optimization algorithms towards certain types of minima—often flatter or simpler—especially in overparameterized regimes. While prior works have associated flatness of the loss landscape with better generalization, tools to mechanistically connect data, optimization algorithms, and the nature of the resulting minima are still limited. For instance, methods like Sharpness-Aware Minimization (SAM) have shown practical gains by explicitly promoting flatness, but lack a unified theoretical framework explaining their influence across different data structures and model architectures. In this work, we introduce a comprehensive linear stability analysis framework to dissect the behavior of optimization algorithms—SGD, random perturbations, and SAM—in neural networks, focusing particularly on two-layer ReLU models. Our approach is built upon a novel coherence measure that captures the interaction between data geometry and gradient similarity, providing new insights into why and how certain solutions are favored",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=soYT1bfsPj": {
    "title": "Robust Regression of General ReLUs with Queries",
    "volume": "poster",
    "abstract": "We study the task of agnostically learning general (as opposed to homogeneous) ReLUs under the Gaussian distribution with respect to the squared loss. In the passive learning setting, recent work gave a computationally efficient algorithm that uses $poly(d,1/\\epsilon)$ labeled examples and outputs a hypothesis with error $O(opt)+\\epsilon$, where $opt$ is the squared loss of the best fit ReLU. Here we focus on the interactive setting, where the learner has some form of query access to the labels of unlabeled examples. Our main result is the first computationally efficient learner that uses $d polylog(1/\\epsilon)+\\tilde{O}(\\min\\{1/p, 1/\\epsilon\\})$ black-box label queries, where $p$ is the bias of the target function, and achieves error $O(opt)+\\epsilon$. We complement our algorithmic result by showing that its query complexity bound is qualitatively near-optimal, even ignoring computational constraints. Finally, we establish that query access is essentially necessary to improve on the label complexity of passive learning. Specifically, for pool-based active learning, any active learner requires $\\tilde{\\Omega}(d/\\epsilon)$ labels, unless it draws a super-polynomial number of unlabeled examples",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qFC728XyeM": {
    "title": "Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent",
    "volume": "poster",
    "abstract": "Transformers have demonstrated remarkable capabilities in multi-step reasoning tasks. However, understandings of the underlying mechanisms by which they acquire these abilities through training remain limited, particularly from a theoretical standpoint. This work investigates how transformers learn to solve symbolic multi-step reasoning problems through chain-of-thought processes, focusing on path-finding in trees. We analyze two intertwined tasks: a backward reasoning task, where the model outputs a path from a goal node to the root, and a more complex forward reasoning task, where the model implements two-stage reasoning by first identifying the goal-to-root path and then reversing it to produce the root-to-goal path. Our theoretical analysis, grounded in the dynamics of gradient descent, shows that trained one-layer transformers can provably solve both tasks with generalization guarantees to unseen trees. In particular, our multi-phase training dynamics for forward reasoning elucidate how different attention heads learn to specialize and coordinate autonomously to solve the two subtasks in a single autoregressive path. These results provide a mechanistic explanation of how trained transformers can implement sequential algorithmic procedures. Moreover, they offer insights into the emergence of reasoning abilities, suggesting that when tasks are structured to take intermediate chain-of-thought steps, even shallow multi-head transformers can effectively solve problems that would otherwise require deeper architectures",
    "checked": true,
    "id": "0f3865b6b27979bdae6896b3171914889216919a",
    "semantic_title": "multi-head transformers provably learn symbolic multi-step reasoning via gradient descent",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=zVkbsGlKn9": {
    "title": "Towards Generalizable Retina Vessel Segmentation with Deformable Graph Priors",
    "volume": "poster",
    "abstract": "Retinal vessel segmentation is critical for medical diagnosis, yet existing models often struggle to generalize across domains due to appearance variability, limited annotations, and complex vascular morphology. We propose GraphSeg, a variational Bayesian framework that integrates anatomical graph priors with structure-aware image decomposition to enhance cross-domain segmentation. GraphSeg factorizes retinal images into structure-preserved and structure-degraded components, enabling domain-invariant representation. A deformable graph prior, derived from a statistical retinal atlas, is incorporated via a differentiable alignment and guided by an unsupervised energy function. Experiments on three public benchmarks (CHASE, DRIVE, HRF) show that GraphSeg consistently outperforms existing methods under domain shifts. These results highlight the importance of jointly modeling anatomical topology and image structure for robust generalizable vessel segmentation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fYjF9KIJd5": {
    "title": "Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated Text",
    "volume": "poster",
    "abstract": "The increasing capabilities of Large Language Models (LLMs) have raised concerns about their misuse in AI-generated plagiarism and social engineering. While various AI-generated text detectors have been proposed to mitigate these risks, many remain vulnerable to simple evasion techniques such as paraphrasing. However, recent detectors have shown greater robustness against such basic attacks. In this work, we introduce \\textbf{Adversarial Paraphrasing}, a training-free attack framework that universally humanizes any AI-generated text to evade detection more effectively. Our approach leverages an off-the-shelf instruction-following LLM to paraphrase AI-generated content under the guidance of an AI text detector, producing adversarial examples that are specifically optimized to bypass detection. Extensive experiments show that our attack is both broadly effective and highly transferable across several detection systems. For instance, compared to simple paraphrasing attack—which, ironically, increases the true positive at 1\\% false positive (T@1\\%F) by 8.57\\% on RADAR and 15.03\\% on Fast-DetectGPT—adversarial paraphrasing, guided by OpenAI-RoBERTa-Large, reduces T@1\\%F by 64.49\\% on RADAR and a striking 98.96\\% on Fast-DetectGPT. Across a diverse set of detectors—including neural network-based, watermark-based, and zero-shot approaches—our attack achieves an average T@1\\%F reduction of 87.88\\% under the guidance of OpenAI-RoBERTa-Large. We also analyze the tradeoff between text quality and our attack success to find that our method can significantly reduce detection rates, with mostly a slight degradation in text quality. Our novel adversarial setup highlights the need for more robust and resilient detection strategies in the light of increasingly sophisticated evasion techniques",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VYY5sG4EMm": {
    "title": "Policy Gradient Methods Converge Globally in Imperfect-Information Extensive-Form Games",
    "volume": "poster",
    "abstract": "Multi-agent reinforcement learning (MARL) has long been seen as inseparable from Markov games (Littman 1994). Yet, the most remarkable achievements of practical MARL have arguably been in extensive-form games (EFGs)---spanning games like Poker, Stratego, and Hanabi. At the same time, little is known about provable equilibrium convergence for MARL algorithms applied to EFGs as they stumble upon the inherent nonconvexity of the optimization landscape and the failure of the value-iteration subroutine in EFGs. To this goal, we utilize contemporary advances in nonconvex optimization theory to prove that regularized alternating policy gradient with (i) *direct policy parametrization*, (ii) *softmax policy parametrization*, and (iii) *softmax policy parametrization with natural policy gradient* updates converge to an approximate Nash equilibrium (NE) in the *last-iterate* in imperfect-information perfect-recall zero-sum EFGs. Namely, we observe that since the individual utilities are concave with respect to the sequence-form strategy, they satisfy gradient dominance w.r.t. the behavioral strategy---or, \\textit{policy}, in reinforcement learning terms. We exploit this structure to further prove that the regularized utility satisfies the much stronger proximal Polyak- Łojasiewicz condition. In turn, we show that the different flavors of alternating policy gradient methods converge to an $\\epsilon$-approximate NE with a number of iterations and trajectory samples that are polynomial in $1/\\epsilon$ and the natural parameters of the game. Our work is a preliminary---yet principled---attempt in bridging the conceptual gap between the theory of Markov and imperfect-information EFGs while it aspires to stimulate a deeper dialogue between them",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tKPqbamNb9": {
    "title": "Does Thinking More Always Help? Mirage of Test-Time Scaling in Reasoning Models",
    "volume": "poster",
    "abstract": "Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek R1) have led to a popular belief that extending thinking traces using prompts like \"Wait\" or \"Let me rethink\" can improve performance. This raises a natural question: Does thinking more at test-time truly lead to better reasoning? To answer this question, we perform a detailed empirical study across models and benchmarks, which reveals a consistent pattern of initial performance improvements from additional thinking followed by a decline, due to \"overthinking\". To understand this non-monotonic trend, we consider a simple probabilistic model, which reveals that additional thinking increases output variance—creating an illusion of improved reasoning while ultimately undermining precision. Thus, observed gains from \"more thinking\" are not true indicators of improved reasoning, but artifacts stemming from the connection between model uncertainty and evaluation metric. This suggests that test-time scaling through extended thinking is not an effective way to utilize the inference thinking budget. Recognizing these limitations, we introduce an alternative test-time scaling approach, parallel thinking, inspired by Best-of-N sampling. Our method generates multiple independent reasoning paths within the same inference budget and selects the most consistent response via majority vote, achieving up to 20% higher accuracy compared to extended thinking. This provides a simple yet effective mechanism for test-time scaling of reasoning models",
    "checked": true,
    "id": "4f06710ea767ed83eaefc713e354bdfd1f447f8b",
    "semantic_title": "does thinking more always help? mirage of test-time scaling in reasoning models",
    "citation_count": 16,
    "authors": []
  },
  "https://openreview.net/forum?id=VcJTTVoysQ": {
    "title": "Alignment of Large Language Models with Constrained Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iyFH9KRGBo": {
    "title": "Correlation Dimension of Autoregressive Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HenpVfO3Wp": {
    "title": "Language Model Behavioral Phases are Consistent Across Architecture, Training Data, and Scale",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UJSaY7p53L": {
    "title": "Compress, Gather, and Recompute: REFORMing Long-Context Processing in Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IBrRNLr6JA": {
    "title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example",
    "volume": "poster",
    "abstract": "We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0\\% to 73.6\\% (8.6\\% improvement beyond format correction), and improves the average performance across six common mathematical reasoning benchmarks from 17.6\\% to 35.7\\% (7.0\\% non-format gain). This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6\\%, average: 35.9\\%), which contains the aforementioned example. Furthermore, RLVR with only two examples even slightly exceeds these results (MATH500: 74.8\\%, average: 36.6\\%). Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples. In addition, we identify some interesting phenomena during 1-shot RLVR, including cross-category generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon we term \\textit{post-saturation generalization}. Moreover, we verify that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the \"grokking\" phenomenon. We also show the critical role of promoting exploration (e.g., by incorporating entropy loss with an appropriate coefficient) in 1-shot RLVR training. We also further discuss related observations about format correction, label robustness and prompt modification. These findings can inspire future work on RLVR efficiency and encourage a re-examination of recent progress and the underlying mechanisms in RLVR. Our code, models, and data are open source at https://github.com/ypwang61/One-Shot-RLVR",
    "checked": true,
    "id": "1122b654f8b47c1aa9c04ff6bbe7561c798e2ad0",
    "semantic_title": "reinforcement learning for reasoning in large language models with one training example",
    "citation_count": 140,
    "authors": []
  },
  "https://openreview.net/forum?id=Kc1WTxZbrP": {
    "title": "LMFusion: Adapting Pretrained Language Models for Multimodal Generation",
    "volume": "poster",
    "abstract": "We present LMFusion, a framework for empowering pretrained text-only large language models (LLMs) with multimodal generative capabilities, enabling them to understand and generate both text and images in arbitrary sequences. LMFusion leverages existing Llama-3's weights for processing texts autoregressively while introducing additional and parallel transformer modules for processing images with diffusion. During training, the data from each modality is routed to its dedicated modules: modality-specific feedforward layers, query-key-value projections, and normalization layers process each modality independently, while the shared self-attention layers allow interactions across text and image features. By freezing the text-specific modules and only training the image-specific modules, LMFusion preserves the language capabilities of text-only LLMs while developing strong visual understanding and generation abilities. Compared to methods that pretrain multimodal generative models from scratch, our experiments demonstrate that, LMFusion improves image understanding by 20% and image generation by 3.6% using only 50% of the FLOPs while maintaining Llama-3's language capabilities. We also demonstrate that this framework can adapt existing vision-language models with multimodal generation ability. Overall, this framework not only leverages existing computational investments in text-only LLMs but also enables the parallel development of language and vision capabilities, presenting a promising direction for efficient multimodal model development",
    "checked": true,
    "id": "a8504c190e8b58177993a457a76acc76b41db6cf",
    "semantic_title": "lmfusion: adapting pretrained language models for multimodal generation",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=bla5qx2sYe": {
    "title": "EddyFormer: Accelerated Neural Simulations of Three-Dimensional Turbulence at Scale",
    "volume": "poster",
    "abstract": "Computationally resolving turbulence remains a central challenge in fluid dynamics due to its multi-scale interactions. Fully resolving large-scale turbulence through direct numerical simulation (DNS) is computationally prohibitive, motivating data-driven machine learning alternatives. In this work, we propose EddyFormer, a Transformer-based spectral-element (SEM) architecture for large-scale turbulence simulation that combines the accuracy of spectral methods with the scalability of the attention mechanism. We introduce an SEM tokenization that decomposes the flow into grid-scale and subgrid-scale components, enabling capture of both local and global features. We create a new three-dimensional isotropic turbulence dataset and train EddyFormer to achieves DNS-level accuracy at 256^3 resolution, providing a 30x speedup over DNS. When applied to unseen domains up to 4x larger than in training, EddyFormer preserves accuracy on physics-invariant metrics-energy spectra, correlation functions, and structure functions-showing domain generalization. On The Well benchmark suite of diverse turbulent flows, EddyFormer resolves cases where prior ML models fail to converge, accurately reproducing complex dynamics across a wide range of physical conditions",
    "checked": true,
    "id": "950bac3493d24e96393c4dabb58edfad3c86b3ae",
    "semantic_title": "eddyformer: accelerated neural simulations of three-dimensional turbulence at scale",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2D4TuZyNnr": {
    "title": "REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving",
    "volume": "poster",
    "abstract": "While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers struggle with neural workloads due to the exponentially large and highly interdependent space of possible transformations. Although existing stochastic search techniques can be effective, they are often sample-inefficient and fail to leverage the structural context underlying compilation decisions. We set out to investigate the research question of whether reasoning with large language models (LLMs), without any retraining, can leverage the context-aware decision space of compiler optimizations to significantly improve sample efficiency. To that end, we introduce a novel compilation framework (dubbed Reasoning Compiler) that formulates optimization as a sequential, context-aware decision process guided by a large language model and structured Monte Carlo tree search (MCTS). The LLM acts as a proposal mechanism, suggesting hardware-informed transformations that reflect the current program state and accumulated performance feedback. MCTS incorporates the LLM-generated proposals to balance exploration and exploitation, facilitating structured, context-sensitive traversal of the expansive compiler optimization space. By achieving substantial speedups with markedly fewer samples than leading neural compilers, our approach demonstrates the potential of LLM-guided reasoning to transform the landscape of compiler optimization",
    "checked": true,
    "id": "c0ff18b4512cc52b5682e7baf1b2f7127644f7ca",
    "semantic_title": "reasoning compiler: llm-guided optimizations for efficient model serving",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=x5KUOlYKQr": {
    "title": "Listwise Preference Diffusion Optimization for User Behavior Trajectories Prediction",
    "volume": "poster",
    "abstract": "Forecasting multi-step user behavior trajectories requires reasoning over structured preferences across future actions, a challenge overlooked by traditional sequential recommendation. This problem is critical for applications such as personalized commerce and adaptive content delivery, where anticipating a user's complete action sequence enhances both satisfaction and business outcomes. We identify an essential limitation of existing paradigms: their inability to capture global, listwise dependencies among sequence items. To address this, we formulate User Behavior Trajectory Prediction (UBTP) as a new task setting that explicitly models longterm user preferences. We introduce Listwise Preference Diffusion Optimization (LPDO), a diffusion-based training framework that directly optimizes structured preferences over entire item sequences. LPDO incorporates a Plackett–Luce supervision signal and derives a tight variational lower bound aligned with listwise ranking likelihoods, enabling coherent preference generation across denoising steps and overcoming the independent-token assumption of prior diffusion methods. To rigorously evaluate multi-step prediction quality, we propose the task-specific metric: Sequential Match (SeqMatch), which measures exact trajectory agreement, and adopt Perplexity (PPL), which assesses probabilistic fidelity. Extensive experiments on real-world user behavior benchmarks demonstrate that LPDO consistently outperforms state-of-the-art baselines, establishing a new benchmark for structured preference learning with diffusion models",
    "checked": true,
    "id": "421cc1405a656e4f0c9a719674c0fc4ce7b50f10",
    "semantic_title": "listwise preference diffusion optimization for user behavior trajectories prediction",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cZVYswQQMt": {
    "title": "ActiveVOO: Value of Observation Guided Active Knowledge Acquisition for Open-World Embodied Lifted Regression Planning",
    "volume": "poster",
    "abstract": "The ability to actively acquire information is essential for open-world planning under partial observability and incomplete knowledge. However, most existing embodied AI systems either assume a known object category or rely on passive perception strategies that exhaustively gather object and relational information from the environment. Such a strategy becomes insufficient in visually complex open-world settings. For instance, a typical household may contain thousands of novel and uniquely configured objects, most of which are irrelevant to the agent's current task. Consequently, open-world agents must be capable of actively identifying and prioritizing task-relevant objects to enable efficient and goal-directed knowledge acquisition. In this work, we introduce ActiveVOO, a novel zero-shot framework for open-world embodied planning that emphasizes object-centric active knowledge acquisition. ActiveVOO employs lifted regression to generate compact, first-order subgoal descriptions that identify task-relevant objects, and provides a principled mechanism to quantify the utility of sensing actions based on commonsense priors derived from LLMs and VLMs. We evaluate ActiveVOO on the visual ALFWorld benchmark, where it achieves substantial improvements over existing LLM- and VLM-based planning approaches, notably outperforming VLMs fine-tuned on ALFWorld data. This work establishes a principled foundation for developing embodied agents capable of actively and efficiently acquiring knowledge to plan and act in open-world environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GyOrgWZZKO": {
    "title": "Exponential Dynamic Energy Network for High Capacity Sequence Memory",
    "volume": "poster",
    "abstract": "The energy paradigm, exemplified by Hopfield networks, offers a principled framework for memory in neural systems by interpreting dynamics as descent on an energy surface. While powerful for static associative memories, it falls short in modeling sequential memory, where transitions between memories are essential. We introduce the Exponential Dynamic Energy Network (EDEN), a novel architecture that extends the energy paradigm to temporal domains by evolving the energy function over multiple timescales. EDEN combines a static high-capacity energy network with a slow, asymmetrically interacting modulatory population, enabling robust and controlled memory transitions. We formally derive short-timescale energy functions that govern local dynamics and use them to analytically compute memory escape times, revealing a phase transition between static and dynamic regimes. The analysis of capacity, defined as the number of memories that can be stored with minimal error rate as a function of the dimensions of the state space (number of feature neurons), for EDEN shows that it achieves exponential sequence memory capacity $\\mathcal{O}(\\gamma^N)$, outperforming the linear capacity $\\mathcal{O}(N)$ of conventional models. Furthermore, EDEN's dynamics resemble the activity of time and ramping cells observed in the human brain during episodic memory tasks, grounding its biological relevance. By unifying static and sequential memory within a dynamic energy framework, EDEN offers a scalable and interpretable model for high-capacity temporal memory in both artificial and biological systems",
    "checked": true,
    "id": "008d98570959ecba0254b07052ea2331a321e23f",
    "semantic_title": "exponential dynamic energy network for high capacity sequence memory",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yh4DPshiWZ": {
    "title": "Bridging Equivariant GNNs and Spherical CNNs for Structured Physical Domains",
    "volume": "poster",
    "abstract": "Many modeling tasks from disparate domains can be framed the same way, computing spherical signals from geometric inputs, for example, computing the radar response of different objects or navigating through an environment. This paper introduces G2Sphere, a general method for mapping object geometries to spherical signals. G2Sphere operates entirely in Fourier space, encoding geometric structure into latent Fourier features using equivariant neural networks and outputting the Fourier coefficients of the continuous target signal, which can be evaluated at any resolution. By utilizing a hybrid GNN-spherical CNN architecture, our method achieves much higher frequency output signal than comparable equivariant GNNs and avoids hand-engineered geometry features used previously by purely spherical methods. We perform experiments on various challenging domains including radar response modeling, aerodynamic drag prediction, and policy learning for manipulation and navigation. We find that G2Sphere outperforms competitive baselines in terms of accuracy and inference time, and we demonstrate that equivariance and Fourier features lead to improved sample efficiency and generalization.The source code is available at: https://github.com/ColinKohler/geometry2sphere",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a1fWrvr72O": {
    "title": "LookWhere? Efficient Visual Recognition by Learning Where to Look and What to See from Self-Supervision",
    "volume": "poster",
    "abstract": "Vision transformers are ever larger, more accurate, and more expensive to compute. At high resolution, the expense is even more extreme as the number of tokens grows quadratically in the image size. We turn to adaptive computation to cope with this cost by learning to predict where to compute. Our LookWhere method divides the computation between a low-resolution selector and a high-resolution extractor without ever processing the full high-resolution input. We jointly pretrain the selector and extractor without task supervision by distillation from a self-supervised teacher, in effect learning where and what to compute at the same time. Unlike prior token reduction methods, which pay to save by pruning already-computed tokens, and prior token selection methods, which require complex and expensive per-task optimization, LookWhere economically and accurately selects and extracts transferrable representations of images. We show that LookWhere excels at sparse recognition on high-resolution inputs (Traffic Signs), maintaining accuracy while reducing FLOPs by 17x and time by 4x, and standard recognition tasks that are global (ImageNet classification) and local (ADE20K segmentation), improving accuracy while reducing time by 1.36x",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e0HQiao2F0": {
    "title": "Mixed-Sample SGD: an End-to-end Analysis of Supervised Transfer Learning",
    "volume": "poster",
    "abstract": "Theoretical works on supervised transfer learning (STL)---where the learner has access to labeled samples from both source and target distributions---have for the most part focused on statistical aspects of the problem, while efficient optimization has received less attention. We consider the problem of designing an SGD procedure for STL that alternates sampling between source and target data, while maintaining statistical transfer guarantees without prior knowledge of the quality of the source data. A main algorithmic difficulty is in understanding how to design such an adaptive sub-sampling mechanism at each SGD step, to automatically gain from the source when it is informative, or bias towards the target and avoid negative transfer when the source is less informative. We show that, such a mixed-sample SGD procedure is feasible for general prediction tasks with convex losses, rooted in tracking an abstract sequence of constrained convex programs that serve to maintain the desired transfer guarantees. We instantiate these results in the concrete setting of linear regression with square loss, and show that the procedure converges, with $1/\\sqrt{T}$ rate, to a solution whose statistical performance on the target is adaptive to the a priori unknown quality of the source. Experiments with synthetic and real datasets support the theory",
    "checked": true,
    "id": "bd69a3fe7d5e4c87c90bee15d50b5bcee36716ce",
    "semantic_title": "mixed-sample sgd: an end-to-end analysis of supervised transfer learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3G56xClPYg": {
    "title": "Copresheaf Topological Neural Networks: A Generalized Deep Learning Framework",
    "volume": "poster",
    "abstract": "We introduce copresheaf topological neural networks (CTNNs), a powerful unifying framework that encapsulates a wide spectrum of deep learning architectures, designed to operate on structured data, including images, point clouds, graphs, meshes, and topological manifolds. While deep learning has profoundly impacted domains ranging from digital assistants to autonomous systems, the principled design of neural architectures tailored to specific tasks and data types remains one of the field's most persistent open challenges. CTNNs address this gap by formulating model design in the language of copresheaves, a concept from algebraic topology that generalizes most practical deep learning models in use today. This abstract yet constructive formulation yields a rich design space from which theoretically sound and practically effective solutions can be derived to tackle core challenges in representation learning, such as long-range dependencies, oversmoothing, heterophily, and non-Euclidean domains. Our empirical results on structured data benchmarks demonstrate that CTNNs consistently outperform conventional baselines, particularly in tasks requiring hierarchical or localized sensitivity. These results establish CTNNs as a principled multi-scale foundation for the next generation of deep learning architectures",
    "checked": true,
    "id": "3f82dc43a94146a698b70f244e3ef55f0bad56ae",
    "semantic_title": "copresheaf topological neural networks: a generalized deep learning framework",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=s4zitEu2R8": {
    "title": "Bridging Expressivity and Scalability with Adaptive Unitary SSMs",
    "volume": "poster",
    "abstract": "Recent work has revealed that state space models (SSMs), while efficient for long-sequence processing, are fundamentally limited in their ability to represent formal languages—particularly due to time-invariant and real-valued recurrence structures. In this work, we draw inspiration from adaptive and structured dynamics observed in biological neural systems and introduce the Adaptive Unitary State Space Model (AUSSM): a novel class of SSMs that leverages skew-symmetric, input-dependent recurrence to achieve unitary evolution and high expressive power. Using algebraic automata theory, we prove that AUSSM can perform modulo counting and simulate solvable group automata at finite precision, enabling SSMs to model a broad class of regular languages out of reach for other SSM architectures. To overcome the practical inefficiencies of adaptive recurrence, we develop a separable convolution formulation and a CUDA implementation that enables scalable parallel training. Empirically, we show that AUSSM and its hybrid variant—interleaved with Mamba—outperform prior SSMs on formal algorithmic tasks such as parity and modular arithmetic, and achieve competent performance on real-world long time-series classification benchmarks. Our results demonstrate that adaptive unitary recurrence provides a powerful and efficient inductive bias for both symbolic and continuous sequence modeling. The code is available at https://github.com/arjunkaruvally/AUSSM",
    "checked": true,
    "id": "5f2287d4594ceafe685661127d3f784aa777c4d5",
    "semantic_title": "bridging expressivity and scalability with adaptive unitary ssms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vOAtjgCAAO": {
    "title": "Backdoor Mitigation via Invertible Pruning Masks",
    "volume": "poster",
    "abstract": "Model pruning has gained traction as a promising defense strategy against backdoor attacks in deep learning. However, existing pruning-based approaches often fall short in accurately identifying and removing the specific parameters responsible for inducing backdoor behaviors. Despite the dominance of fine-tuning-based defenses in recent literature, largely due to their superior performance, pruning remains a compelling alternative, offering greater interpretability and improved robustness in low-data regimes. In this paper, we propose a novel pruning approach featuring a learned \\emph{selection} mechanism to identify parameters critical to both main and backdoor tasks, along with an \\emph{invertible} pruning mask designed to simultaneously achieve two complementary goals: eliminating the backdoor task while preserving it through the inverse mask. We formulate this as a bi-level optimization problem that jointly learns selection variables, a sparse invertible mask, and sample-specific backdoor perturbations derived from clean data. The inner problem synthesizes candidate triggers using the inverse mask, while the outer problem refines the mask to suppress backdoor behavior without impairing clean-task accuracy. Extensive experiments demonstrate that our approach outperforms existing pruning-based backdoor mitigation approaches, maintains strong performance under limited data conditions, and achieves competitive results compared to state-of-the-art fine-tuning approaches. Notably, the proposed approach is particularly effective in restoring correct predictions for compromised samples after successful backdoor mitigation",
    "checked": true,
    "id": "44c54b5f66022b5c5cb45516678fb3041d4c802f",
    "semantic_title": "backdoor mitigation via invertible pruning masks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z0WhTwZscg": {
    "title": "Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents",
    "volume": "poster",
    "abstract": "There is growing interest in integrating high-fidelity visual synthesis capabilities into large language models (LLMs) without compromising their strong reasoning capabilities. Existing methods that directly train LLMs or bridge LLMs and diffusion models usually suffer from costly training since the backbone LLMs have not seen image representations during pretraining. We present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs (MLLMs) and diffusion models using patch-level CLIP image embeddings as latent variables, which are natively aligned with the MLLM's CLIP visual encoder. These patch-level image embeddings are integrated into the diffusion model with a lightweight adaptation of its ControlNet. To retain the original multimodal reasoning capabilities of MLLMs, we equip the MLLM with a visual generation branch initialized from the original MLLM parameters when predicting the patch-level image embeddings. By seamlessly integrating pretrained MLLMs and diffusion models with patch-level CLIP latents, our framework enables high-fidelity controllable image generation with significant training efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or better performance than previous methods in terms of visual fidelity and multimodal understanding, with substantially lower compute during training. We also provide comprehensive ablation studies showing the effectiveness of our design choices. Project page: https://bifrost-1.github.io",
    "checked": true,
    "id": "7e343692d67606123a6d3f910899cefb4aba997b",
    "semantic_title": "bifrost-1: bridging multimodal llms and diffusion models with patch-level clip latents",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=fuOEekc2t3": {
    "title": "Robustly Learning Monotone Single-Index Models",
    "volume": "poster",
    "abstract": "We consider the basic problem of learning Single-Index Models with respect to the square loss under the Gaussian distribution in the presence of adversarial label noise. Our main contribution is the first computationally efficient algorithm for this learning task, achieving a constant factor approximation, that succeeds for the class of {\\em all} monotone activations with bounded moment of order $2 + \\zeta,$ for $\\zeta > 0.$ This class in particular includes all monotone Lipschitz functions and even discontinuous functions like (possibly biased) halfspaces. Prior work for the case of unknown activation either does not attain constant factor approximation or succeeds for a substantially smaller family of activations. The main conceptual novelty of our approach lies in developing an optimization framework that steps outside the boundaries of usual gradient methods and instead identifies a useful vector field to guide the algorithm updates by directly leveraging the problem structure, properties of Gaussian spaces, and regularity of monotone functions",
    "checked": true,
    "id": "4a19952b267b6bc6f1195f5ca2479120dd8a866a",
    "semantic_title": "robustly learning monotone single-index models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bk1IlSAwxR": {
    "title": "RAT: Bridging RNN Efficiency and Attention Accuracy via Chunk-based Sequence Modeling",
    "volume": "poster",
    "abstract": "Transformers have become the cornerstone of modern large-scale language models, but their reliance on softmax attention poses a computational bottleneck at both training and inference. Recurrent models offer high efficiency, but compressing the full sequence into a fixed-size and holistic representation can suffer from memory degradation in long contexts and limit fine-grained retrieval. To address this, we propose RAT, an intermediate design that bridges the efficiency of RNNs and capacity of attention. RAT partitions the input into chunks, applies recurrence within each chunk for local dependencies, and softmax-based attention across chunks for long-range interactions. This design mitigates memory degradation and enables direct access to distant tokens, while retaining computational efficiency. Empirically, with a chunk size of 16, the RAT block achieves a $7\\times$ improvement in training speed for 100K sequence length and $9\\times$ in generation at the 4K position, while maintaining similar performance compared to standard attention. We demonstrate this by training 1.3B parameter models from scratch and performing large-scale evaluations, including short- and long-context benchmarks, as well as supervised fine-tuning (SFT). We further propose a hybrid architecture that interleaves RAT with local attention. By combining efficient long-range modeling with strong local interactions, this hybrid design not only improves inference speed and reduces cache memory usage, but also consistently enhances performance and shows the overall best results. Code is available at \\url{https://github.com/CLAIRE-Labo/RAT}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VCORb5Fw8e": {
    "title": "On topological descriptors for graph products",
    "volume": "poster",
    "abstract": "Topological descriptors have been increasingly utilized for capturing multiscale structural information in relational data. In this work, we consider various filtrations on the (box) product of graphs and the effect on their outputs on the topological descriptors - the Euler characteristic (EC) and persistent homology (PH). In particular, we establish a complete characterization of the expressive power of EC on general color-based filtrations. We also show that the PH descriptors of (virtual) graph products contain strictly more information than the computation on individual graphs, whereas EC does not. Additionally, we provide algorithms to compute the PH diagrams of the product of vertex- and edge-level filtrations on the graph product. We also substantiate our theoretical analysis with empirical investigations on runtime analysis, expressivity, and graph classification performance. Overall, this work paves way for powerful graph persistent descriptors via product filtrations. Code is available at https://github.com/Aalto-QuML/tda_graph_product",
    "checked": true,
    "id": "d904603742689898f361ec69c1c093ef706149a7",
    "semantic_title": "on topological descriptors for graph products",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vHaShO76T8": {
    "title": "Zero-shot World Models via Search in Memory",
    "volume": "poster",
    "abstract": "World Models have vastly permeated the field of Reinforcement Learning. Their ability to model the transition dynamics of an environment have led to tremendous improvements in sample efficiency for online RL. Among them, the most notorious example is Dreamer, a model that learns to act in a diverse set of image-based environments. In this paper, we leverage similarity search and stochastic representations to approximate a world model without a training procedure. We establish a comparison with PlaNet, a well-established world model of the Dreamer family. We evaluate the models on the quality of latent reconstruction and on the perceived similarity of the reconstructed image, on both next-step and long horizon dynamics prediction. The results of our study demonstrate that a search-based world model is comparable to a training based one in both cases. Notably, our model shows stronger performance in long-horizon prediction with respect to the baseline on a range of visually different environments",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=64lk7JQAU1": {
    "title": "CCS: Controllable and Constrained Sampling with Diffusion Models via Initial Noise Perturbation",
    "volume": "poster",
    "abstract": "Diffusion models have emerged as powerful tools for generative tasks, producing high-quality outputs across diverse domains. However, how the generated data responds to the initial noise perturbation in diffusion models remains under-explored, hindering a deeper understanding of the controllability of the sampling process. In this work, we first observe an interesting phenomenon: the relationship between the change of generation outputs and the scale of initial noise perturbation is highly linear through the diffusion ODE sampling process. We then provide both theoretical and empirical analyses to justify this linearity property of the input–output (noise → generation data) relationship. Inspired by these insights, we propose a novel **C**ontrollable and **C**onstrained **S**ampling (CCS) method, along with a new controller algorithm for diffusion models, that enables precise control over both (1) the proximity of individual samples to a target image and (2) the alignment of the sample mean with the target, while preserving high sample quality. We conduct extensive experiments comparing our proposed sampling approach with other methods in terms of both sampling controllability and generated data quality. Results show that CCS achieves significantly more precise controllability while maintaining superior sample quality and diversity, enabling practical applications such as fine-grained and robust image editing. Code: [https://github.com/efzero/diffusioncontroller](https://github.com/efzero/diffusioncontroller)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gfX1nqBKtu": {
    "title": "OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles",
    "volume": "poster",
    "abstract": "We introduce *OpenVLThinker*, one of the first open-source large vision–language models (LVLMs) to exhibit sophisticated chain-of-thought reasoning, achieving notable performance gains on challenging visual reasoning tasks. While text-based reasoning models (e.g., Deepseek R1) show promising results in text-only tasks, distilling their reasoning into LVLMs via supervised fine-tuning (SFT) often results in performance degradation due to imprecise visual grounding. Conversely, purely reinforcement learning (RL)-based methods face a large search space, hindering the emergence of reflective behaviors in smaller models (e.g., 7B LVLMs). Surprisingly, alternating between SFT and RL ultimately results in significant performance improvements after a few iterations. Our analysis reveals that the base model rarely exhibits reasoning behaviors initially, but SFT effectively surfaces these latent actions and narrows the RL search space, accelerating the development of reasoning capabilities. Each subsequent RL stage further refines the model's reasoning skills, producing higher-quality SFT data for continued self-improvement. OpenVLThinker-7B consistently advances performance across six benchmarks demanding mathematical and general reasoning, notably improving MathVista by 3.2\\%, EMMA by 1.4\\%, and HallusionBench by 2.7\\%. Beyond demonstrating the synergy between SFT and RL for complex reasoning tasks, our findings provide early evidence towards achieving R1-style reasoning in multimodal contexts",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MBJJ9Wcpg9": {
    "title": "One-Step is Enough: Sparse Autoencoders for Text-to-Image Diffusion Models",
    "volume": "poster",
    "abstract": "For large language models (LLMs), sparse autoencoders (SAEs) have been shown to decompose intermediate representations that often are not interpretable directly into sparse sums of interpretable features, facilitating better control and subsequent analysis. However, similar analyses and approaches have been lacking for text-to-image models. We investigate the possibility of using SAEs to learn interpretable features for SDXL Turbo, a few-step text-to-image diffusion model. To this end, we train SAEs on the updates performed by transformer blocks within SDXL Turbo's denoising U-net in its 1-step setting. Interestingly, we find that they generalize to 4-step SDXL Turbo and even to the multi-step SDXL base model (i.e., a different model) without additional training. In addition, we show that their learned features are interpretable, causally influence the generation process, and reveal specialization among the blocks. We do so by creating RIEBench, a representation-based image editing benchmark, for editing images while they are generated by turning on and off individual SAE features. This allows us to track which transformer blocks' features are the most impactful depending on the edit category. Our work is the first investigation of SAEs for interpretability in text-to-image diffusion models and our results establish SAEs as a promising approach for understanding and manipulating the internal mechanisms of text-to-image models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7ieS4EYKnB": {
    "title": "Flick: Empowering Federated Learning with Commonsense Knowledge",
    "volume": "poster",
    "abstract": "Federated Learning (FL) has emerged as a privacy-preserving framework for training models on data generated at the edge. However, the heterogeneity of data silos (e.g., label skew and domain shift) often leads to inconsistent learning objectives and suboptimal model performance. Inspired by the data-driven approach, we propose Flick, a novel data generation framework for heterogeneous **F**ederated **L**earning w**i**th **C**ommonsense **K**nowledge from Large Language Models (LLMs). In Flick, the client performs the local data summary to capture client-specific knowledge in textual form. The central server then distills task-relevant, high-quality knowledge from the out-of-the-box LLM -- guided by cross-client-specific insights -- to generate informative text prompts. These prompts direct a generative model in producing synthetic data, enabling global model fine-tuning and local data compensation. This process gradually aligns the label and feature distributions across clients. Extensive results on three datasets demonstrate that Flick improves the global model accuracy by up to 11.43\\%, and accelerates convergence by up to 12.9$\\times$, validating its effectiveness in addressing data heterogeneity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SkhF3cuyev": {
    "title": "SpikingVTG: A Spiking Detection Transformer for Video Temporal Grounding",
    "volume": "poster",
    "abstract": "Video Temporal Grounding (VTG) aims to retrieve precise temporal segments in a video conditioned on natural language queries. Unlike conventional neural frameworks that rely heavily on computationally expensive dense matrix multiplications, Spiking Neural Networks (SNNs)—previously underexplored in this domain—offer a unique opportunity to tackle VTG tasks through bio-plausible spike-based communication and an event-driven accumulation-based computational paradigm. We introduce SpikingVTG, a multi-modal spiking detection transformer, designed to harness the computational simplicity and sparsity of SNNs for VTG tasks. Leveraging the temporal dynamics of SNNs, our model introduces a Saliency Feedback Gating (SFG) mechanism that assigns dynamic saliency scores to video clips and applies multiplicative gating to highlight relevant clips while suppressing less informative ones. SFG enhances performance and reduces computational overhead by minimizing neural activity. We analyze the layer-wise convergence dynamics of SFG-enabled model and apply implicit differentiation at equilibrium to enable efficient, BPTT-free training. To improve generalization and maximize performance, we enable knowledge transfer by optimizing a Cos-L2 representation matching loss that aligns the layer-wise representation and attention maps of a non-spiking teacher with those of our student SpikingVTG. Additionally, we present Normalization-Free (NF)-SpikingVTG, which eliminates non-local operations like softmax and layer normalization, and an extremely quantized 1-bit (NF)-SpikingVTG variant for potential deployment on edge devices. Our models achieve competitive results on QVHighlights, Charades-STA, TACoS, and YouTube Highlights, establishing a strong baseline for multi-modal spiking VTG solutions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KlZUwDP0pR": {
    "title": "Tree of Preferences for Diversified Recommendation",
    "volume": "poster",
    "abstract": "Diversified recommendation has attracted increasing attention from both researchers and practitioners, which can effectively address the homogeneity of recommended items. Existing approaches predominantly aim to infer the diversity of user preferences from observed user feedback. Nonetheless, due to inherent data biases, the observed data may not fully reflect user interests, where underexplored preferences can be overwhelmed or remain unmanifested. Failing to capture these preferences can lead to suboptimal diversity in recommendations. To fill this gap, this work aims to study diversified recommendation from a data-bias perspective. Inspired by the outstanding performance of large language models (LLMs) in zero-shot inference leveraging world knowledge, we propose a novel approach that utilizes LLMs' expertise to uncover underexplored user preferences from observed behavior, ultimately providing diverse and relevant recommendations. To achieve this, we first introduce Tree of Preferences (ToP), an innovative structure constructed to model user preferences from coarse to fine. ToP enables LLMs to systematically reason over the user's rationale behind their behavior, thereby uncovering their underexplored preferences. To guide diversified recommendations using uncovered preferences, we adopt a data-centric approach, identifying candidate items that match user preferences and generating synthetic interactions that reflect underexplored preferences. These interactions are integrated to train a general recommender for diversification. Moreover, we scale up overall efficiency by dynamically selecting influential users during optimization. Extensive evaluations of both diversity and relevance show that our approach outperforms existing methods in most cases and achieves near-optimal performance in others, with reasonable inference latency",
    "checked": false,
    "id": "2e04da4729183c485e5f0f23b51929c42ce5e023",
    "semantic_title": "landowner preferences for tree configurations in rural costa rica: a photo-based survey approach",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jw5TFF3HkH": {
    "title": "Solving Continuous Mean Field Games: Deep Reinforcement Learning for Non-Stationary Dynamics",
    "volume": "poster",
    "abstract": "Mean field games (MFGs) have emerged as a powerful framework for modeling interactions in large-scale multi-agent systems. Despite recent advancements in reinforcement learning (RL) for MFGs, existing methods are typically limited to finite spaces or stationary models, hindering their applicability to real-world problems. This paper introduces a novel deep reinforcement learning (DRL) algorithm specifically designed for non-stationary continuous MFGs. The proposed approach builds upon a Fictitious Play (FP) methodology, leveraging DRL for best-response computation and supervised learning for average policy representation. Furthermore, it learns a representation of the time-dependent population distribution using a Conditional Normalizing Flow. To validate the effectiveness of our method, we evaluate it on three different examples of increasing complexity. By addressing critical limitations in scalability and density approximation, this work represents a significant advancement in applying DRL techniques to complex MFG problems, bringing the field closer to real-world multi-agent systems",
    "checked": true,
    "id": "de99ca045480801946e2c76ade39133b670b8d88",
    "semantic_title": "solving continuous mean field games: deep reinforcement learning for non-stationary dynamics",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=IpZbmX90sI": {
    "title": "Optimal Control for Transformer Architectures: Enhancing Generalization, Robustness and Efficiency",
    "volume": "poster",
    "abstract": "We study Transformers through the perspective of optimal control theory, using tools from continuous-time formulations to derive actionable insights into training and architecture design. This framework improves the performance of existing Transformer models while providing desirable theoretical guarantees, including generalization and robustness. Our framework is designed to be plug-and-play, enabling seamless integration with established Transformer models and requiring only slight changes to the implementation. We conduct seven extensive experiments on tasks motivated by text generation, sentiment analysis, image classification, and point cloud classification. Experimental results show that the framework improves the test performance of the baselines, while being more parameter-efficient. On character-level text generation with nanoGPT, our framework achieves a 46\\% reduction in final test loss while using 42\\% fewer parameters. On GPT-2, our framework achieves a 9.3\\% reduction in final test loss, demonstrating scalability to larger models. To the best of our knowledge, this is the first work that applies optimal control theory to both the training and architecture of Transformers. It offers a new foundation for systematic, theory-driven improvements and moves beyond costly trial-and-error approaches",
    "checked": true,
    "id": "dca2992132b6fa7b2cb0b459dd202ac1dc574283",
    "semantic_title": "optimal control for transformer architectures: enhancing generalization, robustness and efficiency",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Va1IHqY3jG": {
    "title": "Scalable Neural Network Geometric Robustness Validation via Hölder Optimisation",
    "volume": "poster",
    "abstract": "Neural Network (NN) verification methods provide local robustness guarantees for a NN in the dense perturbation space of an input. In this paper we introduce H$^2$V, a method for the validation of local robustness of NNs against geometric perturbations. H$^2$V uniquely employs a Hilbert space-filling construction to recast multi-dimensional problems into single-dimensional ones and Hölder optimisation, iteratively refining the estimation of the Hölder constant for constructing the lower bound. In common with methods, Hölder optimisation might theoretically converge to a local minimum, thereby resulting in a robustness result being incorrect. However, we here identify conditions for H$^2$V to be provably sound, and show experimentally that even outside the soundness conditions, the risk of incorrect results can be minimised by introducing appropriate heuristics in the global optimisation procedure. Indeed, we found no incorrect results validated by H$^2$V on a large set of benchmarks from SoundnessBench and VNN-COMP. To assess the scalability of the approach, we report the results obtained on large NNs ranging from Resnet34 to Resnet152 and vision transformers. These point to SoA scalability of the approach when validating the local robustness of large NNs against geometric perturbations on the ImageNet dataset. Beyond image tasks, we show that the method's scalability enables for the first time the robustness validation of large-scale 3D-NNs in video classification tasks against geometric perturbations for long-sequence input frames on Kinetics/UCF101 datasets",
    "checked": false,
    "id": "5b270b031a262710de907c7c3e1bc09c80b65006",
    "semantic_title": "verification of geometric robustness of neural networks via piecewise linear approximation and lipschitz optimisation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=l3Qq5MU5VX": {
    "title": "AdmTree: Compressing Lengthy Context with Adaptive Semantic Trees",
    "volume": "poster",
    "abstract": "The quadratic complexity of self-attention limits Large Language Models (LLMs) in processing long contexts, a capability vital for many advanced applications. Context compression aims to mitigate this computational barrier while preserving essential semantic information. However, existing methods often falter: explicit methods can sacrifice local detail, while implicit ones may exhibit positional biases, struggle with information degradation, or fail to capture long-range semantic dependencies. We introduce AdmTree, a novel framework for adaptive, hierarchical context compression designed with a core focus on maintaining high semantic fidelity while keep efficiency. AdmTree dynamically segments input based on information density, employing gist tokens to summarize variable-length segments as leaves in a semantic binary tree. This structure, combined with a lightweight aggregation mechanism and a frozen backbone LLM (minimizing new trainable parameters), enables efficient hierarchical abstraction of the context. By effectively preserving fine-grained details alongside global semantic coherence, mitigating position bias, and adapting dynamically to content, AdmTree comprehensively preserves the semantic information of lengthy context",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4kTpb8pITI": {
    "title": "NeuroGenPoisoning: Neuron-Guided Attacks on Retrieval-Augmented Generation of LLM via Genetic Optimization of External Knowledge",
    "volume": "poster",
    "abstract": "Retrieval-Augmented Generation (RAG) empowers Large Language Models (LLMs) to dynamically integrate external knowledge during inference, improving their factual accuracy and adaptability. However, adversaries can inject poisoned external knowledge to override the model's internal memory. While existing attacks iteratively manipulate retrieval content or prompt structure of RAG, they largely ignore the model's internal representation dynamics and neuron-level sensitivities. The underlying mechanism of RAG poisoning has not been fully studied and the effect of knowledge conflict with strong parametric knowledge in RAG is not considered. In this work, we propose NeuroGenPoisoning, a novel attack framework that generates adversarial external knowledge in RAG guided by LLM internal neuron attribution and genetic optimization. Our method first identifies a set of **Poison-Responsive Neurons** whose activation strongly correlates with contextual poisoning knowledge. We then employ a genetic algorithm to evolve adversarial passages that maximally activate these neurons. Crucially, our framework enables massive-scale generation of effective poisoned RAG knowledge by identifying and reusing promising but initially unsuccessful external knowledge variants via observed attribution signals. At the same time, Poison-Responsive Neurons guided poisoning can effectively resolves knowledge conflict. Experimental results across models and datasets demonstrate consistently achieving high Population Overwrite Success Rate (POSR) of over 90\\% while preserving fluency. Empirical evidence shows that our method effectively resolves knowledge conflict",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bkZrAIWK0N": {
    "title": "Distortion of AI Alignment: Does Preference Optimization Optimize for Preferences?",
    "volume": "poster",
    "abstract": "After pre-training, large language models are aligned with human preferences based on pairwise comparisons. State-of-the-art alignment methods (such as PPO-based RLHF and DPO) are built on the assumption of aligning with a single preference model, despite being deployed in settings where users have diverse preferences. As a result, it is not even clear that these alignment methods produce models that satisfy users \\emph{on average} --- a minimal requirement for pluralistic alignment. Drawing on social choice theory and modeling users' comparisons through individual Bradley-Terry (BT) models, we introduce an alignment method's \\emph{distortion}: the worst-case ratio between the optimal achievable average utility, and the average utility of the learned policy. The notion of distortion helps draw sharp distinctions between alignment methods: \\emph{Nash Learning from Human Feedback} achieves the minimax optimal distortion of $(\\frac{1}{2} + o(1)) \\cdot \\beta$ (for the BT temperature $\\beta$), robustly across utility distributions, distributions of comparison pairs, and permissible KL divergences from the reference policy. RLHF and DPO, by contrast, suffer $\\geq (1 - o(1)) \\cdot \\beta$ distortion already without a KL constraint, and $e^{\\Omega(\\beta)}$ or even unbounded distortion in the full setting, depending on how comparison pairs are sampled",
    "checked": true,
    "id": "68d3f73f48805bdcef68d45cf4401dea6bac7f42",
    "semantic_title": "distortion of ai alignment: does preference optimization optimize for preferences?",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=rbBtoVnduo": {
    "title": "Reasoning Models Better Express Their Confidence",
    "volume": "poster",
    "abstract": "Despite their strengths, large language models (LLMs) often fail to communicate their confidence accurately, making it difficult to assess when they might be wrong and limiting their reliability. In this work, we demonstrate that reasoning models that engage in extended chain-of-thought (CoT) reasoning exhibit superior performance not only in problem-solving but also in accurately expressing their confidence. Specifically, we benchmark six reasoning models across six datasets and find that they achieve strictly better confidence calibration than their non-reasoning counterparts in 33 out of the 36 settings. Our detailed analysis reveals that these gains in calibration stem from the slow thinking behaviors of reasoning models (e.g., exploring alternative approaches and backtracking) which enable them to adjust their confidence dynamically throughout their CoT, making it progressively more accurate. In particular, we find that reasoning models become increasingly better calibrated as their CoT unfolds, a trend not observed in non-reasoning models. Moreover, removing slow thinking behaviors from the CoT leads to a significant drop in calibration. Lastly, we show that non-reasoning models also demonstrate enhanced calibration when simply guided to slow think via in-context learning, fully isolating slow thinking as the source of the calibration gains",
    "checked": true,
    "id": "f393201d8eda43de4d995b5a85c9e7ebd7fbb6bf",
    "semantic_title": "reasoning models better express their confidence",
    "citation_count": 20,
    "authors": []
  },
  "https://openreview.net/forum?id=ltPRj2nthL": {
    "title": "Mitigating Reward Over-optimization in Direct Alignment Algorithms with Importance Sampling",
    "volume": "poster",
    "abstract": "Recently, Direct Alignment Algorithms (DAAs) such as Direct Preference Optimization (DPO) have emerged as alternatives to the standard Reinforcement Learning from Human Feedback (RLHF) for aligning large language models (LLMs) with human values. Surprisingly, while DAAs do not use a separate proxy reward model as in RLHF, their performance can still deteriorate over the course of training -- an over-optimization phenomenon found in RLHF where the learning policy exploits the overfitting to inaccuracies of the reward model to achieve high rewards. One attributed source of over-optimization in DAAs is the under-constrained nature of their offline optimization, which can gradually shift probability mass toward non-preferred responses not presented in the preference dataset. This paper proposes a novel importance-sampling approach to mitigate the distribution shift problem of offline DAAs. This approach, called (IS-DAAs), multiplies the DAA objective with an importance ratio that accounts for the reference policy distribution. IS-DAAs additionally avoid the high variance issue associated with importance sampling by clipping the importance ratio to a maximum value. Our extensive experiments demonstrate that IS-DAAs can effectively mitigate over-optimization, especially under low regularization strength, and achieve better performance than other methods designed to address this problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mbmGNCFc75": {
    "title": "Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On from a Single Image",
    "volume": "poster",
    "abstract": "This paper proposes Virtual Fitting Room (VFR), a novel video generative model that produces arbitrarily long virtual try-on videos. Our VFR models long video generation tasks as an auto-regressive, segment-by-segment generation process, eliminating the need for resource-intensive generation and lengthy video data, while providing the flexibility to generate videos of arbitrary length. The key challenges of this task are twofold: ensuring local smoothness between adjacent segments and maintaining global temporal consistency across different segments. To address these challenges, we propose our VFR framework, which ensures smoothness through a prefix video condition and enforces consistency with the anchor video — a 360°-view video that comprehensively captures the human's whole-body appearance. Our VFR generates minute-scale virtual try-on videos with both local smoothness and global temporal consistency under various motions, making it a pioneering work in long virtual try-on video generation. Project Page: https://immortalco.github.io/VirtualFittingRoom/",
    "checked": false,
    "id": "828afe38f1492975dd3c3eae4e0b3e1ff04b7b0b",
    "semantic_title": "virtual fitting room: generating arbitrarily long videos of virtual try-on from a single image - technical preview",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1t4hR9JCcS": {
    "title": "Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones",
    "volume": "poster",
    "abstract": "Despite remarkable advances in coding capabilities, language models (LMs) still struggle with simple syntactic tasks such as generating balanced parentheses. In this study, we investigate the underlying mechanisms behind the persistence of these errors across LMs of varying sizes (124M–7B) to both understand and mitigate the errors. Our study reveals that LMs rely on a number of components (attention heads and FF neurons) that independently make their own predictions. While some components reliably promote correct answers across a generalized range of inputs (i.e., implementing \"sound mechanisms''), others are less reliable and introduce noise by promoting incorrect tokens (i.e., implementing \"faulty mechanisms''). Errors occur when the faulty mechanisms overshadow the sound ones and dominantly affect the predictions. Motivated by this insight, we introduce RASteer, a steering method to systematically identify and increase the contribution of reliable components for improving model performance. RASteer substantially improves performance on balanced parentheses tasks, boosting accuracy of some models from $0$\\% to around $100$\\% without impairing the models' general coding ability. We further demonstrate its broader applicability in arithmetic reasoning tasks, achieving performance gains of up to around $20$\\%",
    "checked": true,
    "id": "e71e9b52adfbeddbf89269cbdb293347e495a627",
    "semantic_title": "failure by interference: language models make balanced parentheses errors when faulty mechanisms overshadow sound ones",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kn4QjJFZaN": {
    "title": "Mechanistic Interpretability of RNNs emulating Hidden Markov Models",
    "volume": "poster",
    "abstract": "Recurrent neural networks (RNNs) provide a powerful approach in neuroscience to infer latent dynamics in neural populations and to generate hypotheses about the neural computations underlying behavior. However, past work has focused on relatively simple, input-driven, and largely deterministic behaviors - little is known about the mechanisms that would allow RNNs to generate the richer, spontaneous, and potentially stochastic behaviors observed in natural settings. Modeling with Hidden Markov Models (HMMs) has revealed a segmentation of natural behaviors into discrete latent states with stochastic transitions between them, a type of dynamics that may appear at odds with the continuous state spaces implemented by RNNs. Here we first show that RNNs can replicate HMM emission statistics and then reverse-engineer the trained networks to uncover the mechanisms they implement. In the absence of inputs, the activity of trained RNNs collapses towards a single fixed point. When driven by stochastic input, trajectories instead exhibit noise-sustained dynamics along closed orbits. Rotation along these orbits modulates the emission probabilities and is governed by transitions between regions of slow, noise-driven dynamics connected by fast, deterministic transitions. The trained RNNs develop highly structured connectivity, with a small set of \"kick neurons\" initiating transitions between these regions. This mechanism emerges during training as the network shifts into a regime of stochastic resonance, enabling it to perform probabilistic computations. Analyses across multiple HMM architectures — fully connected, cyclic, and linear-chain — reveal that this solution generalizes through the modular reuse of the same dynamical motif, suggesting a compositional principle by which RNNs can emulate complex discrete latent dynamics",
    "checked": true,
    "id": "74d06e1eec4b15fdde3ba0d7623f32982785869f",
    "semantic_title": "mechanistic interpretability of rnns emulating hidden markov models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V4SA2FOzQL": {
    "title": "Fundamental Limitations in Pointwise Defences of LLM Finetuning APIs",
    "volume": "poster",
    "abstract": "LLM developers deploy technical mitigations to prevent _fine-tuning misuse attacks_, attacks in which adversaries evade safeguards by fine-tuning the model using a public API. Previous work has established several successful attacks against specific fine-tuning API defences; however, prior attacks training and/or inference samples can be easily flagged as suspicious. In this work, we show that defences of fine-tuning APIs that seek to detect individual harmful training or inference samples ('pointwise' detection) are _fundamentally limited_ in their ability to prevent fine-tuning attacks. We demonstrate a class of 'pointwise-undetectable' attacks that repurpose semantic or syntactic variations in benign model outputs to covertly transmit dangerous knowledge. Our attacks are composed solely of unsuspicious benign samples that can be collected from the model before fine-tuning, meaning training and inference samples are all individually benign and low-perplexity. We test our attacks against the OpenAI fine-tuning API, finding they succeed in eliciting answers to harmful multiple-choice questions, and that they evade an enhanced monitoring system we design that successfully detects other fine-tuning attacks. Our results showing fundamental limitations of defending against pointwise attacks suggest focusing research efforts on mitigations towards multi-point defences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8ROJA0ty0V": {
    "title": "Bounds on the computational complexity of neurons due to dendritic morphology",
    "volume": "poster",
    "abstract": "The simple linear threshold units used in many artificial neural networks have a limited computational capacity. Famously, a single unit cannot handle non-linearly separable problems like XOR. In contrast, real neurons exhibit complex morphologies as well as active dendritic integration, suggesting that their computational capacities outperform those of simple linear units. Considering specific families of Boolean functions, we empirically examine the computational limits of single units that incorporate more complex dendritic structures. For random Boolean functions, we show that there is a phase transition in learnability as a function of the input dimension, with most random functions below a certain critical dimension being learnable and those above not. This critical dimension is best predicted by the overall size of the dendritic arbor. This demonstrates that real neurons have a far higher computational complexity than is usually considered in neural models, whether in machine learning or computational neuroscience. Furthermore, using architectures that are, respectively, more \"apical\" or \"basal\" we show that there are non-trivially disjoint sets of learnable functions by each type of neuron. Importantly, these two types of architectures differ in the robustness and generality of the computations they can perform. The basal-like architecture shows a higher probability of function realization, while the apical-like architecture shows an advantage with fast retraining for different functions. Given the cell-type specificity of morphological characteristics, these results suggest both that different components of the dendritic arbor as well as distinct cell types may have distinct computational roles. In single neurons, morphology sculpts computation, shaping not only what neurons do, but how they learn and adapt. Our analysis offers new directions for neuron-level inductive biases in NeuroAI models using scalable models for neuronal cell-type specific computation",
    "checked": true,
    "id": "876bf2a5a9e1c4c9577d001aec71e0436574b3c2",
    "semantic_title": "bounds on the computational complexity of neurons due to dendritic morphology",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CVp0WCw4a1": {
    "title": "Over-squashing in Spatiotemporal Graph Neural Networks",
    "volume": "poster",
    "abstract": "Graph Neural Networks (GNNs) have achieved remarkable success across various domains. However, recent theoretical advances have identified fundamental limitations in their information propagation capabilities, such as over-squashing, where distant nodes fail to effectively exchange information. While extensively studied in static contexts, this issue remains unexplored in Spatiotemporal GNNs (STGNNs), which process sequences associated with graph nodes. Nonetheless, the temporal dimension amplifies this challenge by increasing the information that must be propagated. In this work, we formalize the spatiotemporal over-squashing problem and demonstrate its distinct characteristics compared to the static case. Our analysis reveals that, counterintuitively, convolutional STGNNs favor information propagation from points temporally distant rather than close in time. Moreover, we prove that architectures that follow either time-and-space or time-then-space processing paradigms are equally affected by this phenomenon, providing theoretical justification for computationally efficient implementations. We validate our findings on synthetic and real-world datasets, providing deeper insights into their operational dynamics and principled guidance for more effective designs",
    "checked": true,
    "id": "028322f87af47c98a0584154b0824cd1d3e9ade8",
    "semantic_title": "over-squashing in spatiotemporal graph neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nzu03J5hW5": {
    "title": "Diverse Influence Component Analysis: A Geometric Approach to Nonlinear Mixture Identifiability",
    "volume": "poster",
    "abstract": "Latent component identification from unknown *nonlinear* mixtures is a foundational challenge in machine learning, with applications in tasks such as self-supervised learning and causal representation learning. Prior work in *nonlinear independent component analysis* (nICA) has shown that auxiliary signals---such as weak supervision---can support *identifiability* of conditionally independent latent components. More recent approaches explore structural assumptions, like sparsity in the Jacobian of the mixing function, to relax such requirements. In this work, we introduce *Diverse Influence Component Analysis* (DICA), a framework that exploits the convex geometry of the mixing function's Jacobian. We propose a *Jacobian Volume Maximization* (J-VolMax) criterion, which enables latent component identification by encouraging diversity in their influence on the observed variables. Under suitable conditions, this approach achieves identifiability without relying on auxiliary information, latent component independence, or Jacobian sparsity assumptions. These results extend the scope of identifiability analysis and offer a complementary perspective to existing methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kYYyEZ1LIX": {
    "title": "Simple and Effective Specialized Representations for Fair Classifiers",
    "volume": "poster",
    "abstract": "Fair classification is a critical challenge that has gained increasing importance due to international regulations and its growing use in high-stakes decision-making settings. Existing methods often rely on adversarial learning or distribution matching across sensitive groups; however, adversarial learning can be unstable, and distribution matching can be computationally intensive. To address these limitations, we propose a novel approach based on the characteristic function distance. Our method ensures that the learned representation contains minimal sensitive information while maintaining high effectiveness for downstream tasks. By utilizing characteristic functions, we achieve a more stable and efficient solution compared to traditional methods. Additionally, we introduce a simple relaxation of the objective function that guarantees fairness in common classification models with no performance degradation. Experimental results on benchmark datasets demonstrate that our approach consistently matches or achieves better fairness and predictive accuracy than existing methods. Moreover, our method maintains robustness and computational efficiency, making it a practical solution for real-world applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eGpI9rz1Pw": {
    "title": "Variance-Aware Feel-Good Thompson Sampling for Contextual Bandits",
    "volume": "poster",
    "abstract": "Variance-dependent regret bounds have received increasing attention in recent studies on contextual bandits. However, most of these studies are focused on upper confidence bound (UCB)-based bandit algorithms, while sampling based bandit algorithms such as Thompson sampling are still understudied. The only exception is the `LinVDTS` algorithm (Xu et al., 2023), which is limited to linear reward function and its regret bound is not optimal with respect to the model dimension. In this paper, we present `FGTSVA`, a variance-aware Thompson Sampling algorithm for contextual bandits with general reward function with optimal regret bound. At the core of our analysis is an extension of the decoupling coefficient, a technique commonly used in the analysis of Feel-good Thompson sampling (FGTS) that reflects the complexity of the model space. With the new decoupling coefficient denoted by $\\mathrm{dc}$, `FGTS-VA` achieves the regret of $\\tilde{\\mathcal{O}}(\\sqrt{\\mathrm{dc}\\cdot\\log|\\mathcal{F}|\\sum_{t=1}^T\\sigma_t^2}+\\mathrm{dc})$, where $|\\mathcal{F}|$ is the size of the model space, $T$ is the total number of rounds, and $\\sigma_t^2$ is the subgaussian norm of the noise (e.g., variance when the noise is Gaussian) at round $t$. In the setting of contextual linear bandits, the regret bound of `FGTSVA` matches that of UCB-based algorithms using weighted linear regression (Zhou and Gu, 2022)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VUbwLjLkws": {
    "title": "Scaling Laws for Gradient Descent and Sign Descent for Linear Bigram Models under Zipf's Law",
    "volume": "poster",
    "abstract": "Recent works have highlighted the optimization difficulties encountered by gradient descent in training the first and last layer of transformer-based language models, which are overcome by optimizers such as Adam. The problem appears linked to the heavy-tailed distribution of words in text data, where the frequency of the $k$th most frequent word $\\pi_k$ is proportional to $1/k$, following Zipf's law. To better understand the impact of the data distribution on training performance, we study a linear bigram model for next-token prediction when the tokens follow a power-law $\\pi_k \\propto 1/k^\\alpha$ parameterized by the exponent $\\alpha$. We derive optimization scaling laws for deterministic gradient descent and sign descent as a proxy for Adam as a function of the power $\\alpha \\geq 0$. This setting differs from existing theoretical investigations in scaling laws which assume that the eigenvalues of the data decay as a power with power $\\alpha > 1$. This assumption effectively makes the problem \"finite dimensional\" as most of the loss comes from a few of the largest eigencomponents. In comparison, we show that the problem is more difficult when the data have heavier tails. The case $\\alpha = 1$ as found in text is ``worst-case'' for gradient descent, in that the number of iterations required to reach a small relative error scales almost linearly with dimension. While the performance of sign descent also depends on the dimension, for Zipf-distributed data the number of iterations scales only with the square-root of the dimension, leading to a large improvement over gradient descent for large vocabularies",
    "checked": true,
    "id": "4bf3d37757b5b41303fccf57bc2adc8b0968521f",
    "semantic_title": "scaling laws for gradient descent and sign descent for linear bigram models under zipf's law",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=BXJWKpEfro": {
    "title": "Failure Prediction at Runtime for Generative Robot Policies",
    "volume": "poster",
    "abstract": "Imitation learning (IL) with generative models, such as diffusion and flow matching, has enabled robots to perform complex, long-horizon tasks. However, distribution shifts from unseen environments or compounding action errors can still cause unpredictable and unsafe behavior, leading to task failure. Therefore, early failure prediction during runtime is essential for deploying robots in human-centered and safety-critical environments. We propose FIPER, a general framework for Failure Prediction at Runtime for generative IL policies that does not require failure data. FIPER identifies two key indicators of impending failure: (i) out-of-distribution (OOD) observations detected via random network distillation in the policy's embedding space, and (ii) high uncertainty in generated actions measured by a novel action-chunk entropy score. Both failure prediction scores are calibrated using a small set of successful rollouts via conformal prediction. A failure alarm is triggered when both indicators, aggregated over short time windows, exceed their thresholds. We evaluate FIPER across five simulation and real-world environments involving diverse failure modes. Our results demonstrate that FIPER better distinguishes actual failures from benign OOD situations and predicts failures more accurately and earlier than existing methods. We thus consider this work an important step towards more interpretable and safer generative robot policies. Code, data, and videos are available at [tum-lsy.github.io/fiper_website](https://tum-lsy.github.io/fiper_website)",
    "checked": true,
    "id": "aa97f421e7c9aae87b347d2c7646951d72140a44",
    "semantic_title": "failure prediction at runtime for generative robot policies",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=8Z3KnaYtw9": {
    "title": "JAMUN: Bridging Smoothed Molecular Dynamics and Score-Based Learning for Conformational Ensemble Generation",
    "volume": "poster",
    "abstract": "Conformational ensembles of protein structures are immensely important both for understanding protein function and drug discovery in novel modalities such as cryptic pockets. Current techniques for sampling ensembles such as molecular dynamics (MD) are computationally inefficient, while many recent machine learning methods do not transfer to systems outside their training data. We propose JAMUN which performs MD in a smoothed, noised space of all-atom 3D conformations of molecules by utilizing the framework of walk-jump sampling. JAMUN enables ensemble generation for small peptides at rates of an order of magnitude faster than traditional molecular dynamics. The physical priors in JAMUN enables transferability to systems outside of its training data, even to peptides that are longer than those originally trained on. Our model, code and weights are available at https://github.com/prescient-design/jamun",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dm3DMOcKIA": {
    "title": "Fair Matroid Selection",
    "volume": "poster",
    "abstract": "We investigate the problem of sequentially selecting elements of an unknown matroid in an online manner to form an independent set, with the goal of maximizing the minimum probability of acceptance across all elements, a property we define as $f$-fairness. Under adversarial arrival orders, we design an $\\alpha(\\ln(k)+1)$-fair algorithm, where $\\alpha$ is the arboricity of the matroid and $k$ is the rank, a result that is nearly optimal. For laminar matroids, we develop an $(2\\alpha-1)$-fair algorithm, which is optimal up to constant factors, achieved through a novel online coloring scheme. In the random arrival order setting, we achieve a $(4+o(1))\\alpha$-fair algorithm for graphic matroids, matching the optimal result up to constant factors, relying on a novel technique for learning a degeneracy ordering using a sampled subset of edges. We further generalize our result to $p$-matchoids, obtaining a $\\beta(p\\ln k+1)$-fair algorithm for the adversarial arrival model, where $\\beta$ is the optimal offline fairness. Notably, all our results can be extended to a setting with no prior knowledge of the matroid with only a logarithmic increase in the fairness factor",
    "checked": false,
    "id": "e31b00d44bf5eb0a2980d0fc6a089b1d1cbabf79",
    "semantic_title": "fair column subset selection",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=oDoPiR8wZJ": {
    "title": "Dynamic Test-Time Compute Scaling in Control Policy: Difficulty-Aware Stochastic Interpolant Policy",
    "volume": "poster",
    "abstract": "Diffusion- and flow-based policies deliver state-of-the-art performance on long-horizon robotic manipulation and imitation-learning tasks. However, these controllers employ a fixed inference budget at every control step, regardless of task complexity, leading to computational inefficiency for simple subtasks while potentially underperforming on challenging ones. To address these issues, we introduce Difficulty-Aware Stochastic Interpolant Policy (DA-SIP), a framework that enables robotic controllers to adaptively adjust their integration horizon in real-time based on task difficulty. Our approach employs a difficulty classifier that analyzes RGB-D observations to dynamically select the step budget, the optimal solver variant, and ODE/SDE integration at each control cycle. DA-SIP builds upon the stochastic interpolant formulation to provide a unified framework that unlocks diverse training and inference configurations for diffusion- and flow-based policies. Through comprehensive benchmarks across diverse manipulation tasks, DA-SIP achieves 2.6-4.4× reduction in total computation time while maintaining task-success rates comparable to fixed maximum-computation baselines. By implementing adaptive computation within this framework, DA-SIP transforms generative robot controllers into efficient, task-aware systems that intelligently allocate inference resources where they provide the greatest benefit",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3FxMJEj6t9": {
    "title": "Replicable Online pricing",
    "volume": "poster",
    "abstract": "We explore the concept of replicability, which ensures algorithmic consistency despite input data variations, for online pricing problems, specifically prophet inequalities and delegation. Given the crucial role of replicability in enhancing transparency in economic decision-making, we present a replicable and nearly optimal pricing strategy for prophet inequalities, achieving a sample complexity of $\\textnormal{poly}(\\log^* |\\mathcal{X}|)$, where $\\mathcal{X}$ is the ground set of distributions. Furthermore, we extend these findings to the delegation problem and establish lower bound that proves the necessity of the $\\log^*|\\mathcal{X}|$ dependence. En route to obtaining these results, we develop a number of technical contributions which are of independent interest. Most notably, we propose a new algorithm for a variant of the heavy hitter problem, which has a nearly linear dependence on the inverse of the heavy hitter parameter, significantly improving upon existing results which have a cubic dependence",
    "checked": false,
    "id": "41db4c3c8953238241caf021bdcb40220e6c6bd7",
    "semantic_title": "replicable online learning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=XxR70zr9Sf": {
    "title": "Linear Transformers Implicitly Discover Unified Numerical Algorithms",
    "volume": "poster",
    "abstract": "A transformer is merely a stack of learned data–to–data maps—yet those maps can hide rich algorithms. We train a linear, attention-only transformer on millions of masked-block completion tasks: each prompt is a masked low-rank matrix whose missing block may be (i) a scalar prediction target or (ii) an unseen kernel slice for Nyström extrapolation. The model sees only input–output pairs and a mean-squared loss; it is given no normal equations, no handcrafted iterations, and no hint that the tasks are related. Surprisingly, after training, algebraic unrolling reveals the same parameter-free update rule across all three resource regimes (full visibility, bandwidth-limited heads, rank-limited attention). We prove that this rule achieves second-order convergence on full-batch problems, cuts distributed iteration complexity, and remains accurate with compute-limited attention. Thus, a transformer trained solely to patch missing blocks implicitly discovers a unified, resource-adaptive iterative solver spanning prediction, estimation, and Nyström extrapolation—highlighting a powerful capability of in-context learning",
    "checked": true,
    "id": "210246d4dc3cd40a4a3507bbc31b384739796e1d",
    "semantic_title": "linear transformers implicitly discover unified numerical algorithms",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vx6bPQWpmL": {
    "title": "Why Masking Diffusion Works: Condition on the Jump Schedule for Improved Discrete Diffusion",
    "volume": "poster",
    "abstract": "Discrete diffusion models, like continuous diffusion models, generate high-quality samples by gradually undoing noise applied to datapoints with a Markov process. Gradual generation in theory comes with many conceptual benefits; for example, inductive biases can be incorporated into the noising Markov process. In practice, however, the consistently best performing discrete diffusion model is masking diffusion, which does not denoise gradually. Here we explain the superior performance of masking diffusion by noting that it makes use of a fundamental difference between continuous and discrete Markov processes: discrete Markov processes evolve by discontinuous jumps at a fixed rate and, unlike other discrete diffusion models, masking diffusion builds in the known distribution of jump times and only learns where to jump to. We show that we can similarly bake in the known distribution of jump times into any discrete diffusion model. The resulting models -- schedule-conditioned diffusion (SCUD) -- generalize classical discrete diffusion and masking diffusion. By applying SCUD to models with noising processes that incorporate inductive biases on images, text, and protein data, we build diffusion models that outperform masking",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lGUzCAdYoq": {
    "title": "Simulation-Based Inference for Adaptive Experiments",
    "volume": "poster",
    "abstract": "Multi-arm bandit experimental designs are increasingly being adopted over standard randomized trials due to their potential to improve outcomes for study participants, enable faster identification of the best-performing options, and/or enhance the precision of estimating key parameters. Current approaches for inference after adaptive sampling either rely on asymptotic normality under restricted experiment designs or underpowered martingale concentration inequalities that lead to weak power in practice. To bypass these limitations, we propose a simulation-based approach for conducting hypothesis tests and constructing confidence intervals for arm specific means and their differences. Our simulation-based approach uses positively biased nuisances to generate additional trajectories of the experiment, which we call \\textit{simulation with optimism}. Using these simulations, we characterize the distribution potentially non-normal sample mean test statistic to conduct inference. We provide guarantees for (i) asymptotic type I error control, (ii) convergence of our confidence intervals, and (iii) asymptotic strong consistency of our estimator over a wide variety of common bandit designs. Our empirical results show that our approach achieves the desired coverage while reducing confidence interval widths by up to 50\\%, with drastic improvements for arms not targeted by the design",
    "checked": true,
    "id": "64c47868ecea7706407015b7b51aad3cfa08ee61",
    "semantic_title": "simulation-based inference for adaptive experiments",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nOsEyBGk1I": {
    "title": "C-SafeGen: Certified Safe LLM Generation with Claim-Based Streaming Guardrails",
    "volume": "poster",
    "abstract": "Despite the remarkable capabilities of large language models (LLMs) across diverse applications, they remain vulnerable to generating content that violates safety regulations and policies. To mitigate these risks, LLMs undergo safety alignment; however, they can still be effectively jailbroken. Off-the-shelf guardrail models are commonly deployed to monitor generations, but these models primarily focus on detection rather than ensuring safe decoding of LLM outputs. Moreover, existing efforts lack rigorous safety guarantees, which are crucial for the universal deployment of LLMs and certifiable compliance with regulatory standards. In this paper, we propose a Claim-based Stream Decoding (CSD) algorithm coupled with a statistical risk guarantee framework using conformal analysis. Specifically, our CSD algorithm integrates a stream guardrail model to safeguard sequential claims generated by LLMs and incorporates a backtracking mechanism to revise claims flagged with high safety risks. We provide theoretical guarantees demonstrating that the CSD algorithm achieves the desired generation distribution subject to safety constraints. Furthermore, we introduce a generation risk certification framework and derive a high-probability upper bound on the safety risk of the proposed CSD algorithm. We extend our approach to online settings, where user queries arrive sequentially, and prove that our method can asymptotically control safety risk to any desired level. Empirical evaluations demonstrate the effectiveness and efficiency of the CSD algorithm compared to state-of-the-art safety decoding approaches. Additionally, we validate the soundness and tightness of the derived safety risk upper bound using realistic data in both offline and online scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r4BrtJRmy9": {
    "title": "Missing Data Imputation by Reducing Mutual Information with Rectified Flows",
    "volume": "poster",
    "abstract": "This paper introduces a novel iterative method for missing data imputation that sequentially reduces the mutual information between data and the corresponding missingness mask. Inspired by GAN-based approaches that train generators to decrease the predictability of missingness patterns, our method explicitly targets this reduction in mutual information. Specifically, our algorithm iteratively minimizes the KL divergence between the joint distribution of the imputed data and missingness mask, and the product of their marginals from the previous iteration. We show that the optimal imputation under this framework can be achieved by solving an ODE whose velocity field minimizes a rectified flow training objective. We further illustrate that some existing imputation techniques can be interpreted as approximate special cases of our mutual-information-reducing framework. Comprehensive experiments on synthetic and real-world datasets validate the efficacy of our proposed approach, demonstrating its superior imputation performance. Our implementation is available at \\url{https://github.com/yujhml/MIRI-Imputation}",
    "checked": true,
    "id": "16f0b9b1abcff1afe85c5a06bda4adb96cfc3e60",
    "semantic_title": "missing data imputation by reducing mutual information with rectified flows",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tSpWkTFASC": {
    "title": "LASeR: Learning to Adaptively Select Reward Models with Multi-Arm Bandits",
    "volume": "poster",
    "abstract": "Reward Models (RMs) are crucial to aligning large language models (LLMs), but the degree to which an RM specialized to one task (e.g. writing) generalizes to new tasks (e.g. math) is often not known a priori, often making using only one fixed RM to train LLMs suboptimal. However, optimizing LLMs with multiple RMs simultaneously can incur a prohibitively high computational cost and lead to conflicting signals from different RMs that may degrade performance. To address these challenges, we introduce LASeR (Learning to Adaptively Select Rewards), which frames reward model selection as a multi-armed bandit problem, iteratively and efficiently training LLMs using multiple RMs by selecting the most well-suited RM for each instance. On commonsense and math reasoning tasks, we show that LASeR boosts iterative LLM training, improving the absolute average accuracy of Llama-3-8B over three datasets by $2.67$% over an ensemble of RM scores while also showing superior efficiency (e.g., a $2\\times$ speedup). Moreover, on WildChat (open-ended instruction-following tasks), LASeR leads to a $72.69$% AlpacaEval win rate over the RM score ensemble baseline. Extending to long-context generation, LASeR improves by $2.96$ F1 points (avg.) on single-document QA tasks and $2.97$ F1 points on few-shot learning over the RM score ensemble baseline with best-of-$n$ sampling. We include our code in the supplementary",
    "checked": false,
    "id": "6a5191459d688538cb48717f30fe87fcc06dc59d",
    "semantic_title": "laser: learning to adaptively select reward models with multi-armed bandits",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=zhOUfuOIzA": {
    "title": "Bigger, Regularized, Categorical: High-Capacity Value Functions are Efficient Multi-Task Learners",
    "volume": "poster",
    "abstract": "Recent advances in language modeling and vision stem from training large models on diverse, multi‑task data. This paradigm has had limited impact in value-based reinforcement learning (RL), where improvements are often driven by small models trained in a single-task context. This is because in multi-task RL sparse rewards and gradient conflicts make optimization of temporal difference brittle. Practical workflows for generalist policies therefore avoid online training, instead cloning expert trajectories or distilling collections of single‑task policies into one agent. In this work, we show that the use of high-capacity value models trained via cross-entropy and conditioned on learnable task embeddings addresses the problem of task interference in online RL, allowing for robust and scalable multi‑task training. We test our approach on 7 multi-task benchmarks with over 280 unique tasks, spanning high degree-of-freedom humanoid control and discrete vision-based RL. We find that, despite its simplicity, the proposed approach leads to state-of-the-art single and multi-task performance, as well as sample-efficient transfer to new tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QfKpJ00t2L": {
    "title": "Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks",
    "volume": "poster",
    "abstract": "Large language models (LLMs) show remarkable promise for democratizing automated reasoning by generating formal specifications. However, a fundamental tension exists: LLMs are probabilistic, while formal verification demands deterministic guarantees. This paper addresses this epistemological gap by comprehensively investigating failure modes and uncertainty quantification (UQ) in LLM-generated formal artifacts. Our systematic evaluation of five frontier LLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's domain-specific impact on accuracy (from +34.8\\% on logical tasks to -44.5\\% on factual ones), with known UQ techniques like the entropy of token probabilities failing to identify these errors. We introduce a probabilistic context-free grammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty taxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy for logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables selective verification, drastically reducing errors (14-100\\%) with minimal abstention, transforming LLM-driven formalization into a reliable engineering discipline",
    "checked": true,
    "id": "0d88e13ccdbfd4a060d9c29f20b9dcb54097cacd",
    "semantic_title": "grammars of formal uncertainty: when to trust llms in automated reasoning tasks",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=KuXnKedjAj": {
    "title": "Pose Splatter: A 3D Gaussian Splatting Model for Quantifying Animal Pose and Appearance",
    "volume": "poster",
    "abstract": "Accurate and scalable quantification of animal pose and appearance is crucial for studying behavior. Current 3D pose estimation techniques, such as keypoint- and mesh-based techniques, often face challenges including limited representational detail, labor-intensive annotation requirements, and expensive per-frame optimization. These limitations hinder the study of subtle movements and can make large-scale analyses impractical. We propose *Pose Splatter*, a novel framework leveraging shape carving and 3D Gaussian splatting to model the complete pose and appearance of laboratory animals without prior knowledge of animal geometry, per-frame optimization, or manual annotations. We also propose a rotation-invariant visual embedding technique for encoding pose and appearance, designed to be a plug-in replacement for 3D keypoint data in downstream behavioral analyses. Experiments on datasets of mice, rats, and zebra finches show *Pose Splatter* learns accurate 3D animal geometries. Notably, *Pose Splatter* represents subtle variations in pose, provides better low-dimensional pose embeddings over state-of-the-art as evaluated by humans, and generalizes to unseen data. By eliminating annotation and per-frame optimization bottlenecks, *Pose Splatter* enables analysis of large-scale, longitudinal behavior needed to map genotype, neural activity, and behavior at high resolutions",
    "checked": true,
    "id": "b7a9afd01069ca1adbe00355430686c21af6a4b0",
    "semantic_title": "pose splatter: a 3d gaussian splatting model for quantifying animal pose and appearance",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y0hymKkn2a": {
    "title": "IPFormer: Visual 3D Panoptic Scene Completion with Context-Adaptive Instance Proposals",
    "volume": "poster",
    "abstract": "Semantic Scene Completion (SSC) has emerged as a pivotal approach for jointly learning scene geometry and semantics, enabling downstream applications such as navigation in mobile robotics. The recent generalization to Panoptic Scene Completion (PSC) advances the SSC domain by integrating instance-level information, thereby enhancing object-level sensitivity in scene understanding. While PSC was introduced using LiDAR modality, methods based on camera images remain largely unexplored. Moreover, recent Transformer-based approaches utilize a fixed set of learned queries to reconstruct objects within the scene volume. Although these queries are typically updated with image context during training, they remain static at test time, limiting their ability to dynamically adapt specifically to the observed scene. To overcome these limitations, we propose IPFormer, the first method that leverages context-adaptive instance proposals at train and test time to address vision-based 3D Panoptic Scene Completion. Specifically, IPFormer adaptively initializes these queries as panoptic instance proposals derived from image context and further refines them through attention-based encoding and decoding to reason about semantic instance-voxel relationships. Extensive experimental results show that our approach achieves state-of-the-art in-domain performance, exhibits superior zero-shot generalization on out-of-domain data, and achieves a runtime reduction exceeding 14$\\times$. These results highlight our introduction of context-adaptive instance proposals as a pioneering effort in addressing vision-based 3D Panoptic Scene Completion",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ou30gzTLJe": {
    "title": "Towards Self-Refinement of Vision-Language Models with Triangular Consistency",
    "volume": "poster",
    "abstract": "Vision-Language Models (VLMs) integrate visual knowledge with the analytical capabilities of Large Language Models (LLMs) through supervised visual instruction tuning, using image-question-answer triplets. However, the potential of VLMs trained without supervised instruction remains largely unexplored. This study validates that VLMs possess inherent self-refinement capabilities, enabling them to generate high-quality supervised data without external inputs and thereby learn autonomously. Specifically, to stimulate the self-refinement ability of VLMs, we propose a self-refinement framework based on a Triangular Consistency principle: within the image-query-answer triangle, any masked elements should be consistently and accurately reconstructed. The framework involves three steps: (1) We enable the instruction generation ability of VLMs by adding multi-task instruction tuning like image$\\rightarrow$question-answer or image-answer$\\rightarrow$question. (2) We generate image-query-answer triplets from unlabeled images and use the Triangular Consistency principle for filtering. (3) The model is further updated using the filtered synthetic data. To investigate the underlying mechanisms behind this self-refinement capability, we conduct a theoretical analysis from a causal perspective. Using the widely recognized LLaVA-1.5 as our baseline, our experiments reveal that the model can autonomously achieve consistent, though deliberately modest, improvements across multiple benchmarks without any external supervision, such as human annotations or environmental feedback. We expect that the insights of this study on the self-refinement ability of VLMs can inspire future research on the learning mechanism of VLMs. Code is available at https://github.com/dengyl20/SRF-LLaVA-1.5",
    "checked": true,
    "id": "250ce1be2ebed02975c0f765f74544c8e3faa5d7",
    "semantic_title": "towards self-refinement of vision-language models with triangular consistency",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=fm14gUThwh": {
    "title": "Multi-head Temporal Latent Attention",
    "volume": "poster",
    "abstract": "While Transformer self-attention offers strong parallelism, the Key-Value (KV) cache grows linearly with sequence length and becomes a bottleneck for inference efficiency. Multi-head latent attention was recently developed to compress the KV cache into a low-rank latent space. This paper proposes Multi-head Temporal Latent Attention (MTLA), which further reduces the KV cache size along the temporal dimension, greatly lowering the memory footprint of self-attention inference. MTLA employs a hyper-network to dynamically merge temporally adjacent KV cache vectors. To address the mismatch between the compressed KV cache and processed sequence lengths, a stride-aware causal mask is proposed to ensure efficient parallel training and consistency with inference behaviour. Experiments across tasks, including speech translation, speech recognition, speech understanding and text summarisation, demonstrate that MTLA achieves competitive performance compared to standard Multi-Head Attention (MHA), while greatly improving inference speed and GPU memory usage. For example, on a English-German speech translation task, MTLA achieves a 5.3$\\times$ speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA, while maintaining translation quality",
    "checked": true,
    "id": "067a0a5f4ccf726366709984729fc478f4bff69f",
    "semantic_title": "multi-head temporal latent attention",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aq3tgx5wcu": {
    "title": "YEAST: Yet Another Sequential Test",
    "volume": "poster",
    "abstract": "The online evaluation of machine learning models is typically conducted through A/B experiments. Sequential statistical tests are valuable tools for analysing these experiments, as they enable researchers to stop data collection early without increasing the risk of false discoveries. However, existing sequential tests either limit the number of interim analyses or suffer from low statistical power. In this paper, we introduce a novel sequential test designed for the continuous monitoring of A/B experiments. We validate our method using semi-synthetic simulations and demonstrate that it outperforms current state-of-the-art sequential testing approaches. Our method is derived using a new technique that inverts a bound on the probability of threshold crossing, based on a classical maximal inequality",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TGuWVMssSa": {
    "title": "Replicable Online Learning",
    "volume": "poster",
    "abstract": "We investigate the concept of algorithmic replicability introduced by Impagliazzo et al.(2022) in an online setting. In our model, the input sequence received by the online learner is generated from time-varying distributions chosen by an adversary (obliviously). Our objective is to design low-regret online algorithms that, with high probability, produce the \\emph{exact same sequence} of actions when run on two independently sampled input sequences generated as described above. We refer to such algorithms as adversarially replicable. Previous works explored replicability in the online setting under inputs generated independently from a fixed distribution; we term this notion as iid-replicability. Our model generalizes to capture both adversarial and iid input sequences, as well as their mixtures, which can be modeled by setting certain distributions as point-masses. We demonstrate adversarially replicable online learning algorithms for online linear optimization and the experts problem that achieve sub-linear regret. Additionally, we propose a general framework for converting an online learner into an adversarially replicable one within our setting, bounding the new regret in terms of the original algorithm's regret. We also present a nearly optimal (in terms of regret) iid-replicable online algorithm for the experts problem, highlighting the distinction between the iid and adversarial notions of replicability. Finally, we establish lower bounds on the regret (in terms of the replicability parameter and time) that any replicable online algorithm must incur",
    "checked": true,
    "id": "41db4c3c8953238241caf021bdcb40220e6c6bd7",
    "semantic_title": "replicable online learning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=Rv6Lz84FlZ": {
    "title": "Compressed and Smooth Latent Space for Text Diffusion Modeling",
    "volume": "poster",
    "abstract": "Autoregressive language models dominate modern text generation, yet their sequential nature introduces fundamental limitations: decoding is slow, and maintaining global coherence remains challenging. Diffusion models offer a promising alternative by enabling parallel generation and flexible control; however, their application to text generation is hindered by the high dimensionality of token-level representations. We introduce Cosmos, a novel approach to text generation that operates entirely in a compressed, smooth latent space tailored specifically for diffusion. This space is learned using an autoencoder trained simultaneously for token-level reconstruction and alignment with frozen activations from a pretrained language encoder, providing robust semantic grounding and enabling effective perturbation‑based augmentations. Empirically, we demonstrate that text representations can be compressed up to $8\\times$ while maintaining generation quality comparable to token‑level diffusion models. Furthermore, increasing the latent sequence length allows \\textsc{Cosmos} to surpass both diffusion‑based and autoregressive baselines. We evaluate Cosmos on four diverse generative tasks including story generation, question generation, summarization, and detoxification and compare it with various generative paradigms. Cosmos achieves comparable or superior generation quality while offering more than $2\\times$ faster inference. Code is released at https://github.com/MeshchaninovViacheslav/cosmos",
    "checked": true,
    "id": "5ec5cc293a35240de1c9d5f4048923294d52e07c",
    "semantic_title": "compressed and smooth latent space for text diffusion modeling",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=16QYhVFvrO": {
    "title": "CALM: Culturally Self-Aware Language Models",
    "volume": "poster",
    "abstract": "Cultural awareness in language models is the capacity to understand and adapt to diverse cultural contexts. However, most existing approaches treat culture as static background knowledge, overlooking its dynamic and evolving nature. This limitation reduces their reliability in downstream tasks that demand genuine cultural sensitivity. In this work, we introduce CALM, a novel framework designed to endow language models with cultural self-awareness. CALM disentangles task semantics from explicit cultural concepts and latent cultural signals, shaping them into structured cultural clusters through contrastive learning. These clusters are then aligned via cross-attention to establish fine-grained interactions among related cultural features and are adaptively integrated through a Mixture-of-Experts mechanism along culture-specific dimensions. The resulting unified representation is fused with the model's original knowledge to construct a culturally grounded internal identity state, which is further enhanced through self-prompted reflective learning, enabling continual adaptation and self-correction. Experiments on the benchmark datasets demonstrate that CALM outperforms state-of-the-art methods",
    "checked": false,
    "id": "02f752e26605b79857b5c8249a53c316ce480292",
    "semantic_title": "self-pluralising culture alignment for large language models",
    "citation_count": 9,
    "authors": []
  },
  "https://openreview.net/forum?id=lTP1RxTFSX": {
    "title": "Smooth Sailing: Lipschitz-Driven Uncertainty Quantification for Spatial Associations",
    "volume": "poster",
    "abstract": "Estimating associations between spatial covariates and responses — rather than merely predicting responses — is central to environmental science, epidemiology, and economics. For instance, public health officials might be interested in whether air pollution has a strictly positive association with a health outcome, and the magnitude of any effect. Standard machine learning methods often provide accurate predictions but offer limited insight into covariate-response relationships. And we show that existing methods for constructing confidence (or credible) intervals for associations can fail to provide nominal coverage in the face of model misspecification and nonrandom locations — despite both being essentially always present in spatial problems. We introduce a method that constructs valid frequentist confidence intervals for associations in spatial settings. Our method requires minimal assumptions beyond a form of spatial smoothness and a homoskedastic Gaussian error assumption. In particular, we do not require model correctness or covariate overlap between training and target locations. Our approach is the first to guarantee nominal coverage in this setting and outperforms existing techniques in both real and simulated experiments. Our confidence intervals are valid in finite samples when the noise of the Gaussian error is known, and we provide an asymptotically consistent estimation procedure for this noise variance when it is unknown",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UfFTBEsLgI": {
    "title": "The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning",
    "volume": "poster",
    "abstract": "Entropy minimization (EM) trains the model to concentrate even more probability mass on its most confident outputs. We show that this simple objective alone, without any labeled data, can substantially improve large language models' (LLMs) performance on challenging math, physics, and coding tasks. We explore three approaches: (1) EM-FT minimizes token-level entropy similarly to instruction finetuning, but on unlabeled outputs drawn from the model; (2) EM-RL: reinforcement learning with negative entropy as the only reward to maximize; (3) EM-INF: inference-time logit adjustment to reduce entropy without any training data or parameter updates. On Qwen-7B, EM-RL, without any labeled data, achieves comparable or better performance than strong RL baselines such as GRPO and RLOO that are trained on 60K labeled examples. Furthermore, EM-INF enables Qwen-32B to match or exceed the performance of proprietary models like GPT-4o, Claude 3 Opus, and Gemini 1.5 Pro on the challenging SciCode benchmark, while being 3x more efficient than self-consistency and sequential refinement. Our findings reveal that many pretrained LLMs possess previously underappreciated reasoning capabilities that can be effectively elicited through entropy minimization alone, without any labeled data or even any parameter updates",
    "checked": true,
    "id": "4aef44e4aeaf28868ae2f1fff2c4eb19ff4df1f6",
    "semantic_title": "the unreasonable effectiveness of entropy minimization in llm reasoning",
    "citation_count": 68,
    "authors": []
  },
  "https://openreview.net/forum?id=H918WyPf0s": {
    "title": "AcuRank: Uncertainty-Aware Adaptive Computation for Listwise Reranking",
    "volume": "poster",
    "abstract": "Listwise reranking with large language models (LLMs) enhances top-ranked results in retrieval-based applications. Due to the limit in context size and high inference cost of long context, reranking is typically performed over a fixed size of small subsets, with the final ranking aggregated from these partial results. This fixed computation disregards query difficulty and document distribution, leading to inefficiencies. We propose AcuRank, an adaptive reranking framework that dynamically adjusts both the amount and target of computation based on uncertainty estimates over document relevance. Using a Bayesian TrueSkill model, we iteratively refine relevance estimates until reaching sufficient confidence levels, and our explicit modeling of ranking uncertainty enables principled control over reranking behavior and avoids unnecessary updates to confident predictions. Results on the TREC-DL and BEIR benchmarks show that our method consistently achieves a superior accuracy–efficiency trade-off and scales better with compute than fixed-computation baselines. These results highlight the effectiveness and generalizability of our method across diverse retrieval tasks and LLM-based reranking models",
    "checked": true,
    "id": "96b7a5c88b033bb5b4b1f7336c0de3b34155fc44",
    "semantic_title": "acurank: uncertainty-aware adaptive computation for listwise reranking",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GdrBPyUNPL": {
    "title": "Non-monotone Submodular Optimization: p -Matchoid Constraints and Fully Dynamic Setting",
    "volume": "poster",
    "abstract": "Submodular maximization subject to a $p$-matchoid constraint has various applications in machine learning, particularly in tasks such as feature selection, video and text summarization, movie recommendation, graph-based learning, and constraint-based optimization. We study this problem in the dynamic setting, where a sequence of insertions and deletions of elements to a $p$-matchoid $\\mathcal{M}(\\mathcal{V},\\mathcal{I})$ occurs over time and the goal is to efficiently maintain an approximate solution. We propose a dynamic algorithm for non-monotone submodular maximization under a $p$-matchoid constraint. For a $p$-matchoid $\\mathcal{M}(\\mathcal{V},\\mathcal{I})$ of rank $k$, defined by a collection of $m$ matroids, our algorithm guarantees a $(2p + 2\\sqrt{p(p+1)} + 1 + \\epsilon)$-approximate solution at any time $t$ in the update sequence, with an expected amortized query complexity of $O(\\epsilon^{-3} pk^4 \\log^2(k))$ per update",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VNbvk5pMS7": {
    "title": "Adversary Aware Optimization for Robust Defense",
    "volume": "poster",
    "abstract": "Deep neural networks remain highly susceptible to adversarial attacks, where small, subtle perturbations to input images may induce misclassification. We propose a novel optimization-based purification framework that directly removes these perturbations by maximizing a Bayesian-inspired objective combining a pretrained diffusion prior with a likelihood term tailored to the adversarial perturbation space. Our method iteratively refines a given input through gradient-based updates of a combined score-based loss to guide the purification process. Unlike existing optimization-based defenses that treat adversarial noise as generic corruption, our approach explicitly integrates the adversarial landscape into the objective. Experiments performed on CIFAR-10 and CIFAR-100 demonstrate strong robust accuracy against a range of common adversarial attacks. Our work offers a principled test-time defense grounded in probabilistic inference using score-based generative models. Our code can be found at \\url{https://github.com/rooshenasgroup/aaopt}",
    "checked": false,
    "id": "28a35c393ed2709ed129f68d0346cf375dd7a34e",
    "semantic_title": "enhancing sharpness-aware optimization through variance suppression",
    "citation_count": 31,
    "authors": []
  },
  "https://openreview.net/forum?id=DliPKnn6e0": {
    "title": "Schrödinger Bridge Matching for Tree-Structured Costs and Entropic Wasserstein Barycentres",
    "volume": "poster",
    "abstract": "Recent advances in flow-based generative modelling have provided scalable methods for computing the Schrödinger Bridge (SB) between distributions, a dynamic form of entropy-regularised Optimal Transport (OT) for the quadratic cost. The successful Iterative Markovian Fitting (IMF) procedure solves the SB problem via sequential bridge-matching steps, presenting an elegant and practical approach with many favourable properties over the more traditional Iterative Proportional Fitting (IPF) procedure. Beyond the standard setting, optimal transport can be generalised to the multi-marginal case in which the objective is to minimise a cost defined over several marginal distributions. Of particular importance are costs defined over a tree structure, from which Wasserstein barycentres can be recovered as a special case. In this work, we extend the IMF procedure to solve for the tree-structured SB problem. Our resulting algorithm inherits the many advantages of IMF over IPF approaches in the tree-based setting. In the case of Wasserstein barycentres, our approach can be viewed as extending the widely used fixed-point approach to use flow-based entropic OT solvers, while requiring only simple bridge-matching steps at each iteration",
    "checked": true,
    "id": "d71bff92c501dcd7892c376d94b666f01309bef8",
    "semantic_title": "schrödinger bridge matching for tree-structured costs and entropic wasserstein barycentres",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Ei6IsmxYrb": {
    "title": "How to Scale Second-Order Optimization",
    "volume": "poster",
    "abstract": "Several recently introduced deep learning optimizers inspired by second-order methods have shown promising speedups relative to the current dominant optimizer AdamW, particularly in relatively small-scale experiments. However, efforts to validate and replicate their successes have reported mixed results, with some finding quickly diminishing advantage over AdamW with scale. In this work, we investigate \\emph{how to scale} second-order optimizers to achieve optimal performance at scale. Through theoretical and empirical analysis, we derive scaling rules for hyperparameters such as learning rate and weight decay as we scale up model width and depth for a wide range of optimizers, including Shampoo, SOAP, and Muon, accounting for the impact of commonly used techniques such as blocking and grafting. For compute-optimal scaling, we find scaling independent weight decay as $1/\\mathrm{width}$ is nearly optimal across optimizers, and that second-order optimizers have a substantially larger optimal model size compared to AdamW for a fixed compute budget. Applying these scaling rules, we show Muon achieves close to $1.4\\times$ or higher speedup over AdamW in training transformer language models, while incorrect scaling can decrease the speedup from $1.4\\times$ to below $1.1\\times$ from $190$M to $640$M parameter models",
    "checked": false,
    "id": "ab815890945e82805df127e28aad9ade82002985",
    "semantic_title": "taylor series approximations for faster robust topology optimization",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=byNNv5Et10": {
    "title": "3BASiL: An Algorithmic Framework for Sparse plus Low-Rank Compression of LLMs",
    "volume": "poster",
    "abstract": "Sparse plus Low-Rank $(\\mathbf{S} + \\mathbf{L}\\mathbf{R})$ decomposition of Large Language Models (LLMs) has emerged as a promising direction in $\\textit{model compression}$, aiming to decompose pre-trained model weights into a sum of sparse and low-rank matrices $\\mathbf{W} \\approx \\mathbf{S} + \\mathbf{LR}$. Despite recent progress, existing methods often suffer from substantial performance degradation compared to dense models. In this work, we introduce $\\texttt{3BASiL-TM}$, an efficient one-shot post-training method for $(\\mathbf{S} + \\mathbf{L}\\mathbf{R})$ decomposition of LLMs that addresses this gap. Our approach first introduces a novel 3-Block Alternating Direction Method of Multipliers (ADMM) method, termed $\\texttt{3BASiL}$, to minimize the layer-wise reconstruction error with convergence guarantees. We then design a transformer-matching ($\\texttt{TM}$) refinement step that jointly optimizes the sparse and low-rank components across transformer layers. This step minimizes a novel memory-efficient loss that aligns outputs at the transformer level. Notably, the $\\texttt{TM}$ procedure is universal as it can enhance any $(\\mathbf{S} + \\mathbf{L}\\mathbf{R})$ decomposition, including pure sparsity. Our numerical experiments show that $\\texttt{3BASiL-TM}$ reduces the WikiText2 perplexity gap to dense LLaMA-8B model by over 30% under a (2:4 Sparse + 64 LR) configuration, compared to prior methods. Moreover, our method achieves over 2.5x faster compression runtime on an A100 GPU compared to SOTA $(\\mathbf{S} + \\mathbf{L}\\mathbf{R})$ method. Our code is available at https://github.com/mazumder-lab/3BASiL",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7FLKzOqsKd": {
    "title": "FrameShield: Adversarially Robust Video Anomaly Detection",
    "volume": "poster",
    "abstract": "Weakly Supervised Video Anomaly Detection (WSVAD) has achieved notable advancements, yet existing models remain vulnerable to adversarial attacks, limiting their reliability. Due to the inherent constraints of weak supervision—where only video-level labels are provided despite the need for frame-level predictions—traditional adversarial defense mechanisms, such as adversarial training, are not effective since video-level adversarial perturbations are typically weak and inadequate. To address this limitation, pseudo-labels generated directly from the model can enable frame-level adversarial training; however, these pseudo-labels are inherently noisy, significantly degrading performance. We therefore introduce a novel Pseudo-Anomaly Generation method called Spatiotemporal Region Distortion (SRD), which creates synthetic anomalies by applying severe augmentations to localized regions in normal videos while preserving temporal consistency. Integrating these precisely annotated synthetic anomalies with the noisy pseudo-labels substantially reduces label noise, enabling effective adversarial training. Extensive experiments demonstrate that our method significantly enhances the robustness of WSVAD models against adversarial attacks, outperforming state-of-the-art methods by an average of 71.0\\% in overall AUROC performance across multiple benchmarks. The implementation and code are publicly available at [FrameShield (GitHub)](https://github.com/rohban-lab/FrameShield)",
    "checked": true,
    "id": "4063b37144015b005f4bd11580b1511c02cf4af8",
    "semantic_title": "frameshield: adversarially robust video anomaly detection",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sfTtFZXINg": {
    "title": "Spectral Analysis of Representational Similarity with Limited Neurons",
    "volume": "poster",
    "abstract": "Understanding representational similarity between neural recordings and computational models is essential for neuroscience, yet remains challenging to measure reliably due to the constraints on the number of neurons that can be recorded simultaneously. In this work, we apply tools from Random Matrix Theory to investigate how such limitations affect similarity measures, focusing on Centered Kernel Alignment (CKA) and Canonical Correlation Analysis (CCA). We propose an analytical framework for representational similarity analysis that relates measured similarities to the spectral properties of the underlying representations. We demonstrate that neural similarities are systematically underestimated under finite neuron sampling, mainly due to eigenvector delocalization. To overcome this, we introduce a denoising method to infer population-level similarity, enabling accurate analysis even with small neuron samples. Theoretical predictions are validated on synthetic and real datasets, offering practical strategies for interpreting neural data under finite sampling constraints",
    "checked": true,
    "id": "e54a3bbb703687cf9494539b95e51cef3b8c1a9a",
    "semantic_title": "spectral analysis of representational similarity with limited neurons",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=3IbKbmNci3": {
    "title": "Information Theoretic Learning for Diffusion Models with Warm Start",
    "volume": "poster",
    "abstract": "Generative models that maximize model likelihood have gained traction in many practical settings. Among them, perturbation-based approaches underpin many state-of-the-art likelihood estimation models, yet they often face slow convergence and limited theoretical understanding. In this paper, we derive a tighter likelihood bound for noise-driven models to improve both the accuracy and efficiency of maximum likelihood learning. Our key insight extends the classical Kullback–Leibler (KL) divergence–Fisher information relationship to arbitrary noise perturbations, going beyond the Gaussian assumption and enabling structured noise distributions. This formulation allows flexible use of randomized noise distributions that naturally account for sensor artifacts, quantization effects, and data distribution smoothing, while remaining compatible with standard diffusion training. Treating the diffusion process as a Gaussian channel, we further express the mismatched entropy between data and model, showing that the proposed objective upper-bounds the negative log-likelihood (NLL). In experiments, our models achieve competitive NLL on CIFAR-10 and state-of-the-art results on ImageNet across multiple resolutions, all without data augmentation, and the framework extends naturally to discrete data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gyn4n8oC9B": {
    "title": "Ravan: Multi-Head Low-Rank Adaptation for Federated Fine-Tuning",
    "volume": "poster",
    "abstract": "Large Language Models (LLMs) have yet to effectively leverage the vast amounts of edge-device data, and Federated Learning (FL) offers a promising paradigm to collaboratively fine-tune LLMs without transferring private edge data to the cloud. To operate within the computational and communication constraints of edge devices, recent literature on federated fine-tuning of LLMs proposes the use of low-rank adaptation (LoRA) and similar parameter-efficient methods. However, LoRA-based methods suffer from accuracy degradation in FL settings, primarily because of data and computational heterogeneity across clients. We propose Ravan, an adaptive multi-head LoRA method that balances parameter efficiency and model expressivity by reparameterizing the weight updates as the sum of multiple LoRA heads, $s_i\\textbf{B}_i\\textbf{H}_i\\textbf{A}_i$, in which only the $\\textbf{H}_i$ parameters and their lightweight scaling factors $s_i$ are trained. These trainable scaling factors let the optimization focus on the most useful heads, recovering a higher-rank approximation of the full update without increasing the number of communicated parameters since clients upload $s_i\\textbf{H}_i$ directly. Experiments on vision and language benchmarks show that Ravan improves test accuracy by 2–8\\% over prior parameter-efficient baselines, making it a robust and scalable solution for federated fine-tuning of LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UQxUhFGUyk": {
    "title": "Emergence of Linear Truth Encodings in Language Models",
    "volume": "poster",
    "abstract": "Recent probing studies reveal that large language models exhibit linear subspaces that separate true from false statements, yet the mechanism behind their emergence is unclear. We introduce a transparent, one-layer transformer toy model that reproduces such truth subspaces end-to-end and exposes one concrete route by which they can arise. We study one simple setting in which truth encoding can emerge: a data distribution where factual statements co-occur with other factual statements (and vice-versa), encouraging the model to learn this distinction in order to lower the LM loss on future tokens. We corroborate this pattern with experiments in pretrained language models. Finally, in the toy setting we observe a two-phase learning dynamic: networks first memorize individual factual associations in a few steps, then---over a longer horizon---learn to linearly separate true from false, which in turn lowers language-modeling loss. Together, these results provide both a mechanistic demonstration and an empirical motivation for how and why linear truth representations can emerge in language models",
    "checked": true,
    "id": "b8061826f8b565a6f8b55dbe6faa9ab574f49106",
    "semantic_title": "emergence of linear truth encodings in language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=s14pdQgoLb": {
    "title": "Greed is Good: A Unifying Perspective on Guided Generation",
    "volume": "poster",
    "abstract": "Training-free guided generation is a widely used and powerful technique that allows the end user to exert further control over the generative process of flow/diffusion models. Generally speaking, two families of techniques have emerged for solving this problem for *gradient-based guidance*: namely, *posterior guidance* (*i.e.*, guidance via projecting the current sample to the target distribution via the target prediction model) and *end-to-end guidance* (*i.e.*, guidance by performing backpropagation throughout the entire ODE solve). In this work, we show that these two seemingly separate families can actually be *unified* by looking at posterior guidance as a *greedy strategy* of *end-to-end guidance*. We explore the theoretical connections between these two families and provide an in-depth theoretical of these two techniques relative to the *continuous ideal gradients*. Motivated by this analysis we then show a method for *interpolating* between these two families enabling a trade-off between compute and accuracy of the guidance gradients. We then validate this work on several inverse image problems and property-guided molecular generation",
    "checked": true,
    "id": "d523767772a515d98da5bae2a29c356efb5e54c9",
    "semantic_title": "greed is good: a unifying perspective on guided generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d2EouMhAAq": {
    "title": "TreeGen: A Bayesian Generative Model for Hierarchies",
    "volume": "poster",
    "abstract": "In this work, we introduce TreeGen, a novel generative framework modeling distributions over hierarchies. We extend Bayesian Flow Networks (BFNs) to enable transitions between probabilistic and discrete hierarchies parametrized via categorical distributions. Our proposed scheduler provides smooth and consistent entropy decay across varying numbers of categories. We empirically evaluate TreeGen on the jet-clustering task in high-energy physics, demonstrating that it consistently generates valid trees that adhere to physical constraints and closely align with ground-truth log-likelihoods. Finally, by comparing TreeGen's samples to the exact posterior distribution and performing likelihood maximization via rejection sampling, we demonstrate that TreeGen outperforms various baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pjfMwXm61w": {
    "title": "Streaming Stochastic Submodular Maximization with On-Demand User Requests",
    "volume": "poster",
    "abstract": "We explore a novel problem in streaming submodular maximization, inspired by the dynamics of news-recommendation platforms. We consider a setting where users can visit a news web\\-site at any time, and upon each visit, the web\\-site must display up to $k$ news items. User interactions are inherently stochastic: each news item presented to the user is consumed with a certain acceptance probability by the user, and each news item covers certain topics. Our goal is to design a streaming algorithm that maximizes the expected total topic coverage. To address this problem, we establish a connection to submodular maximization subject to a matroid constraint. We show that we can effectively adapt previous methods to address our problem when the number of user visits is known in advance or linear-size memory in the stream length is available. However, in more realistic scenarios where only an upper bound on the visits and sublinear memory is available, the algorithms fail to guarantee any bounded performance. To overcome these limitations, we introduce a new online streaming algorithm that achieves a competitive ratio of $1/8\\delta$, where $\\delta$ controls the approximation quality. Moreover, it requires only a single pass over the stream, and uses memory independent of the stream length. Empirically, our algorithms consistently outperform the baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GPTI9GNAYH": {
    "title": "Fourier Token Merging: Understanding and Capitalizing Frequency Domain for Efficient Image Generation",
    "volume": "poster",
    "abstract": "Image generation requires intensive computations and faces challenges due to long latency. Exploiting redundancy in the input images and intermediate representations throughout the neural network pipeline is an effective way to accelerate image generation. Token merging (ToMe) exploits similarities among input tokens by clustering them and merges similar tokens into one, thus significantly reducing the number of tokens that are fed into the transformer block. This work introduces Fourier Token Merging, a new method for understanding and capitalizing frequency domain for efficient image generation. By introducing frequency token merging, we find that transforming the token into the frequency domain representation for clustering can better exert the ability of clustering based on the underlying redundancy after de-correlation. Through analytical and empirical studies, we demonstrate the benefits of using Fourier clustering over the original time domain clustering. We experimented fourier token merging on the stable diffusion model, and the results show up to 25\\% reduction in latency without impairing image quality. The code is available at https://github.com/Fred1031/Fourier-Token-Merging",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZL7RuWd6QK": {
    "title": "Uncertainty Quantification for Deep Regression using Contextualised Normalizing Flows",
    "volume": "poster",
    "abstract": "Quantifying uncertainty in deep regression models is important both for understanding the confidence of the model and for safe decision-making in high-risk domains. Existing approaches that yield prediction intervals overlook distributional information, neglecting the effect of multimodal or asymmetric distributions on decision-making. Similarly, full or approximated Bayesian methods, while yielding the predictive posterior density, demand major modifications to the model architecture and retraining. We introduce MCNF, a novel post hoc uncertainty quantification method that produces both prediction intervals and the full conditioned predictive distribution. MCNF operates on top of the underlying trained predictive model; thus, no predictive model retraining is needed. We provide experimental evidence that the MCNF-based uncertainty estimate is well calibrated, is competitive with state-of-the-art uncertainty quantification methods, and provides richer information for downstream decision-making tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l8razJItEy": {
    "title": "On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study",
    "volume": "poster",
    "abstract": "Recent advances in natural language processing highlight two key factors for improving reasoning in large language models (LLMs): (i) allocating more test-time compute tends to help on harder problems but often introduces redundancy in the reasoning trace, and (ii) compute is most effective when reasoning is systematic and incremental, forming structured chains of thought (CoTs) akin to human problem-solving. To study these factors in isolation, we introduce a controlled setting based on shortest-path tasks in layered graphs. We train decoder-only transformers on question–trace–answer triples using a custom tokenizer, comparing models trained on optimal bottom-up dynamic programming traces with those trained on longer, valid traces involving backtracking. Surprisingly, under the same training-token budget, the latter models generalize better to unseen graphs. This benefit is not due to length alone—injecting arbitrary redundancy into reasoning traces fails to help and can even hurt performance. Instead, we find that generalization correlates with the model's confidence in next-token prediction, suggesting that long, coherent, and locally incremental traces make the training signal easier to optimize",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KQLCim1NhT": {
    "title": "A Unified Framework for Provably Efficient Algorithms to Estimate Shapley Values",
    "volume": "poster",
    "abstract": "Shapley values have emerged as a critical tool for explaining which features impact the decisions made by machine learning models. However, computing exact Shapley values is difficult, generally requiring an exponential (in the feature dimension) number of model evaluations. To address this, many model-agnostic randomized estimators have been developed, the most influential and widely used being the KernelSHAP method (Lundberg & Lee, 2017). While related estimators such as unbiased KernelSHAP (Covert & Lee, 2021) and LeverageSHAP (Musco & Witter, 2025) are known to satisfy theoretical guarantees, bounds for KernelSHAP have remained elusive. We describe a broad and unified framework that encompasses KernelSHAP and related estimators constructed using both with and without replacement sampling strategies. We then prove strong non-asymptotic theoretical guarantees that apply to all estimators from our framework. This provides, to the best of our knowledge, the first theoretical guarantees for KernelSHAP and sheds further light on tradeoffs between existing estimators. Through comprehensive benchmarking on small and medium dimensional datasets for Decision-Tree models, we validate our approach against exact Shapley values, consistently achieving low mean squared error with modest sample sizes. Furthermore, we make specific implementation improvements to enable scalability of our methods to high-dimensional datasets. Our methods, tested on datasets such MNIST and CIFAR10, provide consistently better results compared to the KernelSHAP library",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cZzA2Z0ehX": {
    "title": "Quantifying Task-relevant Similarities in Representations Using Decision Variable Correlations",
    "volume": "poster",
    "abstract": "Previous studies have compared neural activities in the visual cortex to representations in deep neural networks trained on image classification. Interestingly, while some suggest that their representations are highly similar, others argued the opposite. Here, we propose a new approach to characterize the similarity of the decision strategies of two observers (models or brains) using decision variable correlation (DVC). DVC quantifies the image-by-image correlation between the decoded decisions based on the internal neural representations in a classification task. Thus, it can capture task-relevant information rather than general representational alignment. We evaluate DVC using monkey V4/IT recordings and network models trained on image classification tasks. We find that model–model similarity is comparable to monkey-monkey similarity, whereas model–monkey similarity is consistently lower. Strikingly, DVC decreases with increasing network performance on ImageNet-1k. Adversarial training does not improve model–monkey similarity in task-relevant dimensions assessed using DVC, although it markedly increases the model–model similarity. Similarly, pre-training on larger datasets does not improve model–monkey similarity. These results suggest a divergence between the task-relevant representations in monkey V4/IT and those learned by models trained on image classification tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R0B1z8dQcV": {
    "title": "AdvPrefix: An Objective for Nuanced LLM Jailbreaks",
    "volume": "poster",
    "abstract": "Many jailbreak attacks on large language models (LLMs) rely on a common objective: making the model respond with the prefix ``Sure, here is (harmful request)''. While straightforward, this objective has two limitations: limited control over model behaviors, yielding incomplete or unrealistic jailbroken responses, and a rigid format that hinders optimization. We introduce AdvPrefix, a plug-and-play prefix-forcing objective that selects one or more model-dependent prefixes by combining two criteria: high prefilling attack success rates and low negative log-likelihood. AdvPrefix integrates seamlessly into existing jailbreak attacks to mitigate the previous limitations for free. For example, replacing GCG's default prefixes on Llama-3 improves nuanced attack success rates from 14\\% to 80\\%, revealing that current safety alignment fails to generalize to new prefixes. Code and selected prefixes are released",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pvoy6fWaRJ": {
    "title": "KOALA++: Efficient Kalman-Based Optimization with Gradient-Covariance Products",
    "volume": "poster",
    "abstract": "We propose KOALA++, a scalable Kalman-based optimization algorithm that explicitly models structured gradient uncertainty in neural network training. Unlike second-order methods, which rely on expensive second order gradient calculation, our method directly estimates the parameter covariance matrix by recursively updating compact gradient covariance products. This design improves upon the original KOALA framework that assumed diagonal covariance by implicitly capturing richer uncertainty structure without storing the full covariance matrix and avoiding large matrix inversions. Across diverse tasks, including image classification and language modeling, KOALA++ achieves accuracy on par or better than state-of-the-art second-order optimizers while maintaining the efficiency of first-order methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t79rMQGk4S": {
    "title": "Preserving Task-Relevant Information Under Linear Concept Removal",
    "volume": "poster",
    "abstract": "Modern neural networks often encode unwanted concepts alongside task-relevant information, leading to fairness and interpretability concerns. Existing post-hoc approaches can remove undesired concepts but often degrade useful signals. We introduce SPLINCE—Simultaneous Projection for LINear concept removal and Covariance prEservation—which eliminates sensitive concepts from representations while exactly preserving their covariance with a target label. SPLINCE achieves this via an oblique projection that ``splices out'' the unwanted direction yet protects important label correlations. Theoretically, it is the unique solution that removes linear concept predictability and maintains target covariance with minimal embedding distortion. Empirically, SPLINCE outperforms baselines on benchmarks such as Bias in Bios and Winobias, removing protected attributes while minimally damaging main-task information",
    "checked": true,
    "id": "763cbb808e40b87ed48c386ce859bd3b2e7ac1a0",
    "semantic_title": "preserving task-relevant information under linear concept removal",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=52Ehpe0Lu5": {
    "title": "Small Batch Size Training for Language Models: When Vanilla SGD Works, and Why Gradient Accumulation is Wasteful",
    "volume": "poster",
    "abstract": "Conventional wisdom dictates that small batch sizes make language model pretraining and fine-tuning unstable, motivating gradient accumulation, which trades off the number of optimizer steps for a proportional increase in batch size. While it is common to decrease the learning rate for smaller batch sizes, other hyperparameters are often held fixed. In this work, we revisit small batch sizes all the way down to batch size one, and we propose a rule for scaling Adam hyperparameters to small batch sizes. In particular, rather than holding the decay rate of the second moment fixed across batch sizes, we propose to hold its half-life fixed in terms of tokens. We find that small batch sizes (1) train stably, (2) are consistently more robust to hyperparameter choices, (3) achieve equal or better per-FLOP performance than larger batch sizes, and (4) notably enable stable language model training with vanilla SGD, even without momentum, despite storing no optimizer state. Building on these results, we provide practical recommendations for selecting a batch size and setting optimizer hyperparameters. We further recommend against gradient accumulation unless training on multiple devices with multiple model replicas. Finally, we show that a small batch size combined with an optimizer with a small state size can provide the performance benefits of full fine-tuning while maintaining a similar memory footprint to LoRA",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yyb1Gi8e36": {
    "title": "PROFIT: A Specialized Optimizer for Deep Fine Tuning",
    "volume": "poster",
    "abstract": "The fine-tuning of pre-trained models has become ubiquitous in generative AI, computer vision, and robotics. Although much attention has been paid to improving the efficiency of fine-tuning model, there has been less scholarship around fine-tuning specifically for improved model performance. To remedy this gap, we present PROFIT, one of the first optimizers designed to incrementally fine-tune converged models on new tasks and/or datasets. Unlike traditional optimizers such as SGD or Adam, which make minimal assumptions due to random initializations, PROFIT takes the properties of a converged model into account explicitly to regularize the optimization process. Employing a temporal gradient-orthogonalization process, PROFIT outperforms fine-tuning methods in various tasks, from image classification to multimodal language model training to large-scale motion prediction. Moreover, PROFIT is encapsulated as a modular optimizer, which makes it easy to integrate directly into any training pipeline with minimal engineering effort",
    "checked": true,
    "id": "5d289e3161d4f046e4b59832652ca68c69d2357a",
    "semantic_title": "profit: a specialized optimizer for deep fine tuning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R9k13fTGP0": {
    "title": "More of the Same: Persistent Representational Harms Under Increased Representation",
    "volume": "poster",
    "abstract": "To recognize and mitigate the harms of generative AI systems, it is crucial to consider whether and how different societal groups are represented by these systems. A critical gap emerges when naively measuring or improving *who* is represented, as this does not consider *how* people are represented. In this work, we develop GAS(P), an evaluation methodology for surfacing distribution-level group representational biases in generated text, tackling the setting where groups are unprompted (i.e., groups are not specified in the input to generative systems). We apply this novel methodology to investigate gendered representations in occupations across state-of-the-art large language models. We show that, even though the gender distribution when models are prompted to generate biographies leads to a large representation of women, even representational biases persist in how different genders are represented. Our evaluation methodology reveals that there are statistically significant distribution-level differences in the word choice used to describe biographies and personas of different genders across occupations, and we show that many of these differences are associated with representational harms and stereotypes. Our empirical findings caution that naively increasing (unprompted) representation may inadvertently proliferate representational biases, and our proposed evaluation methodology enables systematic and rigorous measurement of the problem",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r8UWp9JeJi": {
    "title": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings",
    "volume": "poster",
    "abstract": "Large language models (LLMs) often concentrate their attention on a few specific tokens referred to as *attention sinks*. Common examples include the first token, a prompt-independent sink, and punctuation tokens, which are prompt-dependent. While the tokens causing the sinks often lack direct semantic meaning, the presence of the sinks is critical for model performance, particularly under model compression and KV-caching. Despite their ubiquity, the function, semantic role, and origin of attention sinks—especially those beyond the first token—remain poorly understood. In this work, we conduct a comprehensive investigation demonstrating that attention sinks: *catch* a sequence of tokens, *tag* them using a common direction in embedding space, and *release* them back into the residual stream, where tokens are later retrieved based on the tags they have acquired. Probing experiments reveal these tags carry semantically meaningful information, such as the truth of a statement. These findings extend to reasoning models, where the mechanism spans more heads and explains greater variance in embeddings, or recent models with query-key normalization, where sinks remain just as prevalent. To encourage future theoretical analysis, we introduce a minimal problem which can be solved through the 'catch, tag, release' mechanism, and where it emerges through training",
    "checked": false,
    "id": "3a3b890fb23e9467861438218be4896ab789e30e",
    "semantic_title": "attention sinks: a'catch, tag, release'mechanism for embeddings",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=Jzr9VOiJYd": {
    "title": "Contimask: Explaining Irregular Time Series via Perturbations in Continuous Time",
    "volume": "poster",
    "abstract": "Explaining black-box models for time series data is critical for the wide-scale adoption of deep learning techniques across domains such as healthcare. Recently, explainability methods for deep time series models have seen significant progress by adopting saliency methods that perturb masked segments of time series to uncover their importance towards the prediction of black-box models. Thus far, such methods have been largely restricted to regular time series. Irregular time series, however, sampled at irregular time intervals and potentially with missing values, are the dominant form of time series in various critical domains (e.g., hospital records). In this paper, we conduct the first evaluation of saliency methods for the interpretation of irregular time series models. We first translate techniques for regular time series into the continuous time realm of irregular time series and show under which circumstances such techniques are still applicable. However, existing perturbation techniques neglect the timing and structure of observed data, e.g., informative missingness when data is not missing at random. Thus, we propose Contimask, a simple framework to also apply non-differentiable perturbations, such as simulating that parts of the data had not been observed using NeuroEvolution. Doing so, we successfully detect how structural differences in the data can bias irregular time series models on a real-world sepsis prediction task where 90% of the data is missing. Source code is available on GitHub",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YzriuQGaNX": {
    "title": "REINFORCE Converges to Optimal Policies with Any Learning Rate",
    "volume": "poster",
    "abstract": "We prove that the classic REINFORCE stochastic policy gradient (SPG) method converges to globally optimal policies in finite-horizon Markov Decision Processes (MDPs) with $\\textit{any}$ constant learning rate. To avoid the need for small or decaying learning rates, we introduce two key innovations in the stochastic bandit setting, which we then extend to MDPs. $\\textbf{First}$, we identify a new exploration property of SPG: the online SPG method samples every action infinitely often (i.o.), improving on previous results that only guaranteed at least two actions would be sampled i.o. This means SPG inherently achieves asymptotic exploration without modification. $\\textbf{Second}$, we eliminate the assumption of unique mean reward values, a condition that previous convergence analyses in the bandit setting relied on, but that does not translate to MDPs. Our results deepen the theoretical understanding of SPG in both bandit problems and MDPs, with a focus on how it handles the exploration-exploitation trade-off when standard optimization and stochastic approximation methods cannot be applied, as is the case with large constant learning rates",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kcmw3OH5bh": {
    "title": "Multi-Environment POMDPs: Discrete Model Uncertainty Under Partial Observability",
    "volume": "poster",
    "abstract": "Multi-environment POMDPs (ME-POMDPs) extend standard POMDPs with discrete model uncertainty. ME-POMDPs represent a finite set of POMDPs that share the same state, action, and observation spaces, but may arbitrarily vary in their transition, observation, and reward models. Such models arise, for instance, when multiple domain experts disagree on how to model a problem. The goal is to find a single policy that is robust against any choice of POMDP within the set, *i.e.*, a policy that maximizes the worst-case reward across all POMDPs. We generalize and expand on existing work in the following way. First, we show that ME-POMDPs can be generalized to POMDPs *with sets of initial beliefs*, which we call *adversarial-belief POMDPs* (AB-POMDPs). Second, we show that any arbitrary ME-POMDP can be reduced to a ME-POMDP that only varies in its transition and reward functions or only in its observation and reward functions, while preserving (optimal) policies. We then devise exact and approximate (point-based) algorithms to compute robust policies for AB-POMDPs, and thus ME-POMDPs. We demonstrate that we can compute policies for standard POMDP benchmarks extended to the multi-environment setting",
    "checked": true,
    "id": "19292cb1d6cdc7c6970a65164d1b3e487e47a7a4",
    "semantic_title": "multi-environment pomdps: discrete model uncertainty under partial observability",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KXmDTGKwhy": {
    "title": "Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does Not Fundamentally Alter It",
    "volume": "poster",
    "abstract": "Does vision-and-language (VL) training change the linguistic representations of language models in meaningful ways? In terms of downstream task performance on text-only tasks, most results in the literature have shown marginal differences. In this work, we start from the hypothesis that the domain in which VL training could have a significant effect is lexical-conceptual knowledge, in particular its taxonomic organization. Through comparing minimal pairs of text-only LMs and their VL-trained counterparts, we first show that the VL models often outperform their text-only counterparts on a text-only question-answering task that requires taxonomic understanding of concepts mentioned in the questions. Using an array of targeted behavioral and representational analyses, we show that the LMs and VLMs do not differ significantly in terms of their taxonomic knowledge itself, but they differ in how they represent questions that contain concepts in a taxonomic relation vs. a non-taxonomic relation. This implies that the taxonomic knowledge itself does not change substantially through additional VL training, but VL training does improve the deployment of this knowledge in the context of a specific task, even when the presentation of the task is purely linguistic",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=arw2uk9Ica": {
    "title": "Bernstein–von Mises for Adaptively Collected Data",
    "volume": "poster",
    "abstract": "Uncertainty quantification (UQ) for adaptively collected data, such as that coming from adaptive experiments, bandits, or reinforcement learning, is necessary for critical elements of data collection such as ensuring safety and conducting after-study inference. The data's adaptivity creates significant challenges for frequentist UQ, yet Bayesian UQ remains the same as if the data were independent and identically distributed (i.i.d.), making it an appealing and commonly used approach. Bayesian UQ requires the (correct) specification of a prior distribution while frequentist UQ does not, but for i.i.d. data the celebrated Bernstein–von Mises theorem shows that as the sample size grows, the prior `washes out' and Bayesian UQ becomes frequentist-valid, implying that the choice of prior need not be a major impediment to Bayesian UQ as it makes no difference asymptotically. This paper for the first time extends the Bernstein–von Mises theorem to adaptively collected data, proving asymptotic equivalence between Bayesian UQ and Wald-type frequentist UQ in this challenging setting. Our results do not require the standard stability condition for validity of Wald-type frequentist UQ, and thus provide positive results on frequentist validity of Bayesian UQ under stability. Counterintuitively however, they also provide a negative result that Bayesian UQ is not asymptotically frequentist valid when stability fails, despite the fact that the prior washes out and Bayesian UQ asymptotically matches standard Wald-type frequentist UQ. We empirically validate our theory (positive and negative) via a range of simulations",
    "checked": false,
    "id": "931b9af7abae4b58d9e7b92095f9f527842e1433",
    "semantic_title": "bernstein-von mises for adaptively collected data",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K6M7QmN1wQ": {
    "title": "From Likelihood to Fitness: Improving Variant Effect Prediction in Protein and Genome Language Models",
    "volume": "poster",
    "abstract": "Generative models trained on natural sequences are increasingly used to predict the effects of genetic variation, enabling progress in therapeutic design, disease risk prediction, and synthetic biology. In the zero-shot setting, variant impact is estimated by comparing the likelihoods of sequences, under the assumption that likelihood serves as a proxy for fitness. However, this assumption often breaks down in practice: sequence likelihood reflects not only evolutionary fitness constraints, but also phylogenetic structure and sampling biases, especially as model capacity increases. We introduce Likelihood-Fitness Bridging (LFB), a simple and general strategy that improves variant effect prediction by averaging model scores across sequences subject to similar selective pressures. Assuming an Ornstein-Uhlenbeck model of evolution, LFB can be viewed as a way to marginalize the effects of genetic drift, although its benefits appear to extend more broadly. LFB applies to existing protein and genomic language models without requiring retraining, and incurs only modest computational overhead. Evaluated on large-scale deep mutational scans and clinical benchmarks, LFB consistently improves predictive performance across model families and sizes. Notably, it reverses the performance plateau observed in larger protein language models, making the largest models the most accurate when combined with LFB. These results suggest that accounting for phylogenetic and sampling biases is essential to realizing the full potential of large sequence models in variant effect prediction",
    "checked": true,
    "id": "7e1ccc78847fce0b298371ede8a94cdd114712b0",
    "semantic_title": "from likelihood to fitness: improving variant effect prediction in protein and genome language models",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=YeZnsJzjii": {
    "title": "Metric Automata Theory: A Unifying Theory of RNNs",
    "volume": "poster",
    "abstract": "We propose Metric Automata Theory, an elegant generalisation of classic Automata Theory to continuous dynamical systems, that constitutes a unifying theory of all kinds of Recurrent Neural Networks (RNNs), including widely-adopted architectures such as xLSTM and State Space Models (SSMs). The theory allows one to analyse RNNs both in the finite and unbounded precision settings seamlessly, while utilising fundamental results of Automata Theory. It also provides a novel notion of robustness that guarantees numerical stability and contributes to stability of learning. We employ the theory to prove a comprehensive set of expressivity results for widely-adopted RNNs, with a focus on robustness and finite-precision. Notably, we contrast the capabilities of xLSTM and SSMs for robustly modelling all star-free regular languages—xLSTM can do so, while SSMs cannot robustly recognize the FLIP-FLOP language. Thus we give a novel perspective on the importance of non-linear recurrences, giving insight for why xLSTM shows superior performance to SSMs on several tasks. We provide an improved understanding of the capabilities of Mamba, a popular SSM model. We show that Mamba is not generally capable of recognising the star-free languages under finite-precision, which is seemingly in contrast with the existing theoretical and empirical results for SSMs. We clarify the picture, by showing that Mamba admits a piecewise-linearly separable state space that allows it to approximate star-free languages, with some length-generalisation abilities. At the same time, Mamba does not admit such state spaces for languages like Parity. This explains why empirically Mamba performs well on star-free languages, and fails on Parity",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=STKDn1LBam": {
    "title": "A Few Moments Please: Scalable Graphon Learning via Moment Matching",
    "volume": "poster",
    "abstract": "Graphons, as limit objects of dense graph sequences, play a central role in the statistical analysis of network data. However, existing graphon estimation methods often struggle with scalability to large networks and resolution-independent approximation, due to their reliance on estimating latent variables or costly metrics such as the Gromov-Wasserstein distance. In this work, we propose a novel, scalable graphon estimator that directly recovers the graphon via moment matching, leveraging implicit neural representations (INRs). Our approach avoids latent variable modeling by training an INR--mapping coordinates to graphon values--to match empirical subgraph counts (i.e., moments) from observed graphs. This direct estimation mechanism yields a polynomial-time solution and crucially sidesteps the combinatorial complexity of Gromov-Wasserstein optimization. Building on foundational results, we establish a theoretical guarantee: when the observed subgraph motifs sufficiently represent those of the true graphon (a condition met with sufficiently large or numerous graph samples), the estimated graphon achieves a provable upper bound in cut distance from the ground truth. Additionally, we introduce MomentMixup, a data augmentation technique that performs mixup in the moment space to enhance graphon-based learning. Our graphon estimation method achieves strong empirical performance--demonstrating high accuracy on small graphs and superior computational efficiency on large graphs--outperforming state-of-the-art scalable estimators in 75\\% of benchmark settings and matching them in the remaining cases. Furthermore, MomentMixup demonstrated improved graph classification accuracy on the majority of our benchmarks",
    "checked": true,
    "id": "e42376524d348b98df2bed76d59de7e6c4fdfbeb",
    "semantic_title": "a few moments please: scalable graphon learning via moment matching",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=xIH95kXNR2": {
    "title": "MDNS: Masked Diffusion Neural Sampler via Stochastic Optimal Control",
    "volume": "poster",
    "abstract": "We study the problem of learning a neural sampler to generate samples from discrete state spaces where the target probability mass function $\\pi\\propto\\mathrm{e}^{-U}$ is known up to a normalizing constant, which is an important task in fields such as statistical physics, machine learning, combinatorial optimization, etc. To better address this challenging task when the state space has a large cardinality and the distribution is multi-modal, we propose **M**asked **D**iffusion **N**eural **S**ampler (**MDNS**), a novel framework for training discrete neural samplers by aligning two path measures through a family of learning objectives, theoretically grounded in the stochastic optimal control of the continuous-time Markov chains. We validate the efficiency and scalability of MDNS through extensive experiments on various distributions with distinct statistical properties, where MDNS learns to accurately sample from the target distributions despite the extremely high problem dimensions and outperforms other learning-based baselines by a large margin. A comprehensive study of ablations and extensions is also provided to demonstrate the efficacy and potential of the proposed framework. Our code is available at https://github.com/yuchen-zhu-zyc/MDNS",
    "checked": true,
    "id": "528302668885787968a3700524a7915cea4abb21",
    "semantic_title": "mdns: masked diffusion neural sampler via stochastic optimal control",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=oeMK0Js4lq": {
    "title": "Higher-Order Learning with Graph Neural Networks via Hypergraph Encodings",
    "volume": "poster",
    "abstract": "Higher-order information is crucial for relational learning in many domains where relationships extend beyond pairwise interactions. Hypergraphs provide a natural framework for modeling such relationships, which has motivated recent extensions of graph neural network (GNN) architectures to hypergraphs. Most of these architectures rely on message-passing to encode higher-order information. In this paper, we propose to instead use hypergraph-level encodings based on characteristics such as hypergraph Laplacians and discrete curvature notions. These encodings can be used on datasets that are naturally parametrized as hypergraphs and on graph-level datasets, which we reparametrize as hypergraphs to compute encodings. In both settings, performance increases significantly, on social networks by more than 10 percent. Our theoretical analysis shows that hypergraph-level encodings provably increase the representational power of message-passing graph neural networks beyond that of their graph-level counterparts. For complete reproducibility, we release our codebase: https://github.com/Weber-GeoML/Hypergraph_Encodings",
    "checked": false,
    "id": "d1325d71079a777417b2dca0eee860afbc1e96d7",
    "semantic_title": "higher-order link prediction via light hypergraph neural network and hybrid aggregator",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=vOijARaWym": {
    "title": "Subgraph Federated Learning via Spectral Methods",
    "volume": "poster",
    "abstract": "We consider the problem of federated learning (FL) with graph-structured data distributed across multiple clients. In particular, we address the common scenario of interconnected subgraphs, where interconnections between clients significantly influence the learning process. Existing approaches suffer from critical limitations, either requiring the exchange of sensitive node embeddings, thereby posing privacy risks, or relying on computationally-intensive steps, which hinders scalability. To tackle these challenges, we propose FedLap, a novel framework that leverages global structure information via Laplacian smoothing in the spectral domain to effectively capture inter-node dependencies while ensuring privacy and scalability. We provide a formal analysis of the privacy of FedLap, demonstrating that it preserves privacy. Notably, FedLap is the first subgraph FL scheme with strong privacy guarantees. Extensive experiments on benchmark datasets demonstrate that the proposed method achieves competitive or superior utility compared to existing techniques",
    "checked": true,
    "id": "73fcff7b70d1d2a47b5c5eefef9bc4e1ac8e617b",
    "semantic_title": "subgraph federated learning via spectral methods",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wFXvD83mhb": {
    "title": "Regression Trees Know Calculus",
    "volume": "poster",
    "abstract": "Regression trees have emerged as a preeminent tool for solving real-world regression problems due to their ability to deal with nonlinearities, interaction effects and sharp discontinuities. In this article, we rather study regression trees applied to well-behaved, differentiable functions, and determine the relationship between node parameters and the local gradient of the function being approximated. We find a simple estimate of the gradient which can be efficiently computed using quantities exposed by popular tree learning libraries. This allows tools developed in the context of differentiable algorithms, like neural nets and Gaussian processes, to be deployed to tree-based models. To demonstrate this, we study measures of model sensitivity defined in terms of integro-differential quantities and demonstrate how to compute them for regression trees using the proposed gradient estimates. Quantitative and qualitative numerical experiments reveal the capability of gradients estimated by regression trees to improve predictive analysis, solve tasks in uncertainty quantification, and provide interpretation of model behavior",
    "checked": true,
    "id": "9e81c1f871f6d7bf1342899ffe1654139b6f6c89",
    "semantic_title": "regression trees know calculus",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o3bftqj17e": {
    "title": "How Benchmark Prediction from Fewer Data Misses the Mark",
    "volume": "poster",
    "abstract": "Large language model (LLM) evaluation is increasingly costly, prompting interest in methods that speed up evaluation by shrinking benchmark datasets. Benchmark prediction (also called efficient LLM evaluation) aims to select a small subset of evaluation points and predict overall benchmark performance from that subset. In this paper, we systematically assess the strengths and limitations of 11 benchmark prediction methods across 19 diverse benchmarks. First, we identify a highly competitive baseline: Take a random sample and fit a regression model on the sample to predict missing entries. Outperforming most existing methods, this baseline challenges the assumption that careful subset selection is necessary for benchmark prediction. Second, we discover that all existing methods crucially depend on model similarity. They work best when interpolating scores among similar models. The effectiveness of benchmark prediction sharply declines when new models have higher accuracy than previously seen models. In this setting of extrapolation, none of the previous methods consistently beat a simple average over random samples. To improve over the sample average, we introduce a new method inspired by augmented inverse propensity weighting. This method consistently outperforms the random sample average even for extrapolation. However, its performance still relies on model similarity and the gains are modest in general. This shows that benchmark prediction fails just when it is most needed: at the evaluation frontier, where the goal is to evaluate new models of unknown capabilities",
    "checked": true,
    "id": "626b6f8536d2efc61c76a6df74cff0b6f1042b90",
    "semantic_title": "how benchmark prediction from fewer data misses the mark",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=9jaQTx1O8T": {
    "title": "Secure and Confidential Certificates of Online Fairness",
    "volume": "poster",
    "abstract": "The \"black-box service model\" enables ML service providers to serve clients while keeping their intellectual property and client data confidential. Confidentiality is critical for delivering ML services legally and responsibly, but makes it difficult for outside parties to verify important model properties such as fairness. Existing methods that assess model fairness confidentially lack either (i) *reliability* because they certify fairness with respect to a static set of data, and therefore fail to guarantee fairness in the presence of distribution shift or service provider malfeasance; and/or (ii) *scalability* due to the computational overhead of confidentiality-preserving cryptographic primitives. We address these problems by introducing *online fairness certificates*, which verify that a model is fair with respect to data received by the service provider *online* during deployment. We then present OATH, a deployably efficient and scalable zero-knowledge proof protocol for confidential online group fairness certification. OATH exploits statistical properties of group fairness via a \"cut-and-choose\" style protocol, enabling scalability improvements over baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I3V0NC9BQK": {
    "title": "Scalable Neural Incentive Design with Parameterized Mean-Field Approximation",
    "volume": "poster",
    "abstract": "Designing incentives for a multi-agent system to induce a desirable Nash equilibrium is both a crucial and challenging problem appearing in many decision-making domains, especially for a large number of agents $N$. Under the exchangeability assumption, we formalize this incentive design (ID) problem as a parameterized mean-field game (PMFG), aiming to reduce complexity via an infinite-population limit. We first show that when dynamics and rewards are Lipschitz, the finite-$N$ ID objective is approximated by the PMFG at rate $\\mathcal{O}(\\frac{1}{\\sqrt{N}})$. Moreover, beyond the Lipschitz-continuous setting, we prove the same $\\mathcal{O}(\\frac{1}{\\sqrt{N}})$ decay for the important special case of sequential auctions, despite discontinuities in dynamics, through a tailored auction-specific analysis. Built on our novel approximation results, we further introduce our Adjoint Mean-Field Incentive Design (AMID) algorithm, which uses explicit differentiation of iterated equilibrium operators to compute gradients efficiently. By uniting approximation bounds with optimization guarantees, AMID delivers a powerful, scalable algorithmic tool for many-agent (large $N$) ID. Across diverse auction settings, the proposed AMID method substantially increases revenue over first-price formats and outperforms existing benchmark methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8I1XNt70lj": {
    "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering",
    "volume": "poster",
    "abstract": "While Multimodal Large Language Models (MLLMs) offer strong perception and reasoning capabilities for image-text input, Visual Question Answering (VQA) focusing on small image details still remains a challenge. Although visual cropping techniques seem promising, recent approaches have several limitations: the need for task-specific fine-tuning, low efficiency due to uninformed exhaustive search, or incompatibility with efficient attention implementations. We address these shortcomings by proposing a training-free visual cropping method, dubbed FOCUS, that leverages MLLM-internal representations to guide the search for the most relevant image region. This is accomplished in four steps: first, we identify the target object(s) in the VQA prompt; second, we compute an object relevance map using the key-value (KV) cache; third, we propose and rank relevant image regions based on the map; and finally, we perform the fine-grained VQA task using the top-ranked region. As a result of this informed search strategy, FOCUS achieves strong performance across four fine-grained VQA datasets and three types of MLLMs. It outperforms three popular visual cropping methods in both accuracy and efficiency, and matches the best-performing baseline, ZoomEye, while requiring 3 – 6.5 × less compute",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=My72tmxg6t": {
    "title": "Deep Taxonomic Networks for Unsupervised Hierarchical Prototype Discovery",
    "volume": "poster",
    "abstract": "Inspired by the human ability to learn and organize knowledge into hierarchical taxonomies with prototypes, this paper addresses key limitations in current deep hierarchical clustering methods. Existing methods often tie the structure to the number of classes and underutilize the rich prototype information available at intermediate hierarchical levels. We introduce deep taxonomic networks, a novel deep latent variable approach designed to bridge these gaps. Our method optimizes a large latent taxonomic hierarchy, specifically a complete binary tree structured mixture-of-Gaussian prior within a variational inference framework, to automatically discover taxonomic structures and associated prototype clusters directly from unlabeled data without assuming true label sizes. We analytically show that optimizing the ELBO of our method encourages the discovery of hierarchical relationships among prototypes. Empirically, our learned models demonstrate strong hierarchical clustering performance, outperforming baselines across diverse image classification datasets using our novel evaluation mechanism that leverages prototype clusters discovered at all hierarchical levels. Qualitative results further reveal that deep taxonomic networks discover rich and interpretable hierarchical taxonomies, capturing both coarse-grained semantic categories and fine-grained visual distinctions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kmooIAb3Pd": {
    "title": "Stochastically Dominant Peer Prediction",
    "volume": "poster",
    "abstract": "Eliciting reliable human feedback is essential for many machine learning tasks, such as learning from noisy labels and aligning AI systems with human preferences. Peer prediction mechanisms incentivize truthful reporting without ground truth verification by scoring agents based on correlations with peers. Traditional mechanisms, which ensure that truth-telling maximizes the \\textbf{expected scores} in equilibrium, can elicit honest information while assuming agents' utilities are \\textbf{linear functions} of their scores. However, in practice, non-linear payment rules are usually preferred, or agents' utilities are inherently non-linear. We propose \\emph{stochastically dominant truthfulness (SD-truthfulness)} as a stronger guarantee: the score distribution of truth-telling stochastically dominates all other strategies, incentivizing truthful reporting for a wide range of monotone utility functions. Our first observation is that no existing peer prediction mechanism naturally satisfies this criterion without strong assumptions. A simple solution - rounding scores into binary lotteries — can enforce SD-truthfulness, but often degrades \\emph{sensitivity}, a key property related to fairness and statistical efficiency. We demonstrate how a more careful application of rounding can better preserve sensitivity. Furthermore, we introduce a new enforced agreement (EA) mechanism that is theoretically guaranteed to be SD-truthful in binary-signal settings and, under mild assumptions, empirically achieves the highest sensitivity among all known SD-truthful mechanisms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WQb0YrFl3H": {
    "title": "Unmasking Puppeteers: Leveraging Biometric Leakage to Expose Impersonation in AI-Based Videoconferencing",
    "volume": "poster",
    "abstract": "AI-based talking-head videoconferencing systems reduce bandwidth by transmitting a latent representation of a speaker's pose and expression, which is used to synthesize frames on the receiver's end. However, these systems are vulnerable to \"puppeteering\" attacks, where an adversary controls the identity of another person in real-time. Traditional deepfake detectors fail here, as all video content is synthetic. We propose a novel biometric defense that detects identity leakage in the transmitted latent representation. Our metric-learning approach disentangles identity cues from pose and expression, enabling detection of unauthorized swaps. Experiments across multiple talking-head models show that our method consistently outperforms prior defenses, operates in real time on consumer GPUs, and generalizes well to out-of-distribution data. By targeting the latent features shared during normal operation, our method offers a practical and robust safeguard against puppeteering",
    "checked": false,
    "id": "6f11bad73c2c2eb55bdc1b3c861f2fedaa0fdcc3",
    "semantic_title": "unmasking puppeteers: leveraging biometric leakage to disarm impersonation in ai-based videoconferencing",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0owHzJrnB6": {
    "title": "H-SPLID: HSIC-based Saliency Preserving Latent Information Decomposition",
    "volume": "poster",
    "abstract": "We introduce H-SPLID, a novel algorithm for learning salient feature representations through the explicit decomposition of salient and non-salient features into separate spaces. We show that H-SPLID promotes learning low-dimensional, task-relevant features. We prove that the expected prediction deviation under input perturbations is upper-bounded by the dimension of the salient subspace and the Hilbert-Schmidt Independence Criterion (HSIC) between inputs and representations. This establishes a link between robustness and latent representation compression in terms of the dimensionality and information preserved. Empirical evaluations on image classification tasks show that models trained with H-SPLID primarily rely on salient input components, as indicated by reduced sensitivity to perturbations affecting non-salient features, such as image backgrounds",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=on6Hf0KP20": {
    "title": "SQLens: An End-to-End Framework for Error Detection and Correction in Text-to-SQL",
    "volume": "poster",
    "abstract": "Text-to-SQL systems translate natural language (NL) questions into SQL queries, enabling non-technical users to interact with structured data. While large language models (LLMs) have shown promising results on the text-to-SQL task, they often produce semantically incorrect yet syntactically valid queries, with limited insight into their reliability. We propose SQLens, an end-to-end framework for fine-grained detection and correction of semantic errors in LLM-generated SQL. SQLens integrates error signals from both the underlying database and the LLM to identify potential semantic errors within SQL clauses. It further leverages these signals to guide query correction. Empirical results on two public benchmarks show that SQLens outperforms the best LLM-based self-evaluation method by 25.78% in F1 for error detection, and improves execution accuracy of out-of-the-box text-to-SQL systems by up to 20%",
    "checked": true,
    "id": "6f5bf29fbb37c9e82801ab4b2e6fe612bdd10367",
    "semantic_title": "sqlens: an end-to-end framework for error detection and correction in text-to-sql",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=b8XmjZmsN0": {
    "title": "Causal Explanation-Guided Learning for Organ Allocation",
    "volume": "poster",
    "abstract": "A central challenge in organ transplantation is the extremely low acceptance rate of donor organ offers—typically in the single digits—leading to high discard rates and suboptimal use of available grafts. Current acceptance models embedded in allocation systems are non-causal, trained on observational data, and fail to generalize to policy-relevant counterfactuals. This limits their reliability for both policy evaluation and simulator-based optimization. In this work, we reframe organ offer acceptance as a counterfactual prediction problem and propose a method to learn from routinely recorded—but often overlooked—refusal explanations. These refusal reasons act as direction-only counterfactual signals: for example, a refusal reason such as \"old donor age\" implies acceptance might have occurred had the donor been younger. We formalize this setting and introduce ClexNet, a novel causal model that learns policy-invariant representations via balanced training and an explanation-guided augmentation loss. On both synthetic and semi-synthetic data, ClexNet outperforms existing acceptance models in predictive performance, generalization, and calibration, offering a robust drop-in improvement for simulators and allocation policy evaluation. Beyond transplantation, our approach provides a general method for incorporating human direction-only explanations as a form of model supervision, improving performance in settings where only observational data is available",
    "checked": false,
    "id": "0f22c6f57261296e30bf55300740285c13fedc05",
    "semantic_title": "working memory shapes information sampling and attention allocation across development",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FIfjDqjV0B": {
    "title": "Unifying Attention Heads and Task Vectors via Hidden State Geometry in In-Context Learning",
    "volume": "poster",
    "abstract": "The unusual properties of in-context learning (ICL) have prompted investigations into the internal mechanisms of large language models. Prior work typically focuses on either special attention heads or task vectors at specific layers, but lacks a unified framework linking these components to the evolution of hidden states across layers that ultimately produce the model's output. In this paper, we propose such a framework for ICL in classification tasks by analyzing two geometric factors that govern performance: the separability and alignment of query hidden states. A fine-grained analysis of layer-wise dynamics reveals a striking two-stage mechanism—separability emerges in early layers, while alignment develops in later layers. Ablation studies further show that Previous Token Heads drive separability, while Induction Heads and task vectors enhance alignment. Our findings thus bridge the gap between attention heads and task vectors, offering a unified account of ICL's underlying mechanisms",
    "checked": true,
    "id": "f87946db23aba932641d77fcae9ae0df3eeb4313",
    "semantic_title": "unifying attention heads and task vectors via hidden state geometry in in-context learning",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=xEauKyHVJj": {
    "title": "RNNs perform task computations by dynamically warping neural representations",
    "volume": "poster",
    "abstract": "Analysing how neural networks represent data features in their activations can help interpret how they perform tasks. Hence, a long line of work has focused on mathematically characterising the geometry of such \"neural representations.\" In parallel, machine learning has seen a surge of interest in understanding how dynamical systems perform computations on time-varying input data. Yet, the link between computation-through-dynamics and representational geometry remains poorly understood. Here, we hypothesise that recurrent neural networks (RNNs) perform computations by dynamically warping their representations of task variables. To test this hypothesis, we develop a Riemannian geometric framework that enables the derivation of the manifold topology and geometry of a dynamical system from the manifold of its inputs. By characterising the time-varying geometry of RNNs, we show that dynamic warping is a fundamental feature of their computations",
    "checked": false,
    "id": "2f31465fe86d0f1fb805c9abdfecfce4850f0d82",
    "semantic_title": "predictive learning enables compositional representations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kxiw0Mnt1y": {
    "title": "RPG360: Robust 360 Depth Estimation with Perspective Foundation Models and Graph Optimization",
    "volume": "poster",
    "abstract": "The increasing use of 360$^\\circ$ images across various domains has emphasized the need for robust depth estimation techniques tailored for omnidirectional images. However, obtaining large-scale labeled datasets for 360$^\\circ$ depth estimation remains a significant challenge. In this paper, we propose RPG360, a training-free robust 360$^\\circ$ monocular depth estimation method that leverages perspective foundation models and graph optimization. Our approach converts 360$^\\circ$ images into six- face cubemap representations, where a perspective foundation model is employed to estimate depth and surface normals. To address depth scale inconsistencies across different faces of the cubemap, we introduce a novel depth scale alignment technique using graph-based optimization, which parameterizes the predicted depth and normal maps while incorporating an additional per-face scale parameter. This optimization ensures depth scale consistency across the six-face cubemap while preserving 3D structural integrity. Furthermore, as foundation models exhibit inherent robustness in zero-shot settings, our method achieves superior performance across diverse datasets, including Matterport3D, Stanford2D3D, and 360Loc. We also demonstrate the versatility of our depth estimation approach by validating its benefits in downstream tasks such as feature matching 3.2 ∼ 5.4% and Structure from Motion 0.2 ∼ 9.7% in AUC@5$^\\circ$",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Km8P3gtJzO": {
    "title": "Predicting partially observable dynamical systems via diffusion models with a multiscale inference scheme",
    "volume": "poster",
    "abstract": "Conditional diffusion models provide a natural framework for probabilistic prediction of dynamical systems and have been successfully applied to fluid dynamics and weather prediction. However, in many settings, the available information at a given time represents only a small fraction of what is needed to predict future states, either due to measurement uncertainty or because only a small fraction of the state can be observed. This is true for example in solar physics, where we can observe the Sun's surface and atmosphere, but its evolution is driven by internal processes for which we lack direct measurements. In this paper, we tackle the probabilistic prediction of partially observable, long-memory dynamical systems, with applications to solar dynamics and the evolution of active regions. We show that standard inference schemes, such as autoregressive rollouts, fail to capture long-range dependencies in the data, largely because they do not integrate past information effectively. To overcome this, we propose a multiscale inference scheme for diffusion models, tailored to physical processes. Our method generates trajectories that are temporally fine-grained near the present and coarser as we move farther away, which enables capturing long-range temporal dependencies without increasing computational cost. When integrated into a diffusion model, we show that our inference scheme significantly reduces the bias of the predicted distributions and improves rollout stability",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zyopvwZbSj": {
    "title": "GeneFlow: Translation of Single-cell Gene Expression to Histopathological Images via Rectified Flow",
    "volume": "poster",
    "abstract": "Spatial transcriptomics technologies can be used to align transcriptomes with histopathological morphology, presenting exciting new opportunities for biomolecular discovery. Using spatial transcriptomic gene expression and corresponding histology data, we construct a novel framework, GeneFlow, to map single- and multi-cell gene expression onto paired cellular images. By combining an attention-based RNA encoder with a conditional UNet guided by rectified flow, we generate high-resolution images with different staining methods (e.g., H\\&E, DAPI) to highlight various cellular/ tissue structures. Rectified flow with high-order ODE solvers creates a continuous, bijective mapping between expression and image manifolds, addressing the many-to-one relationship inherent in this problem. Our method enables the generation of realistic cellular morphology features and spatially resolved intercellular interactions under genetic or chemical perturbations. This enables minimally invasive disease diagnosis by revealing dysregulated patterns in imaging phenotypes. Our rectified flow based method outperforms diffusion methods and baselines in all experiments. Code is available at https://github.com/wangmengbo/GeneFlow",
    "checked": true,
    "id": "a975ba742c61af322f7c44267c2667ff10a839d0",
    "semantic_title": "geneflow: translation of single-cell gene expression to histopathological images via rectified flow",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EPDLFWyNnF": {
    "title": "Geometric Algorithms for Neural Combinatorial Optimization with Constraints",
    "volume": "poster",
    "abstract": "Self-Supervised Learning (SSL) for Combinatorial Optimization (CO) is an emerging paradigm for solving combinatorial problems using neural networks. In this paper, we address a central challenge of SSL for CO: solving problems with discrete constraints. We design an end-to-end differentiable framework that enables us to solve discrete constrained optimization problems with neural networks. Concretely, we leverage algorithmic techniques from the literature on convex geometry and Carathéodory's theorem to decompose neural network outputs into convex combinations of polytope corners that correspond to feasible sets. This decomposition-based approach enables self-supervised training but also ensures efficient quality-preserving rounding of the neural net output into feasible solutions. Extensive experiments in cardinality-constrained optimization show that our approach can consistently outperform neural baselines. We further provide worked-out examples of how our method can be applied beyond cardinality-constrained problems to a diverse set of combinatorial optimization tasks, including finding independent sets in graphs, and solving matroid-constrained problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BlLAmMFEzJ": {
    "title": "NeuralPLexer3: Accurate Biomolecular Complex Structure Prediction with Flow Models",
    "volume": "poster",
    "abstract": "Biomolecular structure determination is essential to a mechanistic understanding of diseases and the development of novel therapeutics. Machine-learning-based structure prediction methods have made significant advancements by computationally predicting protein and bioassembly structures from sequences and molecular topology alone. Despite substantial progress in the field, challenges remain to deliver structure prediction models to real-world drug discovery. Here, we present NeuralPLexer3 -- a physics-inspired flow-based generative model that achieves state-of-the-art prediction accuracy on key biomolecular interaction types and improves training and sampling efficiency compared to its predecessors and alternative methodologies. Examined through existing and new benchmarks, NeuralPLexer3 excels in areas crucial to structure-based drug design, including blind docking, physical validity, and ligand-induced protein conformational changes",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kcSbYJRQub": {
    "title": "AR-RAG: Autoregressive Retrieval Augmentation for Image Generation",
    "volume": "poster",
    "abstract": "We introduce Autoregressive Retrieval Augmentation (AR-RAG), a novel paradigm that enhances image generation by autoregressively incorporating k-nearest neighbor retrievals at the patch level. Unlike prior methods that perform a single, static retrieval before generation and condition the entire generation on fixed reference images, AR-RAG performs context-aware retrievals at each generation step, using prior-generated patches as queries to retrieve and incorporate the most relevant patch-level visual references, enabling the model to respond to evolving generation needs while avoiding limitations (e.g., over-copying, stylistic bias, etc.) prevalent in existing methods. To realize AR-RAG, we propose two parallel frameworks: (1) Distribution-Augmentation in Decoding (DAiD), a training-free plug-and-use decoding strategy that directly merges the distribution of model-predicted patches with the distribution of retrieved patches, and (2) Feature-Augmentation in Decoding (FAiD), a parameter-efficient fine-tuning method that progressively smooths the features of retrieved patches via multi-scale convolution operations and leverages them to augment the image generation process. We validate the effectiveness of AR-RAG on widely adopted benchmarks, including Midjourney-30K, GenEval and DPG-Bench, demonstrating significant performance gains over state-of-the-art image generation models",
    "checked": true,
    "id": "1a59f7625d970720c84b57bba7083344c9fd2760",
    "semantic_title": "ar-rag: autoregressive retrieval augmentation for image generation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=AmZ7uHDJiR": {
    "title": "NFL-BA: Near-Field Light Bundle Adjustment for SLAM in Dynamic Lighting",
    "volume": "poster",
    "abstract": "Simultaneous Localization and Mapping (SLAM) systems typically assume static, distant illumination; however, many real-world scenarios, such as endoscopy, subterranean robotics, and search & rescue in collapsed environments, require agents to operate with a co-located light and camera in the absence of external lighting. In such cases, dynamic near-field lighting introduces strong, view-dependent shading that significantly degrades SLAM performance. We introduce Near-Field Lighting Bundle Adjustment Loss (NFL-BA) which explicitly models near-field lighting as a part of Bundle Adjustment loss and enables better performance for scenes captured with dynamic lighting. NFL-BA can be integrated into neural rendering-based SLAM systems with implicit or explicit scene representations. Our evaluations mainly focus on endoscopy procedure where SLAM can enable autonomous navigation, guidance to unsurveyed regions, blindspot detections, and 3D visualizations, which can significantly improve patient outcomes and endoscopy experience for both physicians and patients. Replacing Photometric Bundle Adjustment loss of SLAM systems with NFL-BA leads to significant improvement in camera tracking, 37% for MonoGS and 14% for EndoGSLAM, and leads to state-of-the-art camera tracking and mapping performance on the C3VD colonoscopy dataset. Further evaluation on indoor scenes captured with phone camera with flashlight turned on, also demonstrate significant improvement in SLAM performance due to NFL-BA",
    "checked": true,
    "id": "b84b01c7b65d48d189d1dfbfc758fb3d18bc935c",
    "semantic_title": "nfl-ba: near-field light bundle adjustment for slam in dynamic lighting",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=DcpUkPHRJw": {
    "title": "Cooperative Bargaining Games Without Utilities: Mediated Solutions from Direction Oracles",
    "volume": "poster",
    "abstract": "Cooperative bargaining games are widely used to model resource allocation and conflict resolution. Traditional solutions assume the mediator can access agents' utility function values and gradients. However, there is an increasing number of settings, such as human-AI interactions, where utility values may be inaccessible or incomparable due to unknown, nonaffine transformations. To model such settings, we consider that the mediator has access only to agents' $\\textit{most preferred directions}-$normalized utility gradients in the decision space. To this end, we propose a cooperative bargaining algorithm where a mediator has access to only the direction oracle of each agent. We prove that unlike popular approaches such as the Nash and Kalai-Smorodinsky bargaining solutions, our approach is invariant to monotonic nonaffine transformations, and that under strong convexity and smoothness assumptions, this approach enjoys global asymptotic convergence to Pareto stationary solutions. Moreover, we show that the bargaining solutions found by our algorithm also satisfy the axioms of symmetry and (under slightly stronger conditions) independence of irrelevant alternatives, which are popular in the literature. Finally, we conduct experiments in two domains, multi-agent formation assignment and mediated stock portfolio allocation, which validate these theoretical results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nfZmQgxyyN": {
    "title": "Rotary Masked Autoencoders are Versatile Learners",
    "volume": "poster",
    "abstract": "Applying Transformers to irregular time-series typically requires specializations to their baseline architecture, which can result in additional computational overhead and increased method complexity. We present the Rotary Masked Autoencoder (RoMAE), which utilizes the popular Rotary Positional Embedding (RoPE) method for continuous positions. RoMAE is an extension to the Masked Autoencoder (MAE) that enables interpolation and representation learning with multidimensional continuous positional information while avoiding any time-series-specific architectural specializations. We showcase RoMAE's performance on a variety of modalities including irregular and multivariate time-series, images, and audio, demonstrating that RoMAE surpasses specialized time-series architectures on difficult datasets such as the DESC ELAsTiCC Challenge while maintaining MAE's usual performance across other modalities. In addition, we investigate RoMAE's ability to reconstruct the embedded continuous positions, demonstrating that including learned embeddings in the input sequence breaks RoPE's relative position property",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=irYb8GGDyh": {
    "title": "Inference-Time Personalized Alignment with a Few User Preference Queries",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=00oRAPDWsX": {
    "title": "KL Penalty Control via Perturbation for Direct Preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d885b445226dc2979ad9d4f2a67f05c4ef18045f",
    "semantic_title": "kl penalty control via perturbation for direct preference optimization",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=FXTg2P8OQz": {
    "title": "BoltzNCE: Learning likelihoods for Boltzmann Generation with Stochastic Interpolants and Noise Contrastive Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "d103b97c6cc629c9bd84d16f6ded9e31bacf8582",
    "semantic_title": "boltznce: learning likelihoods for boltzmann generation with stochastic interpolants and noise contrastive estimation",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=IeM6Io4Rsh": {
    "title": "Global Minimizers of Sigmoid Contrastive Loss",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=um9kHMof0c": {
    "title": "Better Estimation of the Kullback--Leibler Divergence Between Language Models",
    "volume": "poster",
    "abstract": "Estimating the Kullback--Leibler (KL) divergence between language models has many applications, e.g., reinforcement learning from human feedback (RLHF), interpretability, and knowledge distillation. However, computing the exact KL divergence between two arbitrary language models is intractable. Thus, practitioners often resort to sampling-based estimators. While it is easy to fashion a simple Monte Carlo (MC) estimator that provides an unbiased estimate of the KL divergence between language models, this estimator notoriously suffers from high variance and can even result in a negative estimate of the KL divergence, a non-negative quantity. In this paper, we introduce a Rao--Blackwellized estimator that is unbiased and provably has variance less than or equal to that of the standard Monte Carlo estimator. In an empirical study on sentiment-controlled fine-tuning, we show that our estimator provides more stable KL estimates and reduces variance substantially. Additionally, we derive an analogous Rao--Blackwellized estimator of the gradient of the KL divergence, which leads to more stable training and produces models that more frequently appear on the Pareto frontier of reward vs. KL compared to the ones trained with the MC estimator of the gradient",
    "checked": true,
    "id": "cb8780e91d7f03d81970b5c714368a669033542a",
    "semantic_title": "better estimation of the kullback--leibler divergence between language models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ftVlLG9cks": {
    "title": "The Surprising Effectiveness of Negative Reinforcement in LLM Reasoning",
    "volume": "poster",
    "abstract": "Reinforcement learning with verifiable rewards (RLVR) is a promising approach for training language models (LMs) on reasoning tasks that elicit emergent long chains of thought (CoTs). Unlike supervised learning, it updates the model using both correct and incorrect samples via policy gradients. To better understand its mechanism, we decompose the learning signal into reinforcing correct responses and penalizing incorrect ones, referred to as **P**ositive and **N**egative **S**ample **R**einforcement (**PSR** and **NSR**), respectively. We train `Qwen2.5-Math-7B`, `Qwen3-4B` and `Llama-3.1-8B-Instruct` on a mathematical reasoning dataset and uncover a surprising result: training with only negative samples — without reinforcing correct responses — can be highly effective: it consistently improves performance over the base model across the entire Pass@$k$ spectrum $k$ up to 256), often matching or surpassing PPO and GRPO. In contrast, reinforcing only correct responses improves Pass@1 but degrades performance at higher $k$, due to reduced diversity. These inference-scaling trends highlight that solely penalizing incorrect responses may contribute more to performance than previously recognized. Through gradient analysis, we show that NSR works by suppressing incorrect generations and redistributing probability mass toward other plausible candidates, guided by the model's prior beliefs. It refines the model's existing knowledge rather than introducing entirely new behaviors. Building on this insight, we propose a simple variant of the RL objective that upweights NSR, and show that it consistently improves overall Pass@$k$ performance on MATH, AIME 2025, and AMC23. Our code is available at [`https://github.com/TianHongZXY/RLVR-Decomposed`](https://github.com/TianHongZXY/RLVR-Decomposed)",
    "checked": true,
    "id": "a7933f44a76c680b074591deb74b444ab8c748e0",
    "semantic_title": "the surprising effectiveness of negative reinforcement in llm reasoning",
    "citation_count": 49,
    "authors": []
  },
  "https://openreview.net/forum?id=NRvxzOdSPU": {
    "title": "Attention-based clustering",
    "volume": "poster",
    "abstract": "Transformers have emerged as a powerful neural network architecture capable of tackling a wide range of learning tasks. In this work, we provide a theoretical analysis of their ability to automatically extract structure from data in an unsupervised setting. In particular, we demonstrate their suitability for clustering when the input data is generated from a Gaussian mixture model. To this end, we study a simplified two-head attention layer and define a population risk whose minimization with unlabeled data drives the head parameters to align with the true mixture centroids",
    "checked": true,
    "id": "6ee189230b51273d1fd2695e4fc95713c7b6e429",
    "semantic_title": "attention-based clustering",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D9JeNTs5Bu": {
    "title": "ReliabilityRAG: Effective and Provably Robust Defense for RAG-based Web-Search",
    "volume": "poster",
    "abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models by grounding their outputs in external documents. These systems, however, remain vulnerable to attacks on the retrieval corpus, such as prompt injection. RAG-based search systems (e.g., Google's Search AI Overview) present an interesting setting for studying and protecting against such threats, as defense algorithms can benefit from built-in reliability signals—like document ranking—and represent a non-LLM challenge for the adversary due to decades of work to thwart SEO. Motivated by, but not limited to, this scenario, this work introduces ReliabilityRAG, a framework for adversarial robustness that explicitly leverages reliability information of retrieved documents. Our first contribution adopts a graph-theoretic perspective to identify a ``consistent majority'' among retrieved documents to filter out malicious ones. We introduce a novel algorithm based on finding a Maximum Independent Set (MIS) on a document graph where edges encode contradiction. Our MIS variant explicitly prioritizes higher-reliability documents and provides provable robustness guarantees against bounded adversarial corruption under natural assumptions. Recognizing the computational cost of exact MIS for large retrieval sets, our second contribution is a scalable weighted sample and aggregate framework. It explicitly utilizes reliability information, preserving some robustness guarantees while efficiently handling many documents. We present empirical results showing ReliabilityRAG provides superior robustness against adversarial attacks compared to prior methods, maintains high benign accuracy, and excels in long-form generation tasks where prior robustness-focused methods struggled. Our work is a significant step towards more effective, provably robust defenses against retrieved corpus corruption in RAG",
    "checked": true,
    "id": "80c7921964f622abe6c023a2df5d5a824f9c66d9",
    "semantic_title": "reliabilityrag: effective and provably robust defense for rag-based web-search",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cFVQJepi4e": {
    "title": "How Many Tokens Do 3D Point Cloud Transformer Architectures Really Need?",
    "volume": "poster",
    "abstract": "Recent advances in 3D point cloud transformers have led to state-of-the-art results in tasks such as semantic segmentation and reconstruction. However, these models typically rely on dense token representations, incurring high computational and memory costs during training and inference. In this work, we present the finding that tokens are remarkably redundant, leading to substantial inefficiency. We introduce an efficient token merging method and illustrate that it can reduce the token count by up to 90–95% while maintaining competitive performance. This finding challenges the prevailing assumption that more tokens inherently yield better performance and highlights that many current models are over-tokenized and under-optimized for scalability. We validate our method across multiple 3D vision tasks and show consistent improvements in computational efficiency. This work is the first to assess redundancy in large-scale 3D transformer models, providing insights into the development of more efficient 3D foundation architectures. Our code and checkpoints are publicly available at https://gitmerge3d.github.io",
    "checked": true,
    "id": "540781d7ce994b40f026df1480e0048d06b987c4",
    "semantic_title": "how many tokens do 3d point cloud transformer architectures really need?",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2hiNrfMmQ7": {
    "title": "Information Retrieval Induced Safety Degradation in AI Agents",
    "volume": "poster",
    "abstract": "Despite the growing integration of retrieval-enabled AI agents into society, their safety and ethical behavior remain inadequately understood. In particular, the growing integration of LLMs and AI agents with external information sources and real-world environments raises critical questions about how they engage with and are influenced by these external data sources and interactive contexts. This study investigates how expanding retrieval access—from no external sources to Wikipedia-based retrieval and open web search—affects model reliability, bias propagation, and harmful content generation. Through extensive benchmarking of censored and uncensored LLMs and AI Agents, our findings reveal a consistent degradation in refusal rates, bias sensitivity, and harmfulness safeguards as models gain broader access to external sources, culminating in a phenomenon we term safety degradation. Notably, retrieval-enabled agents built on aligned LLMs often behave more unsafely than uncensored models without retrieval. This effect persists even under strong retrieval accuracy and prompt-based mitigation, suggesting that the mere presence of retrieved content reshapes model behavior in structurally unsafe ways. These findings underscore the need for robust mitigation strategies to ensure fairness and reliability in retrieval-enabled and increasingly autonomous AI systems",
    "checked": true,
    "id": "6b7602a9146087542bcdbaade9fe8801103e6730",
    "semantic_title": "information retrieval induced safety degradation in ai agents",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rMqQdJJz5r": {
    "title": "Learning Reconfigurable Representations for Multimodal Federated Learning with Missing Data",
    "volume": "poster",
    "abstract": "Multimodal federated learning in real-world settings often encounters incomplete and heterogeneous data across clients. This results in misaligned local feature representations that limit the effectiveness of model aggregation. Unlike prior work that assumes either differing modality sets without missing input features or a shared modality set with missing features across clients, we consider a more general and realistic setting where each client observes a different subset of modalities and might also have missing input features within each modality. To address the resulting misalignment in learned representations, we propose a new federated learning framework featuring locally adaptive representations based on learnable client-side embedding controls that encode each client's data-missing patterns. These embeddings serve as reconfiguration signals that align the globally aggregated representation with each client's local context, enabling more effective use of shared information. Furthermore, the embedding controls can be algorithmically aggregated across clients with similar data-missing patterns to enhance the robustness of reconfiguration signals in adapting the global representation. Empirical results on multiple federated multimodal benchmarks with diverse data-missing patterns across clients demonstrate the efficacy of the proposed method, achieving up to 36.45\\% performance improvement under severe data incompleteness. The method is also supported by a theoretical analysis with an explicit performance bound that matches our empirical observations. Our source codes are provided at [https://github.com/nmduonggg/PEPSY](https://github.com/nmduonggg/PEPSY)",
    "checked": true,
    "id": "e4858804392445a60a54310206ce110b60f5de14",
    "semantic_title": "learning reconfigurable representations for multimodal federated learning with missing data",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OFSAgrid1R": {
    "title": "Fair Continuous Resource Allocation with Equality of Impact",
    "volume": "poster",
    "abstract": "Recent works have studied fair resource allocation in social settings, where fairness is judged by the impact of allocation decisions rather than more traditional minimum or maximum thresholds on the allocations themselves. Our work significantly adds to this literature by developing continuous resource allocation strategies that adhere to *equality of impact*, a generalization of equality of opportunity. We derive methods to maximize total welfare across groups subject to minimal violation of equality of impact, in settings where the outcomes of allocations are unknown but have a diminishing marginal effect. While focused on a two-group setting, our study addresses a broader class of welfare dynamics than explored in prior work. Our contributions are threefold. First, we introduce *Equality of Impact (EoI)*, a fairness criterion defined via group-level impact functions. Second, we design an online algorithm for non-noisy settings that leverages the problem's geometric structure and achieves constant cumulative fairness regret. Third, we extend this approach to noisy environments with a meta-algorithm and empirically demonstrate that our methods find fair allocations and perform competitively relative to representative baselines",
    "checked": true,
    "id": "bffec0f0a4600a8b5bfc8310c649908a8d807b27",
    "semantic_title": "fair continuous resource allocation with equality of impact",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oB5GHrsfI4": {
    "title": "3D Human Pose Estimation with Muscles",
    "volume": "poster",
    "abstract": "We introduce MusclePose as an end-to-end learnable physics-infused 3D human pose estimator that incorporates muscle-dynamics modeling to infer human dynamics from monocular video. Current physics pose estimators aim to predict physically plausible poses by enforcing the underlying dynamics equations that govern motion. Since this is an underconstrained problem without force-annotated data, methods often estimate kinetics with external physics optimizers that may not be compatible with existing learning frameworks, or are too slow for real-time inference. While more recent methods use a regression-based approach to overcome these issues, the estimated kinetics can be seen as auxiliary predictions, and may not be physically plausible. To this end, we build on existing regression-based approaches, and aim to improve the biofidelity of kinetic inference with a multihypothesis approach --- by inferring joint torques via Lagrange's equations and via muscle dynamics modeling with muscle torque generators. Furthermore, MusclePose predicts detailed human anthropometrics based on values from biomechanics studies, in contrast to existing physics pose estimators that construct their human models with shape primitives. We show that MusclePose is competitive with existing 3D pose estimators in positional accuracy, while also able to infer plausible human kinetics and muscle signals consistent with values from biomechanics studies, without requiring an external physics engine",
    "checked": false,
    "id": "92d61a05d76cc859c073e57eedbd82186aecdaff",
    "semantic_title": "3d human pose estimation with spatio-temporal criss-cross attention",
    "citation_count": 122,
    "authors": []
  },
  "https://openreview.net/forum?id=UoKt9B1aY8": {
    "title": "HiPoNet: A Multi-View Simplicial Complex Network for High Dimensional Point-Cloud and Single-Cell data",
    "volume": "poster",
    "abstract": "In this paper, we propose HiPoNet, an end-to-end differentiable neural network for regression, classification, and representation learning on high-dimensional point clouds. Our work is motivated by single-cell data which can have very high-dimensionality --exceeding the capabilities of existing methods for point clouds which are mostly tailored for 3D data. Moreover, modern single-cell and spatial experiments now yield entire cohorts of datasets (i.e., one data set for every patient), necessitating models that can process large, high-dimensional point-clouds at scale. Most current approaches build a single nearest-neighbor graph, discarding important geometric and topological information. In contrast, HiPoNet models the point-cloud as a set of higher-order simplicial complexes, with each particular complex being created using a reweighting of features. This method thus generates multiple constructs corresponding to different views of high-dimensional data, which in biology offers the possibility of disentangling distinct cellular processes. It then employs simplicial wavelet transforms to extract multiscale features, capturing both local and global topology from each view. We show that geometric and topological information is preserved in this framework both theoretically and empirically. We showcase the utility of HiPoNet on point-cloud level tasks, involving classification and regression of entire point-clouds in data cohorts. Experimentally, we find that HiPoNet outperforms other point-cloud and graph-based models on single-cell data. We also apply HiPoNet to spatial transcriptomics datasets using spatial coordinates as one of the views. Overall, HiPoNet offers a robust and scalable solution for high-dimensional data analysis",
    "checked": true,
    "id": "814776737f5f880c15a21c75d06e1ae89e8f7ebd",
    "semantic_title": "hiponet: a multi-view simplicial complex network for high dimensional point-cloud and single-cell data",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=MULUaaNZFx": {
    "title": "SharpZO: Hybrid Sharpness-Aware Vision Language Model Prompt Tuning via Forward-Only Passes",
    "volume": "poster",
    "abstract": "Fine-tuning vision language models (VLMs) has achieved remarkable performance across various downstream tasks; yet, it requires access to model gradients through backpropagation (BP), making them unsuitable for memory-constrained, inference-only edge devices. To address this limitation, previous work has explored various BP-free fine-tuning methods. However, these approaches often rely on high-variance evolutionary strategies (ES) or zeroth-order (ZO) optimization, and often fail to achieve satisfactory performance. In this paper, we propose a hybrid Sharpness-aware Zeroth-order optimization (SharpZO) approach, specifically designed to enhance the performance of ZO VLM fine-tuning via a sharpness-aware warm-up training. SharpZO features a two-stage optimization process: a sharpness-aware ES stage that globally explores and smooths the loss landscape to construct a strong initialization, followed by a fine-grained local search via sparse ZO optimization. The entire optimization relies solely on forward passes. Detailed theoretical analysis and extensive experiments on CLIP models demonstrate that SharpZO significantly improves accuracy and convergence speed, achieving up to 7\\% average gain over state-of-the-art forward-only methods",
    "checked": true,
    "id": "04f29d7df7521d91971ce98a391745b73e0af4c7",
    "semantic_title": "sharpzo: hybrid sharpness-aware vision language model prompt tuning via forward-only passes",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=i3sWs5614Y": {
    "title": "Transfer Learning on Edge Connecting Probability Estimation Under Graphon Model",
    "volume": "poster",
    "abstract": "Graphon models provide a flexible nonparametric framework for estimating latent connectivity probabilities in networks, enabling a range of downstream applications such as link prediction and data augmentation. However, accurate graphon estimation typically requires a large graph, whereas in practice, one often only observes a small-sized network. One approach to addressing this issue is to adopt a transfer learning framework, which aims to improve estimation in a small target graph by leveraging structural information from a larger, related source graph. In this paper, we propose a novel method, namely GTRANS, a transfer learning framework that integrates neighborhood smoothing and Gromov-Wasserstein optimal transport to align and transfer structural patterns between graphs. To prevent negative transfer, GTRANS includes an adaptive debiasing mechanism that identifies and corrects for target-specific deviations via residual smoothing. We provide theoretical guarantees on the stability of the estimated alignment matrix and demonstrate the effectiveness of GTRANS in improving the accuracy of target graph estimation through extensive synthetic and real data experiments. These improvements translate directly to enhanced performance in downstream applications, such as the graph classification task and the link prediction task",
    "checked": true,
    "id": "816ed9d8b5a1afc3859ab4035306e3ff89b3815f",
    "semantic_title": "transfer learning on edge connecting probability estimation under graphon model",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FiZ7gadynD": {
    "title": "Learning Parameterized Skills from Demonstrations",
    "volume": "poster",
    "abstract": "We present DEPS, an end-to-end algorithm for discovering parameterized skills from expert demonstrations. Our method learns parameterized skill policies jointly with a meta-policy that selects the appropriate discrete skill and continuous parameters at each timestep. Using a combination of temporal variational inference and information-theoretic regularization methods, we address the challenge of degeneracy common in latent variable models, ensuring that the learned skills are temporally extended, semantically meaningful, and adaptable. We empirically show that learning parameterized skills from multitask expert demonstrations significantly improves generalization to unseen tasks. Our method outperforms multitask as well as skill learning baselines on both LIBERO and MetaWorld benchmarks. We also demonstrate that DEPS discovers interpretable parameterized skills, such as an object grasping skill whose continuous arguments define the grasp location",
    "checked": true,
    "id": "536f7361fcaff37d250a5f012c683ee39f6bdf9d",
    "semantic_title": "learning parameterized skills from demonstrations",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7VkFMKBVVp": {
    "title": "The Rise of Parameter Specialization for Knowledge Storage in Large Language Models",
    "volume": "poster",
    "abstract": "Over time, a growing wave of large language models from various series has been introduced to the community. Researchers are striving to maximize the performance of language models with constrained parameter sizes. However, from a microscopic perspective, there has been limited research on how to better store knowledge in model parameters, particularly within MLPs, to enable more effective utilization of this knowledge by the model. In this work, we analyze twenty publicly available open-source large language models to investigate the relationship between their strong performance and the way knowledge is stored in their corresponding MLP parameters. Our findings reveal that as language models become more advanced and demonstrate stronger knowledge capabilities, their parameters exhibit increased specialization. Specifically, parameters in the MLPs tend to be more focused on encoding similar types of knowledge. We experimentally validate that this specialized distribution of knowledge contributes to improving the efficiency of knowledge utilization in these models. Furthermore, by conducting causal training experiments, we confirm that this specialized knowledge distribution plays a critical role in improving the model's efficiency in leveraging stored knowledge",
    "checked": true,
    "id": "8c3873574450e963c3ac9caa50eb4d0ff12bfd9c",
    "semantic_title": "the rise of parameter specialization for knowledge storage in large language models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=5h9mS87Pyt": {
    "title": "The Atlas of In-Context Learning: How Attention Heads Shape In-Context Retrieval Augmentation",
    "volume": "poster",
    "abstract": "Large language models are able to exploit in-context learning to access external knowledge beyond their training data through retrieval-augmentation. While promising, its inner workings remain unclear. In this work, we shed light on the mechanism of in-context retrieval augmentation for question answering by viewing a prompt as a composition of informational components. We propose an attribution-based method to identify specialized attention heads, revealing in-context heads that comprehend instructions and retrieve relevant contextual information, and parametric heads that store entities' relational knowledge. To better understand their roles, we extract function vectors and modify their attention weights to show how they can influence the answer generation process. Finally, we leverage the gained insights to trace the sources of knowledge used during inference, paving the way towards more safe and transparent language models",
    "checked": true,
    "id": "16bdf18caed48aa2afe1a8711bf96630e9f88410",
    "semantic_title": "the atlas of in-context learning: how attention heads shape in-context retrieval augmentation",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=v04csnvCfd": {
    "title": "On the Relation between Rectified Flows and Optimal Transport",
    "volume": "poster",
    "abstract": "This paper investigates the connections between rectified flows, flow matching, and optimal transport. Flow matching is a recent approach to learning generative models by estimating velocity fields that guide transformations from a source to a target distribution. Rectified flow matching aims to straighten the learned transport paths, yielding more direct flows between distributions. Our first contribution is a set of invariance properties of rectified flows and explicit velocity fields. In addition, we also provide explicit constructions and analysis in the Gaussian (not necessarily independent) and Gaussian mixture settings and study the relation to optimal transport. Our second contribution addresses recent claims suggesting that rectified flows, when constrained such that the learned velocity field is a gradient, can yield (asymptotically) solutions to optimal transport problems. We study the existence of solutions for this problem and demonstrate that they only relate to optimal transport under assumptions that are significantly stronger than those previously acknowledged. In particular, we present several counterexamples that invalidate earlier equivalence results in the literature, and we argue that enforcing a gradient constraint on rectified flows is, in general, not a reliable method for computing optimal transport maps",
    "checked": true,
    "id": "8b61e7c4612b443a0620330a73bc432807b6796c",
    "semantic_title": "on the relation between rectified flows and optimal transport",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=fDR4hzavDF": {
    "title": "STaRFormer: Semi-Supervised Task-Informed Representation Learning via Dynamic Attention-Based Regional Masking for Sequential Data",
    "volume": "poster",
    "abstract": "Understanding user intent is essential for situational and context-aware decision-making. Motivated by a real-world scenario, this work addresses intent predictions of smart device users in the vicinity of vehicles by modeling sequential spatiotemporal data. However, in real-world scenarios, environmental factors and sensor limitations can result in non-stationary and irregularly sampled data, posing significant challenges. To address these issues, we propose STaRFormer, a Transformer-based approach that can serve as a universal framework for sequential modeling. STaRFormer utilizes a new dynamic attention-based regional masking scheme combined with a novel semi-supervised contrastive learning paradigm to enhance task-specific latent representations. Comprehensive experiments on 56 datasets varying in types (including non-stationary and irregularly sampled), tasks, domains, sequence lengths, training samples, and applications demonstrate the efficacy of STaRFormer, achieving notable improvements over state-of-the-art approaches",
    "checked": true,
    "id": "15a74e093d13697cafefc3b8a0a06d02b83c1138",
    "semantic_title": "starformer: semi-supervised task-informed representation learning via dynamic attention-based regional masking for sequential data",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d6wOZrzaWd": {
    "title": "Equivariance Everywhere All At Once: A Recipe for Graph Foundation Models",
    "volume": "poster",
    "abstract": "Graph machine learning architectures are typically tailored to specific tasks on specific datasets, which hinders their broader applicability. This has led to a new quest in graph machine learning: \\emph{how to build graph foundation models (GFMs)} capable of generalizing across arbitrary graphs and features? In this work, we present a recipe for designing GFMs for node-level tasks from first principles. The key ingredient underpinning our study is a systematic investigation of the symmetries that a graph foundation model must respect. In a nutshell, we argue that label permutation-equivariance alongside feature permutation-invariance are necessary in addition to the common node permutation-equivariance on each local neighborhood of the graph. To this end, we first characterize the space of linear transformations that are equivariant to permutations of nodes and labels, and invariant to permutations of features. We then prove that the resulting network is a universal approximator on multisets that respect the aforementioned symmetries. Our recipe uses such layers on the multiset of features induced by the local neighborhood of the graph to obtain a class of graph foundation models for node property prediction. We validate our approach through extensive experiments on 29 real-world node classification datasets, demonstrating both strong zero-shot empirical performance and consistent improvement as the number of training graphs increases",
    "checked": true,
    "id": "e9e58e59079e1a9ac58e15b642a5c88aedd9c48c",
    "semantic_title": "equivariance everywhere all at once: a recipe for graph foundation models",
    "citation_count": 8,
    "authors": []
  },
  "https://openreview.net/forum?id=cx50h8q0pG": {
    "title": "A CLT for Polynomial GNNs on Community-Based Graphs",
    "volume": "poster",
    "abstract": "We consider the empirical distribution of the embeddings of a $k$-layer polynomial GNN on a semi-supervised node classification task and prove a central limit theorem for them. Assuming a community based model for the underlying graph, with growing average degree $\\nu_n\\to\\infty$, we show that the empirical distribution of the centered features, when scaled by $\\nu_{n}^{k-1/2}$ converge in 1-Wasserstein distance to a centered stable mixture of multivariate normal distributions. In addition, the joint empirical distribution of uncentered features and labels when normalized by $\\nu_n^k$ approach that of mixture of multivariate normal distributions, with stable means and covariance matrices vanishing as $\\nu_n^{-1}$. We explicitly identify the asymptotic means and covariances, showing that the mixture collapses towards a 1-D version as $k$ is increased. Our results provides a precise and nuanced lens on how oversmoothing presents itself in the large graph limit, in the sparse regime. In particular, we show that training with cross-entropy on these embeddings is asymptotically equivalent to training on these nearly collapsed Gaussian mixtures",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UX143QGvb8": {
    "title": "Causally Reliable Concept Bottleneck Models",
    "volume": "poster",
    "abstract": "Concept-based models are an emerging paradigm in deep learning that constrains the inference process to operate through human-interpretable variables, facilitating explainability and human interaction. However, these architectures, on par with popular opaque neural models, fail to account for the true causal mechanisms underlying the target phenomena represented in the data. This hampers their ability to support causal reasoning tasks, limits out-of-distribution generalization, and hinders the implementation of fairness constraints. To overcome these issues, we propose Causally reliable Concept Bottleneck Models (C$^2$BMs), a class of concept-based architectures that enforce reasoning through a bottleneck of concepts structured according to a model of the real-world causal mechanisms. We also introduce a pipeline to automatically learn this structure from observational data and unstructured background knowledge (e.g., scientific literature). Experimental evidence suggests that C$^2$BMs are more interpretable, causally reliable, and improve responsiveness to interventions w.r.t. standard opaque and concept-based models, while maintaining their accuracy",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=98NrkXPRZ9": {
    "title": "For Better or for Worse, Transformers Seek Patterns for Memorization",
    "volume": "poster",
    "abstract": "Memorization in language models is a critical yet poorly understood phenomenon. In this work, we investigate memorization in transformer-based language models by analyzing their memorization dynamics during training over multiple epochs. We find that memorization is neither a constant accumulation of sequences nor simply dictated by the recency of exposure to these sequences. Instead, much like generalization, memorization appears to be driven by pattern recognition. Tracking memorization dynamics in mixed datasets, we observe that models memorize different sub-datasets in distinct bursts, suggesting that each subset is associated with unique underlying patterns, and that the model prefers to learn these patterns in a consistent order. We also find that easily learnable patterns tend to support generalization on unseen data, while more complex patterns do not. Furthermore, in datasets with weak or absent patterns, larger models may delay memorization relative to smaller ones, a behavior we term $\\textit{overthinking}$. Our results show that the subset of sequences memorized by a model over time is not arbitrary, and give insights into the internal processes a model goes through during training. Our code is available at: https://github.com/mdrpanwar/memorization-patterns",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QsguopCLfc": {
    "title": "T-norm Selection for Object Detection in Autonomous Driving with Logical Constraints",
    "volume": "poster",
    "abstract": "Integrating logical constraints into object detection models for autonomous driving (AD) is a promising way to enhance their compliance with rules and thereby increase the safety of the system. T-norms have been utilized to calculate the constrained loss, i.e., the violations of logical constraints as losses. While prior works have statically selected a few t-norms, we conduct an extensive experimental study to identify the most effective choices, as suboptimal t-norms can lead to undesired model behavior. To this end, we present MOD-ECL, a neurosymbolic framework that implements a wide range of t-norms and applies them in an adaptive manner. It includes an algorithm that selects well-performing t-norms during training and a scheduler that regulates the impact of the constrained loss. We evaluate its effectiveness on the ROAD-R and ROAD-Waymo-R datasets for object detection in AD, using attached common-sense constraints. Our results show that careful selection of parameters is crucial for effective constrained loss behavior. Moreover, our framework not only reduces constraint violations but also, in some cases, improves detection performance. Additionally, our methods offer fine-grained control over the trade-off between accuracy and constraint violation",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yNVDkAjGjw": {
    "title": "Semantic and Visual Crop-Guided Diffusion Models for Heterogeneous Tissue Synthesis in Histopathology",
    "volume": "poster",
    "abstract": "Synthetic data generation in histopathology faces unique challenges: preserving tissue heterogeneity, capturing subtle morphological features, and scaling to unannotated datasets. We present a latent diffusion model that generates realistic heterogeneous histopathology images through a novel dual-conditioning approach combining semantic segmentation maps with tissue-specific visual crops. Unlike existing methods that rely on text prompts or abstract visual embeddings, our approach preserves critical morphological details by directly incorporating raw tissue crops from corresponding semantic regions. For annotated datasets (i.e., Camelyon16, Panda), we extract patches ensuring 20-80% tissue heterogeneity. For unannotated data (i.e., TCGA), we introduce a self-supervised extension that clusters whole-slide images into 100 tissue types using foundation model embeddings, automatically generating pseudo-semantic maps for training. Our method synthesizes high-fidelity images with precise region-wise annotations, achieving superior performance on downstream segmentation tasks. When evaluated on annotated datasets, models trained on our synthetic data show competitive performance to those trained on real data, demonstrating the utility of controlled heterogeneous tissue generation. In quantitative evaluation, prompt‐guided synthesis reduces Fréchet Distance by up to 6× on Camelyon16 (from 430.1 to 72.0) and yields 2–3× lower FD across Panda and TCGA. Downstream DeepLabv3+ models trained solely on synthetic data attain test IoU of 0.71 and 0.95 on Camelyon16 and Panda, within 1–2% of real‐data baselines (0.72 and 0.96). By scaling to 11,765 TCGA whole‐slide images without manual annotations, our framework offers a practical solution for an urgent need for generating diverse, annotated histopathology data, addressing a critical bottleneck in computational pathology",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vZfqDwF09z": {
    "title": "Let Brain Rhythm Shape Machine Intelligence for Connecting Dots on Graphs",
    "volume": "poster",
    "abstract": "In both neuroscience and artificial intelligence (AI), it is well-established that neural \"coupling\" gives rise to dynamically distributed systems. These systems exhibit self-organized spatiotemporal patterns of synchronized neural oscillations, enabling the representation of abstract concepts. By capitalizing on the unprecedented amount of human neuroimaging data, we propose that advancing the theoretical understanding of rhythmic coordination in neural circuits can offer powerful design principles for the next generation of machine learning models with improved efficiency and robustness. To this end, we introduce a physics-informed deep learning framework for \\underline{B}rain \\underline{R}hythm \\underline{I}dentification by \\underline{K}uramoto and \\underline{C}ontrol (coined \\modelname{}) to characterize the synchronization of neural oscillations that shapes the dynamics of evolving cognitive states. Recognizing that brain networks are structurally connected yet behaviorally dynamic, we further conceptualize rhythmic neural activity as an artificial dynamical system of coupled oscillators, offering a shared mechanistic bridge to brain-inspired machine intelligence. By treating each node as an oscillator interacting with its neighbors, this approach moves beyond the conventional paradigm of graph heat diffusion and establishes a new regime of representation compression through oscillatory synchronization. Empirical evaluations demonstrate that this synchronization-driven mechanism not only mitigates over-smoothing in deep GNNs but also enhances the model's capacity for reasoning and solving complex graph-based problems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l5N0P1UJKP": {
    "title": "Hybrid Autoencoders for Tabular Data: Leveraging Model-Based Augmentation in Low-Label Settings",
    "volume": "poster",
    "abstract": "Deep neural networks often under-perform on tabular data due to their sensitivity to irrelevant features and a spectral bias toward smooth, low-frequency functions. These limitations hinder their ability to capture the sharp, high-frequency signals that often define tabular structure, especially under limited labeled samples. While self-supervised learning (SSL) offers promise in such settings, it remains challenging in tabular domains due to the lack of effective data augmentations. We propose a hybrid autoencoder that combines a neural encoder with an oblivious soft decision tree (OSDT) encoder, each guided by its own stochastic gating network that performs sample-specific feature selection. Together, these structurally different encoders and model-specific gating networks implement model-based augmentation, producing complementary input views tailored to each architecture. The two encoders, trained with a shared decoder and cross-reconstruction loss, learn distinct yet aligned representations that reflect their respective inductive biases. During training, the OSDT encoder (robust to noise and effective at modeling localized, high-frequency structure) guides the neural encoder toward representations more aligned with tabular data. At inference, only the neural encoder is used, preserving flexibility and SSL compatibility. Spectral analysis highlights the distinct inductive biases of each encoder. Our method achieves consistent gains in low-label classification and regression across diverse tabular datasets, outperforming deep and tree-based supervised baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XIeE8jbM4K": {
    "title": "Sketched Adaptive Distributed Deep Learning: A Sharp Convergence Analysis",
    "volume": "poster",
    "abstract": "Combining gradient compression with adaptive optimizers is a highly desirable goal in distributed learning, with potential benefits in both fewer communication rounds and less per-round communication. In spite of preliminary empirical promise, certain major challenges in the convergence analysis of such methods have stayed open: handling compression based approximation of both first and second moments (pre-conditioner) which appear as a ratio; avoiding dependence on the number of parameters, which is extremely large in modern deep models; and providing high-probability guarantees instead of in-expectation, which can hide high variance behavior. In this work, we introduce a family of Sketched Adaptive Distributed Learning (SADL) algorithms which can use suitable unbiased gradient sketching for compression with suitable adaptive optimization algorithms. As our main contribution, we provide theoretical convergence guarantees of SADL algorithms which addresses all of the existing challenges. In particular, our guarantees hold with high probability, picks up only a logarithmic dependence on the number of parameters, and the first and second moment approximation is handled precisely yielding a dependence on the intrinsic dimension of the loss Hessian, which is significantly smaller than the full dimensionality of deep learning models. Empirically, the SADL algorithms are shown to be competitive with and often outperform baselines on both vision and language tasks, in both supervised fine-tuning and training-from-scratch regimes. Further, the SADL algorithms are also competitive with the state-of-the-art communication-efficient distributed learning algorithms based on error feedback",
    "checked": false,
    "id": "fc92013f1def6d5b66ea0f4a96df2332dcd1d597",
    "semantic_title": "sketched adaptive federated deep learning: a sharp convergence analysis",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jQH0gdwsuT": {
    "title": "Greedy Sampling Is Provably Efficient For RLHF",
    "volume": "poster",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a key technique for post‑training large language models. Despite its empirical success, the theoretical understanding of RLHF is still limited, as learning the KL-regularized target with only preference feedback poses additional challenges compared with canonical RL. Existing works mostly study the reward-based Bradley-Terry (BT) preference model, and extend classical designs utilizing optimism or pessimism. This work, instead, considers the general preference model (whose practical relevance has been observed recently) and obtains performance guarantees with major, order-wise improvements over existing ones. Surprisingly, these results are derived from algorithms that directly use empirical estimates (i.e., greedy sampling), as opposed to constructing optimistic or pessimistic estimates in previous works. This insight has a deep root in the unique structural property of the optimal policy class under the KL-regularized target, and we further specialize it to the BT model, highlighting the surprising sufficiency of greedy sampling in RLHF",
    "checked": true,
    "id": "9b82f1f266d3ca53a3116e40a6fe4b45010e5035",
    "semantic_title": "greedy sampling is provably efficient for rlhf",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N9sJRKzVK7": {
    "title": "Learning Individual Behavior in Agent-Based Models with Graph Diffusion Networks",
    "volume": "poster",
    "abstract": "Agent-Based Models (ABMs) are powerful tools for studying emergent properties in complex systems. In ABMs, agent behaviors are governed by local interactions and stochastic rules. However, these rules are ad hoc and, in general, non-differentiable, limiting the use of gradient-based methods for optimization, and thus integration with real-world data. We propose a novel framework to learn a differentiable surrogate of any ABM by observing its generated data. Our method combines diffusion models to capture behavioral stochasticity and graph neural networks to model agent interactions. Distinct from prior surrogate approaches, our method introduces a fundamental shift: rather than approximating system-level outputs, it models individual agent behavior directly, preserving the decentralized, bottom-up dynamics that define ABMs. We validate our approach on two ABMs (Schelling's segregation model and a Predator-Prey ecosystem) showing that it replicates individual-level patterns and accurately forecasts emergent dynamics beyond training. Our results demonstrate the potential of combining diffusion models and graph learning for data-driven ABM simulation",
    "checked": true,
    "id": "29cbdb333a06b13046a8cdbca85b7c5d3a1e66a8",
    "semantic_title": "learning individual behavior in agent-based models with graph diffusion networks",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=h5YGwnMTke": {
    "title": "SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models",
    "volume": "poster",
    "abstract": "Large reasoning models (LRMs) excel at complex reasoning tasks but typically generate lengthy sequential chains-of-thought, resulting in long inference times before arriving at the final answer. To address this challenge, we introduce SPRINT, a novel post-training and inference-time framework designed to enable LRMs to dynamically identify and exploit opportunities for parallelization during their reasoning process. SPRINT incorporates an innovative data curation pipeline that reorganizes natural language reasoning trajectories into structured rounds of long-horizon planning and parallel execution. By fine-tuning LRMs on a small amount of such curated data, the models learn to dynamically identify independent subtasks within extended reasoning processes and effectively execute them in parallel. Through extensive evaluations, we demonstrate that models fine-tuned with the SPRINT framework match the performance of reasoning models on complex domains such as mathematics while generating up to 39% fewer sequential tokens on problems requiring more than 8,000 output tokens. Finally, we observe consistent results transferred to two out-of-distribution tasks, namely GPQA and Countdown, with up to 45% and 65% reduction in average sequential tokens respectively for longer reasoning trajectories, while matching the performance of the fine-tuned reasoning model",
    "checked": true,
    "id": "0965a1c7e2b64889ab7f9d69d42b465414378ab2",
    "semantic_title": "sprint: enabling interleaved planning and parallelized execution in reasoning models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=w1FUXt3ujK": {
    "title": "Prot2Text-V2: Protein Function Prediction with Multimodal Contrastive Alignment",
    "volume": "poster",
    "abstract": "Predicting protein function from sequence is a central challenge in computational biology. While existing methods rely heavily on structured ontologies or similarity-based techniques, they often lack the flexibility to express structure-free functional descriptions and novel biological functions. In this work, we introduce Prot2Text-V2, a novel multimodal sequence-to-text model that generates free-form natural language descriptions of protein function directly from amino acid sequences. Our method combines a protein language model as a sequence encoder (ESM-3B) and a decoder-only language model (LLaMA-3.1-8B-Instruct) through a lightweight nonlinear modality projector. A key innovation is our Hybrid Sequence-level Contrastive Alignment Learning (H-SCALE), which improves cross-modal learning by matching mean- and std-pooled protein embeddings with text representations via contrastive loss. After the alignment phase, we apply instruction-based fine-tuning using LoRA on the decoder to teach the model how to generate accurate protein function descriptions conditioned on the protein sequence. We train Prot2Text-V2 on about 250K curated entries from SwissProt and evaluate it under low-homology conditions, where test sequences have low similarity with training samples. Prot2Text-V2 consistently outperforms traditional and LLM-based baselines across various metrics",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zj45hoQhjD": {
    "title": "Scalable In-context Ranking with Generative Models",
    "volume": "poster",
    "abstract": "In-context Ranking (ICR) is an emerging paradigm for Information Retrieval (IR), which leverages contextual understanding of LLMs by directly incorporating the task description, candidate documents, and the query into the model's input prompt and tasking the LLM to identify relevant document(s). While it is effective, efficiency is a significant challenge in this paradigm, especially as the candidate list grows due to quadratic/super-linear scaling of attention operation with context length. To this end, this paper first identifies inherent and exploitable structures in the attention of LLMs finetuned for ICR: (1) inter-document block sparsity: attention is dense within each document block but sparse across different documents in the context; and (2) query-document block relevance: the attention scores from certain query tokens to a document block in middle layers strongly correlate with that document's actual relevance. Motivated by these observations, we introduce BlockRank (Blockwise In-context Ranking), a novel method that adapts the attention operation in an LLM by (a) architecturally enforcing the observed inter-document block sparsity, reducing attention complexity from quadratic to linear without loss in performance, and (b) optimizing query-document block relevance for true relevant documents during fine-tuning using an auxiliary contrastive training objective, improving retrieval in attention. Experiments on BEIR, MSMarco and NQ with Mistral-7B demonstrate that BlockRank Mistral matches or outperforms existing SOTA listwise rankers and controlled fine-tuned baseline while being significantly more efficient at inference (4.7x for 100 MSMarco documents in context) and scaling gracefully to long-context shortlists, around 500 documents in-context (approximately 100K context length) within a second, presenting a scalable and effective solution for ICR",
    "checked": true,
    "id": "c39054cde73dc9709efc721747a78f96842ba962",
    "semantic_title": "scalable in-context ranking with generative models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bythzT0b81": {
    "title": "One Token per Highly Selective Frame: Towards Extreme Compression for Long Video Understanding",
    "volume": "poster",
    "abstract": "Long video understanding is inherently challenging for vision-language models (VLMs) because of the extensive number of frames. With each video frame typically expanding into tens or hundreds of tokens, the limited context length of large language models (LLMs) forces the VLMs to perceive the frames sparsely and lose temporal information. To address this, we explore extreme video token compression towards *one token per frame* at the final LLM layer. Our key insight is that heuristic-based compression, widely adopted by previous methods, is prone to information loss, and this necessitates supervising LLM layers into *learnable* and *progressive* modules for *token-level compression* (LP-Comp). Such compression enables our VLM to digest 2x-4x more frames with improved performance. To further increase the token efficiency, we investigate \\emph{frame-level compression}, which selects the frames most relevant to the queries via the internal attention scores of the LLM layers, named *question-conditioned compression* (QC-Comp). As a notable distinction from previous studies, we mitigate the position bias of LLM attention in long contexts, *i.e.*, the over-concentration on the beginning and end of a sequence, by splitting long videos into short segments and employing local attention. Collectively, our combined *token-level* and *frame-level* leads to an e**x**treme compression model for long video understanding, named **XComp**, achieving a significantly larger compression ratio and enabling denser frame sampling. Our XComp is finetuned from VideoChat-Flash with a data-efficient *supervised compression tuning* stage that only requires 2.5\\% of the supervised fine-tuning data, yet boosts the accuracy from 42.9\\% to 46.2\\% on LVBench and enhances multiple other long video benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mr5Pyb3MLk": {
    "title": "Training a Scientific Reasoning Model for Chemistry",
    "volume": "poster",
    "abstract": "Reasoning models are large language models that use extra \"thought tokens\" before answering, providing both higher accuracy and explicit reasoning for their response. A major question has been whether language model reasoning generalizes beyond mathematics, programming, and logic, where most previous work has focused. We demonstrate that reasoning models can be post-trained in scientific domains without additional domain pretraining, and require substantially less data compared to contemporary domain-specific models. We report ether0, a 24B parameter LLM (based on Mistral-Small-24B) that can reason in natural language and respond with chemical structures. This reasoning model was trained with reinforcement learning on 577,790 experimentally-grounded chemistry tasks involving synthesized organic molecules. Our model outperforms all previous general-purpose chemistry models, frontier models, and humans, and is more data efficient relative to specialized models. We anticipate that this method can be applied to train highly data-efficient language models specialized for predictive and generative tasks across a wide variety of scientific domains",
    "checked": true,
    "id": "82eb3d408ba312148d4f76fa654c1d042092ea30",
    "semantic_title": "training a scientific reasoning model for chemistry",
    "citation_count": 10,
    "authors": []
  },
  "https://openreview.net/forum?id=wKG45sR1Jq": {
    "title": "Active Seriation: Efficient Ordering Recovery with Statistical Guarantees",
    "volume": "poster",
    "abstract": "We consider the problem of active seriation, where the goal is to recover an unknown ordering of $n$ items based on noisy observations of pairwise similarities. The similarities are assumed to correlate with the underlying ordering: pairs of items that are close in the ordering tend to have higher similarity scores, and vice versa. In the active setting, the learner sequentially selects which item pairs to query and receives noisy similarity measurements. We propose a novel active seriation algorithm that provably recovers the correct ordering with high probability. Furthermore, we provide optimal performance guarantees in terms of both the probability of error and the number of observations required for successful recovery",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HtwKJQzt7R": {
    "title": "Toward Artificial Palpation: Representation Learning of Touch on Soft Bodies",
    "volume": "poster",
    "abstract": "Palpation, the use of touch in medical examination, is almost exclusively performed by humans. We investigate a proof of concept for an artificial palpation method based on self-supervised learning. Our key idea is that an encoder-decoder framework can learn a **representation** from a sequence of tactile measurements that contains all the relevant information about the palpated object. We conjecture that such a representation can be used for downstream tasks such as tactile imaging and change detection. With enough training data, it should capture intricate patterns in the tactile measurements that go beyond a simple map of forces -- the current state of the art. To validate our approach, we both develop a simulation environment and collect a real-world dataset of soft objects and corresponding ground truth images obtained by magnetic resonance imaging (MRI). We collect palpation sequences using a robot equipped with a tactile sensor, and train a model that predicts sensory readings at different positions on the object. We investigate the representation learned in this process, and demonstrate its use in imaging and change detection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JMlYzOMDxI": {
    "title": "Contrastive Learning with Data Misalignment: Feature Purity, Training Dynamics and Theoretical Generalization Guarantees",
    "volume": "poster",
    "abstract": "Contrastive learning is a powerful framework for learning discriminative representations from image-text pairs. Despite its success, its theoretical foundations, especially when the image-text pair exhibits misalignment, remain underexplored. This paper provides the first theoretical analysis of contrastive learning under data misalignment, proving how the ground-truth modality-paired features are amplified while spurious features are suppressed through the training dynamics analysis. Specifically, we study two nonlinear encoders trained jointly with a contrastive loss and demonstrate that noisy (or misaligned) data pairs result in mixed representations and degrade the model's generalization ability. In contrast, recaptioning and filtering improve the data alignment, which in turn purifies the features learned by neurons and subsequently enhances generalization. Our analysis identifies feature purity as a key factor in the success of contrastive learning and offers insights into how data quality and training procedures impact representation learning and downstream generalization. Theoretical insights are supported by experiments on standard benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JV6ZOUb7BD": {
    "title": "Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage",
    "volume": "poster",
    "abstract": "We present the design and implementation of a new lifetime-aware tensor offloading framework for GPU memory expansion using low-cost PCIe-based solid-state drives (SSDs). Our framework, TERAIO, is developed explicitly for large language model (LLM) training with multiple GPUs and multiple SSDs. Its design is driven by our observation that the active tensors take only a small fraction (1.7% on average) of allocated GPU memory in each LLM training iteration, the inactive tensors are usually large and will not be used for a long period of time, creating ample opportunities for offloading/prefetching tensors to/from slow SSDs without stalling the GPU training process. TERAIO accurately estimates the lifetime (active period of time in GPU memory) of each tensor with the profiling of the first few iterations in the training process. With the tensor lifetime analysis, TERAIO will generate an optimized tensor offloading/prefetching plan and integrate it into the compiled LLM program via PyTorch. TERAIO has a runtime tensor migration engine to execute the offloading/prefetching plan via GPUDirect storage, which allows direct tensor migration between GPUs and SSDs for alleviating the CPU bottleneck and maximizing the SSD bandwidth utilization. In comparison with state-of-the-art studies such as ZeRO-Offload and ZeRO-Infinity, we show that TERAIO improves the training performance of various LLMs by 1.47× on average, and achieves 80.7% of the ideal performance assuming unlimited GPU memory",
    "checked": true,
    "id": "ccb4736592b45ea8ee044f9282f387e067bbce9d",
    "semantic_title": "cost-efficient llm training with lifetime-aware tensor offloading via gpudirect storage",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b4X6cz1F9l": {
    "title": "Cross-fluctuation phase transitions reveal sampling dynamics in diffusion models",
    "volume": "poster",
    "abstract": "We analyse how the sampling dynamics of distributions evolve in score-based diffusion models using \\emph{cross-fluctuations}, a centered-moment statistic from statistical physics. Specifically, we show that starting from an unbiased isotropic normal distribution, samples undergo sharp, discrete transitions, eventually forming distinct events of a desired distribution while progressively revealing finer structure. As this process is reversible, these transitions also occur in reverse, where intermediate states progressively merge, tracing a path back to the initial distribution. We demonstrate that these transitions can be detected as discontinuities in $n^{\\text{th}}$-order cross-fluctuations. For variance-preserving SDEs, we derive a closed-form for these cross-fluctuations that is efficiently computable for the reverse trajectory. %, thus tying cross-fluctuation dynamics to event formation within the desired distribution. We find that detecting these transitions directly boosts sampling efficiency, accelerates class-conditional and rare-class generation, and improves two zero-shot tasks--image classification and style transfer--without expensive grid search or retraining. We also show that this viewpoint unifies classical coupling and mixing from finite Markov chains with continuous dynamics while extending to stochastic SDEs and non Markovian samplers. Our framework therefore bridges discrete Markov chain theory, phase analysis, and modern generative modeling",
    "checked": true,
    "id": "588aa37bbf5319e43ae1dbc90ba13851e88f87ae",
    "semantic_title": "cross-fluctuation phase transitions reveal sampling dynamics in diffusion models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7YKbyBb0kF": {
    "title": "Smoothed Differentiation Efficiently Mitigates Shattered Gradients in Explanations",
    "volume": "poster",
    "abstract": "Explaining complex machine learning models is a fundamental challenge when developing safe and trustworthy deep learning applications. To date, a broad selection of explainable AI (XAI) algorithms exist. One popular choice is SmoothGrad, which has been conceived to alleviate the well-known shattered gradient problem by smoothing gradients through convolution. SmoothGrad proposes to solve this high-dimensional convolution integral by sampling -- typically approximating the convolution with limited precision. Higher numbers of samples would amount to higher precision in approximating the convolution but also to higher computing demand, therefore in practice only few samples are used in SmoothGrad. In this work we propose a well founded novel method _SmoothDiff_ to resolve this tradeoff yielding a _speedup of over two orders of magnitude_. Specifically, _SmoothDiff_ leverages automatic differentiation to decompose the expected values of Jacobians across a network architecture, directly targeting only the non-linearities responsible for shattered gradients and making it easy to implement. We demonstrate SmoothDiff's excellent speed and performance in a number of experiments and benchmarks. Thus, SmoothDiff greatly enhances the usability (quality and speed) of SmoothGrad -- a popular workhorse of XAI",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6QbbaEGkO7": {
    "title": "Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-based Decoding",
    "volume": "poster",
    "abstract": "Diffusion models excel at capturing the natural design spaces of images, molecules, DNA, RNA, and protein sequences. However, rather than merely generating designs that are natural, we often aim to optimize downstream reward functions while preserving the naturalness of these design spaces. Existing methods for achieving this goal often require differentiable proxy models (e.g., classifier guidance or DPS) or involve computationally expensive fine-tuning of diffusion models (e.g., classifier-free guidance, RL-based fine-tuning). In our work, we propose a new method to address these challenges. Our algorithm is an iterative sampling method that integrates soft value functions, which looks ahead to how intermediate noisy states lead to high rewards in the future, into the standard inference procedure of pre-trained diffusion models. Notably, our approach avoids fine-tuning generative models and eliminates the need to construct differentiable models. This enables us to (1) directly utilize non-differentiable features/reward feedback, commonly used in many scientific domains, and (2) apply our method to recent discrete diffusion models in a principled way. Finally, we demonstrate the effectiveness of our algorithm across several domains, including image generation, molecule generation, and DNA/RNA sequence generation",
    "checked": true,
    "id": "a673ef39237e227381fccf5b1d154d96de428f1d",
    "semantic_title": "derivative-free guidance in continuous and discrete diffusion models with soft value-based decoding",
    "citation_count": 73,
    "authors": []
  },
  "https://openreview.net/forum?id=emM7U3WKMO": {
    "title": "Diversity Is All You Need for Contrastive Learning: Spectral Bounds on Gradient Magnitudes",
    "volume": "poster",
    "abstract": "Contrastive learning thrives—or fails—based on how we construct \\emph{positive} and \\emph{negative} pairs. In the absence of explicit labels, models must infer semantic structure from these proxy signals. Early work on Siamese networks \\citep{chopra2005learning,hadsell2006dimensionality} already showed that pair construction directly shapes learned representations. In modern contrastive frameworks, poor pair selection remains a primary failure mode: it either causes collapse, where all embeddings converge to a point, or wastes the representational capacity of the space \\citep{chen2020simple,tian2020makes,khosla2020supervised}. Contemporary methods typically generate positives via semantic-preserving augmentations (crop, jitter, view transform), while negatives are drawn from other elements in the mini-batch under the assumption that different images are semantically dissimilar. But this assumption breaks down in fine-grained, low-diversity, or high-resolution settings \\citep{kalantidis2020hard,robinson2020contrastive,chuang2020debiased}, motivating techniques such as hard-negative mining and debiased losses \\citep{bachman2019learning,tian2020makes}. \\paragraph{Beyond pairs: batch-level diversity.} While most prior work focuses on \\emph{which} individual negatives to select, we study the geometry of the entire batch. Our central observation is this: the overall \\emph{diversity} of the batch embedding space strongly governs both training dynamics and representational quality. If diversity is too low, the model sees nearly identical negatives and gradients vanish—leading to collapse. If diversity is too high, negatives become almost orthogonal, but the resulting gradients shrink in magnitude, and learning slows. Optimal training thus occurs within a \\emph{moderate diversity window}: high enough to avoid collapse, low enough to preserve update strength",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tlmKcZFAtL": {
    "title": "Sequentially Auditing Differential Privacy",
    "volume": "poster",
    "abstract": "We propose a practical sequential test for auditing differential privacy guarantees of black-box mechanisms. The test processes streams of mechanisms' outputs providing anytime-valid inference while controlling Type I error, overcoming the fixed sample size limitation of previous batch auditing methods. Experiments show this test detects violations with sample sizes that are orders of magnitude smaller than existing methods, reducing this number from 50K to a few hundred examples, across diverse realistic mechanisms. Notably, it identifies DP-SGD privacy violations in \\textit{under} one training run, unlike prior methods needing full model training",
    "checked": true,
    "id": "7c2e871857560e1d57efeb00e845e5e9589e8ec4",
    "semantic_title": "sequentially auditing differential privacy",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aAW7ibNYOT": {
    "title": "Stability and Oracle Inequalities for Optimal Transport Maps between General Distributions",
    "volume": "poster",
    "abstract": "Optimal transport (OT) provides a powerful framework for comparing and transforming probability distributions, with wide applications in generative modeling, AI4Science and statistical inference. However, existing estimation theory typically requires stringent smoothness conditions on the underlying Brenier potentials and assumes bounded distribution supports, limiting practical applicability. In this paper, we introduce a unified theoretical framework for semi-dual OT map estimation that relaxes both of these restrictions. Building on sieved convex conjugate, our framework has two key contributions: (i) a new map stability bounds that holds without any second-order regularity assumptions on the true Brenier potentials, and (ii) an oracle inequality that cleanly decomposes the estimation error into statistical error, sieved bias, and approximation error. Specifically, our approximation error is measured in the $L^\\infty$ norm rather than Sobolev norm in the existing results, aligning more naturally with classical approximation theory. Leveraging these tools, we provide statistical error of semi-dual estimators with mild and verifiable conditions on the true OT map. Moreover, we establish the first theoretical guarantee for deep neural network OT map estimator between general distributions, with Tanh network function class as an example",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ar62cqTduE": {
    "title": "Linear Attention for Efficient Bidirectional Sequence Modeling",
    "volume": "poster",
    "abstract": "Linear Transformers and State Space Models have emerged as efficient alternatives to softmax Transformers for causal sequence modeling, enabling parallel training via matrix multiplication and efficient RNN-style inference. However, despite their success in causal tasks, no unified framework exists for applying Linear Transformers to bidirectional sequence modeling. We introduce LION, the first framework to systematically extend Linear Transformers to the bidirectional setting. LION generalizes three core representations commonly used in the causal case—full Linear Attention , bidirectional RNN, and chunkwise parallel form—to the bidirectional setting. These forms are theoretically equivalent and enable models to exploit the strengths of each during training and inference. We prove that a broad class of Linear Transformers can be extended using LION and validate our framework via three core examples based on the choice of decay type: LION-LIT, the bidirectional extension of [25]; LION-D, based on [44]; and LION-S, a variant using selective decay [34, 13]. Across standard bidirectional tasks, LION enables models to match or exceed the performance of softmax Transformers, while offering significantly faster training and more efficient inference than existing State Space Models",
    "checked": true,
    "id": "667077ddedb84ad9bc3cb676ea9e5e68275d970d",
    "semantic_title": "linear attention for efficient bidirectional sequence modeling",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=3JlBQRvod7": {
    "title": "ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning",
    "volume": "poster",
    "abstract": "Binary choices, as often used for reinforcement learning from human feedback (RLHF), convey only the *direction* of a preference. A person may choose apples over oranges and bananas over grapes, but *which preference is stronger*? Strength is crucial for decision-making under uncertainty and generalization of preference models, but hard to measure reliably. Metadata such as response times and inter-annotator agreement can serve as proxies for strength, but are often noisy and confounded. We propose ResponseRank to address the challenge of learning from noisy strength signals. Our method uses relative differences in these signals to *rank responses to pairwise comparisons by their inferred preference strength*. Signals are only considered locally within carefully constructed strata, controlling for systemic variation. This enables robust learning of utility differences consistent with strength-derived rankings, all while making minimal assumptions. Our contributions are threefold: (1) ResponseRank, a novel method that robustly learns preference strength by leveraging locally valid relative strength signals; (2) empirical evidence of improved sample efficiency and robustness across diverse tasks: synthetic preference learning (with simulated response times), language modeling (with annotator agreement), and RL control tasks (with simulated episode returns); and (3) the *Pearson Distance Correlation (PDC)*, a novel metric that isolates cardinal utility learning from ordinal accuracy",
    "checked": false,
    "id": "026efae6ab96b4765f0615cc597fef0fbc9f320a",
    "semantic_title": "test-time reasoning through visual human preferences with vlms and soft rewards",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9uqGAlhgBS": {
    "title": "Smoothed Agnostic Learning of Halfspaces over the Hypercube",
    "volume": "poster",
    "abstract": "Agnostic learning of Boolean halfspaces is a fundamental problem in computational learning theory, but it is known to be computationally hard even for weak learning. Recent work \\citep{chandrasekaran2024smoothed} proposed smoothed analysis as a way to bypass such hardness, but existing frameworks rely on additive Gaussian perturbations, making them unsuitable for discrete domains. We introduce a new smoothed agnostic learning framework for Boolean inputs, where perturbations are modeled via random bit flips. This defines a natural discrete analogue of smoothed optimality generalizing the Gaussian case. Under strictly subexponential assumptions on the input distribution, we give an efficient algorithm for learning halfspaces in this model, with runtime and sample complexity $\\tilde{O}(n^{\\mathrm{poly}(\\frac{1}{\\sigma\\epsilon})})$. Previously, such algorithms were known only with strong structural assumptions for the discrete hypercube—for example, independent coordinates or symmetric distributions. Our result provides the first computationally efficient guarantee for smoothed agnostic learning of halfspaces over the Boolean hypercube, bridging the gap between worst-case intractability and practical learnability in discrete settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sz2i4dwDem": {
    "title": "Efficient Quadratic Corrections for Frank-Wolfe Algorithms",
    "volume": "poster",
    "abstract": "We develop a Frank-Wolfe algorithm with corrective steps, generalizing previous algorithms including Blended Conditional Gradients, Blended Pairwise Conditional Gradients, and Fully-Corrective Frank-Wolfe. For this, we prove tight convergence guarantees together with an optimal face identification property. Furthermore, we propose two highly efficient corrective steps for convex quadratic objectives based on linear optimization or linear system solving, akin to Wolfe's Minimum-Norm Point algorithm, and prove finite-time convergence under suitable conditions. Beyond optimization problems that are directly quadratic, we revisit two algorithms, Split Conditional Gradient and Second-Order Conditional Gradient Sliding, which can leverage quadratic corrections to accelerate the solution of their quadratic subproblems. We show improved convergence rates for the first and prove broader applicability for the second. Finally, we demonstrate substantial computational speedups for Frank-Wolfe-based algorithms with quadratic corrections across the considered problem classes",
    "checked": true,
    "id": "7fc804430ef40c2e2e4684dfe2ae91220ce96fd9",
    "semantic_title": "efficient quadratic corrections for frank-wolfe algorithms",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=teB4aqJsNP": {
    "title": "LLM Unlearning via Neural Activation Redirection",
    "volume": "poster",
    "abstract": "The ability to selectively remove knowledge from LLMs is highly desirable. However, existing methods often struggle with balancing unlearning efficacy and retain model utility, and lack controllability at inference time to emulate base model behavior as if it had never seen the unlearned data. In this paper, we propose LUNAR, a novel unlearning method grounded in the Linear Representation Hypothesis and operates by redirecting the representations of unlearned data to activation regions that expresses its inability to answer. We show that contrastive features are not a prerequisite for effective activation redirection, and LUNAR achieves state-of-the-art unlearning performance and superior controllability. Specifically, LUNAR achieves between 2.9x and 11.7x improvement in the combined unlearning efficacy and model utility score (Deviation Score) across various base models and generates coherent, contextually appropriate responses post-unlearning. Moreover, LUNAR effectively reduces parameter updates to a single down-projection matrix, a novel design that significantly enhances efficiency by 20x and robustness. Finally, we demonstrate that LUNAR is robust to white-box adversarial attacks and versatile in real-world scenarios, including handling sequential unlearning requests",
    "checked": false,
    "id": "1d6f745dd1c22a1da8104dfd6f3fa48f84165fd2",
    "semantic_title": "lunar: llm unlearning via neural activation redirection",
    "citation_count": 13,
    "authors": []
  },
  "https://openreview.net/forum?id=e4IlBqhbTO": {
    "title": "C3PO: Optimized Large Language Model Cascades with Probabilistic Cost Constraints for Reasoning",
    "volume": "poster",
    "abstract": "Large language models (LLMs) have achieved impressive results on complex reasoning tasks, but their high inference cost remains a major barrier to real-world deployment. A promising solution is to use cascaded inference, where small, cheap models handle easy queries, and only the hardest examples are escalated to more powerful models. However, existing cascade methods typically rely on supervised training with labeled data, offer no theoretical generalization guarantees, and provide limited control over test-time computational cost. We introduce **C3PO** (*Cost Controlled Cascaded Prediction Optimization*), a self-supervised framework for optimizing LLM cascades under probabilistic cost constraints. By focusing on minimizing regret with respect to the most powerful model (MPM), C3PO avoids the need for labeled data by constructing a cascade using only unlabeled model outputs. It leverages conformal prediction to bound the probability that inference cost exceeds a user-specified budget. We provide theoretical guarantees on both cost control and generalization error, and show that our optimization procedure is effective even with small calibration sets. Empirically, C3PO achieves state-of-the-art performance across a diverse set of reasoning benchmarks including GSM8K, MATH-500, BigBench-Hard and AIME, outperforming strong LLM cascading baselines in both accuracy and cost-efficiency. Our results demonstrate that principled, label-free cascade optimization can enable scalable LLM deployment",
    "checked": true,
    "id": "c42e3975bd4ac0cf3c98bee718de14d79464d908",
    "semantic_title": "c3po: optimized large language model cascades with probabilistic cost constraints for reasoning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ayjmOqf79t": {
    "title": "A data and task-constrained mechanistic model of the mouse outer retina shows robustness to contrast variations",
    "volume": "poster",
    "abstract": "Visual processing starts in the outer retina where photoreceptors transform light into electrochemical signals. These signals are modulated by inhibition from horizontal cells and sent to the inner retina via excitatory bipolar cells. The outer retina is thought to play an important role in contrast invariant coding of visual information, but how the different cell types implement this computation together remains incompletely understood. To understand the role of each cell type, we developed a fully-differentiable biophysical model of a circular patch of mouse outer retina. The model includes 200 cone photoreceptors with a realistic phototransduction cascade and ribbon synapses as well as horizontal and bipolar cells, all with cell-type specific ion channels. Going beyond decades of work constraining biophysical models of neurons only by experimental data, we used a dual approach, constraining some parameters of the model with available measurements and others by a visual task: (1) We fit the parameters of the cone models to whole cell patch-clamp measurements of photocurrents and two-photon glutamate imaging measurements of synaptic release. (2) We then trained the spatiotemporal outer retina model with photoreceptors and the other cell types to perform a visual classification task with varying contrast and luminance levels. We found that our outer retina model could learn to solve the classification task despite contrast and luminance variance in the stimuli. Testing different cell type compositions and connectivity patterns, we found that feedback from horizontal cells did not further improve task performance beyond that of excitatory photoreceptors and bipolar cells. This is surprising given that horizontal cells are positioned to mediate communication across cones and that they add to the model's number of trainable parameters. Finally, we found that our model generalized better to out of distribution contrast levels than a linear classifier. Our work shows how the nonlinearities found in the outer retina can accomplish contrast invariant classification and teases apart the contributions of different cell types",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2sa13vyCn0": {
    "title": "pLSTM: parallelizable Linear Source Transition Mark networks",
    "volume": "poster",
    "abstract": "Modern recurrent architectures, such as xLSTM and Mamba, have recently challenged the Transformer in language modeling. However, their structure constrains their applicability to sequences only or requires processing multi-dimensional data structures, such as images or molecular graphs, in a pre-defined sequential order. In contrast, Multi-Dimensional RNNs (MDRNNs) are well suited for data with a higher level structure, like 2D grids, trees, and directed acyclic graphs (DAGs). In this work, we extend the notion of multi-dimensionality to linear RNNs. We introduce parallelizable Linear Source Transition Mark networks (pLSTMs) using Source, Transition, and Mark gates that act on the linegraph of a general DAG. This enables parallelization in analogy to parallel associative scans and the chunkwise-recurrent form of sequential linear RNNs, but for DAGs. For regular grids (1D and 2D), like images, this scheme can be efficiently implemented using einsum operations, concatenations, and padding in logarithmic time. pLSTMs tackle the vanishing/exploding activation/gradient problem for long distances in DAGs via two distinct modes: a directed propagation mode (P-mode) and a diffusive distribution mode (D-mode). To showcase the long-range capabilities of pLSTM, we introduce arrow-pointing extrapolation as a synthetic computer vision task that contains long-distance directional information. We demonstrate that pLSTMs generalize well to larger image sizes, whereas Transformers struggle to extrapolate. On established molecular graph and computer vision benchmarks, pLSTMs also show strong performance. The complete code is available at https://github.com/ml-jku/plstm_experiments",
    "checked": true,
    "id": "77fddf1ad19e98a609c84aa8a9f41c47727a0c07",
    "semantic_title": "plstm: parallelizable linear source transition mark networks",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=eD0lYGQKn0": {
    "title": "Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free Structural Topology Optimization",
    "volume": "poster",
    "abstract": "Structural topology optimization (TO) is central to engineering design but remains computationally intensive due to complex physics and hard constraints. Existing deep-learning methods are limited to fixed square grids, a few hand-coded boundary conditions, and post-hoc optimization, preventing general deployment. We introduce Optimize Any Topology (OAT), a foundation-model framework that directly predicts minimum-compliance layouts for arbitrary aspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines a resolution- and shape-agnostic autoencoder with an implicit neural-field decoder and a conditional latent-diffusion model trained on OpenTO, a new corpus of 2.2 million optimized structures covering 2 million unique boundary-condition configurations. On four public benchmarks and two challenging unseen tests, OAT lowers mean compliance up to 90% relative to the best prior models and delivers sub-1 second inference on a single GPU across resolutions from 64 × 64 to 256 x 256 and aspect ratios as high as 10:1. These results establish OAT as a general, fast, and resolution-free framework for physics-aware topology optimization and provide a large-scale dataset to spur further research in generative modeling for inverse design. Code & data can be found at https://github.com/ahnobari/OptimizeAnyTopology",
    "checked": true,
    "id": "54d32e314b957c52a40d4858b954ce03d046050d",
    "semantic_title": "optimize any topology: a foundation model for shape- and resolution-free structural topology optimization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3l7Invcjmn": {
    "title": "RespoDiff: Dual-Module Bottleneck Transformation for Responsible & Faithful T2I Generation",
    "volume": "poster",
    "abstract": "The rapid advancement of diffusion models has enabled high-fidelity and semantically rich text-to-image generation; however, ensuring fairness and safety remains an open challenge. Existing methods typically improve fairness and safety at the expense of semantic fidelity and image quality. In this work, we propose RespoDiff, a novel framework for responsible text-to-image generation that incorporates a dual-module transformation on the intermediate bottleneck representations of diffusion models. Our approach introduces two distinct learnable modules: one focused on capturing and enforcing responsible concepts, such as fairness and safety, and the other dedicated to maintaining semantic alignment with neutral prompts. To facilitate the dual learning process, we introduce a novel score-matching objective that enables effective coordination between the modules. Our method outperforms state-of-the-art methods in responsible generation by ensuring semantic alignment while optimizing both objectives without compromising image fidelity. Our approach improves responsible and semantically coherent generation by \\textasciitilde20\\% across diverse, unseen prompts. Moreover, it integrates seamlessly into large-scale models like SDXL, enhancing fairness and safety. The project page is available at https://vssilpa.github.io/respodiff_project_page",
    "checked": true,
    "id": "78429f802a4d87146bf42a38fcf2c034038092d0",
    "semantic_title": "respodiff: dual-module bottleneck transformation for responsible & faithful t2i generation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pe1ypX9gBO": {
    "title": "SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer",
    "volume": "poster",
    "abstract": "Deploying reinforcement learning (RL) safely in the real world is challenging, as policies trained in simulators must face the inevitable *sim-to-real gap*. Robust safe RL techniques are provably safe, however difficult to scale, while domain randomization is more practical yet prone to unsafe behaviors. We address this gap by proposing SPiDR, short for Sim-to-real via Pessimistic Domain Randomization—a scalable algorithm with provable guarantees for safe sim-to-real transfer. SPiDR uses domain randomization to incorporate the uncertainty about the sim-to-real gap into the safety constraints, making it versatile and highly compatible with existing training pipelines. Through extensive experiments on sim-to-sim benchmarks and two distinct real-world robotic platforms, we demonstrate that SPiDR effectively ensures safety despite the sim-to-real gap while maintaining strong performance",
    "checked": true,
    "id": "2ec7d7a5caa49c58443c0206b2c870b4dd9d5a6e",
    "semantic_title": "spidr: a simple approach for zero-shot safety in sim-to-real transfer",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IJryQAOy0p": {
    "title": "Remasking Discrete Diffusion Models with Inference-Time Scaling",
    "volume": "poster",
    "abstract": "Part of the success of diffusion models stems from their ability to perform iterative refinement, i.e., repeatedly correcting outputs during generation. However, modern masked discrete diffusion lacks this capability: when a token is generated, it cannot be updated again, even when it introduces an error. Here, we address this limitation by introducing the remasking diffusion model (ReMDM) sampler, a method that can be applied to pretrained masked diffusion models in a principled way and that is derived from a discrete diffusion model with a custom remasking backward process. Most interestingly, ReMDM endows discrete diffusion with a form of inference-time compute scaling. By increasing the number of sampling steps, ReMDM generates natural language outputs that approach the quality of autoregressive models, whereas when the computation budget is limited, ReMDM better maintains quality. ReMDM also improves sample quality of masked diffusion models for discretized images, and in scientific domains such as molecule design, ReMDM facilitates diffusion guidance and pushes the Pareto frontier of controllability relative to classical masking and uniform noise diffusion. When applied to large pretrained diffusion language models, ReMDM boosts the model's performance on downstream tasks requiring factual knowledge grasp and reasoning ability",
    "checked": true,
    "id": "19c3a5d9d32c57cd1482c8376f208cc2b2333334",
    "semantic_title": "remasking discrete diffusion models with inference-time scaling",
    "citation_count": 59,
    "authors": []
  },
  "https://openreview.net/forum?id=qBPb7g1SEa": {
    "title": "Let a Neural Network be Your Invariant",
    "volume": "poster",
    "abstract": "Safety verification ensures that a system avoids undesired behaviour. Liveness complements safety, ensuring that the system also achieves its desired objectives. A complete specification of functional correctness must combine both safety and liveness. Proving with mathematical certainty that a system satisfies a safety property demands presenting an appropriate inductive invariant of the system, whereas proving liveness requires showing a measure of progress witnessed by a ranking function. Neural model checking has recently introduced a data-driven approach to the formal verification of reactive systems, albeit focusing on ranking functions and thus addressing liveness properties only. In this paper, we extend and generalise neural model checking to additionally encompass inductive invariants and thus safety properties as well. Given a system and a linear temporal logic specification of safety and liveness, our approach alternates a learning and a checking component towards the construction of a provably sound neural certificate. Our new method introduces a neural certificate architecture that jointly represents inductive invariants as proofs of safety, and ranking functions as proofs of liveness. Moreover, our new architecture is amenable to training using constraint solvers, accelerating prior neural model checking work otherwise based on gradient descent. We experimentally demonstrate that our method is orders of magnitude faster than the state-of-the-art model checkers on pure liveness and combined safety and liveness verification tasks written in SystemVerilog, while enabling the verification of richer properties than was previously possible for neural model checking",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2xDdVkWrqq": {
    "title": "Understanding the Gain from Data Filtering in Multimodal Contrastive Learning",
    "volume": "poster",
    "abstract": "The success of modern multimodal representation learning relies on internet-scale datasets. Due to the low quality of a large fraction of raw web data, data curation has become a critical step in the training pipeline. Filtering using a trained model (i.e., teacher-based filtering) has emerged as a successful solution, leveraging a pre-trained model to compute quality scores. To explain the empirical success of teacher-based filtering, we characterize the performance of filtered contrastive learning under the standard bimodal data generation model. Denoting $\\eta\\in(0,1]$ as the fraction of data with correctly matched modalities among $n$ paired samples, we utilize a linear contrastive learning setup to show a provable benefit of data filtering: $(i)$ the error without filtering is upper and lower bounded by $\\frac{1}{\\eta \\sqrt{n}}$, and $(ii)$ the error with teacher-based filtering is upper bounded by $\\frac{1}{\\sqrt{\\eta n}}$ in the large $\\eta$ regime, and by $\\frac{1}{\\sqrt{n}}$ in the small $\\eta$ regime",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e85xf2d17F": {
    "title": "Online Optimization for Offline Safe Reinforcement Learning",
    "volume": "poster",
    "abstract": "We study the problem of Offline Safe Reinforcement Learning (OSRL), where the goal is to learn a reward-maximizing policy from fixed data under a cumulative cost constraint. We propose a novel OSRL approach that frames the problem as a minimax objective and solves it by combining offline RL with online optimization algorithms. We prove the approximate optimality of this approach when integrated with an approximate offline RL oracle and no-regret online optimization. We also present a practical approximation that can be combined with any offline RL algorithm, eliminating the need for offline policy evaluation. Empirical results on the DSRL benchmark demonstrate that our method reliably enforces safety constraints under stringent cost budgets, while achieving high rewards. The code is available at https://github.com/yassineCh/O3SRL",
    "checked": true,
    "id": "cfbd9c4b73aefbd5008218afc2ed39b4fcd451c1",
    "semantic_title": "online optimization for offline safe reinforcement learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EMa1ih7Wdt": {
    "title": "Global Minimizers of ℓ p -Regularized Objectives Yield the Sparsest ReLU Neural Networks",
    "volume": "poster",
    "abstract": "Overparameterized neural networks can interpolate a given dataset in many different ways, prompting the fundamental question: which among these solutions should we prefer, and what explicit regularization strategies will provably yield these solutions? This paper addresses the challenge of finding the sparsest interpolating ReLU network—i.e., the network with the fewest nonzero parameters or neurons—a goal with wide-ranging implications for efficiency, generalization, interpretability, theory, and model compression. Unlike post hoc pruning approaches, we propose a continuous, almost-everywhere differentiable training objective whose global minima are guaranteed to correspond to the sparsest single-hidden-layer ReLU networks that fit the data. This result marks a conceptual advance: it recasts the combinatorial problem of sparse interpolation as a smooth optimization task, potentially enabling the use of gradient-based training methods. Our objective is based on minimizing $\\ell^p$ quasinorms of the weights for $0 < p < 1$, a classical sparsity-promoting strategy in finite-dimensional settings. However, applying these ideas to neural networks presents new challenges: the function class is infinite-dimensional, and the weights are learned using a highly nonconvex objective. We prove that, under our formulation, global minimizers correspond exactly to sparsest solutions. Our work lays a foundation for understanding when and how continuous sparsity-inducing objectives can be leveraged to recover sparse networks through training",
    "checked": false,
    "id": "178e8e5eded4af746282a1a3e202a8e2c9287faf",
    "semantic_title": "global minimizers of ℓp-regularized objectives yield the sparsest relu neural networks",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=67xkPEM3bZ": {
    "title": "How to Train Your LLM Web Agent: A Statistical Diagnosis",
    "volume": "poster",
    "abstract": "Large language model (LLM) agents for web interfaces have advanced rapidly, yet open-source systems still lag behind proprietary agents. Bridging this gap is key to enabling customizable, efficient, and privacy-preserving agents. Two challenges hinder progress: the reproducibility issues in RL and LLM agent training, where results often depend on sensitive factors like seeds and decoding parameters, and the focus of prior work on single-step tasks, overlooking the complexities of web-based, multi-step decision-making. We address these gaps by providing a statistically driven study of training LLM agents for web tasks. Our two-stage pipeline combines imitation learning from a Llama 3.3 70B teacher with on-policy fine-tuning via Group Relative Policy Optimization (GRPO) on a Llama 3.1 8B student. Through 240 configuration sweeps and rigorous bootstrapping, we chart the first compute allocation curve for open-source LLM web agents. Our findings show that dedicating one-third of compute to teacher traces and the rest to RL improves MiniWoB++ success by 6 points and closes 60\\% of the gap to GPT-4o on WorkArena, while cutting GPU costs by 45\\%. We introduce a principled hyperparameter sensitivity analysis, offering actionable guidelines for robust and cost-effective agent training",
    "checked": true,
    "id": "4009db3a123f9771f8ef1f06cf4277ec3790afcd",
    "semantic_title": "how to train your llm web agent: a statistical diagnosis",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=T4qJuQCFAK": {
    "title": "Activation-Informed Merging of Large Language Models",
    "volume": "poster",
    "abstract": "Model merging, a method that combines the parameters and embeddings of multiple fine-tuned large language models (LLMs), offers a promising approach to enhance model performance across various tasks while maintaining computational efficiency. This paper introduces Activation-Informed Merging (AIM), a technique that integrates the information from the activation space of LLMs into the merging process to improve performance and robustness. AIM is designed as a flexible, complementary solution that is applicable to any existing merging method. It aims to preserve critical weights from the base model, drawing on principles from continual learning (CL) and model compression. Utilizing a task-agnostic calibration set, AIM selectively prioritizes essential weights during merging. We empirically demonstrate that AIM significantly enhances the performance of merged models across multiple benchmarks. Our findings suggest that considering the activation-space information can provide substantial advancements in the model merging strategies for LLMs with up to 40% increase in benchmark performance. Our code is publicly available at https://github.com/ahnobari/ActivationInformedMerging",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ANl9HuwIMs": {
    "title": "Asymmetric Dual Self-Distillation for 3D Self-Supervised Representation Learning",
    "volume": "poster",
    "abstract": "Learning semantically meaningful representations from unstructured 3D point clouds remains a central challenge in computer vision, especially in the absence of large-scale labeled datasets. While masked point modeling (MPM) is widely used in self-supervised 3D learning, its reconstruction-based objective can limit its ability to capture high-level semantics. We propose AsymDSD, an Asymmetric Dual Self-Distillation framework that unifies masked modeling and invariance learning through prediction in the latent space rather than the input space. AsymDSD builds on a joint embedding architecture and introduces several key design choices: an efficient asymmetric setup, disabling attention between masked queries to prevent shape leakage, multi-mask sampling, and a point cloud adaptation of multi-crop. AsymDSD achieves state-of-the-art results on ScanObjectNN (90.53\\%) and further improves to 93.72\\% when pretrained on 930k shapes, surpassing prior methods",
    "checked": true,
    "id": "b76e0a24790fe43f428d4e5c9c0590775bf31ea8",
    "semantic_title": "asymmetric dual self-distillation for 3d self-supervised representation learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JInukYoZPI": {
    "title": "Scalable Policy-Based RL Algorithms for POMDPs",
    "volume": "poster",
    "abstract": "The continuous nature of belief states in POMDPs presents significant computational challenges in learning the optimal policy. In this paper, we consider an approach that solves a Partially Observable Reinforcement Learning (PORL) problem by approximating the corresponding POMDP model into a finite-state Markov Decision Process (MDP) (called Superstate MDP). We first derive theoretical guarantees that improve upon prior work that relate the optimal value function of the transformed Superstate MDP to the optimal value function of the original POMDP. Next, we propose a policy-based learning approach with linear function approximation to learn the optimal policy for the Superstate MDP. Consequently, our approach shows that a POMDP can be approximately solved using TD-learning followed by Policy Optimization by treating it as an MDP, where the MDP state corresponds to a finite history. We show that the approximation error decreases exponentially with the length of this history. To the best of our knowledge, our finite-time bounds are the first to explicitly quantify the error introduced when applying standard TD learning to a setting where the true dynamics are not Markovian",
    "checked": true,
    "id": "2969ef115910e3232bf22da217a2ae67708de5a6",
    "semantic_title": "scalable policy-based rl algorithms for pomdps",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DIjRvEKOeG": {
    "title": "SUMO: Subspace-Aware Moment-Orthogonalization for Accelerating Memory-Efficient LLM Training",
    "volume": "poster",
    "abstract": "Low-rank gradient-based optimization methods have significantly improved memory efficiency during the training of large language models (LLMs), enabling operations within constrained hardware without sacrificing performance. However, these methods primarily emphasize memory savings, often overlooking potential acceleration in convergence due to their reliance on standard isotropic steepest descent techniques, which can perform suboptimally in the highly anisotropic landscapes typical of deep networks, particularly LLMs. In this paper, we propose SUMO (Subspace-Aware Moment-Orthogonalization), an optimizer that employs exact singular value decomposition (SVD) for moment orthogonalization within a dynamically adapted low-dimensional subspace, enabling norm-inducing steepest descent optimization steps. By explicitly aligning optimization steps with the spectral characteristics of the loss landscape, SUMO effectively mitigates approximation errors associated with commonly used methods like Newton-Schulz orthogonalization approximation. We theoretically establish an upper bound on these approximation errors, proving their dependence on the condition numbers of moments, conditions we analytically demonstrate are encountered during LLM training. Furthermore, we both theoretically and empirically illustrate that exact orthogonalization via SVD substantially improves convergence rates while reducing overall complexity. Empirical evaluations confirm that SUMO accelerates convergence, enhances stability, improves performance, and reduces memory requirements by up to 20\\% compared to state-of-the-art methods",
    "checked": true,
    "id": "252088cd1e48bd81772e634ea6c5bef209ae451f",
    "semantic_title": "sumo: subspace-aware moment-orthogonalization for accelerating memory-efficient llm training",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=Q5bJZArYUZ": {
    "title": "Learning from Interval Targets",
    "volume": "poster",
    "abstract": "We study the problem of regression with interval targets, where only upper and lower bounds on target values are available in the form of intervals. This problem arises when the exact target label is expensive or impossible to obtain, due to inherent uncertainties. In the absence of exact targets, traditional regression loss functions cannot be used. First, we study the methodology of using a loss functions compatible with interval targets, for which we establish non-asymptotic generalization bounds based on smoothness of the hypothesis class that significantly relaxing prior assumptions of realizability and small ambiguity degree. Second, we propose a novel min-max learning formulation: minimize against the worst-case (maximized) target labels within the provided intervals. The maximization problem in the latter is non-convex, but we show that good performance can be achieved with the incorporation of smoothness constraints. Finally, we perform extensive experiments on real-world datasets and show that our methods achieve state-of-the-art performance",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hdT7UC7oG6": {
    "title": "Omnipresent Yet Overlooked: Heat Kernels in Combinatorial Bayesian Optimization",
    "volume": "poster",
    "abstract": "Bayesian Optimization (BO) has the potential to solve various combinatorial tasks, ranging from materials science to neural architecture search. However, BO requires specialized kernels to effectively model combinatorial domains. Recent efforts have introduced several combinatorial kernels, but the relationships among them are not well understood. To bridge this gap, we develop a unifying framework based on heat kernels, which we derive in a systematic way and express as simple closed-form expressions. Using this framework, we prove that many successful combinatorial kernels are either related or equivalent to heat kernels, and validate this theoretical claim in our experiments. Moreover, our analysis confirms and extends the results presented in Bounce: certain algorithms' performance decreases substantially when the unknown optima of the function do not have a certain structure. In contrast, heat kernels are not sensitive to the location of the optima. Lastly, we show that a fast and simple pipeline, relying on heat kernels, is able to achieve state-of-the-art results, matching or even outperforming certain slow or complex algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XqHrG8lBai": {
    "title": "Information-Computation Tradeoffs for Noiseless Linear Regression with Oblivious Contamination",
    "volume": "poster",
    "abstract": "We study the task of noiseless linear regression under Gaussian covariates in the presence of additive oblivious contamination. Specifically, we are given i.i.d.\\ samples from a distribution $(x, y)$ on $\\mathbb R^d \\times \\mathbb R$ with $x \\sim \\mathcal N(0,I_d)$ and $y = x^\\top \\beta + z$, where $z$ is drawn from an unknown distribution that is independent of $x$. Moreover, $z$ satisfies $\\mathbb P[z = 0] = \\alpha>0$. The goal is to accurately recover the regressor $\\beta$ to small $\\ell_2$-error. Ignoring computational considerations, this problem is known to be solvable using $O(d/\\alpha)$ samples. On the other hand, the best known polynomial-time algorithms require $\\Omega(d/\\alpha^2)$ samples. Here we provide formal evidence that the quadratic dependence in $1/\\alpha$ is inherent for efficient algorithms. Specifically, we show that any efficient Statistical Query algorithm for this task requires VSTAT complexity at least $\\tilde{\\Omega}(d^{1/2}/\\alpha^2)$",
    "checked": true,
    "id": "7f7861674cc9ce4b8fa0e5442a4a5f171e890b99",
    "semantic_title": "information-computation tradeoffs for noiseless linear regression with oblivious contamination",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zt3RKc6VBp": {
    "title": "Know Thyself by Knowing Others: Learning Neuron Identity from Population Context",
    "volume": "poster",
    "abstract": "Identifying the functional identity of individual neurons is essential for interpreting circuit dynamics, yet it remains a major challenge in large-scale _in vivo_ recordings where anatomical and molecular labels are often unavailable. Here we introduce NuCLR, a self-supervised framework that learns context-aware representations of neuron identity by modeling each neuron's role within the broader population. NuCLR employs a spatio-temporal transformer that captures both within-neuron dynamics and across-neuron interactions. It is trained with a sample-wise contrastive objective that encourages temporally-stable and discriminative embeddings. Across multiple open-access datasets, NuCLR outperforms prior methods in both cell type and brain region classification. Critically, it exhibits strong zero-shot generalization to entirely new populations, without any retraining or access to stimulus labels. Furthermore, we demonstrate that our framework scales effectively with data size. Overall, our results demonstrate that modeling population context is crucial for understanding neuron identity and that rich signal for cell-typing and neuron localization is present in neural activity alone. Code available at: https://github.com/nerdslab/nuclr",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uJinTwXfbs": {
    "title": "A Hierarchy of Graphical Models for Counterfactual Inferences",
    "volume": "poster",
    "abstract": "Graphical models have been widely used as parsimonious encoders of assumptions of the underlying causal system and provide a basis for causal inferences. Models encoding stronger constraints tend to require higher expressive power, which are also harder, and sometimes impossible to empirically falsify. In this paper, we introduce two new collections of distributions that include counterfactual quan- tities which are experimentally accessible under counterfactual randomizations. Correspondingly, we define two new classes of graphical models for encoding empirically testable constraints in these distributions. We further present a sound and complete calculus, based on counterfactual calculus, which licenses inferences in these two new models with rules that are within the empirically falsifiable bound- ary. Finally, we formulate a hierarchy over several graphical models based on the constraints they encode and study the fundamental trade-off between the expressive power and empirical falsifiability of different models across the hierarchy",
    "checked": false,
    "id": "e36af1d3094ec8ed85c12f8b1da74ccb4c501e71",
    "semantic_title": "euclid: field-level inference of primordial non-gaussianity and cosmic initial conditions",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=heoHY4vcRl": {
    "title": "SpEx: A Spectral Approach to Explainable Clustering",
    "volume": "poster",
    "abstract": "Explainable clustering by axis-aligned decision trees was introduced by Moshkovitz et al. (2020) and has gained considerable interest. Prior work has focused on minimizing the price of explainability for specific clustering objectives, lacking a general method to fit an explanation tree to any given clustering, without restrictions. In this work, we propose a new and generic approach to explainable clustering, based on spectral graph partitioning. With it, we design an explainable clustering algorithm that can fit an explanation tree to any given non-explainable clustering, or directly to the dataset itself. Moreover, we show that prior algorithms can also be interpreted as graph partitioning, through a generalized framework due to Trevisan (2013) wherein cuts are optimized in two graphs simultaneously. Our experiments show the favorable performance of our method compared to baselines on a range of datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SiBVbL7rsX": {
    "title": "Localizing Knowledge in Diffusion Transformers",
    "volume": "poster",
    "abstract": "Understanding how knowledge is distributed across the layers of generative models is crucial for improving interpretability, controllability, and adaptation. While prior work has explored knowledge localization in UNet-based architectures, Diffusion Transformer (DiT)-based models remain underexplored in this context. In this paper, we propose a model- and knowledge-agnostic method to localize where specific types of knowledge are encoded within the DiT blocks. We evaluate our method on state-of-the-art DiT-based models, including PixArt-$\\alpha$, FLUX, and SANA, across six diverse knowledge categories. We show that the identified blocks are both interpretable and causally linked to the expression of knowledge in generated outputs. Building on these insights, we apply our localization framework to two key applications: *model personalization* and *knowledge unlearning*. In both settings, our localized fine-tuning approach enables efficient and targeted updates, reducing computational cost, improving task-specific performance, and better preserving general model behavior with minimal interference to unrelated or surrounding content. Overall, our findings offer new insights into the internal structure of DiTs and introduce a practical pathway for more interpretable, efficient, and controllable model editing",
    "checked": true,
    "id": "6ad7518a35ce3abb3a229597437b4466d3e98e32",
    "semantic_title": "localizing knowledge in diffusion transformers",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ifGwVxiigF": {
    "title": "Fast Last-Iterate Convergence of SGD in the Smooth Interpolation Regime",
    "volume": "poster",
    "abstract": "We study population convergence guarantees of stochastic gradient descent (SGD) for smooth convex objectives in the interpolation regime, where the noise at optimum is zero or near zero. The behavior of the last iterate of SGD in this setting---particularly with large (constant) stepsizes---has received growing attention in recent years due to implications for the training of over-parameterized models, as well as to analyzing forgetting in continual learning and to understanding the convergence of the randomized Kaczmarz method for solving linear systems. We establish that after $T$ steps of SGD on $\\beta$-smooth convex loss functions with stepsize $0 < \\eta < 2/\\beta$, the last iterate exhibits expected excess risk $\\widetilde{O}(\\tfrac{1}{\\eta (2-\\beta \\eta) T^{1-\\beta\\eta/2}} + \\tfrac{\\eta}{(2-\\beta\\eta)^2} T^{\\beta\\eta/2} \\sigma_\\star^2)$, where $\\sigma_\\star^2$ denotes the variance of the stochastic gradients at the optimum. In particular, for a well-tuned stepsize we obtain a near optimal $\\widetilde{O}(1/T + \\sigma_\\star/\\sqrt T)$ rate for the last iterate, extending the results of Varre et al. (2021) beyond least squares regression; and when $\\sigma_\\star=0$ we obtain a rate of $O(1/\\sqrt T)$ with $\\eta=1/\\beta$, improving upon the best-known $O(T^{-1/4})$ rate recently established by Evron et al. (2025) in the special case of realizable linear regression",
    "checked": true,
    "id": "5ada7780f2351335e5534ec3adfed243cabf19cf",
    "semantic_title": "fast last-iterate convergence of sgd in the smooth interpolation regime",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=sWIbJWiEuA": {
    "title": "Adaptive Distraction: Probing LLM Contextual Robustness with Automated Tree Search",
    "volume": "poster",
    "abstract": "Large Language Models (LLMs) often struggle to maintain their original performance when faced with semantically coherent but task-irrelevant contextual information. Although prior studies have explored this issue using fixed-template or retrieval-based distractions, such static methods show limited effectiveness against contemporary models. To address this problem, we propose a dynamic distraction generation framework based on tree search, where the generation process is guided by model behavior. Without modifying the original question or answer, the method efficiently produces challenging adaptive distractions across multiple datasets, enabling systematic stress testing of LLMs' contextual robustness. Experiments on four benchmarks demonstrate that the generated distractions lead to an average performance drop of over 45\\% for mainstream models. Further comparisons of mitigation strategies show that prompt-based optimization methods yield limited gains, whereas post-training approaches (e.g., DPO) significantly enhance the model's contextual robustness. The results indicate that these issues do not stem from knowledge deficits in LLMs, but from a fundamental inability to maintain consistent reasoning under contextual distraction, posing a major challenge to the reliability of LLMs in real-world applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oq7Rgvfqvj": {
    "title": "Co-Regularization Enhances Knowledge Transfer in High Dimensions",
    "volume": "poster",
    "abstract": "Most existing transfer learning algorithms for high-dimensional models employ a two-step regularization framework, whose success heavily hinges on the assumption that the pre-trained model closely resembles the target. To relax this assumption, we propose a co-regularization process to directly exploit beneficial knowledge from the source domain for high-dimensional generalized linear models. The proposed method learns the target parameter by constraining the source parameters to be close to the target one, thereby preventing fine-tuning failures caused by significantly deviated pre-trained parameters. Our theoretical analysis demonstrates that the proposed method accommodates a broader range of sources than existing two-step frameworks, thus being more robust to less similar sources. Its effectiveness is validated through extensive empirical studies",
    "checked": false,
    "id": "e4a788326c49f26dc4816ab061959b653a9ddd68",
    "semantic_title": "limited-resource adapters are regularizers, not linguists",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sYVKHgZz2h": {
    "title": "Non-convex entropic mean-field optimization via Best Response flow",
    "volume": "poster",
    "abstract": "We study the problem of minimizing non-convex functionals on the space of probability measures, regularized by the relative entropy (KL divergence) with respect to a fixed reference measure, as well as the corresponding problem of solving entropy-regularized non-convex-non-concave min-max problems. We utilize the Best Response flow (also known in the literature as the fictitious play flow) and study how its convergence is influenced by the relation between the degree of non-convexity of the functional under consideration, the regularization parameter and the tail behaviour of the reference measure. In particular, we demonstrate how to choose the regularizer, given the non-convex functional, so that the Best Response operator becomes a contraction with respect to the $L^1$-Wasserstein distance, which ensures the existence of its unique fixed point that is then shown to be the unique global minimizer for our optimization problem. This extends recent results where the Best Response flow was applied to solve convex optimization problems regularized by the relative entropy with respect to arbitrary reference measures, and with arbitrary values of the regularization parameter. Our results explain precisely how the assumption of convexity can be relaxed, at the expense of making a specific choice of the regularizer. Additionally, we demonstrate how these results can be applied in reinforcement learning in the context of policy optimization for Markov Decision Processes and Markov games with softmax parametrized policies in the mean-field regime",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TjWdyVWBAG": {
    "title": "ExGra-Med: Extended Context Graph Alignment for Medical Vision-Language Models",
    "volume": "poster",
    "abstract": "State-of-the-art medical multi-modal LLMs (med-MLLMs), such as LLaVA-Med and BioMedGPT, primarily depend on scaling model size and data volume, with training driven largely by autoregressive objectives. However, we reveal that this approach can lead to weak vision-language alignment, making these models overly dependent on costly instruction-following data. To address this, we introduce ExGra-Med, a novel multi-graph alignment framework that jointly aligns images, instruction responses, and extended captions in the latent space, advancing semantic grounding and cross-modal coherence. To scale to large LLMs (e.g., LLaMa-7B), we develop an efficient end-to-end training scheme using black-box gradient estimation, enabling fast and scalable optimization. Empirically, ExGra-Med matches LLaVA-Med's performance using just 10\\% of pre-training data, achieving a 20.13\\% gain on VQA-RAD and approaching full-data performance. It also outperforms strong baselines like BioMedGPT and RadFM on visual chatbot and zero-shot classification tasks, demonstrating its promise for efficient, high-quality vision-language integration in medical AI",
    "checked": true,
    "id": "20085d97121b8b958d42030b1bdb9da143d42f55",
    "semantic_title": "exgra-med: extended context graph alignment for medical vision-language models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=0Y7AxxNCYh": {
    "title": "Explaining and Mitigating Crosslingual Tokenizer Inequities",
    "volume": "poster",
    "abstract": "The number of tokens it takes to encode parallel text in different languages is known to vary. These disparities are called *token premiums*. Having high token premiums leads to less throughput during training and increases costs at inference. In this paper, we show that even after controlling for dataset size, vocabulary size, and data content, monolingual tokenizers exhibit a wide range of token premiums across languages. To understand the cross-linguistic differences that cause these token premiums, we train a suite of approximately 7,000 comparable monolingual tokenizers for 97 languages, manipulating tokenization algorithm vocabulary size, and dataset size. We measure token premiums and test for a relationship between factors such as data similarity (between tokenizer training and evaluation), vocabulary size, and pre-tokenization. We also investigate the role of language-specific features such as writing system and word length. We find that similarity between training and test data does not impact token premiums, but vocabulary size and pre-tokenization do. While simply increasing vocabulary size does not lead to reduced token premium effects, we can determine an \"optimal\" vocabulary size for each language to achieve significantly reduced token premium effects. We also train superword tokenizers which allow merges over whitespaces, and we find that they both reduce token premium effects and improve compression overall. Thus, intervening on the vocabulary size or the pre-tokenizer significantly reduces crosslingual token premium effects",
    "checked": true,
    "id": "67a3b9bbbcfc3e95b7329246373e18c2762242f1",
    "semantic_title": "explaining and mitigating crosslingual tokenizer inequities",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qYSgnmT3dp": {
    "title": "Non-Markovian Discrete Diffusion with Causal Language Models",
    "volume": "poster",
    "abstract": "Discrete diffusion models offer a flexible, controllable approach to structured sequence generation, yet they still lag behind causal language models in expressive power. A key limitation lies in their reliance on the Markovian assumption, which restricts each step to condition only on the current state, leading to potential uncorrectable error accumulation. In this paper, We introduce CaDDi, a discrete diffusion model that conditions on the entire generative trajectory, thereby lifting the Markov constraint and allowing the model to revisit and improve past states. By unifying sequential (causal) and temporal (diffusion) reasoning in a single non‑Markovian transformer, CaDDi also treats standard causal language models as a special case and permits the direct reuse of pretrained LLM weights with no architectural changes. Empirically, CaDDi outperforms state‑of‑the‑art discrete diffusion baselines on natural‑language benchmarks, substantially narrowing the remaining gap to large autoregressive transformers",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hvykiwFiF8": {
    "title": "Gaussian-Augmented Physics Simulation and System Identification with Complex Colliders",
    "volume": "poster",
    "abstract": "System identification involving the geometry, appearance, and physical properties from video observations is a challenging task with applications in robotics and graphics. Recent approaches have relied on fully differentiable Material Point Method (MPM) and rendering for simultaneous optimization of these properties. However, they are limited to simplified object-environment interactions with planar colliders and fail in more challenging scenarios where objects collide with non-planar surfaces. We propose AS-DiffMPM, a differentiable MPM framework that enables physical property estimation with arbitrarily shaped colliders. Our approach extends existing methods by incorporating a differentiable collision handling mechanism, allowing the target object to interact with complex rigid bodies while maintaining end-to-end optimization. We show AS-DiffMPM can be easily interfaced with various novel view synthesis methods as a framework for system identification from visual observations",
    "checked": true,
    "id": "458287e74f220219f25dabf03b5c25e5491f0cdc",
    "semantic_title": "gaussian-augmented physics simulation and system identification with complex colliders",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=idqBQnot0t": {
    "title": "When Lower-Order Terms Dominate: Adaptive Expert Algorithms for Heavy-Tailed Losses",
    "volume": "poster",
    "abstract": "We consider the problem setting of prediction with expert advice with possibly heavy-tailed losses, i.e.\\ the only assumption on the losses is an upper bound on their second moments, denoted by $\\theta$. We develop adaptive algorithms that do not require any prior knowledge about the range or the second moment of the losses. Existing adaptive algorithms have what is typically considered a lower-order term in their regret guarantees. We show that this lower-order term, which is often the maximum of the losses, can actually dominate the regret bound in our setting. Specifically, we show that even with small constant $\\theta$, this lower-order term can scale as $\\sqrt{KT}$, where $K$ is the number of experts and $T$ is the time horizon. We propose adaptive algorithms with improved regret bounds that avoid the dependence on such a lower-order term and guarantee $\\mathcal{O}(\\sqrt{\\theta T\\log(K)})$ regret in the worst case, and $\\mathcal{O}(\\theta \\log(KT)/\\Delta_{\\min})$ regret when the losses are sampled i.i.d.\\ from some fixed distribution, where $\\Delta_{\\min}$ is the difference between the mean losses of the second best expert and the best expert. Additionally, when the loss function is the squared loss, our algorithm also guarantees improved regret bounds over prior results",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IoSLbwZkal": {
    "title": "On the Stability of Graph Convolutional Neural Networks: A Probabilistic Perspective",
    "volume": "poster",
    "abstract": "Graph convolutional neural networks (GCNNs) have emerged as powerful tools for analyzing graph-structured data, achieving remarkable success across diverse applications. However, the theoretical understanding of the stability of these models, i.e., their sensitivity to small changes in the graph structure, remains in rather limited settings, hampering the development and deployment of robust and trustworthy models in practice. To fill this gap, we study how small perturbations in the graph topology affect GCNN outputs and propose a novel formulation for analyzing model stability. Unlike prior studies that focus only on worst-case perturbations, our distribution-aware formulation characterizes output perturbations across a broad range of input data. This way, our framework enables, for the first time, a probabilistic perspective on the interplay between the statistical properties of the node data and perturbations in the graph topology. We conduct extensive experiments to validate our theoretical findings and demonstrate their benefits over existing baselines, in terms of both representation stability and adversarial attacks on downstream tasks. Our results demonstrate the practical significance of the proposed formulation and highlight the importance of incorporating data distribution into stability analysis",
    "checked": true,
    "id": "8216ab5632062a4cfbc257d0bdecebc5301b974a",
    "semantic_title": "on the stability of graph convolutional neural networks: a probabilistic perspective",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f0660KxvT2": {
    "title": "TRACE: Grounding Time Series in Context for Multimodal Embedding and Retrieval",
    "volume": "poster",
    "abstract": "The ubiquity of dynamic data in domains such as weather, healthcare, and energy underscores a growing need for effective interpretation and retrieval of time-series data. These data are inherently tied to domain-specific contexts, such as clinical notes or weather narratives, making cross-modal retrieval essential not only for downstream tasks but also for developing robust time-series foundation models by retrieval-augmented generation (RAG). Despite the increasing demand, time-series retrieval remains largely underexplored. Existing methods often lack semantic grounding, struggle to align heterogeneous modalities, and have limited capacity for handling multi-channel signals. To address this gap, we propose TRACE, a generic multimodal retriever that grounds time-series embeddings in aligned textual context. TRACE enables fine-grained channel-level alignment and employs hard negative mining to facilitate semantically meaningful retrieval. It supports flexible cross-modal retrieval modes, including Text-to-Timeseries and Timeseries-to-Text, effectively linking linguistic descriptions with complex temporal patterns. By retrieving semantically relevant pairs, TRACE enriches downstream models with informative context, leading to improved predictive accuracy and interpretability. Beyond a static retrieval engine, TRACE also serves as a powerful standalone encoder, with lightweight task-specific tuning that refines context-aware representations while maintaining strong cross-modal alignment. These representations achieve state-of-the-art performance on downstream forecasting and classification tasks. Extensive experiments across multiple domains highlight its dual utility, as both an effective encoder for downstream applications and a general-purpose retriever to enhance time-series models",
    "checked": true,
    "id": "dd803543cf260c90d127bc9f0f1bdf1f140e970a",
    "semantic_title": "trace: grounding time series in context for multimodal embedding and retrieval",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=hb2DeWIkmE": {
    "title": "SHAP zero Explains Biological Sequence Models with Near-zero Marginal Cost for Future Queries",
    "volume": "poster",
    "abstract": "The growing adoption of machine learning models for biological sequences has intensified the need for interpretable predictions, with Shapley values emerging as a theoretically grounded standard for model explanation. While effective for local explanations of individual input sequences, scaling Shapley-based interpretability to extract global biological insights requires evaluating thousands of sequences—incurring exponential computational cost per query. We introduce SHAP zero, a novel algorithm that amortizes the cost of Shapley value computation across large-scale biological datasets. After a one-time model sketching step, SHAP zero enables near-zero marginal cost for future queries by uncovering an underexplored connection between Shapley values, high-order feature interactions, and the sparse Fourier transform of the model. Applied to models of guide RNA efficacy, DNA repair outcomes, and protein fitness, SHAP zero explains predictions orders of magnitude faster than existing methods, recovering rich combinatorial interactions previously inaccessible at scale. This work opens the door to principled, efficient, and scalable interpretability for black-box sequence models in biology",
    "checked": true,
    "id": "94dcd70cf52bad92978709cf2665ce807223cb5f",
    "semantic_title": "shap zero explains biological sequence models with near-zero marginal cost for future queries",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=ay7WDSq0Kb": {
    "title": "Non-equilibrium Annealed Adjoint Sampler",
    "volume": "poster",
    "abstract": "Recently, there has been significant progress in learning-based diffusion samplers, which aim to sample from a given unnormalized density. Many of these approaches formulate the sampling task as a stochastic optimal control (SOC) problem using a canonical uninformative reference process, which limits their ability to efficiently guide trajectories toward the target distribution. In this work, we propose the **Non-Equilibrium Annealed Adjoint Sampler (NAAS)**, a novel SOC-based diffusion framework that employs annealed reference dynamics as a non-stationary base SDE. This annealing structure provides a natural progression toward the target distribution and generates informative reference trajectories, thereby enhancing the stability and efficiency of learning the control. Owing to our SOC formulation, our framework can incorporate a variety of SOC solvers, thereby offering high flexibility in algorithmic design. As one instantiation, we employ a lean adjoint system inspired by adjoint matching, enabling efficient and scalable training. We demonstrate the effectiveness of NAAS across a range of tasks, including sampling from classical energy landscapes and molecular Boltzmann distributions",
    "checked": true,
    "id": "3468627d3329ff9d82a45b34d346948f6c327c01",
    "semantic_title": "non-equilibrium annealed adjoint sampler",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=AYDMNzpJPv": {
    "title": "PhysVLM-AVR: Active Visual Reasoning for Multimodal Large Language Models in Physical Environments",
    "volume": "poster",
    "abstract": "Visual reasoning in multimodal large language models (MLLMs) has primarily been studied in passive, static settings, limiting their effectiveness in real-world physical environments where an embodied agent must contend with incomplete information due to occlusion or a limited field of view. Humans, in contrast, leverage their embodiment to actively explore and interact with their environment—moving, examining, and manipulating objects—to gather information through a closed-loop process integrating perception, reasoning, and action. Inspired by this capability, we introduce the Active Visual Reasoning (AVR) task, extending visual reasoning to a paradigm of embodied interaction in partially observable environments. AVR necessitates embodied agents to: (1) actively acquire information via sequential physical actions, (2) integrate observations across multiple steps for coherent reasoning, and (3) dynamically adjust decisions based on evolving visual feedback. To rigorously evaluate AVR, we introduce CLEVR-AVR, a simulation benchmark featuring multi-round interactive environments designed to assess both reasoning correctness and information-gathering efficiency. We present AVR-152k, a large-scale dataset that offers rich Chain-of-Thought (CoT) annotations detailing iterative reasoning for uncertainty identification, action-conditioned information gain prediction, and information-maximizing action selection, crucial for training agents in a higher-order Markov Decision Process. Building on this, we develop PhysVLM-AVR, an embodied MLLM achieving state-of-the-art performance on CLEVR-AVR, embodied reasoning (OpenEQA, RoboVQA), and passive visual reasoning (GeoMath, Geometry30K). Our analysis also reveals that current embodied MLLMs, despite detecting information incompleteness, struggle to actively acquire and integrate new information through interaction, highlighting a fundamental gap in active reasoning capabilities",
    "checked": true,
    "id": "4ef690813fa33b73780f75d69ac7d9e04adfa18b",
    "semantic_title": "physvlm-avr: active visual reasoning for multimodal large language models in physical environments",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xWCfTMjQH5": {
    "title": "Spurious-Aware Prototype Refinement for Reliable Out-of-Distribution Detection",
    "volume": "poster",
    "abstract": "Out-of-distribution (OOD) detection is crucial for ensuring the reliability and safety of machine learning models in real-world applications, where they frequently face data distributions unseen during training. Despite progress, existing methods are often vulnerable to spurious correlations that mislead models and compromise robustness. To address this, we propose SPROD, a novel prototype-based OOD detection approach that explicitly addresses the challenge posed by unknown spurious correlations. Our post-hoc method refines class prototypes to mitigate bias from spurious features without additional data or hyperparameter tuning, and is broadly applicable across diverse backbones and OOD detection settings. We conduct a comprehensive spurious correlation OOD detection benchmarking, comparing our method against existing approaches and demonstrating its superior performance across challenging OOD datasets, such as CelebA, Waterbirds, UrbanCars, Spurious Imagenet, and the newly introduced Animals MetaCoCo. On average, SPROD improves AUROC by 4.8% and FPR@95 by 9.4% over the second best",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2VX79YLT9s": {
    "title": "Understanding outer learning rates in Local SGD",
    "volume": "poster",
    "abstract": "Modern machine learning often requires training with large batch size, distributed data, and massively parallel compute hardware (like mobile and other edge devices or distributed data centers). Communication becomes a major bottleneck in such settings but methods like Local Stochastic Gradient Descent (Local SGD) show great promise to reduce the global communication need. Local SGD consists of three parts: a local optimization processes, an aggregation mechanism, and an outer optimizer that uses the aggregated updates from the nodes to produce a new model. While there exists an extensive literature on understanding the impact of hyperparameters in the local optimization process, the choice of outer optimizer and its hyperparameters is less clear. We study the role of the outer learning in Local SGD, and prove new convergence guarantees for the algorithm. In particular, we show that tuning the outer learning rate allows us to (a) trade off between optimization error and stochastic gradient noise variance, and (b) make up for ill-tuning of the inner learning rate. Our theory suggests that the outer learning rate should sometimes be set to values greater than $1$. We extend our results to apply to when we use momentum in the outer optimizer, and also introduce a novel data-dependent analysis of Local SGD that yields further insights on outer learning rate tuning. We conduct comprehensive experiments with standard language models and various outer optimizers to validate our theory",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PNU2mPauvd": {
    "title": "Beyond Greedy Exits: Improved Early Exit Decisions for Risk Control and Reliability",
    "volume": "poster",
    "abstract": "Early-Exit Deep Neural Networks enable adaptive inference by allowing prediction at intermediary layers, significantly reducing computational costs and latency. Most of the early exit strategies greedily exit a sample at an intermediary layer if the confidence in class prediction exceeds a predefined threshold that is set using a static validation set. This is problematic as the model might be overconfident in a wrong class. Also, they are not robust to distribution shifts encountered in deployment, which can undermine model trustworthiness and accuracy. To address these challenges, we propose UAT that adapts the threshold for exit decisions using a Multi-Armed Bandit framework, enabling online, unsupervised adjustment of exit decisions. UAT makes decisions based on a new reward function that assesses predictive certainty and its reliability to balance computational efficiency and prediction quality while penalizing unnecessary late exits. We provide guarantees on risk achieved by UAT and validate its performance on diverse tasks spanning vision-language understanding, text generation, and classification. Our framework demonstrates consistent improvements in speedup $(1.70-2.10\\times)$ with a minimal performance drop $(<2)$\\% as compared to full model performance",
    "checked": true,
    "id": "bf9d99a8ed87cb70ff88caaff9fadab6c79c8f44",
    "semantic_title": "beyond greedy exits: improved early exit decisions for risk control and reliability",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dCcWKeO4y4": {
    "title": "Stability and Sharper Risk Bounds with Convergence Rate O ~ ( 1 / n 2 )",
    "volume": "poster",
    "abstract": "Prior work (Klochkov \\& Zhivotovskiy, 2021) establishes at most $O\\left(\\log (n)/n\\right)$ excess risk bounds via algorithmic stability for strongly-convex learners with high probability. We show that under the similar common assumptions — Polyak-Lojasiewicz condition, smoothness, and Lipschitz continous for losses — rates of $O\\left(\\log^2(n)/n^2\\right)$ are at most achievable. To our knowledge, our analysis also provides the tightest high-probability bounds for gradient-based generalization gaps in nonconvex settings",
    "checked": false,
    "id": "1bccbb78aeba83601b15bf5b928cf490abd47066",
    "semantic_title": "stability and sharper risk bounds with convergence rate o(1/n2)",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LsmUgStXby": {
    "title": "AI-Generated Video Detection via Perceptual Straightening",
    "volume": "poster",
    "abstract": "The rapid advancement of generative AI enables highly realistic synthetic video, posing significant challenges for content authentication and raising urgent concerns about misuse. Existing detection methods often struggle with generalization and capturing subtle temporal inconsistencies. We propose $ReStraV$ ($Re$presentation $Stra$ightening for $V$ideo), a novel approach to distinguish natural from AI-generated videos. Inspired by the ``perceptual straightening'' hypothesis—which suggests real-world video trajectories become more straight in neural representation domain—we analyze deviations from this expected geometric property. Using a pre-trained self-supervised vision transformer (DINOv2), we quantify the temporal curvature and stepwise distance in the model's representation domain. We aggregate statistical and signals descriptors of these measures for each video and train a classifier. Our analysis shows that AI-generated videos exhibit significantly different curvature and distance patterns compared to real videos. A lightweight classifier achieves state-of-the-art detection performance (e.g., $97.17$ % accuracy and $98.63$ % AUROC on the VidProM benchmark, substantially outperforming existing image- and video-based methods. ReStraV is computationally efficient, it is offering a low-cost and effective detection solution. This work provides new insights into using neural representation geometry for AI-generated video detection",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wmkd9qYty4": {
    "title": "Learning Task-Agnostic Representations through Multi-Teacher Distillation",
    "volume": "poster",
    "abstract": "Casting complex inputs into tractable representations is a critical step across various fields. Diverse embedding models emerge from differences in architectures, loss functions, input modalities and datasets, each capturing unique aspects of the input. Multi-teacher distillation leverages this diversity to enrich representations but often remains tailored to specific tasks. We introduce a task-agnostic framework based on a ``majority vote\" objective function. We demonstrate that this function is bounded by the mutual information between the student and the teachers' embeddings, leading to a task-agnostic distillation loss that eliminates dependence on task-specific labels or prior knowledge. Comprehensive evaluations across text, vision models, and molecular modeling show that our method effectively leverages teacher diversity, resulting in representations enabling better performance for a wide range of downstream tasks such as classification, clustering, or regression. Additionally, we train and release state-of-the-art embedding models, enhancing downstream performance in various modalities",
    "checked": true,
    "id": "983985c18aee527121d6e9957ae6da3c7351389f",
    "semantic_title": "learning task-agnostic representations through multi-teacher distillation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2aANZeYgxS": {
    "title": "Bivariate Matrix-valued Linear Regression (BMLR): Finite-sample performance under Identifiability and Sparsity Assumptions",
    "volume": "poster",
    "abstract": "This paper studies a bilinear matrix-valued regression model where both predictors and responses are matrices. For each observation $t$, the response \\( Y_t \\in \\mathbb{R}^{n \\times p} \\) and predictor \\( X_t \\in \\mathbb{R}^{m \\times q} \\) satisfy $Y_t = A^* X_t B^* + E_t,$ with \\( A^* \\in \\mathbb{R}_+^{n \\times m} \\) (row-wise \\(\\ell_1\\)-normalized), \\( B^* \\in \\mathbb{R}^{q \\times p} \\), and \\( E_t \\) independent Gaussian noise matrices. The goal is to estimate \\( A^* \\) and \\( B^* \\) from the observed pairs \\( (X_t, Y_t) \\). We propose explicit, optimization-free estimators and establish non-asymptotic error bounds, including sparse settings. Simulations confirm the theoretical rates and demonstrate strong finite-sample performance. We further illustrate the practical utility of our method through an image denoising application on real data",
    "checked": true,
    "id": "381910809472374db64af0b7c567593714e6b06b",
    "semantic_title": "bivariate matrix-valued linear regression (bmlr): finite-sample performance under identifiability and sparsity assumptions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BneVLr7iis": {
    "title": "Reducing the Probability of Undesirable Outputs in Language Models Using Probabilistic Inference",
    "volume": "poster",
    "abstract": "Reinforcement learning (RL) has become a predominant technique to align language models (LMs) with human preferences or promote outputs which are deemed to be desirable by a given reward function. Standard RL approaches optimize average reward, while methods explicitly focused on reducing the probability of undesired outputs typically come at a cost to average-case performance. To improve this tradeoff, we introduce RePULSe, a new training method that augments the standard RL loss with an additional loss that uses learned proposals to guide sampling low-reward outputs, and then reduces those outputs' probability. We run experiments demonstrating that RePULSe produces a better tradeoff of expected reward versus the probability of undesired outputs and is more adversarially robust, compared to standard RL alignment approaches and alternatives",
    "checked": true,
    "id": "c17f3218e3b85f3426e981edf80135ee05f82ecb",
    "semantic_title": "reducing the probability of undesirable outputs in language models using probabilistic inference",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ngtFOxkQ8b": {
    "title": "Stratify or Die: Rethinking Data Splits in Image Segmentation",
    "volume": "poster",
    "abstract": "Random splitting of datasets in image segmentation often leads to unrepresentative test sets, resulting in biased evaluations and poor model generalization. While stratified sampling has proven effective for addressing label distribution imbalance in classification tasks, extending these ideas to segmentation remains challenging due to the multi-label structure and class imbalance typically present in such data. Building on existing stratification concepts, we introduce Iterative Pixel Stratification (IPS), a straightforward, label-aware sampling method tailored for segmentation tasks. Additionally, we present Wasserstein-Driven Evolutionary Stratification (WDES), a novel genetic algorithm designed to minimize the Wasserstein distance, thereby optimizing the similarity of label distributions across dataset splits. We prove that WDES is globally optimal given enough generations. Using newly proposed statistical heterogeneity metrics, we evaluate both methods against random sampling and find that WDES consistently produces more representative splits. Applying WDES across diverse segmentation tasks, including street scenes, medical imaging, and satellite imagery, leads to lower performance variance and improved model evaluation. Our results also highlight the particular value of WDES in handling small, imbalanced, and low-diversity datasets, where conventional splitting strategies are most prone to bias",
    "checked": true,
    "id": "d32f412cae961687b97c9ff1034f5b253b6d22ec",
    "semantic_title": "stratify or die: rethinking data splits in image segmentation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QuqsEIVWIG": {
    "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation",
    "volume": "poster",
    "abstract": "Scaling language models unlocks impressive capabilities, but the accompanying computational and memory demands make both training and deployment expensive. Existing efficiency efforts typically target either parameter sharing or adaptive computation, leaving open the question of how to attain both simultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework that combines the two axes of efficiency inside a single Recursive Transformer. MoR reuses a shared stack of layers across recursion steps to achieve parameter efficiency, while lightweight routers enable adaptive token-level thinking by dynamically assigning different recursion depths to individual tokens. This allows MoR to focus quadratic attention computation only among tokens still active at a given recursion depth, further improving memory access efficiency by selectively caching only their key-value pairs. Beyond these core mechanisms, we also propose a KV sharing variant that reuses KV pairs from the first recursion, specifically designed to further decrease memory footprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms a new Pareto frontier: at equal training FLOPs and smaller model sizes, it significantly lowers validation perplexity and improves few-shot accuracy, while delivering higher throughput compared with vanilla and existing recursive baselines",
    "checked": true,
    "id": "875418bb21265276be9298b50e52a6c53ff3a202",
    "semantic_title": "mixture-of-recursions: learning dynamic recursive depths for adaptive token-level computation",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=6FEpFCqH7o": {
    "title": "DictPFL: Efficient and Private Federated Learning on Encrypted Gradients",
    "volume": "poster",
    "abstract": "Federated Learning (FL) enables collaborative model training across institutions without sharing raw data. However, gradient sharing still risks privacy leakage, such as gradient inversion attacks. Homomorphic Encryption (HE) can secure aggregation but often incurs prohibitive computational and communication overhead. Existing HE-based FL methods sit at two extremes: encrypting all gradients for full privacy at high cost, or partially encrypting gradients to save resources while exposing vulnerabilities. We present **DictPFL**, a practical framework that achieves full gradient protection with minimal overhead. DictPFL encrypts every transmitted gradient while keeping non-transmitted parameters local, preserving privacy without heavy computation. It introduces two key modules: **Decompose-for-Partial-Encrypt (DePE)**, which decomposes model weights into a static dictionary and an updatable lookup table—only the latter is encrypted and aggregated, while the static dictionary remains local and requires neither sharing nor encryption; and **Prune-for-Minimum-Encrypt (PrME)**, which applies encryption-aware pruning to minimize encrypted parameters via consistent, history-guided masks. Experiments show that DictPFL reduces communication cost by 402-748$\\times$ and accelerates training by 28-65$\\times$ compared to fully encrypted FL, while outperforming state-of-the-art selective encryption methods by 51-155$\\times$ in overhead and 4-19$\\times$ in speed. Remarkably, DictPFL's runtime is within 2$\\times$ of plaintext FL, demonstrating, for the first time, that HE-based private federated learning is practical for real-world deployment. The code is publicly available at https://github.com/UCF-ML-Research/DictPFL",
    "checked": true,
    "id": "e672a7dfa6e03f6546fff5cdfc434ecd07255483",
    "semantic_title": "dictpfl: efficient and private federated learning on encrypted gradients",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RnbJPkakkm": {
    "title": "HELM: Hyperbolic Large Language Models via Mixture-of-Curvature Experts",
    "volume": "poster",
    "abstract": "Frontier large language models (LLMs) have shown great success in text modeling and generation tasks across domains. However, natural language exhibits inherent semantic hierarchies and nuanced geometric structure, which current LLMs do not capture completely owing to their reliance on Euclidean operations such as dot-products and norms. Furthermore, recent studies have shown that not respecting the underlying geometry of token embeddings leads to training instabilities and degradation of generative capabilities. These findings suggest that shifting to non-Euclidean geometries can better align language models with the underlying geometry of text. We thus propose to operate fully in $\\textit{Hyperbolic space}$, known for its expansive, scale-free, and low-distortion properties. To this end, we introduce $\\textbf{HELM}$, a family of $\\textbf{H}$yp$\\textbf{E}$rbolic Large $\\textbf{L}$anguage $\\textbf{M}$odels, offering a geometric rethinking of the Transformer-based LLM that addresses the representational inflexibility, missing set of necessary operations, and poor scalability of existing hyperbolic LMs. We additionally introduce a $\\textbf{Mi}$xture-of-$\\textbf{C}$urvature $\\textbf{E}$xperts model, $\\textbf{HELM-MiCE}$, where each expert operates in a distinct curvature space to encode more fine-grained geometric structure from text, as well as a dense model, $\\textbf{HELM-D}$. For $\\textbf{HELM-MiCE}$, we further develop hyperbolic Multi-Head Latent Attention ($\\textbf{HMLA}$) for efficient, reduced-KV-cache training and inference. For both models, we further develop essential hyperbolic equivalents of rotary positional encodings and root mean square normalization. We are the first to train fully hyperbolic LLMs at billion-parameter scale, and evaluate them on well-known benchmarks such as MMLU and ARC, spanning STEM problem-solving, general knowledge, and commonsense reasoning. Our results show consistent gains from our $\\textbf{HELM}$ architectures – up to 4\\% – over popular Euclidean architectures used in LLaMA and DeepSeek with superior semantic hierarchy modeling capabilities, highlighting the efficacy and enhanced reasoning afforded by hyperbolic geometry in large-scale language model pretraining",
    "checked": true,
    "id": "b48eb1e524423cf35fd565cfd50c9a790e2ad85b",
    "semantic_title": "helm: hyperbolic large language models via mixture-of-curvature experts",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=Smy93p3NCT": {
    "title": "Transformers are almost optimal metalearners for linear classification",
    "volume": "poster",
    "abstract": "Transformers have demonstrated impressive in-context learning (ICL) capabilities, raising the question of whether they can serve as metalearners that adapt to new tasks using only a small number of in-context examples, without any further training. While recent theoretical work has studied transformers' ability to perform ICL, most of these analyses do not address the formal metalearning setting, where the objective is to solve a collection of related tasks more efficiently than would be possible by solving each task individually. In this paper, we provide the first theoretical analysis showing that a simplified transformer architecture trained via gradient descent can act as a near-optimal metalearner in a linear classification setting. We consider a natural family of tasks where each task corresponds to a class-conditional Gaussian mixture model, with the mean vectors lying in a shared $k$-dimensional subspace of $\\mathbb{R}^d$. After training on a sufficient number of such tasks, we show that the transformer can generalize to a new task using only $\\widetilde{O}(k / \\widetilde{R}^4)$ in-context examples, where $\\widetilde{R}$ denotes the signal strength at test time. This performance (almost) matches that of an optimal learner that knows exactly the shared subspace and significantly outperforms any learner that only has access to the in-context data, which requires $\\Omega(d / \\widetilde{R}^4)$ examples to generalize",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vzi96rTe4w": {
    "title": "scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery",
    "volume": "poster",
    "abstract": "We present scPilot, the first systematic framework to practice \\textit{omics-native reasoning}: a large language model (LLM) converses in natural language while directly inspecting single-cell RNA-seq data and on-demand bioinformatics tools. scPilot converts core single-cell analyses, i.e., cell-type annotation, developmental-trajectory reconstruction, and transcription-factor targeting, into step-by-step reasoning problems that the model must solve, justify, and, when needed, revise with new evidence. To measure progress, we release \\scbench, a suite of 9 expertly curated datasets and graders that faithfully evaluate the omics-native reasoning capability of scPilot w.r.t various LLMs. Experiments with o1 show that \\textit{iterative} omics-native reasoning lifts average accuracy by 11\\% for cell-type annotation and Gemini 2.5 Pro cuts trajectory graph-edit distance by 30\\% versus one-shot prompting, while generating transparent reasoning traces that explain marker gene ambiguity and regulatory logic. By grounding LLMs in raw omics data, scPilot enables auditable, interpretable, and diagnostically informative single-cell analyses",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Eyis2h3tba": {
    "title": "Set-LLM: A Permutation-Invariant LLM",
    "volume": "poster",
    "abstract": "While large language models (LLMs) demonstrate impressive capabilities across numerous applications, their robustness remains a critical concern. This paper is motivated by a specific vulnerability: the order sensitivity of LLMs. This vulnerability manifests itself as the order bias observed when LLMs decide between possible options (for example, a preference for the first option) and the tendency of LLMs to provide different answers when options are reordered. The use cases for this scenario extend beyond the classical case of multiple-choice question answering to the use of LLMs for multidocument tasks and as automated evaluators in AI pipelines. We introduce Set-LLM, a novel architectural adaptation for pretrained LLMs that enables the processing of mixed set-text inputs with permutation invariance guarantees. The adaptations involve a new attention mask and new positional encodings specifically designed for sets. We provide a theoretical proof of invariance and demonstrate through experiments that Set-LLM can be trained effectively, achieving comparable or improved performance and maintaining the runtime of the original model, while altogether eliminating order sensitivity",
    "checked": true,
    "id": "9191238fcdc621dfbde5ed40caa68a5010feee67",
    "semantic_title": "set-llm: a permutation-invariant llm",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=V3WQoshcZe": {
    "title": "MEIcoder: Decoding Visual Stimuli from Neural Activity by Leveraging Most Exciting Inputs",
    "volume": "poster",
    "abstract": "Decoding visual stimuli from neural population activity is crucial for understanding the brain and for applications in brain-machine interfaces. However, such biological data is often scarce, particularly in primates or humans, where high-throughput recording techniques, such as two-photon imaging, remain challenging or impossible to apply. This, in turn, poses a challenge for deep learning decoding techniques. To overcome this, we introduce MEIcoder, a biologically informed decoding method that leverages neuron-specific most exciting inputs (MEIs), a structural similarity index measure loss, and adversarial training. MEIcoder achieves state-of-the-art performance in reconstructing visual stimuli from single-cell activity in primary visual cortex (V1), especially excelling on small datasets with fewer recorded neurons. Using ablation studies, we demonstrate that MEIs are the main drivers of the performance, and in scaling experiments, we show that MEIcoder can reconstruct high-fidelity natural-looking images from as few as 1,000-2,500 neurons and less than 1,000 training data points. We also propose a unified benchmark with over 160,000 samples to foster future research. Our results demonstrate the feasibility of reliable decoding in early visual system and provide practical insights for neuroscience and neuroengineering applications",
    "checked": true,
    "id": "0384cfec3be5fd8ba301f4e2d27ac956c5113000",
    "semantic_title": "meicoder: decoding visual stimuli from neural activity by leveraging most exciting inputs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yst8MHfcgP": {
    "title": "A Minimalist Example of Edge-of-Stability and Progressive Sharpening",
    "volume": "poster",
    "abstract": "Recent advances in deep learning optimization have unveiled two intriguing phenomena under large learning rates: Edge of Stability (EoS) and Progressive Sharpening (PS), challenging classical Gradient Descent (GD) analyses. Current research approaches, using either generalist frameworks or minimalist examples, face significant limitations in explaining these phenomena. This paper advances the minimalist approach by introducing a two-layer network with a two-dimensional input, where one dimension is relevant to the response and the other is irrelevant. Through this model, we rigorously prove the existence of progressive sharpening and self-stabilization under large learning rates, and establish non-asymptotic analysis of the training dynamics and sharpness along the entire GD trajectory. Besides, we connect our minimalist example to existing works by reconciling the existence of a well-behaved \"stable set\" between minimalist and generalist analyses, and extending the analysis of Gradient Flow Solution sharpness to our two-dimensional input scenario. These findings provide new insights into the EoS phenomenon from both parameter and input data distribution perspectives, potentially informing more effective optimization strategies in deep learning practice",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oOPdUnswhZ": {
    "title": "Large Language Models as Model Organisms for Human Associative Learning",
    "volume": "poster",
    "abstract": "Associative learning--forming links between co-occurring items--is fundamental to human cognition, reshaping internal representations in complex ways. Testing hypotheses on how representational changes occur in biological systems is challenging, but large language models (LLMs) offer a scalable alternative. Building on LLMs' in-context learning, we adapt a cognitive neuroscience associative learning paradigm and investigate how representations evolve across six models. Our initial findings reveal a non-monotonic pattern consistent with the Non-Monotonic Plasticity Hypothesis, with moderately similar items differentiating after learning. Leveraging the controllability of LLMs, we further show that this differentiation is modulated by the overlap of associated items with the broader vocabulary--a factor we term vocabulary interference, capturing how new associations compete with prior knowledge. We find that higher vocabulary interference amplifies differentiation, suggesting that representational change is influenced by both item similarity and global competition. Our findings position LLMs not only as powerful tools for studying representational dynamics in human-like learning systems, but also as accessible and general computational models for generating new hypotheses about the principles underlying memory reorganization in the brain",
    "checked": true,
    "id": "ad127de9082f31cd7a60e79b10567b166f29c927",
    "semantic_title": "large language models as model organisms for human associative learning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R9xJSk5SQ2": {
    "title": "DEFT: Decompositional Efficient Fine-Tuning for Text-to-Image Models",
    "volume": "poster",
    "abstract": "Efficient fine-tuning of pre-trained Text-to-Image (T2I) models involves adjusting the model to suit a particular task or dataset while minimizing computational resources and limiting the number of trainable parameters. However, it often faces challenges in striking a trade-off between aligning with the target distribution: learning a novel concept from a limited image for personalization and retaining the instruction ability needed for unifying multiple tasks, all while maintaining editability (aligning with a variety of prompts or in-context generation). In this work, we introduce DEFT, Decompositional Efficient Fine-Tuning, an efficient fine-tuning framework that adapts a pre-trained weight matrix by decomposing its update into two components with two trainable matrices: (1) a projection onto the complement of a low-rank subspace spanned by a low-rank matrix, and (2) a low-rank update. The single trainable low-rank matrix defines the subspace, while the other trainable low-rank matrix enables parameter adaptation within that subspace. We conducted extensive experiments on the Dreambooth and Dreambench Plus datasets for personalization, the InsDet dataset for object and scene adaptation, and the VisualCloze dataset for a universal image generation framework through visual in-context learning with both Stable Diffusion and a unified model. Our results demonstrated state-of-the-art performance, highlighting the emergent properties of efficient fine-tuning. Our code is available on \\href{https://github.com/MAXNORM8650/DEFT}{DEFT}",
    "checked": true,
    "id": "7d80ab20b1e367bc663c7f3d7900d376dd881b13",
    "semantic_title": "deft: decompositional efficient fine-tuning for text-to-image models",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oMi4uyNOlL": {
    "title": "Gradient Descent as Loss Landscape Navigation: a Normative Framework for Deriving Learning Rules",
    "volume": "poster",
    "abstract": "Learning rules—prescriptions for updating model parameters to improve performance—are typically assumed rather than derived. Why do some learning rules work better than others, and under what assumptions can a given rule be considered optimal? We propose a theoretical framework that casts learning rules as policies for navigating (partially observable) loss landscapes, and identifies optimal rules as solutions to an associated optimal control problem. A range of well-known rules emerge naturally within this framework under different assumptions: gradient descent from short-horizon optimization, momentum from longer-horizon planning, natural gradients from accounting for parameter space geometry, non-gradient rules from partial controllability, and adaptive optimizers like Adam from online Bayesian inference of loss landscape shape. We further show that continual learning strategies like weight resetting can be understood as optimal responses to task uncertainty. By unifying these phenomena under a single objective, our framework clarifies the computational structure of learning and offers a principled foundation for designing adaptive algorithms",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DvwKWKG1Ul": {
    "title": "Differentiation Through Black-Box Quadratic Programming Solvers",
    "volume": "poster",
    "abstract": "Differentiable optimization has attracted significant research interest, particularly for quadratic programming (QP). Existing approaches for differentiating the solution of a QP with respect to its defining parameters often rely on specific integrated solvers. This integration limits their applicability, including their use in neural network architectures and bi-level optimization tasks, restricting users to a narrow selection of solver choices. To address this limitation, we introduce **dQP**, a modular and solver-agnostic framework for plug-and-play differentiation of virtually any QP solver. A key insight we leverage to achieve modularity is that, once the active set of inequality constraints is known, both the solution and its derivative can be expressed using simplified linear systems that share the same matrix. This formulation fully decouples the computation of the QP solution from its differentiation. Building on this result, we provide a minimal-overhead, open-source implementation (<https://github.com/cwmagoon/dQP>) that seamlessly integrates with over 15 state-of-the-art solvers. Comprehensive benchmark experiments demonstrate dQP's robustness and scalability, particularly highlighting its advantages in large-scale sparse problems",
    "checked": true,
    "id": "cb60f20c2beaf9759bd1a744450db5e6322757b3",
    "semantic_title": "differentiation through black-box quadratic programming solvers",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=GYLMYTYqZg": {
    "title": "Low Precision Streaming PCA",
    "volume": "poster",
    "abstract": "Low-precision Streaming PCA estimates the top principal component in a streaming setting under limited precision. We establish an information‐theoretic lower bound on the quantization resolution required to achieve a target accuracy for the leading eigenvector. We study Oja's algorithm for streaming PCA under linear and nonlinear stochastic quantization. The quantized variants use unbiased stochastic quantization of the weight vector and the updates. Under mild moment and spectral-gap assumptions on the data distribution, we show that a batched version achieves the lower bound up to logarithmic factors under both schemes. This leads to a nearly _dimension-free_ quantization error in the nonlinear quantization setting. Empirical evaluations on synthetic streams validate our theoretical findings and demonstrate that our low-precision methods closely track the performance of standard Oja's algorithm",
    "checked": false,
    "id": "d2f91e6570c2c3126f2dd486f96c086b41e21fb8",
    "semantic_title": "low-precision streaming pca",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KoVKLxn3Nb": {
    "title": "Conformal Prediction Beyond the Seen: A Missing Mass Perspective for Uncertainty Quantification in Generative Models",
    "volume": "poster",
    "abstract": "Uncertainty quantification (UQ) is essential for safe deployment of generative AI models such as large language models (LLMs), especially in high-stakes applications. Conformal prediction (CP) offers a principled uncertainty quantification framework, but classical methods focus on regression and classification, relying on geometric distances or softmax scores--tools that presuppose structured outputs. We depart from this paradigm by studying CP in a query-only setting, where prediction sets must be constructed solely from finite queries to a black-box generative model, introducing a new trade-off between coverage, test-time query budget, and informativeness. We introduce **Conformal Prediction with Query Oracle** (CPQ), a framework characterizing the optimal interplay between these objectives. Our finite-sample algorithm is built on two core principles: one governs the optimal query policy, and the other defines the optimal mapping from queried samples to prediction sets. Remarkably, both are rooted in the classical **missing mass problem** in statistics. Specifically, the optimal query policy depends on the rate of decay--or the derivative--of the missing mass, for which we develop a novel estimator. Meanwhile, the optimal mapping hinges on the missing mass itself, which we estimate using Good-Turing estimators. We then turn our focus to implementing our method for language models, particularly in open-ended LLM tasks involving question answering, multi-step reasoning, and structured information extraction, where outputs are vast, variable, and often under-specified. Fine-grained experiments on three real-world open-ended tasks and two LLMs, show CPQ's applicability to **any black-box LLM** and highlight: (1) individual contribution of each principle to CPQ's performance, and (2) CPQ's ability to yield significantly more informative prediction sets than existing conformal methods for language uncertainty quantification",
    "checked": true,
    "id": "899df6b6dfef99ec7422fdbaf74b6a9ccaccb6dc",
    "semantic_title": "conformal prediction beyond the seen: a missing mass perspective for uncertainty quantification in generative models",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=iEtCCt6FjP": {
    "title": "Towards Resilient Safety-driven Unlearning for Diffusion Models against Downstream Fine-tuning",
    "volume": "poster",
    "abstract": "Text-to-image (T2I) diffusion models have achieved impressive image generation quality and are increasingly fine-tuned for personalized applications. However, these models often inherit unsafe behaviors from toxic pretraining data, raising growing safety concerns. While recent safety-driven unlearning methods have made promising progress in suppressing model toxicity, they are found to be fragile to downstream fine-tuning, as we reveal that state-of-the-art methods largely fail to retain their effectiveness even when fine-tuned on entirely benign datasets. To mitigate this problem, in this paper, we propose ResAlign, a safety-driven unlearning framework with enhanced resilience against downstream fine-tuning. By modeling downstream fine-tuning as an implicit optimization problem with a Moreau envelope-based reformulation, ResAlign enables efficient gradient estimation to minimize the recovery of harmful behaviors. Additionally, a meta-learning strategy is proposed to simulate a diverse distribution of fine-tuning scenarios to improve generalization. Extensive experiments across a wide range of datasets, fine-tuning methods, and configurations demonstrate that ResAlign consistently outperforms prior unlearning approaches in retaining safety, while effectively preserving benign generation capability. Our code and pretrained models are publicly available at https://github.com/AntigoneRandy/ResAlign",
    "checked": true,
    "id": "25bd4aa11792e15d9686f3e0315ebf34f1aabf2c",
    "semantic_title": "towards resilient safety-driven unlearning for diffusion models against downstream fine-tuning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=19MEprgUub": {
    "title": "A Black-Box Debiasing Framework for Conditional Sampling",
    "volume": "poster",
    "abstract": "Conditional sampling is a fundamental task in Bayesian statistics and generative modeling. Consider the problem of sampling from the posterior distribution $P\\_{X|Y=y^\\*}$ for some observation $y^\\*$, where the likelihood $P\\_{Y|X}$ is known, and we are given $n$ i.i.d. samples $D=\\\\{X\\_i\\\\}\\_{i=1}^n$ drawn from an unknown prior distribution $\\pi\\_X$. Suppose that $f(\\hat{\\pi}\\_{X^n})$ is the distribution of a posterior sample generated by an algorithm (e.g. a conditional generative model or the Bayes rule) when $\\hat{\\pi}\\_{X^n}$ is the empirical distribution of the training data. Although averaging over the randomness of the training data $D$, we have $\\mathbb{E}\\_D\\left(\\hat{\\pi}\\_{X^n}\\right)= \\pi\\_X$, we do not have $\\mathbb{E}\\_D\\left\\\\{f(\\hat{\\pi}\\_{X^n})\\right\\\\}= f(\\pi\\_X)$ due to the nonlinearity of $f$, leading to a bias. In this paper we propose a black-box debiasing scheme that improves the accuracy of such a naive plug-in approach. For any integer $k$ and under boundedness of the likelihood and smoothness of $f$, we generate samples $\\hat{X}^{(1)},\\dots,\\hat{X}^{(k)}$ and weights $w\\_1,\\dots,w\\_k$ such that $\\sum_{i=1}^kw_iP\\_{\\hat{X}^{(i)}}$ is a $k$-th order approximation of $f(\\pi\\_X)$, where the generation process treats $f$ as a black-box. Our generation process achieves higher accuracy when averaged over the randomness of the training data, without degrading the variance, which can be interpreted as improving memorization without compromising generalization in generative models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ud7VVZ693U": {
    "title": "Benign Overfitting in Single-Head Attention",
    "volume": "poster",
    "abstract": "The phenomenon of benign overfitting, where a trained neural network perfectly fits noisy training data but still achieves near-optimal test performance, has been extensively studied in recent years for linear models and fully-connected/convolutional networks. In this work, we study benign overfitting in a single-head softmax attention model, which is the fundamental building block of Transformers. We prove that under appropriate conditions, the model exhibits benign overfitting in a classification setting already after two steps of gradient descent. Moreover, we show conditions where a minimum-norm/maximum-margin interpolator exhibits benign overfitting. We study how the overfitting behavior depends on the signal-to-noise ratio (SNR) of the data distribution, namely, the ratio between norms of signal and noise tokens, and prove that a sufficiently large SNR is both necessary and sufficient for benign overfitting",
    "checked": true,
    "id": "d685b5a57462b095e5bbde0a1033acf321cefd99",
    "semantic_title": "benign overfitting in single-head attention",
    "citation_count": 5,
    "authors": []
  },
  "https://openreview.net/forum?id=xNUNxRj2vJ": {
    "title": "A Pre-training Framework for Relational Data with Information-theoretic Principles",
    "volume": "poster",
    "abstract": "Relational databases underpin critical infrastructure across a wide range of domains, yet the design of generalizable pre-training strategies for learning from relational databases remains an open challenge due to task heterogeneity. Specifically, there exist many possible downstream tasks, as tasks are defined based on relational schema graphs, temporal dependencies, and SQL-defined label logics. An effective pre-training framework is desired to take these factors into account in order to obtain task-aware representations. By incorporating knowledge of the underlying distribution that drives label generation, downstream tasks can benefit from relevant side-channel information. To bridge this gap, we introduce Task Vector Estimation (TVE), a novel pre-training framework that constructs predictive supervisory signals via set-based aggregation over schema traversal graphs, explicitly modeling next-window relational dynamics. We formalize our approach through an information-theoretic lens, demonstrating that task-informed representations retain more relevant signals than those obtained without task priors. Extensive experiments on the RelBench benchmark show that TVE consistently outperforms traditional pre-training baselines. Our findings advocate for pre-training objectives that encode task heterogeneity and temporal structure as design principles for predictive modeling on relational databases. Our code is publicly available at https://github.com/quang-truong/task-vector-estimation",
    "checked": true,
    "id": "01ca774cfbb0c678df2222932b2de32cbc70030b",
    "semantic_title": "a pre-training framework for relational data with information-theoretic principles",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=En1F2gjza6": {
    "title": "P-Law: Predicting Quantitative Scaling Law with Entropy Guidance in Large Recommendation Models",
    "volume": "poster",
    "abstract": "With the growing size of data and models in Large Recommendation Models, the time required for debugging has become increasingly prohibitive, underscoring the urgent need for effective guidance in parameter configuration. The Scaling Law (SL) offers analogous guidance in the Sequential Language domain, having achieved significant success by predicting model loss when scaling model size. However, the existing guidance from SL for Sequential Recommendation (SR) remains qualitative, which is because quantitative analysis of SL on SR encounters challenges with quality measurement on redundant sequences along with loss-performance discrepancy. In response, we introduce the Performance Law (P-Law) for SR models, which predicts model performance across various settings, intending to provide a quantitative framework for guiding the parameter optimization of future models. Initially, Performance Law utilizes Real Entropy to measure data quality, aiming to remove the low-quality influence of low-entropy redundant sequences. Subsequently, Performance Law investigates a fitting decay term, which facilitated the prediction of the major loss-performance discrepancy phenomena of overfitting, ultimately achieving quantitative performance prediction. Extensive experiment on various datasets demonstrates the effectiveness of Performance Law by displaying exceptional quantitative prediction ability against the original and modified qualitative SL. Additional application experiments on optimal parameter prediction and model expansion potential prediction also demonstrated the broad applicability of the Performance Law. Our code is available at https://github.com/USTC-StarTeam/P-Law",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kvI0QTVRQD": {
    "title": "Equivariance by Contrast: Identifiable Equivariant Embeddings from Unlabeled Finite Group Actions",
    "volume": "poster",
    "abstract": "We propose Equivariance by Contrast (EbC) to learn equivariant embeddings from observation pairs $(\\mathbf{y}, g \\cdot \\mathbf{y})$, where $g$ is drawn from a finite group acting on the data. Our method jointly learns a latent space and a group representation in which group actions correspond to invertible linear maps—without relying on group-specific inductive biases. We validate our approach on the infinite dSprites dataset with structured transformations defined by the finite group $G:= (R_m \\times \\mathbb{Z}_n \\times \\mathbb{Z}_n)$, combining discrete rotations and periodic translations. The resulting embeddings exhibit high-fidelity equivariance, with group operations faithfully reproduced in latent space. On synthetic data, we further validate the approach on the non-abelian orthogonal group $O(n)$ and the general linear group $GL(n)$. We also provide a theoretical proof for identifiability. While broad evaluation across diverse group types on real-world data remains future work, our results constitute the first successful demonstration of general-purpose encoder-only equivariant learning from group action observations alone, including non-trivial non-abelian groups and a product group motivated by modeling affine equivariances in computer vision",
    "checked": true,
    "id": "82df1e810cab98d99c0718fd747a2e6e35736ac3",
    "semantic_title": "equivariance by contrast: identifiable equivariant embeddings from unlabeled finite group actions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=87c2JwNJa0": {
    "title": "LoMix: Learnable Weighted Multi-Scale Logits Mixing for Medical Image Segmentation",
    "volume": "poster",
    "abstract": "U‑shaped networks output logits at multiple spatial scales, each capturing a different blend of coarse context and fine detail. Yet, training still treats these logits in isolation—either supervising only the final, highest‑resolution logits or applying deep supervision with identical loss weights at every scale—without exploring *mixed‑scale* combinations. Consequently, the decoder output misses the complementary cues that arise only when coarse and fine predictions are fused. To address this issue, we introduce LoMix ($\\underline{Lo}$gits $\\underline{Mix}$ing), a Neural Architecture Search (NAS)‑inspired, differentiable plug-and-play module that **generates** new mixed‑scale outputs and **learns** how exactly each of them should guide the training process. More precisely, LoMix mixes the multi-scale decoder logits with four lightweight fusion operators: addition, multiplication, concatenation, and attention-based weighted fusion, yielding a rich set of synthetic \"mutant'' maps. Every original or mutant map is given a softplus loss weight that is co‑optimized with network parameters, mimicking a one‑step architecture search that automatically discovers the most useful scales, mixtures, and operators. Plugging LoMix into recent U-shaped architectures (i.e., PVT‑V2‑B2 backbone with EMCAD decoder) on Synapse 8‑organ dataset improves DICE by +4.2\\% over single‑output supervision, +2.2\\% over deep supervision, and +1.5\\% over equally weighted additive fusion, all with **zero** inference overhead. When training data are scarce (e.g., one or two labeled scans, 5\\% of the trainset), the advantage grows to +9.23\\%, underscoring LoMix's data efficiency. Across four benchmarks and diverse U-shaped networks, LoMiX improves DICE by up to +13.5\\% over single-output supervision, confirming that learnable weighted mixed‑scale fusion generalizes broadly while remaining data efficient, fully interpretable, and overhead-free at inference. Our implementation is available at https://github.com/SLDGroup/LoMix",
    "checked": true,
    "id": "e16e6d2e9bf83e2091d840b1fee5f07c59f01570",
    "semantic_title": "lomix: learnable weighted multi-scale logits mixing for medical image segmentation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tJZKaDSSTX": {
    "title": "Robust and Computation-Aware Gaussian Processes",
    "volume": "poster",
    "abstract": "Gaussian processes (GPs) are widely used for regression and optimization tasks such as Bayesian optimization (BO) due to their expressiveness and principled uncertainty estimates. However, in settings with large datasets corrupted by outliers, standard GPs and their sparse approximations struggle with computational tractability and robustness. We introduce Robust Computation-aware Gaussian Process (RCaGP), a novel GP model that jointly addresses these challenges by combining a principled treatment of approximation-induced uncertainty with robust generalized Bayesian updating. The key insight is that robustness and approximation-awareness are not orthogonal but intertwined: approximations can exacerbate the impact of outliers, and mitigating one without the other is insufficient. Unlike previous work that focuses narrowly on either robustness or approximation quality, RCaGP combines both in a principled and scalable framework, thus effectively managing both outliers and computational uncertainties introduced by approximations such as low-rank matrix multiplications. Our model ensures more conservative and reliable uncertainty estimates, a property we rigorously demonstrate. Additionally, we establish a robustness property and show that the mean function is key to preserving it, motivating a tailored model selection scheme for robust mean functions. Empirical results confirm that solving these challenges jointly leads to superior performance across both clean and outlier-contaminated settings, both on regression and high-throughput Bayesian optimization benchmarks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XX7XpzSMHd": {
    "title": "Semi-infinite Nonconvex Constrained Min-Max Optimization",
    "volume": "poster",
    "abstract": "Semi-Infinite Programming (SIP) has emerged as a powerful framework for modeling problems with infinite constraints, however, its theoretical development in the context of nonconvex and large-scale optimization remains limited. In this paper, we investigate a class of nonconvex min-max optimization problems with nonconvex infinite constraints, motivated by applications such as adversarial robustness and safety-constrained learning. We propose a novel inexact dynamic barrier primal-dual algorithm and establish its convergence properties. Specifically, under the assumption that the squared infeasibility residual function satisfies the Lojasiewicz inequality with exponent $\\theta \\in (0,1)$, we prove that the proposed method achieves $\\mathcal{O}(\\epsilon^{-3})$, $\\mathcal{O}(\\epsilon^{-6\\theta})$, and $\\mathcal{O}(\\epsilon^{-3\\theta/(1-\\theta)})$ iteration complexities to achieve an $\\epsilon$-approximate stationarity, infeasibility, and complementarity slackness, respectively. Numerical experiments on robust multitask learning with task priority further illustrate the practical effectiveness of the algorithm",
    "checked": true,
    "id": "c8fd747933ff6fea76b48432cc66c4b54512938f",
    "semantic_title": "semi-infinite nonconvex constrained min-max optimization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eWxKpdAdXH": {
    "title": "Refusal Direction is Universal Across Safety-Aligned Languages",
    "volume": "poster",
    "abstract": "Refusal mechanisms in large language models (LLMs) are essential for ensuring safety. Recent research has revealed that refusal behavior can be mediated by a single direction in activation space, enabling targeted interventions to bypass refusals. While this is primarily demonstrated in an English-centric context, appropriate refusal behavior is important for any language, but poorly understood. In this paper, we investigate the refusal behavior in LLMs across 14 languages using \\textit{PolyRefuse}, a multilingual safety dataset created by translating malicious and benign English prompts into these languages. We uncover the surprising cross-lingual universality of the refusal direction: a vector extracted from English can bypass refusals in other languages with near-perfect effectiveness, without any additional fine-tuning. Even more remarkably, refusal directions derived from any safety-aligned language transfer seamlessly to others. We attribute this transferability to the parallelism of refusal vectors across languages in the embedding space and identify the underlying mechanism behind cross-lingual jailbreaks. These findings provide actionable insights for building more robust multilingual safety defenses and pave the way for a deeper mechanistic understanding of cross-lingual vulnerabilities in LLMs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KxoPiQ03BT": {
    "title": "Advancing Interpretability of CLIP Representations with Concept Surrogate Model",
    "volume": "poster",
    "abstract": "Contrastive Language-Image Pre-training (CLIP) generates versatile multimodal embeddings for diverse applications, yet the specific information captured within these representations is not fully understood. Current explainability techniques often target specific tasks, overlooking the rich, general semantics inherent in the representations. Our objective is to reveal the concepts encoded in CLIP embeddings by learning a surrogate representation, which is expressed as a linear combination of human-understandable concepts evident in the image. Our method, which we term EXPLAIN-R, introduces a novel approach that leverages CLIP's learned instance-instance similarity to train a surrogate model that faithfully mimics CLIP's behavior. From the trained surrogate, we derive concept scores for each input image; these scores quantify the contribution of each concept and act as the explanation for the representation. Quantitative evaluations on multiple datasets demonstrate our method's superior faithfulness over the baseline. Moreover, a user study confirms that our explanations are perceived as more relevant, complete, and useful. Our work provides a novel approach for interpreting CLIP image representations, enhancing the user interpretability of representations and fostering more trustworthy AI systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jiCLUPq5xv": {
    "title": "Harnessing the Universal Geometry of Embeddings",
    "volume": "poster",
    "abstract": "We introduce the first method for translating text embeddings from one vector space to another without any paired data, encoders, or predefined sets of matches. Our unsupervised approach translates any embedding to and from a universal latent representation (i.e., a universal semantic structure conjectured by the Platonic Representation Hypothesis). Our translations achieve high cosine similarity across model pairs with different architectures, parameter counts, and training datasets. The ability to translate unknown embeddings into a different space while preserving their geometry has serious implications for the security of vector databases. An adversary with access only to embedding vectors can extract sensitive information about the underlying documents, sufficient for classification and attribute inference",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=de3kwOXQ9e": {
    "title": "Low Rank Gradients and Where to Find Them",
    "volume": "poster",
    "abstract": "This paper investigates low-rank structure in the gradients of the training loss for two-layer neural networks while relaxing the usual isotropy assumptions on the training data and parameters. We consider a spiked data model in which the bulk can be anisotropic and ill-conditioned, we do not require independent data and weight matrices and we also analyze both the mean-field and neural-tangent-kernel scalings. We show that the gradient with respect to the input weights is approximately low rank and is dominated by two rank-one terms: one aligned with the bulk data–residue, and another aligned with the rank one spike in the input data. We characterize how properties of the training data, the scaling regime and the activation function govern the balance between these two components. Additionally, we also demonstrate that standard regularizers, such as weight decay, input noise and Jacobian penalties, also selectively modulate these components. Experiments on synthetic and real data corroborate our theoretical predictions",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T3ReIjtbYy": {
    "title": "Latent Principle Discovery for Language Model Self-Improvement",
    "volume": "poster",
    "abstract": "When language model (LM) users aim to improve the quality of its generations, it is crucial to specify concrete behavioral attributes that the model should strive to reflect. However, curating such principles across many domains, even non-exhaustively, requires a labor-intensive annotation process. To automate this process, we propose eliciting these latent attributes that guide model reasoning toward human-preferred responses by explicitly modeling them in a self-correction setting. Our approach mines new principles from the LM itself and compresses the discovered elements to an interpretable set via clustering. Specifically, we employ a form of posterior-regularized Monte Carlo Expectation-Maximization to both identify a condensed set of the most effective latent principles and teach the LM to strategically invoke them in order to intrinsically refine its responses. We demonstrate that bootstrapping our algorithm over multiple iterations enables smaller language models (7-8B parameters) to self-improve, achieving +8-10\\% in AlpacaEval win-rate, an average of +0.3 on MT-Bench, and +19-23\\% in principle-following win-rate on IFEval. We also show that clustering the principles yields interpretable and diverse model-generated constitutions while retaining model performance. The gains that our method achieves highlight the potential of automated, principle-driven post-training recipes toward continual self-improvement",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FiM0M8gcct": {
    "title": "A-Mem: Agentic Memory for LLM Agents",
    "volume": "poster",
    "abstract": "While large language model (LLM) agents can effectively use external tools for complex real-world tasks, they require memory systems to leverage historical experiences. Current memory systems enable basic storage and retrieval but lack sophisticated memory organization, despite recent attempts to incorporate graph databases. Moreover, these systems' fixed operations and structures limit their adaptability across diverse tasks. To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking. When a new memory is added, we generate a comprehensive note containing multiple structured attributes, including contextual descriptions, keywords, and tags. The system then analyzes historical memories to identify relevant connections, establishing links where meaningful similarities exist. Additionally, this process enables memory evolution -- as new memories are integrated, they can trigger updates to the contextual representations and attributes of existing historical memories, allowing the memory network to continuously refine its understanding. Our approach combines the structured organization principles of Zettelkasten with the flexibility of agent-driven decision making, allowing for more adaptive and context-aware memory management. Empirical experiments on six foundation models show superior improvement against existing SOTA baselines. The code is available at \\url{https://anonymous.4open.science/r/AgenticMemory-76B4}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r8U08iSppJ": {
    "title": "Balancing Gradient and Hessian Queries in Non-Convex Optimization",
    "volume": "poster",
    "abstract": "We develop optimization methods which offer new trade-offs between the number of gradient and Hessian computations needed to compute the critical point of a non-convex function. We provide a method that for a twice-differentiable $f\\colon \\mathbb{R}^d \\rightarrow \\mathbb{R}$ with $L_2$-Lipschitz Hessian, and input initial point with $\\Delta$-bounded sub-optimality and sufficiently small $\\epsilon > 0$ outputs an $\\epsilon$-critical point, i.e., a point $x$ such that $\\|\\nabla f(x)\\| \\leq \\epsilon$, using $\\tilde{O}(\\Delta L_2^{1/4} n_H^{-1/2}\\epsilon^{-9/4})$ queries to a gradient oracle and $n_H$ queries to a Hessian oracle. As a consequence, we obtain an improved gradient query complexity of $\\tilde{O}(d^{1/3}L_2^{1/2}\\Delta\\epsilon^{-3/2})$ in the case of bounded dimension and of $\\tilde{O}(\\Delta^{3/2} L_2^{3/4}\\epsilon^{-9/4})$ in the case where we are allowed only a single Hessian query. We obtain these results through a more general algorithm which can handle approximate Hessian computations and recovers known prior state-of-the-art bounds of computing an $\\epsilon$-critical point, under the additional assumption that $f$ has an $L_1$-Lipschitz gradient, with $O(\\Delta L_2^{1/4}\\epsilon^{-7/4})$-gradient queries",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k5TbfYPYuc": {
    "title": "Generating Multi-Table Time Series EHR from Latent Space with Minimal Preprocessing",
    "volume": "poster",
    "abstract": "Electronic Health Records (EHR) are time-series relational databases that record patient interactions and medical events over time, serving as a critical resource for healthcare research and applications. However, privacy concerns and regulatory restrictions limit the sharing and utilization of such sensitive data, necessitating the generation of synthetic EHR datasets. Unlike previous EHR synthesis methods—which typically generate medical records consisting of expert-chosen features (e.g., a few vital signs, structured codes only)—we introduce RawMed, the first framework to synthesize multi-table, time-series EHR data that closely resembles raw EHRs. Using text-based representation and compression techniques, RawMed captures complex structures and temporal dynamics with minimal lossy preprocessing. We also propose a new evaluation framework for multi-table time-series synthetic EHRs, assessing distributional similarity, inter-table relationships, temporal dynamics, and privacy. Validated on two open-source EHR datasets, RawMed outperforms baseline models in fidelity and utility. The code is available at [https://github.com/eunbyeol-cho/RawMed](https://github.com/eunbyeol-cho/RawMed)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4lH9MOuGfy": {
    "title": "WaLRUS: Wavelets for Long range Representation Using State Space Methods",
    "volume": "poster",
    "abstract": "State-Space Models (SSMs) have proven to be powerful tools for online function approximation and for modeling long-range dependencies in sequential data. While recent methods such as HiPPO have demonstrated strong performance using a few polynomial bases, they remain limited by their reliance on closed-form solutions for specific, well-behaved bases. The SaFARi framework generalizes this approach, enabling the construction of SSMs from arbitrary frames, including non-orthogonal and redundant ones, thus allowing an infinite diversity of possible \"species'' within the SSM family. In this paper, we introduce WaLRUS (Wavelets for Long-range Representation Using SSMs), a new species of SaFARi built from Daubechies wavelet frames. We instantiate two variants, scaled-Walrus and translated-Walrus, and show that their multiresolution and localized nature offers significant advantages in representing non-smooth and transient signals. We compare Walrus to HiPPO-based models and demonstrate improved accuracy, better numerical properties, and more efficient implementations for online function approximation tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JPogehP8By": {
    "title": "Brain-Informed Fine-Tuning for Improved Multilingual Understanding in Language Models",
    "volume": "poster",
    "abstract": "Recent studies have demonstrated that fine-tuning language models with brain data can improve their semantic understanding, although these findings have so far been limited to English. Interestingly, similar to the shared multilingual embedding space of pretrained multilingual language models, human studies provide strong evidence for a shared semantic system in bilingual individuals. Here, we investigate whether fine-tuning language models with bilingual brain data changes model representations in a way that improves them across multiple languages. To test this, we fine-tune monolingual and multilingual language models using brain activity recorded while bilingual participants read stories in English and Chinese. We then evaluate how well these representations generalize to the bilingual participants' first language, their second language, and several other languages that the participants are not fluent in. We assess the fine-tuned language models on brain encoding performance and downstream NLP tasks. Our results show that bilingual brain-informed fine-tuned language models outperform their vanilla (pretrained) counterparts in both brain encoding performance and most downstream NLP tasks across multiple languages. These findings suggest that brain-informed fine-tuning improves multilingual understanding in language models, offering a bridge between cognitive neuroscience and NLP research. We make our code publicly available",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lO7RGax6u9": {
    "title": "Timely Clinical Diagnosis through Active Test Selection",
    "volume": "poster",
    "abstract": "There is growing interest in using machine learning (ML) to support clinical diagnosis, but most approaches rely on static, fully observed datasets and fail to reflect the sequential, resource-aware reasoning clinicians use in practice. Diagnosis remains complex and error prone, especially in high-pressure or resource-limited settings, underscoring the need for frameworks that help clinicians make timely and cost-effective decisions. We propose ACTMED (Adaptive Clinical Test selection via Model-based Experimental Design), a diagnostic framework that integrates Bayesian Experimental Design (BED) with large language models (LLMs) to better emulate real-world diagnostic reasoning. At each step, ACTMED selects the test expected to yield the greatest reduction in diagnostic uncertainty for a given patient. LLMs act as flexible simulators, generating plausible patient state distributions and supporting belief updates without requiring structured, task-specific training data. Clinicians can remain in the loop; reviewing test suggestions, interpreting intermediate outputs, and applying clinical judgment throughout. We evaluate ACTMED on real-world datasets and show it can optimize test selection to improve diagnostic accuracy, interpretability, and resource use. This represents a step toward transparent, adaptive, and clinician-aligned diagnostic systems that generalize across settings with reduced reliance on domain-specific data",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IuC5rIZsMS": {
    "title": "Learning-Augmented Online Bidding in Stochastic Settings",
    "volume": "poster",
    "abstract": "Online bidding is a classic optimization problem, with several applications in online decision-making, the design of interruptible systems, and the analysis of approximation algorithms. In this work, we study online bidding under learning-augmented settings that incorporate stochasticity, in either the prediction oracle or the algorithm itself. In the first part, we study bidding under distributional predictions, and find Pareto-optimal algorithms that offer the best-possible tradeoff between the consistency and the robustness of the algorithm. In the second part, we study the power and limitations of randomized bidding algorithms, by presenting upper and lower bounds on the consistency/robustness tradeoffs. Previous works focused predominantly on oracles that do not leverage stochastic information on the quality of the prediction, and deterministic algorithms",
    "checked": true,
    "id": "7654004263678271b4fd0acec4249444be6a07df",
    "semantic_title": "learning-augmented online bidding in stochastic settings",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6IuURCuooO": {
    "title": "Aligning Compound AI Systems via System-level DPO",
    "volume": "poster",
    "abstract": "Compound AI systems, comprising multiple interacting components such as LLMs, foundation models, and external tools, have demonstrated remarkable improvements compared to single models in various tasks. To ensure their effective deployment in real-world applications, aligning these systems with human preferences is crucial. However, aligning the compound system via policy optimization, unlike the alignment of a single model, is challenging for two main reasons: (i) non-differentiable interactions between components make end-to-end gradient-based optimization method inapplicable, and (ii) system-level preferences cannot be directly transformed into component-level preferences. To address these challenges, we first formulate compound AI systems as Directed Acyclic Graphs (DAGs), explicitly modeling both component interactions and the associated data flows. Building on this formulation, we introduce SysDPO, a framework that extends Direct Preference Optimization (DPO) to enable joint system-level alignment. We propose two variants, SysDPO-Direct and SysDPO-Sampling, tailored for scenarios depending on whether we construct a system-specific preference dataset. We empirically demonstrate the effectiveness of our approach across two applications: the joint alignment of a language model and a diffusion model, and the joint alignment of an LLM collaboration system",
    "checked": true,
    "id": "a2e08a2fce333a7c8961103fe022bb9b334030cf",
    "semantic_title": "aligning compound ai systems via system-level dpo",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=wszZlP1K14": {
    "title": "Learning Juntas under Markov Random Fields",
    "volume": "poster",
    "abstract": "We give an algorithm for learning $O(\\log n)$ juntas in polynomial-time with respect to Markov Random Fields (MRFs) in a smoothed analysis framework, where only the external field has been randomly perturbed. This is a broad generalization of the work of Kalai and Teng, who gave an algorithm that succeeded with respect to smoothed *product* distributions (i.e., MRFs whose dependency graph has no edges). Our algorithm has two phases: (1) an unsupervised structure learning phase and (2) a greedy supervised learning algorithm. This is the first example where algorithms for learning the structure of undirected graphical models have downstream applications to supervised learning",
    "checked": true,
    "id": "7b9ed9a61e00b8b7a214c983f12c08b4a76a02e9",
    "semantic_title": "learning juntas under markov random fields",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1wmP48quNb": {
    "title": "Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation",
    "volume": "poster",
    "abstract": "Large Language Models have demonstrated impressive fluency across diverse tasks, yet their tendency to produce toxic content remains a critical challenge for AI safety and public trust. Existing toxicity mitigation approaches primarily manipulate individual neuron activations, but these methods suffer from instability, context dependence, and often compromise the model's core language abilities. To address these shortcomings, we investigate three key questions: the stability of neuron-level toxicity indicators, the advantages of structural (layer-wise) representations, and the interpretability of mechanisms driving toxic generation. Through extensive experiments on Jigsaw and ToxiCN datasets, we show that aggregated layer-wise features provide more robust signals than single neurons. Moreover, we observe conceptual limitations in prior works that conflate toxicity detection experts and generation experts within neuron-based interventions. To mitigate this, we propose a novel principled intervention technique, EigenShift, based on eigen-decomposition of the language model's final output layer. This method selectively targets generation-aligned components, enabling precise toxicity suppression without impairing linguistic competence. Our method requires no additional training or fine-tuning, incurs minimal computational cost, and is grounded in rigorous theoretical analysis",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OZljvntsto": {
    "title": "Grids Often Outperform Implicit Neural Representation at Compressing Dense Signals",
    "volume": "poster",
    "abstract": "Implicit Neural Representations (INRs) have recently shown impressive results, but their fundamental capacity, implicit biases, and scaling behavior remain poorly understood. We investigate the performance of diverse INRs across a suite of 2D and 3D real and synthetic signals with varying effective bandwidth, as well as both overfitting and generalization tasks including tomography, super-resolution, and denoising. By stratifying performance according to model size as well as signal type and bandwidth, our results shed light on how different INR and grid representations allocate their capacity. We find that, for most tasks and signals, a simple regularized grid with interpolation trains faster and to higher quality than any INR with the same number of parameters. We also find limited settings–namely fitting binary signals such as shape contours–where INRs outperform grids, to guide future development and use of INRs towards the most advantageous applications",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o0HgWRmyY1": {
    "title": "GraSS: Scalable Data Attribution with Gradient Sparsification and Sparse Projection",
    "volume": "poster",
    "abstract": "Gradient-based data attribution methods, such as influence functions, are critical for understanding the impact of individual training samples without requiring repeated model retraining. However, their scalability is often limited by the high computational and memory costs associated with per-sample gradient computation. In this work, we propose **GraSS**, a novel gradient compression algorithm and its variants **FactGraSS** for linear layers specifically, that explicitly leverage the inherent sparsity of per-sample gradients to achieve sub-linear space and time complexity. Extensive experiments demonstrate the effectiveness of our approach, achieving substantial speedups while preserving data influence fidelity. In particular, **FactGraSS** achieves up to 165% faster throughput on billion-scale models compared to the previous state-of-the-art baselines. Our code is publicly available at https://github.com/TRAIS-Lab/GraSS",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kSgZiAAwDU": {
    "title": "Stochastic Momentum Methods for Non-smooth Non-Convex Finite-Sum Coupled Compositional Optimization",
    "volume": "poster",
    "abstract": "Finite-sum Coupled Compositional Optimization (FCCO), characterized by its coupled compositional objective structure, emerges as an important optimization paradigm for addressing a wide range of machine learning problems. In this paper, we focus on a challenging class of non-convex non-smooth FCCO, where the outer functions are non-smooth weakly convex or convex and the inner functions are smooth or weakly convex. Existing state-of-the-art result face two key limitations: (1) a high iteration complexity of $O(1/\\epsilon^6)$ under the assumption that the stochastic inner functions are Lipschitz continuous in expectation; (2) reliance on vanilla SGD-type updates, which are not suitable for deep learning applications. Our main contributions are two fold: (i) We propose stochastic momentum methods tailored for non-smooth FCCO that come with provable convergence guarantees; (ii) We establish a **new state-of-the-art** iteration complexity of $O(1/\\epsilon^5)$. Moreover, we apply our algorithms to multiple inequality constrained non-convex optimization problems involving smooth or weakly convex functional inequality constraints. By optimizing a smoothed hinge penalty based formulation, we achieve a **new state-of-the-art** complexity of $O(1/\\epsilon^5)$ for finding an (nearly) $\\epsilon$-level KKT solution. Experiments on three tasks demonstrate the effectiveness of the proposed algorithms",
    "checked": true,
    "id": "82f1d5008147efb12da7001d98e0948bcabb3467",
    "semantic_title": "stochastic momentum methods for non-smooth non-convex finite-sum coupled compositional optimization",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JCTTLKEBza": {
    "title": "PolarQuant: Leveraging Polar Transformation for Key Cache Quantization and Decoding Acceleration",
    "volume": "poster",
    "abstract": "The increasing demand for long-context generation has made the KV cache in large language models a bottleneck in memory consumption. Quantizing the cache to lower bit widths is an effective way to reduce memory costs; however, previous methods struggle with key cache quantization due to outliers, resulting in suboptimal performance. We propose a novel quantization approach PolarQuant, which provides a new perspective for key cache quantization and efficiently addresses the outlier dilemma. We observe that the distribution of the key states reveals well-structured patterns under polar transformation. Outliers generally appear in only one of the two dimensions, which are rotated together by a specific angle when rotary position embeddings are applied. When represented as two-dimensional vectors, these dimensions exhibit well-organized patterns, with radii and angles smoothly distributed in polar space. This alleviates the channel-wise outliers, making them well-suited for key cache quantization. PolarQuant divides key vectors into groups of two-dimensional sub-vectors, encoding them as the quantized radius and the polar angle, rather than quantizing original key vectors directly. PolarQuant achieves the superior efficiency in KV cache quantization and accelerates the decoding process by turning the query-key inner product into a table lookup, all while maintaining the downstream performance of full-precision models. Our code is available at https://github.com/ericshwu/PolarQuant",
    "checked": false,
    "id": "d13fefced76173da6c8c3eb25790906b33aba9b5",
    "semantic_title": "polarquant: leveraging polar transformation for efficient key cache quantization and decoding acceleration",
    "citation_count": 2,
    "authors": []
  },
  "https://openreview.net/forum?id=FzfYoUp8F1": {
    "title": "Learning World Models for Interactive Video Generation",
    "volume": "poster",
    "abstract": "Foundational world models must be both interactive and preserve spatialtemporal coherence to enable effective future planning with different action choices. However, present models for long video generation have limited inherent world modeling capabilities due to two main challenges: compounding errors and insufficient memory mechanisms. We enhance image-to-video models with interactive capabilities through additional action conditioning and autoregressive framework, and reveal that compounding error is inherently irreducible in autoregressive video generation, while insufficient memory mechanism leads to incoherence of world models. We propose video retrieval augmented generation (VRAG) with explicit global state conditioning, which significantly reduces long-term compounding errors and increases spatialtemporal consistency of video world models. In contrast, naive autoregressive generation with extended context windows and retrieval-augmented generation prove less effective for video generation, primarily due to the limited in-context learning capabilities of current video models. Our work illuminates the fundamental challenges in video world models and establishes a comprehensive benchmark for improving video generation models with internal world modeling capabilities",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z1wIUZtBmK": {
    "title": "COALA: Numerically Stable and Efficient Framework for Context-Aware Low-Rank Approximation",
    "volume": "poster",
    "abstract": "Recent studies suggest that context-aware low-rank approximation is a useful tool for compression and fine-tuning of modern large-scale neural networks. In this type of approximation, a norm is weighted by a matrix of input activations, significantly improving metrics over the unweighted case. Nevertheless, existing methods for neural networks suffer from numerical instabilities due to their reliance on classical formulas involving explicit Gram matrix computation and their subsequent inversion. We demonstrate that this can degrade the approximation quality or cause numerically singular matrices. To address these limitations, we propose a novel _inversion-free regularized framework_ that is based entirely on stable decompositions and overcomes the numerical pitfalls of prior art. Our method can handle all possible challenging scenarios: (1)&nbsp;when calibration matrices exceed GPU memory capacity, (2)&nbsp;when input activation matrices are nearly singular, and even (3)&nbsp;when insufficient data prevents unique approximation. For the latter, we prove that our solution converges to a desired approximation and derive explicit error bounds",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WDdBhcwzGe": {
    "title": "Multi-Token Prediction Needs Registers",
    "volume": "poster",
    "abstract": "Multi-token prediction has emerged as a promising objective for improving language model pretraining, but its benefits have not consistently generalized to other settings such as fine-tuning. In this paper, we propose MuToR, a simple and effective approach to multi-token prediction that interleaves learnable register tokens into the input sequence, each tasked with predicting future targets. Compared to existing methods, MuToR offers several key advantages: it introduces only a negligible number of additional parameters, requires no architectural changes—ensuring compatibility with off-the-shelf pretrained language models—and remains aligned with the next-token pretraining objective, making it especially well-suited for supervised fine-tuning. Moreover, it naturally supports scalable prediction horizons. We demonstrate the effectiveness and versatility of MuToR across a range of use cases, including supervised fine-tuning, parameter-efficient fine-tuning (PEFT), and pretraining, on challenging generative tasks in both language and vision domains",
    "checked": true,
    "id": "bc8f65e37416af8bbbf87d6af88b1267d4c7321f",
    "semantic_title": "multi-token prediction needs registers",
    "citation_count": 3,
    "authors": []
  },
  "https://openreview.net/forum?id=Qm6ah1hpFA": {
    "title": "Brain-Like Processing Pathways Form in Models With Heterogeneous Experts",
    "volume": "poster",
    "abstract": "The brain is made up of a vast set of heterogeneous regions that dynamically organize into pathways as a function of task demands. Examples of such pathways can be found in the interactions between cortical and subcortical networks during learning, or in sub-networks specializing for task characteristics such as difficulty or modality. Despite the large role these pathways play in cognition, the mechanisms through which brain regions organize into pathways remain unclear. In this work, we use an extension of the Heterogeneous Mixture-of-Experts architecture to show that heterogeneous regions do not form processing pathways by themselves, implying that the brain likely implements specific constraints which result in the reliable formation of pathways. We identify three biologically relevant inductive biases that encourage pathway formation: a routing cost imposed on the use of more complex regions, a scaling factor that reduces this cost when task performance is low, and randomized expert dropout. When comparing our resulting Mixture-of-Pathways model with the brain, we observe that the artificial pathways in our model match how the brain uses cortical and subcortical systems to learn and solve tasks of varying difficulty. In summary, we introduce a novel framework for investigating how the brain forms task-specific pathways through inductive biases, and the effects these biases have on the behavior of Mixture-of-Experts models",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mH9FJi3KTX": {
    "title": "Magical: Medical Lay Language Generation via Semantic Invariance and Layperson-tailored Adaptation",
    "volume": "poster",
    "abstract": "Medical Lay Language Generation (MLLG) plays a vital role in improving the accessibility of complex scientific content for broader audiences. Recent literature to MLLG commonly employ parameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA) to fine-tuning large language models (LLMs) using paired expert-lay language datasets. However, LoRA struggles with the challenges posed by multi-source heterogeneous MLLG datasets. Specifically, through a series of exploratory experiments, we reveal that standard LoRA fail to meet the requirement for semantic fidelity and diverse lay-style generation in MLLG task. To address these limitations, we propose Magical, an asymmetric LoRA architecture tailored for MLLG under heterogeneous data scenarios. Magical employs a shared matrix A for abstractive summarization, along with multiple isolated matrices B for diverse lay-style generation. To preserve semantic fidelity during the lay language generation process, Magical introduces a Semantic Invariance Constraint to mitigate semantic subspace shifts on matrix A. Furthermore, to better adapt to diverse lay-style generation, Magical incorporates the Recommendation-guided Switch, an externally interface to prompt the LLM to switch between different matrices B. Experimental results on three real-world lay language generation datasets demonstrate that Magical consistently outperforms prompt-based methods, vanilla LoRA, and its recent variants, while also reducing trainable parameters by 31.66%. Our code is publicly available at https://github.com/tianlwang/Magical.git",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7lLnWh0otf": {
    "title": "Dynamic Semantic-Aware Correlation Modeling for UAV Tracking",
    "volume": "poster",
    "abstract": "UAV tracking can be widely applied in scenarios such as disaster rescue, environmental monitoring, and logistics transportation. However, existing UAV tracking methods predominantly emphasize speed and lack exploration in semantic awareness, which hinders the search region from extracting accurate localization information from the template. The limitation results in suboptimal performance under typical UAV tracking challenges such as camera motion, fast motion, and low resolution, etc. To address this issue, we propose a dynamic semantic aware correlation modeling tracking framework. The core of our framework is a Dynamic Semantic Relevance Generator, which, in combination with the correlation map from the Transformer, explore semantic relevance. The approach enhances the search region's ability to extract important information from the template, improving accuracy and robustness under the aforementioned challenges. Additionally, to enhance the tracking speed, we design a pruning method for the proposed framework. Therefore, we present multiple model variants that achieve trade-offs between speed and accuracy, enabling flexible deployment according to the available computational resources. Experimental results validate the effectiveness of our method, achieving competitive performance on multiple UAV tracking datasets",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aoVCFtox89": {
    "title": "Marginal-Nonuniform PAC Learnability",
    "volume": "poster",
    "abstract": "We revisit the classical model of nonuniform PAC learning, introduced by Benedek and Itai [1994], where generalization guarantees may depend on the target concept (but not on the marginal distribution). In this work, we propose and study a complementary variant, which we call *marginal-nonuniform learning*. In this setting, guarantees may depend on the marginal distribution over the domain, but must hold uniformly over all concepts. This captures the intuition that some data distributions are inherently easier to learn from than others, allowing for a flexible, distribution-sensitive view of learnability. Our main result is a complete characterization of the achievable learning rates in this model, revealing a trichotomy: exponential rates of the form $e^{-n}$ arise precisely when the hypothesis class is finite; linear rates of the form $d/n$ are achievable when a recently introduced combinatorial parameter, the VC-eluder dimension $d$, is finite; and arbitrarily slow rates may occur when $d = \\infty$. Additionally, in the original (concept-)nonuniform model, we show that for all learnable classes linear rates are achievable. We conclude by situating marginal-nonuniform learning within the landscape of universal learning, and by discussing its relationship to other distribution-dependent learning paradigms",
    "checked": false,
    "id": "6f99fe4d147a89b2c7a60563868eae79b3aff267",
    "semantic_title": "inherent limitations of dimensions for characterizing learnability of distribution classes",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=iXjVis9Cwn": {
    "title": "Dynamic Configuration for Cutting Plane Separators via Reinforcement Learning on Incremental Graph",
    "volume": "poster",
    "abstract": "Cutting planes (cuts) are essential for solving mixed-integer linear programming (MILP) problems, as they tighten the feasible solution space and accelerate the solving process. Modern MILP solvers offer diverse cutting plane separators to generate cuts, enabling users to leverage their potential complementary strengths to tackle problems with different structures. Recent machine learning approaches learn to configure separators based on problem-specific features, selecting effective separators and deactivating ineffective ones to save unnecessary computing time. However, they ignore the dynamics of separator efficacy at different stages of cut generation and struggle to adapt the configurations for the evolving problems after multiple rounds of cut generation. To address this challenge, we propose a novel **dyn**amic **sep**arator configuration (**DynSep**) method that models separator configuration in different rounds as a reinforcement learning task, making decisions based on an incremental triplet graph updated by iteratively added cuts. Specifically, we tokenize the incremental subgraphs and utilize a decoder-only Transformer as our policy to autoregressively predict when to halt separation and which separators to activate at each round. Evaluated on synthetic and large-scale real-world MILP problems, DynSep speeds up average solving time by 64% on easy and medium datasets, and reduces primal-dual gap integral within the given time limit by 16% on hard datasets. Moreover, experiments demonstrate that DynSep well generalizes to MILP instances of significantly larger sizes than those seen during training",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hmepi1Fm2g": {
    "title": "zip2zip: Inference-Time Adaptive Tokenization via Online Compression",
    "volume": "poster",
    "abstract": "Tokenization efficiency plays a critical role in the performance and cost of large language models (LLMs), yet most models rely on static tokenizers optimized on general-purpose corpora. These tokenizers' fixed vocabularies often fail to adapt to domain- or language-specific inputs, leading to longer token sequences and higher computational costs. We introduce zip2zip, a novel method for achieving context-adaptive tokenization in LLMs at inference time. Leveraging an online data compression algorithm (Lempel–Ziv–Welch), zip2zip dynamically expands its active vocabulary at inference time by continuously replacing fragmented token sequences with more compact hypertokens, which it can immediately output during generation. In doing so, the model refines its internal tokenization scheme to match the token distribution of the current context, reducing redundancy and improving representational efficiency. zip2zip consists of three key components: (1) a tokenizer based on Lempel–Ziv–Welch compression that incrementally merges co-occurring tokens into reusable hypertokens on the fly; (2) a dynamic embedding (and unembedding) layer that computes embeddings for newly formed hypertokens at runtime; and (3) a variant of autoregressive language modeling that pretrains the model to handle hypertokenized, compressed text sequences as inputs and outputs. We show that an existing LLM can be uptrained for zip2zip in 10 GPU-hours via parameter-efficient finetuning. The resulting LLM performs test-time adaptation, learning to use hypertokens in unseen contexts and reducing input and output tokens by 15–40%. Code and models are released at https://github.com/epfl-dlab/zip2zip",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QupYhLEVNj": {
    "title": "ProDAG: Projected Variational Inference for Directed Acyclic Graphs",
    "volume": "poster",
    "abstract": "Directed acyclic graph (DAG) learning is a central task in structure discovery and causal inference. Although the field has witnessed remarkable advances over the past few years, it remains statistically and computationally challenging to learn a single (point estimate) DAG from data, let alone provide uncertainty quantification. We address the difficult task of quantifying graph uncertainty by developing a Bayesian variational inference framework based on novel, provably valid distributions that have support directly on the space of sparse DAGs. These distributions, which we use to define our prior and variational posterior, are induced by a projection operation that maps an arbitrary continuous distribution onto the space of sparse weighted acyclic adjacency matrices. While this projection is combinatorial, it can be solved efficiently using recent continuous reformulations of acyclicity constraints. We empirically demonstrate that our method, \\texttt{ProDAG}, can outperform state-of-the-art alternatives in both accuracy and uncertainty quantification",
    "checked": true,
    "id": "054f368444ba63e6a7190610c37c2eeb6a03bddb",
    "semantic_title": "prodag: projected variational inference for directed acyclic graphs",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wyv81ezGgv": {
    "title": "VividFace: A Robost and High-Fidelity Video Face Swapping Framework",
    "volume": "poster",
    "abstract": "Video face swapping has seen increasing adoption in diverse applications, yet existing methods primarily trained on static images struggle to address temporal consistency and complex real-world scenarios. To overcome these limitations, we propose the first video face swapping framework, VividFace, a robust and high-fidelity diffusion-based framework. VividFace employs a novel hybrid training strategy that leverages abundant static image data alongside temporal video sequences, enabling it to effectively model temporal coherence and identity consistency in videos. Central to our approach is a carefully designed diffusion model integrated with a specialized VAE, capable of processing image-video hybrid data efficiently. To further enhance identity and pose disentanglement, we introduce and release the Attribute-Identity Disentanglement Triplet (AIDT) dataset, comprising a large-scale collection of triplets where each set contains three face images—two sharing the same pose and two sharing the same identity. Augmented comprehensively with occlusion scenarios, AIDT significantly boosts the robustness of VividFace against occlusions. Moreover, we incorporate advanced 3D reconstruction techniques as conditioning inputs to address significant pose variations effectively. Extensive experiments demonstrate that VividFace achieves state-of-the-art performance in identity preservation, temporal consistency, and visual realism, surpassing existing methods while requiring fewer inference steps. Our framework notably mitigates common challenges such as temporal flickering, identity loss, and sensitivity to occlusions and pose variations. The AIDT dataset, source code, and pre-trained weights will be released to support future research. The code and pretrained weights are available on the [project page](https://hao-shao.com/projects/vividface.html)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KBJSV1XApq": {
    "title": "Ascent Fails to Forget",
    "volume": "poster",
    "abstract": "Contrary to common belief, we show that gradient ascent-based unconstrained optimization methods frequently fail to perform machine unlearning, a phenomenon we attribute to the inherent statistical dependence between the forget and retain data sets. This dependence, which can manifest itself even as simple correlations, undermines the misconception that these sets can be independently manipulated during unlearning. We provide empirical and theoretical evidence showing these methods often fail precisely due to this overlooked relationship. For random forget sets, this dependence means that degrading forget set metrics (which, for a retrained model, should mirror test set metrics) inevitably harms overall test performance. Going beyond random sets, we consider logistic regression as an instructive example where a critical failure mode emerges: inter-set dependence causes gradient descent-ascent iterations to progressively diverge from the ideal retrained model. Strikingly, these methods can converge to solutions that are not only far from the retrained ideal but are potentially even further from it than the original model itself, rendering the unlearning process actively detrimental. A toy example further illustrates how this dependence can trap models in inferior local minima, inescapable via finetuning. Our findings highlight that the presence of such statistical dependencies, even when manifest only as correlations, can be sufficient for ascent-based unlearning to fail. Our theoretical insights are corroborated by experiments on complex neural networks, demonstrating that these methods do not perform as expected in practice due to this unaddressed statistical interplay",
    "checked": true,
    "id": "016788a80dfb5b94bca43b3dd61564d4f42995f9",
    "semantic_title": "ascent fails to forget",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c7leN0pIQv": {
    "title": "Unifying and Enhancing Graph Transformers via a Hierarchical Mask Framework",
    "volume": "poster",
    "abstract": "Graph Transformers (GTs) have emerged as a powerful paradigm for graph representation learning due to their ability to model diverse node interactions. However, existing GTs often rely on intricate architectural designs tailored to specific interactions, limiting their flexibly. To address this, we propose a unified hierarchical mask framework that reveals an underlying equivalence between model architecture and attention mask construction. This framework enables a consistent modeling paradigm by capturing diverse interactions through carefully designed attention masks. Theoretical analysis under this framework demonstrates that the probability of correct classification positively correlates with the receptive field size and label consistency, leading to a fundamental design principle: An effective attention mask should ensure both a sufficiently large receptive field and a high level of label consistency. While no single existing mask satisfies this principle across all scenarios, our analysis reveals that hierarchical masks offer complementary strengths—motivating their effective integration. Then, we introduce M$^3$Dphormer, a Mixture-of-Experts based Graph Transformer with Multi-Level Masking and Dual Attention Computation. M$^3$Dphormer incorporates three theoretically grounded hierarchical masks and employs a bi-level expert routing mechanism to adaptively integrate multi-level interaction information. To ensure scalability, we further introduce a dual attention computation scheme that dynamically switches between dense and sparse modes based on local mask sparsity. Extensive experiments across multiple benchmarks demonstrate that M$^3$Dphormer achieves state-of-the-art performance, validating the effectiveness of our unified framework and model design",
    "checked": true,
    "id": "ac3eefb23f79cea57cae742528b17d52e97d6476",
    "semantic_title": "unifying and enhancing graph transformers via a hierarchical mask framework",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oRMfTkP6kC": {
    "title": "MOBO-OSD: Batch Multi-Objective Bayesian Optimization via Orthogonal Search Directions",
    "volume": "poster",
    "abstract": "Bayesian Optimization (BO) is a powerful tool for optimizing expensive black-box objective functions. While extensive research has been conducted on the single-objective optimization problem, the multi-objective optimization problem remains challenging. In this paper, we propose MOBO-OSD, a multi-objective Bayesian Optimization algorithm designed to generate a diverse set of Pareto optimal solutions by solving multiple constrained optimization problems, referred to as MOBO-OSD subproblems, along orthogonal search directions (OSDs) defined with respect to an approximated convex hull of individual objective minima. By employing a well-distributed set of OSDs, MOBO-OSD ensures broad coverage of the objective space, enhancing both solution diversity and hypervolume performance. To further improve the density of the set of the Pareto optimal candidate solutions without requiring an excessive number of subproblems, we leverage a Pareto Front Estimation technique to generate additional solutions in the neighborhood of existing solutions. Additionally, MOBO-OSD supports batch optimization, enabling parallel function evaluations to accelerate the optimization process when resources are available. Through extensive experiments and analysis on a variety of synthetic and real-world benchmark functions with two to six objectives, we demonstrate that MOBO-OSD consistently outperform the state-of-the-art algorithms",
    "checked": true,
    "id": "d6ba4475a263418033d270e80ff431747388f38b",
    "semantic_title": "mobo-osd: batch multi-objective bayesian optimization via orthogonal search directions",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KIlw9nWydt": {
    "title": "WHAT MAKES MATH PROBLEMS HARD FOR REINFORCEMENT LEARNING: A CASE STUDY",
    "volume": "poster",
    "abstract": "Using a long-standing conjecture from combinatorial group theory, we explore, from multiple perspectives, the challenges of finding rare instances carrying disproportionately high rewards. Based on lessons learned in the context defined by the Andrews--Curtis conjecture, we analyze how reinforcement learning agents handle problems of varying hardness. We also address many mathematical questions as a part of our study. Notably, we demonstrate the length reducibility of all but two presentations in the Akbulut--Kirby series (1981), and resolve various potential counterexamples in the Miller--Schupp series (1991), including three infinite subfamilies",
    "checked": true,
    "id": "6dd4f7609e064350bf7f83c01f169bb911778d49",
    "semantic_title": "what makes math problems hard for reinforcement learning: a case study",
    "citation_count": 6,
    "authors": []
  },
  "https://openreview.net/forum?id=4qE0rnS93Z": {
    "title": "Preference Optimization on Pareto Sets: On a Theory of Multi-Objective Optimization",
    "volume": "poster",
    "abstract": "In multi-objective optimization, a single decision vector must balance the trade-offs across many objectives. Pareto-optimal solutions are those achieving optimal trade-offs, where improving any objective comes at a cost to another. As many different decisions can be Pareto optimal, this raises the question of which solution to pick and how. We formulate this problem as one of optimizing a preference function over the set of Pareto-optimal solutions, or Pareto-constrained optimization for short. It poses significant challenges: not only is the constraint set defined implicitly, but it is also generally non-convex and non-smooth, even when the objectives are strongly convex. We propose an equivalent formulation of the problem where the constraint set is the simplex, leading to clearer notions of optimality and stationarity that improve upon existing definitions in literature. We give an algorithm with a last-iterate convergence rate of $O(K^{-1/2})$ to stationarity when the preference function is Lipschitz smooth and when the objective functions are strongly convex and Lipschitz smooth. Motivated by applications like Reinforcement Learning with Human Feedback (RLHF), we also extend this algorithm to the case where access to the preference function is only available through dueling feedback",
    "checked": false,
    "id": "037bd68d4c290031a931884c49a15676a82e3a3c",
    "semantic_title": "optimization on pareto sets: on a theory of multi-objective optimization",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=8Vy1x9IO0Z": {
    "title": "Contextual Dynamic Pricing with Heterogeneous Buyers",
    "volume": "poster",
    "abstract": "We initiate the study of contextual dynamic pricing with a heterogeneous population of buyers, where a seller repeatedly posts prices (over $T$ rounds) that depend on the observable $d$-dimensional context and receives binary purchase feedback. Unlike prior work assuming homogeneous buyer types, in our setting the buyer's valuation type is drawn from an unknown distribution with finite support size $K_{\\star}$. We develop a contextual pricing algorithm based on optimistic posterior sampling with regret $\\widetilde{O}(K_{\\star}\\sqrt{dT})$, which we prove to be tight in $d$ and $T$ up to logarithmic terms. Finally, we refine our analysis for the non-contextual pricing case, proposing a variance-aware zooming algorithm that achieves the optimal dependence on $K_{\\star}$",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wZPAnkQ5q5": {
    "title": "Computational Hardness of Reinforcement Learning with Partial q π -Realizability",
    "volume": "poster",
    "abstract": "This paper investigates the computational complexity of reinforcement learning within a novel linear function approximation regime, termed partial $q^{\\pi}$-realizability. In this framework, the objective is to learn an $\\epsilon$-optimal policy with respect to a predefined policy set $\\Pi$, under the assumption that all value functions corresponding to policies in $\\Pi$ are linearly realizable. This framework adopts assumptions that are weaker than those in the $q^{\\pi}$-realizability setting yet stronger than those in the q*-realizability setup. As a result, it provides a more practical model for reinforcement learning scenarios where function approximation naturally arise. We prove that learning an $\\epsilon$-optimal policy in this newly defined setting is computationally hard. More specifically, we establish NP-hardness under a parameterized greedy policy set (i.e., argmax) and, further, show that—unless NP = RP—an exponential lower bound (exponential in feature vector dimension) holds when the policy set contains softmax policies, under the Randomized Exponential Time Hypothesis. Our hardness results mirror those obtained in the $q^*$-realizability settings, and suggest that computational difficulty persists even when the policy class $ \\Pi $ is expanded beyond the optimal policy, reinforcing the unbreakable nature of the computational hardness result regarding partial $ q^{\\pi} $-realizability under two important policy sets. To establish our negative result, our primary technical contribution is a reduction from two complexity problems, $\\delta$-Max-3SAT and $\\delta$-Max-3SAT($b$), to instances of our problem settings: GLinear-$\\kappa$-RL (under the greedy policy set) and SLinear-$\\kappa$-RL (under the softmax policy set), respectively. Our findings indicate that positive computational results are generally unattainable in the context of partial $ q^{\\pi} $-realizability, in sharp contrast to the $ q^{\\pi} $-realizability setting under a generative access model",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nadVPpcMvn": {
    "title": "On Local Limits of Sparse Random Graphs: Color Convergence and the Refined Configuration Model",
    "volume": "poster",
    "abstract": "Local convergence has emerged as a fundamental tool for analyzing sparse random graph models. We introduce a new notion of local convergence, _color convergence_, based on the Weisfeiler–Leman algorithm. Color convergence fully characterizes the class of random graphs that are well-behaved in the limit for message-passing graph neural networks. Building on this, we propose the _Refined Configuration Model_ (RCM), a random graph model that generalizes the configuration model. The RCM is universal with respect to local convergence among locally tree-like random graph models, including Erdős–Rényi, stochastic block and configuration models. Finally, this framework enables a complete characterization of the random trees that arise as local limits of such graphs",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zDU5sfYK1Z": {
    "title": "Curriculum Design for Trajectory-Constrained Agent: Compressing Chain-of-Thought Tokens in LLMs",
    "volume": "poster",
    "abstract": "Training agents to operate under strict constraints during deployment, such as limited resource budgets or stringent safety requirements, presents significant challenges, especially when these constraints render the task complex. In this work, we propose a curriculum learning strategy that gradually tightens constraints during training, enabling the agent to incrementally master the deployment requirements. Inspired by self-paced learning techniques in unconstrained reinforcement learning (RL), our approach facilitates a smoother transition to challenging environments by initially training on simplified versions of the constraints and progressively introducing the full deployment conditions. We provide a theoretical analysis using an RL agent in a binary-tree Markov Decision Process (MDP) to demonstrate that our curriculum strategy can accelerate training relative to a baseline approach that imposes the trajectory constraints from the outset. Moreover, we empirically validate the effectiveness and generality of our method across both RL and large language model (LLM) agents in diverse settings, including a binary-tree MDP, a multi-task navigation domain, and a math reasoning task with two benchmarks. These results highlight the potential of curriculum design in enhancing the efficiency and performance of agents operating under complex trajectory constraints during deployment. Moreover, when applied to LLMs, our strategy enables compression of output chain-of-thought tokens, achieving a substantial inference speedup on consumer hardware, demonstrating its effectiveness for resource-constrained deployment",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UdOEZgWJLc": {
    "title": "Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought",
    "volume": "poster",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in many applications, including challenging reasoning problems via chain-of-thought (CoT) techniques that generate ``thinking tokens'' before answering the questions. While existing theoretical works demonstrate that CoT with discrete tokens boosts the capability of LLMs, recent work on continuous CoT lacks a theoretical understanding of why it outperforms discrete counterparts in various reasoning tasks, such as directed graph reachability, a fundamental graph reasoning problem that includes many practical domain applications as special cases. In this paper, we prove that a two-layer transformer with $D$ steps of continuous CoT can solve the directed graph reachability problem, where $D$ is the diameter of the graph, while the best known result of constant-depth transformers with discrete CoT requires $O(n^2)$ decoding steps where $n$ is the number of vertices ($D<n$). In our construction, each continuous thought vector is a superposition state that encodes multiple search frontiers simultaneously (i.e., parallel breadth-first search (BFS)), while discrete CoT must choose a single path sampled from the superposition state, which leads to a sequential search that requires many more steps and may be trapped in local solutions. We also performed extensive experiments to verify that our theoretical construction aligns well with the empirical solution obtained via training dynamics. Notably, encoding of multiple search frontiers as a superposition state automatically emerges in training continuous CoT, without explicit supervision to guide the model to explore multiple paths simultaneously",
    "checked": true,
    "id": "71282c8446fcb8e184d0007182cb4fad90da6587",
    "semantic_title": "reasoning by superposition: a theoretical perspective on chain of continuous thought",
    "citation_count": 18,
    "authors": []
  },
  "https://openreview.net/forum?id=Pa5pKAeAO7": {
    "title": "Ditch the Denoiser: Emergence of Noise Robustness in Self-Supervised Learning from Data Curriculum",
    "volume": "poster",
    "abstract": "Self-Supervised Learning (SSL) has become a powerful solution to extract rich representations from unlabeled data. Yet, SSL research is mostly focused on clean, curated and high-quality datasets. As a result, applying SSL on noisy data remains a challenge, despite being crucial to applications such as astrophysics, medical imaging, geophysics or finance. In this work, we present a fully self-supervised framework that enables noise-robust representation learning without requiring a denoiser at inference or downstream fine-tuning. Our method first trains an SSL denoiser on noisy data, then uses it to construct a denoised-to-noisy data curriculum (i.e., training first on denoised, then noisy samples) for pretraining a SSL backbone (e.g., DINOv2), combined with a teacher-guided regularization that anchors noisy embeddings to their denoised counterparts. This process encourages the model to internalize noise robustness. Notably, the denoiser can be discarded after pretraining, simplifying deployment. On ImageNet-1k with ViT-B under extreme Gaussian noise ($\\sigma=255$, SNR = 0.72 dB), our method improves linear probing accuracy by 4.8\\% over DINOv2, demonstrating that denoiser-free robustness can emerge from noise-aware pretraining. The code is available at https://github.com/wenquanlu/noisy_dinov2",
    "checked": true,
    "id": "7f6d29587b34eb398d284373972a0a3fd4439e65",
    "semantic_title": "ditch the denoiser: emergence of noise robustness in self-supervised learning from data curriculum",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=2jwAjomEDB": {
    "title": "R-KV: Redundancy-aware KV Cache Compression for Reasoning Models",
    "volume": "poster",
    "abstract": "Reasoning models have demonstrated impressive performance in self-reflection and chain-of-thought reasoning. However, they often produce excessively long outputs, leading to prohibitively large key-value (KV) caches during inference. While chain-of-thought inference significantly improves performance on complex reasoning tasks, it can also lead to reasoning failures when deployed with existing KV cache compression approaches. To address this, we propose Redundancy-aware KV Cache Compression for Reasoning models (R-KV), a novel method specifically targeting redundant tokens in reasoning models. Our method preserves nearly 100% of the full KV cache performance using only 10% of the KV cache, substantially outperforming existing KV cache baselines, which reach only 60% of the performance. Remarkably, R-KV even achieves 105% of full KV cache performance with 38% of the KV cache. This KV-cache reduction also leads to a 50% memory saving and a 2x speedup over standard chain-of-thought reasoning inference. Experimental results show that R-KV consistently outperforms existing KV cache compression baselines across two mathematical reasoning datasets",
    "checked": true,
    "id": "48d73af1a5820c5c4fa56a7dc310ee6de7421a3f",
    "semantic_title": "r-kv: redundancy-aware kv cache compression for reasoning models",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=KYTFXxTJ12": {
    "title": "Towards Fully FP8 GEMM LLM Training at Scale",
    "volume": "poster",
    "abstract": "Despite the significant potential of FP8 data formats for large language model (LLM) pre-training, their adoption has been limited due to challenges in maintaining stability at scale. Existing approaches often rely on suboptimal fine-grained FP8 kernels or fall back to higher-precision matrix multiplications (GEMMs) in sensitive components, such as attention projections, compromising potential throughput gains. We introduce a new class of LLM architectures that, for the first time, support FP8 computation for all GEMMs within transformer blocks during both forward and backward passes. This enables unprecedented throughput gains, particularly at scale, while matching the downstream performance of standard BF16 training. Our architecture design reduces large outlier activations, promoting stable long-term FP8 training. Additionally, we identify key metrics for monitoring low-precision training and predicting potential future divergences",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=boNYskaXnO": {
    "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache",
    "volume": "poster",
    "abstract": "Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce $\\textbf{NSNQuant}$, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation—$\\textbf{1)}$ a token-wise normalization ($\\textbf{N}$ormalize), $\\textbf{2)}$ a channel-wise centering ($\\textbf{S}$hift), and $\\textbf{3)}$ a second token-wise normalization ($\\textbf{N}$ormalize)—with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\\times$ throughput gains over full-precision baselines",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1XLjrmKZ4p": {
    "title": "GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents",
    "volume": "poster",
    "abstract": "Recent Graphical User Interface (GUI) agents replicate the R1-Zero paradigm, coupling online Reinforcement Learning (RL) with explicit chain-of-thought reasoning prior to object grounding and thereby achieving substantial performance gains. In this paper, we first conduct extensive analysis experiments of three key components of that training pipeline: input design, output evaluation, and policy update—each revealing distinct challenges arising from blindly applying general-purpose RL without adapting to GUI grounding tasks. Input design: Current templates encourage the model to generate chain-of-thought reasoning, but longer chains unexpectedly lead to worse grounding performance. Output evaluation: Reward functions based on hit signals or box area allow models to exploit box size, leading to reward hacking and poor localization quality. Policy update: Online RL tends to overfit easy examples due to biases in length and sample difficulty, leading to under-optimization on harder cases. To address these issues, we propose three targeted solutions. First, we adopt a $\\textbf{Fast Thinking Template}$ that encourages direct answer generation, reducing excessive reasoning during training. Second, we incorporate a box size constraint into the reward function to mitigate reward hacking. Third, we revise the RL objective by adjusting length normalization and adding a difficulty-aware scaling factor, enabling better optimization on hard samples. Our $\\textbf{GUI-G1-3B}$, trained on 17K public samples with Qwen2.5-VL-3B-Instruct, achieves $\\textbf{90.3\\%}$ accuracy on ScreenSpot and $\\textbf{37.1\\%}$ on ScreenSpot-Pro. This surpasses all prior models of similar size and even outperforms the larger UI-TARS-7B, establishing a new state-of-the-art in GUI agent grounding",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y5Diyh9XEQ": {
    "title": "Asymptotic theory of SGD with a general learning-rate",
    "volume": "poster",
    "abstract": "Stochastic gradient descent (SGD) with polynomially decaying step‐sizes has long underpinned theoretical analyses, yielding a broad spectrum of statistically attractive guarantees. Yet in practice, such schedules find rare use due to their prohibitively slow convergence, revealing a persistent gap between theory and empirical performance. In this paper, we introduce a unified framework that quantifies the uncertainty of online SGD under arbitrary learning‐rate choices. In particular, we provide the first comprehensive convergence characterizations for two widely used but theoretically under-examined schemes—cyclical learning rates and linear decay to zero. Our results not only explain the observed behavior of these schedules but also facilitate principled tools for statistical inference and algorithm design. All theoretical findings are corroborated by extensive simulations across diverse settings",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pFFFRi2TcC": {
    "title": "Sample complexity of data-driven tuning of model hyperparameters in neural networks with structured parameter-dependent dual function",
    "volume": "poster",
    "abstract": "Modern machine learning algorithms, especially deep learning-based techniques, typically involve careful hyperparameter tuning to achieve the best performance. Despite the surge of intense interest in practical techniques like Bayesian optimization and random search-based approaches to automating this laborious and compute-intensive task, the fundamental learning-theoretic complexity of tuning hyperparameters for deep neural networks is poorly understood. Inspired by this glaring gap, we initiate the formal study of hyperparameter tuning complexity in deep learning through a recently introduced data-driven setting. We assume that we have a series of learning tasks, and we have to tune hyperparameters to do well on average over the distribution of tasks. A major difficulty is that the utility function as a function of the hyperparameter is very volatile, and furthermore, it is given implicitly by an optimization problem over the model parameters. To tackle this challenge, we introduce a new technique to characterize the discontinuities and oscillations of the utility function on any fixed problem instance as we vary the hyperparameter; our analysis relies on subtle concepts, including tools from algebraic geometry, differential geometry, and constrained optimization. We use this to show that the learning-theoretic complexity of the corresponding family of utility functions is bounded. We instantiate our results and provide sample complexity bounds for concrete applications—tuning a hyperparameter that interpolates neural activation functions and setting the kernel parameter in graph neural networks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MrRdupucYb": {
    "title": "Q ♯ : Provably Optimal Distributional RL for LLM Post-Training",
    "volume": "poster",
    "abstract": "Reinforcement learning (RL) post-training is crucial for LLM alignment and reasoning, but existing policy-based methods, such as PPO and DPO, can fall short of fixing shortcuts inherited from pre-training. In this work, we introduce $Q\\sharp$, a value-based algorithm for KL-regularized RL that guides the reference policy using the optimal regularized $Q$ function. We propose to learn the optimal $Q$ function using distributional RL on an aggregated online dataset. Unlike prior value-based baselines that guide the model using unregularized $Q$-values, our method is theoretically principled and provably learns the optimal policy for the KL-regularized RL problem. Empirically, $Q\\sharp$ outperforms prior baselines in math reasoning benchmarks while maintaining a smaller KL divergence to the reference policy. Theoretically, we establish a reduction from KL-regularized RL to no-regret online learning, providing the first bounds for deterministic MDPs under only realizability. Thanks to distributional RL, our bounds are also variance-dependent and converge faster when the reference policy has small variance. In sum, our results highlight $Q\\sharp$ as an effective approach for post-training LLMs, offering both improved performance and theoretical guarantees. The code can be found at \\url{https://github.com/jinpz/q_sharp}",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SkdhLeuq8P": {
    "title": "Ask a Strong LLM Judge when Your Reward Model is Uncertain",
    "volume": "poster",
    "abstract": "Reward model (RM) plays a pivotal role in reinforcement learning with human feedback (RLHF) for aligning large language models (LLMs). However, classical RMs trained on human preferences are vulnerable to reward hacking and generalize poorly to out-of-distribution (OOD) inputs. By contrast, strong LLM judges equipped with reasoning capabilities demonstrate superior generalization, even without additional training, but incur significantly higher inference costs, limiting their applicability in online RLHF. In this work, we propose an uncertainty-based routing framework that efficiently complements a fast RM with a strong but costly LLM judge. Our approach formulates advantage estimation in policy gradient (PG) methods as pairwise preference classification, enabling principled uncertainty quantification to guide routing. Uncertain pairs are forwarded to the LLM judge, while confident ones are evaluated by the RM. Experiments on RM benchmarks demonstrate that our uncertainty-based routing strategy significantly outperforms random judge calling at the same cost, and downstream alignment results showcase its effectiveness in improving online RLHF",
    "checked": true,
    "id": "e43f9e5479ff63a7445c6c133f9fbeac45167ebd",
    "semantic_title": "ask a strong llm judge when your reward model is uncertain",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=88IP02bmrg": {
    "title": "Near-Optimal Regret-Queue Length Tradeoff in Online Learning for Two-Sided Markets",
    "volume": "poster",
    "abstract": "We study a two-sided market, wherein, price-sensitive heterogeneous customers and servers arrive and join their respective queues. A compatible customer-server pair can then be matched by the platform, at which point, they leave the system. Our objective is to design pricing and matching algorithms that maximize the platform's profit, while maintaining reasonable queue lengths. As the demand and supply curves governing the price-dependent arrival rates may not be known in practice, we design a novel online-learning-based pricing policy and establish its near-optimality. In particular, we prove a tradeoff among three performance metrics: $\\tilde{O}(T^{1-\\gamma})$ regret, $\\tilde{O}(T^{\\gamma/2})$ average queue length, and $\\tilde{O}(T^{\\gamma})$ maximum queue length for $\\gamma \\in (0, 1/6]$, significantly improving over existing results (Yang & Ying, 2024). Moreover, barring the permissible range of $\\gamma$, we show that this trade-off between regret and average queue length is optimal up to logarithmic factors under a class of policies, matching the optimal one as in (Varma et al., 2023) which assumes the demand and supply curves to be known. Our proposed policy has two noteworthy features: a dynamic component that optimizes the tradeoff between low regret and small queue lengths; and a probabilistic component that resolves the tension between obtaining useful samples for fast learning and maintaining small queue lengths",
    "checked": true,
    "id": "9a572f63e3c7c8cc4d88d2dd390813fbd8246059",
    "semantic_title": "near-optimal regret-queue length tradeoff in online learning for two-sided markets",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LamTzqRHvL": {
    "title": "Planning with Quantized Opponent Models",
    "volume": "poster",
    "abstract": "Planning under opponent uncertainty is a fundamental challenge in multi-agent environments, where an agent must act while inferring the hidden policies of its opponents. Existing type-based methods rely on manually defined behavior classes and struggle to scale, while model-free approaches are sample-inefficient and lack a principled way to incorporate uncertainty into planning. We propose Quantized Opponent Models (QOM), which learn a compact catalog of opponent types via a quantized autoencoder and maintain a Bayesian belief over these types online. This posterior supports both a belief-weighted meta-policy and a Monte-Carlo planning algorithm that directly integrates uncertainty, enabling real-time belief updates and focused exploration. Experiments show that QOM achieves superior performance with lower search cost, offering a tractable and effective solution for belief-aware planning",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7bEPq5MAQi": {
    "title": "Learning Counterfactual Outcomes Under Rank Preservation",
    "volume": "poster",
    "abstract": "Counterfactual inference aims to estimate the counterfactual outcome at the individual level given knowledge of an observed treatment and the factual outcome, with broad applications in fields such as epidemiology, econometrics, and management science. Previous methods rely on a known structural causal model (SCM) or assume the homogeneity of the exogenous variable and strict monotonicity between the outcome and exogenous variable. In this paper, we propose a principled approach for identifying and estimating the counterfactual outcome. We first introduce a simple and intuitive rank preservation assumption to identify the counterfactual outcome without relying on a known structural causal model. Building on this, we propose a novel ideal loss for theoretically unbiased learning of the counterfactual outcome and further develop a kernel-based estimator for its empirical estimation. Our theoretical analysis shows that the rank preservation assumption is not stronger than the homogeneity and strict monotonicity assumptions, and shows that the proposed ideal loss is convex, and the proposed estimator is unbiased. Extensive semi-synthetic and real-world experiments are conducted to demonstrate the effectiveness of the proposed method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ql3sENn0mi": {
    "title": "Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards",
    "volume": "poster",
    "abstract": "Reinforcement learning (RL) is increasingly used to align large language models (LLMs). Off-policy methods offer greater implementation simplicity and data efficiency than on-policy techniques, but often result in suboptimal performance. In this work, we study the intermediate range of algorithms between off-policy RL and supervised fine-tuning by analyzing a simple off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with $r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$ emphasizes high-reward samples, while raising it penalizes low-reward ones more heavily. We first provide a theoretical analysis of this off-policy REINFORCE algorithm, showing that when the baseline $V$ lower-bounds the expected reward, the algorithm enjoys a policy improvement guarantee. Our analysis reveals that while on-policy updates can safely leverage both positive and negative signals, off-policy updates benefit from focusing more on positive rewards than on negative ones. We validate our findings experimentally in a controlled stochastic bandit setting and through fine-tuning state-of-the-art LLMs on reasoning tasks",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rxdon7jWni": {
    "title": "MALinZero: Efficient Low-Dimensional Search for Mastering Complex Multi-Agent Planning",
    "volume": "poster",
    "abstract": "Monte Carlo Tree Search (MCTS), which leverages Upper Confidence Bound for Trees (UCTs) to balance exploration and exploitation through randomized sampling, is instrumental to solving complex planning problems. However, for multi-agent planning, MCTS is confronted with a large combinatorial action space that often grows exponentially with the number of agents. As a result, the branching factor of MCTS during tree expansion also increases exponentially, making it very difficult to efficiently explore and exploit during tree search. To this end, we propose MALinZero, a new approach to leverage low-dimensional representational structures on joint-action returns and enable efficient MCTS in complex multi-agent planning. Our solution can be viewed as projecting the joint-action returns into the low-dimensional space representable using a contextual linear bandit problem formulation. We solve the contextual linear bandit problem with convex and $\\mu$-smooth loss functions -- in order to place more importance on better joint actions and mitigate potential representational limitations -- and derive a linear Upper Confidence Bound applied to trees (LinUCT) to enable novel multi-agent exploration and exploitation in the low-dimensional space. We analyze the regret of MALinZero for low-dimensional reward functions and propose an $(1-\\tfrac1e)$-approximation algorithm for the joint action selection by maximizing a sub-modular objective. MALinZero demonstrates state-of-the-art performance on multi-agent benchmarks such as matrix games, SMAC, and SMACv2, outperforming both model-based and model-free multi-agent reinforcement learning baselines with faster learning speed and better performance",
    "checked": true,
    "id": "475df79e1e140bcde323c19b4a9a460b81584c81",
    "semantic_title": "malinzero: efficient low-dimensional search for mastering complex multi-agent planning",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9vKJyCUfMH": {
    "title": "Tensor Decomposition Networks for Accelerating Machine Learning Force Field Computations",
    "volume": "poster",
    "abstract": "SO(3)-equivariant networks are the dominant models for machine learning interatomic potentials (MLIPs). The key operation of such networks is the Clebsch-Gordan (CG) tensor product, which is computationally expensive. To accelerate the computation, we develop tensor decomposition networks (TDNs) as a class of approximately equivariant networks whose CG tensor products are replaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP) decomposition. With the CP decomposition, we prove (i) a uniform bound on the induced error of SO(3)-equivariance, and (ii) the universality of approximating any equivariant bilinear map. To further reduce the number of parameters, we propose path-weight sharing that ties all multiplicity-space weights across the O(L^3) CG paths into a single path without compromising equivariance, where L is the maximum angular degree. The resulting layer acts as a plug-and-play replacement for tensor products in existing networks, and the computational complexity of tensor products is reduced from O(L^6) to O(L^4). We evaluate TDNs on PubChemQCR, a newly curated molecular relaxation dataset containing 105 million DFT-calculated snapshots. We also use existing datasets, including OC20, and OC22. Results show that TDNs achieve competitive performance with dramatic speedup in computations. Our code is publicly available as part of the AIRS library (https://github.com/divelab/AIRS/tree/main/OpenMol/TDN)",
    "checked": false,
    "id": "f8e93e20fa573baf305887910aa18cb22e573a5d",
    "semantic_title": "enhancing unmanned aerial vehicle object detection via tensor decompositions and positive–negative momentum optimizers",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=BpwIPxSSdb": {
    "title": "Evaluating multiple models using labeled and unlabeled data",
    "volume": "poster",
    "abstract": "It is difficult to evaluate machine learning classifiers without large labeled datasets, which are often unavailable. In contrast, unlabeled data is plentiful, but not easily used for evaluation. Here, we introduce Semi-Supervised Model Evaluation (SSME), a method that uses both labeled and unlabeled data to evaluate machine learning classifiers. The key idea is to estimate the joint distribution of ground truth labels and classifier scores using a semi-supervised mixture model. The semi-supervised mixture model allows SSME to learn from three sources of information: unlabeled data, multiple classifiers, and probabilistic classifier scores. Once fit, the mixture model enables estimation of any metric that is a function of classifier scores and ground truth labels (e.g., accuracy or AUC). We derive theoretical bounds on the error of these estimates, showing that estimation error decreases with the number of classifiers and the amount of unlabeled data. We present experiments in four domains where obtaining large labeled datasets is often impractical: healthcare, content moderation, molecular property prediction, and text classification. Our results demonstrate that SSME estimates performance more accurately than do competing methods, reducing error by 5.1x relative to using labeled data alone and 2.4x relative to the next best method",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lwn1rLB8t7": {
    "title": "Rethinking Neural Combinatorial Optimization for Vehicle Routing Problems with Different Constraint Tightness Degrees",
    "volume": "poster",
    "abstract": "Recent neural combinatorial optimization (NCO) methods have shown promising problem-solving ability without requiring domain-specific expertise. Most existing NCO methods use training and testing data with a fixed constraint value and lack research on the effect of constraint tightness on the performance of NCO methods. This paper takes the capacity-constrained vehicle routing problem (CVRP) as an example to empirically analyze the NCO performance under different tightness degrees of the capacity constraint. Our analysis reveals that existing NCO methods overfit the capacity constraint, and they can only perform satisfactorily on a small range of the constraint values but poorly on other values. To tackle this drawback of existing NCO methods, we develop an efficient training scheme that explicitly considers varying degrees of constraint tightness and propose a multi-expert module to learn a generally adaptable solving strategy. Experimental results show that the proposed method can effectively overcome the overfitting issue, demonstrating superior performance on the CVRP and CVRP with time windows (CVRPTW) with various constraint tightness degrees. The code is available at [https://github.com/CIAM-Group/Rethinking\\_Constraint\\_Tightness](https://github.com/CIAM-Group/Rethinking\\_Constraint\\_Tightness)",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ia1nRDN00L": {
    "title": "Estimation of Treatment Effects in Extreme and Unobserved Data",
    "volume": "poster",
    "abstract": "Causal effect estimation seeks to determine the impact of an intervention from observational data. However, the existing causal inference literature primarily addresses treatment effects on frequently occurring events. But what if we are interested in estimating the effects of a policy intervention whose benefits, while potentially important, can only be observed and measured in rare yet impactful events, such as extreme climate events? The standard causal inference methodology is not designed for this type of inference since the events of interest may be scarce in the observed data and some degree of extrapolation is necessary. Extreme Value Theory (EVT) provides methodologies for analyzing statistical phenomena in such extreme regimes. We introduce a novel framework for assessing treatment effects in extreme data to capture the causal effect at the occurrence of rare events of interest. In particular, we employ the theory of multivariate regular variation to model extremities. We develop a consistent estimator for extreme treatment effects and present a rigorous non-asymptotic analysis of its performance. We illustrate the performance of our estimator using both synthetic and semi-synthetic data",
    "checked": true,
    "id": "08766c5279a134b1c852f3245f40051a7487d279",
    "semantic_title": "estimation of treatment effects in extreme and unobserved data",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XEGDKcoQQ1": {
    "title": "Imagine Beyond ! Distributionally Robust Autoencoding for State Space Coverage in Online Reinforcement Learning",
    "volume": "poster",
    "abstract": "Goal-Conditioned Reinforcement Learning (GCRL) enables agents to autonomously acquire diverse behaviors, but faces major challenges in visual environments due to high-dimensional, semantically sparse observations. In the online setting, where agents learn representations while exploring, the latent space evolves with the agent's policy, to capture newly discovered areas of the environment. However, without incentivization to maximize state coverage in the representation, classical approaches based on auto-encoders may converge to latent spaces that over-represent a restricted set of states frequently visited by the agent. This is exacerbated in an intrinsic motivation setting, where the agent uses the distribution encoded in the latent space to sample the goals it learns to master. To address this issue, we propose to progressively enforce distributional shifts towards a uniform distribution over the full state space, to ensure a full coverage of skills that can be learned in the environment. We introduce DRAG (Distributionally Robust Auto-Encoding for GCRL), a method that combines the $\\beta$-VAE framework with Distributionally Robust Optimization (DRO). DRAG leverage an adversarial neural weighter of training states of the VAE, to account for the mismatch between the current data distribution and unseen parts of the environment. This allows the agent to construct semantically meaningful latent spaces beyond its immediate experience. Our approach improves state space coverage and downstream control performance on hard exploration environments such as mazes and robotic control involving walls to bypass, without relying on pre-training nor prior environment knowledge",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t94tALZvZE": {
    "title": "MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs",
    "volume": "poster",
    "abstract": "Language models deployed in real-world systems often require post-hoc updates to incorporate new or corrected knowledge. However, editing such models efficiently and reliably—without retraining or forgetting previous information—remains a major challenge. Existing methods for lifelong model editing either compromise generalization, interfere with past edits, or fail to scale to long editing sequences. We propose MEMOIR, a novel scalable framework that injects knowledge through a residual memory, i.e., a dedicated parameter module, while preserving the core capabilities of the pre-trained model. By sparsifying input activations through data-dependent masks, MEMOIR confines each edit to a distinct subset of the memory parameters, minimizing interference among edits. At inference, it identifies relevant edits by comparing the sparse activation patterns of new queries to those stored during editing. This enables generalization to rephrased queries by activating only the relevant knowledge while suppressing unnecessary memory activation for unrelated prompts. Experiments on question answering, hallucination correction, and out-of-distribution generalization benchmarks across LLaMA-3 and Mistral demonstrate that MEMOIR achieves state-of-the-art performance across reliability, generalization, and locality metrics, scaling to thousands of sequential edits with minimal forgetting",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZtB34bQI54": {
    "title": "Constrained Entropic Unlearning: A Primal-Dual Framework for Large Language Models",
    "volume": "poster",
    "abstract": "Large Language Models (LLMs) deployed in real-world settings increasingly face the need to unlearn sensitive, outdated, or proprietary information. Existing unlearning methods typically formulate forgetting and retention as a regularized trade-off, combining both objectives into a single scalarized loss. This often leads to unstable optimization and degraded performance on retained data, especially under aggressive forgetting. We propose a new formulation of LLM unlearning as a constrained optimization problem: forgetting is enforced via a novel logit-margin flattening loss that explicitly drives the output distribution toward uniformity on a designated forget set, while retention is preserved through a hard constraint on a separate retain set. Compared to entropy-based objectives, our loss is softmax-free, numerically stable, and maintains non-vanishing gradients, enabling more efficient and robust optimization. We solve the constrained problem using a scalable primal-dual algorithm that exposes the trade-off between forgetting and retention through the dynamics of the dual variable, all without any extra computational overhead. Evaluations on the TOFU and MUSE benchmarks across diverse LLM architectures demonstrate that our approach consistently matches or exceeds state-of-the-art baselines, effectively removing targeted information while preserving downstream utility",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ehfIVCHxkX": {
    "title": "Collaborative and Confidential Junction Trees for Hybrid Bayesian Networks",
    "volume": "poster",
    "abstract": "Bayesian Network models are a powerful tool to collaboratively optimize production processes in various manufacturing industries. When interacting, collaborating parties must preserve their business secrets by maintaining the confidentiality of their model structures and parameters. While most realistic industry scenarios involve hybrid settings, handling both discrete and continuous data, current state-of-the-art methods for collaborative and confidential inference only support discrete data and have high communication costs. In a centralized setting, Junction Trees enable efficient inference even in hybrid scenarios without discretizing continuous variables, but no extension for collaborative and confidential scenarios exists. To address this research gap, we introduce Hybrid CCJT, the first framework for confidential multiparty inference in hybrid domains with semi-honest, non-colluding adversaries, comprising: (i) a method to construct a strongly-rooted Junction Tree across collaborating parties through a novel construct of interface cliques; and, (ii) a protocol for confidential inference built upon multiparty computation primitives comprising a one-time alignment phase and a belief propagation system for combining the inference results across the Junction Tree cliques. Extensive evaluation on nine datasets shows that Hybrid CCJT improves the predictive accuracy of continuous target variables by 32% on average compared to the state-of-the-art, while reducing communication costs by a median 10.4x under purely discrete scenarios",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PlcH6HJku4": {
    "title": "Counterfactual Implicit Feedback Modeling",
    "volume": "poster",
    "abstract": "In recommendation systems, implicit feedback data can be automatically recorded and is more common than explicit feedback data. However, implicit feedback poses two challenges for relevance prediction, namely (a) positive-unlabeled (PU): negative feedback does not necessarily imply low relevance and (b) missing not at random (MNAR): items that are popular or frequently recommended tend to receive more clicks than other items, even if the user does not have a significant interest in them. Existing methods either overlook the MNAR issue or fail to account for the inherent mechanism of the PU issue. As a result, they may lead to inaccurate relevance predictions or inflated biases and variances. In this paper, we formulate the implicit feedback problem as a counterfactual estimation problem with missing treatment variables. Prediction of the relevance in implicit feedback is equivalent to answering the counterfactual question that ``whether a user would click a specific item if exposed to it?\". To solve the counterfactual question, we propose the Counterfactual Implicit Feedback (Counter-IF) prediction approach that divides the user-item pairs into four disjoint groups, namely definitely positive (DP), highly exposed (HE), highly unexposed (HU), and unknown (UN) groups. Specifically, Counter-IF first performs missing treatment imputation with different confidence levels from raw implicit feedback, then estimates the counterfactual outcomes via causal representation learning that combines pointwise loss and pairwise loss based on the user-item pairs stratification. Theoretically the generalization bound of the learned model is derived. Extensive experiments are conducted on publicly available datasets to demonstrate the effectiveness of our approach. The code is available at https://github.com/zhouchuanCN/NeurIPS25-Counter-IF",
    "checked": false,
    "id": "2dffb901382c6055a4d6bc9d07f4e9f6ae0e520e",
    "semantic_title": "leveraging implicit feedback from deployment data in dialogue",
    "citation_count": 11,
    "authors": []
  },
  "https://openreview.net/forum?id=Ep4mYI7OLF": {
    "title": "A Private Approximation of the 2nd-Moment Matrix of Any Subsamplable Input",
    "volume": "poster",
    "abstract": "We study the problem of differentially private second moment estimation and present a new algorithm that achieve strong privacy-utility trade-offs even for worst-case inputs under subsamplability assumptions on the data. We call an input $(m,\\alpha,\\beta)$-subsamplable if a random subsample of size $m$ (or larger) preserves w.p $\\geq 1-\\beta$ the spectral structure of the original second moment matrix up to a multiplicative factor of $1\\pm \\alpha$. Building upon subsamplability, we give a recursive algorithmic framework similar to Kamath et al (2019) that abides zero-Concentrated Differential Privacy (zCDP) while preserving w.h.p the accuracy of the second moment estimation upto an arbitrary factor of $(1\\pm\\gamma)$. We then show how to apply our algorithm to approximate the second moment matrix of a distribution $\\mathcal{D}$, even when a noticeable fraction of the input are outliers",
    "checked": true,
    "id": "44690b38d10d50662d2535a9d59c05de5e9059af",
    "semantic_title": "a private approximation of the 2nd-moment matrix of any subsamplable input",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7LulWI9QJy": {
    "title": "GauSAM: Contour‑Guided 2D Gaussian Fields for Multi‑Scale Medical Image Segmentation with Segment Anything",
    "volume": "poster",
    "abstract": "Effective multiscale medical image segmentation requires simultaneously preserving smooth spatial continuity and accurately delineating high-frequency boundaries, yet pixel-wise decoders often fail to maintain this balance consistently across varying resolutions. We introduce GauSAM, which seamlessly integrates contour‑guided 2D Gaussian probability fields into the Segment Anything Model to address these challenges. In our framework, segmentation masks are parameterized as continuous probability fields of learnable 2D Gaussian primitives, enforcing spatially smooth and structurally consistent. Contourlet transforms extract rich multidirectional frequency information, notably edges and fine textures, which dynamically guide the spatial distribution of Gaussian primitives to substantially improve boundary fidelity in complex structures. The incorporation of these high-frequency contour priors also enriches the expressive capacity of the SAM image encoder. Extensive experiments on diverse 2D medical segmentation tasks confirm that GauSAM consistently delivers robust generalization and state-of-the-art performance with only 1.2M trainable parameters. The official implementation of GauSAM is publicly available at https://github.com/Quinten-Wu504/GauSAM",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fnq0M3KEUO": {
    "title": "A Counterfactual Semantics for Hybrid Dynamical Systems",
    "volume": "poster",
    "abstract": "Models of hybrid dynamical systems are widely used to answer questions about the causes and effects of dynamic events in time. Unfortunately, existing causal reasoning formalisms lack support for queries involving the dynamically triggered, discontinuous interventions that characterize hybrid dynamical systems. This mismatch can lead to ad-hoc and error-prone causal analysis workflows in practice. To bridge the gap between the needs of hybrid systems users and current causal inference capabilities, we develop a rigorous counterfactual semantics by formalizing interventions as transformations to the constraints of hybrid systems. Unlike interventions in a typical structural causal model, however, interventions in hybrid systems can easily render the model ill-posed. Thus, we identify mild conditions under which our interventions maintain solution existence, uniqueness, and measurability by making explicit connections to established hybrid systems theory. To illustrate the utility of our framework, we formalize a number of canonical causal estimands and explore a case study on the probabilities of causation with applications to fishery management. Our work simultaneously expands the modeling possibilities available to causal inference practitioners and begins to unlock decades of causality research for users of hybrid systems",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bct6UQcsiA": {
    "title": "LOPT: Learning Optimal Pigovian Tax in Sequential Social Dilemmas",
    "volume": "poster",
    "abstract": "Multi-agent reinforcement learning (MARL) has emerged as a powerful framework for modeling autonomous agents that independently optimize their individual objectives. However, in mixed-motive MARL environments, rational self-interested behaviors often lead to collectively suboptimal outcomes situations commonly referred to as social dilemmas. A key challenge in addressing social dilemmas lies in accurately quantifying and representing them in a numerical form that captures how self-interested agent behaviors impact social welfare. To address this challenge, \\textit{externalities} in the economic concept is adopted and extended to denote the unaccounted-for impact of one agent's actions on others, as a means to rigorously quantify social dilemmas. Based on this measurement, a novel method, \\textbf{L}earning \\textbf{O}ptimal \\textbf{P}igovian \\textbf{T}ax (\\textbf{LOPT}) is proposed. Inspired by Pigovian taxes, which are designed to internalize externalities by imposing cost on negative societal impacts, LOPT employs an auxiliary tax agent that learns an optimal Pigovian tax policy to reshape individual rewards aligned with social welfare, thereby promoting agent coordination and mitigating social dilemmas. We support LOPT with theoretical analysis and validate it on standard MARL benchmarks, including Escape Room and Cleanup. Results show that by effectively internalizing externalities that quantify social dilemmas, LOPT aligns individual objectives with collective goals, significantly improving social welfare over state-of-the-art baselines",
    "checked": false,
    "id": "4bbc2bb1d73cfde8a04ffe763374e5601da4b4e3",
    "semantic_title": "learning optimal \"pigovian tax\" in sequential social dilemmas",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=KMbl8lg5Rv": {
    "title": "Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models",
    "volume": "poster",
    "abstract": "Large language models (LLMs) have transformed natural language processing, but their reliable deployment requires effective uncertainty quantification (UQ). Existing UQ methods are often heuristic and lack a fully probabilistic foundation. This paper begins by providing a theoretical justification for the role of perturbations in UQ for LLMs. We then introduce a dual random walk perspective, modeling input–output pairs as two Markov chains with transition probabilities defined by semantic similarity. Building on this, we propose a fully probabilistic framework based on an inverse model, which quantifies uncertainty by evaluating the diversity of the input space conditioned on a given output through systematic perturbations. Within this framework, we define a new uncertainty measure, Inv-Entropy. A key strength of our framework is its flexibility: it supports various definitions of uncertainty measures, embeddings, perturbation strategies, and similarity metrics. We also propose GAAP, a perturbation algorithm based on genetic algorithms, which enhances the diversity of sampled inputs. In addition, we introduce a new evaluation metric, Temperature Sensitivity of Uncertainty (TSU), which directly assesses uncertainty without relying on correctness as a proxy. Extensive experiments demonstrate that Inv-Entropy outperforms existing semantic UQ methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XDtwXau0BX": {
    "title": "MIX: A Multi-view Time-Frequency Interactive Explanation Framework for Time Series Classification",
    "volume": "poster",
    "abstract": "Deep learning models for time series classification (TSC) have achieved impressive performance, but explaining their decisions remains a significant challenge. Existing post-hoc explanation methods typically operate solely in the time domain and from a single-view perspective, limiting both faithfulness and robustness. In this work, we propose MIX (Multi-view Time-Frequency Interactive EXplanation Framework), a novel framework that helps to explain deep learning models in a multi-view setting by leveraging multi-resolution, time-frequency views constructed using the Haar Discrete Wavelet Transform (DWT). MIX introduces an interactive cross-view refinement scheme, where explanation's information from one view is propagated across views to enhance overall interpretability. To align with user-preferred perspectives, we propose a greedy selection strategy that traverses the multi-view space to identify the most informative features. Additionally, we present OSIGV, a user-aligned segment-level attribution mechanism based on overlapping windows for each view, and introduce keystone-first IG, a method that refines explanations in each view using additional information from another view. Extensive experiments across multiple TSC benchmarks and model architectures demonstrate that MIX significantly outperforms state-of-the-art (SOTA) methods in terms of explanation faithfulness and robustness",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=enhFXzKii4": {
    "title": "Layer as Puzzle Pieces: Compressing Large Language Models through Layer Concatenation",
    "volume": "poster",
    "abstract": "Large Language Models (LLMs) excel at natural language processing tasks, but their massive size leads to high computational and storage demands. Recent works have sought to reduce their model size through layer-wise structured pruning. However, they tend to ignore retaining the capabilities in the pruned part. In this work, we re-examine structured pruning paradigms and uncover several key limitations: 1) notable performance degradation due to direct layer removal, 2) incompetent linear weighted layer aggregation, and 3) the lack of effective post-training recovery mechanisms. To address these limitations, we propose CoMe, including a progressive layer pruning framework with a Concatenation-based Merging technology and a hierarchical distillation post-training process. Specifically, we introduce a channel sensitivity metric that utilizes activation intensity and weight norms for fine-grained channel selection. Subsequently, we employ a concatenation-based layer merging method to fuse the most critical channels in the adjacent layers, enabling a progressive model size reduction. Finally, we propose a hierarchical distillation protocol, which leverages the correspondences between the original and pruned model layers established during pruning, enabling efficient knowledge transfer. Experiments on seven benchmarks show that CoMe achieves state-of-the-art performance; when pruning 30% of LLaMA-2-7b's parameters, the pruned model retains 83% of its original average accuracy",
    "checked": true,
    "id": "f1f197a33558d5b410d738bf4e5d932c98d9a628",
    "semantic_title": "layer as puzzle pieces: compressing large language models through layer concatenation",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eLFoTtgCNP": {
    "title": "Accelerated Distance-adaptive Methods for Hölder Smooth and Convex Optimization",
    "volume": "poster",
    "abstract": "This paper introduces new parameter-free first-order methods for convex optimization problems in which the objective function exhibits Hölder smoothness. Inspired by the recently proposed distance-over-gradient (DOG) technique, we propose an accelerated distance-adaptive method which achieves optimal anytime convergence rates for Hölder smooth problems without requiring prior knowledge of smoothness parameters or explicit parameter tuning. Importantly, our parameter-free approach removes the necessity of specifying target accuracy in advance, addressing a significant limitation found in the universal fast gradient methods(Nesterov,2015). We further present a parameter-free accelerated method that eliminates the need for line-search procedures and extend it to convex stochastic optimization. Preliminary experimental results highlight the effectiveness of our approach in convex nonsmooth problems and its advantages over existing parameter-free or accelerated methods",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BmEH70Wjcu": {
    "title": "Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation and Confidence Awareness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UfQAFbP6xq": {
    "title": "Think-RM: Enabling Long-Horizon Reasoning in Generative Reward Models",
    "volume": "poster",
    "abstract": "",
    "checked": true,
    "id": "abc473899b15276c087c8ecf64e649169a1d1382",
    "semantic_title": "think-rm: enabling long-horizon reasoning in generative reward models",
    "citation_count": 4,
    "authors": []
  },
  "https://openreview.net/forum?id=KOwhczyFpg": {
    "title": "TRoVe: Discovering Error-Inducing Static Feature Biases in Temporal Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1BOiVpBtZy": {
    "title": "HCRMP: An LLM-Hinted Contextual Reinforcement Learning Framework for Autonomous Driving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": "210c44d337547f3d9e1a8baf6b5ca4c6a21f4dd4",
    "semantic_title": "hcrmp: a llm-hinted contextual reinforcement learning framework for autonomous driving",
    "citation_count": 1,
    "authors": []
  },
  "https://openreview.net/forum?id=J4w4RtwLyB": {
    "title": "Graph-KV: Breaking Sequence via Injecting Structural Biases into Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0T8i3uXq3O": {
    "title": "A Stable Whitening Optimizer for Efficient Neural Network Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uFTLo48OHF": {
    "title": "Social World Model-Augmented Mechanism Design Policy Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a9HOg4f9Gh": {
    "title": "Density Ratio-Free Doubly Robust Proxy Causal Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VV1HfpDCF1": {
    "title": "On the Emergence of Linear Analogies in Word Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ghybX0Qlls": {
    "title": "LocDiff: Identifying Locations on Earth by Diffusing in the Hilbert Space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HThoCI90b4": {
    "title": "Principled Long-Tailed Generative Modeling via Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iehVIoSDzu": {
    "title": "Efficient Algorithms for Robust and Partial Semi-Discrete Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tp6ds3Dfqo": {
    "title": "Rope to Nope and Back Again: A New Hybrid Attention Strategy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0GDlX9JFf2": {
    "title": "Doubly-Robust Estimation of Counterfactual Policy Mean Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nzwjvpCO4F": {
    "title": "InstructFlow: Adaptive Symbolic Constraint-Guided Code Generation for Long-Horizon Planning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e0Dn7dg395": {
    "title": "A Practical Guide for Incorporating Symmetry in Diffusion Policy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jmnt0F21K7": {
    "title": "SING: SDE Inference via Natural Gradients",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xQfZprbSWL": {
    "title": "Spectral Compressive Imaging via Chromaticity-Intensity Decomposition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bYRSuZteeK": {
    "title": "Quantifying Cross-Modality Memorization in Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kW2u5szHb6": {
    "title": "WaveAR: Wavelet-Aware Continuous Autoregressive Diffusion for Accurate Human Motion Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WO7rfbEGRU": {
    "title": "Composition and Alignment of Diffusion Models using Constrained Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Ook5bXnPr": {
    "title": "TrajAgent: An LLM-Agent Framework for Trajectory Modeling via Large-and-Small Model Collaboration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FYbe7r0mxu": {
    "title": "Neural Mutual Information Estimation with Vector Copulas",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RldcKC2ybn": {
    "title": "Kernel-based Equalized Odds: A Quantification of Accuracy-Fairness Trade-off in Fair Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ktNJgpmjjP": {
    "title": "AutoJudge: Judge Decoding Without Manual Annotation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z1Cvcovlms": {
    "title": "Sim-LLM: Optimizing LLM Inference at the Edge through Inter-Task KV Reuse",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5TWdcO9h4O": {
    "title": "Memory by accident: a theory of learning as a byproduct of network stabilization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9KIvyH3L7c": {
    "title": "Improving Reward Models with Proximal Policy Exploration for Preference-Based Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gzYuvZg28E": {
    "title": "Consistent Sampling and Simulation: Molecular Dynamics with Energy-Based Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QU1SArYwKB": {
    "title": "Optical Coherence Tomography Harmonization with Anatomy-Guided Latent Metric Schrödinger Bridges",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v34WBRPSon": {
    "title": "Size-adaptive Hypothesis Testing for Fairness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3FBByWp6GL": {
    "title": "Learning to Specialize: Joint Gating-Expert Training for Adaptive MoEs in Decentralized Settings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kLNtfA47dL": {
    "title": "Plug-and-Play Context Feature Reuse for Efficient Masked Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3VvdoCcVPU": {
    "title": "Token-Level Self-Play with Importance-Aware Guidance for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YEakcHa98a": {
    "title": "Explicitly Modeling Subcortical Vision with a Neuro-Inspired Front-End Improves CNN Robustness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bdFJbP7542": {
    "title": "Towards Provable Emergence of In-Context Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RkdTtznSAL": {
    "title": "Real-World Reinforcement Learning of Active Perception Behaviors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tmLsEYtg1K": {
    "title": "Representation-Level Counterfactual Calibration for Debiased Zero-Shot Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8M9T7Nl454": {
    "title": "VisDiff: SDF-Guided Polygon Generation for Visibility Reconstruction, Characterization and Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wg9gAqjAHb": {
    "title": "Separating the 'what' and 'how' of compositional computation to enable reuse and continual learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X13jOIhnog": {
    "title": "Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W9Y0jtf45v": {
    "title": "LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RW3cHh3HgY": {
    "title": "GRIT: Teaching MLLMs to Think with Images",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R24ZqNwoDz": {
    "title": "DynaAct: Large Language Model Reasoning with Dynamic Action Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dI4LrguKyz": {
    "title": "A Unified Framework for the Transportability of Population-Level Causal Measures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qkhfgo5gw1": {
    "title": "Beyond Scores: Proximal Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nG0URHBvFW": {
    "title": "Valid Inference with Imperfect Synthetic Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wm5cizmdcm": {
    "title": "Incentive-Aware Dynamic Resource Allocation under Long-Term Cost Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mt2UnV8jmB": {
    "title": "Time-uniform and Asymptotic Confidence Sequence of Quantile under Local Differential Privacy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ly5DnRIgCZ": {
    "title": "QiMeng-CodeV-R1: Reasoning-Enhanced Verilog Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NuCtKoflsV": {
    "title": "DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ufKaXYJt1F": {
    "title": "Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SoRiaijTGr": {
    "title": "DeltaProduct: Improving State-Tracking in Linear RNNs via Householder Products",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n7yVbKH7c3": {
    "title": "A Unified Framework for Variable Selection in Model-Based Clustering with Missing Not at Random",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oHB4Ee77uG": {
    "title": "CoT Red-Handed: Stress Testing Chain-of-Thought Monitoring",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zugMif2nm6": {
    "title": "Sparse Meets Dense: Unified Generative Recommendations with Cascaded Sparse-Dense Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XFzDI9CPUm": {
    "title": "Hamiltonian Neural PDE Solvers through Functional Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DV5z7VcaUA": {
    "title": "Are Language Models Efficient Reasoners? A Perspective from Logic Programming",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NligLHO7yG": {
    "title": "HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hchAR53gA0": {
    "title": "Chirality in Action: Time-Aware Video Representation Learning by Latent Straightening",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9JM03CQwzC": {
    "title": "Generation as Search Operator for Test-Time Scaling of Diffusion-based Combinatorial Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kMIhNcIZGb": {
    "title": "Evaluating LLM-contaminated Crowdsourcing Data Without Ground Truth",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VPPqfZWsuO": {
    "title": "Improved Confidence Regions and Optimal Algorithms for Online and Offline Linear MNL Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=chYXaetMmz": {
    "title": "Mind the Gap: Removing the Discretization Gap in Differentiable Logic Gate Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uX4dyc7Z5Z": {
    "title": "Group-Level Data Selection for Efficient Pretraining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ifneY2bnlQ": {
    "title": "AdaTS: Learning Adaptive Time Series Representations via Dynamic Soft Contrasts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NC7FPrUpKi": {
    "title": "Permutation Equivariant Neural Controlled Differential Equations for Dynamic Graph Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qspsIrurcR": {
    "title": "Reward-Instruct: A Reward-Centric Approach to Fast Photo-Realistic Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BsQg0ciABH": {
    "title": "Adversarial Diffusion for Robust Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KW7hQhOJiJ": {
    "title": "Improved Best-of-Both-Worlds Regret for Bandits with Delayed Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GS9o7u5njS": {
    "title": "Structured Reinforcement Learning for Combinatorial Decision-Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S9YexPytpi": {
    "title": "GAMMA: Gated Multi-hop Message Passing for Homophily-Agnostic Node Representation in GNNs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2t5IfPAzbX": {
    "title": "Model Reconciliation via Cost-Optimal Explanations in Probabilistic Logic Programming",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o9iReV4FGm": {
    "title": "Fast attention mechanisms: a tale of parallelism",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VBKgukQlRG": {
    "title": "Efficient Multi-bit Quantization Network Training via Weight Bias Correction and Bit-wise Coreset Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pCRm6g0RnA": {
    "title": "ALMGuard: Safety Shortcuts and Where to Find Them as Guardrails for Audio–Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8z3cOVER4z": {
    "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AOj3idcleS": {
    "title": "Optimize the Unseen - Fast NeRF Cleanup with Free Space Prior",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=91H9CSvdwl": {
    "title": "Safety Pretraining: Toward the Next Generation of Safe AI",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n3M8h9mqDm": {
    "title": "Generalization Bounds for Rank-sparse Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jOsuKwiCL0": {
    "title": "Value-Guided Search for Efficient Chain-of-Thought Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QUN6uidabr": {
    "title": "Provable Meta-Learning with Low-Rank Adaptations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=siPeAstQLq": {
    "title": "C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kuoD6G0Suq": {
    "title": "Planning without Search: Refining Frontier LLMs with Offline Goal-Conditioned RL",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=17O8DqToyr": {
    "title": "MGE-LDM: Joint Latent Diffusion for Simultaneous Music Generation and Source Extraction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DpuquFb1J8": {
    "title": "Machine Unlearning under Overparameterization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SbfdxWibDn": {
    "title": "C-NAV: Towards Self-Evolving Continual Object Navigation in Open World",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FxyG5xHjxb": {
    "title": "Finite-Time Bounds for Average-Reward Fitted Q-Iteration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lg2rP98oTf": {
    "title": "Pin the Tail on the Model: Blindfolded Repair of User-Flagged Failures in Text-to-Image Services",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1V3Toke6XP": {
    "title": "Teaching Language Models to Evolve with Users: Dynamic Profile Modeling for Personalized Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZbLeNBiCiQ": {
    "title": "Causal Discovery over Clusters of Variables in Markovian Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BOEZYnC8nR": {
    "title": "Adaptive Kernel Design for Bayesian Optimization Is a Piece of CAKE with LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kf9eNbp4wy": {
    "title": "Diffusion Feature Field for Text-based 3D Editing with Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uvyr9bYwL6": {
    "title": "A*-Thought: Efficient Reasoning via Bidirectional Compression for Low-Resource Settings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QBjuYL4gAX": {
    "title": "When and how can inexact generative models still sample from the data manifold?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RnByxq9toW": {
    "title": "Physics-informed Reduced Order Modeling of Time-dependent PDEs via Differentiable Solvers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nsySOGmHf9": {
    "title": "Factor Decorrelation Enhanced Data Removal from Deep Predictive Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fFr5GI6P5P": {
    "title": "GLNCD: Graph-Level Novel Category Discovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PhnquAa8eV": {
    "title": "Inductive Domain Transfer In Misspecified Simulation-Based Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9vDpL9lgKS": {
    "title": "Inverse Optimization Latent Variable Models for Learning Costs Applied to Route Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=30iBKSQMXn": {
    "title": "Retro-R1: LLM-based Agentic Retrosynthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xp7B8rkh7L": {
    "title": "LoRA vs Full Fine-tuning: An Illusion of Equivalence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cYO7k2fA8v": {
    "title": "OmniFC: Rethinking Federated Clustering via Lossless and Secure Distance Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yMXn86pzWx": {
    "title": "Visual Jenga: Discovering Object Dependencies via Counterfactual Inpainting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m7Aw57oI3U": {
    "title": "Curriculum Model Merging: Harmonizing Chemical LLMs for Enhanced Cross-Task Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Flntl1YwZg": {
    "title": "A Gradient Guided Diffusion Framework for Chance Constrained Programming",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gEeIutncjh": {
    "title": "Do Automatic Factuality Metrics Measure Factuality? A Critical Evaluation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gBgvuTd9Hx": {
    "title": "One Token Embedding Is Enough to Deadlock Your Large Reasoning Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5pQFE4yIZ5": {
    "title": "Distributive Fairness in Large Language Models: Evaluating Alignment with Human Values",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n0I0IvdfB3": {
    "title": "The Narrow Gate: Localized Image-Text Communication in Native Multimodal Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ccHq9OScXW": {
    "title": "Optimal Regret Bounds via Low-Rank Structured Variation in Non-Stationary Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZTYlxJZF1z": {
    "title": "Seeing through Uncertainty: Robust Task-Oriented Optimization in Visual Navigation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hehoz0QgeF": {
    "title": "Auto-Search and Refinement: An Automated Framework for Gender Bias Mitigation in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JScOknY0Su": {
    "title": "FerretNet: Efficient Synthetic Image Detection via Local Pixel Dependencies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ODgWBaErst": {
    "title": "Distribution-Aware Tensor Decomposition for Compression of Convolutional Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DOaqwuhEjc": {
    "title": "Discovering Data Structures: Nearest Neighbor Search and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H7vg3IgvHU": {
    "title": "Generalized Linear Bandits: Almost Optimal Regret with One-Pass Update",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E6ZdfjtoiX": {
    "title": "Efficient Data Selection at Scale via Influence Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1zKElu2MuQ": {
    "title": "Matching Markets Meet LLMs: Algorithmic Reasoning with Ranked Preferences",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E6gwPtWjb1": {
    "title": "LLM Meets Diffusion: A Hybrid Framework for Crystal Material Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OF7OLxvY0t": {
    "title": "Training-Free Test-Time Adaptation via Shape and Style Guidance for Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E6SFbnPiVP": {
    "title": "FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lbtOctHDQ3": {
    "title": "Flatness is Necessary, Neural Collapse is Not: Rethinking Generalization via Grokking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Spt8cAJq0": {
    "title": "SYMPHONY: Synergistic Multi-agent Planning with Heterogeneous Language Model Assembly",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QPJjiNCRq1": {
    "title": "Improving Generalization of Neural Combinatorial Optimization for Vehicle Routing Problems via Test-Time Projection Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ja3p9Dylmh": {
    "title": "HairFree: Compositional 2D Head Prior for Text-Driven 360° Bald Texture Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oY1Xnt83oJ": {
    "title": "DRIFT: Dynamic Rule-Based Defense with Injection Isolation for Securing LLM Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rqn7XLNUAv": {
    "title": "FRAM: Frobenius-Regularized Assignment Matching with Mixed-Precision Computing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tT3EQdFHtX": {
    "title": "The Flood Complex: Large-Scale Persistent Homology on Millions of Points",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=btJUnAPQ7j": {
    "title": "Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SPrb3ZH7hZ": {
    "title": "Reading Recognition in the Wild",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ElTbpJp7b9": {
    "title": "PhysDiff: A Physically-Guided Diffusion Model for Multivariate Time Series Anomaly Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kbwSqgVLYj": {
    "title": "Reasoning is Periodicity? Improving Large Language Models Through Effective Periodicity Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YS0a4YpQ1C": {
    "title": "Disentangling Hyperedges through the Lens of Category Theory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SXCjvhEQZO": {
    "title": "On Optimal Steering to Achieve Exact Fairness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ioYzPQb7cL": {
    "title": "Incentivizing Desirable Effort Profiles in Strategic Classification: The Role of Causality and Uncertainty",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1C4mXyh31p": {
    "title": "UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ifsyZYYDNs": {
    "title": "SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TRHT8ksdUQ": {
    "title": "How Patterns Dictate Learnability in Sequential Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LSS6JMvgha": {
    "title": "Towards Robust Uncertainty Calibration for Composed Image Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BM1GqmkEHS": {
    "title": "Simultaneous Statistical Inference for Off-Policy Evaluation in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FgjcLXIUjr": {
    "title": "Rewind-to-Delete: Certified Machine Unlearning for Nonconvex Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RqE5PlQsU5": {
    "title": "Projection-Manifold Regularized Latent Diffusion for Robust General Image Fusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3iEWfZh5R2": {
    "title": "Learning to Generalize: An Information Perspective on Neural Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2ym6uVbgMN": {
    "title": "A Beyond-Worst-Case Analysis of Greedy k-means++",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pcG6NRJKu7": {
    "title": "Finding and Reactivating Post-Trained LLMs' Hidden Safety Mechanisms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e2WesV6Voe": {
    "title": "Sequence Modeling with Spectral Mean Flows",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GEzd5K5s5u": {
    "title": "State-Covering Trajectory Stitching for Diffusion Planners",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Izewdgcl8U": {
    "title": "How Does Label Noise Gradient Descent Improve Generalization in the Low SNR Regime?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=isATAFP71B": {
    "title": "SE-Agent: Self-Evolution Trajectory Optimization in Multi-Step Reasoning with LLM-Based Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8RMs5San6e": {
    "title": "Representation Consistency for Accurate and Coherent LLM Answer Aggregation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FLdLPUqnsP": {
    "title": "Revisiting Logit Distributions for Reliable Out-of-Distribution Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Idmk7O4sWA": {
    "title": "Analyzing Similarity Metrics for Data Selection for Language Model Pretraining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4KRERpdVDC": {
    "title": "Finding separatrices of dynamical flows with Deep Koopman Eigenfunctions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8dTBydCMSr": {
    "title": "Better NTK Conditioning: A Free Lunch from (ReLU) Nonlinear Activation in Wide Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KhavyzO9kK": {
    "title": "Register and [CLS] tokens induce a decoupling of local and global features in large ViTs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=klmc4fwPLd": {
    "title": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OROkRBW6cu": {
    "title": "A Learning-Augmented Dynamic Programming Approach for Orienteering Problem with Time Windows",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pbokMgz8e1": {
    "title": "SDPGO: Efficient Self-Distillation Training Meets Proximal Gradient Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RsZv37DGka": {
    "title": "IMPACT: Irregular Multi-Patch Adversarial Composition Based on Two‑Phase Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6eLlczxbrp": {
    "title": "Vad-R1: Towards Video Anomaly Reasoning via Perception-to-Cognition Chain-of-Thought",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wk5B5W5OHo": {
    "title": "VFRTok: Variable Frame Rates Video Tokenizer with Duration-Proportional Information Assumption",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l8QMnn6wfb": {
    "title": "Weak-shot Keypoint Estimation via Keyness and Correspondence Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8WdkG7g6Az": {
    "title": "Reward-Aware Proto-Representations in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y9LHDCKeeN": {
    "title": "PID-controlled Langevin Dynamics for Faster Sampling on Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VHWmDTYI2O": {
    "title": "Non-Adaptive Adversarial Face Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tfbu0ITAez": {
    "title": "SIGMA: Refining Large Language Model Reasoning via Sibling-Guided Monte Carlo Augmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=39PoZNT4XX": {
    "title": "KSP: Kolmogorov-Smirnov metric-based Post-Hoc Calibration for Survival Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cANLnJb1FX": {
    "title": "Are Large Reasoning Models Good Translation Evaluators? Analysis and Performance Boost",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wHx7UuRm7G": {
    "title": "Segment Anything Model Meets Semi-supervised Medical Image Segmentation: A Novel Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kpdFjNitGW": {
    "title": "un 2 CLIP: Improving CLIP's Visual Detail Capturing Ability via Inverting unCLIP",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KYlIC6sLhw": {
    "title": "HollowFlow: Efficient Sample Likelihood Evaluation using Hollow Message Passing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OACw1Fqy6Y": {
    "title": "Overcoming Challenges of Long-Horizon Prediction in Driving World Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yeyaKpaufr": {
    "title": "Enforcing convex constraints in Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4Q1vA6P9J9": {
    "title": "Cascaded Language Models for Cost-Effective Human–AI Decision-Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YAc0O13qMc": {
    "title": "A Geometry-Aware Metric for Mode Collapse in Time Series Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hJRklWq4Ah": {
    "title": "Revisiting Frank-Wolfe for Structured Nonconvex Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3dsOpH1xHK": {
    "title": "The Bias-Variance Tradeoff in Data-Driven Optimization: A Local Misspecification Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zXlB9A5xya": {
    "title": "Imagined Autocurricula",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oElWLpkOux": {
    "title": "OrbitZoo: Real Orbital Systems Challenges for Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mpRWBiVVkS": {
    "title": "Fair Minimum Labeling: Efficient Temporal Network Activations for Reachability and Equity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mImUMLEpdf": {
    "title": "Locally Optimal Private Sampling: Beyond the Global Minimax",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pyDAihUMKV": {
    "title": "Relaxing partition admissibility in Cluster-DAGs: a causal calculus with arbitrary variable clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QVheAhJefR": {
    "title": "Learning Preferences without Interaction for Cooperative AI: A Hybrid Offline-Online Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0pbUfmwNTy": {
    "title": "DyFlow: Dynamic Workflow Framework for Agentic Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=szBFUtBzWP": {
    "title": "TANDEM: Bi-Level Data Mixture Optimization with Twin Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sJl73HJGNj": {
    "title": "AiDE-Q: Synthetic Labeled Datasets Can Enhance Learning Models for Quantum Property Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ngxGNQE1M2": {
    "title": "Truth over Tricks: Measuring and Mitigating Shortcut Learning in Misinformation Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oul46PkP7Z": {
    "title": "LeVo: High-Quality Song Generation with Multi-Preference Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=koZxsoeuAO": {
    "title": "Find your Needle: Small Object Image Retrieval via Multi-Object Attention Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eX0m4qMYVN": {
    "title": "ClusterFusion: Expanding Operator Fusion Scope for LLM Inference via Cluster-Level Collective Primitive",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZR2mdBrhJX": {
    "title": "PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7mTECPRtll": {
    "title": "Mitigating Hallucination in VideoLLMs via Temporal-Aware Activation Engineering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=284GWLFtjU": {
    "title": "DEAL: Diffusion Evolution Adversarial Learning for Sim-to-Real Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b7uniOw0sZ": {
    "title": "Which Data Attributes Stimulate Math and Code Reasoning? An Investigation via Influence Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j3pkYPmbot": {
    "title": "Learning to Add, Multiply, and Execute Algorithmic Instructions Exactly with Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XzXGqoUNUa": {
    "title": "Oryx: a Scalable Sequence Model for Many-Agent Coordination in Offline MARL",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QLQqMndNoL": {
    "title": "Practical Bayes-Optimal Membership Inference Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UwAgCUWJ21": {
    "title": "Kernel von Mises Formula of the Influence Function",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zj1uV88eYU": {
    "title": "LBMKGC: Large Model-Driven Balanced Multimodal Knowledge Graph Completion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eMu3uKRRbZ": {
    "title": "Modeling Dynamic Neural Activity by combining Naturalistic Video Stimuli and Stimulus-independent Latent Factors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YJx8AofTF5": {
    "title": "Program Synthesis via Test-Time Transduction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xPcKmKSEis": {
    "title": "Visual Thoughts: A Unified Perspective of Understanding Multimodal Chain-of-Thought",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iZy3ZgoHrD": {
    "title": "ZeroPatcher: Training-free Sampler for Video Inpainting and Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Lb78wXdFO": {
    "title": "Pattern-Guided Adaptive Prior for Structure Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n1XlwRmF7v": {
    "title": "Enhancing Graph Classification Robustness with Singular Pooling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Eufm2Jmjod": {
    "title": "Learning to cluster neuronal function",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b2IU6QOOfo": {
    "title": "PaZO: Preconditioned Accelerated Zeroth-Order Optimization for Fine-Tuning LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=38n8pFvldK": {
    "title": "CroPe: Cross-Modal Semantic Compensation Adaptation for All Adverse Scene Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xud9JYzgSp": {
    "title": "Unlocking SLM Potential for Data Analysis Code Generation via Non-Parametric Knowledge Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qcFTezC4x9": {
    "title": "Event-based HDR Structured Light",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9odue1eopm": {
    "title": "HypoBootstrap: A Bootstrapping Framework for Inductive Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lTEjpeTXIo": {
    "title": "LaM-SLidE: Latent Space Modeling of Spatial Dynamical Systems via Linked Entities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rHBuLD2slP": {
    "title": "VIKING: Deep variational inference with stochastic projections",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yRxXTdElLv": {
    "title": "SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in Real-World Applications",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9wQNsSiLV0": {
    "title": "How Ensembles of Distilled Policies Improve Generalisation in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CyUq9D99vE": {
    "title": "ParamMute: Suppressing Knowledge-Critical FFNs for Faithful Retrieval-Augmented Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7EjdHkOwc4": {
    "title": "PANGEA: Projection-Based Augmentation with Non-Relevant General Data for Enhanced Domain Adaptation in LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l6hAqx4eoB": {
    "title": "Explainably Safe Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ajBIcOBcKV": {
    "title": "Concentration and excess risk bounds for imbalanced classification with synthetic oversampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pf3SVNhAQB": {
    "title": "From Information to Generative Exponent: Learning Rate Induces Phase Transitions in SGD",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tCiijwAXij": {
    "title": "Scaling Image Geo-Localization to Continent Level",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DG8zwjoWGZ": {
    "title": "Spatiotemporal Consensus with Scene Prior for Unsupervised Domain Adaptive Person Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tAq9Gxdhr0": {
    "title": "Disentangling Latent Shifts of In-Context Learning with Weak Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lz5BUjArK4": {
    "title": "Multi-Expert Distributionally Robust Optimization for Out-of-Distribution Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r5tbCL9vAZ": {
    "title": "GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iRQM8Ehgl9": {
    "title": "Hadamax Encoding: Elevating Performance in Model-Free Atari",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ul5mlXrLZb": {
    "title": "OOD Detection with Relative Angles",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NcxmgX95ue": {
    "title": "URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TgCkj4uEPl": {
    "title": "Capturing Individual Human Preferences with Reward Features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ejcn7IDkzT": {
    "title": "Robust Satisficing Gaussian Process Bandits Under Adversarial Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZDpPfg9pDc": {
    "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zJSZupQ889": {
    "title": "SALS: Sparse Attention in Latent Space for KV Cache Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Ybm2W1o9m": {
    "title": "Query-Efficient Locally Private Hypothesis Selection via the Scheffe Graph",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=790HoTFKUE": {
    "title": "Towards Robust Parameter-Efficient Fine-Tuning for Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T6d5IYr8PJ": {
    "title": "Certifying Deep Network Risks and Individual Predictions with PAC-Bayes Loss via Localized Priors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b2u1yrTwFK": {
    "title": "Dyn-O: Building Structured World Models with Object-Centric Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7NEP4jGKwA": {
    "title": "Instance-Level Composed Image Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1fOGTbO5Sx": {
    "title": "Wonder Wins Ways: Curiosity-Driven Exploration through Multi-Agent Contextual Calibration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y3Q3nod80m": {
    "title": "Differentially Private Quantiles with Smaller Error",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RBx1AfoL2J": {
    "title": "Spectral Conditioning of Attention Improves Transformer Performance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WENq0drMfO": {
    "title": "Unfolding the Black Box of Recurrent Neural Networks for Path Integration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5ofJyjgrth": {
    "title": "Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dbjzqvmd7t": {
    "title": "Vector Database Watermarking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qeL8fi8GS7": {
    "title": "Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ezSyZM6Lp7": {
    "title": "Multi-Task Vehicle Routing Solver via Mixture of Specialized Experts under State-Decomposable MDP",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fUhWmqCnqW": {
    "title": "Taxonomy of reduction matrices for Graph Coarsening",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vritEZz28d": {
    "title": "BlurGuard: A Simple Approach for Robustifying Image Protection Against AI-Powered Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qGFvTIMS3W": {
    "title": "Automated Model Discovery via Multi-modal & Multi-step Pipeline",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NMQUvjAY5x": {
    "title": "Hawk: Leveraging Spatial Context for Faster Autoregressive Text-to-Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MBJ46gd1CT": {
    "title": "OWL: Optimized Workforce Learning for General Multi-Agent Assistance in Real-World Task Automation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JzWtqd9CGJ": {
    "title": "Efficient Last-Iterate Convergence in Solving Extensive-Form Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kz6eUL86XP": {
    "title": "Do Language Models Use Their Depth Efficiently?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8HYeWMf0W3": {
    "title": "LILO: Learning to Reason at the Frontier of Learnability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2pCTBJQLwF": {
    "title": "Just One Layer Norm Guarantees Stable Extrapolation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=75JiIa0fU1": {
    "title": "FedWMSAM: Fast and Flat Federated Learning via Weighted Momentum and Sharpness-Aware Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X3RVQNOIXZ": {
    "title": "Increasing the Utility of Synthetic Images through Chamfer Guidance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aJ7AdfOfij": {
    "title": "Beyond Token Probes: Hallucination Detection via Activation Tensors with ACT-ViT",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=swwAPbtU07": {
    "title": "Mixture of Scope Experts at Test: Generalizing Deeper Graph Neural Networks with Shallow Variants",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4qVWY12KQT": {
    "title": "QiMeng-NeuComBack: Self-Evolving Translation from IR to Assembly Code",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8hahZ1h9CG": {
    "title": "PIVNO: Particle Image Velocimetry Neural Operator",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yaS3JWQRQ6": {
    "title": "Temporal Representation Alignment: Successor Features Enable Emergent Compositionality in Robot Instruction Following",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qku7g56aWf": {
    "title": "Direct Numerical Layout Generation for 3D Indoor Scene Synthesis via Spatial Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C3sKfe8e1n": {
    "title": "Adapting to Stochastic and Adversarial Losses in Episodic MDPs with Aggregate Bandit Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=23ichdd74N": {
    "title": "Backward Conformal Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ap8OIosN8p": {
    "title": "Robust Label Proportions Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4wnhbcppot": {
    "title": "Multimodal Bandits: Regret Lower Bounds and Optimal Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qS3WgmGs9s": {
    "title": "SketchMind: A Multi-Agent Cognitive Framework for Assessing Student-Drawn Scientific Sketches",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fsTj0BNxyH": {
    "title": "A High-Dimensional Statistical Method for Optimizing Transfer Quantities in Multi-Source Transfer Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9F2Cmgo17M": {
    "title": "PARCO: Parallel AutoRegressive Models for Multi-Agent Combinatorial Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9GHaLDORNL": {
    "title": "SGAR: Structural Generative Augmentation for 3D Human Motion Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NFuoxl5CCc": {
    "title": "Bootstrap Your Uncertainty: Adaptive Robust Classification Driven by Optimal-Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AumtzgHji1": {
    "title": "The Persistence of Neural Collapse Despite Low-Rank Bias",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1cURNMriee": {
    "title": "Streaming Audio Generation from Discrete Tokens via Streaming Flow Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6aaixHco6C": {
    "title": "RFMPose: Generative Category-level Object Pose Estimation via Riemannian Flow Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sZ22INHr9H": {
    "title": "Adaptive and Multi-scale Affinity Alignment for Hierarchical Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n6AfpsbD7n": {
    "title": "Reconciling Geospatial Prediction and Retrieval via Sparse Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P23UMiw7iJ": {
    "title": "Offline Goal-conditioned Reinforcement Learning with Quasimetric Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yRxX01oRIi": {
    "title": "Evaluating the Inductive Abilities of Large Language Models: Why Chain-of-Thought Reasoning Sometimes Hurts More Than Helps",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bomqTo3pin": {
    "title": "HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fyeSq3m8CY": {
    "title": "Tensor-Parallelism with Partially Synchronized Activations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h3LlJ6Bh4S": {
    "title": "Process vs. Outcome Reward: Which is Better for Agentic RAG Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kLAwT8G3Q1": {
    "title": "The Parameterized Complexity of Computing the VC-Dimension",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0fBQAckQK3": {
    "title": "GASP: Efficient Black-Box Generation of Adversarial Suffixes for Jailbreaking LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=16kX08MCav": {
    "title": "Limitations of Normalization in Attention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aY97JGello": {
    "title": "Safe and Stable Control via Lyapunov-Guided Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nOv6z9RHA5": {
    "title": "Bridging Time and Linguistics: LLMs as Time Series Analyzer through Symbolization and Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4lR9OhAisI": {
    "title": "MIND: Material Interface Generation from UDFs for Non-Manifold Surface Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SAEBMGJegn": {
    "title": "Text-to-Code Generation for Modular Building Layouts in Building Information Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qz3s5ANjnN": {
    "title": "Distances for Markov chains from sample streams",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1DmvJtwaDg": {
    "title": "Resolution of Simpson's paradox via the common cause principle",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p8lKcNkJRi": {
    "title": "Dense SAE Latents Are Features, Not Bugs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pcLsaspXSi": {
    "title": "Sparse Optimistic Information Directed Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UORjUcFmrn": {
    "title": "The Complexity of Correlated Equilibria in Generalized Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lDh78hf5Vk": {
    "title": "Optimal Rates in Continual Linear Regression via Increasing Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IfYBGaVFuD": {
    "title": "MSTAR: Box-free Multi-query Scene Text Retrieval with Attention Recycling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A4JiifkTzq": {
    "title": "SPRO: Improving Image Generation via Self-Play",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e96KCdEfRu": {
    "title": "DitHub: A Modular Framework for Incremental Open-Vocabulary Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oYIXUwmNig": {
    "title": "Prediction-Powered Semi-Supervised Learning with Online Power Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cx45ACt9Lg": {
    "title": "The third pillar of causal analysis? A measurement perspective on causal representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jSc7HD7mPj": {
    "title": "BADiff: Bandwidth Adaptive Diffusion Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nWTQREGMLG": {
    "title": "Statistical inference for Linear Stochastic Approximation with Markovian Noise",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=got7rMaVin": {
    "title": "Training-free Online Video Step Grounding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1YLpf8nUIq": {
    "title": "SPARKE: Scalable Prompt-Aware Diversity and Novelty Guidance in Diffusion Models via RKE Score",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sdK5Ufoo2d": {
    "title": "Curious Causality-Seeking Agents Learn Meta Causal World",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sv41aaGTit": {
    "title": "LLM-Driven Treatment Effect Estimation Under Inference Time Text Confounding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sLfMvrkn6T": {
    "title": "TimeEmb: A Lightweight Static-Dynamic Disentanglement Framework for Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hMZnFo0FLF": {
    "title": "Controlling the Flow: Stability and Convergence for Stochastic Gradient Descent with Decaying Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wSDE3karoF": {
    "title": "ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type Neighborhoods",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pc6M9h3T9m": {
    "title": "Beyond Verifiable Rewards: Scaling Reinforcement Learning in Language Models to Unverifiable Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lJYwfYcoZX": {
    "title": "Approximating Shapley Explanations in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rQCl1sf62w": {
    "title": "BEAST: Efficient Tokenization of B-Splines Encoded Action Sequences for Imitation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9aElHWiZ72": {
    "title": "From Faults to Features: Pretraining to Learn Robust Representations against Sensor Failures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UxzfzqgnpZ": {
    "title": "You Only Spectralize Once: Taking a Spectral Detour to Accelerate Graph Neural Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6QjgmXRItM": {
    "title": "Explaining the Law of Supply and Demand via Online Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t7NLyOtPEi": {
    "title": "Latent Mixture of Symmetries for Sample-Efficient Dynamic Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jv7OHhQ0YP": {
    "title": "RepGuard: Adaptive Feature Decoupling for Robust Backdoor Defense in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CmKar1zptJ": {
    "title": "Post Hoc Regression Refinement via Pairwise Rankings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nUFGbRWl5W": {
    "title": "OSTAR: Optimized Statistical Text-classifier with Adversarial Resistance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HIV6t8BAZY": {
    "title": "Siegel Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZacSviw4em": {
    "title": "Towards Understanding Transformers in Learning Random Walks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rHMjAiaLzi": {
    "title": "STACI: Spatio-Temporal Aleatoric Conformal Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ogml5bxDH3": {
    "title": "Value-Guided Decision Transformer: A Unified Reinforcement Learning Framework for Online and Offline Settings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MyCKtV9CpR": {
    "title": "Centralized Reward Agent for Knowledge Sharing and Transfer in Multi-Task Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7VN0iICXZj": {
    "title": "Optimal Rates for Generalization of Gradient Descent for Deep ReLU Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=utvu4PJ0Ct": {
    "title": "Attack via Overfitting: 10-shot Benign Fine-tuning to Jailbreak LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Swik6KVV4u": {
    "title": "Enhancing Bioactivity Prediction via Spatial Emptiness Representation of Protein-ligand Complex and Union of Multiple Pockets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SeC5Zb8Orf": {
    "title": "Knowledge Graph Enhanced Generative Multi-modal Models for Class-Incremental Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hyolWgWWyg": {
    "title": "D2SA: Dual-Stage Distribution and Slice Adaptation for Efficient Test-Time Adaptation in MRI Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=padrqCRyCP": {
    "title": "Tight Asymptotics of Extreme Order Statistics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t6tObHgZSt": {
    "title": "Geometric Mixture Models for Electrolyte Conductivity Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tDT6HF7jK5": {
    "title": "Motion Matters: Compact Gaussian Streaming for Free-Viewpoint Video Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ztgYn0Uk94": {
    "title": "Mamba Goes HoME: Hierarchical Soft Mixture-of-Experts for 3D Medical Image Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=14ZMESMh5V": {
    "title": "DISCO: DISCrete nOise for Conditional Control in Text-to-Image Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZDbhQrgwoT": {
    "title": "Proximalized Preference Optimization for Diverse Feedback Types: A Decomposed Perspective on DPO",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NKJzAFYYI4": {
    "title": "Diffusion-Guided Graph Data Augmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ynwl0V1YH0": {
    "title": "Learning Grouped Lattice Vector Quantizers for Low-Bit LLM Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QNXWTA7PZS": {
    "title": "Building 3D Representations and Generating Motions From a Single Image via Video-Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jkCH6iwFZm": {
    "title": "Adaptive Inference-Time Scaling via Cyclic Diffusion Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7DY7kB8wyZ": {
    "title": "LLM Layers Immediately Correct Each Other",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MZwGG87Jev": {
    "title": "RiverMamba: A State Space Model for Global River Discharge and Flood Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H55l4vLQV8": {
    "title": "Beyond Last-Click: An Optimal Mechanism for Ad Attribution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NzPwDutzz8": {
    "title": "rStar-Coder: Scaling Competitive Code Reasoning with a Large-Scale Verified Dataset",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mG9xItYI0D": {
    "title": "From Average-Iterate to Last-Iterate Convergence in Games: A Reduction and Its Applications",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fqsl9IfbfJ": {
    "title": "Contextual Thompson Sampling via Generation of Missing Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CAz7UGRdLs": {
    "title": "Raw2Drive: Reinforcement Learning with Aligned World Models for End-to-End Autonomous Driving (in CARLA v2)",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YxPI1c5e09": {
    "title": "Causal Climate Emulation with Bayesian Filtering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aXpbgG5z6I": {
    "title": "AI Debate Aids Assessment of Controversial Claims",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NKPud1Jnyt": {
    "title": "V-CECE: Visual Counterfactual Explanations via Conceptual Edits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JDKYmIh4rO": {
    "title": "Noise Consistency Training: A Native Approach for One-step Generator in Learning Additional Controls",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xGmS1i0pDq": {
    "title": "Nonlinearly Preconditioned Gradient Methods: Momentum and Stochastic Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x6pqrvXgfY": {
    "title": "Distribution Learning Meets Graph Structure Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p93zLNCzKW": {
    "title": "Data Fusion for Partial Identification of Causal Effects",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fyqqd1lHDb": {
    "title": "Enhanced Expert Merging for Mixture-of-Experts in Graph Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wtYcS4kxpF": {
    "title": "Learning normalized image densities via dual score matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PTLIM88XYR": {
    "title": "Advancing Compositional Awareness in CLIP with Efficient Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xUx2B2NHvj": {
    "title": "Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5wdssRcI2Z": {
    "title": "DoseSurv: Predicting Personalized Survival Outcomes under Continuous-Valued Treatments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PqAGmIg0az": {
    "title": "APML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LHsQSC89Pt": {
    "title": "STAR-Bets: Sequential TArget-Recalculating Bets for Tighter Confidence Intervals",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vzgkhVS5EC": {
    "title": "Connectome-Based Modelling Reveals Orientation Maps in the Drosophila Optic Lobe",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o8n5oNDsiq": {
    "title": "MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pv964N1RYb": {
    "title": "DynaPhArM: Adaptive and Physics-Constrained Modeling for Target-Drug Complexes with Drug-Specific Adaptations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sye27MizdM": {
    "title": "Topology-aware Graph Diffusion Model with Persistent Homology",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kdqzbx8YGU": {
    "title": "OmniSegmentor: A Flexible Multi-Modal Learning Framework for Semantic Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gyeEozWwFO": {
    "title": "Soft-consensual Federated Learning for Data Heterogeneity via Multiple Paths",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EAtSPyQ09Z": {
    "title": "ShapeX: Shapelet-Driven Post Hoc Explanations for Time Series Classification Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XMzxZ6h68o": {
    "title": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t3LTjkXDJA": {
    "title": "Breaking the Order Barrier: Off-Policy Evaluation for Confounded POMDPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Iy4cAXotrf": {
    "title": "Model Provenance Testing for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KsmgnL6PUb": {
    "title": "Learning Efficient Fuse-and-Refine for Feed-Forward 3D Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=drZzzGUlbG": {
    "title": "Quasi-Self-Concordant Optimization with ℓ ∞ Lewis Weights",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LmcTbBvgjP": {
    "title": "Error Feedback under ( L 0 , L 1 ) -Smoothness: Normalization and Momentum",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V8Kbz7l2cr": {
    "title": "Reward Reasoning Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JQvx6rAqGr": {
    "title": "What Really is a Member? Discrediting Membership Inference via Poisoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yyWeSAsOhs": {
    "title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QH2xGLgObM": {
    "title": "Alleviating Hallucinations in Large Language Models through Multi-Model Contrastive Decoding and Dynamic Hallucination Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gimtybo07H": {
    "title": "DINO-Foresight: Looking into the Future with DINO",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fYW1PKawwJ": {
    "title": "STRATUS: A Multi-agent System for Autonomous Reliability Engineering of Modern Clouds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r3LYE0Ct3G": {
    "title": "PlanU: Large Language Model Reasoning through Planning under Uncertainty",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Or1pDhbSag": {
    "title": "AccuQuant: Simulating Multiple Denoising Steps for Quantizing Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iWWPtwXfnO": {
    "title": "Sparse Diffusion Autoencoder for Test-time Adapting Prediction of Complex Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gcAGeE8Cch": {
    "title": "Jury-and-Judge Chain-of-Thought for Uncovering Toxic Data in 3D Visual Grounding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YWE9na9Jai": {
    "title": "SNEAKDOOR: Stealthy Backdoor Attacks against Distribution Matching-based Dataset Condensation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bEP87LNTfX": {
    "title": "Bridging Human and LLM Judgments: Understanding and Narrowing the Gap",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=loznSxLomv": {
    "title": "FACE: A General Framework for Mapping Collaborative Filtering Embeddings into LLM Tokens",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MzN2Xi7EMa": {
    "title": "Attribution-Driven Adaptive Token Pruning for Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jZs26lJ0pl": {
    "title": "Guiding Cross-Modal Representations with MLLM Priors via Preference Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GjkuXI7hX7": {
    "title": "Interpreting Emergent Features in Deep Learning-based Side-channel Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=liefJOFVfH": {
    "title": "Scaling Data-Driven Probabilistic Robustness Analysis for Semantic Segmentation Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zZecO3RZ7Z": {
    "title": "Datasets, Documents, and Repetitions: The Practicalities of Unequal Data Quality",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iEegka1gfw": {
    "title": "From Contextual Combinatorial Semi-Bandits to Bandit List Classification: Improved Sample Complexity with Sparse Rewards",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hFxOZjHyTg": {
    "title": "InfiniPot-V: Memory-Constrained KV Cache Compression for Streaming Video Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3QjESmXftM": {
    "title": "Exploring the Translation Mechanism of Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=im3FJ6quii": {
    "title": "U-REPA: Aligning Diffusion U-Nets to ViTs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=STMB6q9pYZ": {
    "title": "SRHand: Super-Resolving Hand Images and 3D Shapes via View/Pose-aware Neural Image Representations and Explicit Meshes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yEddfz9SgJ": {
    "title": "Enhancing Infrared Vision: Progressive Prompt Fusion Network and Benchmark",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xuvdo6oMkE": {
    "title": "Rethinking the Role of Verbatim Memorization in LLM Privacy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4Hzsuzl76a": {
    "title": "Blindfolded Experts Generalize Better: Insights from Robotic Manipulation and Videogames",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B4NT8TexNS": {
    "title": "InvisibleInk: High-Utility and Low-Cost Text Generation with Differential Privacy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kK8cFKu1U7": {
    "title": "Cameras as Relative Positional Encoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6r2acwubHf": {
    "title": "Unsupervised Federated Graph Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9StiSCqeRw": {
    "title": "Eulerian Neural Network Informed by Chemical Transport for Air Quality Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k71nsscO9b": {
    "title": "Adaptive Re-calibration Learning for Balanced Multimodal Intention Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ofBqR4l0TD": {
    "title": "Rethinking Tokenized Graph Transformers for Node Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ILf0VJYQBD": {
    "title": "Fast Local Search Algorithms for Clustering with Adaptive Sampling and Bandit Strategies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lC4WKmTScD": {
    "title": "Prior-Guided Diffusion Planning for Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=voHNTOO5CG": {
    "title": "Removing Concepts from Text-to-Image Models with Only Negative Samples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0jQUNQsZra": {
    "title": "SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cmN8Wbvanr": {
    "title": "Can Large Language Models Master Complex Card Games?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zhCv5uZ8bh": {
    "title": "Dual-Flow: Transferable Multi-Target, Instance-Agnostic Attacks via In-the-wild Cascading Flow Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V4oTkK7cQz": {
    "title": "Risk-aware Direct Preference Optimization under Nested Risk Measure",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s3maemwE5M": {
    "title": "L 2 M: Mutual Information Scaling Law for Long-Context Language Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mOWpnUPLQy": {
    "title": "Energy Loss Functions for Physical Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y68Q09Vc4K": {
    "title": "DSAS: A Universal Plug-and-Play Framework for Attention Optimization in Multi-Document Question Answering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5aIVAGHCa5": {
    "title": "Resource-Constrained Federated Continual Learning: What Does Matter?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wHsFqmM1rp": {
    "title": "TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SyJ3PdcokV": {
    "title": "Generative diffusion for perceptron problems: statistical physics analysis and efficient algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MfhFiU28hv": {
    "title": "An Improved Algorithm for Adversarial Linear Contextual Bandits via Reduction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=24vq7c6MpR": {
    "title": "Learning Multi-Source and Robust Representations for Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LSCAG7LPz4": {
    "title": "Flexible Realignment of Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RTjr4DnS79": {
    "title": "Metis: A Foundation Speech Generation Model with Masked Generative Pre-training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NwLOIEi9vX": {
    "title": "Optimizing the Unknown: Black Box Bayesian Optimization with Energy-Based Model and Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4pe5ZNNJyG": {
    "title": "Leveraging Conditional Dependence for Efficient World Model Denoising",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FXBBy1caOX": {
    "title": "Large Language Models for Lossless Image Compression: Next-Pixel Prediction in Language Space is All You Need",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lswfDFAYWR": {
    "title": "Prompt-guided Disentangled Representation for Action Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O8Ifwhylnr": {
    "title": "Conformal Online Learning of Deep Koopman Linear Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9CMUyo2xpO": {
    "title": "NegoCollab: A Common Representation Negotiation Approach for Heterogeneous Collaborative Perception",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wuMdBGMe3y": {
    "title": "Recognition through Reasoning: Reinforcing Image Geo-localization with Large Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tiGFiCrmKm": {
    "title": "Finding Low-Rank Matrix Weights in DNNs via Riemannian Optimization: RAdaGrad and RAdamW",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yzl5tL0Z2M": {
    "title": "Semantic Representation Attack against Aligned Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HCSjARBq5T": {
    "title": "From Specificity to Generality: Revisiting Generalizable Artifacts in Detecting Face Deepfakes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WDAKFpWftI": {
    "title": "NestedFP: High-Performance, Memory-Efficient Dual-Precision Floating Point Support for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MyxVp2kogT": {
    "title": "Consistent Supervised-Unsupervised Alignment for Generalized Category Discovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N4cyRMuLyl": {
    "title": "On Vanishing Gradients, Over-Smoothing, and Over-Squashing in GNNs: Bridging Recurrent and Graph Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vLUW0OZGWD": {
    "title": "Optimism Without Regularization: Constant Regret in Zero-Sum Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TrHeq0yFhv": {
    "title": "SensorLM: Learning the Language of Wearable Sensors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YtmfBtIOPQ": {
    "title": "Dataset Distillation of 3D Point Clouds via Distribution Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LjtgTpWH71": {
    "title": "Hybrid Latent Reasoning via Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GHAtg6RQ3b": {
    "title": "Self-Supervised Discovery of Neural Circuits in Spatially Patterned Neural Responses with Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LZ4IKybwWl": {
    "title": "HQA-VLAttack: Towards High Quality Adversarial Attack on Vision-Language Pre-Trained Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yGWLWjM4nq": {
    "title": "DiffLiG: Diffusion-enhanced Liquid Graph with Attention Propagation for Grid-to-Station Precipitation Correction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x9ozsFz4yK": {
    "title": "Diffusion on Demand: Selective Caching and Modulation for Efficient Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rRxFIOoEeF": {
    "title": "Revisiting Multi-Agent World Modeling from a Diffusion-Inspired Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zn4F6os6cq": {
    "title": "From Synapses to Dynamics: Obtaining Function from Structure in a Connectome Constrained Model of the Head Direction Circuit",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dAAz7afWJR": {
    "title": "Robust and Scalable Autonomous Reinforcement Learning in Irreversible Environments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=biM1m6VXSD": {
    "title": "Controllable 3D Molecular Generation for Structure-Based Drug Design Through Bayesian Flow Networks and Gradient Integration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BWpMTkRrEO": {
    "title": "Event-Guided Consistent Video Enhancement with Modality-Adaptive Diffusion Pipeline",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G67ZNmeWJ5": {
    "title": "NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8tetkBNSWZ": {
    "title": "INST-IT: Boosting Instance Understanding via Explicit Visual Prompt Instruction Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cHMP2IAhML": {
    "title": "HoloLLM: Multisensory Foundation Model for Language-Grounded Human Sensing and Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=irni27kAeP": {
    "title": "Dynamic Shadow Unveils Invisible Semantics for Video Outpainting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X2xLfqX24x": {
    "title": "Eagle 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SbJAGSdLhY": {
    "title": "Geometry-Aware Collaborative Multi-Solutions Optimizer for Model Fine-Tuning with Parameter Efficiency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rRhp7LkBZV": {
    "title": "InstanceAssemble: Layout-Aware Image Generation via Instance Assembling Attention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yv7zKaptjo": {
    "title": "Precise Information Control in Long-Form Text Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ARZiMmb619": {
    "title": "Posterior Sampling by Combining Diffusion Models with Annealed Langevin Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4pUumnQxDG": {
    "title": "RAPTR: Radar-based 3D Pose Estimation using Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=stUD2uNafM": {
    "title": "Block Coordinate Descent for Neural Networks Provably Finds Global Minima",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r2ykUnzuGt": {
    "title": "ReCAP: Recursive Context-Aware Reasoning and Planning for Large Language Model Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wdI2WKCN3P": {
    "title": "HoT-VI: Reparameterizable Variational Inference for Capturing Instance-Level High-Order Correlations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cd2MWwIIHu": {
    "title": "MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LrnZDU9g7N": {
    "title": "EquiTabPFN: A Target-Permutation Equivariant Prior Fitted Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vhzqdxuSB3": {
    "title": "EditInfinity: Image Editing with Binary-Quantized Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xAKOn2IfsP": {
    "title": "AdaptDel: Adaptable Deletion Rate Randomized Smoothing for Certified Robustness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iBwW1DxQaa": {
    "title": "Directed-Tokens: A Robust Multi-Modality Alignment Approach to Large Language-Vision Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xpjWEgf8zi": {
    "title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cJRggDnFg2": {
    "title": "End-to-End Low-Light Enhancement for Object Detection with Learned Metadata from RAWs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=phWwTrrJFv": {
    "title": "Kinetics: Rethinking Test-Time Scaling Law",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M226WElHp5": {
    "title": "Scalable Exploration via Ensemble++",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=paiyYD81Wr": {
    "title": "Zero-Shot Performance Prediction for Probabilistic Scaling Laws",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6uwV6ytamU": {
    "title": "DualCnst: Enhancing Zero-Shot Out-of-Distribution Detection via Text-Image Consistency in Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jeen4x145W": {
    "title": "Improved Scaling Laws in Linear Regression via Data Reuse",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X8x70g0SSw": {
    "title": "Unveiling Extraneous Sampling Bias with Data Missing-Not-At-Random",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LZrRvYBqsJ": {
    "title": "Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NUaIywqBtK": {
    "title": "Neural Fractional Attention Differential Equations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lBdhx5nKOL": {
    "title": "A Statistical Theory of Contrastive Learning via Approximate Sufficient Statistics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o9iTaY21mq": {
    "title": "FSI-Edit: Frequency and Stochasticity Injection for Flexible Diffusion-Based Image Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6KlIzfkTfi": {
    "title": "Purity Law for Neural Routing Problem Solvers with Enhanced Generalizability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=peYBx7AiKw": {
    "title": "Learning to Route: Per-Sample Adaptive Routing for Multimodal Multitask Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9yusqX9DpR": {
    "title": "Self-Challenging Language Model Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l42UGsdrNn": {
    "title": "Zebra-Llama: Towards Extremely Efficient Hybrid Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z5KTxW5sJd": {
    "title": "From Replication to Redesign: Exploring Pairwise Comparisons for LLM-Based Peer Review",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ToMjBgXwhw": {
    "title": "Aligning Text to Image in Diffusion Models is Easier Than You Think",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tcisuhGsQZ": {
    "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2qd4lpXz7u": {
    "title": "How Memory in Optimization Algorithms Implicitly Modifies the Loss",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vbFZlM7nRK": {
    "title": "Towards Robust Zero-Shot Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yqBIKzTFT8": {
    "title": "MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q8QMhJFSk4": {
    "title": "RankMatch: A Novel Approach to Semi-Supervised Label Distribution Learning Leveraging Rank Correlation between Labels",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9FvWYqcNLa": {
    "title": "Isotropic Noise in Stochastic and Quantum Convex Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uhFx1RGD1g": {
    "title": "Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=57MeabE6QU": {
    "title": "Distributionally Robust Performative Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6xL4MRFeJo": {
    "title": "Manipulating 3D Molecules in a Fixed-Dimensional E(3)-Equivariant Latent Space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cGmcHJEFnY": {
    "title": "BlockDecoder: Boosting ASR Decoders with Context and Merger Modules",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QjJHUR0sXU": {
    "title": "Accelerating Multimodal Large Language Models via Dynamic Visual-Token Exit and the Empirical Findings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PllZG6K37g": {
    "title": "Information-theoretic Generalization Analysis for VQ-VAEs: A Role of Latent Variables",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UuHzyclzYR": {
    "title": "Learning Orthogonal Multi-Index Models: A Fine-Grained Information Exponent Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ividIRK8Ib": {
    "title": "Hierarchical Self-Attention: Generalizing Neural Attention Mechanics to Multi-Scale Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p5PFCVsXWf": {
    "title": "Empower Words: DualGround for Structured Phrase and Sentence-Level Temporal Grounding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lrwntEIcYj": {
    "title": "Direct Alignment with Heterogeneous Preferences",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ei20AX8EkP": {
    "title": "Measure-Theoretic Anti-Causal Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eK61hWzAAl": {
    "title": "RayFusion: Ray Fusion Enhanced Collaborative Visual Perception",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XwMDUND6iO": {
    "title": "MACS: Multi-Agent Reinforcement Learning for Optimization of Crystal Structures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kNXTUCksnh": {
    "title": "The Burden of Interactive Alignment with Inconsistent Preferences",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EgArbnS0BA": {
    "title": "AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I4fBSpDOha": {
    "title": "Focus-Then-Reuse: Fast Adaptation in Visual Perturbation Environments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cnG7h8ucip": {
    "title": "DeepASA: An Object-Oriented Multi-Purpose Network for Auditory Scene Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LVPq1j357N": {
    "title": "Learning Personalized Ad Impact via Contextual Reinforcement Learning under Delayed Rewards",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=twSE0WA1vh": {
    "title": "Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FDX7EB9CDv": {
    "title": "4D3R: Motion-Aware Neural Reconstruction and Rendering of Dynamic Scenes from Monocular Videos",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tDG6bY48ch": {
    "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following Models Need for Efficient Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QjnKsujXVG": {
    "title": "Chain of Execution Supervision Promotes General Reasoning in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=458m5RSMgJ": {
    "title": "ESCA: Enabling Seamless Codec Avatar Execution through Algorithm and Hardware Co-Optimization for Virtual Reality",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IDSTtDw4Cs": {
    "title": "SiriuS: Self-improving Multi-agent Systems via Bootstrapped Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eS3xJTjSgm": {
    "title": "Data-Free Model Extraction for Black-box Recommender Systems via Graph Convolutions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BL5r7hu9GD": {
    "title": "EnzyControl: Adding Functional and Substrate-Specific Control for Enzyme Backbone Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7k7cubl1iL": {
    "title": "CIDD: Collaborative Intelligence for Structure-Based Drug Design Empowered by LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8I5ENDN5ux": {
    "title": "Beyond Node-Centric Modeling: Sketching Signed Networks with Simplicial Complexes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jugw5alhnF": {
    "title": "MutualVPR: A Mutual Learning Framework for Resolving Supervision Inconsistencies via Adaptive Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YXcYD2nLmc": {
    "title": "Robust Ego-Exo Correspondence with Long-Term Memory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4QRoLzD11x": {
    "title": "From Softmax to Score: Transformers Can Effectively Implement In-Context Denoising Steps",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mk9ykil8eP": {
    "title": "HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=phC2nmJywQ": {
    "title": "Sharp Gap-Dependent Variance-Aware Regret Bounds for Tabular MDPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jj4NdJtXwp": {
    "title": "Geometry of Decision Making in Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7UbXEQNny7": {
    "title": "Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MWF1ZzYnxJ": {
    "title": "Conformal Linguistic Calibration: Trading-off between Factuality and Specificity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=myPtOTn3si": {
    "title": "Functional data analysis for multivariate distributions through Wasserstein slicing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KkONOZwvSU": {
    "title": "Defending Multimodal Backdoored Models by Repulsive Visual Prompt Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wEOmS8Aw1W": {
    "title": "Towards 3D Objectness Learning in an Open World",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ox3U97Svtl": {
    "title": "MPS-Prover: Advancing Stepwise Theorem Proving by Multi-Perspective Search and Data Curation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QQhQIqons0": {
    "title": "SEC-bench: Automated Benchmarking of LLM Agents on Real-World Software Security Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EeIb2ba8F4": {
    "title": "Characterization and Learning of Causal Graphs from Hard Interventions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FDruZlKWUb": {
    "title": "Tracing the Representation Geometry of Language Models from Pretraining to Post-training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VuVhgEiu20": {
    "title": "TTRL: Test-Time Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kke9TwtKi0": {
    "title": "Subspace Networks: Scaling Decentralized Training with Communication-Efficient Model Parallelism",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x3qnrhfhX0": {
    "title": "Mixtures of Subspaces for Bandwidth Efficient Context Parallel Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HkumwkgqJo": {
    "title": "SPMDM: Enhancing Masked Diffusion Models through Simplifing Sampling Path",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=73EcEGo6LQ": {
    "title": "Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models with Conditional Score Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KD4wgunbhO": {
    "title": "Understanding Adam Requires Better Rotation Dependent Assumptions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C7BIQRM57T": {
    "title": "Momentum Multi-Marginal Schrödinger Bridge Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3aNvX9TQTo": {
    "title": "Disentangling Superpositions: Interpretable Brain Encoding Model with Sparse Concept Atoms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hesM5BWtOJ": {
    "title": "Staggered Environment Resets Improve Massively Parallel On-Policy Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y2wt5c1Uhu": {
    "title": "DreamLight: Towards Harmonious and Consistent Image Relighting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t7LKc0MMW6": {
    "title": "Alternating Gradient Flows: A Theory of Feature Learning in Two-layer Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kHLEYGI9x9": {
    "title": "Robust Equilibria in Continuous Games: From Strategic to Dynamic Robustness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UKt31LbRPI": {
    "title": "When Are Concepts Erased From Diffusion Models?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IGKluZvI6J": {
    "title": "Nonparametric Quantile Regression with ReLU-Activated Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9FAm023EmI": {
    "title": "Hierarchical Demonstration Order Optimization for Many-shot In-Context Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FNYFSolinQ": {
    "title": "Unifying Symbolic Music Arrangement: Track-Aware Reconstruction and Structured Tokenization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KXMIIVUB9U": {
    "title": "Self-Improving Embodied Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M5GLj4Fgvp": {
    "title": "Prompt Tuning Transformers for Data Memorization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pFL7RThdFN": {
    "title": "Network two-sample test for block models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1ZuzFUMtx6": {
    "title": "SemCoT: Accelerating Chain-of-Thought Reasoning through Semantically-Aligned Implicit Tokens",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bBUUOQI0N6": {
    "title": "In-Context Learning Strategies Emerge Rationally",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZKYxfU8de7": {
    "title": "Optimality and NP-Hardness of Transformers in Learning Markovian Dynamical Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=un1TRwNgiv": {
    "title": "Thinking vs. Doing: Improving Agent Reasoning by Scaling Test-Time Interaction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OfMCET0hqJ": {
    "title": "Time Reversal Symmetry for Efficient Robotic Manipulations in Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hNh3V1DXs5": {
    "title": "On the Sample Complexity Bounds of Bilevel Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RjkAnKu6Hi": {
    "title": "QuARI: Query Adaptive Retrieval Improvement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3pF7rt9fQM": {
    "title": "Correlated Low-Rank Adaptation for ConvNets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wpdLr5GmTz": {
    "title": "Beyond Prediction: Managing the Repercussions of Machine Learning Applications",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WAFD6VYIEa": {
    "title": "Offline RL by Reward-Weighted Fine-Tuning for Conversation Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cot6mZPkWo": {
    "title": "CAT: Content-Adaptive Image Tokenization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6VoDizmIoY": {
    "title": "H3D-DGS: Exploring Heterogeneous 3D Motion Representation for Deformable 3D Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MgN8Px0NA5": {
    "title": "Keeping an Eye on LLM Unlearning: The Hidden Risk and Remedy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gXoMU9YYdY": {
    "title": "Sketch-Augmented Features Improve Learning Long-Range Dependencies in Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KIximJjqjV": {
    "title": "Quality-Driven Curation of Remote Sensing Vision-Language Data via Learned Scoring Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u0YyIHljXO": {
    "title": "Objective Soups: Multilingual Multi-Task Modeling for Speech Processing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pydeKTMrJr": {
    "title": "Cross City Traffic Flow Generation via Retrieval Augmented Diffusion Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5JasBI1p8p": {
    "title": "Adaptive Sigmoid Clipping for Balancing the Direction–Magnitude Mismatch Trade-off in Differentially Private Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ravS5h8MNg": {
    "title": "HyperGraphRAG: Retrieval-Augmented Generation via Hypergraph-Structured Knowledge Representation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2845H8Ua5D": {
    "title": "ForceVLA: Enhancing VLA Models with a Force-aware MoE for Contact-rich Manipulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hVYp0WzyLK": {
    "title": "UniDomain: Pretraining a Unified PDDL Domain from Real-World Demonstrations for Generalizable Robot Task Planning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aCPFvEg22L": {
    "title": "Gaussian Approximation and Concentration of Constant Learning-Rate Stochastic Gradient Descent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ur295YVtmt": {
    "title": "ReMA: Learning to Meta-Think for LLMs with Multi-agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B5UvFCiisp": {
    "title": "GenIR: Generative Visual Feedback for Mental Image Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MuxBO5f8mL": {
    "title": "PointTruss: K-Truss for Point Cloud Registration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n4V3MSqK77": {
    "title": "Agentic Plan Caching: Test-Time Memory for Fast and Cost-Efficient LLM Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U7ab14f4YQ": {
    "title": "Split Gibbs Discrete Diffusion Posterior Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oDcAGSXZZP": {
    "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0QNmAvQQqj": {
    "title": "GFM-RAG: Graph Foundation Model for Retrieval Augmented Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JYB6wFcbky": {
    "title": "Graph-based Symbolic Regression with Invariance and Constraint Encoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rIwUDnRGky": {
    "title": "Language‑Bias‑Resilient Visual Question Answering via Adaptive Multi‑Margin Collaborative Debiasing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aE0bCvXXBt": {
    "title": "Transformers Provably Learn Chain-of-Thought Reasoning with Length Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mgURpQ4NXO": {
    "title": "DoDo-Code: an Efficient Levenshtein Distance Embedding-based Code for 4-ary IDS Channel",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1GIQOV3NAj": {
    "title": "Tru-POMDP: Task Planning Under Uncertainty via Tree of Hypotheses and Open-Ended POMDPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IL1wvzOgqD": {
    "title": "Towards A Translative Model of Sperm Whale Vocalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hzMkfIrdDT": {
    "title": "Know What You Don't Know: Uncertainty Calibration of Process Reward Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r029oluPS7": {
    "title": "DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway Global-Local Fusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ygHWfrwFmO": {
    "title": "CMoB: Modality Valuation via Causal Effect for Balanced Multimodal Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yJpBVE4vfo": {
    "title": "Mint: A Simple Test-Time Adaptation of Vision-Language Models against Common Corruptions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3SUkvb8PRo": {
    "title": "FlowPrune: Accelerating Attention Flow Calculation by Pruning Flow Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b6ASJBXtgP": {
    "title": "Towards Large-Scale In-Context Reinforcement Learning by Meta-Training in Randomized Worlds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KfZm1bkS8C": {
    "title": "Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uXKgVqYTJ2": {
    "title": "Learning Expandable and Adaptable Representations for Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sedMS4kPz2": {
    "title": "On the Complexity of Finding Stationary Points in Nonconvex Simple Bilevel Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iqsjzVqmWF": {
    "title": "Statistics Caching Test-Time Adaptation for Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xoNrbfbekM": {
    "title": "Lost in Latent Space: An Empirical Study of Latent Diffusion Models for Physics Emulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kgzRy6nD6D": {
    "title": "Selective Learning for Deep Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cldPfIoRiA": {
    "title": "G 2 M : A Generalized Gaussian Mirror Method to Boost Feature Selection Power",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gf4oPoluAV": {
    "title": "TAI3: Testing Agent Integrity in Interpreting User Intent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y8Hv7EdcRF": {
    "title": "OVS Meets Continual Learning: Towards Sustainable Open-Vocabulary Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RZp2YKtg4q": {
    "title": "Shaping Sequence Attractor Schema in Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uG9F00zKJF": {
    "title": "Re-coding for Uncertainties: Edge-awareness Semantic Concordance for Resilient Event-RGB Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fhuqIxoPcr": {
    "title": "RODS: Robust Optimization Inspired Diffusion Sampling for Detecting and Reducing Hallucination in Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EdKl4PulMX": {
    "title": "Complexity Scaling Laws for Neural Models using Combinatorial Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8KuKSKLott": {
    "title": "Latent Zoning Network: A Unified Principle for Generative Modeling, Representation Learning, and Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QEU047bE8p": {
    "title": "Mamba Modulation: On the Length Generalization of Mamba Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OQFfM96ZcD": {
    "title": "Token Perturbation Guidance for Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NQIxuYM9TR": {
    "title": "KAIROS: Scalable Model-Agnostic Data Valuation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9fUr5iFU9j": {
    "title": "Confounding Robust Deep Reinforcement Learning: A Causal Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tle6DK5Tad": {
    "title": "Balanced Token Pruning: Accelerating Vision Language Models Beyond Local Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vM4PIjsJDG": {
    "title": "Anomaly Detection by an Ensemble of Random Pairs of Hyperspheres",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vqoiuHbOsG": {
    "title": "Posterior Contraction for Sparse Neural Networks in Besov Spaces with Intrinsic Dimensionality",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FS5bIryxEx": {
    "title": "SpectraLDS: Provable Distillation for Linear Dynamical Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7PP2yyeDv6": {
    "title": "Aha! - Predicting What Matters Next: Online Highlight Detection Without Looking Ahead",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mN3CMpfWR6": {
    "title": "SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7cirmREfbc": {
    "title": "Test Time Scaling for Neural Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HIK0cjRp3f": {
    "title": "Stackelberg Learning with Outcome-based Payment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZdmmOAN4h3": {
    "title": "Breaking the Frozen Subspace: Importance Sampling for Low-Rank Optimization in LLM Pretraining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QKo4c3LAz3": {
    "title": "Understanding the Evolution of the Neural Tangent Kernel at the Edge of Stability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0wC6h6KGH8": {
    "title": "Machine Unlearning in 3D Generation: A Perspective-Coherent Acceleration Framework",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sLNz60fJFF": {
    "title": "Any Large Language Model Can Be a Reliable Judge: Debiasing with a Reasoning-based Bias Detector",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o0AASFieVc": {
    "title": "VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EOgaGafwv5": {
    "title": "GST-UNet: A Neural Framework for Spatiotemporal Causal Inference with Time-Varying Confounding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TubIV2Q4Ck": {
    "title": "Coupled Data and Measurement Space Dynamics for Enhanced Diffusion Posterior Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=15Bs3nTAYR": {
    "title": "Convergence Theorems for Entropy-Regularized and Distributional Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eHm1EdLypn": {
    "title": "Robust Contextual Pricing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bjUDrU4wPv": {
    "title": "QiMeng-MuPa: Mutual-Supervised Learning for Sequential-to-Parallel Code Translation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3GPB0zbcSi": {
    "title": "Improved Regret and Contextual Linear Extension for Pandora's Box and Prophet Inequality",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vTJFQu5YXz": {
    "title": "Tackling Feature-Classifier Mismatch in Federated Learning via Prompt-Driven Feature Transformation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GrDEV4InKZ": {
    "title": "What Matters in Data for DPO?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GJbrk55Bt8": {
    "title": "List-Level Distribution Coupling with Applications to Speculative Decoding and Lossy Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T3DF4yxcPz": {
    "title": "Multiscale guidance of protein structure prediction with heterogeneous cryo-EM data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4xGJZkdjCU": {
    "title": "On Fairness of Unified Multimodal Large Language Model for Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6rpy7X1Of8": {
    "title": "Delving into Large Language Models for Effective Time-Series Anomaly Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QjmRIgTcU8": {
    "title": "REP: Resource-Efficient Prompting for Rehearsal-Free Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BAYTbpUSrF": {
    "title": "Stochastic Regret Guarantees for Online Zeroth- and First-Order Bilevel Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4IIeoaNZ3a": {
    "title": "Embedding Principle of Homogeneous Neural Network for Classification Problem",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Em9QmNobh0": {
    "title": "Principled Fine-tuning of LLMs from User-Edits: A Medley of Preference, Supervision, and Reward",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RjZUldI6Qk": {
    "title": "WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SVGkn64X4m": {
    "title": "Automatic Visual Instrumental Variable Learning for Confounding-Resistant Domain Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c8AjdgdHnD": {
    "title": "DISCO: Disentangled Communication Steering for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xs5aoDH3HX": {
    "title": "Equi-mRNA: Protein Translation Equivariant Encoding for mRNA Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wOAasEVZ0L": {
    "title": "Active Target Discovery under Uninformative Priors: The Power of Permanent and Transient Memory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PymOnHw4Ty": {
    "title": "TS-RAG: Retrieval-Augmented Generation based Time Series Foundation Models are Stronger Zero-Shot Forecaster",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tIR9Naukr3": {
    "title": "EUGens: Efficient, Unified and General Dense Layers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jj0nJQYFlW": {
    "title": "Simultaneous Modeling of Protein Conformation and Dynamics via Autoregression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MBQZwQ6vFd": {
    "title": "A Minimalistic Unified Framework for Incremental Learning across Image Restoration Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JZ1fVVS3uk": {
    "title": "High-order Interactions Modeling for Interpretable Multi-Agent Q-Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yPDNQvuyYM": {
    "title": "PDPO: Parametric Density Path Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rl2aAyJfQz": {
    "title": "Regret Analysis of Average-Reward Unichain MDPs via an Actor-Critic Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uO0EHn4EmE": {
    "title": "Learning Repetition-Invariant Representations for Polymer Informatics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gwxoQ8MtZD": {
    "title": "Online Feedback Efficient Active Target Discovery in Partially Observable Environments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zytITzY4IW": {
    "title": "Private Zeroth-Order Optimization with Public Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a9MfGUHjF8": {
    "title": "Ada-R1: Hybrid-CoT via Bi-Level Adaptive Reasoning Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PdKhoj6goO": {
    "title": "Handling Missing Responses under Cluster Dependence with Applications to Language Model Evaluation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tnTM6JJuLi": {
    "title": "What Happens During the Loss Plateau? Understanding Abrupt Learning in Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R3xbcRIzUd": {
    "title": "Role-aware Multi-agent Reinforcement Learning for Coordinated Emergency Traffic Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IMzol5Ywsr": {
    "title": "A Single-Loop First-Order Algorithm for Linearly Constrained Bilevel Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MOAmJKj2Uc": {
    "title": "Towards Straggler-Resilient Split Federated Learning: An Unbalanced Update Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JenfC3ovzU": {
    "title": "Amortized Sampling with Transferable Normalizing Flows",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FsgwcrJWp8": {
    "title": "VisualLens: Personalization through Task-Agnostic Visual History",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eXO6g7BmOA": {
    "title": "OSVI-WM: One-Shot Visual Imitation for Unseen Tasks using World-Model-Guided Trajectory Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uRJ8WAJxHC": {
    "title": "Leveraging semantic similarity for experimentation with AI-generated treatments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qAggjeV2JO": {
    "title": "InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gOzcYNLKvr": {
    "title": "Localized Data Shapley: Accelerating Valuation for Nearest Neighbor Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YOv9CANvDv": {
    "title": "Understanding and Improving Fast Adversarial Training against l 0 Bounded Perturbations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JsNUE84Hxi": {
    "title": "Self-Adapting Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xd7cXU2P4G": {
    "title": "From Noise to Narrative: Tracing the Origins of Hallucinations in Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kYisDXzTk7": {
    "title": "Near-Optimal Sample Complexity for Online Constrained MDPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=63ljkCGMhE": {
    "title": "VGGT-SLAM: Dense RGB SLAM Optimized on the SL(4) Manifold",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KXOn2Z6bEI": {
    "title": "Shapley-Based Data Valuation for Weighted k -Nearest Neighbors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VS9N6q6b0k": {
    "title": "Closed-Form Training Dynamics Reveal Learned Features and Linear Structure in Word2Vec-like Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AzOUIMzDxC": {
    "title": "Few-Shot Knowledge Distillation of LLMs With Counterfactual Explanations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=96liIPUPXG": {
    "title": "Self supervised learning for in vivo localization of microelectrode arrays using raw local field potential",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oAbaGU9N1X": {
    "title": "A Scalable, Causal, and Energy Efficient Framework for Neural Decoding with Spiking Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tdMWo3jB21": {
    "title": "Generative Graph Pattern Machine",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PxgIElCohI": {
    "title": "Fantastic Features and Where to Find Them: A Probing Method to combine Features from Multiple Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t04casiy7F": {
    "title": "Structured Spectral Reasoning for Frequency-Adaptive Multimodal Recommendation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JbvSQm5h1l": {
    "title": "Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=di43GKTaPz": {
    "title": "Technical Debt in In-Context Learning: Diminishing Efficiency in Long Context",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hsD30Qljz0": {
    "title": "Causality-Induced Positional Encoding for Transformer-Based Representation Learning of Non-Sequential Features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sGY0TisiMB": {
    "title": "Individual Fairness In Strategic Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bFXbLQzRoZ": {
    "title": "Power Lines: Scaling laws for weight decay and batch size in LLM pre-training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BeSFxKX3Po": {
    "title": "Identifying interactions across brain areas while accounting for individual-neuron dynamics with a Transformer-based variational autoencoder",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uih8cWS3JF": {
    "title": "Improved Regret Bounds for Linear Bandits with Heavy-Tailed Rewards",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e0Pq8EZx37": {
    "title": "Optimal community detection in dense bipartite graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=soMxYQaMnF": {
    "title": "Generator-Mediated Bandits: Thompson Sampling for GenAI-Powered Adaptive Interventions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wxh5Xz7NpJ": {
    "title": "Remarkable Robustness of LLMs: Stages of Inference?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L3UfIfNxb7": {
    "title": "Partial Correlation Network Estimation by Semismooth Newton Methods",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NsPilIfypy": {
    "title": "Uncoupled and Convergent Learning in Monotone Games under Bandit Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LwPjJHVWSn": {
    "title": "Learning Without Augmenting: Unsupervised Time Series Representation Learning via Frame Projections",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RIkHzQbpeR": {
    "title": "Conformal Prediction Beyond the Horizon: Distribution-Free Inference for Policy Evaluation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Fs64f1Li0": {
    "title": "Deep Tree Tensor Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mDjEKAwJOF": {
    "title": "BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b7Ka31HvXU": {
    "title": "Aligning Evaluation with Clinical Priorities: Calibration, Label Shift, and Error Costs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k1fbdnwjCH": {
    "title": "Heterogeneous Graph Transformers for Simultaneous Mobile Multi-Robot Task Allocation and Scheduling under Temporal Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EyOtIOmMUh": {
    "title": "Synthesizing Photorealistic and Dynamic Urban Environments for Multimodal Robot Navigation and Collaboration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4TP1SuInE5": {
    "title": "Anti-Aliased 2D Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UG1723eoKq": {
    "title": "SPOT-Trip: Dual-Preference Driven Out-of-Town Trip Recommendation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zNqDCSokDR": {
    "title": "Bootstrap Off-policy with World Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bkyl98YQRa": {
    "title": "A Data-Driven Prism: Multi-View Source Separation with Diffusion Model Priors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HDeIb67lJe": {
    "title": "Informed Correctors for Discrete Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sNTqqdPVSv": {
    "title": "Conformal Prediction for Ensembles: Improving Efficiency via Score-Based Aggregation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ew8bJkSt3g": {
    "title": "From Programs to Poses: Factored Real-World Scene Generation via Learned Program Libraries",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1LPPMAUlaT": {
    "title": "Automated Composition of Agents: A Knapsack Approach for Agentic Component Selection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6h7HLx1kbH": {
    "title": "On Union-Closedness of Language Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hVR3023UP2": {
    "title": "Combining Cost Constrained Runtime Monitors for AI Safety",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=miCXNqXyVS": {
    "title": "Normalization in Attention Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z2lWGBx2v2": {
    "title": "Is Your Diffusion Model Actually Denoising?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=naAUSeyoZ7": {
    "title": "Language Models (Mostly) Know When to Stop Reading",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mVNryhp1q0": {
    "title": "The Complexity of Finding Local Optima in Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ll5miDx8KB": {
    "title": "From Flat to Hierarchical: Extracting Sparse Representations with Matching Pursuit",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QOjUNzOkRN": {
    "title": "Parallelizing MCMC Across the Sequence Length",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xcqrAzYzCj": {
    "title": "Revisiting Bi-Linear State Transitions in Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T85ADT8a2y": {
    "title": "A Unified Framework for Fair Graph Generation: Theoretical Guarantees and Empirical Advances",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=18GBPdnuXs": {
    "title": "Adaptive Quantization in Generative Flow Networks for Probabilistic Sequential Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aVWVx7cqIE": {
    "title": "Computable universal online learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cWnZLIdeKn": {
    "title": "Scaling Laws for Robust Comparison of Open Foundation Language-Vision Models and Datasets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rem4dgVrFg": {
    "title": "Unifying Re-Identification, Attribute Inference, and Data Reconstruction Risks in Differential Privacy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nM9j2F7jL3": {
    "title": "Identifying Macro Causal Effects in C-DMGs over DMGs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AiZxn84Wdo": {
    "title": "Training Language Models to Reason Efficiently",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XFP5jntOdx": {
    "title": "SymRTLO: Enhancing RTL Code Optimization with LLMs and Neuron-Inspired Symbolic Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3nza35A6I4": {
    "title": "Training Language Models to Generate Quality Code with Program Analysis Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wUoK24u4x7": {
    "title": "Pinpointing Attention-Causal Communication in Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fru52tkjHf": {
    "title": "ASGO: Adaptive Structured Gradient Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=efuY2yYC1U": {
    "title": "DPA: A one-stop metric to measure bias amplification in classification datasets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5YMZfufpfY": {
    "title": "Improving Energy Natural Gradient Descent through Woodbury, Momentum, and Randomization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IKxKs3rF9V": {
    "title": "4KAgent: Agentic Any Image to 4K Super-Resolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LIGBnhb83e": {
    "title": "Hawaii: Hierarchical Visual Knowledge Transfer for Efficient Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6wbykApw7A": {
    "title": "Pareto-Optimal Energy Alignment for Designing Nature-Like Antibodies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oCXyxhgCiZ": {
    "title": "BrainEC-LLM: Brain Effective Connectivity Estimation by Multiscale Mixing LLM",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tD9mebUimw": {
    "title": "Truthful Aggregation of LLMs with an Application to Online Advertising",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MKEDsVWHd0": {
    "title": "AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4E3I17pNEl": {
    "title": "Yggdrasil: Bridging Dynamic Speculation and Static Runtime for Latency-Optimal Tree-Based LLM Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z6d5MRMDNf": {
    "title": "Faithful Group Shapley Value",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=inN0WrBJVc": {
    "title": "How Many Domains Suffice for Domain Generalization? A Tight Characterization via the Domain Shattering Dimension",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=79GwCnXYej": {
    "title": "Tight Bounds for Answering Adaptively Chosen Concentrated Queries",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zswylB4Wnt": {
    "title": "RAST: Reasoning Activation in LLMs via Small-model Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z6mwI6VcHA": {
    "title": "A compressive-expressive communication framework for compositional representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XQrGTggLvT": {
    "title": "LLM Strategic Reasoning: Agentic Study through Behavioral Game Theory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sCgYsBVIVG": {
    "title": "Multi-modal contrastive learning adapts to intrinsic dimensions of shared latent variables",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zho5kN8jTn": {
    "title": "Optimal Adjustment Sets for Nonparametric Estimation of Weighted Controlled Direct Effect",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fe5Wc0fZUT": {
    "title": "Automatic Auxiliary Task Selection and Adaptive Weighting Boost Molecular Property Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9V2SVEl1vP": {
    "title": "When Models Know More Than They Can Explain: Quantifying Knowledge Transfer in Human-AI Collaboration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RALtozQipi": {
    "title": "OmniDraft: A cross-vocabulary, online adaptive drafter for on-device speculative decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wORrUUmffG": {
    "title": "Distance Adaptive Beam Search for Provably Accurate Graph-Based Nearest Neighbor Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oj5tVkjbHD": {
    "title": "BraVE: Offline Reinforcement Learning for Discrete Combinatorial Action Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hwRVeHkLvO": {
    "title": "Reliably detecting model failures in deployment without labels",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ceb788Uigr": {
    "title": "On the Convergence of Stochastic Smoothed Multi-Level Compositional Gradient Descent Ascent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wbifKziqd2": {
    "title": "DIsoN: Decentralized Isolation Networks for Out-of-Distribution Detection in Medical Imaging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VJn4UKnEYS": {
    "title": "Subsampled Ensemble Can Improve Generalization Tail Exponentially",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IFjjzfkC65": {
    "title": "MoRIC: A Modular Region-based Implicit Codec for Image Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cNMYAzhdn5": {
    "title": "Geometry-Aware Edge Pooling for Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZK7dqwWxex": {
    "title": "Learning to Solve Complex Problems via Dataset Decomposition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B23WUS3W8Z": {
    "title": "In Silico Mapping of Visual Categorical Selectivity Across the Whole Brain",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j42rziWq1n": {
    "title": "ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual Perception in VLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gjiCml2CNG": {
    "title": "Enforcing Hard Linear Constraints in Deep Learning Models with Decision Rules",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pZM3BsyzbM": {
    "title": "Empowering Decision Trees via Shape Function Branching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nTfhNThKX2": {
    "title": "Sparta Alignment: Collectively Aligning Multiple Language Models through Combat",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gMi6LHMzDP": {
    "title": "ProtoPairNet: Interpretable Regression through Prototypical Pair Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fcLVqvyiqV": {
    "title": "HMARL-CBF – Hierarchical Multi-Agent Reinforcement Learning with Control Barrier Functions for Safety-Critical Autonomous Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gvh6sU0uUt": {
    "title": "Least squares variational inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K7s0lIjhWx": {
    "title": "Improving the Generation and Evaluation of Synthetic Data for Downstream Medical Causal Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Toy96yYopR": {
    "title": "Enhancing Tactile-based Reinforcement Learning for Robotic Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=14irEkV01l": {
    "title": "Gene Regulatory Network Inference in the Presence of Selection Bias and Latent Confounders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zYEZ5KqtDO": {
    "title": "Heterogeneous Swarms: Jointly Optimizing Model Roles and Weights for Multi-LLM Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WkztaHpjt1": {
    "title": "Hankel Singular Value Regularization for Highly Compressible State Space Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vCTnwpAcma": {
    "title": "Calibrating Translation Decoding with Quality Estimation on LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TMSNlEKKeM": {
    "title": "Avoiding exp(R) scaling in RLHF through Preference-based Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EYC29OdxgA": {
    "title": "Structure-Aware Fusion with Progressive Injection for Multimodal Molecular Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NM5dprhsGK": {
    "title": "Learning from Demonstrations via Capability-Aware Goal Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7TY89cqLfE": {
    "title": "Sound Logical Explanations for Mean Aggregation Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jSeWBdH0Xx": {
    "title": "Better Tokens for Better 3D: Advancing Vision-Language Modeling in 3D Medical Imaging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uVxQEIgXfL": {
    "title": "3D Gaussian Flats: Hybrid 2D/3D Photometric Scene Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s3Uk3lrfjy": {
    "title": "INC: An Indirect Neural Corrector for Auto-Regressive Hybrid PDE Solvers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MKLcSmSTQI": {
    "title": "Imitation Learning with Temporal Logic Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ldni3xeyIa": {
    "title": "Topology-Aware Conformal Prediction for Stream Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Eu2Tqcvxih": {
    "title": "Efficient Multimodal Dataset Distillation via Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2zHrRV4J1O": {
    "title": "Random Search Neural Networks for Efficient and Expressive Graph Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=28qUA2bSe5": {
    "title": "AutoVLA: A Vision-Language-Action Model for End-to-End Autonomous Driving with Adaptive Reasoning and Reinforcement Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W3JnXa9mW2": {
    "title": "CellCLIP - Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oN5YVZ9JeF": {
    "title": "T-SHIRT: Token-Selective Hierarchical Data Selection for Instruction Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zl4FR39Ibh": {
    "title": "Learning Cocoercive Conservative Denoisers via Helmholtz Decomposition for Poisson Imaging Inverse Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6H4tDTHalg": {
    "title": "Eliciting Reasoning in Language Models with Cognitive Tools",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CGx4XU9rCA": {
    "title": "Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CGLoEvCllI": {
    "title": "On the Entropy Calibration of Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gnRgO4ZHrN": {
    "title": "Understanding Fairness and Prediction Error through Subspace Decomposition and Influence Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=REKmD78Z0J": {
    "title": "Near-Exponential Savings for Population Mean Estimation with Active Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vJtnJfS2mQ": {
    "title": "Dimensional Collapse in VQVAEs: Evidence and Remedies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A5Y8Uh5Szl": {
    "title": "Exploration from a Primal-Dual Lens: Value-Incentivized Actor-Critic Methods for Sample-Efficient Online RL",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8t89BYV781": {
    "title": "RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w1yccSuVtg": {
    "title": "Preference-Based Dynamic Ranking Structure Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1Gxt8c8pC": {
    "title": "Valid Selection among Conformal Sets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KXCJjzbmMh": {
    "title": "Nearly-Linear Time Private Hypothesis Selection with the Optimal Approximation Factor",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LlEJZ7x0R8": {
    "title": "Learning non-equilibrium diffusions with Schrödinger bridges: from exactly solvable to simulation-free",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W5Ht05jF4c": {
    "title": "Diffusion Beats Autoregressive in Data-Constrained Settings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Bqui2s3xFi": {
    "title": "Latent Space Factorization in LoRA",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SPLtr5UThn": {
    "title": "Fully Dynamic Algorithms for Chamfer Distance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YLlpF71IZJ": {
    "title": "Robust Distortion-Free Watermark for Autoregressive Audio Generation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9YNJ03jYsU": {
    "title": "AltLoRA: Towards Better Gradient Approximation in Low-Rank Adaptation with Alternating Projections",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LPWzV8zrgj": {
    "title": "REMI: Reconstructing Episodic Memory During Internally Driven Path Planning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HvklLrtyxK": {
    "title": "Doubly Robust Alignment for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wIM0y07NGX": {
    "title": "MESS+: Dynamically Learned Inference-Time LLM Routing in Model Zoos with Service Level Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0okFLZvtKs": {
    "title": "TensorRL-QAS: Reinforcement learning with tensor networks for improved quantum architecture search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1n5TJh3LEb": {
    "title": "Low-Rank Graphon Learning for Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nCAdkkAeR9": {
    "title": "Understanding Contrastive Learning via Gaussian Mixture Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ggtc3xXff2": {
    "title": "AmorLIP: Efficient Language-Image Pretraining via Amortization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XDisynd63Y": {
    "title": "The Quotient Bayesian Learning Rule",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MMYTA3v66p": {
    "title": "Causal Discovery and Inference through Next-Token Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n1hYQmpCSW": {
    "title": "PRSformer: Disease Prediction from Million-Scale Individual Genotypes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2w7nx3U07Q": {
    "title": "RCCDA: Adaptive Model Updates in the Presence of Concept Drift under a Constrained Resource Budget",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sEjp1MGMDZ": {
    "title": "In-Context Compositional Learning vis Sparse Coding Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uNqTxj5brQ": {
    "title": "Fast Inference for Augmented Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q0g3CPS96G": {
    "title": "Consistent Story Generation: Unlocking the Potential of Zigzag Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nbMeRvNb7A": {
    "title": "Nested Learning: The Illusion of Deep Learning Architectures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hzM0FYJXLN": {
    "title": "Diffusion Models Meet Contextual Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TgczQwE1Iu": {
    "title": "Manipulating Feature Visualizations with Gradient Slingshots",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D5TSSVkTBA": {
    "title": "Exploiting LLMs for Automatic Hypothesis Assessment via a Logit-Based Calibrated Prior",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oQbTbio99T": {
    "title": "Understanding and Enhancing Mask-Based Pretraining towards Universal Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tvEE9KQcLi": {
    "title": "Offline imitation learning in Q π -realizable MDPs without expert realizability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B6DhWv3DZo": {
    "title": "Convergent Functions, Divergent Forms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O1yi3lqp97": {
    "title": "The Boundaries of Fair AI in Medical Image Prognosis: A Causal Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=49Rc51iCso": {
    "title": "Mitigating Overthinking in Large Reasoning Models via Manifold Steering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qF5IrJfJDS": {
    "title": "Scaling Epidemic Inference on Contact Networks: Theory and Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ujg9XKwqWT": {
    "title": "Synthetic-powered predictive inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pzHuesCvcO": {
    "title": "Align Your Flow: Scaling Continuous-Time Flow Map Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K9PcELos78": {
    "title": "ChA-MAEViT: Unifying Channel-Aware Masked Autoencoders and Multi-Channel Vision Transformers for Improved Cross-Channel Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ERQRSnqLRb": {
    "title": "Generative Distribution Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yvGnOqy0Zf": {
    "title": "Logical Expressiveness of Graph Neural Networks with Hierarchical Node Individualization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vbn7VvloTd": {
    "title": "Statistical Inference under Performativity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8owMKkQIy0": {
    "title": "Transferring Causal Effects using Proxies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MtdC1XS6RN": {
    "title": "Inferring stochastic dynamics with growth from cross-sectional data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wiWNpjcYWH": {
    "title": "Performative Risk Control: Calibrating Models for Reliable Deployment under Performativity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qmvEe6nd2I": {
    "title": "Hephaestus: Mixture Generative Modeling with Energy Guidance for Large-scale QoS Degradation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ToEgGjClB9": {
    "title": "CPO: Condition Preference Optimization for Controllable Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NNiwGUY50Y": {
    "title": "ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WF9AyBTIjw": {
    "title": "Adversarial generalization of unfolding (model-based) networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j8XnFfTvXF": {
    "title": "KL-Regularized RLHF with Multiple Reference Models: Exact Solutions and Sample Complexity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1ajMY3nUKg": {
    "title": "Tight Lower Bounds and Improved Convergence in Performative Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VwjHVS4zZN": {
    "title": "Small Resamples, Sharp Guarantees: Convergence Rates for Resampled Studentized Quantile Estimators",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CMmKcHFDKL": {
    "title": "Theoretically Grounded Framework for LLM Watermarking: A Distribution-Adaptive Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eSIRst0WVy": {
    "title": "Don't Trade Off Safety: Diffusion Regularization for Constrained Offline RL",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bd8kppxyB3": {
    "title": "Revisiting Glorot Initialization for Long-Range Linear Recurrences",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ENYvdnyhLl": {
    "title": "Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g2XE40zTrj": {
    "title": "Towards Unified and Lossless Latent Space for 3D Molecular Latent Diffusion Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eSZcf8LCxz": {
    "title": "BecomingLit: Relightable Gaussian Avatars with Hybrid Neural Shading",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xiP7TZG58o": {
    "title": "Context-Aware Regularization with Markovian Integration for Attention-Based Nucleotide Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HOa6LTIjyn": {
    "title": "Prediction-Powered Causal Inferences",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oum1txoy1D": {
    "title": "FSNet: Feasibility-Seeking Neural Network for Constrained Optimization with Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IOO3nJtvbY": {
    "title": "LORE: Lagrangian-Optimized Robust Embeddings for Visual Encoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sfz8ueAT6E": {
    "title": "Follow-the-Perturbed-Leader Nearly Achieves Best-of-Both-Worlds for the m-Set Semi-Bandit Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fCCzufIJHu": {
    "title": "Probably Approximately Precision and Recall Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wU8IKGLpbi": {
    "title": "Graph Persistence goes Spectral",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IByOCSsDU5": {
    "title": "Bridging the gap to real-world language-grounded visual concept learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LUvu0H3uIG": {
    "title": "DGH: Dynamic Gaussian Hair",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y9nxhKcgAA": {
    "title": "On the VC dimension of deep group convolutional neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wH3F1ZoK70": {
    "title": "Salient Concept-Aware Generative Data Augmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fiXCcxGysZ": {
    "title": "Token Embeddings Violate the Manifold Hypothesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ySFDPoiANu": {
    "title": "Execution Guided Line-by-Line Code Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oTocSpfYvJ": {
    "title": "DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KxhCJc8BOg": {
    "title": "Riemannian Proximal Sampler for High-accuracy Sampling on Manifolds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bTclOYRfYJ": {
    "title": "Bézier Splatting for Fast and Differentiable Vector Graphics Rendering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z5FGi0vyCr": {
    "title": "Composing Linear Layers from Irreducibles",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cl4PCrKrc5": {
    "title": "Image Super-Resolution with Guarantees via Conformalized Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lSLSzYuyfX": {
    "title": "μ PC: Scaling Predictive Coding to 100+ Layer Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YPsJha5HXQ": {
    "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hqmdhrnzfp": {
    "title": "Less Greedy Equivalence Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8JdPqAMpi4": {
    "title": "Are Greedy Task Orderings Better Than Random in Continual Linear Regression?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lvDsc5lxma": {
    "title": "Pruning Spurious Subgraphs for Graph Out-of-Distribution Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p324ryBKTc": {
    "title": "Time Series Generation Under Data Scarcity: A Unified Generative Modeling Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qhYp0kHjUS": {
    "title": "PREAMBLE: Private and Efficient Aggregation via Block Sparse Vectors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0dp90Bq8r8": {
    "title": "Non-stationary Equivariant Graph Neural Networks for Physical Dynamics Simulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0XKZFK4hQt": {
    "title": "Human Texts Are Outliers: Detecting LLM-generated Texts via Out-of-distribution Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UpzRoxVbQD": {
    "title": "Tight Bounds on the Distortion of Randomized and Deterministic Distributed Voting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gLU0UV85Kv": {
    "title": "Statistical Inference for Gradient Boosting Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5B3cyuEpOc": {
    "title": "Provable Sample-Efficient Transfer Learning Conditional Diffusion Models via Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gp2vgxWROE": {
    "title": "Rethinking Verification for LLM Code Generation: From Generation to Testing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w3dMbY6rFe": {
    "title": "Lyapunov-Stable Adaptive Control for Multimodal Concept Drift",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5fSkinHw7w": {
    "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T7KZbdzAXB": {
    "title": "GLVD: Guided Learned Vertex Descent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B7Bc9xzl2o": {
    "title": "Latent Retrieval Augmented Generation of Cross-Domain Protein Binders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nk9QDWOFt8": {
    "title": "Latency NMS Attacks: Is It Real Life or Is It Just Fantasy?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nl02znfTCT": {
    "title": "Acceleration via silver step-size on Riemannian manifolds with applications to Wasserstein space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ty9n72fZ1K": {
    "title": "MoPFormer: Motion-Primitive Transformer for Wearable-Sensor Activity Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G8wWeZVmla": {
    "title": "Transforming Gaps into Gains: Bridging Model and Data Heterogeneity in Federated Learning via Knowledge Weak-Aware Zones",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IpYEKM0Bla": {
    "title": "Quantifying Distributional Invariance in Causal Subgraph for IRM-Free Graph Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gfGLEtWAqV": {
    "title": "Improved Algorithms for Fair Matroid Submodular Maximization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ggP2r9S9wY": {
    "title": "Skill-Driven Neurosymbolic State Abstractions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rc3XO4RARL": {
    "title": "MaNGO — Adaptable Graph Network Simulators via Meta-Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oAgwvZay2U": {
    "title": "Guided Diffusion Sampling on Function Spaces with Applications to PDEs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GA3NBpzQ1x": {
    "title": "Linear Mixture Distributionally Robust Markov Decision Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uVjuiPP4aP": {
    "title": "ComRank: Ranking Loss for Multi-Label Complementary Label Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8ZiElzQxf1": {
    "title": "Inference-Time Hyper-Scaling with KV Cache Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qq9okO8fbB": {
    "title": "Improving planning and MBRL with temporally-extended actions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oGc1qHAUBJ": {
    "title": "Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NpRbTTgRBG": {
    "title": "RaySt3R: Predicting Novel Depth Maps for Zero-Shot Object Completion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hsf4gi5iEN": {
    "title": "TokMan:Tokenize Manhattan Mask Optimization for Inverse Lithography",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S2kbLrV7mv": {
    "title": "Informed Initialization for Bayesian Optimization and Active Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o4opuUiFCP": {
    "title": "Instance-Dependent Regret Bounds for Nonstochastic Linear Partial Monitoring",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7yRwAEWxto": {
    "title": "InstructSAM: A Training-free Framework for Instruction-Oriented Remote Sensing Object Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cGkfMGQdCy": {
    "title": "Enhancing the Outcome Reward-based RL Training of MLLMs with Self-Consistency Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eb71SNTjux": {
    "title": "Right for the Right Reasons: Avoiding Reasoning Shortcuts via Prototypical Neurosymbolic AI",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KxPBABa2pl": {
    "title": "QuanDA: Quantile-Based Discriminant Analysis for High-Dimensional Imbalanced Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rH4aGTL4jY": {
    "title": "A Tale of Two Symmetries: Exploring the Loss Landscape of Equivariant Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vaosMuNvOt": {
    "title": "QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NKNryrCGYn": {
    "title": "FreqPolicy: Efficient Flow-based Visuomotor Policy via Frequency Consistency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4TUpqyDJbz": {
    "title": "Theoretical Insights into In-context Learning with Unlabeled Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=idir4VfpuZ": {
    "title": "VeriLoC: Line-of-Code Level Prediction of Hardware Design Quality from Verilog Code",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HnJ1UkuJXS": {
    "title": "Shapley-Coop: Credit Assignment for Emergent Cooperation in Self-Interested LLM Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pn8UB4PmbF": {
    "title": "Efficient Adaptive Experimentation with Noncompliance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EqWZ1yVRfN": {
    "title": "Local Curvature Descent: Squeezing More Curvature out of Standard and Polyak Gradient Descent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nSAa6BaJtK": {
    "title": "VarFlow: Proper Scoring-Rule Diffusion Distillation via Energy Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i7vgeipxNf": {
    "title": "High Resolution UDF Meshing via Iterative Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M6gpmbPFty": {
    "title": "Residual Stream Analysis of Overfitting And Structural Disruptions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ctJxU8v3bY": {
    "title": "Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9DbkBqvuCI": {
    "title": "Anatomically inspired digital twins capture hierarchical object representations in visual cortex",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aYd4wSCle4": {
    "title": "Homogeneous Algorithms Can Reduce Competition in Personalized Pricing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cvFFar8cAr": {
    "title": "Homogeneous Keys, Heterogeneous Values: Exploiting Local KV Cache Asymmetry for Long-Context LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zzUXS4f91r": {
    "title": "DisCO: Reinforcing Large Reasoning Models with Discriminative Constrained Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fEg31YjLct": {
    "title": "Weak-to-Strong Generalization under Distribution Shifts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ACagRwCCqu": {
    "title": "ReinFlow: Fine-tuning Flow Matching Policy with Online Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7CADRzMLou": {
    "title": "SIFusion: A Unified Fusion Framework for Multi-granularity Arctic Sea Ice Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jAlvha5M8v": {
    "title": "Detecting Data Deviations in Electronic Health Records",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nik6BjmLm2": {
    "title": "Imbalances in Neurosymbolic Learning: Characterization and Mitigating Strategies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lq4nneD2xX": {
    "title": "G1 : Teaching LLMs to Reason on Graphs with Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bt2vhIphJ0": {
    "title": "Incentivizing Time-Aware Fairness in Data Sharing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5MGClYw1cR": {
    "title": "Binary Quadratic Quantization: Beyond First-Order Quantization for Real-Valued Matrix Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wk65okms3T": {
    "title": "Discrete Neural Flow Samplers with Locally Equivariant Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pceKiO7cEr": {
    "title": "Point Cloud Synthesis Using Inner Product Transforms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BgwtOwFSvY": {
    "title": "Flow based approach for Dynamic Temporal Causal models with non-Gaussian or Heteroscedastic Noises",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M83RPhdsX4": {
    "title": "A Theory for Worst-Case vs. Average-Case Guarantees for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oanhUGY6un": {
    "title": "Gradient Multi-Normalization for Efficient LLM Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SNJhYhO3a9": {
    "title": "Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t5ylXsc2Es": {
    "title": "ChromFound: Towards A Universal Foundation Model for Single-Cell Chromatin Accessibiltiy Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5UQpW4Jov4": {
    "title": "FaCT: Faithful Concept Traces for Explaining Neural Network Decisions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MLprqOvAAK": {
    "title": "Teaching Transformers to Solve Combinatorial Problems through Efficient Trial & Error",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AsRB5nmlOD": {
    "title": "SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dK2BnCH68p": {
    "title": "Equivariant Eikonal Neural Networks: Grid-Free, Scalable Travel-Time Prediction on Homogeneous Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VGB2TV0QUE": {
    "title": "Hierarchical Optimization via LLM-Guided Objective Evolution for Mobility-on-Demand Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GQHUET0V6f": {
    "title": "Multimodal LiDAR-Camera Novel View Synthesis with Unified Pose-free Neural Fields",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zKV3CN40tE": {
    "title": "BeyondMix: Leveraging Structural Priors and Long-Range Dependencies for Domain-Invariant LiDAR Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CKOyN2HShu": {
    "title": "ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H26A7cl91u": {
    "title": "On Reasoning Strength Planning in Large Reasoning Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IHZngJNasX": {
    "title": "SPFL: Sequential updates with Parallel aggregation for Enhanced Federated Learning under Category and Domain Shifts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qfP6IDxOrA": {
    "title": "Omni-DNA: A Genomic Model Supporting Sequence Understanding, Long-context, and Textual Annotation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iY1zuKydO0": {
    "title": "DISC: Dynamic Decomposition Improves LLM Inference Scaling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TpdrvezvEA": {
    "title": "No-Regret Learning Under Adversarial Resource Constraints: A Spending Plan Is All You Need!",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eHRFb3DSZS": {
    "title": "ZeCO: Zero-Communication Overhead Sequence Parallelism for Linear Attention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MddNT1GXbi": {
    "title": "WarpGAN: Warping-Guided 3D GAN Inversion with Style-Based Novel View Inpainting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8YniJnJQ0P": {
    "title": "Detecting High-Stakes Interactions with Activation Probes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7M9JGaoAeC": {
    "title": "Infinite Neural Operators: Gaussian processes on functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uS5ch7GjZ4": {
    "title": "SPARTAN: A Sparse Transformer World Model Attending to What Matters",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pdlepT2alu": {
    "title": "Parameter Dynamics of Online Machine Learning and Test-time Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wRQmQ6UXYF": {
    "title": "RoME: Domain-Robust Mixture-of-Experts for MILP Solution Prediction across Domains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hfKPMjiDnv": {
    "title": "Generalizing while preserving monotonicity in comparison-based preference learning models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dBC55QIEnq": {
    "title": "Orthogonal Contrastive Learning for Multi-Representation fMRI Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yzv6kysYbw": {
    "title": "Variational Task Vector Composition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G7CiAs8xyw": {
    "title": "Foundations of Top- k Decoding for Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uIQX0IA8nq": {
    "title": "Graph Few-Shot Learning via Adaptive Spectrum Experts and Cross-Set Distribution Calibration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=snWG3KVTQJ": {
    "title": "Statistical Parity with Exponential Weights",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VIP5mV8x0b": {
    "title": "When Does Curriculum Learning Help? A Theoretical Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IjDlvzBSVp": {
    "title": "SoPo: Text-to-Motion Generation Using Semi-Online Preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FT6VHWYdvd": {
    "title": "Whitened Score Diffusion: A Structured Prior for Imaging Inverse Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ae9uoV8FCz": {
    "title": "RANK++LETR: Learn to Rank and Optimize Candidates for Line Segment Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lu4cGylISh": {
    "title": "Uncertainty-Aware Multi-Objective Reinforcement Learning-Guided Diffusion Models for 3D De Novo Molecular Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HmsEHahtGx": {
    "title": "Adaptable Safe Policy Learning from Multi-task Data with Constraint Prioritized Decision Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VZQ04Ojhu5": {
    "title": "ConfTuner: Training Large Language Models to Express Their Confidence Verbally",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aI3Oacoyi6": {
    "title": "Distributed Multi-Agent Bandits Over Erdős-Rényi Random Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0IStUxUDGJ": {
    "title": "Sculpting Features from Noise: Reward-Guided Hierarchical Diffusion for Task-Optimal Feature Transformation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=htiPttfiZM": {
    "title": "Optimistic Online-to-Batch Conversions for Accelerated Convergence and Universality",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1nWqhiulqD": {
    "title": "FlowDAS: A Stochastic Interpolant-based Framework for Data Assimilation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sTR3xGGU6W": {
    "title": "Value Improved Actor Critic Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yl9LxRL5tj": {
    "title": "Interpretable and Parameter Efficient Graph Neural Additive Models with Random Fourier Features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jmLCBLeEC4": {
    "title": "AdvEDM: Fine-grained Adversarial Attack against VLM-based Embodied Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PNgG4H3q9D": {
    "title": "Mixture-of-Experts Operator Transformer for Large-Scale PDE Pre-Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aXnUn8vush": {
    "title": "True Impact of Cascade Length in Contextual Cascading Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t0PtZOlo3y": {
    "title": "FairNet: Dynamic Fairness Correction without Performance Loss via Contrastive Conditional LoRA",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3MiADMHY62": {
    "title": "Efficient Policy Optimization in Robust Constrained MDPs with Iteration Complexity Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XR5y4nvTfz": {
    "title": "LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+ FPS",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xdNAVP7TGy": {
    "title": "70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float (DFloat11)",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qVDEM93mCP": {
    "title": "Taming Hyperparameter Sensitivity in Data Attribution: Practical Selection Without Costly Retraining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hCc6obJhlj": {
    "title": "BAM-ICL: Causal Hijacking In-Context Learning with Budgeted Adversarial Manipulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YufSVJxDgt": {
    "title": "Inpainting the Neural Picture: Inferring Unrecorded Brain Area Dynamics from Multi-Animal Datasets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=to1VYVar9W": {
    "title": "Breaking AR's Sampling Bottleneck: Provable Acceleration via Diffusion Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LCPoXt0pzm": {
    "title": "KungfuBot: Physics-Based Humanoid Whole-Body Control for Learning Highly-Dynamic Skills",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zJzu9evD5K": {
    "title": "LittleBit: Ultra Low-Bit Quantization via Latent Factorization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oupeovfx0L": {
    "title": "SymMaP: Improving Computational Efficiency in Linear Solvers through Symbolic Preconditioning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tMBPJureSx": {
    "title": "Learning 3D Anisotropic Noise Distributions Improves Molecular Force Fields",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mHfpziOtTW": {
    "title": "ADPretrain: Advancing Industrial Anomaly Detection via Anomaly Representation Pretraining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9I1XjEEtsh": {
    "title": "Global Convergence for Average Reward Constrained MDPs with Primal-Dual Actor Critic Algorithm",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FrXHdcTEzE": {
    "title": "TabSTAR: A Tabular Foundation Model for Tabular Data with Text Fields",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=asS4W7Yw5e": {
    "title": "Reinventing Multi-Agent Collaboration through Gaussian-Image Synergy in Diffusion Policies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vo2UHqMu8t": {
    "title": "Antidistillation Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fmCnNQjZrr": {
    "title": "STAR: Spatial-Temporal Tracklet Matching for Multi-Object Tracking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bdGsKis3Ew": {
    "title": "Turning the Tables: Enabling Backward Transfer via Causal-Aware LoRA in Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nimTd1IJz1": {
    "title": "STNet: Spectral Transformation Network for Solving Operator Eigenvalue Problem",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oXSkzIXgbk": {
    "title": "Bayesian Concept Bottleneck Models with LLM Priors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CF6qEI6rH0": {
    "title": "Latent Refinement via Flow Matching for Training-free Linear Inverse Problem Solving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F7BpMMN01x": {
    "title": "Finite-Sample Analysis of Policy Evaluation for Robust Average Reward Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zQqDqfja4Y": {
    "title": "Depth-Supervised Fusion Network for Seamless-Free Image Stitching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dx1qQ9OAbb": {
    "title": "On Minimax Estimation of Parameters in Softmax-Contaminated Mixture of Experts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KZn7TDOL4J": {
    "title": "MagCache: Fast Video Generation with Magnitude-Aware Cache",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SAbQLqf8XL": {
    "title": "NeuSymEA: Neuro-symbolic Entity Alignment via Variational Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QKICx7eSMJ": {
    "title": "Generating Computational Cognitive models using Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rAGWvnpcKe": {
    "title": "ArchCAD-400K: A Large-Scale CAD drawings Dataset and New Baseline for Panoptic Symbol Spotting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CHN4HG9R5e": {
    "title": "E2E-VGuard: Adversarial Prevention for Production LLM-based End-To-End Speech Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UlpAIz6o5u": {
    "title": "Enhancing Privacy in Multimodal Federated Learning with Information Theory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0cCkauhKit": {
    "title": "Mind the Quote: Enabling Quotation-Aware Dialogue in LLMs via Plug-and-Play Modules",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xhKsnxJ9FD": {
    "title": "PINN Balls: Scaling Second-Order Methods for PINNs with Domain Decomposition and Adaptive Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bjoHB7IN6b": {
    "title": "Seeing is Believing? Mitigating OCR Hallucinations in Multimodal Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o1g8NWkxqf": {
    "title": "Topology of Reasoning: Understanding Large Reasoning Models through Reasoning Graph Properties",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iyu4sLQZvW": {
    "title": "First Attentions Last: Better Exploiting First Attentions for Efficient Parallel Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=me2vIZfgQf": {
    "title": "RoMa: A Robust Model Watermarking Scheme for Protecting IP in Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Z3wQSu3mH": {
    "title": "FocalCodec: Low-Bitrate Speech Coding via Focal Modulation Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dkXDyM66ce": {
    "title": "Convergence of the Gradient Flow for Shallow ReLU Networks on Weakly Interacting Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZwDMrArTBg": {
    "title": "Validating LLM-as-a-Judge Systems under Rating Indeterminacy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rc489jcc30": {
    "title": "AdaLRS: Loss-Guided Adaptive Learning Rate Search for Efficient Foundation Model Pretraining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SBPWnXhwjq": {
    "title": "Emergent Temporal Correspondences from Video Diffusion Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rlH3e7VlY8": {
    "title": "MTL-KD: Multi-Task Learning Via Knowledge Distillation for Generalizable Neural Vehicle Routing Solver",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yrNw1R8o2W": {
    "title": "Personalized Subgraph Federated Learning with Differentiable Auxiliary Projections",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1KXST1ksJ2": {
    "title": "Learning to Plan Like the Human Brain via Visuospatial Perception and Semantic-Episodic Synergistic Decision-Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZHoryZEpvl": {
    "title": "Flexible inference for animal learning rules using neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ffBF6hYuQv": {
    "title": "BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zmCBCbr2Wj": {
    "title": "Plug-and-play Feature Causality Decomposition for Multimodal Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZtXT584LrT": {
    "title": "Large Language Bayes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pBFVoll8Xa": {
    "title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KQoVgPOM1S": {
    "title": "Hybrid Boundary Physics-Informed Neural Networks for Solving Navier-Stokes Equations with Complex Boundary",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FE91MHgEg2": {
    "title": "Promptable 3-D Object Localization with Latent Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GqzMEkVp2i": {
    "title": "Online Locally Differentially Private Conformal Prediction via Binary Inquiries",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vsLS698sBi": {
    "title": "Attack by Yourself: Effective and Unnoticeable Multi-Category Graph Backdoor Attacks with Subgraph Triggers Pool",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WkUzrUsqR9": {
    "title": "Geo-Sign: Hyperbolic Contrastive Regularisation for Geometrically Aware Sign Language Translation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tqoAQMk4AH": {
    "title": "ε -Optimally Solving Two-Player Zero-Sum POSGs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r8zHRmM4uE": {
    "title": "Reasoning Beyond Points: A Visual Introspective Approach for Few-Shot 3D Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6geRIdlFWJ": {
    "title": "SubTrack++ : Gradient Subspace Tracking for Scalable LLM Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IiPSP4OUYx": {
    "title": "MLLM-For3D: Adapting Multimodal Large Language Model for 3D Reasoning Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aBUG2Phwdt": {
    "title": "Spectral Graph Coarsening Using Inner Product Preservation and the Grassmann Manifold",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iO2wfcFfHK": {
    "title": "Generating and Checking DNN Verification Proofs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AYqtMLRwzj": {
    "title": "Belief-Calibrated Multi-Agent Consensus Seeking for Complex NLP Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rzJkKeliDK": {
    "title": "Neural Correlates of Serial Dependence: Synaptic Short-term Plasticity Orchestrates Repulsion and Attraction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8ySOcf7UpM": {
    "title": "Feedback Guidance of Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tOwP9z1Zde": {
    "title": "Kernel Regression in Structured Non-IID Settings: Theory and Implications for Denoising Score Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EGK487IYAW": {
    "title": "One Filters All: A Generalist Filter For State Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nufqobhME7": {
    "title": "Revealing Multimodal Causality with Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VaE33hkqmg": {
    "title": "Spiking Meets Attention: Efficient Remote Sensing Image Super-Resolution with Attention Spiking Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QINnsnppv8": {
    "title": "Memory Injection Attacks on LLM Agents via Query-Only Interaction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x7fCiuCCAu": {
    "title": "Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tjH0YvT5wv": {
    "title": "Gaussian Process Upper Confidence Bound Achieves Nearly-Optimal Regret in Noise-Free Gaussian Process Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0kAfwgpeug": {
    "title": "Sample-Efficient Multi-Round Generative Data Augmentation for Long-Tail Instance Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3zx087XPtz": {
    "title": "AVCD: Mitigating Hallucinations in Audio-Visual Large Language Models through Contrastive Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zb3QO7HLIj": {
    "title": "FLAME: Fast Long-context Adaptive Memory for Event-based Vision",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dopfjQFr65": {
    "title": "Efficient Adaptive Federated Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BG0Hbee5si": {
    "title": "Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xSHqNf5Pdc": {
    "title": "Every Rollout Counts: Optimal Resource Allocation for Efficient Test-Time Scaling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r2GebY4MnU": {
    "title": "EgoDTM: Towards 3D-Aware Egocentric Video-Language Pretraining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DCc4OyNX8A": {
    "title": "Hybrid Re-matching for Continual Learning with Parameter-Efficient Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D19hc2XPeZ": {
    "title": "Robust LLM Alignment via Distributionally Robust Direct Preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DaNnkQJSQf": {
    "title": "Sparse Autoencoders Learn Monosemantic Features in Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GhVQnjVjlI": {
    "title": "Optimal Online Change Detection via Random Fourier Features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O4mwSIH1vs": {
    "title": "ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ycMpNwzUAA": {
    "title": "MetaDefense: Defending Fine-tuning based Jailbreak Attack Before and During Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2kuuhLfStt": {
    "title": "RePO: Understanding Preference Learning Through ReLU-Based Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qq19n9LZ97": {
    "title": "FedSVD: Adaptive Orthogonalization for Private Federated Learning with LoRA",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UGjWlxU6GY": {
    "title": "FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XewZ4rJYKZ": {
    "title": "ReservoirTTA: Prolonged Test-time Adaptation for Evolving and Recurring Domains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Qn6skg175": {
    "title": "Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tH8Q2TRSZf": {
    "title": "Online Bilateral Trade With Minimal Feedback: Don't Waste Seller's Time",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VeZkY3JjWV": {
    "title": "Pixel Reasoner: Incentivizing Pixel Space Reasoning via Curiosity-Driven Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ey0mro4vJ6": {
    "title": "Continuous Soft Actor-Critic: An Off-Policy Learning Method Robust to Time Discretization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=moiVS9AY0q": {
    "title": "Preference-Guided Diffusion for Multi-Objective Offline Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=movsqor65f": {
    "title": "Parameter Efficient Fine-tuning via Explained Variance Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rgk129n73h": {
    "title": "Reliable Decision‑Making via Calibration‑Oriented Retrieval‑Augmented Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jaMPaFDAaZ": {
    "title": "FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i5rApSWC9E": {
    "title": "Enhancing Deep Batch Active Learning for Regression with Imperfect Data Guided Selection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0398lUtZqs": {
    "title": "Reproducing Kernel Banach Space Models for Neural Networks with Application to Rademacher Complexity Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s2hidSUFdj": {
    "title": "Automatic Synthetic Data and Fine-grained Adaptive Feature Alignment for Composed Person Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LtON4qr1if": {
    "title": "On Efficiency-Effectiveness Trade-off of Diffusion-based Recommenders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ax0y9DvLCp": {
    "title": "AlignedGen: Aligning Style Across Generated Images",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v6kyF3S7dM": {
    "title": "Flex-Judge: Text-Only Reasoning Unleashes Zero-Shot Multimodal Evaluators",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jWrDyknUZ8": {
    "title": "Exploring and Leveraging Class Vectors for Classifier Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1mokb8ohOQ": {
    "title": "ForgerySleuth: Empowering Multimodal Large Language Models for Image Manipulation Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=27aIOGfkAV": {
    "title": "Sequential Multi-Agent Dynamic Algorithm Configuration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SkAY3KHKn2": {
    "title": "GyroSwin: 5D Surrogates for Gyrokinetic Plasma Turbulence Simulations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lEXyU72PpT": {
    "title": "Bilevel Optimization for Adversarial Learning Problems: Sharpness, Generation, and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SYKwGnik3w": {
    "title": "DyMoDreamer: World Modeling with Dynamic Modulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jQn9oYY4sz": {
    "title": "On the Role of Hidden States of Modern Hopfield Network in Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9GN5Jsa3lv": {
    "title": "Sloth: scaling laws for LLM skills to predict multi-benchmark performance across families",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pcwI5cNFJK": {
    "title": "What Does It Take to Build a Performant Selective Classifier?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zp7W2QmxHS": {
    "title": "Implicit-ARAP: Efficient Handle-Guided Neural Field Deformation via Local Patch Meshing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=41mBrhy6U4": {
    "title": "Provably Efficient Online RLHF with One-Pass Reward Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m5jmATYmsi": {
    "title": "Topology-Aware Learning of Tubular Manifolds via SE(3)-Equivariant Network on Ball B-Spline Curve",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yg1yfaKolw": {
    "title": "JailBound: Jailbreaking Internal Safety Boundaries of Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LOVuux2kvs": {
    "title": "Don't Let It Fade: Preserving Edits in Diffusion Language Models via Token Timestep Allocation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kvsa8ZXd0W": {
    "title": "Data Mixture Optimization: A Multi-fidelity Multi-scale Bayesian Framework",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CVekdZDzLG": {
    "title": "DUAL: Learning Diverse Kernels for Aggregated Two-sample and Independence Testing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=heJ7NRInjs": {
    "title": "RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=omyNP89YW6": {
    "title": "Tree-Based Premise Selection for Lean4",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WbpzGpVWVx": {
    "title": "CamSAM2: Segment Anything Accurately in Camouflaged Videos",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pNwiFucAtA": {
    "title": "MOF-BFN: Metal-Organic Frameworks Structure Prediction via Bayesian Flow Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xYik0sKYVo": {
    "title": "Feature Distillation is the Better Choice for Model-Heterogeneous Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e0ltwNcoIp": {
    "title": "Free-Lunch Color-Texture Disentanglement for Stylized Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fVgnP5WHXX": {
    "title": "VADTree: Explainable Training-Free Video Anomaly Detection via Hierarchical Granularity-Aware Tree",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PkSDCXP6LP": {
    "title": "Anchor-based Maximum Discrepancy for Relative Similarity Testing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wI6oHXeTR8": {
    "title": "Mixture of Noise for Pre-Trained Model-Based Class-Incremental Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JTF0HYfGiL": {
    "title": "Generalizable Hand-Object Modeling from Monocular RGB Images via 3D Gaussians",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pXoR0Sy4WQ": {
    "title": "Probabilistic Stability Guarantees for Feature Attributions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9xXjWwAoUF": {
    "title": "A Frustratingly Simple Yet Highly Effective Attack Baseline: Over 90% Success Rate Against the Strong Black-box Models of GPT-4.5/4o/o1",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w1x9WMMHvi": {
    "title": "Multivariate Time Series Anomaly Detection with Idempotent Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qMm7tC1zvj": {
    "title": "EfficientNav: Towards On-Device Object-Goal Navigation with Navigation Map Caching and Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j6vywikodV": {
    "title": "Discovering Symbolic Partial Differential Equation by Abductive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uHGG3s0XyZ": {
    "title": "3D Visual Illusion Depth Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vEFAR8KH1l": {
    "title": "Tight High-Probability Bounds for Nonconvex Heavy-Tailed Scenario under Weaker Assumptions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h0aA8xpL4U": {
    "title": "DePass: Unified Feature Attributing by Simple Decomposed Forward Pass",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5GoxWETVvV": {
    "title": "Sampling by averaging: A multiscale approach to score estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AlSHcopwHi": {
    "title": "Federated Continual Learning via Orchestrating Multi-Scale Expertise",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o2y6BS6mm0": {
    "title": "Don't Forget the Enjoin: FocalLoRA for Instruction Hierarchical Alignment in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wlpf0Vg4yU": {
    "title": "Mitigating Semantic Collapse in Partially Relevant Video Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JxkZyedsdS": {
    "title": "Trained Mamba Emulates Online Gradient Descent in In-Context Linear Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oeWgBOowL6": {
    "title": "CausalVTG: Towards Robust Video Temporal Grounding via Causal Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KvWGmvuuKR": {
    "title": "Local-Global Associative Frames for Symmetry-Preserving Crystal Structure Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gRKorMGYlR": {
    "title": "TractoTransformer: Diffusion MRI Streamline Tractography using CNN and Transformer Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nHkg4yc7SP": {
    "title": "Improving Video Generation with Human Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2DAvXR77xh": {
    "title": "Self-supervised Learning of Echocardiographic Video Representations via Online Cluster Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=13xP0J2d6E": {
    "title": "BlurDM: A Blur Diffusion Model for Image Deblurring",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3JUhkxVlyF": {
    "title": "Mechanism Design for LLM Fine-tuning with Multiple Reward Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uEFC25uUwU": {
    "title": "The φ Curve: The Shape of Generalization through the Lens of Norm-based Capacity Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EGYwfs4XhI": {
    "title": "Improving Diffusion-based Inverse Algorithms under Few-Step Constraint via Linear Extrapolation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1bb974Q4jJ": {
    "title": "No-Regret Thompson Sampling for Finite-Horizon Markov Decision Processes with Gaussian Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ovUuNzZZbK": {
    "title": "Improved Training Technique for Shortcut Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KQ9KCDS4zp": {
    "title": "Foundation Cures Personalization: Improving Personalized Models' Prompt Consistency via Hidden Foundation Knowledge",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gcrDTxZTl0": {
    "title": "FIPER: Factorized Features for Robust Image Super-Resolution and Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hhg1TPk3RG": {
    "title": "AnimateQR: Bridging Aesthetics and Functionality in Dynamic QR Code Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1cFDvuWujv": {
    "title": "PLD: A Choice-Theoretic List-Wise Knowledge Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mE74JKHTCE": {
    "title": "Tracing the Roots: Leveraging Temporal Dynamics in Diffusion Trajectories for Origin Attribution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gvex75bPMI": {
    "title": "GradMetaNet: An Equivariant Architecture for Learning on Gradients",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mGjmPnzGQo": {
    "title": "SAP: Exact Sorting in Splatting via Screen-Aligned Primitives",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ILZ7ZPEHD5": {
    "title": "Personalized Exercise Recommendation with Semantically-Grounded Knowledge Tracing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lTAtQW1Ufi": {
    "title": "From Human Attention to Diagnosis: Semantic Patch-Level Integration of Vision-Language Models in Medical Imaging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vyI84ToW3q": {
    "title": "Impartial Selection with Predictions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=36TBVGwzAY": {
    "title": "GRIP: A Graph-Based Reasoning Instruction Producer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B9P4mozXYO": {
    "title": "Decomposing motor units through elimination for real-time intention driven assistive neurotechnology",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lG82mcrKJf": {
    "title": "Fortifying Time Series: DTW-Certified Robust Anomaly Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wq5G71w7Zx": {
    "title": "Sparse Image Synthesis via Joint Latent and RoI Flow",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=prm9Y3jM0R": {
    "title": "Online robust locally differentially private learning for nonparametric regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gggvYyiYam": {
    "title": "Prompt-Guided Alignment with Information Bottleneck Makes Image Compression Also a Restorer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=diGiWMO95N": {
    "title": "Efficient RAW Image Deblurring with Adaptive Frequency Modulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CtoIG9Iwas": {
    "title": "Learning Pattern-Specific Experts for Time Series Forecasting Under Patch-level Distribution Shift",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GoFYIRUVAz": {
    "title": "MiCADangelo: Fine-Grained Reconstruction of Constrained CAD Models from 3D Scans",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JyxsfFEiua": {
    "title": "Ultra-high Resolution Watermarking Framework Resistant to Extreme Cropping and Scaling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YngHXbJM8g": {
    "title": "SEMPO: Lightweight Foundation Models for Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D7zMB5uEHw": {
    "title": "Autoencoding Random Forests",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gIGtOg4DNa": {
    "title": "MeCeFO: Enhancing LLM Training Robustness via Fault-Tolerant Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MQJGqVeDd4": {
    "title": "Rethinking Nighttime Image Deraining via Learnable Color Space Transformation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aNpj43Uh35": {
    "title": "Multi-Objective One-Shot Pruning for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hTOWYQNsrb": {
    "title": "Semi-supervised Graph Anomaly Detection via Robust Homophily Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rzqs4q9ooa": {
    "title": "Randomized-MLP Regularization Improves Domain Adaptation and Interpretability in DINOv2",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a3l3K9khbL": {
    "title": "Quantization Error Propagation: Revisiting Layer-Wise Post-Training Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DbzREoPwmM": {
    "title": "Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BDZpNjIC1y": {
    "title": "Fused View-Time Attention and Feedforward Reconstruction for 4D Scene Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NnRQOPzL9P": {
    "title": "Elucidated Rolling Diffusion Models for Probabilistic Forecasting of Complex Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ny6YPsXCOY": {
    "title": "Generative Pre-trained Autoregressive Diffusion Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yZzhaHygWW": {
    "title": "Optimizing Retrieval for RAG via Reinforced Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tqriGodQ79": {
    "title": "Hippocampal-like Sequential Editing for Continual Knowledge Updates in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KG7j1V0rzS": {
    "title": "Sparse Gaussian Processes: Structured Approximations and Power-EP Revisited",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3xwsD68F2K": {
    "title": "VLM in a flash: I/O-Efficient Sparsification of Vision-Language Model via Neuron Chunking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u7eIoTviuM": {
    "title": "Stochastic Shortest Path with Sparse Adversarial Costs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y5IUGnpDJ8": {
    "title": "Attention with Trained Embeddings Provably Selects Important Tokens",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lEsvczuPVj": {
    "title": "REDOUBT: Duo Safety Validation for Autonomous Vehicle Motion Planning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VfIOdGiBAv": {
    "title": "Scaling Diffusion Transformers Efficiently via μ P",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=foy0BWzOFL": {
    "title": "Availability-aware Sensor Fusion via Unified Canonical Space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=80L235oVBe": {
    "title": "Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rPsUx09RJV": {
    "title": "Auditing Meta-Cognitive Hallucinations in Reasoning Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sl4KqWBkDq": {
    "title": "Deep Gaussian from Motion: Exploring 3D Geometric Foundation Models for Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mPuOMcN9E7": {
    "title": "Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x8lg9aihwl": {
    "title": "VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OKf5SSUhJF": {
    "title": "Enhancing Diffusion-based Unrestricted Adversarial Attacks via Adversary Preferences Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Di0RasgbQ6": {
    "title": "SPACE: SPike-Aware Consistency Enhancement for Test-Time Adaptation in Spiking Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MxYvh8zpbR": {
    "title": "GMM-based VAE model with Normalising Flow for effective stochastic segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tjw0ACu3NL": {
    "title": "Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yISJGSdzdd": {
    "title": "MISA: Memory-Efficient LLMs Optimization with Module-wise Importance Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6cB7DXAh1m": {
    "title": "Revisiting End-to-End Learning with Slide-level Supervision in Computational Pathology",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tl3Sg0SBEU": {
    "title": "PUATE: Efficient ATE Estimation from Treated (Positive) and Unlabeled Units",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xWYL9Ki32T": {
    "title": "Efficient Pre-Training of LLMs via Topology-Aware Communication Alignment on More Than 9600 GPUs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WL55AVdvxq": {
    "title": "MAP Estimation with Denoisers: Convergence Rates and Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Emwu0jyOBg": {
    "title": "CAMILA: Context-Aware Masking for Image Editing with Language Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8MHTtHZRFL": {
    "title": "Masked Diffusion Models as Energy Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CrBRKsP3yT": {
    "title": "UniMRSeg: Unified Modality-Relax Segmentation via Hierarchical Self-Supervised Compensation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n4nmiIq3qj": {
    "title": "CSGO: Content-Style Composition in Text-to-Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4zhhgKkVzY": {
    "title": "Cypher-RI: Reinforcement Learning for Integrating Schema Selection into Cypher Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P63bBMXCIH": {
    "title": "Learning Dynamics of RNNs in Closed-Loop Environments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eXckZbaYma": {
    "title": "SCOUT: Teaching Pre-trained Language Models to Enhance Reasoning via Flow Chain-of-Thought",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4AAwJAsGvb": {
    "title": "Understanding and Enhancing Message Passing on Heterophilic Graphs via Compatibility Matrix",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YTbLri0siT": {
    "title": "Spike-timing-dependent Hebbian learning as noisy gradient descent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=24wDPGiDzA": {
    "title": "Unified Scaling Laws for Compressed Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NatwrOmOvM": {
    "title": "Embodied Crowd Counting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V5efEA8nIr": {
    "title": "Spike4DGS: Towards High-Speed Dynamic Scene Rendering with 4D Gaussian Splatting via a Spike Camera Array",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yflq8Bhjrw": {
    "title": "Confusion-Driven Self-Supervised Progressively Weighted Ensemble Learning for Non-Exemplar Class Incremental Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FfccSikDfZ": {
    "title": "SHAP Meets Tensor Networks: Provably Tractable Explanations with Parallelism",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3molquhFpv": {
    "title": "HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zaV9s8iM2T": {
    "title": "UniGTE: Unified Graph–Text Encoding for Zero-Shot Generalization across Graph Tasks and Domains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dbioYc7qav": {
    "title": "Silencer: From Discovery to Mitigation of Self-Bias in LLM-as-Benchmark-Generator",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u4j0LtCYid": {
    "title": "Demystifying Language Model Forgetting with Low-rank Example Associations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x7t7B5CFHm": {
    "title": "Token Bottleneck: One Token to Remember Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YFQ0Xx5YlW": {
    "title": "MIHC: Multi-View Interpretable Hypergraph Neural Networks with Information Bottleneck for Chip Congestion Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5puCVHYtDB": {
    "title": "IPSI: Enhancing Structural Inference with Automatically Learned Structural Priors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ocBuEUl6Yz": {
    "title": "Extremely Simple Multimodal Outlier Synthesis for Out-of-Distribution Detection and Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eVhA17qoVJ": {
    "title": "A Generalized Label Shift Perspective for Cross-Domain Gaze Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G10Y4vrhGF": {
    "title": "FedFree: Breaking Knowledge-sharing Barriers through Layer-wise Alignment in Heterogeneous Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z9MxyboJ7R": {
    "title": "Spatially-aware Weights Tokenization for NeRF-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nI7wKr4eop": {
    "title": "Object-X: Learning to Reconstruct Multi-Modal 3D Object Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RwCaBZ4w5P": {
    "title": "Analogy-based Multi-Turn Jailbreak against Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=16mDq7m2OK": {
    "title": "MoGe-2: Accurate Monocular Geometry with Metric Scale and Sharp Details",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2FmTcj6WMB": {
    "title": "Asymptotically Stable Quaternion-valued Hopfield-structured Neural Network with Periodic Projection-based Supervised Learning Rules",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wc1VZ2bVJn": {
    "title": "TokenSqueeze: Performance-Preserving Compression for Reasoning LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VCj7knCJhn": {
    "title": "The Overthinker's DIET: Cutting Token Calories with DIfficulty-AwarE Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2hn4XhONIl": {
    "title": "Spectral Learning for Infinite-Horizon Average-Reward POMDPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0KOfAUiHua": {
    "title": "Towards Minimizing Feature Drift in Model Merging: Layer-wise Task Vector Fusion for Adaptive Knowledge Integration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sMtiGB2YZT": {
    "title": "Panacea: Mitigating Harmful Fine-tuning for Large Language Models via Post-fine-tuning Perturbation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5YDO8XjYjR": {
    "title": "VLForgery Face Triad: Detection, Localization and Attribution via Multimodal Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g8zr9rxRHm": {
    "title": "Synthesize Privacy-Preserving High-Resolution Images via Private Textual Intermediaries",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yKUwkihcsi": {
    "title": "Iterative Tool Usage Exploration for Multimodal Agents via Step-wise Preference Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QRKg5GA9Zo": {
    "title": "How Classifier Features Transfer to Downstream: An Asymptotic Analysis in a Two-Layer Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oCOOe8tt8v": {
    "title": "Leaving No OOD Instance Behind: Instance-Level OOD Fine-Tuning for Anomaly Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=flIdch9eTf": {
    "title": "High Dynamic Range Imaging with Time-Encoding Spike Camera",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3CiNHxWcBr": {
    "title": "A Physics-preserved Transfer Learning Method for Differential Equations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GMiC4ccyHn": {
    "title": "Optimized Minimal 3D Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Phrzarx9NG": {
    "title": "UniMotion: A Unified Motion Framework for Simulation, Prediction and Planning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gHeMLGEVJz": {
    "title": "Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qr5uMEs6iR": {
    "title": "Large Language Models as End-to-end Combinatorial Optimization Solvers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3p06mDUV5O": {
    "title": "Dimension-free Score Matching and Time Bootstrapping for Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u2Lgi4NIe7": {
    "title": "Counterfactual Image Editing with Disentangled Causal Latent Space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XtNiw8OQsy": {
    "title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HK96GI5s7G": {
    "title": "REPA Works Until It Doesn't: Early-Stopped, Holistic Alignment Supercharges Diffusion Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wsmP79iuyT": {
    "title": "Fairness-Regularized Online Optimization with Switching Costs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7DyKKOU6HR": {
    "title": "Self-supervised Blending Structural Context of Visual Molecules for Robust Drug Interaction Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1OuhWYrwgW": {
    "title": "Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=udHMDBrfTv": {
    "title": "Faithful Dynamic Imitation Learning from Human Intervention with Dynamic Regret Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DzQ8Dvt5qS": {
    "title": "LeapFactual: Reliable Visual Counterfactual Explanation Using Conditional Flow Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=43v0RAyGrg": {
    "title": "L2RSI: Cross-view LiDAR-based Place Recognition for Large-scale Urban Scenes via Remote Sensing Imagery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2YAGKZVXQC": {
    "title": "Additive Models Explained: A Computational Complexity Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I960M7VBng": {
    "title": "Towards Robust Pseudo-Label Learning in Semantic Segmentation: An Encoding Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KOebVBmhqx": {
    "title": "RoomEditor: High-Fidelity Furniture Synthesis with Parameter-Sharing U-Net",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9jONuWKoLj": {
    "title": "CAMO: Convergence-Aware Multi-Fidelity Bayesian Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KMaBmPHkVj": {
    "title": "CHPO: Constrained Hybrid-action Policy Optimization for Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DIVHwy7wfh": {
    "title": "DOVTrack: Data-Efficient Open-Vocabulary Tracking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BbWrp6O8Lm": {
    "title": "When Semantics Mislead Vision: Mitigating Large Multimodal Models Hallucinations in Scene Text Spotting and Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sUjwDdyspc": {
    "title": "MotionBind: Multi-Modal Human Motion Alignment for Retrieval, Recognition, and Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u7jtLj46i9": {
    "title": "MindForge: Empowering Embodied Agents with Theory of Mind for Lifelong Cultural Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kxv0M6I7Ud": {
    "title": "Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JzEgfRTzPW": {
    "title": "Beyond Average Value Function in Precision Medicine: Maximum Probability-Driven Reinforcement Learning for Survival Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ENJhSqVkXH": {
    "title": "Rao-Blackwell Gradient Estimators for Equivariant Denoising Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xm57IXqU0n": {
    "title": "Contextual Integrity in LLMs via Reasoning and Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GNWAJcaUrV": {
    "title": "AtlasGS: Atlanta-world Guided Surface Reconstruction with Implicit Structured Gaussians",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z3lGbdexVY": {
    "title": "SceneDecorator: Towards Scene-Oriented Story Generation with Scene Planning and Scene Consistency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2hzyfArJi6": {
    "title": "Bridging Scales: Spectral Theory Reveals How Local Connectivity Rules Sculpt Global Neural Dynamics in Spatially Extended Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oBOVYRRSy2": {
    "title": "Enhancing 3D Reconstruction for Dynamic Scenes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L26J2etiCX": {
    "title": "Rao-Blackwellised Reparameterisation Gradients",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C5e4TGdCJ3": {
    "title": "Towards Single-Source Domain Generalized Object Detection via Causal Visual Prompts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jCuEeQF7uP": {
    "title": "CADMorph: Geometry‑Driven Parametric CAD Editing via a Plan–Generate–Verify Loop",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c9EAmyYPOv": {
    "title": "Inference-Time Text-to-Video Alignment with Diffusion Latent Beam Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v4AT18kysa": {
    "title": "SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous Speech Translation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qI95wZZCWh": {
    "title": "Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=duVF6odtBO": {
    "title": "Robust Sampling for Active Statistical Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OwU0mgfKUi": {
    "title": "A 3 E: Towards Compositional Model Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8MBEvClECb": {
    "title": "Rebalancing Contrastive Alignment with Bottlenecked Semantic Increments in Text-Video Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZAu7sADxfh": {
    "title": "ShortListing Model: A Streamlined Simplex Diffusion for Discrete Variable Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Qo2SRcHgU": {
    "title": "AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=spjAvoZpVW": {
    "title": "MobileODE: An Extra Lightweight Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YFb7KFE39x": {
    "title": "Multi-Objective Reinforcement Learning with Max-Min Criterion: A Game-Theoretic Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hUCgQWItCX": {
    "title": "PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1mILyDyPDf": {
    "title": "MTRec: Learning to Align with User Preferences via Mental Reward Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BfO6od6JD6": {
    "title": "Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n7FPVDWI6C": {
    "title": "Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HqsE29wxnS": {
    "title": "FairDD: Fair Dataset Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JWmfbzw8ur": {
    "title": "Harnessing Feature Resonance under Arbitrary Target Alignment for Out-of-Distribution Node Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cQxLCVa9u7": {
    "title": "HyGen: Efficient LLM Serving via Elastic Online-Offline Request Co-location",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dbZo5cLlV9": {
    "title": "Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TecJ926Vgn": {
    "title": "Differentially Private Federated Low Rank Adaptation Beyond Fixed-Matrix",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JqyEIr41M4": {
    "title": "You Can Trust Your Clustering Model: A Parameter-free Self-Boosting Plug-in for Deep Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LuGevcG5IB": {
    "title": "Parameter-free Algorithms for the Stochastically Extended Adversarial Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e8pm93koQU": {
    "title": "S 2 Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tPJg65EB7D": {
    "title": "Brain Harmony: A Multimodal Foundation Model Unifying Morphology and Function into 1D Tokens",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2dpiR9fqUk": {
    "title": "CovMatch: Cross-Covariance Guided Multimodal Dataset Distillation with Trainable Text Encoder",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e7HEbUVryj": {
    "title": "ForceFM: Enhancing Protein-Ligand Predictions through Force-Guided Flow Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JebheQvpIb": {
    "title": "BridgePure: Limited Protection Leakage Can Break Black-Box Data Protection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N6ZzwaYSxQ": {
    "title": "Learning Neural Exposure Fields for View Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=utA0BT3BKF": {
    "title": "InstructRestore: Region-Customized Image Restoration with Human Instructions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kNoh1TuV4R": {
    "title": "Tractable Multinomial Logit Contextual Bandits with Non-Linear Utilities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WQq5JPGQ0C": {
    "title": "RadZero: Similarity-Based Cross-Attention for Explainable Vision-Language Alignment in Chest X-ray with Zero-Shot Multi-Task Capability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4BsrGHtvW5": {
    "title": "Adaptive Context Length Optimization with Low-Frequency Truncation for Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7tXGIbrIA5": {
    "title": "Graph Your Own Prompt",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a64D9Vl7wK": {
    "title": "Predicting Empirical AI Research Outcomes with Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v13yQBxhut": {
    "title": "The Logical Expressiveness of Temporal GNNs via Two-Dimensional Product Logics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R1CximX3Cw": {
    "title": "Practical Kernel Selection for Kernel-based Conditional Independence Test",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gPPPfwU4jj": {
    "title": "Geometric Logit Decoupling for Energy-Based Graph Out-of-distribution Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uGjTP2kFcX": {
    "title": "Entropy-Calibrated Label Distribution Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zlMupLoKRf": {
    "title": "Causality Meets the Table: Debiasing LLMs for Faithful TableQA via Front-Door Intervention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c6CAVKlKmU": {
    "title": "WorldMem: Long-term Consistent World Simulation with Memory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8LO0vLRXpz": {
    "title": "Enhancing LLM Planning for Robotics Manipulation through Hierarchical Procedural Knowledge Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q3DtkFJ1Ap": {
    "title": "MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xpuci4SV06": {
    "title": "DevFD : Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NWP8KYKC0c": {
    "title": "Space Group Equivariant Crystal Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JvHif4fyeP": {
    "title": "Agents Robust to Distribution Shifts Learn Causal World Models Even Under Mediation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kaU1No4iiI": {
    "title": "Collective Counterfactual Explanations: Balancing Individual Goals and Collective Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FvXI7DNQfz": {
    "title": "OASIS: One-Shot Federated Graph Learning via Wasserstein Assisted Knowledge Integration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GcjpjIHDZn": {
    "title": "Obliviator Reveals the Cost of Nonlinear Guardedness in Concept Erasure",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O880k3JoC8": {
    "title": "Mamba Only Glances Once (MOGO): A Lightweight Framework for Efficient Video Action Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E1FrjgaG1J": {
    "title": "Demystifying Reasoning Dynamics with Mutual Information: Thinking Tokens are Information Peaks in LLM Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nqVLJTPoQC": {
    "title": "Training-Free Guidance Beyond Differentiability: Scalable Path Steering with Tree Search in Diffusion and Flow Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AZ1iyo58F8": {
    "title": "Elevating Visual Perception in Multimodal LLMs with Visual Embedding Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dC5TWysDsZ": {
    "title": "Amplifying Prominent Representations in Multimodal Learning via Variational Dirichlet Process",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nHNYDM6PVz": {
    "title": "NFIG: Multi-Scale Autoregressive Image Generation via Frequency Ordering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ILP0eZAor6": {
    "title": "Optimal Dynamic Regret by Transformers for Non-Stationary Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q5L7jygjpi": {
    "title": "Vocabulary-Guided Gait Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zZLfHw4Erp": {
    "title": "UltraLED: Learning to See Everything in Ultra-High Dynamic Range Scenes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LAflniLUwx": {
    "title": "Enhancing Vision-Language Model Reliability with Uncertainty-Guided Dropout Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iLbXyPNTeb": {
    "title": "VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gpCleSGCkV": {
    "title": "The VLLM Safety Paradox: Dual Ease in Jailbreak Attack and Defense",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dAwKePZvcN": {
    "title": "SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fqsxfizLpk": {
    "title": "Distributed mediation analysis with communication efficiency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tAKbMv3sf7": {
    "title": "X 2 -DFD: A framework for e X plainable and e X tendable Deepfake Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nWO8SkQYqJ": {
    "title": "AdaptGrad: Adaptive Sampling to Reduce Noise",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NHw8muIAcL": {
    "title": "Simple Distillation for One-Step Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7LKKHBAMzH": {
    "title": "WebThinker: Empowering Large Reasoning Models with Deep Research Capability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fozfLcLAEt": {
    "title": "Self-Boost via Optimal Retraining: An Analysis via Approximate Message Passing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ifMZw5fm7K": {
    "title": "Abstract Counterfactuals for Language Model Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Iw1nDtYmT": {
    "title": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LNWyf2RR1V": {
    "title": "Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=usOkGv1S7M": {
    "title": "UFT: Unifying Supervised and Reinforcement Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=urDdBuhbLx": {
    "title": "A faster training algorithm for regression trees with linear leaves, and an analysis of its complexity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RX5tYZAaeF": {
    "title": "Diffusion-Driven Two-Stage Active Learning for Low-Budget Semantic Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BL8h1Axr0i": {
    "title": "Storyboard-guided Alignment for Fine-grained Video Action Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fzyBjGrpp3": {
    "title": "Revisiting Follow-the-Perturbed-Leader with Unbounded Perturbations in Bandit Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1F2qhendd": {
    "title": "Self-Refining Language Model Anonymizers via Adversarial Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QQUhGPST45": {
    "title": "Personalized Federated Conformal Prediction with Localization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ExVMnClnrM": {
    "title": "Enhancing Consistency of Flow-Based Image Editing through Kalman Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lE2cD7C9fk": {
    "title": "On Inductive Biases That Enable Generalization in Diffusion Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bh56ijtRb9": {
    "title": "What We Miss Matters: Learning from the Overlooked in Point Cloud Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eLHIERUitQ": {
    "title": "FAME: Adaptive Functional Attention with Expert Routing for Function-on-Function Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dCGQlVRa2B": {
    "title": "Beyond Value Functions: Single-Loop Bilevel Optimization under Flatness Conditions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wdyOwMISSR": {
    "title": "Point-RFT: Improving Multimodal Reasoning with Visually Grounded Reinforcement Finetuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Phi6C3kFy1": {
    "title": "Semi-Supervised Regression with Heteroscedastic Pseudo-Labels",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gOUAak6f2P": {
    "title": "Hessian-guided Perturbed Wasserstein Gradient Flows for Escaping Saddle Points",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=stiJen3iNI": {
    "title": "K-DeCore: Facilitating Knowledge Transfer in Continual Structured Knowledge Reasoning via Knowledge Decoupling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rRPyI2h0yN": {
    "title": "Eve3D: Elevating Vision Models for Enhanced 3D Surface Reconstruction via Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zOFxp98km2": {
    "title": "Online Portfolio Selection with ML Predictions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F8K6emqT6G": {
    "title": "Adaptive Stochastic Coefficients for Accelerating Diffusion Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OPOBV0zXu7": {
    "title": "Multi-Scale Finetuning for Encoder-based Time Series Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eTgXolhWCH": {
    "title": "Contrastive Self-Supervised Learning As Neural Manifold Packing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9o7oH6DAHB": {
    "title": "LoTA-QAF: Lossless Ternary Adaptation for Quantization-Aware Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SoRe80Tg48": {
    "title": "Shallow Flow Matching for Coarse-to-Fine Text-to-Speech Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hAi0JapiZ7": {
    "title": "Real-Time Scene-Adaptive Tone Mapping for High-Dynamic Range Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rdz6ESQYkK": {
    "title": "GraphChain: Large Language Models for Large-scale Graph Analysis via Tool Chaining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OGgV9hpVGD": {
    "title": "From Style to Facts: Mapping the Boundaries of Knowledge Injection with Finetuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PMdHrorFMF": {
    "title": "Multi-Modal View Enhanced Large Vision Models for Long-Term Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BZSTC2RMEd": {
    "title": "Pruning-Robust Mamba with Asymmetric Multi-Scale Scanning Paths",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QSK8VqiijI": {
    "title": "Dual Prototype-Enhanced Contrastive Framework for Class-Imbalanced Graph Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sJd4DpYOis": {
    "title": "SpecEM: Training-Free LLM Ensembling via Iterative Drafting, Verification, and Online Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nJq5z21eUk": {
    "title": "Learning to Watermark: A Selective Watermarking Framework for Large Language Models via Multi-Objective Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Z8ckcrfq1": {
    "title": "Conditional Diffusion Anomaly Modeling on Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=URB690A5r5": {
    "title": "BlockScan: Detecting Anomalies in Blockchain Transactions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kAuckbcMvi": {
    "title": "Unlearning-Aware Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c6RDAutyNE": {
    "title": "GPO: Learning from Critical Steps to Improve LLM Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5LJoDO9qYt": {
    "title": "IDOL: Meeting Diverse Distribution Shifts with Prior Physics for Tropical Cyclone Multi-Task Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=15GCs8DoSm": {
    "title": "Memory-Augmented Potential Field Theory: A Framework for Adaptive Control in Non-Convex Domains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TmU3yfLTsS": {
    "title": "Learning to price with resource constraints: from full information to machine-learned prices",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jReV7xyjXy": {
    "title": "Multimodal 3D Genome Pre-training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2a36EMSSTp": {
    "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HuSSR12Yot": {
    "title": "Clip-and-Verify: Linear Constraint-Driven Domain Clipping for Accelerating Neural Network Verification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M4Laq0Y5WG": {
    "title": "Jasmine: Harnessing Diffusion Prior for Self-supervised Depth Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hVFtXE19Me": {
    "title": "U-CAN: Unsupervised Point Cloud Denoising with Consistency-Aware Noise2Noise Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wmuWMEoIL9": {
    "title": "SAS: Simulated Attention Score",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fGBCRZQVse": {
    "title": "Theoretical Benefit and Limitation of Diffusion Language Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9OyMsbuzL5": {
    "title": "OpenMMEgo: Enhancing Egocentric Understanding for LMMs with Open Weights and Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l1n22nHG4A": {
    "title": "Image Stitching in Adverse Condition: A Bidirectional-Consistency Learning Framework and Benchmark",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9dEU1gq3v9": {
    "title": "Exploring the Design Space of Diffusion Bridge Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4NaW9mbTqq": {
    "title": "Seeing the Wind from a Falling Leaf",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=43C31u7nxV": {
    "title": "PoGDiff: Product-of-Gaussians Diffusion Models for Imbalanced Text-to-Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ppOCvEonKT": {
    "title": "DeltaPhi: Physical States Residual Learning for Neural Operators in Data-Limited PDE Solving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VvZBJItmtV": {
    "title": "Greedy Algorithms for Structured Bandits: A Sharp Characterization of Asymptotic Success / Failure",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TYoYJStuN9": {
    "title": "A Reliable Cryptographic Framework for Empirical Machine Unlearning Evaluation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qla5PqFL0s": {
    "title": "A Unified Reasoning Framework for Holistic Zero-Shot Video Anomaly Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Awl3ZhDHuQ": {
    "title": "Learning to Better Search with Language Models via Guided Reinforced Self-Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GUPx2otaKL": {
    "title": "OOD-Barrier: Build a Middle-Barrier for Open-Set Single-Image Test Time Adaptation via Vision Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6LGyChG6Ep": {
    "title": "REArtGS: Reconstructing and Generating Articulated Objects via 3D Gaussian Splatting with Geometric and Motion Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FeUGQ6AiKR": {
    "title": "Flux4D: Flow-based Unsupervised 4D Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2K9QsDaqkM": {
    "title": "On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wj4lM45xQR": {
    "title": "LayerNavigator: Finding Promising Intervention Layers for Efficient Activation Steering in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fSPRy5uw0R": {
    "title": "Bilevel Network Learning via Hierarchically Structured Sparsity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rBlWKIUQey": {
    "title": "Search and Refine During Think: Facilitating Knowledge Refinement for Improved Retrieval-Augmented Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AQ21krZgax": {
    "title": "Formal Models of Active Learning from Contrastive Examples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VUoY5kacG5": {
    "title": "PurpCode: Reasoning for Safer Code Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dR58v9Dd42": {
    "title": "Continuity and Isolation Lead to Doubts or Dilemmas in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CH76rSKWZr": {
    "title": "Test-Time Adaptation of Vision-Language Models for Open-Vocabulary Semantic Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hio3T2OwHB": {
    "title": "Toward Human Deictic Gesture Target Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ojnzhb9EqA": {
    "title": "Adaptive Data-Borrowing for Improving Treatment Effect Estimation using External Controls",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vS1M06Px6u": {
    "title": "MLE-STAR: Machine Learning Engineering Agent via Search and Targeted Refinement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2fFRIIwau6": {
    "title": "Cognitive Predictive Processing: A Human-inspired Framework for Adaptive Exploration in Open-World Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pWzcPRIKp8": {
    "title": "Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0M1gi4P4ka": {
    "title": "Unleashing the Power of One-Step Diffusion based Image Super-Resolution via a Large-Scale Diffusion Discriminator",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t4irkwEXhr": {
    "title": "SingRef6D: Monocular Novel Object Pose Estimation with a Single RGB Reference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ul683x8lpK": {
    "title": "FlexWorld: Progressively Expanding 3D Scenes for Flexible-View Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BrmR69AhUg": {
    "title": "Synergy Between the Strong and the Weak: Spiking Neural Networks are Inherently Self-Distillers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dY29IUPTuy": {
    "title": "Rethinking Fair Federated Learning from Parameter and Client View",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=To2fQWv0zP": {
    "title": "DKDR: Dynamic Knowledge Distillation for Reliability in Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mGEPbyJ8OT": {
    "title": "FEEDBACK FRICTION: LLMs Struggle to Fully Incorporate External Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ATewcZPbDj": {
    "title": "Accelerating 3D Molecule Generative Models with Trajectory Diagnosis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vrbUfvcNZ6": {
    "title": "Faster Fixed-Point Methods for Multichain MDPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ygNaCTGUwJ": {
    "title": "IGD: Token Decisiveness Modeling via Information Gain in LLMs for Personalized Recommendation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DLt2Ep1S3q": {
    "title": "Automaton Constrained Q-Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LDjBDk3Czb": {
    "title": "BaRISTA: Brain Scale Informed Spatiotemporal Representation of Human Intracranial Neural Activity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MjOf5qnEX7": {
    "title": "Optimal Single-Policy Sample Complexity and Transient Coverage for Average-Reward Offline RL",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vgfG8sEVf9": {
    "title": "Investigating Hallucinations of Time Series Foundation Models through Signal Subspace Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yONFNHGoeP": {
    "title": "Logic-in-Frames: Dynamic Keyframe Search via Visual Semantic-Logical Verification for Long Video Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ly2wXKIByI": {
    "title": "Open-World Drone Active Tracking with Goal-Centered Rewards",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s3MwCBuqav": {
    "title": "Unleashing Diffusion Transformers for Visual Correspondence by Modulating Massive Activations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MeawZGFIcT": {
    "title": "Towards Generalizable Detector for Generated Image",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fXG1BvwqGt": {
    "title": "Less is More: an Attention-free Sequence Prediction Modeling for Offline Embodied Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QRlVickNdN": {
    "title": "Continual Knowledge Adaptation for Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D8nHwexHNv": {
    "title": "Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wsR7VYXbdR": {
    "title": "HiMoLE: Towards OOD-Robust LoRA via Hierarchical Mixture of Experts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QeJOsm3qkS": {
    "title": "Dynamic Gaussian Splatting from Defocused and Motion-blurred Monocular Videos",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=573IcLusXq": {
    "title": "Brain-like Variational Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YJ7H3amL0k": {
    "title": "The Promise of RL for Autoregressive Image Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xvxgG668th": {
    "title": "DyMU: Dynamic Merging and Virtual Unmerging for Efficient Variable-Length VLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LRYgQuz7kY": {
    "title": "Physics-informed Value Learner for Offline Goal-Conditioned Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YbKdduMtyN": {
    "title": "1000+ FPS 4D Gaussian Splatting for Dynamic Scene Rendering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NNHgm6VJkC": {
    "title": "GeneMAN: Generalizable Single-Image 3D Human Reconstruction from Multi-Source Human Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VcRc99EdTZ": {
    "title": "PandaPose: 3D Human Pose Lifting from a Single Image via Propagating 2D Pose Prior to 3D Anchor Space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RpE4HeuX69": {
    "title": "Practical and Effective Code Watermarking for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R42O6v84cX": {
    "title": "DualMPNN: Harnessing Structural Alignments for High-Recovery Inverse Protein Folding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xpkJiQNC0E": {
    "title": "Role Bias in Diffusion Models: Diagnosing and Mitigating through Intermediate Decomposition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5VmjFzoHG2": {
    "title": "One Stone with Two Birds: A Null-Text-Null Frequency-Aware Diffusion Models for Text-Guided Image Inpainting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KWRKCXRwNg": {
    "title": "Ground-Compose-Reinforce: Grounding Language in Agentic Behaviours using Limited Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WdL3O58gde": {
    "title": "Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5l8GydIsby": {
    "title": "RESAnything: Attribute Prompting for Arbitrary Referring Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jl0O0MYLyh": {
    "title": "D-VST: Diffusion Transformer for Pathology-Correct Tone-Controllable Cross-Dye Virtual Staining of Whole Slide Images",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DLVn11YIHx": {
    "title": "MVSMamba: Multi-View Stereo with State Space Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hkvyosg7Yx": {
    "title": "Permissioned LLMs: Enforcing Access Control in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ftZEltGArK": {
    "title": "From Pose to Muscle: Multimodal Learning for Piano Hand Muscle Electromyography",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IJmTOL4Raz": {
    "title": "Stochastic Gradients under Nuisances",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1vSLxdJNq8": {
    "title": "Bringing SAM to new heights: leveraging elevation data for tree crown segmentation from drone imagery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0UGbZ0QAXi": {
    "title": "Cross-Domain Graph Data Scaling: A Showcase with Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SptbUlfhJg": {
    "title": "StyleGuard: Preventing Text-to-Image-Model-based Style Mimicry Attacks by Style Perturbations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D4j2K5lknb": {
    "title": "DynaNav: Dynamic Feature and Layer Selection for Efficient Visual Navigation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ou9HeYvNhB": {
    "title": "Contact Map Transfer with Conditional Diffusion Model for Generalizable Dexterous Grasp Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tO0YJqdgV9": {
    "title": "On the Coexistence and Ensembling of Watermarks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mmAVwmJgCP": {
    "title": "A Unified Analysis of Stochastic Gradient Descent with Arbitrary Data Permutations and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MWikv8GJfY": {
    "title": "Uncertainty-Based Smooth Policy Regularisation for Reinforcement Learning with Few Demonstrations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hT7Nj7SAQb": {
    "title": "Cross-Modal Representational Knowledge Distillation for Enhanced Spike-informed LFP Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MZ25Rt5DiP": {
    "title": "Differentially Private Relational Learning with Entity-level Privacy Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jOHgjZaGqd": {
    "title": "Dynamical modeling of nonlinear latent factors in multiscale neural activity with real-time inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n6SrVj7I0g": {
    "title": "MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PPcM4cWgbp": {
    "title": "Neural Thermodynamics: Entropic Forces in Deep and Universal Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ibLGUkBWlz": {
    "title": "Preference Learning with Lie Detectors can Induce Honesty or Evasion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gsi42ohBoM": {
    "title": "Personalized Safety in LLMs: A Benchmark and A Planning-Based Agent Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VIf1Ygh1Pv": {
    "title": "CORAL: Disentangling Latent Representations in Long-Tailed Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FhJLFLPqH5": {
    "title": "Semi-supervised Vertex Hunting, with Applications in Network and Text Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ir8u0crTcA": {
    "title": "UniGen: Enhanced Training & Test-Time Strategies for Unified Multimodal Understanding and Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tGLZj8GWx3": {
    "title": "The Structural Complexity of Matrix-Vector Multiplication",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N67DlqK5C4": {
    "title": "Certifying Stability of Reinforcement Learning Policies using Generalized Lyapunov Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Aj3wL41C7p": {
    "title": "Neuro-Spectral Architectures for Causal Physics-Informed Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9JX8XrTVEz": {
    "title": "Multiplayer Federated Learning: Reaching Equilibrium with Less Communication",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FZp6cheXt2": {
    "title": "Better Language Model Inversion by Compactly Representing Next-Token Distributions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qmk40WMlSC": {
    "title": "Learning Skill-Attributes for Transferable Assessment in Video",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SoA8rMxDaF": {
    "title": "Projecting Assumptions: The Duality Between Sparse Autoencoders and Concept Geometry",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5WyqKH9nOS": {
    "title": "Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=otIdC4tsYf": {
    "title": "Learning to Condition: A Neural Heuristic for Scalable MPE Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jvrfcl3z1V": {
    "title": "Differentiable Structure Learning and Causal Discovery for General Binary Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iHXCz8c204": {
    "title": "FLOWING: Implicit Neural Flows for Structure-Preserving Morphing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GaL0ja9ygG": {
    "title": "Turbocharging Gaussian Process Inference with Approximate Sketch-and-Project",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aXO0xg0ttW": {
    "title": "Adaptive Divergence Regularized Policy Optimization for Fine-tuning Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gGfWAC7sqg": {
    "title": "Coupling Generative Modeling and an Autoencoder with the Causal Bridge",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MqGZIJxZ1z": {
    "title": "Variational Uncertainty Decomposition for In-Context Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v6Oo0zO2oA": {
    "title": "LangHOPS: Language Grounded Hierarchical Open-Vocabulary Part Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lni933mlvF": {
    "title": "Adaptive Riemannian ADMM for Nonsmooth Optimization: Optimal Complexity without Smoothing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7n2Kv5BUz2": {
    "title": "MR. Video: MapReduce as an Effective Principle for Long Video Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oVLNtTT2l4": {
    "title": "A Computationally Viable Numerical Gradient-based Technique for Optimal Covering Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MnT0QgNWFC": {
    "title": "GeoClip: Geometry-Aware Clipping for Differentially Private SGD",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0mOBdNsI3L": {
    "title": "Approximately Aligned Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A3vDXf4PUO": {
    "title": "Thumb on the Scale: Optimal Loss Weighting in Last Layer Retraining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rOuRpOA6pm": {
    "title": "Physics-informed machine learning with domain decomposition and global dynamics for three-dimensional intersecting flows",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0ZnXGzLcOg": {
    "title": "Privacy Reasoning in Ambiguous Contexts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6gJ2ZykQ5W": {
    "title": "AION-1: Omnimodal Foundation Model for Astronomical Sciences",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RkHUDvy9QR": {
    "title": "Understanding while Exploring: Semantics-driven Active Mapping",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WYSCCw7mCe": {
    "title": "Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VsZzTSyk5p": {
    "title": "Absorb and Converge: Provable Convergence Guarantee for Absorbing Discrete Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FXWwYz1p8a": {
    "title": "Edit Flows: Variable Length Discrete Flow Matching with Sequence-Level Edit Operations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2ClM0g9OFT": {
    "title": "Correcting misinterpretations of additive models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fDl6RolijS": {
    "title": "Discrete Diffusion Models: Novel Analysis and New Sampler Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nirpTMWOzt": {
    "title": "G-Net: A Provably Easy Construction of High-Accuracy Random Binary Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=47iGNYmTQR": {
    "title": "A Cramér–von Mises Approach to Incentivizing Truthful Data Sharing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IIjiNTR1cV": {
    "title": "ZeroSep: Separate Anything in Audio with Zero Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aywsq2Bkom": {
    "title": "Learning-Augmented Online Bipartite Fractional Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k1sAEsMeyV": {
    "title": "Bayesian Optimization with Preference Exploration using a Monotonic Neural Network Ensemble",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1jDAYXfcS2": {
    "title": "This Time is Different: An Observability Perspective on Time Series Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Xmr8WTAld": {
    "title": "Generate, but Verify: Reducing Hallucination in Vision-Language Models with Retrospective Resampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PLBVtJt4td": {
    "title": "Analyzing Fine-Grained Alignment and Enhancing Vision Understanding in Multimodal Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9q6sNvAaqO": {
    "title": "i MIND: Insightful Multi-subject Invariant Neural Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=04p7u1gIsv": {
    "title": "Quantifying Uncertainty in the Presence of Distribution Shifts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BuYtcTUMyA": {
    "title": "AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dXqqFte3KT": {
    "title": "Less is More: Local Intrinsic Dimensions of Contextual Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jRnGy4ZzK5": {
    "title": "Extragradient Method for ( L 0 , L 1 ) -Lipschitz Root-finding Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YVZbaVikBp": {
    "title": "Generalization Error Analysis for Selective State-Space Models Through the Lens of Attention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nGEq3D6FFX": {
    "title": "Compress & Cache: Vision token compression for efficient generation and retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BH9niRIiy2": {
    "title": "HyPlaneHead: Rethinking Tri-plane-like Representations in Full-Head Image Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bYVxDz3JAt": {
    "title": "Graph Diffusion that can Insert and Delete",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sJbA3xMx1B": {
    "title": "FraPPE: Fast and Efficient Preference-Based Pure Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uG8kRtNGEI": {
    "title": "Fix False Transparency by Noise Guided Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WBcBhT1NKO": {
    "title": "Accelerated Sampling from Masked Diffusion Models via Entropy Bounded Unmasking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AJAEHgM8lc": {
    "title": "Conditional Gradient Methods with Standard LMO for Stochastic Simple Bilevel Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jTBxyQempF": {
    "title": "SparseDiT: Token Sparsification for Efficient Diffusion Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bYF7Gvv0s0": {
    "title": "Towards Physical Understanding in Video Generation: A 3D Point Regularization Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MU0JuT0A54": {
    "title": "Convergence Rates for Gradient Descent on the Edge of Stability for Overparametrised Least Squares",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x5lITYXmW2": {
    "title": "Act Only When It Pays: Efficient Reinforcement Learning for LLM Reasoning via Selective Rollouts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NGgLhJKttI": {
    "title": "One Subgoal at a Time: Zero-Shot Generalization to Arbitrary Linear Temporal Logic Requirements in Multi-Task Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8JLpE8YnjD": {
    "title": "Short-length Adversarial Training Helps LLMs Defend Long-length Jailbreak Attacks: Theoretical and Empirical Evidence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9YXk1RnC9o": {
    "title": "Fuse2Match: Training-Free Fusion of Flow, Diffusion, and Contrastive Models for Zero-Shot Semantic Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aTlVrJRrNg": {
    "title": "Online Two-Stage Submodular Maximization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xma2G1ak3H": {
    "title": "POCO: Scalable Neural Forecasting through Population Conditioning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8gsAg9TqhK": {
    "title": "Variational Inference with Mixtures of Isotropic Gaussians",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BDkNRlGmP9": {
    "title": "Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=45igeoC560": {
    "title": "Loquetier: A Virtualized Multi-LoRA Framework for Unified LLM Fine-tuning and Serving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AtSukKLcLM": {
    "title": "Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QNtJO0jTf3": {
    "title": "Learning Equilibria from Data: Provably Efficient Multi-Agent Imitation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=91l4ZTMpO4": {
    "title": "Best-of-N Jailbreaking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zj0L8xIZYP": {
    "title": "Sequential Monte Carlo for Policy Optimization in Continuous POMDPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mVRphqQKnb": {
    "title": "The Omni-Expert: A Computationally Efficient Approach to Achieve a Mixture of Experts in a Single Expert Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lJSAtyx9Uc": {
    "title": "HYPRL: Reinforcement Learning of Control Policies for Hyperproperties",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8ZdWmpYxT0": {
    "title": "AdaMSS: Adaptive Multi-Subspace Approach for Parameter-Efficient Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M1b7IuY6Co": {
    "title": "Constrained Sampling for Language Models Should Be Easy: An MCMC Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qh458ZamHm": {
    "title": "Synthesizing Performance Constraints for Evaluating and Improving Code Efficiency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZBlHEeSvKd": {
    "title": "PaTH Attention: Position Encoding via Accumulating Householder Transformations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gEpXPbX8VV": {
    "title": "Learning Generalizable Shape Completion with SIM(3) Equivariance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1wlZoi4w5o": {
    "title": "The Unseen Threat: Residual Knowledge in Machine Unlearning under Perturbed Samples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HFcQGutJJn": {
    "title": "Uncertainty Quantification for Physics-Informed Neural Networks with Extended Fiducial Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3G0IWDIoRG": {
    "title": "Top-H Decoding: Adapting the Creativity and Coherence with Bounded Entropy in Text Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g2vViuEVDS": {
    "title": "Intrinsic Goals for Autonomous Agents: Model-Based Exploration in Virtual Zebrafish Predicts Ethological Behavior and Whole-Brain Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6MmOy2Ji8V": {
    "title": "Value Gradient Guidance for Flow Matching Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GSAKL9tc7L": {
    "title": "FoGE: Fock Space inspired encoding for graph prompting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UXTR6ZYV1x": {
    "title": "Neural Combinatorial Optimization for Time Dependent Traveling Salesman Problem",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EbjFkpXMke": {
    "title": "SAO-Instruct: Free-form Audio Editing using Natural Language Instructions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KwHsZJatB8": {
    "title": "Stabilizing LTI Systems under Partial Observability: Sample Complexity and Fundamental Limits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XQXjTfrtwc": {
    "title": "Evolution of Information in Interactive Decision Making: A Case Study for Multi-Armed Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lDh9wSb9nP": {
    "title": "Regret Lower Bounds for Decentralized Multi-Agent Stochastic Shortest Path Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DxKP2E0xK2": {
    "title": "Universal Cross-Tokenizer Distillation via Approximate Likelihood Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q39uZC6RSo": {
    "title": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality Text-to-Video Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j7L5AiVqJQ": {
    "title": "Layer-Wise Modality Decomposition for Interpretable Multimodal Sensor Fusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rccgEdFTlH": {
    "title": "Final-Model-Only Data Attribution with a Unifying View of Gradient-Based Methods",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OuklL6Q3sO": {
    "title": "Fast Solvers for Discrete Diffusion Models: Theory and Applications of High-Order Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E7gH8L4yHi": {
    "title": "Janus-Pro-R1: Advancing Collaborative Visual Comprehension and Generation via Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cZMno8E3yp": {
    "title": "Preventing Shortcuts in Adapter Training via Providing the Shortcuts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M2HxuxqNrb": {
    "title": "MIBP-Cert: Certified Training against Data Perturbations with Mixed-Integer Bilinear Programs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=stpe7UeETz": {
    "title": "Corrector Sampling in Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cnrwqlr8dg": {
    "title": "What Moves the Eyes: Doubling Mechanistic Model Performance Using Deep Networks to Discover and Test Cognitive Hypotheses",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JRmIvBcnWc": {
    "title": "GRAPE: Optimize Data Mixture for Group Robust Multi-target Adaptive Pretraining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YY1MPKBHp7": {
    "title": "Two-Steps Diffusion Policy for Robotic Manipulation via Genetic Denoising",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ymmY3rrD1t": {
    "title": "Spectral Analysis of Diffusion Models with Application to Schedule Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IiEtQPGVyV": {
    "title": "Efficient semantic uncertainty quantification in language models via diversity-steered sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eZIqkDf8m2": {
    "title": "Generating Physically Sound Designs from Text and a Set of Physical Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m3huAdsaGI": {
    "title": "Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aAxk0cw6GW": {
    "title": "Differentiable extensions with rounding guarantees for combinatorial optimization over permutations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0T9lN3m3pc": {
    "title": "Rescaled Influence Functions: Accurate Data Attribution in High Dimension",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uQmUjgR8Er": {
    "title": "Estimating Hitting Times Locally at Scale",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S6SKa97Gm0": {
    "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iBLAYaEKdL": {
    "title": "Constrained Posterior Sampling: Time Series Generation with Hard Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A1Zte0ixms": {
    "title": "What's Producible May Not Be Reachable: Measuring the Steerability of Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KTDAbnFsQj": {
    "title": "Nemotron-Flash: Towards Latency-Optimal Hybrid Small Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b6SWqFEOSF": {
    "title": "Private Training Large-scale Models with Efficient DP-SGD",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C69741fMFX": {
    "title": "MUSTAFAR: Promoting Unstructured Sparsity for KV Cache Pruning in LLM Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9MLfWJZXTn": {
    "title": "Simple and Optimal Sublinear Algorithms for Mean Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uBEiZd2P0K": {
    "title": "Conformal Risk Training: End-to-End Optimization of Conformal Risk Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TVD7cVIPCp": {
    "title": "In-context Learning of Linear Dynamical Systems with Transformers: Approximation Bounds and Depth-separation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=epIGnGgcKD": {
    "title": "Stable Port-Hamiltonian Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DkSeM3AZVs": {
    "title": "Guiding LLM Decision-Making with Fairness Reward Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zo4zYTR8vn": {
    "title": "Analog Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dxK2QgEKvz": {
    "title": "Kuramoto Orientation Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ixl7e5pR52": {
    "title": "GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uBaFH7aQnC": {
    "title": "KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM Inference in Resource-Constrained Environments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1PVdiFbDz0": {
    "title": "On Extending Direct Preference Optimization to Accommodate Ties",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y9zhXirhCa": {
    "title": "Provably Efficient Multi-Task Meta Bandit Learning via Shared Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qNP0FNxiET": {
    "title": "Does Representation Guarantee Welfare?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KYb3sw847W": {
    "title": "Distance-informed Neural Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pDWwz9F7Zh": {
    "title": "Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ubrecCeZrc": {
    "title": "Regional Explanations: Bridging Local and Global Variable Importance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=duunKHvWKz": {
    "title": "Enhanced Cyclic Coordinate Descent Methods for Elastic Net Penalized Linear Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d6RH6W6cul": {
    "title": "A Unified Approach to Submodular Maximization Under Noise",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4GyTBGBVsB": {
    "title": "Black-Box Membership Inference Attack for LVLMs via Prior Knowledge-Calibrated Memory Probing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UE0cxjNnIw": {
    "title": "Scaling Up Active Testing to Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xpY3C8HxNh": {
    "title": "Escaping Collapse: The Strength of Weak Data for Large Language Model Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JHlBLb6Jpz": {
    "title": "Decreasing Entropic Regularization Averaged Gradient for Semi-Discrete Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V8FnYzDX35": {
    "title": "Exploiting Task Relationships in Continual Learning via Transferability-Aware Task Embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rH0aOLyjYQ": {
    "title": "NoisyGRPO: Incentivizing Multimodal CoT Reasoning via Noise Injection and Bayesian Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jd8UcXJDCz": {
    "title": "Unsupervised Learning for Optimal Transport plan prediction between unbalanced graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0DaJaV4vRj": {
    "title": "A Learning-Augmented Approach to Online Allocation Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7rBeyE4nie": {
    "title": "Differentially Private Gomory-Hu Trees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vWeyFCtYxx": {
    "title": "Data-Dependent Regret Bounds for Constrained MABs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NHz3BlszTR": {
    "title": "MindOmni: Unleashing Reasoning Generation in Vision Language Models with RGPO",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8x5OmcFtJV": {
    "title": "LEDiT: Your Length-Extrapolatable Diffusion Transformer without Positional Encoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZsySJqavh3": {
    "title": "New Parallel and Streaming Algorithms for Directed Densest Subgraph",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sT9nd1WQ76": {
    "title": "Taming Adversarial Constraints in CMDPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1YCb07JMyl": {
    "title": "Maximizing the Value of Predictions in Control: Accuracy Is Not Enough",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pV17ra3AxZ": {
    "title": "LaX: Boosting Low-Rank Training of Foundation Models via Latent Crossing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lbjKWBzK9k": {
    "title": "Do Neural Networks Need Gradient Descent to Generalize? A Theoretical Study",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1KTnLdrBR2": {
    "title": "Concept Incongruence: An Exploration of Time and Death in Role Playing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sa6icF1Mnd": {
    "title": "Fair Representation Learning with Controllable High Confidence Guarantees via Adversarial Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NWQ8KeoWje": {
    "title": "Markov Persuasion Processes: Learning to Persuade From Scratch",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ct60KFYMIf": {
    "title": "Pessimistic Data Integration for Policy Evaluation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A2pmvkqOgp": {
    "title": "Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3FTVceZQrh": {
    "title": "Semantic Surgery: Zero-Shot Concept Erasure in Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=56CgYnf9Dr": {
    "title": "HyperMARL: Adaptive Hypernetworks for Multi-Agent RL",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ycCi4SkzPH": {
    "title": "Coresets for Clustering Under Stochastic Noise",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=thc972q5a9": {
    "title": "Scalable and adaptive prediction bands with kernel sum-of-squares",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RDe4Ntw2oy": {
    "title": "When majority rules, minority loses: bias amplification of gradient descent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8VkTkb6WL6": {
    "title": "HEIR: Learning Graph-Based Motion Hierarchies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4exx1hUffq": {
    "title": "EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sTyrh0LjoH": {
    "title": "Quantitative convergence of trained neural networks to Gaussian processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O1abxStFcy": {
    "title": "Exact Expressive Power of Transformers with Padding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l1DDTSqFq7": {
    "title": "Beyond Scalars: Concept-Based Alignment Analysis in Vision Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2sZSxmswxn": {
    "title": "Unveiling Transformer Perception by Exploring Input Manifolds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zhgfM0dJ3F": {
    "title": "Constrained Optimization From a Control Perspective via Feedback Linearization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=psfk761b6H": {
    "title": "Thinker: Learning to Think Fast and Slow",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MmjW4VGKbh": {
    "title": "A Bayesian Fast-Slow Framework to Mitigate Interference in Non-Stationary Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5pHfYe10iX": {
    "title": "A Little Depth Goes a Long Way: The Expressive Power of Log-Depth Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xpi0LpWbvF": {
    "title": "Fourier Analysis Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xj0DXLQZCS": {
    "title": "World Models as Reference Trajectories for Rapid Motor Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C35FCYZBXp": {
    "title": "VIBE: Annotation-Free Video-to-Text Information Bottleneck Evaluation for TL;DR",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=apusEkBW3h": {
    "title": "Reconstruct, Inpaint, Test-Time Finetune: Dynamic Novel-view Synthesis from Monocular Videos",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9zD2i7YRot": {
    "title": "NoisyRollout: Reinforcing Visual Reasoning with Data Augmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NG7kM4wxaN": {
    "title": "MixAT: Combining Continuous and Discrete Adversarial Training for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BEYzDKMEhX": {
    "title": "FHGS: Feature-Homogenized Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ex72DkOeNS": {
    "title": "Sampling from multi-modal distributions with polynomial query complexity in fixed dimension via reverse diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FnFf7Ru2ur": {
    "title": "From Bytes to Ideas: Language Modeling with Autoregressive U-Nets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9lhijvd0fs": {
    "title": "Object-Centric Concept-Bottlenecks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6pjzFIyFBo": {
    "title": "Learning Chern Numbers of Multiband Topological Insulators with Gauge Equivariant Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ku3XdvO88g": {
    "title": "GuardReasoner-VL: Safeguarding VLMs via Reinforced Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k2wzVFXZmC": {
    "title": "Explainable Reinforcement Learning from Human Feedback to Improve Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IGowQfG5oA": {
    "title": "Taught Well Learned Ill: Towards Distillation-conditional Backdoor Attack",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tmbx9zGVWb": {
    "title": "Sampling 3D Molecular Conformers with Diffusion Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d4eAHgIttM": {
    "title": "Class conditional conformal prediction for multiple inputs by p-value aggregation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vKmWKHlQBe": {
    "title": "A Single-Loop Gradient Algorithm for Pessimistic Bilevel Optimization via Smooth Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UuhoYEaUhT": {
    "title": "Unbalanced Optimal Total Variation Transport: A Theoretical Approach to Spatial Resource Allocation Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=An0ePypuOJ": {
    "title": "Transition Matching: Scalable and Flexible Generative Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rIudtwY0VM": {
    "title": "Bridging Arbitrary and Tree Metrics via Differentiable Gromov Hyperbolicity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iD3UTLZ6B9": {
    "title": "BundleFlow: Deep Menus for Combinatorial Auctions by Diffusion-Based Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Bxn5XSgrO": {
    "title": "Transductive Conformal Inference for Full Ranking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ceCJPoZOKJ": {
    "title": "In-Context Learning of Stochastic Differential Equations with Foundation Inference Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JoBCAKbCpC": {
    "title": "CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human Head Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xoL5zo1O86": {
    "title": "Reasoning Is Not a Race: When Stopping Early Beats Going Deeper",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AQsko3PPUe": {
    "title": "Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WXo0DLSaIf": {
    "title": "Unified 2D-3D Discrete Priors for Noise-Robust and Calibration-Free Multiview 3D Human Pose Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iqBdAYwzmz": {
    "title": "Accelerating Model-Free Optimization via Averaging of Cost Samples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fjBsE0kKk4": {
    "title": "ZEUS: Zero-shot Embeddings for Unsupervised Separation of Tabular Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5b5wZg6Zeo": {
    "title": "A solvable model of learning generative diffusion: theory and insights",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6NczjqEcO5": {
    "title": "On the SAC-BL Algorithm for Anomaly Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4xvN7uOKZt": {
    "title": "Incentivizing Truthful Language Models via Peer Elicitation Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ooiHIklvN5": {
    "title": "Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ahJfROJOYt": {
    "title": "RIGNO: A Graph-based Framework For Robust And Accurate Operator Learning For PDEs On Arbitrary Domains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MNqBFIGT8l": {
    "title": "The Nuclear Route: Sharp Asymptotics of ERM in Overparameterized Quadratic Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=72CEG36B0Q": {
    "title": "VESSA: Video-based objEct-centric Self-Supervised Adaptation for Visual Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ntOPGmlrkV": {
    "title": "FLUX: Efficient Descriptor-Driven Clustered Federated Learning under Arbitrary Distribution Shifts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lh5sXuGfk8": {
    "title": "Securing the Language of Life: Inheritable Watermarks from DNA Language Models to Proteins",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2n6fd8QlEC": {
    "title": "Inference with correlated priors using sisters cells",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hKKbtN4cp9": {
    "title": "Differentiable Generalized Sliced Wasserstein Plans",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CBsANtjBV4": {
    "title": "Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=onyzhnhApp": {
    "title": "SSTAG: Structure-Aware Self-Supervised Learning Method for Text-Attributed Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aPPnmuuNhx": {
    "title": "Universal Visuo-Tactile Video Understanding for Embodied Interaction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FthMPOhgfp": {
    "title": "Spatial-Aware Decision-Making with Ring Attractors in Reinforcement Learning Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kE4XEY7Bbc": {
    "title": "Model–Behavior Alignment under Flexible Evaluation: When the Best-Fitting Model Isn't the Right One",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FB5nWEQV7K": {
    "title": "Improving Decision Trees through the Lens of Parameterized Local Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v4e5Fb3mQL": {
    "title": "CG-SSL: Concept-Guided Self-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TzHX2RWUdE": {
    "title": "Monitoring Risks in Test-Time Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LqgRi1avf5": {
    "title": "RobIA: Robust Instance-aware Continual Test-time Adaptation for Deep Stereo",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c768Z1FwDL": {
    "title": "NeuralSurv: Deep Survival Analysis with Bayesian Uncertainty Quantification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LpY1jgtk8I": {
    "title": "Learning Latent Variable Models via Jarzynski-adjusted Langevin Algorithm",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mJEBhuCim2": {
    "title": "DiCoFlex: Model-Agnostic Diverse Counterfactuals with Flexible Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XLa5Puhqzg": {
    "title": "Compact Memory for Continual Logistic Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UPELg2oUo3": {
    "title": "PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WRwr2YZ4zt": {
    "title": "TimeXL: Explainable Multi-modal Time Series Prediction with LLM-in-the-Loop",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=thJ6aFoKrh": {
    "title": "Value Diffusion Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RDdfQc5Ts1": {
    "title": "AOR: Anatomical Ontology-Guided Reasoning for Medical Large Multimodal Model in Chest X-Ray Interpretation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cWn3RXJQ7G": {
    "title": "Is Limited Participant Diversity Impeding EEG-based Machine Learning?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o2agVeHrW4": {
    "title": "TSENOR: Highly-Efficient Algorithm for Finding Transposable N:M Sparse Masks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7AMriz7I3K": {
    "title": "BeliefMapNav: 3D Voxel-Based Belief Map for Zero-Shot Object Navigation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bPNzBXl1n7": {
    "title": "Perturbation Bounds for Low-Rank Inverse Approximations under Noise",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vqLoLqUUNB": {
    "title": "Knowledge Starts with Practice: Knowledge-Aware Exercise Generative Recommendation with Adaptive Multi-Agent Cooperation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rdp1dLxyMI": {
    "title": "Shortcuts and Identifiability in Concept-based Models from a Neuro-Symbolic Lens",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IJGEtuVqwf": {
    "title": "Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vqaWAmuzRt": {
    "title": "EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ybH0avRV4n": {
    "title": "Beyond Pairwise Connections: Extracting High-Order Functional Brain Network Structures under Global Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NSKRyz07WD": {
    "title": "Model Editing for Vision Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=feuFyonHks": {
    "title": "CyIN: Cyclic Informative Latent Space for Bridging Complete and Incomplete Multimodal Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yEq201U9AM": {
    "title": "Policy Optimized Text-to-Image Pipeline Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WwzurufeFN": {
    "title": "E-BATS: Efficient Backpropagation-Free Test-Time Adaptation for Speech Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mQOaC3uH2r": {
    "title": "When Does Closeness in Distribution Imply Representational Similarity? An Identifiability Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LrBWGwVfCA": {
    "title": "Mind the GAP! The Challenges of Scale in Pixel-based Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KUHrL5NYHe": {
    "title": "SEGA: Shaping Semantic Geometry for Robust Hashing under Noisy Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jIJhQ51NxO": {
    "title": "Streaming Federated Learning with Markovian Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SBoxuusTUn": {
    "title": "Bayes optimal learning of attention-indexed models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aFf30XJpl4": {
    "title": "Revisiting Agnostic Boosting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o3x3RV8XHd": {
    "title": "Federated Multi-armed Bandits with Efficient Bit-Level Communications",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YwMHgqHYyT": {
    "title": "Small Singular Values Matter: A Random Matrix Analysis of Transformer Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5nBwN6XQOD": {
    "title": "Zero-shot protein stability prediction by inverse folding models: a free energy interpretation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uazfjnFL0G": {
    "title": "LUNA: Efficient and Topology-Agnostic Foundation Model for EEG Signal Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TJhHb6CscW": {
    "title": "Progress Reward Model for Reinforcement Learning via Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w9xETx7HT1": {
    "title": "Solving Inverse Problems with FLAIR",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dOGXKBL7IE": {
    "title": "Seeing What Matters: Generalizable AI-generated Video Detection with Forensic-Oriented Augmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=afZ1JeJjYj": {
    "title": "LinPrim: Linear Primitives for Differentiable Volumetric Rendering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7cqKVDgFZQ": {
    "title": "Curly Flow Matching for Learning Non-gradient Field Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dHOSTp8MBl": {
    "title": "SAM-R1: Leveraging SAM for Reward Feedback in Multimodal Segmentation via Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=64WeVllQjq": {
    "title": "Multi-Kernel Correlation-Attention Vision Transformer for Enhanced Contextual Understanding and Multi-Scale Integration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o8F5lNOTG6": {
    "title": "Continuous Q-Score Matching: Diffusion Guided Reinforcement Learning for Continuous-Time Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EdP45Yxdc3": {
    "title": "Orthogonal Survival Learners for Estimating Heterogeneous Treatment Effects from Time-to-Event Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A5O41ntKjk": {
    "title": "PRESCRIBE: Predicting Single-Cell Responses with Bayesian Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vVU1KTOsju": {
    "title": "Scaling Laws for Optimal Data Mixtures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Evz0xPema0": {
    "title": "Generative RLHF-V: Learning Principles from Multi-modal Human Preference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=odWYytUjl1": {
    "title": "Counterfactual reasoning: an analysis of in-context emergence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M1OqlaNrw7": {
    "title": "Dynamic Siamese Expansion Framework for Improving Robustness in Online Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ayR2JfRYRS": {
    "title": "PhysioWave: A Multi-Scale Wavelet-Transformer for Physiological Signal Representation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=STLolzI6q1": {
    "title": "Infrequent Exploration in Linear Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bXPC81PuFP": {
    "title": "VaporTok: RL-Driven Adaptive Video Tokenizer with Prior & Task Awareness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YBrBFn4nSM": {
    "title": "One SPACE to Rule Them All: Jointly Mitigating Factuality and Faithfulness Hallucinations in LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AOe1aUhEQQ": {
    "title": "Benford's Curse: Tracing Digit Bias to Numerical Hallucination in LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BpufPXvSdb": {
    "title": "Treatment Effect Estimation for Optimal Decision-Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bA9rhgWOHk": {
    "title": "Computational Algebra with Attention: Transformer Oracles for Border Basis Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s9NkfkUuEr": {
    "title": "Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bIt1x6SgHX": {
    "title": "TRACE: Contrastive learning for multi-trial time series data in neuroscience",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MO4U4mg0oT": {
    "title": "Test-Time Adaptive Object Detection with Foundation Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QrARiRYatf": {
    "title": "PairEdit: Learning Semantic Variations for Exemplar-based Image Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lB8YNp1YWv": {
    "title": "Strategic Cost Selection in Participatory Budgeting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yI55mj6anU": {
    "title": "Prompt Tuning Decision Transformers with Structured and Scalable Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ltOnam1gY7": {
    "title": "VTON-VLLM: Aligning Virtual Try-On Models with Human Preferences",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BOiQ7Kd5Lx": {
    "title": "Follow the Energy, Find the Path: Riemannian Metrics from Energy-Based Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0YUHjKjjUY": {
    "title": "Deep Legendre Transform",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rmL109fdAJ": {
    "title": "Optimal Regret of Bandits under Differential Privacy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PspS5w3MC8": {
    "title": "Robustness in Both Domains: CLIP Needs a Robust Text Encoder",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KN2UMEvUOb": {
    "title": "Visual Diversity and Region-aware Prompt Learning for Zero-shot HOI Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=onhjdWCxZY": {
    "title": "HiFC: High-efficiency Flash-based KV Cache Swapping for Scaling LLM Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QIv5aXEAcc": {
    "title": "Theoretical Investigation of Adafactor for Non-Convex Smooth Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sYeE1obXGG": {
    "title": "Point-MaDi: Masked Autoencoding with Diffusion for Point Cloud Pre-training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GDbyUZiARK": {
    "title": "HermesFlow: Seamlessly Closing the Gap in Multimodal Understanding and Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K6ijewH34E": {
    "title": "Bridging Brains and Concepts: Interpretable Visual Decoding from fMRI with Semantic Bottlenecks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9BpefUVeDP": {
    "title": "Optimal kernel regression bounds under energy-bounded noise",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tuA2R6gZEA": {
    "title": "LongMagpie: A Self-synthesis Method for Generating Large-scale Long-context Instructions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZTHYaSxqmq": {
    "title": "SynBrain: Enhancing Visual-to-fMRI Synthesis via Probabilistic Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BkSRQ1y37l": {
    "title": "Cancer Survival Analysis via Zero-shot Tumor Microenvironment Segmentation on Low-resolution Whole Slide Pathology Images",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X5Hk8aMs6w": {
    "title": "Self-Verification Provably Prevents Model Collapse in Recursive Synthetic Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9iYKXx5ieE": {
    "title": "Temporal-Difference Variational Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W2Ntqsq97T": {
    "title": "Conditional Distribution Compression via the Kernel Conditional Mean Embedding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fbTGCEfztk": {
    "title": "PhysDiff-VTON: Cross-Domain Physics Modeling and Trajectory Optimization for Virtual Try-On",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7dBPm5c5ue": {
    "title": "Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CDQ0MI4rLw": {
    "title": "ZEBRA: Towards Zero-Shot Cross-Subject Generalization for Universal Brain Visual Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hg5UGeAr1Q": {
    "title": "Why Knowledge Distillation Works in Generative Models: A Minimal Working Explanation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QPEAZ182Ps": {
    "title": "Estimating Model Performance Under Covariate Shift Without Labels",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I9VNWQ15Ni": {
    "title": "PRESTO: Preimage-Informed Instruction Optimization for Prompting Black-Box LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RE97LT26w8": {
    "title": "True Zero-Shot Inference of Dynamical Systems Preserving Long-Term Statistics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LuKlBH8DAT": {
    "title": "AugGen: Synthetic Augmentation using Diffusion Models Can Improve Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GBMzJLhsRj": {
    "title": "Provable Scaling Laws for the Test-Time Compute of Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BqFXm4HO9p": {
    "title": "Near-Optimal Quantum Algorithms for Computing (Coarse) Correlated Equilibria of General-Sum Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8e5SJJ1cjY": {
    "title": "SegGraph: Leveraging Graphs of SAM Segments for Few-Shot 3D Part Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b4pEeU44AB": {
    "title": "Learning Intractable Multimodal Policies with Reparameterization and Diversity Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5SqbLPaBww": {
    "title": "Epistemic Uncertainty for Generated Image Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0uo2GPd94f": {
    "title": "Towards a Geometric Understanding of Tensor Learning via the t-Product",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zsUOQRUFOy": {
    "title": "Accelerating Feature Conformal Prediction via Taylor Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wryhlhA8QI": {
    "title": "Emergent Risk Awareness in Rational Agents under Resource Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HW55AwGEC8": {
    "title": "Model Merging in Pre-training of Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EdpkLRZZvu": {
    "title": "PoseCrafter: Extreme Pose Estimation with Hybrid Video Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IhOgbtClHL": {
    "title": "Anytime-valid, Bayes-assisted, Prediction-Powered Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f4GBN307sm": {
    "title": "Preference Distillation via Value based Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w0xm9oG8im": {
    "title": "STRIDER: Navigation via Instruction-Aligned Structural Decision Space Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x0i7wvRLHK": {
    "title": "Exploring the limits of strong membership inference attacks on large language models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BFW1fkB8ck": {
    "title": "Modelling the control of offline processing with reinforcement learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gdyw9m5juh": {
    "title": "VITRIX-CLIPIN: Enhancing Fine-Grained Visual Understanding in CLIP via Instruction-Editing Data and Long Captions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SAlCQdk5lx": {
    "title": "Riemannian Consistency Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pKQcmLHoGG": {
    "title": "Conditioning Matters: Training Diffusion Policies is Faster Than You Think",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ig4gfspaOq": {
    "title": "Wavy Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qXSFkP0ELS": {
    "title": "Atom of Thoughts for Markov LLM Test-Time Scaling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gJ05Gm5VxQ": {
    "title": "Time-R1: Post-Training Large Vision Language Model for Temporal Video Grounding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qtPe7altLq": {
    "title": "Jamais Vu: Exposing the Generalization Gap in Supervised Semantic Correspondence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bLXfEMe1Dk": {
    "title": "Feature-Based Instance Neighbor Discovery: Advanced Stable Test-Time Adaptation in Dynamic World",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HfdzglsZQH": {
    "title": "Neurosymbolic Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UspcIEkK0y": {
    "title": "UGM2N: An Unsupervised and Generalizable Mesh Movement Network via M-Uniform Loss",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SR6ivhZCQK": {
    "title": "Impact of Dataset Properties on Membership Inference Vulnerability of Deep Transfer Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KAMsbarp3w": {
    "title": "Understanding and Rectifying Safety Perception Distortion in VLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eRgYGhFRgZ": {
    "title": "Causal-R: A Causal-Reasoning Geometry Problem Solver for Optimized Solution Exploration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fDjDVE4qdj": {
    "title": "Think Only When You Need with Large Hybrid-Reasoning Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yB5L6ryIkb": {
    "title": "FNOPE: Simulation-based inference on function spaces with Fourier Neural Operators",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eIojV2epgX": {
    "title": "Synergistic Tensor and Pipeline Parallelism",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DlkM0q4Cvk": {
    "title": "Dependency Matters: Enhancing LLM Reasoning with Explicit Knowledge Grounding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=86b23oNkg9": {
    "title": "Counterfactual Reasoning for Steerable Pluralistic Value Alignment of Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qmBMPInbZC": {
    "title": "What Can RL Bring to VLA Generalization? An Empirical Study",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oQYq9L1NVT": {
    "title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QzOBE4mi2N": {
    "title": "FedLPA: Local Prior Alignment for Heterogeneous Federated Generalized Category Discovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QQybz4enRc": {
    "title": "Safely Learning Controlled Stochastic Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T45dDL37V0": {
    "title": "SAD Neural Networks: Divergent Gradient Flows and Asymptotic Optimality via o-minimal Structures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5cgm5dV5hr": {
    "title": "UniZyme: A Unified Protein Cleavage Site Predictor Enhanced with Enzyme Active-Site Knowledge",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JRFMzQnYXl": {
    "title": "Stab-SGD: Noise-Adaptivity in Smooth Optimization with Stability Ratios",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zIbNGkaYij": {
    "title": "A Reinforcement Learning-based Bidding Strategy for Data Consumers in Auction-based Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TNZse5q2Tr": {
    "title": "Generating Creative Chess Puzzles",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Iv11TSweoJ": {
    "title": "I2-NeRF: Learning Neural Radiance Fields Under Physically-Grounded Media Interactions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j0D69KmUYQ": {
    "title": "PALQO: Physics-informed model for Accelerating Large-scale Quantum Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t22zHB6yQ0": {
    "title": "Interactive Anomaly Detection for Articulated Objects via Motion Anticipation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vBtfIafffU": {
    "title": "Distributional Adversarial Attacks and Training in Deep Hedging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bjV8Y38aFF": {
    "title": "Improving Model Representation and Reducing KV Cache via Skip Connections with First Value Heads",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PDgagkX9zj": {
    "title": "FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XdwPWKbxd9": {
    "title": "RvLLM: LLM Runtime Verification with Domain Knowledge",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q2JDxTDmJ5": {
    "title": "Learning with Restricted Boltzmann Machines: Asymptotics of AMP and GD in High Dimensions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G2uILEbcLF": {
    "title": "UEPI: Universal Energy-Behavior-Preserving Integrators for Energy Conservative/Dissipative Differential Equations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N4OFsehhi7": {
    "title": "Algorithm- and Data-Dependent Generalization Bounds for Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=68OW8tLSh2": {
    "title": "Towards Reliable LLM-based Robots Planning via Combined Uncertainty Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g42mGfR6We": {
    "title": "MoORE: SVD-based Model MoE-ization for Conflict- and Oblivion-Resistant Multi-Task Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ja2wA4UncJ": {
    "title": "DyG-Mamba: Continuous State Space Modeling on Dynamic Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BCWQ5w9aGd": {
    "title": "Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via Internal Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h3lyFa5e1W": {
    "title": "SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2VdsYVXLDl": {
    "title": "Zero-Shot Detection of LLM-Generated Text via Implicit Reward Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bSs0d6NLiw": {
    "title": "Toward Efficient Inference Attacks: Shadow Model Sharing via Mixture-of-Experts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NFvAa2hNzH": {
    "title": "Lifelong Test-Time Adaptation via Online Learning in Tracked Low-Dimensional Subspace",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZZMkEooail": {
    "title": "Optimal Spectral Transitions in High-Dimensional Multi-Index Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CYG3kmFsgM": {
    "title": "Pragmatic Heterogeneous Collaborative Perception via Generative Communication Mechanism",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=by8N8HCqAG": {
    "title": "Exponential Convergence Guarantees for Iterative Markovian Fitting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rdGMyTPhui": {
    "title": "Merging on the Fly Without Retraining: A Sequential Approach to Scalable Continual Model Merging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hmd8CqMo3E": {
    "title": "Semantic-guided Diverse Decoding for Large Language Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aAhhMr0TX9": {
    "title": "MIRAGE: Assessing Hallucination in Multimodal Reasoning Chains of MLLM",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vpp3FGNLi6": {
    "title": "NeuroPath: Neurobiology-Inspired Path Tracking and Reflection for Semantically Coherent Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4MvqmXnCEr": {
    "title": "Self-Verifying Reflection Helps Transformers with CoT Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YStO2i0oyK": {
    "title": "Opinion Maximization in Social Networks by Modifying Internal Opinions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t0cpjJxj1s": {
    "title": "Analyzing the Power of Chain of Thought through Memorization Capabilities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ixOpURt7wC": {
    "title": "Exploiting Dynamic Sparsity in Einsum",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hn8z3Ee4Xa": {
    "title": "Asymptotics of SGD in Sequence-Single Index Models and Single-Layer Attention Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w4qJ056WhI": {
    "title": "SpaceServe: Spatial Multiplexing of Complementary Encoders and Decoders for Multimodal LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U0WEtpp6kO": {
    "title": "Order-Level Attention Similarity Across Language Models: A Latent Commonality",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3pix9mTbTI": {
    "title": "Restricted Global-Aware Graph Filters Bridging GNNs and Transformer for Node Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u1lNQH5upa": {
    "title": "SpatialLM: Training Large Language Models for Structured Indoor Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8AtYSW3VdX": {
    "title": "Switchable Token-Specific Codebook Quantization For Face Image Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iYHaTWZORB": {
    "title": "Tight Bounds for Maximum Weight Matroid Independent Set and Matching in the Zero Communication Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yHi8Ao6GAe": {
    "title": "MAPLE: Multi-scale Attribute-enhanced Prompt Learning for Few-shot Whole Slide Image Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a8mYhA3Fz8": {
    "title": "Adv-SSL: Adversarial Self-Supervised Representation Learning with Theoretical Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gjjsDmlQRW": {
    "title": "Fairness-aware Anomaly Detection via Fair Projection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D6w7wIN360": {
    "title": "DynaPipe: Dynamic Layer Redistribution for Efficient Serving of LLMs with Pipeline Parallelism",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UlkcH5Ccrk": {
    "title": "Contrastive Representations for Temporal Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9AMNN3U7Ug": {
    "title": "GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PRCizVyL1K": {
    "title": "SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bMKUD39Vcc": {
    "title": "Learning Theory for Kernel Bilevel Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ghhKZ0NaQN": {
    "title": "DGSolver: Diffusion Generalist Solver with Universal Posterior Sampling for Image Restoration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GV82iAD70j": {
    "title": "SCoT: Unifying Consistency Models and Rectified Flows via Straight-Consistent Trajectories",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qpFAZyMhwz": {
    "title": "A Bayesian Approach to Contextual Dynamic Pricing using the Proportional Hazards Model with Discrete Price Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0KAdFd5zku": {
    "title": "Conditional Forecasts and Proper Scoring Rules for Reliable and Accurate Performative Predictions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ft971e7HH5": {
    "title": "IneqSearch: Hybrid Reasoning for Olympiad Inequality Proofs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V5kzCSeaXF": {
    "title": "Structured Temporal Causality for Interpretable Multivariate Time Series Anomaly Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hxfjmc95rl": {
    "title": "SONAR: Long-Range Graph Propagation Through Information Waves",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JEqlA8N88d": {
    "title": "No Loss, No Gain: Gated Refinement and Adaptive Compression for Prompt Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4KengZ1RNX": {
    "title": "Adjusting Initial Noise to Mitigate Memorization in Text-to-Image Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bqaiMuNzfK": {
    "title": "Fairness-aware Bayes Optimal Functional Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gUPGGCM4WH": {
    "title": "Chain-of-Retrieval Augmented Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a2JTVVvcEl": {
    "title": "Video-R1: Reinforcing Video Reasoning in MLLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xgvSwwlLah": {
    "title": "Efficiently Maintaining the Multilingual Capacity of MCLIP in Downstream Cross-Modal Retrieval Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KtaHv0YUyh": {
    "title": "miniF2F-Lean Revisited: Reviewing Limitations and Charting a Path Forward",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XF1OzY8mEI": {
    "title": "Towards Principled Unsupervised Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4VKVUmE1I8": {
    "title": "RAGRouter: Learning to Route Queries to Multiple Retrieval-Augmented Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TXHc1gEEIk": {
    "title": "Continual Release Moment Estimation with Differential Privacy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qGYim5iYYG": {
    "title": "Approximation and Generalization Abilities of Score-based Neural Network Generative Models for Sub-Gaussian Distributions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=klOr9y9nMU": {
    "title": "CORE: Reducing UI Exposure in Mobile Agents via Collaboration Between Cloud and Local LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=70wK1HKZPp": {
    "title": "Oracle-Efficient Combinatorial Semi-Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s3LMqrwwHJ": {
    "title": "FANS: A Flatness-Aware Network Structure for Generalization in Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2YPypUwWIv": {
    "title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NBNwoHfMyf": {
    "title": "Blockwise Flow Matching: Improving Flow Matching Models For Efficient High-Quality Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t6EPMcudln": {
    "title": "Layer-wise Update Aggregation with Recycling for Communication-Efficient Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e40dYCosQd": {
    "title": "SilentStriker: Toward Stealthy Bit-Flip Attacks on Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x2BsIdJJJW": {
    "title": "ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ppKDXf55lY": {
    "title": "DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IMmYPJSTPe": {
    "title": "Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eiLzaZqXas": {
    "title": "Universal Few-shot Spatial Control for Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rvRaxhcjc6": {
    "title": "Heavy-Ball Momentum Method in Continuous Time and Discretization Error Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mzxGGzeLCL": {
    "title": "EAReranker: Efficient Embedding Adequacy Assessment for Retrieval Augmented Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2jzGEudVdS": {
    "title": "Synergy over Discrepancy: A Partition-Based Approach to Multi-Domain LLM Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bbVB4ZaqEf": {
    "title": "FedQS: Optimizing Gradient and Model Aggregation for Semi-Asynchronous Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UFAKqq77e3": {
    "title": "Whose Instructions Count? Resolving Preference Bias in Instruction Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OpAGOfAhT0": {
    "title": "Aligning by Misaligning: Boundary-aware Curriculum Learning for Multimodal Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KstkQi6e7q": {
    "title": "Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment for Fast and Faithful VLM Captioning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MQ91RWfxMD": {
    "title": "Under the Shadow: Exploiting Opacity Variation for Fine-grained Shadow Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nsE0QN904q": {
    "title": "Kernel Learning with Adversarial Features: Numerical Efficiency and Adaptive Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BpbJc1Jfbv": {
    "title": "EchoShot: Multi-Shot Portrait Video Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7tmVW6ZMmU": {
    "title": "KINDLE: Knowledge-Guided Distillation for Prior-Free Gene Regulatory Network Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=418UYQKGqB": {
    "title": "Attention! Your Vision Language Model Could Be Maliciously Manipulated",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1nL84tQNnK": {
    "title": "Conformal Prediction for Causal Effects of Continuous Treatments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gm65gK3uOJ": {
    "title": "Glance2Gaze: Efficient Vision-Language Models from Glance Fusion to Gaze Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Sb7YIHK4e": {
    "title": "Sample Complexity of Distributionally Robust Average-Reward Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YjZYMHvlRs": {
    "title": "MixSignGraph: A Sign Sequence is Worth Mixed Graphs of Nodes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iD2OqoCs4D": {
    "title": "Δ Energy : Optimizing Energy Change During Vision-Language Alignment Improves both OOD Detection and OOD Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Cpw7YftBm": {
    "title": "LoSplit: Loss-Guided Dynamic Split for Training-Time Defense Against Graph Backdoor Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WPHpBnKvdq": {
    "title": "Multi-agent KTO: Enhancing Strategic Interactions of Large Language Model in Language Game",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GKLePUzyO8": {
    "title": "Recurrent Self-Attention Dynamics: An Energy-Agnostic Perspective from Jacobians",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ewyR20zwqA": {
    "title": "X-Mahalanobis: Transformer Feature Mixing for Reliable OOD Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FjNHmO39pp": {
    "title": "Measure gradients, not activations! Enhancing neuronal activity in deep reinforcement learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FFEFRQGlsS": {
    "title": "Learning from Delayed Feedback in Games via Extra Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i7LuLGc2mu": {
    "title": "Effective Neural Approximations for Geometric Optimization Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9G0qc90PyI": {
    "title": "Inexact Column Generation for Bayesian Network Structure Learning via Difference-of-Submodular Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I4PJYZvfW5": {
    "title": "Your Pre-trained LLM is Secretly an Unsupervised Confidence Calibrator",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qNNjudaNhb": {
    "title": "One Head to Rule Them All: Amplifying LVLM Safety through a Single Critical Attention Head",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z2vJpjopJk": {
    "title": "Learning to Flow from Generative Pretext Tasks for Neural Architecture Encoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0KnFWyzrbj": {
    "title": "DSCS: Fast CPDAG-Based Verification of Collapsible Submodels in High-Dimensional Bayesian Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iQHuVNte6w": {
    "title": "OmniVCus: Feedforward Subject-driven Video Customization with Multimodal Control Conditions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7wEvjzkNXg": {
    "title": "Unified Reinforcement and Imitation Learning for Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wrFaF1gA6R": {
    "title": "Risk-Averse Constrained Reinforcement Learning with Optimized Certainty Equivalents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8FN25PlktS": {
    "title": "Adaptive Batch-Wise Sample Scheduling for Direct Preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xL5kQNdN6k": {
    "title": "No-Regret Online Autobidding Algorithms in First-price Auctions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mf4V1SK0np": {
    "title": "Self-Supervised Contrastive Learning is Approximately Supervised Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3pORFyKzh1": {
    "title": "VideoRFT: Incentivizing Video Reasoning Capability in MLLMs via Reinforced Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pIZxEOZCId": {
    "title": "TabDPT: Scaling Tabular Foundation Models on Real Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G1jrjumK1b": {
    "title": "AF-UMC: An Alignment-Free Fusion Framework for Unaligned Multi-View Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=It69GYOep0": {
    "title": "Nearly-Linear Time and Massively Parallel Algorithms for k -anonymity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IKVkpjSJzJ": {
    "title": "EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CANUXhPoyn": {
    "title": "Flow-Based Policy for Online Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qTXlFwlggv": {
    "title": "Language Models Are Capable of Metacognitive Monitoring and Control of Their Internal Activations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gZzLjIYzH1": {
    "title": "Infinite-Width Limit of a Single Attention Layer: Analysis via Tensor Programs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XPyAukgsFf": {
    "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WabVVQKTUF": {
    "title": "Touch in the Wild: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M9JmlA6Cgf": {
    "title": "A Diffusion Model for Regular Time Series Generation from Irregular Data with Completion and Masking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hqULlozHuH": {
    "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising 1-Consistency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xrAqVVk2qe": {
    "title": "TP-MDDN: Task-Preferenced Multi-Demand-Driven Navigation with Autonomous Decision-Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JSbVO7dNYE": {
    "title": "Towards Multiscale Graph-based Protein Learning with Geometric Secondary Structural Motifs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j9kTHUzEDX": {
    "title": "Contextual Online Pricing with (Biased) Offline Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IYVknFxsJb": {
    "title": "System Prompt Optimization with Meta-Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9V3crVSPH7": {
    "title": "RAD: Training an End-to-End Driving Policy via Large-Scale 3DGS-based Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QcItn1s1jO": {
    "title": "Deep Edge Filter: Return of the Human-Crafted Layer in Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Oir5nKRKVP": {
    "title": "Learning Stochastic Multiscale Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GKX9w731gu": {
    "title": "FlowRefiner: A Robust Traffic Classification Framework against Label Noise",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YmbQ0qnQ76": {
    "title": "O ( T ) Static Regret and Instance Dependent Constraint Violation for Constrained Online Convex Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=89ZIautowR": {
    "title": "Lattice Boltzmann Model for Learning Real-World Pixel Dynamicity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d4mZyZB5I9": {
    "title": "Efficient Training-Free Online Routing for High-Volume Multi-LLM Serving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9YkEcAqiIK": {
    "title": "Lifelong Safety Alignment for Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r9zzTQLnxw": {
    "title": "Learning Gradient Boosted Decision Trees with Algorithmic Recourse",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rAuRLePL2R": {
    "title": "Týr-the-Pruner: Structural Pruning LLMs via Global Sparsity Distribution Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=daZ5x88IiE": {
    "title": "Constrained Linear Thompson Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hgJQcuDwm1": {
    "title": "SQL-R1: Training Natural Language to SQL Reasoning Model By Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Owu54YW8HR": {
    "title": "Zero-Shot Trajectory Planning for Signal Temporal Logic Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UPxyTg7dTL": {
    "title": "Training-free Detection of AI-generated images via Cropping Robustness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CGTnDiL01P": {
    "title": "TADA: Improved Diffusion Sampling with Training-free Augmented DynAmics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G34xkRFiUn": {
    "title": "Gains: Fine-grained Federated Domain Adaptation in Open Set",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XF4JM2MTSF": {
    "title": "CDFlow: Building Invertible Layers with Circulant and Diagonal Matrices",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vTWNVYuvuF": {
    "title": "Beyond Accuracy: Dissecting Mathematical Reasoning for LLMs Under Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jtMDzggo6M": {
    "title": "Learning Sparse Approximate Inverse Preconditioners for Conjugate Gradient Solvers on GPUs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eA3Aum0rpA": {
    "title": "Scaling Language-centric Omnimodal Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iDcPkDrlaW": {
    "title": "Towards Unsupervised Open-Set Graph Domain Adaptation via Dual Reprogramming",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fkYZKArFLF": {
    "title": "BrainFlow: A Holistic Pathway of Dynamic Neural System on Manifold",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mI9HqgVuTS": {
    "title": "Federated Dialogue-Semantic Diffusion for Emotion Recognition under Incomplete Modalities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jIq2zVhBLN": {
    "title": "UGoDIT: Unsupervised Group Deep Image Prior Via Transferable Weights",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TonANdlBoe": {
    "title": "Private Statistical Estimation via Truncation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1WkolvkTVi": {
    "title": "Rethinking Circuit Completeness in Language Models: AND, OR, and ADDER Gates",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cLJfumTWLI": {
    "title": "Flexible MOF Generation with Torsion-Aware Flow Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z6b7xavA3Y": {
    "title": "Generative Model Inversion Through the Lens of the Manifold Hypothesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9nlTapr2sd": {
    "title": "Dual-Comb Ghost Imaging with Transformer-Based Reconstruction for Optical Fiber Endomicroscopy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OH7U836jKk": {
    "title": "CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PlQqwb7Bte": {
    "title": "Continuous-time Riemannian SGD and SVRG Flows on Wasserstein Probabilistic Space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D1Iw4Unvfc": {
    "title": "ReAgent-V: A Reward-Driven Multi-Agent Framework for Video Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DqRbfiTdKK": {
    "title": "LLM at Network Edge: A Layer-wise Efficient Federated Fine-tuning Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xXVEsGWSs8": {
    "title": "Searching Efficient Semantic Segmentation Architectures via Dynamic Path Selection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zxfwVts5it": {
    "title": "MultiNet: Adaptive Multi-Viewed Subgraph Convolutional Networks for Graph Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LqgWOxDlrI": {
    "title": "Mechanism Design via the Interim Relaxation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hVoIz6xD9Q": {
    "title": "Adversarial Locomotion and Motion Imitation for Humanoid Policy Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7OdU0LYXLr": {
    "title": "On the Loss of Context Awareness in General Instruction Fine-tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EJ34X5VWwu": {
    "title": "Normalizing Flows are Capable Models for Continuous Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8q2kReYRDn": {
    "title": "TF-MAS: Training-free Mamba2 Architecture Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XLvHmzaHsx": {
    "title": "The Rich and the Simple: On the Implicit Bias of Adam and SGD",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RGViRzEeme": {
    "title": "Max Entropy Moment Kalman Filter for Polynomial Systems with Arbitrary Noise",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NLLLkiF3xa": {
    "title": "Johnson-Lindenstrauss Lemma Beyond Euclidean Geometry",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8s0qknrCVK": {
    "title": "Iterative Foundation Model Fine-Tuning on Multiple Rewards",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ro1a0MTRq5": {
    "title": "Uncertainty Quantification with the Empirical Neural Tangent Kernel",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1ktdvp1EYI": {
    "title": "Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5OnejG2SQH": {
    "title": "Masked Gated Linear Unit",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0i8ClSr3kQ": {
    "title": "Latent Chain-of-Thought for Visual Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XDTTwmjhAg": {
    "title": "Whole-Body Conditioned Egocentric Video Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w22e5MrS4X": {
    "title": "Large Stepsizes Accelerate Gradient Descent for Regularized Logistic Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QuG74VCsXF": {
    "title": "Visual Sync: Multi‑Camera Synchronization via Cross‑View Object Motion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g56WiaXKGF": {
    "title": "REOrdering Patches Improves Vision Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gH4BRa4ZP3": {
    "title": "Scaling Embedding Layers in Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VwPt1WDQNB": {
    "title": "Trajectory Balance with Asynchrony: Decoupling Exploration and Learning for Fast, Scalable LLM Post-Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VNTj7PGlrz": {
    "title": "Unlabeled Data Improves Fine-Grained Image Zero-shot Classification with Multimodal LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5qdIdWflzg": {
    "title": "Efficient PAC Learning for Realizable-Statistic Models via Convex Surrogates",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=efDNv5XvVo": {
    "title": "MS-GS: Multi-Appearance Sparse-View 3D Gaussian Splatting in the Wild",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WjhS0EpJH7": {
    "title": "Solving and Learning Partial Differential Equations with Variational Q-Exponential Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HKfZwLjSwQ": {
    "title": "LLM Query Scheduling with Prefix Reuse and Latency Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ak4tP0vvna": {
    "title": "Risk-Averse Total-Reward Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=elfvik1mkg": {
    "title": "When Can Model-Free Reinforcement Learning be Enough for Thinking?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bZ0MXXoldX": {
    "title": "Revisiting LRP: Positional Attribution as the Missing Ingredient for Transformer Explainability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9QPlVgpiQ2": {
    "title": "FedRACE: A Hierarchical and Statistical Framework for Robust Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iweeVl1RHU": {
    "title": "Gradient Alignment in Physics-informed Neural Networks: A Second-Order Optimization Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZSAWYtIwGg": {
    "title": "PT-MoE: An Efficient Finetuning Framework for Integrating Mixture-of-Experts into Prompt Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hhjnvIfvpa": {
    "title": "Efficiently Escaping Saddle Points under Generalized Smoothness via Self-Bounding Regularity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F05XG0rxOD": {
    "title": "Statistical Guarantees for High-Dimensional Stochastic Gradient Descent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OgLRVB78nM": {
    "title": "Actor-Free Continuous Control via Structurally Maximizable Q-Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fGOZjuzvJb": {
    "title": "Generative Data Augmentation via Diffusion Distillation, Adversarial Alignment, and Importance Reweighting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b6H64u6TqI": {
    "title": "Tiled Flash Linear Attention: More Efficient Linear RNN and xLSTM Kernels",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eGfi5k7RP6": {
    "title": "SOMBRL: Scalable and Optimistic Model-Based RL",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XOw7Yf8qN3": {
    "title": "DynaGuide: Steering Diffusion Polices with Active Dynamic Guidance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cbWApYUvZ9": {
    "title": "Finite-Time Analysis of Stochastic Nonconvex Nonsmooth Optimization on the Riemannian Manifolds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o0JSYq1TQ4": {
    "title": "ThermalGen: Style-Disentangled Flow-Based Generative Models for RGB-to-Thermal Image Translation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bTgxLGMGdF": {
    "title": "MATCH: Multi-faceted Adaptive Topo-Consistency for Semi-Supervised Histopathology Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eszmES7j1F": {
    "title": "Flatten Graphs as Sequences: Transformers are Scalable Graph Generators",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2oRz0NNOQQ": {
    "title": "Learning from A Single Markovian Trajectory: Optimality and Variance Reduction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PSvsmbCrGs": {
    "title": "Implicit Generative Property Enhancer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cjcm5LYVWm": {
    "title": "Compliant Residual DAgger: Improving Real-World Contact-Rich Manipulation with Human Corrections",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EfDIApcjgI": {
    "title": "Entropic Time Schedulers for Generative Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jH9TtAhMkp": {
    "title": "OrdShap: Feature Position Importance for Sequential Black-Box Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7LTTzYXyJ1": {
    "title": "Better Training Data Attribution via Better Inverse Hessian-Vector Products",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3veDGO9KiK": {
    "title": "Exploring the Noise Robustness of Online Conformal Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LyH2ISbOV8": {
    "title": "Automated Detection of Visual Attribute Reliance with a Self-Reflective Agent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G9TkX2sy8Z": {
    "title": "Ranking-based Preference Optimization for Diffusion Models from Implicit User Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hTZ0SJCGQX": {
    "title": "Learning from Reward-Free Offline Data: A Case for Planning with Latent Dynamics Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DxiP59Z81m": {
    "title": "REGen: Multimodal Retrieval-Embedded Generation for Long-to-Short Video Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y5xuVoVz7g": {
    "title": "Double Descent Meets Out-of-Distribution Detection: Theoretical Insights and Empirical Analysis on the Role of Model Complexity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VyjFOO9cFi": {
    "title": "Convergence of Clipped SGD on Convex ( L 0 , L 1 ) -Smooth Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xcrAO2jYPi": {
    "title": "Smooth Regularization for Efficient Video Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=axlGOT58e8": {
    "title": "LLM Safety Alignment is Divergence Estimation in Disguise",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WqZKdj4cnl": {
    "title": "Learning-Augmented Algorithms for k -median via Online Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R3YQrZYnQa": {
    "title": "Alias-Free ViT: Fractional Shift Invariance via Linear Attention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v7UqniC9pF": {
    "title": "TiRex: Zero-Shot Forecasting Across Long and Short Horizons with Enhanced In-Context Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=82r0lqYIWg": {
    "title": "Pretraining a Shared Q-Network for Data-Efficient Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N3E1cU8Cv3": {
    "title": "SDTagNet: Leveraging Text-Annotated Navigation Maps for Online HD Map Construction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MjkopMaVAI": {
    "title": "Bi-Level Decision-Focused Causal Learning for Large-Scale Marketing Optimization: Bridging Observational and Experimental Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IHX5V6zQpY": {
    "title": "LayerCraft: Enhancing Text-to-Image Generation with CoT Reasoning and Layered Object Integration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vu7C8peYqo": {
    "title": "Self-Training with Dynamic Weighting for Robust Gradual Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N9HLe9iPhj": {
    "title": "Can Agent Fix Agent Issues?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qpIRwMubs9": {
    "title": "ϵ -Seg: Sparsely Supervised Semantic Segmentation of Microscopy Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rKASv92Myl": {
    "title": "EVODiff: Entropy-aware Variance Optimized Diffusion Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b3tygCf3T7": {
    "title": "Private Geometric Median in Nearly-Linear Time",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qH70UC6SNy": {
    "title": "Accelerated Vertical Federated Adversarial Learning through Decoupling Layer-Wise Dependencies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=alCdOdH7zn": {
    "title": "UGG-ReID: Uncertainty-Guided Graph Model for Multi-Modal Object Re-Identification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CIeWaq0JDl": {
    "title": "ProfiX: Improving Profile-Guided Optimization in Compilers with Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ENp2kCdYE8": {
    "title": "Seg4Diff: Unveiling Open-Vocabulary Semantic Segmentation in Text-to-Image Diffusion Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CDEbgwJ6ET": {
    "title": "AMBER: Adaptive Mesh Generation by Iterative Mesh Resolution Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V0gQ8o3wsF": {
    "title": "Soft Task-Aware Routing of Experts for Equivariant Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zQK6IluJi3": {
    "title": "Dynamic Masking and Auxiliary Hash Learning for Enhanced Cross-Modal Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RSSk1SHxrH": {
    "title": "Monotone and Separable Set Functions: Characterizations and Neural Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NLf5qt6sUe": {
    "title": "Robust Estimation Under Heterogeneous Corruption Rates",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PZ7YLONKiI": {
    "title": "3D-GSRD: 3D Molecular Graph Auto-Encoder with Selective Re-mask Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5sgK63Zshg": {
    "title": "SteerConf: Steering LLMs for Confidence Elicitation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6Had86RHix": {
    "title": "MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UCV21BsuqA": {
    "title": "Classical Planning with LLM-Generated Heuristics: Challenging the State of the Art with Python Code",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kd6hcHUl9C": {
    "title": "MPCache: MPC-Friendly KV Cache Eviction for Efficient Private LLM Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dlQ1iUpQNf": {
    "title": "Human-assisted Robotic Policy Refinement via Action Preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x0KcjteNds": {
    "title": "Geometric Algebra-Enhanced Bayesian Flow Network for RNA Inverse Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PInrbKWnbV": {
    "title": "Uncertainty-Informed Meta Pseudo Labeling for Surrogate Modeling with Limited Labeled Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hp78SvTU2N": {
    "title": "Confidence-Aware With Prototype Alignment for Partial Multi-label Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aWWRPyGMie": {
    "title": "Hyper-GoalNet : Goal-Conditioned Manipulation Policy Learning with HyperNetworks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vejx32FeWt": {
    "title": "A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I6beY5rU64": {
    "title": "Multimodal Causal Reasoning for UAV Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C9JMOzKD9k": {
    "title": "Strassen Attention, Split VC Dimension and Compositionality in Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DAyKP1tvwI": {
    "title": "OLinear: A Linear Model for Time Series Forecasting in Orthogonally Transformed Domain",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RCXF0UEmuE": {
    "title": "Sample-efficient Learning of Concepts with Theoretical Guarantees: from Data to Concepts without Interventions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rcX3qxO0z6": {
    "title": "DAAC: Discrepancy-Aware Adaptive Contrastive Learning for Medical Time series",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zssWxiiJZ1": {
    "title": "Scalable and Cost-Efficient de Novo Template-Based Molecular Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PQYazNKEYo": {
    "title": "VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P4xaLYXBRe": {
    "title": "SPACE: Noise Contrastive Estimation Stabilizes Self-Play Fine-Tuning for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u3aRwVkBv1": {
    "title": "Convolution Goes Higher-Order: A Biologically Inspired Mechanism Empowers Image Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8uhXfdSJmA": {
    "title": "Pool Me Wisely: On the Effect of Pooling in Transformer-Based Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PsVp5Gxm0s": {
    "title": "Epistemic Uncertainty Estimation in Regression Ensemble Models with Pairwise Epistemic Estimators",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WFujqJ5UBV": {
    "title": "Path Gradients after Flow Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oUYmk8WaG0": {
    "title": "Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DrBwhFUqCY": {
    "title": "When Models Don't Collapse: On the Consistency of Iterative MLE",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1Qo6DzdPOG": {
    "title": "From Sequence to Structure: Uncovering Substructure Reasoning in Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xgmMdTTzjj": {
    "title": "Modeling Neural Activity with Conditionally Linear Dynamical Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NMvMYtRjkg": {
    "title": "An Investigation of Memorization Risk in Healthcare Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qg4wmZSkWI": {
    "title": "ChatbotID: Identifying Chatbots with Granger Causality Test",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tM4cHBD7kD": {
    "title": "PseuZO: Pseudo-Zeroth-Order Algorithm for Training Deep Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xaxuzubN31": {
    "title": "Multi-dataset Joint Pre-training of Emotional EEG Enables Generalizable Affective Computing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V13dSX1wAs": {
    "title": "PermLLM: Learnable Channel Permutation for N:M Sparse Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ghyYc7hgSU": {
    "title": "Probing Equivariance and Symmetry Breaking in Convolutional Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4DbDJYnX5W": {
    "title": "A Dynamic Learning Strategy for Dempster-Shafer Theory with Applications in Classification and Enhancement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=48L3BEtH8w": {
    "title": "Robust Explanations of Graph Neural Networks via Graph Curvatures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mcYO3DccbQ": {
    "title": "From stability of Langevin diffusion to convergence of proximal MCMC for non-log-concave sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OtAiYPP6GA": {
    "title": "On Logic-based Self-Explainable Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ywzGKDStrm": {
    "title": "Dendritic Resonate-and-Fire Neuron for Effective and Efficient Long Sequence Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xtKNbPTnMA": {
    "title": "Sampled Estimators For Softmax Must Be Biased",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U2AF01VJyg": {
    "title": "Channel Matters: Estimating Channel Influence for Multivariate Time Series",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cSuPEKNyAo": {
    "title": "Combinatorial Ski Rental Problem: Robust and Learning-Augmented Algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kkhRTTmXFV": {
    "title": "Local-Global Coupling Spiking Graph Transformer for Brain Disorders Diagnosis from Two Perspectives",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g9olLDlaqH": {
    "title": "Neural Emulator Superiority: When Machine Learning for PDEs Surpasses its Training Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=41ZbysfW4h": {
    "title": "Tree-Sliced Entropy Partial Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5TTUwJGqHT": {
    "title": "DON'T NEED RETRAINING: A Mixture of DETR and Vision Foundation Models for Cross-Domain Few-Shot Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B0Gfxhr8V5": {
    "title": "BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fLKrX29Zy6": {
    "title": "ToF-IP: Time-of-Flight Enhanced Sparse Inertial Poser for Real-time Human Motion Capture",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O58KDUfB4x": {
    "title": "Rethinking Losses for Diffusion Bridge Samplers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EpGtAOr4vs": {
    "title": "Out-of-Distribution Generalized Graph Anomaly Detection with Homophily-aware Environment Mixup",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=utXSSdD9mt": {
    "title": "A Generalist Intracortical Motor Decoder",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N7Vg94l0gA": {
    "title": "MS-BART: Unified Modeling of Mass Spectra and Molecules for Structure Elucidation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2EJrs3gUO6": {
    "title": "Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iT2ZisemFs": {
    "title": "ShapeEmbed: a self-supervised learning framework for 2D contour quantification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yw9TdHy18N": {
    "title": "Put CASH on Bandits: A Max K-Armed Problem for Automated Machine Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RfNiN2rENM": {
    "title": "Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Video Temporal Grounding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3qeTs05bRL": {
    "title": "Bayesian Ego-graph inference for Networked Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ONE9LYBQYS": {
    "title": "Scale-invariant attention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x46uZ1HarC": {
    "title": "Disentangled Representation Learning via Modular Compositional Bias",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uaQWgFk2Pf": {
    "title": "FairImagen: Post-Processing for Bias Mitigation in Text-to-Image Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jt9iMeQQvM": {
    "title": "MixPrompt: Efficient Mixed Prompting for Multimodal Semantic Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z67on2D0j1": {
    "title": "Equilibrium Policy Generalization: A Reinforcement Learning Framework for Cross-Graph Zero-Shot Generalization in Pursuit-Evasion Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aqgLyQOECN": {
    "title": "Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PhoyfQG9Vb": {
    "title": "Performative Validity of Recourse Explanations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2ogTw5ue7v": {
    "title": "Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fmNQkLfsys": {
    "title": "Revolutionizing Graph Aggregation: From Suppression to Amplification via BoostGCN",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T98sh6P0Vm": {
    "title": "Improving the Euclidean Diffusion Generation of Manifold Data by Mitigating Score Function Singularity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iLYV4iIC0c": {
    "title": "Continuous Subspace Optimization for Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PBvlBI6c30": {
    "title": "FlowMoE: A Scalable Pipeline Scheduling Framework for Distributed Mixture-of-Experts Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eqvg92dZfL": {
    "title": "Graph Alignment via Birkhoff Relaxation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5aeD5UbiLv": {
    "title": "GIST: Greedy Independent Set Thresholding for Max-Min Diversification with Submodular Utility",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WCPAHpVAUp": {
    "title": "Optimal Graph Clustering without Edge Density Signals",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jDKhljBQb8": {
    "title": "Long-tailed Recognition with Model Rebalancing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4t8dYmu5vG": {
    "title": "Succeed or Learn Slowly: Sample Efficient Off-Policy Reinforcement Learning for Mobile App Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wNMK5o0Vfg": {
    "title": "S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ypPxYsmZPx": {
    "title": "DCA: Graph-Guided Deep Embedding Clustering for Brain Atlases",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H9BdN4f2vz": {
    "title": "FedIGL: Federated Invariant Graph Learning for Non-IID Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iBRfJY91QQ": {
    "title": "Exploiting the Asymmetric Uncertainty Structure of Pre-trained VLMs on the Unit Hypersphere",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BNWR4JOmdT": {
    "title": "SignFlow Bipartite Subgraph Network For Large-Scale Graph Link Sign Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OJMLzo8rEX": {
    "title": "HubGT: Fast Graph Transformer with Decoupled Hierarchy Labeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rEhVHla9zp": {
    "title": "BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sSZ9OM08KT": {
    "title": "Buffer layers for Test-Time Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GfyvbLYKMW": {
    "title": "Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shift with Unlabeled Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wlw8jkGscY": {
    "title": "Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=77zz0JTNjn": {
    "title": "DualOptim: Enhancing Efficacy and Stability in Machine Unlearning with Dual Optimizers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q3CU6ltlqE": {
    "title": "D 2 GS: Dense Depth Regularization for LiDAR-free Urban Scene Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r2fZv2pxMa": {
    "title": "HyperMixup: Hypergraph-Augmented with Higher-order Information Mixup",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cCYUFaR6En": {
    "title": "GVPO: Group Variance Policy Optimization for Large Language Model Post-Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N6ujq5Yfwa": {
    "title": "Uncertainty Estimation by Flexible Evidential Deep Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ORsrbGTXQB": {
    "title": "Protein Inverse Folding From Structure Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ckW70ls93V": {
    "title": "Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vFLrQgI6MW": {
    "title": "Boundary-to-Region Supervision for Offline Safe Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XbAtacZeEp": {
    "title": "Beyond Benign Overfitting in Nadaraya-Watson Interpolators",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aGB48BEN6N": {
    "title": "Improved Robust Estimation for Erdős-Rényi Graphs: The Sparse Regime and Optimal Breakdown Point",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2zZzdAMyYi": {
    "title": "Adaptive Fission: Post-training Encoding for Low-latency Spike Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gVtk4lzhcl": {
    "title": "Multi-View Oriented GPLVM: Expressiveness and Efficiency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p6yplmeC6F": {
    "title": "Neural B-frame Video Compression with Bi-directional Reference Harmonization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7DDmAHXWmi": {
    "title": "Bandit and Delayed Feedback in Online Structured Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=08mjueZ0Iq": {
    "title": "VLMLight: Safety-Critical Traffic Signal Control via Vision-Language Meta-Control and Dual-Branch Reasoning Architecture",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XVm8KOO3Ri": {
    "title": "GUI Exploration Lab: Enhancing Screen Navigation in Agents via Multi-Turn Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OeXukLC6SK": {
    "title": "Multimodal Negative Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qjV3YHW3PD": {
    "title": "Enhancing Safety in Reinforcement Learning with Human Feedback via Rectified Policy Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z0BgfL1FRV": {
    "title": "VETA-DiT: Variance-Equalized and Temporally Adaptive Quantization for Efficient 4-bit Diffusion Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7WgYEIOLdv": {
    "title": "HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AU2eaY2QEu": {
    "title": "Evolving and Regularizing Meta-Environment Learner for Fine-Grained Few-Shot Class-Incremental Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ruzMpz4rBC": {
    "title": "Language Ranker: A Lightweight Ranking framework for LLM Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M96edY67nS": {
    "title": "Repurposing AlphaFold3-like Protein Folding Models for Antibody Sequence and Structure Co-design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nK5WovvHk2": {
    "title": "Learning Temporal 3D Semantic Scene Completion via Optical Flow Guidance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y4AXO2pFAh": {
    "title": "Optimal Best Arm Identification under Differential Privacy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l2YPV99mnB": {
    "title": "Individually Fair Diversity Maximization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pdc0yEoOj6": {
    "title": "Leveraging robust optimization for llm alignment under distribution shifts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OFz4VDn0SO": {
    "title": "Tool-Augmented Spatiotemporal Reasoning for Streamlining Video Question Answering Task",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mQlX8ZF6vW": {
    "title": "Learning Simple Interpolants for Linear Integer Arithmetic",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b387eWFV3V": {
    "title": "Generalization Bounds for Kolmogorov-Arnold Networks (KANs) and Enhanced KANs with Lower Lipschitz Complexity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SurremoXPu": {
    "title": "BioCG: Constrained Generative Modeling for Biochemical Interaction Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DTvviEnW2A": {
    "title": "StreamBridge: Turning Your Offline Video Large Language Model into a Proactive Streaming Assistant",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MVlKSYR7HX": {
    "title": "MoonCast: High-Quality Zero-Shot Podcast Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gr9VtbcsPy": {
    "title": "Adaptive Data Analysis for Growing Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r2jhtAD0yZ": {
    "title": "Demystifying Spectral Feature Learning for Instrumental Variable Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KEVjRT4haB": {
    "title": "A Geometric Analysis of PCA",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YAycrn1Idm": {
    "title": "MIDAS: Misalignment-based Data Augmentation Strategy for Imbalanced Multimodal Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lCsVtkMusN": {
    "title": "Fast Computation and Optimization for Opinion-Based Quantities of Friedkin-Johnsen Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3k1z56W015": {
    "title": "Thompson Sampling for Multi-Objective Linear Contextual Bandit",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DQMjemrVhe": {
    "title": "What Do Latent Action Models Actually Learn?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ix4or1zPZw": {
    "title": "Diffusion Guided Adversarial State Perturbations in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RvCzlqwBXU": {
    "title": "One-Step Diffusion-Based Image Compression with Semantic Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i9Vn8vV99g": {
    "title": "Dynamical Properties of Tokens in Self-Attention and Effects of Positional Encoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VPm6afl0Sc": {
    "title": "Towards Generalizable Multi-Policy Optimization with Self-Evolution for Job Scheduling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oYR9qQIR3f": {
    "title": "End-to-End Vision Tokenizer Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Jz4PENUsRo": {
    "title": "Multi-Class Support Vector Machine with Differential Privacy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kdHz4y1ADc": {
    "title": "Enhancing Visual Prompting through Expanded Transformation Space and Overfitting Mitigation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yXitkQJmpj": {
    "title": "FuncGenFoil: Airfoil Generation and Editing Model in Function Space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1YzRtbaAzZ": {
    "title": "RUAGO: Effective and Practical Retain-Free Unlearning via Adversarial Attack and OOD Generator",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FrQd4NoQ16": {
    "title": "CReFT-CAD: Boosting Orthographic Projection Reasoning for CAD via Reinforcement Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4HZaFk9O4r": {
    "title": "Enabling Differentially Private Federated Learning for Speech Recognition: Benchmarks, Adaptive Optimizers, and Gradient Clipping",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QlDyoo8qLY": {
    "title": "Approximate Gradient Coding for Distributed Learning with Heterogeneous Stragglers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RVkuARTXet": {
    "title": "Holistic Order Prediction in Natural Scenes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1qKvZX4qnp": {
    "title": "FlexVAR: Flexible Visual Autoregressive Modeling without Residual Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cg2S1qqNSq": {
    "title": "Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tVRtDIwDmQ": {
    "title": "Enhancing Personalized Multi-Turn Dialogue with Curiosity Reward",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZyiBk1ZinG": {
    "title": "DreamPRM: Domain-reweighted Process Reward Model for Multimodal Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B3iPTZh7Za": {
    "title": "Meta-Learning an In-Context Transformer Model of Human Higher Visual Cortex",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ULblO61XZ0": {
    "title": "SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4q5ZYP0ynu": {
    "title": "Multiplication-Free Parallelizable Spiking Neurons with Efficient Spatio-Temporal Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0NdS4xCngO": {
    "title": "Reinforcement Learning Finetunes Small Subnetworks in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GctsZXLCpl": {
    "title": "Improving Generative Behavior Cloning via Self-Guidance and Adaptive Chunking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5szmKUMhB7": {
    "title": "DMol: A Highly Efficient and Chemical Motif-Preserving Molecule Generation Platform",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ayzWTxb9ZD": {
    "title": "Activation-Guided Consensus Merging for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=thHhKPlt8q": {
    "title": "On the Integration of Spatial-Temporal Knowledge: A Lightweight Approach to Atmospheric Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jTUIo4Kl6u": {
    "title": "Dense Backpropagation Improves Training for Sparse Mixture-of-Experts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rDqZjKIeda": {
    "title": "Can Dependencies Induced by LLM-Agent Workflows Be Trusted?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JkVQmaE5pK": {
    "title": "SMARTraj 2 : A Stable Multi-City Adaptive Method for Multi-View Spatio-Temporal Trajectory Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DfeHiyTz2W": {
    "title": "FedEL: Federated Elastic Learning for Heterogeneous Devices",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gt8iKyg12u": {
    "title": "VQ-Seg: Vector-Quantized Token Perturbation for Semi-Supervised Medical Image Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VGv5y60sXC": {
    "title": "Continuous Diffusion Model for Language Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BAscADMoQc": {
    "title": "Metropolis-Hastings Sampling for 3D Gaussian Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JRkFZl0TJ2": {
    "title": "RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l75RyRcevf": {
    "title": "Steering When Necessary: Flexible Steering Large Language Models with Backtracking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0THi1tG1HY": {
    "title": "Reduction-based Pseudo-label Generation for Instance-dependent Partial Label Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vm84b0Gksj": {
    "title": "DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KsmlBRhLgs": {
    "title": "Rainbow Delay Compensation: A Multi-Agent Reinforcement Learning Framework for Mitigating Observation Delays",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=77Kw7b7EYb": {
    "title": "HeroFilter: Adaptive Spectral Graph Filter for Varying Heterophilic Relations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ewgZItWaHh": {
    "title": "3D Gaussian Splatting based Scene-independent Relocalization with Unidirectional and Bidirectional Feature Fusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uAegI6Hh6G": {
    "title": "Squared families are useful conjugate priors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=15mlgnyaFt": {
    "title": "Diffusion Transformers for Imputation: Statistical Efficiency and Uncertainty Quantification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LfncIaLHnI": {
    "title": "GMV: A Unified and Efficient Graph Multi-View Learning Framework",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w1Y7RZC3QT": {
    "title": "Document Summarization with Conformal Importance Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wTxh9bKSXh": {
    "title": "DeepKD: A Deeply Decoupled and Denoised Knowledge Distillation Trainer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N2f6N4jopz": {
    "title": "GraphTOP: Graph Topology-Oriented Prompting for Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qUmULptJuY": {
    "title": "CoC-VLA: Delving into Adversarial Domain Transfer for Explainable Autonomous Driving via Chain-of-Causality Visual-Language-Action Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vzcVDwLtwA": {
    "title": "Efficient Utility-Preserving Machine Unlearning with Implicit Gradient Surgery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pAfdjWD9pN": {
    "title": "On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WyQ20adbUb": {
    "title": "Learning to Rank for In-Context Example Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vAxGuGmshO": {
    "title": "AlignVLM: Bridging Vision and Language Latent Spaces for Multimodal Document Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gkG8JOOUF4": {
    "title": "Adaptive Preference Arithmetic: A Personalized Agent with Adaptive Preference Arithmetic for Dynamic Preference Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EpEluUcQ6A": {
    "title": "Stealthy Yet Effective: Distribution-Preserving Backdoor Attacks on Graph Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XOiZ9ydssl": {
    "title": "Understanding challenges to the interpretation of disaggregated evaluations of algorithmic fairness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ENkss6GiKG": {
    "title": "KScope: A Framework for Characterizing the Knowledge Status of Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QQS7TudonJ": {
    "title": "Training-Free Safe Denoisers for Safe Use of Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dtH7hOwTeS": {
    "title": "Pairwise Calibrated Rewards for Pluralistic Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HF5A73jmxq": {
    "title": "DrivingRecon: Large 4D Gaussian Reconstruction Model For Autonomous Driving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lF6SHARvmG": {
    "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4JLZsmWBJf": {
    "title": "LiteReality: Graphic-Ready 3D Scene Reconstruction from RGB-D Scans",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h2ttG6HkID": {
    "title": "Counterfactual Identifiability via Dynamic Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eGcpmWdPg5": {
    "title": "An Information-theoretical Framework for Understanding Out-of-distribution Detection with Pretrained Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aixLLnS70r": {
    "title": "Fast Zeroth-Order Convex Optimization with Quantum Gradient Methods",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dRjt4vlYVQ": {
    "title": "Weaver: Shrinking the Generation-Verification Gap by Scaling Compute for Verification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GQXeLGYMda": {
    "title": "FEAT: Free energy Estimators with Adaptive Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GMvRmyunw4": {
    "title": "Re-ttention: Ultra Sparse Visual Generation via Attention Statistical Reshape",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q1W0O5p1w1": {
    "title": "ADMN: A Layer-Wise Adaptive Multimodal Network for Dynamic Input Noise and Compute Resources",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=29FRqmVQK8": {
    "title": "Scalable Best-of-N Selection for Large Language Models via Self-Certainty",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dsUK4Xql8Z": {
    "title": "Object-centric binding in Contrastive Language-Image Pretraining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TLCMyeihR9": {
    "title": "Dependency Parsing is More Parameter-Efficient with Normalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NjxW4m6KdH": {
    "title": "Random Forest Autoencoders for Guided Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dH8mKmvADv": {
    "title": "Learning in Compact Spaces with Approximately Normalized Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IBFnEaArnz": {
    "title": "Predicting the Performance of Black-box Language Models with Follow-up Queries",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wtcv48HImz": {
    "title": "Longer Context, Deeper Thinking: Uncovering the Role of Long-Context Ability in Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=drmIBh6v2U": {
    "title": "Disentangling misreporting from genuine adaptation in strategic settings: a causal approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nq4hnk2NNV": {
    "title": "On Hierarchies of Fairness Notions in Cake Cutting: From Proportionality to Super Envy-Freeness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ag2vRsIIQ7": {
    "title": "A General-Purpose Theorem for High-Probability Bounds of Stochastic Approximation with Polyak Averaging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yFdNygEryH": {
    "title": "Overcoming Sparsity Artifacts in Crosscoders to Interpret Chat-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3kVM0m60Q5": {
    "title": "Fast constrained sampling in pre-trained diffusion models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qR5B1Es9Ty": {
    "title": "On the Mechanisms of Weak-to-Strong Generalization: A Theoretical Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gQ8kIhu8JA": {
    "title": "Model Selection for Off-policy Evaluation: New Algorithms and Experimental Protocol",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=btBqWTbf6q": {
    "title": "Pre-trained Large Language Models Learn to Predict Hidden Markov Models In-context",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IkvQqD7hk3": {
    "title": "With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Yy2E8VWMY": {
    "title": "Audits Under Resource, Data, and Access Constraints: Scaling Laws For Less Discriminatory Alternatives",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ddyJqXyCxE": {
    "title": "Sparse Polyak: an adaptive step size rule for high-dimensional M-estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HTLa6Ao0jG": {
    "title": "Instant4D: 4D Gaussian Splatting in Minutes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2yekKXLCLc": {
    "title": "Vicinity-Guided Discriminative Latent Diffusion for Privacy-Preserving Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TiE8aTc3Zg": {
    "title": "Association-Focused Path Aggregation for Graph Fraud Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gz6dujD5j0": {
    "title": "Toward Interpretable Evaluation Measures for Time Series Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k4wAXbEPD6": {
    "title": "Individual Regret in Cooperative Stochastic Multi-Armed Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Nk61RqtN9y": {
    "title": "The Cost of Compression: Tight Quadratic Black-Box Attacks on Sketches for ℓ 2 Norm Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pdE9onSn2h": {
    "title": "SnapMoGen: Human Motion Generation from Expressive Texts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zoNpnBlJWh": {
    "title": "Robust Integrated Learning and Pauli Noise Mitigation for Parametrized Quantum Circuits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rEUbDhWaXh": {
    "title": "Embeddings as Probabilistic Equivalence in Logic Programs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BE6QmLdJqY": {
    "title": "Understanding Representation Dynamics of Diffusion Models via Low-Dimensional Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d8yAQOSLwv": {
    "title": "Gompertz Linear Units: Leveraging Asymmetry for Enhanced Learning Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cE2yJsFwhw": {
    "title": "Restricted Spectral Gap Decomposition for Simulated Tempering Targeting Mixture Distributions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XtNtockTSl": {
    "title": "FreeControl: Efficient, Training-Free Structural Control via One-Step Attention Extraction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3lVBn7U07P": {
    "title": "Finite Sample Analysis of Linear Temporal Difference Learning with Arbitrary Features",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vUtQFnlDyv": {
    "title": "DataRater: Meta-Learned Dataset Curation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yzvpEHNL70": {
    "title": "Optimistic Query Routing in Clustering-based Approximate Maximum Inner Product Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NFM8F5cV0V": {
    "title": "Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pJWozQn9p4": {
    "title": "A Fair Federated Learning Method for Handling Client Participation Probability Inconsistencies in Heterogeneous Environments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cIYguQc97T": {
    "title": "Template-Guided 3D Molecular Pose Generation via Flow Matching and Differentiable Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GbRVlyVOqH": {
    "title": "Dataset Distillation for Pre-Trained Self-Supervised Vision Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W0DDiJeZo6": {
    "title": "Vocabulary In-Context Learning in Transformers: Benefits of Positional Encoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jDJUjqUies": {
    "title": "CHiQPM: Calibrated Hierarchical Interpretable Image Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bzUdTkYrWe": {
    "title": "Thresholds for sensitive optimality and Blackwell optimality in stochastic games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iSvAAHGFSw": {
    "title": "A Theoretical Framework for Grokking: Interpolation followed by Riemannian Norm Minimisation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e5Evni5zZY": {
    "title": "Tracing Back the Malicious Clients in Poisoning Attacks to Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=49QC2D8exO": {
    "title": "ReDi: Rectified Discrete Flow",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wpkmEV57YT": {
    "title": "Learning to Focus: Causal Attention Distillation via Gradient‐Guided Token Pruning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A7WfzMiffv": {
    "title": "Competitive Advantage Attacks to Decentralized Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jR1lvwexLt": {
    "title": "Self-Evolving Pseudo-Rehearsal for Catastrophic Forgetting with Task Similarity in LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eJkkWbe18D": {
    "title": "ZigzagPointMamba: Spatial-Semantic Mamba for Point Cloud Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tv2McpEdyH": {
    "title": "Dynamic and Chemical Constraints to Enhance the Molecular Masked Graph Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KPlskOuZ46": {
    "title": "Towards Doctor-Like Reasoning: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EeAHhNwXPV": {
    "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gmwsy7TlFI": {
    "title": "Enhancing the Maximum Effective Window for Long-Term Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rMQvbxxmLe": {
    "title": "From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lGyXq0LOeQ": {
    "title": "EAP-GP: Mitigating Saturation Effect in Gradient-based Automated Circuit Identification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bChA28jPpm": {
    "title": "Pseudo-Riemannian Graph Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pm50n1szgP": {
    "title": "ImageSentinel: Protecting Visual Datasets from Unauthorized Retrieval-Augmented Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BUkXhMb7ml": {
    "title": "Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pLDpenGIjl": {
    "title": "Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8ftsTxZOLQ": {
    "title": "Certifying Concavity and Monotonicity in Games via Sum-of-Squares Hierarchies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4nDqyCS3h2": {
    "title": "Protocols for Verifying Smooth Strategies in Bandits and Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cfd5S9108a": {
    "title": "Natural Gradient VI: Guarantees for Non-Conjugate Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fb7maZToPl": {
    "title": "Hybrid-Collaborative Augmentation and Contrastive Sample Adaptive-Differential Awareness for Robust Attributed Graph Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XXCzRmQcJC": {
    "title": "Structure Matters: Dynamic Policy Gradient",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vLLYlSK6qJ": {
    "title": "Enhancing Zero-Shot Black-Box Optimization via Pretrained Models with Efficient Population Modeling, Interaction, and Stable Gradient Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dEi1S731lk": {
    "title": "Parallel Scaling Law for Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MlJyAvQaxp": {
    "title": "ATLAS: Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kN0YHWGDPH": {
    "title": "Effortless, Simulation-Efficient Bayesian Inference using Tabular Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1gCUv4SzaZ": {
    "title": "Connecting Jensen–Shannon and Kullback–Leibler Divergences: A New Bound for Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9ednYuGHN1": {
    "title": "Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sj5wiTCtu6": {
    "title": "Point4Bit: Post Training 4-bit Quantization for Point Cloud 3D Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8PHOPPH35D": {
    "title": "Faster Generic Identification in Tree-Shaped Structural Causal Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1GCbcBJt83": {
    "title": "Non-stationary Bandit Convex Optimization: A Comprehensive Study",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NYcnZaA4vO": {
    "title": "Fast Rate Bounds for Multi-Task and Meta-Learning with Different Sample Sizes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NQ8YNyX2uy": {
    "title": "The Computational Complexity of Counting Linear Regions in ReLU Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8P5MUySaqi": {
    "title": "Continual Optimization with Symmetry Teleportation for Multi-Task Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FBjTMq2Rby": {
    "title": "Multiclass Loss Geometry Matters for Generalization of Gradient Descent in Separable Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MNSiBGNAvx": {
    "title": "SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via Prune-then-Restore Mechanism",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D9jXj7nceM": {
    "title": "NTKMTL: Mitigating Task Imbalance in Multi-Task Learning from Neural Tangent Kernel Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=047VzZEpnu": {
    "title": "Spik-NeRF: Spiking Neural Networks for Neural Radiance Fields",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yFerzf9v1b": {
    "title": "Distil-E2D: Distilling Image-to-Depth Priors for Event-Based Monocular Depth Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Hjfzh5Eyk": {
    "title": "A TRIANGLE Enables Multimodal Alignment Beyond Cosine Similarity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BmRNz1TpCc": {
    "title": "GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=49ueGcxA8W": {
    "title": "XVerse: Consistent Multi-Subject Control of Identity and Semantic Attributes via DiT Modulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T64Fa2hCZn": {
    "title": "Dual-Path Temporal Decoder for End-to-End Multi-Object Tracking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qNHy8EPufN": {
    "title": "Seg2Any: Open-set Segmentation-Mask-to-Image Generation with Precise Shape and Semantic Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WK5JqIWiTW": {
    "title": "Image Token Matters: Mitigating Hallucination in Discrete Tokenizer-based Large Vision-Language Models via Latent Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w7Fe5rHLvJ": {
    "title": "Principled Model Routing for Unknown Mixtures of Source Domains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oLGtPYdRzU": {
    "title": "Attractive Metadata Attack: Inducing LLM Agents to Invoke Malicious Tools",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HL1j92hb6z": {
    "title": "First SFT, Second RL, Third UPT: Continual Improving Multi-Modal LLM Reasoning via Unsupervised Post-Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6oTJCnTkUA": {
    "title": "QBasicVSR: Temporal Awareness Adaptation Quantization for Video Super-Resolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BpAx3OuNOr": {
    "title": "Unlearned but Not Forgotten: Data Extraction after Exact Unlearning in LLM",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MtyF5hCI7Y": {
    "title": "ICLScan: Detecting Backdoors in Black-Box Large Language Models via Targeted In-context Illumination",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1nSynwHvu2": {
    "title": "Dynamic Bundling with Large Language Models for Zero-Shot Inference on Text-Attributed Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MhOhzdjK0e": {
    "title": "Deep learning for continuous-time stochastic control with jumps",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m7uj1vIZ62": {
    "title": "Retrosynthesis Planning via Worst-path Policy Optimisation in Tree-structured MDPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xzabk07lao": {
    "title": "On scalable and efficient training of diffusion samplers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fSFgcEVDT2": {
    "title": "Irrational Complex Rotations Empower Low-bit Optimizers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C7Ed8V44JY": {
    "title": "Text to Sketch Generation with Multi-Styles",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9e2H9DhKPa": {
    "title": "DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uS4Wmg7PmE": {
    "title": "Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y60FhgO07j": {
    "title": "Multi-step Visual Reasoning with Visual Tokens Scaling and Verification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fVs2BCjCqC": {
    "title": "Think before Recommendation: Autonomous Reasoning-enhanced Recommender",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GGj0QFSo5m": {
    "title": "MEgoHand: Multimodal Egocentric Hand-Object Interaction Motion Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PboL38rBrR": {
    "title": "MoE-Gyro: Self-Supervised Over-Range Reconstruction and Denoising for MEMS Gyroscopes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ewP2Gx85wb": {
    "title": "Split conformal classification with unsupervised calibration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o6keqobP13": {
    "title": "Bisecle: Binding and Separation in Continual Learning for Video Language Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oDA6t2RaFC": {
    "title": "Learning with Statistical Equality Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cRKLbmU4dT": {
    "title": "Robust Minimax Boosting with Performance Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b8j0Gbg4N3": {
    "title": "KeeA*: Epistemic Exploratory A* Search via Knowledge Calibration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m7zEbdAMsh": {
    "title": "Where and How to Perturb: On the Design of Perturbation Guidance in Diffusion and Flow Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cujJ9AXZO3": {
    "title": "Generalized and Invariant Single-Neuron In-Vivo Activity Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UKJkad8aUF": {
    "title": "Energy-based generator matching: A neural sampler for general state space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X9diEuva9R": {
    "title": "AREAL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A0aUS60Kvk": {
    "title": "On the Optimality of the Median-of-Means Estimator under Adversarial Contamination",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xtHJ0eNEUv": {
    "title": "Exploring Neural Granger Causality with xLSTMs: Unveiling Temporal Dependencies in Complex Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TgGH1bY6kl": {
    "title": "How Different from the Past? Spatio-Temporal Time Series Forecasting with Self-Supervised Deviation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Orpf8yDjdj": {
    "title": "The Curse of Depth in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hwKnBsXwXd": {
    "title": "Point or Line? Using Line-based Representation for Panoptic Symbol Spotting in CAD Drawings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8WKOk4U9R4": {
    "title": "Neural Collapse is Globally Optimal in Deep Regularized ResNets and Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CQBiYvXLgz": {
    "title": "Redundancy-Aware Test-Time Graph Out-of-Distribution Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0sUihPtncP": {
    "title": "SurfelSplat: Learning Efficient and Generalizable Gaussian Surfel Representations for Sparse-View Surface Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4iehXI36QG": {
    "title": "OpenOmni: Advancing Open-Source Omnimodal Large Language Models with Progressive Multimodal Alignment and Real-time Emotional Speech Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hdJXzKZjY9": {
    "title": "Reliable Lifelong Multimodal Editing: Conflict-Aware Retrieval Meets Multi-Level Guidance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JlVn0XRpy0": {
    "title": "xLSTM-Mixer: Multivariate Time Series Forecasting by Mixing via Scalar Memories",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=z5vZDI2r6J": {
    "title": "NeedleInATable: Exploring Long-Context Capability of Large Language Models towards Long-Structured Tables",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EeyvDitalf": {
    "title": "Fact-R1: Towards Explainable Video Misinformation Detection with Deep Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1Y8MXuJlIY": {
    "title": "Localist Topographic Expert Routing: A Barrel Cortex-Inspired Modular Network for Sensorimotor Processing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jDYuadVajk": {
    "title": "Scalable, Explainable and Provably Robust Anomaly Detection with One-Step Flow Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pzPyxXjHrT": {
    "title": "Interpretable Next-token Prediction via the Generalized Induction Head",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=brV7U7svgS": {
    "title": "RiboFlow: Conditional De Novo RNA Co-Design via Synergistic Flow Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t3NzYXXICp": {
    "title": "Empirical Study on Robustness and Resilience in Cooperative Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yhfx12Azz1": {
    "title": "Coreset for Robust Geometric Median: Eliminating Size Dependency on Outliers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cx1KfZerNY": {
    "title": "Videos are Sample-Efficient Supervisions: Behavior Cloning from Videos via Latent Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zL4JRfBr7R": {
    "title": "Consistency of Physics-Informed Neural Networks for Second-Order Elliptic Equations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=COifgrjzXR": {
    "title": "Self-Supervised Direct Preference Optimization for Text-to-Image Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3vLp3J7540": {
    "title": "A Multimodal BiMamba Network with Test-Time Adaptation for Emotion Recognition Based on Physiological Signals",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gxw10T7uOm": {
    "title": "OmniGen-AR: AutoRegressive Any-to-Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a8sJEH4Cjb": {
    "title": "NeSyPr: Neurosymbolic Proceduralization For Efficient Embodied Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3CRt6uJ77o": {
    "title": "A Signed Graph Approach to Understanding and Mitigating Oversmoothing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tWhkNXPshv": {
    "title": "Multiresolution Analysis and Statistical Thresholding on Dynamic Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aqtVpWl0gI": {
    "title": "One-Step Diffusion for Detail-Rich and Temporally Consistent Video Super-Resolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vj48eXaQDM": {
    "title": "Learned Prefix Caching for Efficient LLM Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T9OQ094qR8": {
    "title": "Convex Approximation of Two-Layer ReLU Networks for Hidden State Differential Privacy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=prGyR9id7X": {
    "title": "A Partition Cover Approach to Tokenization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AXlquRUO0S": {
    "title": "Sum Estimation under Personalized Local Differential Privacy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oedSIxYWql": {
    "title": "Online Learning of Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6ICFqmixlS": {
    "title": "Towards Thinking-Optimal Scaling of Test-Time Compute for LLM Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2sMk2ShRdP": {
    "title": "Precise Diffusion Inversion: Towards Novel Samples and Few-Step Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RDw0GU1rmS": {
    "title": "Variational Polya Tree",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jvVQeSMeGM": {
    "title": "Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7qq1UeCYL6": {
    "title": "Degrees of Freedom for Linear Attention: Distilling Softmax Attention with Optimal Feature Efficiency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=20JDhbJqn3": {
    "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PQcSYOBZii": {
    "title": "DC4GS: Directional Consistency-Driven Adaptive Density Control for 3D Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xMreCfgWok": {
    "title": "SGN: Shifted Window-Based Hierarchical Variable Grouping for Multivariate Time Series Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XgQVL1uP34": {
    "title": "Collapsing Taylor Mode Automatic Differentiation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X3zarVyJ5B": {
    "title": "Learning Relative Gene Expression Trends from Pathology Images in Spatial Transcriptomics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=72UR53jN7T": {
    "title": "ThinkAct: Vision-Language-Action Reasoning via Reinforced Visual Latent Planning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yBWRrqPwyN": {
    "title": "Lie Detector: Unified Backdoor Detection via Cross-Examination Framework",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Iytf59QZzl": {
    "title": "Preference Optimization by Estimating the Ratio of the Data Distribution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LVRGLn1Ejj": {
    "title": "UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hYjlAA6wXT": {
    "title": "Functional Virtual Adversarial Training for Semi-Supervised Time Series Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wztnOaiNz7": {
    "title": "Non-Convex Tensor Recovery from Tube-Wise Sensing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qbVbZWxUib": {
    "title": "Efficient Part-level 3D Object Generation via Dual Volume Packing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7F61opnmRi": {
    "title": "Functional Matching of Logic Subgraphs: Beyond Structural Isomorphism",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bAZilxOZEp": {
    "title": "Learning in Stackelberg Mean Field Games: A Non-Asymptotic Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YtwEVr2ONF": {
    "title": "Noise-Robustness Through Noise: A Framework combining Asymmetric LoRA with Poisoning MoE",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9z6ZzeKqPC": {
    "title": "Can Class-Priors Help Single-Positive Multi-Label Learning?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gXNOCWcnAW": {
    "title": "Bidirectional Representations Augmented Autoregressive Biological Sequence Generation: Application in De Novo Peptide Sequencing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MNduv07wAu": {
    "title": "System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZnsR3waLUo": {
    "title": "VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via View Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aaRYsPAcuE": {
    "title": "Local Learning for Covariate Selection in Nonparametric Causal Effect Estimation with Latent Variables",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L73CiyZvNy": {
    "title": "Motion4D: Learning 3D-Consistent Motion and Semantics for 4D Scene Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vusd1Hw2D9": {
    "title": "Multi-Agent Debate for LLM Judges with Adaptive Stability Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eD1VyYURiq": {
    "title": "DepthVanish: Optimizing Adversarial Interval Structures for Stereo-Depth-Invisible Patches",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v771lscnlS": {
    "title": "Last-Iterate Convergence of Smooth Regret Matching + Variants in Learning Nash Equilibria",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ktC3cDu320": {
    "title": "Adversarial Attacks against Closed-Source MLLMs via Feature Optimal Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6j9xJ9pBjm": {
    "title": "GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal Graph Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C9653lXaFO": {
    "title": "Multi-order Orchestrated Curriculum Distillation for Model-Heterogeneous Federated Graph Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FWflRgqt8X": {
    "title": "FRBNet: Revisiting Low-Light Vision through Frequency-Domain Radial Basis Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EaTRrceoU9": {
    "title": "Scalable Valuation of Human Feedback through Provably Robust Model Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zjq1CkKDGt": {
    "title": "GeoRanker: Distance-Aware Ranking for Worldwide Image Geolocalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gzQYPO0d7b": {
    "title": "DAIL: Beyond Task Ambiguity for Language-Conditioned Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0JSolJVzjd": {
    "title": "Improved Approximation Algorithms for Chromatic and Pseudometric-Weighted Correlation Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nw6Kx91J48": {
    "title": "LuxDiT: Lighting Estimation with Video Diffusion Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xk9GSBCfcn": {
    "title": "Train to Defend: First Defense Against Cryptanalytic Neural Network Parameter Extraction Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=59n2g6RqjT": {
    "title": "How to Auto-optimize Prompts for Domain Tasks? Adaptive Prompting and Reasoning through Evolutionary Domain Knowledge Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N5vXT7AGuo": {
    "title": "AffordBot: 3D Fine-grained Embodied Reasoning via Multimodal Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EyNzLH7BZK": {
    "title": "ViSPLA: Visual Iterative Self-Prompting for Language-Guided 3D Affordance Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XsQNqRdcdh": {
    "title": "Once Upon an Input: Reasoning via Per-Instance Program Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vcB1OwtWUZ": {
    "title": "Improving Model-Based Reinforcement Learning by Converging to Flatter Minima",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hDx3pFaYeH": {
    "title": "Kernel Density Steering: Inference-Time Scaling via Mode Seeking for Image Restoration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w9gEF4Iwtx": {
    "title": "CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CGNJL6CeV0": {
    "title": "Measuring AI Ability to Complete Long Software Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vLYpKbZBkD": {
    "title": "Generalization Bound of Gradient Flow through Training Trajectory and Data-dependent Kernel",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5Y8I2dKc91": {
    "title": "OmniCast: A Masked Latent Diffusion Model for Weather Forecasting Across Time Scales",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FBfN5tYl6b": {
    "title": "Optimal Estimation of the Best Mean in Multi-Armed Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MykdD1wZDV": {
    "title": "Asymptotically exact variational flows via involutive MCMC kernels",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HkTOnCUQ1z": {
    "title": "Single-pass Adaptive Image Tokenization for Minimum Program Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qXgGX081j3": {
    "title": "REN: Fast and Efficient Region Encodings from Patch-Based Image Encoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1cjLvtFOmL": {
    "title": "Block-Diagonal LoRA for Eliminating Communication Overhead in Tensor Parallel LoRA Serving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R9MUFuXG2o": {
    "title": "Information-Driven Design of Imaging Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bsjJ7c1mEV": {
    "title": "Matchings Under Biased and Correlated Evaluations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ice2BHIumz": {
    "title": "Steering Generative Models with Experimental Data for Protein Fitness Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qabko39AS5": {
    "title": "Regression-adjusted Monte Carlo Estimators for Shapley Values and Probabilistic Values",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pe18madbPm": {
    "title": "The Matrix: Infinite-Horizon World Generation with Real-Time Moving Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fz1mm2EsQ9": {
    "title": "Tracking and Understanding Object Transformations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A4t17y2lXa": {
    "title": "Robust Distributed Estimation: Extending Gossip Algorithms to Ranking and Trimmed Means",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GKt3VRaCU1": {
    "title": "seq-JEPA: Autoregressive Predictive Learning of Invariant-Equivariant World Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mUJU8LmhZY": {
    "title": "Adaptive Frontier Exploration on Graphs with Applications to Network-Based Disease Testing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pY65QWWFlm": {
    "title": "Lessons Learned: A Multi-Agent Framework for Code LLMs to Learn and Improve",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MJvwM5dBZM": {
    "title": "ShorterBetter: Guiding Reasoning Models to Find Optimal Inference Length for Efficient Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zefDc9oi5T": {
    "title": "RefLoRA: Refactored Low-Rank Adaptation for Efficient Fine-Tuning of Large Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DbJ2f80JF8": {
    "title": "Neural Collapse under Gradient Flow on Shallow ReLU Networks for Orthogonally Separable Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jHdZKRiOp8": {
    "title": "Efficient Spectral Control of Partially Observed Linear Dynamical Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AD8ksC9bw1": {
    "title": "Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ToNRHqX6xq": {
    "title": "MIP against Agent: Malicious Image Patches Hijacking Multimodal OS Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AZLj6ObEDF": {
    "title": "IBGS: Image-Based Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ecqTSIMR2o": {
    "title": "Vision‑Language‑Vision Auto‑Encoder: Scalable Knowledge Distillation from Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9tCJHfF6M4": {
    "title": "Tightening Regret Lower and Upper Bounds in Restless Rising Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TxedB8hI5O": {
    "title": "Scaling RL to Long Videos",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1iSnpztjbD": {
    "title": "Compositional Discrete Latent Code for High Fidelity, Productive Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tVpneDH5ov": {
    "title": "Deep Learning with Plausible Deniability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vvEVQrm9M8": {
    "title": "DUO: No Compromise to Accuracy Degradation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Rx6m16By6l": {
    "title": "Harmony in Divergence: Towards Fast, Accurate, and Memory-efficient Zeroth-order LLM Fine-tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SsHCyEBMLz": {
    "title": "BetaConform : Efficient MAP Estimation of LLM Ensemble Judgment Performance with Prior Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NKcwN347H7": {
    "title": "InfantAgent-Next: A Multimodal Generalist Agent for Automated Computer Interaction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3rRzYrpO70": {
    "title": "Steering Information Utility in Key-Value Memory for Language Model Post-Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0J0y9vSCWf": {
    "title": "Towards Identifiability of Hierarchical Temporal Causal Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0XCZWAo7wN": {
    "title": "Online Time Series Forecasting with Theoretical Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XOIKLlSiDq": {
    "title": "Can DPO Learn Diverse Human Values? A Theoretical Scaling Law",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zhMl4Smau7": {
    "title": "LoRA-EnVar: Parameter-Efficient Hybrid Ensemble Variational Assimilation for Weather Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Ia0KiVAut": {
    "title": "Improving Retrieval-Augmented Generation through Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=skS03tzYNw": {
    "title": "ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SZdfUp9kZY": {
    "title": "Temporal In‑Context Fine‑Tuning for Versatile Control of Video Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U806q3iILo": {
    "title": "Praxis-VLM: Vision-Grounded Decision Making via Text-Driven Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u3a2AX0icx": {
    "title": "Let LRMs Break Free from Overthinking via Self-Braking Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x8xtRQ5GIk": {
    "title": "No Object Is an Island: Enhancing 3D Semantic Segmentation Generalization with Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OcMpSh79aE": {
    "title": "HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CAB0EjD9EK": {
    "title": "CodeCrash: Exposing LLM Fragility to Misleading Natural Language in Code Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tRXt10xKc5": {
    "title": "ContextAgent: Context-Aware Proactive LLM Agents with Open-world Sensory Perceptions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hs3FrjwyVZ": {
    "title": "Learning When to Think: Shaping Adaptive Reasoning in R1-Style Models via Multi-Stage RL",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3fpYXmBiTY": {
    "title": "When Causal Dynamics Matter: Adapting Causal Strategies through Meta-Aware Interventions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1qKUVyymXs": {
    "title": "ACCO: Accumulate While You Communicate for Communication-Overlapped Sharded LLM Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Yk4GnB3DY": {
    "title": "See through the Dark: Learning Illumination-affined Representations for Nighttime Occupancy Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qjee4tiBGZ": {
    "title": "AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3L1sGJh3Li": {
    "title": "VLMs can Aggregate Scattered Training Patches",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xPQyRmZ8hz": {
    "title": "Rethinking Approximate Gaussian Inference in Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GStPx9lQEL": {
    "title": "Mesh Interpolation Graph Network for Dynamic and Spatially Irregular Global Weather Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2YxtR50mho": {
    "title": "Conditional Panoramic Image Generation via Masked Autoregressive Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0E2Um9n3YK": {
    "title": "An Efficient Local Search Approach for Polarized Community Discovery in Signed Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EpgMSwJY8t": {
    "title": "StreamBP: Memory-Efficient Exact Backpropagation for Long Sequence Training of LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w1b2Jqemj6": {
    "title": "Adversarial Graph Fusion for Incomplete Multi-view Semi-supervised Learning with Tensorial Imputation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rsTWR7z4PB": {
    "title": "Approximation theory for 1-Lipschitz ResNets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0enwkxV3sx": {
    "title": "VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Iguyg0LULD": {
    "title": "Variational Regularized Unbalanced Optimal Transport: Single Network, Least Action",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NOUF43YrIL": {
    "title": "Mol-LLaMA: Towards General Understanding of Molecules in Large Molecular Language Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VrXjAfdwrN": {
    "title": "Elastic Robust Unlearning of Specific Knowledge in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gwT1GOKiaO": {
    "title": "Open-Vocabulary Part Segmentation via Progressive and Boundary-Aware Strategy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TSd6GrXTFW": {
    "title": "Preference-Driven Multi-Objective Combinatorial Optimization with Conditional Computation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x1wZoyS0rC": {
    "title": "SceneWeaver: All-in-One 3D Scene Synthesis with an Extensible and Self-Reflective Agent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zEj1FSYCRn": {
    "title": "ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AQnjBIFCBQ": {
    "title": "CoFFT: Chain of Foresight-Focus Thought for Visual Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JMtVNZ8Np7": {
    "title": "Feature Unlearning: Theoretical Foundations and Practical Applications with Shuffling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OrmLtoFF60": {
    "title": "Covariate-moderated Empirical Bayes Matrix Factorization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xTWWKMxY1x": {
    "title": "ROSE: Remove Objects with Side Effects in Videos",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FU62P5IXhK": {
    "title": "Future-Aware End-to-End Driving: Bidirectional Modeling of Trajectory Planning and Scene Evolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z6DJJIN8IJ": {
    "title": "Modeling Cell Dynamics and Interactions with Unbalanced Mean Field Schrödinger Bridge",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aDTcN3yZGE": {
    "title": "Reinforcing the Diffusion Chain of Lateral Thought with Diffusion Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6jKed3sx4f": {
    "title": "Bi-Level Knowledge Transfer for Multi-Task Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ztVk8XNffY": {
    "title": "HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0rVD66dXqT": {
    "title": "Gaze-VLM: Bridging Gaze and VLMs through Attention Regularization for Egocentric Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ONebBepE9l": {
    "title": "Information-Theoretic Reward Decomposition for Generalizable RLHF",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=58Vr1KOWG9": {
    "title": "GPLQ: A General, Practical, and Lightning QAT Method for Vision Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X7ak8ohJPg": {
    "title": "Domain Adaptive Hashing Retrieval via VLM Assisted Pseudo-Labeling and Dual Space Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zb0FzAjWN4": {
    "title": "Diffusion-Driven Progressive Target Manipulation for Source-Free Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kThBNZTMaw": {
    "title": "Learning Across the Gap: Hybrid Multi-armed Bandits with Heterogeneous Offline and Online Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UFqqqJZwM7": {
    "title": "Progressive Data Dropout: An Embarrassingly Simple Approach to Train Faster",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tYyzs6nFgM": {
    "title": "GD 2 : Robust Graph Learning under Label Noise via Dual-View Prediction Discrepancy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tIA46eoqhn": {
    "title": "Hyperbolic Dataset Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fxDCgOruk0": {
    "title": "APOLLO: Automated LLM and Lean Collaboration for Advanced Formal Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yslRXs9gcJ": {
    "title": "Sparc3D: Sparse Representation and Construction for High-Resolution 3D Shapes Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Hweh7Wx8R": {
    "title": "Efficient and Generalizable Mixed-Precision Quantization via Topological Entropy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hPfICQIDOm": {
    "title": "Vicinal Label Supervision for Reliable Aleatoric and Epistemic Uncertainty Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PESrAH82Zh": {
    "title": "Vision Foundation Models as Effective Visual Tokenizers for Autoregressive Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WtMuGdHvh6": {
    "title": "S 2 M-Former: Spiking Symmetric Mixing Branchformer for Brain Auditory Attention Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H7e5RpeIi4": {
    "title": "Improving Time Series Forecasting via Instance-aware Post-hoc Revision",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lg51XnHt2H": {
    "title": "Conformal Prediction in The Loop: A Feedback-Based Uncertainty Model for Trajectory Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OEawM2coNT": {
    "title": "Partition to Evolve: Niching-enhanced Evolution with LLMs for Automated Algorithm Discovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hvaEMv4MVD": {
    "title": "SALoM: Structure Aware Temporal Graph Networks with Long-Short Memory Updater",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tgwjgDoaQq": {
    "title": "Time-Evolving Dynamical System for Learning Latent Representations of Mouse Visual Neural Activity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zfk5IoAtP0": {
    "title": "Gate to the Vessel: Residual Experts Restore What SAM Overlooks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6zfILEmD3u": {
    "title": "CoP: Agentic Red-teaming for Large Language Models using Composition of Principles",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yLApxMEja7": {
    "title": "DIFFSSR: Stereo Image Super-resolution Using Differential Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P69X3V4WwH": {
    "title": "StruDiCO: Structured Denoising Diffusion with Gradient-free Inference-stage Boosting for Memory and Time Efficient Combinatorial Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=05f6PdEPs2": {
    "title": "E-MoFlow: Learning Egomotion and Optical Flow from Event Data via Implicit Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Iyn0D3cFvt": {
    "title": "Efficient Federated Learning against Byzantine Attacks and Data Heterogeneity via Aggregating Normalized Gradients",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fa0kehLK6s": {
    "title": "SongBloom: Coherent Song Generation via Interleaved Autoregressive Sketching and Diffusion Refinement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gioV0q55AJ": {
    "title": "A Closer Look to Positive-Unlabeled Learning from Fine-grained Perspectives: An Empirical Study",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ERGIrG9GBR": {
    "title": "VLM-R³: Region Recognition, Reasoning, and Refinement for Enhanced Multimodal Chain-of-Thought",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KWFnxtO78b": {
    "title": "Convex Potential Mirror Langevin Algorithm for Efficient Sampling of Energy-Based Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DTq0CLhN4A": {
    "title": "Balancing Positive and Negative Classification Error Rates in Positive-Unlabeled Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KrzTGSqnK9": {
    "title": "Zero-Shot Blind-Spot Image Denoising via Cross-Scale Non-Local Pixel Refilling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dk2qprCnu8": {
    "title": "Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D Rewards",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RjN1LYymST": {
    "title": "Entropy Rectifying Guidance for Diffusion and Flow Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EDRvVEqjau": {
    "title": "CARE: Decoding-Time Safety Alignment via Rollback and Introspection Intervention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i99ZhFw6GN": {
    "title": "AegisGuard: RL-Guided Adapter Tuning for TEE-Based Efficient & Secure On-Device Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y0LHrY3x1S": {
    "title": "T2V-OptJail: Discrete Prompt Optimization for Text-to-Video Jailbreak Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zD5cUX67b9": {
    "title": "Continual Model Merging without Data: Dual Projections for Balancing Stability and Plasticity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4tRMm1JJhw": {
    "title": "RankSEG-RMA: An Efficient Segmentation Algorithm via Reciprocal Moment Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QFOhUboCvp": {
    "title": "Object Concepts Emerge from Motion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7N6pJb16sh": {
    "title": "Accident Anticipation via Temporal Occurrence Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y2eWc6jrlu": {
    "title": "Speculative Jacobi-Denoising Decoding for Accelerating Autoregressive Text-to-image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KYFTBpKIOr": {
    "title": "DAMamba: Vision State Space Model with Dynamic Adaptive Scan",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sv1b20eCnL": {
    "title": "The Fluorescent Veil: A Stealthy and Effective Physical Adversarial Patch Against Traffic Sign Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XIqlxqNDCL": {
    "title": "Who Reasons in the Large Language Models?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q7YnqREWLq": {
    "title": "Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and Cross-Modal Consistency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IwjkwtkPGb": {
    "title": "SimSort: A Data-Driven Framework for Spike Sorting by Large-Scale Electrophysiology Simulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UPHlqbZFZB": {
    "title": "VideoVLA: Video Generators Can Be Generalizable Robot Manipulators",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YJok3mWB0q": {
    "title": "Finite Sample Analyses for Continuous-time Linear Systems: System Identification and Online Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EXqp8sA37r": {
    "title": "T2SMark: Balancing Robustness and Diversity in Noise-as-Watermark for Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E0cjqfM55C": {
    "title": "Learning Interactive World Model for Object-Centric Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cuUsD5FJbe": {
    "title": "Enhancing GUI Agent with Uncertainty-Aware Self-Trained Evaluator",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3q6lJTN45T": {
    "title": "Uncertainty-Guided Exploration for Efficient AlphaZero Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V1FlwrsseI": {
    "title": "ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eFB9VlI3ew": {
    "title": "Place Cells as Multi-Scale Position Embeddings: Random Walk Transition Kernels for Path Planning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IIgVYnadfR": {
    "title": "BNMusic: Blending Environmental Noises into Personalized Music",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XKVhXWkPbp": {
    "title": "CPathAgent: An Agent-based Foundation Model for Interpretable High-Resolution Pathology Image Analysis Mimicking Pathologists' Diagnostic Logic",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=021PIPyOU1": {
    "title": "ALTER: All-in-One Layer Pruning and Temporal Expert Routing for Efficient Diffusion Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rXFzVRZsbt": {
    "title": "Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ou2TWBW4zm": {
    "title": "GeoLink: Empowering Remote Sensing Foundation Model with OpenStreetMap Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LiQH1MOCMs": {
    "title": "Towards Better & Faster Autoregressive Image Generation: From the Perspective of Entropy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QoiFdfZUJv": {
    "title": "CAD-Coder: Text-to-CAD Generation with Chain-of-Thought and Geometric Reward",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NQSWkmjODD": {
    "title": "Learning to Instruct for Visual Instruction Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yT8v2QFv5w": {
    "title": "Holistic Large-Scale Scene Reconstruction via Mixed Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TFhV2J2ZlA": {
    "title": "Novel Class Discovery for Point Cloud Segmentation via Joint Learning of Causal Representation and Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h0LzGQq6uO": {
    "title": "Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TdmzrkdLG0": {
    "title": "LLM Interpretability with Identifiable Temporal-Instantaneous Representation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qj8y1MjzLk": {
    "title": "Provable Watermarking for Data Poisoning Attacks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tM5mjMfFmS": {
    "title": "DuSA: Fast and Accurate Dual-Stage Sparse Attention Mechanism Accelerating Both Training and Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0yowBBK6tT": {
    "title": "GeoCAD: Local Geometry-Controllable CAD Generation with Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dSznm3mQTD": {
    "title": "LARGO: Latent Adversarial Reflection through Gradient Optimization for Jailbreaking LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EYxLmZRSK1": {
    "title": "COME: Adding Scene-Centric Forecasting Control to Occupancy World Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vO8LLoNWWk": {
    "title": "Learning to Reason under Off-Policy Guidance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8UMlKkCrNj": {
    "title": "Revisiting 1-peer exponential graph for enhancing decentralized learning efficiency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4xl00JWQ1z": {
    "title": "PointMAC: Meta-Learned Adaptation for Robust Test-Time Point Cloud Completion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xpi6YxNSiI": {
    "title": "Beyond Modality Collapse: Representation Blending for Multimodal Dataset Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2p4AtivyZz": {
    "title": "OPHR: Mastering Volatility Trading with Multi-Agent Deep Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9LoVCfMLDl": {
    "title": "Agnostic Continuous-Time Online Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jW8nBi6y9F": {
    "title": "Large Language Models Think Too Fast To Explore Effectively",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X6djSzDKps": {
    "title": "Bridging the Gap Between Cross-Domain Theory and Practical Application: A Case Study on Molecular Dissolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3qDFW07Lw7": {
    "title": "A Provable Approach for End-to-End Safe Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yGf8DSwR09": {
    "title": "Rebalancing Return Coverage for Conditional Sequence Modeling in Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L8pbQy0SsG": {
    "title": "Are Pixel-Wise Metrics Reliable for Computerized Tomography Reconstruction?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qDm3fpLYDW": {
    "title": "When Thinking Drifts: Evidential Grounding for Robust Video Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VhGUS8kyaC": {
    "title": "STRCMP : Integrating Graph Structural Priors with Language Models for Combinatorial Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1esFEjMUBS": {
    "title": "Flow Field Reconstruction with Sensor Placement Policy Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZlWxWlevZ0": {
    "title": "Bridging Critical Gaps in Convergent Learning: How Representational Alignment Evolves Across Layers, Training, and Distribution Shifts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xAHozxfuUW": {
    "title": "Conformal Information Pursuit for Interactively Guiding Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kZahfVKYbl": {
    "title": "Track, Inpaint, Resplat: Subject-driven 3D and 4D Generation with Progressive Texture Infilling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uOOlHOq500": {
    "title": "Variational Supervised Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=J0SbYYY0po": {
    "title": "Slow Transition to Low-Dimensional Chaos in Heavy-Tailed Recurrent Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zY5J1vp7tZ": {
    "title": "RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=G6K2NepP7S": {
    "title": "Can Multi-Modal LLMs Provide Live Step-by-Step Task Guidance?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w15atmg0uX": {
    "title": "Normalize Filters! Classical Wisdom for Deep Vision",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v5ru9MGjsW": {
    "title": "Generalization Guarantees for Learning Score-Based Branch-and-Cut Policies in Integer Programming",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yK4Xu7DDd6": {
    "title": "Beyond O ~ ( T ) Constraint Violation for Online Convex Optimization with Adversarial Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4zea5Bcemp": {
    "title": "Neural Networks for Learnable and Scalable Influence Estimation of Instruction Fine-Tuning Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3iPmM7cLzx": {
    "title": "ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting from Monocular Videos",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PhHrlDKcx1": {
    "title": "Discovering Latent Graphs with GFlowNets for Diverse Conditional Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rbYPITgH4Z": {
    "title": "Channel Simulation and Distributed Compression with Ensemble Rejection Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Lq5LnWLa9": {
    "title": "CoUn: Empowering Machine Unlearning via Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XyCDB1Uiqa": {
    "title": "Momentum-SAM: Sharpness Aware Minimization without Computational Overhead",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wav9TP5SOR": {
    "title": "Risk Bounds For Distributional Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JjTLq7SXaB": {
    "title": "TopER: Topological Embeddings in Graph Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LjmXrUsSrg": {
    "title": "Object-Centric Representation Learning for Enhanced 3D Semantic Scene Graph Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d3FjdOwLEd": {
    "title": "Smooth Quadratic Prediction Markets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eOLdGbXT6t": {
    "title": "ToolRL: Reward is All Tool Learning Needs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UQjhremICx": {
    "title": "The Adaptive Complexity of Minimizing Relative Fisher Information",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3uUmJzSSOW": {
    "title": "UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V0lnRCH73U": {
    "title": "Breaking the Gradient Barrier: Unveiling Large Language Models for Strategic Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UzEaeDovxl": {
    "title": "RGNMR: A Gauss-Newton method for robust matrix completion with theoretical guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m4sQcq5UK5": {
    "title": "Gatekeeper: Improving Model Cascades Through Confidence Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iSDKE8vec6": {
    "title": "Modeling the Economic Impacts of AI Openness Regulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t3CfT2y2Ag": {
    "title": "Wukong's 72 Transformations: High-fidelity Textured 3D Morphing via Flow Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TCxzqnbyAF": {
    "title": "Context-Aware Hierarchical Learning: A Two-Step Paradigm towards Safer LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fqfYzp4GKi": {
    "title": "Martingale Posterior Neural Networks for Fast Sequential Decision Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RTzbr0k56C": {
    "title": "One-Step Offline Distillation of Diffusion-based Models via Koopman Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HnYk0mMEIt": {
    "title": "JAFAR: Jack up Any Feature at Any Resolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iI7zx3ETyY": {
    "title": "Single GPU Task Adaptation of Pathology Foundation Models for Whole Slide Image Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dFlqhts0jS": {
    "title": "Towards Unsupervised Domain Bridging via Image Degradation in Semantic Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sv1pbyc8ZT": {
    "title": "UFM: A Simple Path towards Unified Dense Correspondence with Flow",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UB5u3KvOS1": {
    "title": "ScatterAD: Temporal-Topological Scattering Mechanism for Time Series Anomaly Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QSLlPljXxz": {
    "title": "Learning to Steer: Input-dependent Steering for Multimodal LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cuhQZdh2Oq": {
    "title": "Crucible: Quantifying the Potential of Control Algorithms through LLM Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=snmF71gb7Q": {
    "title": "Problem-Parameter-Free Decentralized Bilevel Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lUHwUuQGdV": {
    "title": "Towards Unified Multimodal Interleaved Generation via Group Relative Policy Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R8n2h7hzqS": {
    "title": "Flow Matching Neural Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wgeN1eD54P": {
    "title": "Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored Mixture-of-Experts Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c1snZ9dvCR": {
    "title": "SHGR: A Generalized Maximal Correlation Coefficient",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=U7gu9x1xzx": {
    "title": "PolyPose: Deformable 2D/3D Registration via Polyrigid Transformations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8nOMhDFpkU": {
    "title": "Traversal Verification for Speculative Tree Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E0033NpFm0": {
    "title": "Ultrametric Cluster Hierarchies: I Want ‘em All!",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SWsDJbIloF": {
    "title": "An Iterative Algorithm for Differentially Private k -PCA with Adaptive Noise",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Due3iZPa6u": {
    "title": "λ -Orthogonality Regularization for Compatible Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eYKz5M7Aws": {
    "title": "On the O ( d K 1 / 4 ) Convergence Rate of AdamW Measured by ℓ 1 Norm",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VBx4yMNtjt": {
    "title": "Scaling Law with Learning Rate Annealing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E0PaeSszLz": {
    "title": "A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D4gOopmVT2": {
    "title": "Constructing an Optimal Behavior Basis for the Option Keyboard",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NY3LzmUXl7": {
    "title": "Riemannian Flow Matching for Brain Connectivity Matrices via Pullback Geometry",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Pi1ON44YFP": {
    "title": "Sample and Map from a Single Convex Potential: Generation using Conjugate Moment Measures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2xeYuWd7WZ": {
    "title": "Lorentz Local Canonicalization: How to make any Network Lorentz-Equivariant",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6TmLco2L2D": {
    "title": "HoPE: Hybrid of Position Embedding for Long Context Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ioXn68lBjO": {
    "title": "DecompNet: Enhancing Time Series Forecasting Models with Implicit Decomposition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qmbG6u7DK0": {
    "title": "MOTION: Multi-Sculpt Evolutionary Coarsening for Federated Continual Graph Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YMPYLesItf": {
    "title": "GUI-Rise: Structured Reasoning and History Summarization for GUI Navigation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BDKkFwskot": {
    "title": "FALCON: Fine-grained Activation Manipulation by Contrastive Orthogonal Unalignment for Large Language Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8fECf5YbJY": {
    "title": "ADG: Ambient Diffusion-Guided Dataset Recovery for Corruption-Robust Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hVdD72iom4": {
    "title": "Watermarking Autoregressive Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=II0T40Q785": {
    "title": "FlowMixer: A Depth-Agnostic Neural Architecture for Interpretable Spatiotemporal Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SVHerutWxp": {
    "title": "CPPO: Accelerating the Training of Group Relative Policy Optimization-Based Reasoning Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cLbGkINOLP": {
    "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i9zoexiRFA": {
    "title": "Fairness under Competition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GDdGGXF4Kg": {
    "title": "A Closer Look at NTK Alignment: Linking Phase Transitions in Deep Image Regression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aXAkNlbnGa": {
    "title": "Joint Velocity-Growth Flow Matching for Single-Cell Dynamics Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=k1nSnc6rhr": {
    "title": "The Effect of Optimal Self-Distillation in Noisy Gaussian Mixture Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cxb5EsQHW3": {
    "title": "Rethinking Optimal Verification Granularity for Compute-Efficient Test-Time Scaling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lwIQC4MVJZ": {
    "title": "Efficient Large Language Model Inference with Neural Block Linearization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=isAYKdLwtB": {
    "title": "Effective Policy Learning for Multi-Agent Online Coordination Beyond Submodular Objectives",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UQT2inkLmb": {
    "title": "Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fqpbXJ2QtC": {
    "title": "Revitalizing SVD for Global Covariance Pooling: Halley's Method to Overcome Over-Flattening",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hk4cCTukeI": {
    "title": "Triplets Better Than Pairs: Towards Stable and Effective Self-Play Fine-Tuning for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y1z7SAS8q8": {
    "title": "Accurate and Efficient Low-Rank Model Merging in Core Space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1A4Nlibwl5": {
    "title": "Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yfcpdY4gMP": {
    "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Y2OabORdd": {
    "title": "Understanding Generalization in Physics Informed Models through Affine Variety Dimensions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=13BJ5FYG8E": {
    "title": "Learning a Cross-Modal Schrödinger Bridge for Visual Domain Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t2wMLGmcXd": {
    "title": "Neural Green's Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nUZaI7aRb2": {
    "title": "Compositional Reasoning with Transformers, RNNs, and Chain of Thought",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zPgPDHupcE": {
    "title": "Delving into Cascaded Instability: A Lipschitz Continuity View on Image Restoration and Object Detection Synergy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xJ2lGfFOv7": {
    "title": "One Prompt Fits All: Universal Graph Adaptation for Pretrained Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fPPfFMVTZo": {
    "title": "Continuous Simplicial Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fjoKScAL4D": {
    "title": "Towards General Modality Translation with Contrastive and Predictive Latent Diffusion Bridge",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=waHF2ekuf2": {
    "title": "Straight-Line Diffusion Model for Efficient 3D Molecular Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SbGtQpm2vP": {
    "title": "Structural Information-based Hierarchical Diffusion for Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NWtkmDPAA6": {
    "title": "PMLF: A Physics-Guided Multiscale Loss Framework for Structurally Heterogeneous Time Series",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Auy2DmlJBO": {
    "title": "MIRA: Medical Time Series Foundation Model for Real-World Health Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8DCyv8x58O": {
    "title": "MINGLE: Mixture of Null-Space Gated Low-Rank Experts for Test-Time Continual Model Merging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l1L0Yhh6x6": {
    "title": "On the Stability and Generalization of Meta-Learning: the Impact of Inner-Levels",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Cx6Kqto5h1": {
    "title": "Learn and Ensemble Bridge Adapters for Multi-domain Task Incremental Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YJxGJP8feU": {
    "title": "macOSWorld: A Multilingual Interactive Benchmark for GUI Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mBgGg30LZu": {
    "title": "Dynamic Diffusion Schrödinger Bridge in Astrophysical Observational Inversions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tD7MLq0dbZ": {
    "title": "Composing Global Solutions to Reasoning Tasks via Algebraic Objects in Neural Nets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0NexR6LDiG": {
    "title": "Rooms from Motion: Un-posed Indoor 3D Object Detection as Localization and Mapping",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2nIAtsUC27": {
    "title": "Improve Temporal Reasoning in Multimodal Large Language Models via Video Contrastive Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EATkC9iHE3": {
    "title": "RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fg5WE2vxgM": {
    "title": "Adaptive Algorithms with Sharp Convergence Rates for Stochastic Hierarchical Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LkqK19tqgx": {
    "title": "TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ch22ry5Axg": {
    "title": "Decoupled Entropy Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZF93vyH9He": {
    "title": "SeRL: Self-play Reinforcement Learning for Large Language Models with Limited Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cdgVrsu7T5": {
    "title": "RETRO SYNFLOW: Discrete Flow-Matching for Accurate and Diverse Single-Step Retrosynthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NhAi1w3s8Z": {
    "title": "How Far Are We from Optimal Reasoning Efficiency?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Kq08RIeXxI": {
    "title": "Panoptic Captioning: An Equivalence Bridge for Image and Text",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qRPIWtf3SE": {
    "title": "Learning single index models via harmonic decomposition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RmqWt1btxQ": {
    "title": "Transcending Cost-Quality Tradeoff in Agent Serving via Session-Awareness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VewW9QYUA6": {
    "title": "TRUST: Test-Time Refinement using Uncertainty-Guided SSM Traverses",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9c8J2C7ajq": {
    "title": "From Pretraining to Pathology: How Noise Leads to Catastrophic Inheritance in Medical Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xh3AIqtOHM": {
    "title": "SkyLadder: Better and Faster Pretraining via Context Window Scheduling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dtYKDOBkc7": {
    "title": "Continual Gaussian Mixture Distribution Modeling for Class Incremental Semantic Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aPLc1nWWIU": {
    "title": "AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant Adversarial Patches",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AuOZDp4gy7": {
    "title": "AliO: Output Alignment Matters in Long-Term Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QkqQSf1REo": {
    "title": "Rising from Ashes: Generalized Federated Learning via Dynamic Parameter Reset",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I9gqXOSypB": {
    "title": "Differentially Private Bilevel Optimization: Efficient Algorithms with Near-Optimal Rates",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9t2OtyQ9mf": {
    "title": "Enhancing Optimizer Stability: Momentum Adaptation of The NGN Step-size",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IRtGJNgVYk": {
    "title": "It's Hard to Be Normal: The Impact of Noise on Structure-agnostic Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NoC9HT7Kf7": {
    "title": "VideoHallu: Evaluating and Mitigating Multi-modal Hallucinations on Synthetic Video Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DorAT49sxj": {
    "title": "WALL-E: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jcvX8XFNqX": {
    "title": "Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x2Rk0lSQra": {
    "title": "AutoData: A Multi-Agent System for Open Web Data Collection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tiEsJtw3FH": {
    "title": "LT-Soups: Bridging Head and Tail Classes via Subsampled Model Soups",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QqCv9SI0X3": {
    "title": "Joint Design of Protein Surface and Backbone Using a Diffusion Bridge Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MdqirFiD38": {
    "title": "Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LbNL8xGai2": {
    "title": "S'MoRE: Structural Mixture of Residual Experts for Parameter-Efficient LLM Fine-tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ljJGBcpn7q": {
    "title": "AutoPartGen: Autoregressive 3D Part Generation and Discovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zb16xZ1NGB": {
    "title": "scSplit: Bringing Severity Cognizance to Image Decomposition in Fluorescence Microscopy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d1dL1ymD6N": {
    "title": "GoRA: Gradient-driven Adaptive Low Rank Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D88yhLFzDR": {
    "title": "The Lighthouse of Language: Enhancing LLM Agents via Critique-Guided Improvement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HRBhNqkG03": {
    "title": "Intervene-All-Paths: Unified Mitigation of LVLM Hallucinations across Alignment Formats",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N3UNXzWRRG": {
    "title": "FreqPolicy: Frequency Autoregressive Visuomotor Policy with Continuous Tokens",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yNej0aGtAZ": {
    "title": "Deeper with Riemannian Geometry: Overcoming Oversmoothing and Oversquashing for Graph Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vNmWbINtwH": {
    "title": "Aux-Think: Exploring Reasoning Strategies for Data-Efficient Vision-Language Navigation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R3C3RY25eS": {
    "title": "Boosting the Uniqueness of Neural Networks Fingerprints with Informative Triggers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5WkXhmvvgk": {
    "title": "Strategyproof Reinforcement Learning from Human Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lcUpF96w7Z": {
    "title": "Analytic Energy-Guided Policy Optimization for Offline Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yzHiEmLSk8": {
    "title": "Visual Instruction Bottleneck Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tPI9Sw04sZ": {
    "title": "Physics-informed Neural Operator for Pansharpening",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8JwMjKDppz": {
    "title": "SNAP: Low-Latency Test-Time Adaptation with Sparse Updates",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S2fswKCN9T": {
    "title": "Minimum Width for Deep, Narrow MLP: A Diffeomorphism Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GAa7Gy7FW7": {
    "title": "Optimal Minimum Width for the Universal Approximation of Continuously Differentiable Functions by Deep Narrow MLPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kL2pnzClyD": {
    "title": "Efficient Parametric SVD of Koopman Operator for Stochastic Dynamical Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZdQhQOXVku": {
    "title": "Make Information Diffusion Explainable: LLM-based Causal Framework for Diffusion Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wd9Y1C3KXs": {
    "title": "TC-Light: Temporally Coherent Generative Rendering for Realistic World Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yZy6f3icew": {
    "title": "TRiCo: Triadic Game-Theoretic Co-Training for Robust Semi-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wy2FwyNYro": {
    "title": "Attention (as Discrete-Time Markov) Chains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kHDUCUqPEc": {
    "title": "Future Link Prediction Without Memory or Aggregation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Twzlf6qFv": {
    "title": "Rendering-Aware Reinforcement Learning for Vector Graphics Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DpeJYRFRQY": {
    "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ciEdnQ0bLn": {
    "title": "Towards Reliable Identification of Diffusion-based Image Manipulations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W0sqoTL7rL": {
    "title": "Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination in Object Goal Navigation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Wlx38NK0Om": {
    "title": "Pairwise Optimal Transports for Training All-to-All Flow-Based Condition Transfer Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ngvolie2nH": {
    "title": "Representational Difference Explanations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cWlRC064GK": {
    "title": "Understanding Differential Transformer Unchains Pretrained Self-Attentions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YB9VGCClv9": {
    "title": "Diffusion Model as a Noise-Aware Latent Reward Model for Step-Level Preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5QneXT1qoc": {
    "title": "FedRAM: Federated Reweighting and Aggregation for Multi-Task Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s3TvewYDKk": {
    "title": "An Analysis of Concept Bottleneck Models: Measuring, Understanding, and Mitigating the Impact of Noisy Annotations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6BHDre6WQW": {
    "title": "Hierachical Balance Packing: Towards Efficient Supervised Fine-tuning for Long-Context LLM",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9eVXIN9Vij": {
    "title": "CodeMerge: Codebook-Guided Model Merging for Robust Test-Time Adaptation in Autonomous Driving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=59HMRbpQ25": {
    "title": "Towards Unsupervised Training of Matching-based Graph Edit Distance Solver via Preference-aware GAN",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gcWJyQlFlz": {
    "title": "Private Online Learning against an Adaptive Adversary: Realizable and Agnostic Settings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AqMIOl2JuW": {
    "title": "SyncHuman: Synchronizing 2D and 3D Generative Models for Single-view Human Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YVOCwQF9k2": {
    "title": "Orientation Matters: Making 3D Generative Models Orientation-Aligned",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2iNwD8EzOr": {
    "title": "Online Inverse Linear Optimization: Efficient Logarithmic-Regret Algorithm, Robustness to Suboptimality, and Lower Bound",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FFmiWufVWk": {
    "title": "Adaptive Discretization for Consistency Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w1gMJMbRxG": {
    "title": "Understanding Bias Terms in Neural Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=idxHcrwBYP": {
    "title": "Learning Shared Representations from Unpaired Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WGYEOAB0Rk": {
    "title": "Efficient Randomized Experiments Using Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hlpRj222RG": {
    "title": "PixPerfect: Seamless Latent Diffusion Local Editing with Discriminative Pixel-Space Refinement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=supy1n4Tev": {
    "title": "Diffusion Transformers as Open-World Spatiotemporal Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AHjspi4R22": {
    "title": "GLID 2 E: A Gradient-Free Lightweight Fine-tune Approach for Discrete Biological Sequence Design",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W0eTcFyx44": {
    "title": "ChemOrch: Empowering LLMs with Chemical Intelligence via Groundbreaking Synthetic Instructions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EBwfFrw5VA": {
    "title": "Cognitive Mirrors: Exploring the Diverse Functional Roles of Attention Heads in LLM Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8an1xVyKxS": {
    "title": "VaMP: Variational Multi-Modal Prompt Learning for Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=be6u40gq0Y": {
    "title": "GeoVideo: Introducing Geometric Regularization into Video Generation Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tsEHIfv2lM": {
    "title": "UniTransfer: Video Concept Transfer via Progressive Spatio-Temporal Decomposition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2aIoEG2Hwz": {
    "title": "HiMaCon: Discovering Hierarchical Manipulation Concepts from Unlabeled Multi-Modal Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IGJHOcoQKp": {
    "title": "UniTraj: Learning a Universal Trajectory Foundation Model from Billion-Scale Worldwide Traces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=65llKR17s4": {
    "title": "Scaling Up Parameter Generation: A Recurrent Diffusion Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PMSNd8xTHp": {
    "title": "ParetoQ: Improving Scaling Laws in Extremely Low-bit LLM Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nxnBaaRLnz": {
    "title": "Don't Think Longer, Think Wisely: Optimizing Thinking Dynamics for Large Reasoning Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=feLdTALuq3": {
    "title": "Evaluating and Learning Optimal Dynamic Treatment Regimes under Truncation by Death",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2ExVHRE6jd": {
    "title": "Graph Data Selection for Domain Adaptation: A Model-Free Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=o87dDXYLXC": {
    "title": "Concept-Guided Interpretability via Neural Chunking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PWRORdXJN1": {
    "title": "Act to See, See to Act: Diffusion-Driven Perception-Action Interplay for Adaptive Policies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lOXirB5NeJ": {
    "title": "Vinci: Deep Thinking in Text-to-Image Generation using Unified Model with Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lqywifxoo1": {
    "title": "Routing Mamba: Scaling State Space Models with Mixture-of-Experts Projection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZO8LyCizx9": {
    "title": "GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q8oLLyA34Q": {
    "title": "Pareto Optimal Risk-Agnostic Distributional Bandits with Heavy-Tail Rewards",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KjKfzItQrt": {
    "title": "Angular Constraint Embedding via SpherePair Loss for Constrained Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6MUgQXkxIC": {
    "title": "Personalized Visual Content Generation in Conversational Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BiEwLgytGu": {
    "title": "Multi-Objective Hyperparameter Selection via Hypothesis Testing on Reliability Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ssocZIfk94": {
    "title": "FACE: Faithful Automatic Concept Extraction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tA0l08qS5N": {
    "title": "Optimizing Chain-of-Thought Reasoners via Gradient Variance Minimization in Rejection Sampling and RL",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Mgug9GUNy7": {
    "title": "Sharp Matrix Empirical Bernstein Inequalities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Aiqtzn3Qo2": {
    "title": "Discovering Compositional Hallucinations in LVLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0wV5HR7M4P": {
    "title": "TEMPO: Temporal Multi-scale Autoregressive Generation of Protein Conformational Ensembles",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fi24ry0BX5": {
    "title": "Learning and Planning Multi-Agent Tasks via an MoE-based World Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O1MHVstfBQ": {
    "title": "MV-CoLight: Efficient Object Compositing with Consistent Lighting and Shadow Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3jjeQNeCsl": {
    "title": "FlowNet: Modeling Dynamic Spatio-Temporal Systems via Flow Propagation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IV6JqdW0BE": {
    "title": "Towards the Resistance of Neural Network Fingerprinting to Fine-tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Zfgvo65gxm": {
    "title": "Online Mixture of Experts: No-Regret Learning for Optimal Collective Decision-Making",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=16BGOheRzm": {
    "title": "Decentralized Dynamic Cooperation of Personalized Models for Federated Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3mOZvQfYpR": {
    "title": "Variance-Reduced Long-Term Rehearsal Learning with Quadratic Programming Reformulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NR5QjnbWGD": {
    "title": "Online Learning in the Repeated Mediated Newsvendor Problem",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5fAtzK3hBd": {
    "title": "ViewCraft3D: High-fidelity and View-Consistent 3D Vector Graphics Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4qPAOYjSlx": {
    "title": "How Particle System Theory Enhances Hypergraph Message Passing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OiqQGjNvyJ": {
    "title": "OMiSO: Adaptive optimization of state-dependent brain stimulation to shape neural population states",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TLxJAqucpL": {
    "title": "Design-Based Bandits Under Network Interference: Trade-Off Between Regret and Statistical Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dH6KDsG5GK": {
    "title": "Online Experimental Design With Estimation-Regret Trade-off Under Network Interference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P9gY05BDkW": {
    "title": "Rethinking Residual Distribution in Locate-then-Edit Model Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=10s01YrlKp": {
    "title": "metaTextGrad: Automatically optimizing language model optimizers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=38GF07Tmtr": {
    "title": "PlanarGS: High-Fidelity Indoor 3D Gaussian Splatting Guided by Vision-Language Planar Priors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0fq8vYOnxi": {
    "title": "CoVoMix2: Advancing Zero-Shot Dialogue Generation with Fully Non-Autoregressive Flow Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yk1iqV9Etr": {
    "title": "Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KfRfTAJpjh": {
    "title": "Tackling Continual Offline RL through Selective Weights Activation on Aligned Spaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2T6GWekWSm": {
    "title": "Beyond Single-Task: Robust Multi-Task Length Generalization for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Uu7UGn5i2A": {
    "title": "Universally Invariant Learning in Equivariant GNNs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ml2TynfZI0": {
    "title": "Integrating Drug Substructures and Longitudinal Electronic Health Records for Personalized Drug Recommendation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4449zuaaeL": {
    "title": "Effects of Dropout on Performance in Long-range Graph Learning Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=upU88pUpzX": {
    "title": "Support Vector Generation: Kernelizing Large Language Models for Efficient Zero‑Shot NLP",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6qBK9ePmtM": {
    "title": "The Underappreciated Power of Vision Models for Graph Structural Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IUhOGH0WiL": {
    "title": "Detoxifying Large Language Models via Autoregressive Reward Guided Representation Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MnWWHumqMc": {
    "title": "When Do Transformers Outperform Feedforward and Recurrent Networks? A Statistical Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ByzRO25Bjr": {
    "title": "Deployment Efficient Reward-Free Exploration with Linear Function Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ODCHMTXKHO": {
    "title": "Communication-Efficient Diffusion Denoising Parallelization via Reuse-then-Predict Mechanism",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m3Sz3tFxIV": {
    "title": "Learning quadratic neural networks in high dimensions: SGD dynamics and scaling laws",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f3sZjkQbv2": {
    "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kWsptXXt2o": {
    "title": "A Regularized Newton Method for Nonconvex Optimization with Global and Local Complexity Guarantees",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kA2H90nm26": {
    "title": "Emergence and scaling laws in SGD learning of shallow neural networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xx0cJGXU7n": {
    "title": "Non-rectangular Robust MDPs with Normed Uncertainty Sets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CYh2VrTM8N": {
    "title": "Median Selection with Noisy and Structural Information",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OixkI1jSZD": {
    "title": "On the Convergence of Single-Timescale Actor-Critic",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RGUcF6pIZN": {
    "title": "EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU Utilization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wjXKFrUFzA": {
    "title": "Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0rGJzKTqMs": {
    "title": "Learning CAD Modeling Sequences via Projection and Part Awareness",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ceTeM2Xl1n": {
    "title": "Improving the Straight-Through Estimator with Zeroth-Order Information",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A1u6BFAEGx": {
    "title": "NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gjvbsLyCC3": {
    "title": "Zooming from Context to Cue: Hierarchical Preference Optimization for Multi-Image MLLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WZQXaTNYEB": {
    "title": "Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7RErGDbO6K": {
    "title": "Faster Algorithms for Structured John Ellipsoid Computation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B9T5kKcKTy": {
    "title": "FlashMo: Geometric Interpolants and Frequency-Aware Sparsity for Scalable Efficient Motion Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0xFgYI6oZr": {
    "title": "Efficient k -Sparse Band–Limited Interpolation with Improved Approximation Ratio",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PceO8Cvvlj": {
    "title": "Differential Privacy for Euclidean Jordan Algebra with Applications to Private Symmetric Cone Programming",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B5fFInNlhZ": {
    "title": "Partial Physics Informed Diffusion Model for Ocean Chlorophyll Concentration Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9t7gfYUp0U": {
    "title": "MLZero: A Multi-Agent System for End-to-end Machine Learning Automation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3isHlkiykj": {
    "title": "Improving Progressive Generation with Decomposable Flow Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=83HlJrZigZ": {
    "title": "Unbiased Sliced Wasserstein Kernels for High-Quality Audio Captioning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XQIa0vGIum": {
    "title": "Explore In-Context Message Passing Operator for Graph Neural Networks in A Mean Field Game",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=098tsRhmQ7": {
    "title": "Uncover Governing Law of Pathology Propagation Mechanism Through A Mean-Field Game",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xRb0taq0pD": {
    "title": "On Group Sufficiency Under Label Bias",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RboGoiTB6s": {
    "title": "Fréchet Geodesic Boosting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=af9gTPce7W": {
    "title": "Wasserstein Transfer Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E3bMQCKgiX": {
    "title": "OCN: Effectively Utilizing Higher-Order Common Neighbors for Better Link Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sORaSF9Uxo": {
    "title": "Intrinsic Benefits of Categorical Distributional Loss: Uncertainty-aware Regularized Exploration in Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QHuqQeKIr4": {
    "title": "Seg-VAR:Image Segmentation with Visual Autoregressive Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P5yoTfwyyD": {
    "title": "DiffEye: Diffusion-Based Continuous Eye-Tracking Data Generation Conditioned on Natural Images",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zvYxXhlQHM": {
    "title": "Training the Untrainable: Introducing Inductive Bias via Representational Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mor7s1NGBV": {
    "title": "CTSketch: Compositional Tensor Sketching for Scalable Neurosymbolic Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nMLvTru6Zo": {
    "title": "Dynamics of Spontaneous Topic Changes in Next Token Prediction with Self-Attention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4FUdUFvvmp": {
    "title": "PANTHER: Generative Pretraining Beyond Language for Sequential User Behavior Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bV5is3iodg": {
    "title": "AC-LoRA: (Almost) Training-Free Access Control Aware Multi-Modal LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZQ80jIrtzl": {
    "title": "How to Learn a Star: Binary Classification with Starshaped Polyhedral Sets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NXPaQRErIT": {
    "title": "VR-Drive: Viewpoint-Robust End-to-End Driving with Feed-Forward 3D Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v8InI8hobW": {
    "title": "Learning to Clean: Reinforcement Learning for Noisy Label Correction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ofyy0Dzk6k": {
    "title": "Efficient Verified Unlearning For Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JMq90N6lLe": {
    "title": "Unified all-atom molecule generation with neural fields",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CJ00kttEjh": {
    "title": "MetaSlot: Break Through the Fixed Number of Slots in Object-Centric Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KR6tnkb6h4": {
    "title": "MobileUse: A Hierarchical Reflection-Driven GUI Agent for Autonomous Mobile Operation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=it0kkaFFpK": {
    "title": "On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in Large Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mhARf5VzCn": {
    "title": "Low-Rank Head Avatar Personalization with Registers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UjSBOwKZ02": {
    "title": "OPMapper: Enhancing Open-Vocabulary Semantic Segmentation with Multi-Guidance Information",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QWQB1ReBkJ": {
    "title": "Native-Resolution Image Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5UtsjOGsDx": {
    "title": "Turning Sand to Gold: Recycling Data to Bridge On-Policy and Off-Policy Learning via Causal Bound",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TtHvmhjNui": {
    "title": "Self-Calibrating BCIs: Ranking and Recovery of Mental Targets Without Labels",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d0bm4upnQ8": {
    "title": "ST 2 360D: Spatial-to-Temporal Consistency for Training-free 360 Monocular Depth Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gVKxz6M1ov": {
    "title": "DAA: Amplifying Unknown Discrepancy for Test-Time Discovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bW9SJJ9cHN": {
    "title": "Breakthrough Sensor-Limited Single View: Towards Implicit Temporal Dynamics for Time Series Domain Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AIlaBrwwJO": {
    "title": "GraphKeeper: Graph Domain-Incremental Learning via Knowledge Disentanglement and Preservation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F9jumRh9iu": {
    "title": "Scaling Off-Policy Reinforcement Learning with Batch and Weight Normalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EY096v0Jmg": {
    "title": "Active Test-time Vision-Language Navigation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SYgoqXyoaQ": {
    "title": "Efficient Rectified Flow for Image Fusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h3gaIBwemp": {
    "title": "Neural Attention Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iooQFm2qHf": {
    "title": "Handling Label Noise via Instance-Level Difficulty Modeling and Dynamic Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mVh0lIsdUl": {
    "title": "Per-Architecture Training-Free Metric Optimization for Neural Architecture Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2exr4mlbx1": {
    "title": "See&Trek: Training-Free Spatial Prompting for Multimodal Large Language Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4uy6GI3vzo": {
    "title": "On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5ZDT1dxojA": {
    "title": "DLoFT: Gradient-Decoupled Fine-Tuning for Generalizable Long Chain-of-Thought Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QsQGMijLhL": {
    "title": "Generalizing Experience for Language Agents with Hierarchical MetaFlows",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VmM8JVwVqV": {
    "title": "Second-Order Convergence in Private Stochastic Non-Convex Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=da1u6Cy7Mq": {
    "title": "Mask Image Watermarking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JYcMMJihdR": {
    "title": "Quantum Speedups for Minimax Optimization and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xpwFuMmzeq": {
    "title": "Continuous Concepts Removal in Text-to-image Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qILtlupwrR": {
    "title": "Computation and Memory-Efficient Model Compression with Gradient Reweighting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9env0BdcDV": {
    "title": "Unifying Text Semantics and Graph Structures for Temporal Text-attributed Graphs with Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uKSr3KgEqV": {
    "title": "Towards Graph Foundation Models: Training on Knowledge Graphs Enables Transferability to General Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IjMZfMVyLF": {
    "title": "CATransformers: Carbon Aware Transformers Through Joint Model-Hardware Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P81Q9NBwxp": {
    "title": "Single-Teacher View Augmentation: Boosting Knowledge Distillation via Angular Diversity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t8YRsWY6HM": {
    "title": "Mitra: Mixed Synthetic Priors for Enhancing Tabular Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MbqPKsVe1d": {
    "title": "4DGCPro: Efficient Hierarchical 4D Gaussian Compression for Progressive Volumetric Video Streaming",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2xjcosH3yQ": {
    "title": "A unified framework for establishing the universal approximation of transformer-type architectures",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oG1CIBWQ77": {
    "title": "Uncertainty-aware Preference Alignment for Diffusion Policies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2tqcROH3iO": {
    "title": "Looking Beyond the Known: Towards a Data Discovery Guided Open-World Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8XVlFz3peB": {
    "title": "Neural Tangent Knowledge Distillation for Optical Convolutional Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BZ0igCEeoU": {
    "title": "Fractional Langevin Dynamics for Combinatorial Optimization via Polynomial-Time Escape",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6QJZDAIhfk": {
    "title": "F-Adapter: Frequency-Adaptive Parameter-Efficient Fine-Tuning in Scientific Machine Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DBybUx7ARy": {
    "title": "Accelerating Block Coordinate Descent for LLM Finetuning via Landscape Expansion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dpmMg6aK1D": {
    "title": "Enhanced Self-Distillation Framework for Efficient Spiking Neural Network Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=itXwzGwGFC": {
    "title": "Training-Free Bayesianization for Low-Rank Adapters of Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2LfZDy2Ldo": {
    "title": "Controlled Visual Hallucination via Thalamus-Driven Decoupling Network for Domain Adaptation of Black-Box Predictors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5aaQSJYX3D": {
    "title": "Puzzles: Unbounded Video-Depth Augmentation for Scalable End-to-End 3D Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YTk1kATzOd": {
    "title": "Enhancing Interpretability in Deep Reinforcement Learning through Semantic Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rzjWktciDM": {
    "title": "Solving Discrete (Semi) Unbalanced Optimal Transport with Equivalent Transformation Mechanism and KKT-Multiplier Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=867TaCMfCj": {
    "title": "A Cautionary Tale on Integrating Studies with Disparate Outcome Measures for Causal Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=a95Vd41o1u": {
    "title": "STree: Speculative Tree Decoding for Hybrid State Space Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iwKT7MEZZw": {
    "title": "Sign-In to the Lottery: Reparameterizing Sparse Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yDh9s1qPzB": {
    "title": "Revisiting Semi-Supervised Learning in the Era of Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Ck1qqojSL": {
    "title": "On Geometry-Enhanced Parameter-Efficient Fine-Tuning for 3D Scene Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZtKXAbHQ43": {
    "title": "Efficient Safe Meta-Reinforcement Learning: Provable Near-Optimality and Anytime Safety",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VYdzGigCBC": {
    "title": "R 2 ec: Towards Large Recommender Models with Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CkmLys7ipp": {
    "title": "ODG: Occupancy Prediction Using Dual Gaussians",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xic1sPAmfC": {
    "title": "Neural MJD: Neural Non-Stationary Merton Jump Diffusion for Time Series Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Yg5QkFNHnT": {
    "title": "PINNs with Learnable Quadrature",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e8DrPuJekZ": {
    "title": "Robust Federated Finetuning of LLMs via Alternating Optimization of LoRA",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PAIVwOaAnq": {
    "title": "Shape it Up! Restoring LLM Safety during Finetuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ONUZ08OAZL": {
    "title": "Single-Step Operator Learning for Conditioned Time-Series Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i8IqEzpHaJ": {
    "title": "Language Models Can Predict Their Own Behavior",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kgjY80e1LF": {
    "title": "Symmetry-Preserving Conformer Ensemble Networks for Molecular Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q2cFqXOj3h": {
    "title": "Exploring Structural Degradation in Dense Representations for Self-supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uGDNHlslgO": {
    "title": "MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OhUu5PlRkF": {
    "title": "CAML: Collaborative Auxiliary Modality Learning for Multi-Agent Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=heIh4lkBEd": {
    "title": "RULE: Reinforcement UnLEarning Achieves Forget-retain Pareto Optimality",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H2m4chAfig": {
    "title": "CRRL: Learning Channel-invariant Neural Representations for High-performance Cross-day Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S1zkFSby8G": {
    "title": "PAC-Bayes Bounds for Multivariate Linear Regression and Linear Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E1eVGJ5RYG": {
    "title": "From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan Face Aging",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eIf9GNcA5n": {
    "title": "DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gb2wOj7V74": {
    "title": "On the Robustness of Transformers against Context Hijacking for Linear Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QdfdwsboOE": {
    "title": "Deferring Concept Bottleneck Models: Learning to Defer Interventions to Inaccurate Experts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vYgkWtq6F1": {
    "title": "FSEO: Few-Shot Evolutionary Optimization via Meta-Learning for Expensive Multi-Objective Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7VMg7Jb7AL": {
    "title": "Show-o2: Improved Native Unified Multimodal Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gNwJTjxmBe": {
    "title": "UniCTokens: Boosting Personalized Understanding and Generation via Unified Concept Tokens",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cSomL3lHTa": {
    "title": "Last Iterate Convergence in Monotone Mean Field Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9ccmoYhZue": {
    "title": "MODEL SHAPLEY: Find Your Ideal Parameter Player via One Gradient Backpropagation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RhRLbCt7MS": {
    "title": "Functional Complexity-adaptive Temporal Tensor Decomposition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wYBP37ujg3": {
    "title": "Minimal Semantic Sufficiency Meets Unsupervised Domain Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JML6Zi5J0s": {
    "title": "Covering Multiple Objectives with a Small Set of Solutions Using Bayesian Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V7RRnsAlbY": {
    "title": "Native Segmentation Vision Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cipx3rwfWp": {
    "title": "System-Embedded Diffusion Bridge Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=es4TTVGJ9x": {
    "title": "Adaptive LoRA Experts Allocation and Selection for Federated Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rbodZQBfd9": {
    "title": "Meta-Learning Objectives for Preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Iicv9iTPcU": {
    "title": "RAD: Towards Trustworthy Retrieval-Augmented Multi-modal Clinical Diagnosis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D3RUIzJxvL": {
    "title": "Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and Styles",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1YprrVfIp8": {
    "title": "Stackelberg Self-Annotation: A Robust Approach to Data-Efficient LLM Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z4hi1a9FsB": {
    "title": "Understand Before You Generate: Self-Guided Training for Autoregressive Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=96I8PGPALv": {
    "title": "Unlocking Multimodal Mathematical Reasoning via Process Reward Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2T6QXSP8Cf": {
    "title": "DOTA: Distributional Test-time Adaptation of Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QQqDBRRslp": {
    "title": "Toward a Unified Geometry Understanding : Riemannian Diffusion Framework for Graph Generation and Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hWtvsL51hO": {
    "title": "SpiderSolver: A Geometry-Aware Transformer for Solving PDEs on Complex Geometries",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tMf3keuPOl": {
    "title": "Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tu3P6KSHGN": {
    "title": "Safety Depth in Large Language Models: A Markov Chain Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=v6vBK4t8vB": {
    "title": "Bilevel ZOFO: Efficient LLM Fine-Tuning and Meta-Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8GjSf9Rh7Z": {
    "title": "Titans: Learning to Memorize at Test Time",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GkB1ZUNz83": {
    "title": "FedRTS: Federated Robust Pruning via Combinatorial Thompson Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QnUNr9Xuoo": {
    "title": "Implicit Modeling for Transferability Estimation of Vision Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6Bu7naotbL": {
    "title": "Train on Pins and Test on Obstacles for Rectilinear Steiner Minimum Tree",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wufHTz3phF": {
    "title": "LightFair: Towards an Efficient Alternative for Fair T2I Diffusion via Debiasing Pre-trained Text Encoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vvD0Bre3Dk": {
    "title": "The Dual Nature of Plasticity Loss in Deep Continual Learning: Dissection and Mitigation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TeocEZCWnr": {
    "title": "Exact and Linear Convergence for Federated Learning under Arbitrary Client Participation is Attainable",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7aSBAw7tJf": {
    "title": "Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h2kwURAFkJ": {
    "title": "Interaction-Centric Knowledge Infusion and Transfer for Open Vocabulary Scene Graph Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s91i4zNebu": {
    "title": "SRSR: Enhancing Semantic Accuracy in Real-World Image Super-Resolution with Spatially Re-Focused Text-Conditioning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rYeZkHV3Yi": {
    "title": "Understanding and Improving Adversarial Robustness of Neural Probabilistic Circuits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QZYZ0Xm58q": {
    "title": "VLA-Cache: Efficient Vision-Language-Action Manipulation via Adaptive Token Caching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qv0XyK5kdI": {
    "title": "DIPO: Dual-State Images Controlled Articulated Object Generation Powered by Diverse Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SuewCbLYBS": {
    "title": "Fuz-RL: A Fuzzy-Guided Robust Framework for Safe Reinforcement Learning under Uncertainty",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xVsC90U8yl": {
    "title": "Sample-Efficient Tabular Self-Play for Offline Robust Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cPO8dUCJOc": {
    "title": "Dense Metric Depth Estimation via Event-based Differential Focus Volume Prompting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0LUu5rd0GQ": {
    "title": "Mitigating Sexual Content Generation via Embedding Distortion in Text-conditioned Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5KuiDLT0rm": {
    "title": "StableGuard: Towards Unified Copyright Protection and Tamper Localization in Latent Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0e1KaHtgvU": {
    "title": "PartCrafter: Structured 3D Mesh Generation via Compositional Latent Diffusion Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oh2MpJHGHW": {
    "title": "Mitigating Occlusions in Virtual Try-On via A Simple-Yet-Effective Mask-Free Framework",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rNcIJi7N65": {
    "title": "Stepsize anything: A unified learning rate schedule for budgeted-iteration training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZY5mzxlnCA": {
    "title": "CoDA: Coordinated Diffusion Noise Optimization for Whole-Body Manipulation of Articulated Objects",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5fKXeqtBuB": {
    "title": "PocketSR: The Super-Resolution Expert in Your Pocket Mobiles",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OZUl49U6p6": {
    "title": "DualFocus: Depth from Focus with Spatio-Focal Dual Variational Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vDV912fa3t": {
    "title": "TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7aDKD8RBUw": {
    "title": "EA3D: Online Open-World 3D Object Extraction from Streaming Videos",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=02qdHz1LU0": {
    "title": "Model-Guided Dual-Role Alignment for High-Fidelity Open-Domain Video-to-Audio Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uG8bjnN1Nl": {
    "title": "Noise Matters: Optimizing Matching Noise for Diffusion Classifiers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2KKqp7MWJM": {
    "title": "AgentAuditor: Human-level Safety and Security Evaluation for LLM Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=owNPAl7LNK": {
    "title": "Spiking Neural Networks Need High-Frequency Information",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YTwRZP8mNO": {
    "title": "PLANA3R: Zero-shot Metric Planar 3D Reconstruction via Feed-forward Planar Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bWZSy9ARPD": {
    "title": "ReCon-GS: Continuum-Preserved Guassian Streaming for Fast and Compact Reconstruction of Dynamic Scenes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DZnBoIAy4V": {
    "title": "Assignments for Congestion-Averse Agents: Seeking Competitive and Envy-Free Solutions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=s3IT4Qo7bm": {
    "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wEk10M4CPD": {
    "title": "Model-Informed Flows for Bayesian Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DooE6euxAS": {
    "title": "Gaussian Regression-Driven Tensorized Incomplete Multi-View Clustering with Dual Manifold Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HgLaVgCpCl": {
    "title": "Conformal Prediction for Time-series Forecasting with Change Points",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n30sfYPrDD": {
    "title": "3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fy5InEg0OL": {
    "title": "Do different prompting methods yield a common task representation in language models?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I0hRN2HMeH": {
    "title": "Aligning What Matters: Masked Latent Adaptation for Text-to-Audio-Video Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1aCYFQRvtz": {
    "title": "Let Them Talk: Audio-Driven Multi-Person Conversational Video Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iZk78dZ1Ap": {
    "title": "Gemstones: A Model Suite for Multi-Faceted Scaling Laws",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p8HwGym5EH": {
    "title": "Metritocracy: Representative Metrics for Lite Benchmarks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LeJwcMLIlJ": {
    "title": "BiggerGait: Unlocking Gait Recognition with Layer-wise Representations from Large Vision Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=skx3QgGKEF": {
    "title": "Uncertainty Estimation on Graphs with Structure Informed Stochastic Partial Differential Equations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8LMwwt8E2s": {
    "title": "Probabilistic Token Alignment for Large Language Model Fusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vpJDCWOnPj": {
    "title": "A Generalized Binary Tree Mechanism for Private Approximation of All-Pair Shortest Distances",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=btW3QTadkW": {
    "title": "Dynamic Diameter in High-Dimensions against Adaptive Adversary and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xB1ZNgq0Xp": {
    "title": "Synthetic Series-Symbol Data Generation for Time Series Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xlQ4QUB9VC": {
    "title": "ARMesh: Autoregressive Mesh Generation via Next-Level-of-Detail Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SF2POTDz2o": {
    "title": "STEAD: Robust Provably Secure Linguistic Steganography with Diffusion Language Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jbLLxhnoty": {
    "title": "Reinforced Context Order Recovery for Adaptive Reasoning and Planning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ykg7dr6c1Y": {
    "title": "Adaptive Gradient Masking for Balancing ID and MLLM-based Representations in Recommendation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7wdi1LaocD": {
    "title": "AlphaFold Database Debiasing for Robust Inverse Folding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xr73jEYG29": {
    "title": "LLM-PySC2: Starcraft II learning environment for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CI7ZQnZTKu": {
    "title": "Assessing the quality of denoising diffusion models in Wasserstein distance: noisy score and optimal bounds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nsv3ogqRIU": {
    "title": "Adaptive Latent-Space Constraints in Personalized Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P6PijkA5lz": {
    "title": "Image Editing As Programs with Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qGW1GulvFH": {
    "title": "TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yPXOfBoQL7": {
    "title": "FALQON: Accelerating LoRA Fine-tuning with Low-Bit Floating-Point Arithmetic",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xjC5NqqSHs": {
    "title": "Robust Cross-modal Alignment Learning for Cross-Scene Spatial Reasoning and Grounding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iXy0ncNepZ": {
    "title": "Zeroth-Order Optimization Finds Flat Minima",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qB1UKvjO0N": {
    "title": "Towards Predicting Any Human Trajectory In Context",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rYv42fDKQi": {
    "title": "Backpropagation-Free Test-Time Adaptation via Probabilistic Gaussian Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NABGO9Bful": {
    "title": "UniViT: Unifying Image and Video Understanding in One Vision Encoder",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Swbyx5E7U": {
    "title": "TranSUN: A Preemptive Paradigm to Eradicate Retransformation Bias Intrinsically from Regression Models in Recommender Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fLAoFt2pea": {
    "title": "RrED: Black-box Unsupervised Domain Adaptation via Rectifying-reasoning Errors of Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9Zwl2Ly28N": {
    "title": "EgoThinker: Unveiling Egocentric Reasoning with Spatio-Temporal CoT",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DG0F1cdjN7": {
    "title": "RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=45bQUVXmwl": {
    "title": "DP²O-SR: Direct Perceptual Preference Optimization for Real-World Image Super-Resolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=on22Rx5A4F": {
    "title": "Explaining Similarity in Vision-Language Encoders with Weighted Banzhaf Interactions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KFPTmlW6ec": {
    "title": "FastJAM: a Fast Joint Alignment Model for Images",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KculbVVCly": {
    "title": "Multilevel neural simulation-based inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UcKYgtOIuX": {
    "title": "Test3R: Learning to Reconstruct 3D at Test Time",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h5NsMrUK4g": {
    "title": "One Sample is Enough to Make Conformal Prediction Robust",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yoKpumjWXc": {
    "title": "Learning Human-Object Interaction as Groups",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vSLzoUoJt6": {
    "title": "OWMM-Agent: Open World Mobile Manipulation With Multi-modal Agentic Data Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gUbQZ7AtaZ": {
    "title": "Chiron-o1: Igniting Multimodal Large Language Models towards Generalizable Medical Reasoning via Mentor-Intern Collaborative Search",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MaglIUQKVX": {
    "title": "PPMStereo: Pick-and-Play Memory Construction for Consistent Dynamic Stereo Matching",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LSoVMbITbp": {
    "title": "Meta Guidance: Incorporating Inductive Biases into Deep Time Series Imputers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JddJvNSiHk": {
    "title": "SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QkX9obx6UR": {
    "title": "ReID5o: Achieving Omni Multi-modal Person Re-identification in a Single Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8EhH3A9hoP": {
    "title": "InstaInpaint: Instant 3D-Scene Inpainting with Masked Large Reconstruction Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X2u8esISdb": {
    "title": "DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5d83SyLm0Z": {
    "title": "Generating Full-field Evolution of Physical Dynamics from Irregular Sparse Observations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NosdT1FHPv": {
    "title": "Rectified CFG++ for Flow Based Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MY4KpJt3LM": {
    "title": "LVLM-Driven Attribute-Aware Modeling for Visible-Infrared Person Re-Identification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AlRJVX5CRi": {
    "title": "Revisiting Orbital Minimization Method for Neural Operator Decomposition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nFc38gSYze": {
    "title": "Active Measurement: Efficient Estimation at Scale",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=R6m6bNnmWm": {
    "title": "VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9qCejdqPXa": {
    "title": "Text-to-Decision Agent: Offline Meta-Reinforcement Learning from Natural Language Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D6PwC6Xogv": {
    "title": "AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hw0hl4JmvA": {
    "title": "FIGRDock: Fast Interaction-Guided Regression for Flexible Docking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SLDYuNGwvU": {
    "title": "Evaluating Robustness of Monocular Depth Estimation with Procedural Scene Perturbations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p2EycV4XOa": {
    "title": "Domain-RAG: Retrieval-Guided Compositional Image Generation for Cross-Domain Few-Shot Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gCvByDI4FN": {
    "title": "YOLOv12: Attention-Centric Real-Time Object Detectors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1lyKflUOhp": {
    "title": "ChatVLA-2: Vision-Language-Action Model with Open-World Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0g9gVoA7sn": {
    "title": "Dual-Space Semantic Synergy Distillation for Continual Learning of Unlabeled Streams",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3d4wuHgeon": {
    "title": "Solving Partial Differential Equations via Radon Neural Operator",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LKAp7Dknxf": {
    "title": "LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=atY7t0Krp6": {
    "title": "Unified Transferability Metrics for Time Series Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jpiSagi8aV": {
    "title": "RLVR-World: Training World Models with Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7L4NvUtZY3": {
    "title": "FlashBias: Fast Computation of Attention with Bias",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3p4272zl7q": {
    "title": "Rationalized All-Atom Protein Design with Unified Multi-Modal Bayesian Flow",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bAJCfIywYl": {
    "title": "Generalization Bounds for Model-based Algorithm Configuration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yh1t1yFtXG": {
    "title": "Prior-Guided Flow Matching for Target-Aware Molecule Design with Learnable Atom Number",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kn0AyMYw0v": {
    "title": "Fit the Distribution: Cross-Image/Prompt Adversarial Attacks on Multimodal Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C2QMbkp7iq": {
    "title": "Reinforcement Learning Meets Masked Generative Models: Mask-GRPO for Text-to-Image Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sbPlIVIZN9": {
    "title": "Self-Supervised Selective-Guided Diffusion Model for Old-Photo Face Restoration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=812rmogRgf": {
    "title": "FRN: Fractal-Based Recursive Spectral Reconstruction Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wGeQr5cARm": {
    "title": "The Mirage of Performance Gains: Why Contrastive Decoding Fails to Mitigate Object Hallucinations in MLLMs?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=894Yo61h1P": {
    "title": "Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tTwZhy8JqY": {
    "title": "SeCon-RAG: A Two-Stage Semantic Filtering and Conflict-Free Framework for Trustworthy RAG",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SYDmbsqHI0": {
    "title": "MOSDT: Self-Distillation-Based Decision Transformer for Multi-Agent Offline Safe Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ecw0Cina9N": {
    "title": "Non-Singularity of the Gradient Descent Map for Neural Networks with Piecewise Analytic Activations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cnUq7GkS6d": {
    "title": "Retrieval is Not Enough: Enhancing RAG through Test-Time Critique and Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V75MK7uh67": {
    "title": "Personalized Bayesian Federated Learning with Wasserstein Barycenter Aggregation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sbmYVM4zRr": {
    "title": "Chain-of-Model Learning for Language Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hFaXVjRFHI": {
    "title": "SpatialReasoner: Towards Explicit and Generalizable 3D Spatial Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RgJsjv6cUY": {
    "title": "Interpreting vision transformers via residual replacement model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=theJ4Qiz55": {
    "title": "Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error Diagnosis in GUI Automation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nJ5GhQO35W": {
    "title": "Walking the Schrödinger Bridge: A Direct Trajectory for Text-to-3D Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OnViSNPqbT": {
    "title": "Controllable Human-centric Keyframe Interpolation with Generative Prior",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EeIEvZlmVg": {
    "title": "SuperCLIP: CLIP with Simple Classification Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7aYBgYDFhh": {
    "title": "Real-World Adverse Weather Image Restoration via Dual-Level Reinforcement Learning with High-Quality Cold Start",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wTBhWbCRpN": {
    "title": "Multi-scale Temporal Prediction via Incremental Generation and Multi-agent Collaboration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zxZPpVoCNO": {
    "title": "EPA: Boosting Event-based Video Frame Interpolation with Perceptually Aligned Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oUghNi5XWc": {
    "title": "SCOPE: Saliency-Coverage Oriented Token Pruning for Efficient Multimodel LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qMWD6qYHdk": {
    "title": "Dual Alignment Framework for Few-shot Learning with Inter-Set and Intra-Set Shifts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VswQY0peMr": {
    "title": "Neural Hamiltonian Diffusions for Modeling Structured Geometric Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YktFxpaEmR": {
    "title": "Selftok-Zero: Reinforcement Learning for Visual Generation via Discrete and Autoregressive Visual Tokens",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4asFznbzJg": {
    "title": "Fast-in-Slow: A Dual-System VLA Model Unifying Fast Manipulation within Slow Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vAT2xlaWJY": {
    "title": "MemSim: A Bayesian Simulator for Evaluating Memory of LLM-based Personal Assistants",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=l6C6Pw30Gl": {
    "title": "Mixture of Inputs: Text Generation Beyond Discrete Token Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZW2BADJKGU": {
    "title": "Harnessing the Computation Redundancy in ViTs to Boost Adversarial Transferability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OYciB30Z4n": {
    "title": "Seeing the Arrow of Time in Large Multimodal Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m51t6RKfGH": {
    "title": "Rectifying Shortcut Behaviors in Preference-based Reward Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JrZY7ilKLs": {
    "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D2NR5Zq6PG": {
    "title": "The Indra Representation Hypothesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GkztRjE4P4": {
    "title": "Stitch and Tell: A Structured Data Augmentation Method for Spatial Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2bgwni6Ber": {
    "title": "Conformal Inference under High-Dimensional Covariate Shifts via Likelihood-Ratio Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vzVoMvpSYC": {
    "title": "HOComp: Interaction-Aware Human-Object Composition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0D3ja9s17M": {
    "title": "FlexSelect: Flexible Token Selection for Efficient Long Video Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DaNAqCT0XA": {
    "title": "PBR-SR: Mesh PBR Texture Super Resolution from 2D Image Priors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5xwyxupsLL": {
    "title": "PipeFusion: Patch-level Pipeline Parallelism for Diffusion Transformers Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mPOQZMBKaN": {
    "title": "Hyper-Modality Enhancement for Multimodal Sentiment Analysis with Missing Modalities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yADOHvnzXr": {
    "title": "AlphaBeta is not as good as you think: a simple random games model for a better analysis of deterministic game-solving algorithms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6Mah2bx7ZI": {
    "title": "Logic.py: Bridging the Gap between LLMs and Constraint Solvers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gOwqPdBlRB": {
    "title": "Robust Hallucination Detection in LLMs via Adaptive Token Selection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LrIRYbn3Rn": {
    "title": "SynCL: A Synergistic Training Strategy with Instance-Aware Contrastive Learning for End-to-End Multi-Camera 3D Tracking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F2ILoE1eDj": {
    "title": "DPAIL: Training Diffusion Policy for Adversarial Imitation Learning without Policy Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dRLWcpBQxS": {
    "title": "Tree Ensemble Explainability through the Hoeffding Functional Decomposition and TreeHFD Algorithm",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0qGtaRTsCo": {
    "title": "CF-VLM：CounterFactual Vision-Language Fine-tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YDWRTYgR79": {
    "title": "MAT-Agent: Adaptive Multi-Agent Training Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u8aoIxG1A4": {
    "title": "Temporal Smoothness-Aware Rate-Distortion Optimized 4D Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D0YNbanYfB": {
    "title": "Video Perception Models for 3D Scene Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=98TY64tOFB": {
    "title": "CoT-lized Diffusion: Let's Reinforce T2I Generation Step-by-step",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jquTBzt3Av": {
    "title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e9B2NPQanB": {
    "title": "UniLumos: Fast and Unified Image and Video Relighting with Physics-Plausible Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=S43003uMGq": {
    "title": "Unified Multimodal Chain-of-Thought Reward Model through Reinforcement Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TEAASoJWsb": {
    "title": "Generalized Contrastive Learning for Universal Multimodal Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fYSPRGmS6l": {
    "title": "MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=noiiyIk3hh": {
    "title": "Frame In-N-Out: Unbounded Controllable Image-to-Video Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VrYCLQ5inI": {
    "title": "Faster Video Diffusion with Trainable Sparse Attention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lRs6qSMKH1": {
    "title": "SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1DgSkx8L63": {
    "title": "ShotBench: Expert-Level Cinematic Understanding in Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EKx21Vonwo": {
    "title": "Nearly Dimension-Independent Convergence of Mean-Field Black-Box Variational Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0bvc7Zslu3": {
    "title": "Table2LaTeX-RL: High-Fidelity LaTeX Code Generation from Table Images via Reinforced Multimodal Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vhPy3NMsO5": {
    "title": "OmniResponse: Online Multimodal Conversational Response Generation in Dyadic Interactions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wQfO6aEgbE": {
    "title": "Moment- and Power-Spectrum-Based Gaussianity Regularization for Text-to-Image Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L1m5124sNQ": {
    "title": "Dimension-Reduction Attack! Video Generative Models are Experts on Controllable Image Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h6D4Ns9LBD": {
    "title": "OmniTry: Virtual Try-On Anything without Masks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1EnpXg8s4v": {
    "title": "GeoComplete: Geometry-Aware Diffusion for Reference-Driven Image Completion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z01gNsO9SW": {
    "title": "Learning to Control Free-Form Soft Swimmers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RyYDCQ2ilV": {
    "title": "Omnidirectional 3D Scene Reconstruction from Single Image",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TbQCWblZAZ": {
    "title": "WMCopier: Forging Invisible Watermarks on Arbitrary Images",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kXieirlPjF": {
    "title": "Agentic RL Scaling Law: Spontaneous Code Execution for Mathematical Problem Solving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cT54nK58lH": {
    "title": "Diversity-oriented Deep Multi-modal Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=d4Y576ManB": {
    "title": "SAMA: Towards Multi-Turn Referential Grounded Video Chat with Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GplW3hkvnr": {
    "title": "Incomplete Multi-view Clustering via Hierarchical Semantic Alignment and Cooperative Completion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2w64oCNRFV": {
    "title": "From Black-box to Causal-box: Towards Building More Interpretable Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ugOn7Pohxv": {
    "title": "Towards Prospective Medical Image Reconstruction via Knowledge-Informed Dynamic Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dbaYQyruY2": {
    "title": "Limited Preference Data? Learning Better Reward Model with Latent Space Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BcYfsMMpV1": {
    "title": "Imagine360: Immersive 360 Video Generation from Perspective Anchor",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vmYcNhs8Av": {
    "title": "IllumiCraft: Unified Geometry and Illumination Diffusion for Controllable Video Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xlDmm4r98R": {
    "title": "FACT: Mitigating Inconsistent Hallucinations in LLMs via Fact-Driven Alternating Code-Text Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BBZEcVu1nA": {
    "title": "Personalized Image Editing in Text-to-Image Diffusion Models via Collaborative Direct Preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tahkGZjjWA": {
    "title": "Constrained Diffusers for Safe Planning and Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zwmq0MsIMG": {
    "title": "Novel View Synthesis from A Few Glimpses via Test-Time Natural Video Completion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uWEcZkrSkZ": {
    "title": "MuSLR: Multimodal Symbolic Logical Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3oh7lBEF7X": {
    "title": "Autoregressive Motion Generation with Gaussian Mixture-Guided Latent Sampling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iM8154lJCx": {
    "title": "Learning Spatial-Aware Manipulation Ordering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=t49olghJ3w": {
    "title": "From Self-Check to Consensus: Bayesian Strategic Decoding in Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kzdLY8C8oa": {
    "title": "A Near-optimal, Scalable and Parallelizable Framework for Stochastic Bandits Robust to Adversarial Corruptions and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kxuIIMBVTg": {
    "title": "Refining Norms: A Post-hoc Framework for OOD Detection in Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m0q8NfGWnv": {
    "title": "MF-LLM: Simulating Population Decision Dynamics via a Mean-Field Large Language Model Framework",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cU4ow1odRe": {
    "title": "ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and Reactive Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PR7VmMTsxC": {
    "title": "VITRIX-UniViTAR: Unified Vision Transformer with Native Resolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4H3xG3aDS1": {
    "title": "GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language Models and Enhanced Reasoning Chains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JZuDhckogK": {
    "title": "OmniConsistency: Learning Style-Agnostic Consistency from Paired Stylization Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DOb47fj0cl": {
    "title": "RelationAdapter: Learning and Transferring Visual Relation with Diffusion Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JJ29N4MklH": {
    "title": "StegoZip: Enhancing Linguistic Steganography Payload in Practice with Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=goNuX2kQzd": {
    "title": "AutoEdit: Automatic Hyperparameter Tuning for Image Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IDHbeHDBiS": {
    "title": "Instant Video Models: Universal Adapters for Stabilizing Image-Based Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z6aBp0AJI1": {
    "title": "Energy Landscape-Aware Vision Transformers: Layerwise Dynamics and Adaptive Task-Specific Training via Hopfield States",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BKLS9IMrNZ": {
    "title": "Dynamic View Synthesis as an Inverse Problem",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LWuhOoHpo5": {
    "title": "OSKAR: Omnimodal Self-supervised Knowledge Abstraction and Representation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YHU7ZGomO1": {
    "title": "Learning Differential Pyramid Representation for Tone Mapping",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fgW3IZzxmE": {
    "title": "msf-CNN: Patch-based Multi-Stage Fusion with Convolutional Neural Networks for TinyML",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UhgpXbOcK1": {
    "title": "Gradient-Weight Alignment as a Train-Time Proxy for Generalization in Classification Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cOSXcSmmJL": {
    "title": "IntrinsiX: High-Quality PBR Generation using Image Priors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jDxFD45kkc": {
    "title": "PoLAR: Polar-Decomposed Low-Rank Adapter Representation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FmUa6bKscB": {
    "title": "Learning to Generate Human-Human-Object Interactions from Textual Descriptions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kqHzgTV9AU": {
    "title": "Balanced Conic Rectified Flow",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8pEqukyGrj": {
    "title": "CryptoMoE: Privacy-Preserving and Scalable Mixture of Experts Inference via Balanced Expert Routing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yTFJmGFsEy": {
    "title": "Reviving DSP for Advanced Theorem Proving in the Era of Reasoning Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CiOSjynX6b": {
    "title": "MiniMax-Remover: Taming Bad Noise Helps Video Object Removal",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VMqxRPqdPw": {
    "title": "Mixture-of-Experts Meets In-Context Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JnKfAqLJb4": {
    "title": "ReMindRAG: Low-Cost LLM-Guided Knowledge Graph Traversal for Efficient RAG",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PA47sKU8CU": {
    "title": "Image as a World: Generating Interactive World from Single Image via Panoramic Video Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mUcUY0f4u9": {
    "title": "BIPNN: Learning to Solve Binary Integer Programming via Hypergraph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6lxplXcCds": {
    "title": "BMW: Bidirectionally Memory bank reWriting for Unsupervised Person Re-Identification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L5VPvuPMKs": {
    "title": "Hypergraph-Enhanced Contrastive Learning for Multi-View Clustering with Hyper-Laplacian Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6uKIm4bfEe": {
    "title": "Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NCslwlWjqB": {
    "title": "FreeInv: Free Lunch for Improving DDIM Inversion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aAxHUzTdhe": {
    "title": "CaliGCL: Calibrated Graph Contrastive Learning via Partitioned Similarity and Consistency Discrimination",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ezi8uXAanu": {
    "title": "Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zbucdbZ0fU": {
    "title": "ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ne3nYEcGsf": {
    "title": "Diff-ICMH: Harmonizing Machine and Human Vision in Image Compression with Generative Prior",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JwnAItQF9v": {
    "title": "GRIFFIN: Effective Token Alignment for Faster Speculative Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jqDtzUQkmu": {
    "title": "Accelerating Parallel Diffusion Model Serving with Residual Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VKJTyhAtoA": {
    "title": "Reframing Gaussian Splatting Densification with Complexity-Density Consistency of Primitives",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Xez853WUqf": {
    "title": "3DPE-Gaze:Unlocking the Potential of 3D Facial Priors for Generalized Gaze Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kjWB8iaO3l": {
    "title": "CLIPGaussian: Universal and Multimodal Style Transfer Based on Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JxxKj8pow1": {
    "title": "Scaling Speculative Decoding with Lookahead Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nn51ewu5k2": {
    "title": "Efficiently Scaling LLM Reasoning Programs with Certaindex",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VzWjHxE8PF": {
    "title": "TAPIP3D: Tracking Any Point in Persistent 3D Geometry",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iZC5xoQQkX": {
    "title": "Causal LLM Routing: End-to-End Regret Minimization from Observational Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rgrpS4SFNF": {
    "title": "Second-order Optimization under Heavy-Tailed Noise: Hessian Clipping and Sample Complexity Limits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2PhVe7p9xD": {
    "title": "Unsupervised Trajectory Optimization for 3D Registration in Serial Section Electron Microscopy using Neural ODEs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FAiIRMvIwy": {
    "title": "DesignX: Human-Competitive Algorithm Designer for Black-Box Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eX5aXfJQZc": {
    "title": "Force Prompting: Video Generation Models Can Learn And Generalize Physics-based Control Signals",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2ncMTlR9nC": {
    "title": "CamEdit: Continuous Camera Parameter Control for Photorealistic Image Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gd1Dm1gHhg": {
    "title": "BioOSS: A Bio-Inspired Oscillatory State System with Spatio-Temporal Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tRvzEL64dY": {
    "title": "Interpreting Arithmetic Reasoning in Large Language Models using Game-Theoretic Interactions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P382qaxvjc": {
    "title": "HAODiff: Human-Aware One-Step Diffusion via Dual-Prompt Guidance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VkSd42HWil": {
    "title": "Dual-Res Tandem Mamba-3D: Bilateral Breast Lesion Detection and Classification on Non-contrast Chest CT",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0EcomthIKh": {
    "title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mHHrnCWwrD": {
    "title": "Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oIBwHvF930": {
    "title": "MEGADance: Mixture-of-Experts Architecture for Genre-Aware 3D Dance Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dIHSZTx9Lu": {
    "title": "Hardware-aligned Hierarchical Sparse Attention for Efficient Long-term Memory Access",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KoCytqC9gx": {
    "title": "VL-SAM-V2: Open-World Object Detection with General and Specific Query Fusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QhKnVxk96T": {
    "title": "MoEMeta: Mixture-of-Experts Meta Learning for Few-Shot Relational Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SOc0tHCewe": {
    "title": "SSR: Enhancing Depth Perception in Vision-Language Models via Rationale-Guided Spatial Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KmI7Mbctul": {
    "title": "Asymmetric Dual-Lens Video Deblurring",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zDOo34mbpl": {
    "title": "Accelerated Evolving Set Processes for Local PageRank Computation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uyJcF4cwMc": {
    "title": "A Plug-and-Play Query Synthesis Active Learning Framework for Neural PDE Solvers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y9f6ZYmVCD": {
    "title": "Strategic Classification with Non-Linear Classifiers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KzU33wR875": {
    "title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lkw2WJLdbh": {
    "title": "Graphs Help Graphs: Multi-Agent Graph Socialized Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PrYDDxphym": {
    "title": "Neural Stochastic Flows: Solver-Free Modelling and Inference for SDE Solutions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=22CqLfjiVl": {
    "title": "Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BtPg90UEbw": {
    "title": "Recurrent Memory for Online Interdomain Gaussian Processes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FACJ0478oQ": {
    "title": "FOCUS: Unified Vision-Language Modeling for Interactive Editing Driven by Referential Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=01hPO0uJhS": {
    "title": "Who You Are Matters: Bridging Interests and Social Roles via LLM-Enhanced Logic Recommendation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cpgCK7LdgU": {
    "title": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QKOdOlfzSe": {
    "title": "NAUTILUS: A Large Multimodal Model for Underwater Scene Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tXnyVPNOfa": {
    "title": "SAEMark: Steering Personalized Multilingual LLM Watermarks with Sparse Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Vvb27TQzO9": {
    "title": "Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot Demonstrations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qcdoHkkHcb": {
    "title": "SPICED: A Synaptic Homeostasis-Inspired Framework for Unsupervised Continual EEG Decoding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5WKEH9LhAQ": {
    "title": "Block-Biased Mamba for Long-Range Sequence Processing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p4jKtPCcUh": {
    "title": "Perceive Anything: Recognize, Explain, Caption, and Segment Anything in Images and Videos",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w7hiWakSAq": {
    "title": "Venus-MAXWELL: Efficient Learning of Protein-Mutation Stability Landscapes using Protein Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fllsm01JWS": {
    "title": "Learning the Plasticity: Plasticity-Driven Learning Framework in Spiking Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9r3OQhPiqT": {
    "title": "An Adaptive Algorithm for Bilevel Optimization on Riemannian Manifolds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2X0Fy0jqPa": {
    "title": "Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nTc0LSqtqE": {
    "title": "Vision Function Layer in Multimodal LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ILr4UNiZcQ": {
    "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p27bSdc3FS": {
    "title": "Fair Deepfake Detectors Can Generalize",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kQAnOaayIo": {
    "title": "PC-Net: Weakly Supervised Compositional Moment Retrieval via Proposal-Centric Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jM4ULVYF66": {
    "title": "FedGPS: Statistical Rectification Against Data Heterogeneity in Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r7FR4UN7uk": {
    "title": "A Set of Generalized Components to Achieve Effective Poison-only Clean-label Backdoor Attacks with Collaborative Sample Selection and Triggers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L9vV3wVC72": {
    "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gvq2AfuVEA": {
    "title": "HMVLM:Human Motion-Vision-Language Model via MoE LoRA",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jTCiQpV0Lx": {
    "title": "Unlocker: Disentangle the Deadlock of Learning between Label-noisy and Long-tailed Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uVYqwEgIpE": {
    "title": "Towards Implicit Aggregation: Robust Image Representation for Place Recognition in the Transformer Era",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Sk1WgkHdht": {
    "title": "State Space Prompting via Gathering and Spreading Spatio-Temporal Information for Video Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F74FXkicGK": {
    "title": "Class-aware Domain Knowledge Fusion and Fission for Continual Test-Time Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B9Aj2aWLKi": {
    "title": "CLEAR: Conv-Like Linearization Revs Pre-Trained Diffusion Transformers Up",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=j6DPMcyjMG": {
    "title": "Planning and Learning in Average Risk-aware MDPs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=alrbY3gwNB": {
    "title": "RigAnyFace: Scaling Neural Facial Mesh Auto-Rigging with Unlabeled Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rbr2mMSBua": {
    "title": "SAM2Flow: Interactive Optical Flow Estimation with Dual Memory for in vivo Microcirculation Analysis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fNOCsycDG4": {
    "title": "Regret-Optimal Q-Learning with Low Cost for Single-Agent and Federated Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vXGySQIPyL": {
    "title": "Constrained Feedback Learning for Non-Stationary Multi-Armed Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nPYtkZu65y": {
    "title": "Breaking the Compression Ceiling: Data-Free Pipeline for Ultra-Efficient Delta Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IVWHe60vfA": {
    "title": "Distribution-Aligned Decoding for Efficient LLM Task Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9T6Pu6iWL6": {
    "title": "SRA-CL: Semantic Retrieval Augmented Contrastive Learning for Sequential Recommendation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X8C20fJXtx": {
    "title": "Hierarchical Koopman Diffusion: Fast Generation with Interpretable Diffusion Trajectory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pKqLOmF3Lf": {
    "title": "C 2 Prompt: Class-aware Client Knowledge Interaction for Federated Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MoMXPzwVMb": {
    "title": "Luminance-Aware Statistical Quantization: Unsupervised Hierarchical Learning for Illumination Enhancement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aTqfufujj7": {
    "title": "From Indicators to Insights: Diversity-Optimized for Medical Series-Text Decoding via LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hiiaHn3pWd": {
    "title": "Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pm4Bl3D6XF": {
    "title": "Rethinking Hebbian Principle: Low-Dimensional Structural Projection for Unsupervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oWnAlRn3X1": {
    "title": "Memory-Efficient Training with In-Place FFT Implementation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jJweE513Am": {
    "title": "Degradation-Aware Dynamic Schrödinger Bridge for Unpaired Image Restoration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jIQycAt6aU": {
    "title": "ALTo: Adaptive-Length Tokenizer for Autoregressive Mask Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pebVFFVs2R": {
    "title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=awRfy4xAO5": {
    "title": "Target Speaker Extraction through Comparing Noisy Positive and Negative Audio Enrollments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y5wMoIbdDs": {
    "title": "STRAP: Spatio-Temporal Pattern Retrieval for Out-of-Distribution Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mj8VN4MyrO": {
    "title": "ThinkSound: Chain-of-Thought Reasoning in Multimodal LLMs for Audio Generation and Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TUCHMVzXin": {
    "title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EJZKxsS1Bl": {
    "title": "Purest Quantum State Identification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=evTXNxaH0f": {
    "title": "CQ-DINO: Mitigating Gradient Dilution via Category Queries for Vast Vocabulary Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eZNdkwJYbN": {
    "title": "QuadricFormer: Scene as Superquadrics for 3D Semantic Occupancy Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=m4rBrmNA9y": {
    "title": "Glocal Information Bottleneck for Time Series Imputation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I3Ep3gQUaw": {
    "title": "DSRF: A Dynamic and Scalable Reasoning Framework for Solving RPMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BpSGN4pErp": {
    "title": "NUTS: Eddy-Robust Reconstruction of Surface Ocean Nutrients via Two-Scale Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NxlsOnkGYV": {
    "title": "Influence Functions for Edge Edits in Non-Convex Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yO2zE1yIYZ": {
    "title": "The Future Unmarked: Watermark Removal in AI-Generated Images via Next-Frame Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=X8oEu4Gs3W": {
    "title": "VQToken: Neural Discrete Token Representation Learning for Extreme Token Reduction in Video Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7x5X6gTCUH": {
    "title": "Miss-ReID: Delivering Robust Multi-Modality Object Re-Identification Despite Missing Modalities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L2W4wQsNkY": {
    "title": "MindJourney: Test-Time Scaling with World Models for Spatial Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GrPo8NTtzK": {
    "title": "Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qE3knKF1rz": {
    "title": "PANDA: Towards Generalist Video Anomaly Detection via Agentic AI Engineer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xvsPQuUHef": {
    "title": "A Novel General Framework for Sharp Lower Bounds in Succinct Stochastic Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SAGZBti9lj": {
    "title": "Learning Dense Hand Contact Estimation from Imbalanced Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SELYlDHZk2": {
    "title": "EfficientVLA: Training-Free Acceleration and Compression for Vision-Language-Action Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7VLxvVEtHh": {
    "title": "PanoWan: Lifting Diffusion Video Generation Models to 360 ∘ with Latitude/Longitude-aware Mechanisms",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UogChes7e9": {
    "title": "Computational Budget Should Be Considered in Data Selection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nOZSYSdzOP": {
    "title": "Unveiling Environmental Sensitivity of Individual Gains in Influence Maximization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yFjgV3cJje": {
    "title": "4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2h8vXbEufN": {
    "title": "Denoising Trajectory Biases for Zero-Shot AI-Generated Image Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xVveBDPrgI": {
    "title": "Breaking the Discretization Barrier of Continuous Physics Simulation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SfuH4dShOA": {
    "title": "MultiScale Contextual Bandits for Long Term Objectives",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1amnhVRQ3l": {
    "title": "Grounded Reinforcement Learning for Visual Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=D4hcJPkJ3y": {
    "title": "NeurIPT: Foundation Model for Neural Interfaces",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BND9CutZf6": {
    "title": "Geometric Imbalance in Semi-Supervised Node Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q1acmWAO9r": {
    "title": "Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Sk8CaQWdv": {
    "title": "Decoding Causal Structure: End-to-End Mediation Pathways Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ShJ80Yu12r": {
    "title": "CAGE: Continuity-Aware edGE Network Unlocks Robust Floorplan Reconstruction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JH0DczMsAV": {
    "title": "DCI: Dual-Conditional Inversion for Boosting Diffusion-Based Image Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fkYFDxiLpi": {
    "title": "CrossAD: Time Series Anomaly Detection with Cross-scale Associations and Cross-window Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F9SSJLg55j": {
    "title": "Unleashing the Potential of Multimodal LLMs for Zero-Shot Spatio-Temporal Video Grounding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7yOl9qiLWd": {
    "title": "BrainODE: Neural Shape Dynamics for Age- and Disease-aware Brain Trajectories",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8y18QBU2s6": {
    "title": "Retrv-R1: A Reasoning-Driven MLLM Framework for Universal and Efficient Multimodal Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kWGJa9ZO3M": {
    "title": "Watch and Listen: Understanding Audio-Visual-Speech Moments with Multimodal LLM",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=I64ZLbUP6u": {
    "title": "MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent Material Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qUnjCEcN6S": {
    "title": "Incomplete Multi-view Deep Clustering with Data Imputation and Alignment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wCbOKbZ7kf": {
    "title": "SpecReason: Fast and Accurate Inference-Time Compute via Speculative Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=u1HclHIsLQ": {
    "title": "Can Diffusion Models Disentangle? A Theoretical Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=os4QYDf3Ms": {
    "title": "Backdoor Cleaning without External Guidance in MLLM Fine-tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RRBve5GwjS": {
    "title": "Proxy Target: Bridging the Gap Between Discrete Spiking Neural Networks and Continuous Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TIbQixHEFD": {
    "title": "MaintainCoder: Maintainable Code Generation Under Dynamic Requirements",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CrxR6CYeQn": {
    "title": "Accurately Predicting Protein Mutational Effects via a Hierarchical Many-Body Attention Network",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oCBKGw5HNf": {
    "title": "Flow-GRPO: Training Flow Matching Models via Online RL",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5g9qls1V7Q": {
    "title": "Self-diffusion for Solving Inverse Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xgiMK8FtSI": {
    "title": "LLM-DAMVC: A Large Language Model Assisted Dynamic Agent for Multi-View Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aPBBkHDK4W": {
    "title": "RiOSWorld: Benchmarking the Risk of Multimodal Computer-Use Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=efwbxMJ5M6": {
    "title": "Pre-Trained Policy Discriminators are General Reward Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p6Huickfj7": {
    "title": "Bridging Sign and Spoken Languages: Pseudo Gloss Generation for Sign Language Translation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=12z8KLMQJD": {
    "title": "Audio-Sync Video Generation with Multi-Stream Temporal Control",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8Q4xTf2SYC": {
    "title": "Heterogeneous Adversarial Play in Interactive Environments",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LWmfHjJnrx": {
    "title": "Revisiting Residual Connections: Orthogonal Updates for Stable and Efficient Deep Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Gppo2JImHs": {
    "title": "dKV-Cache: The Cache for Diffusion Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zjOXZEXQKZ": {
    "title": "Activity Pruning for Efficient Spiking Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oHjLfABsK4": {
    "title": "VideoREPA: Learning Physics for Video Generation through Relational Alignment with Foundation Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uodE9CAXaF": {
    "title": "OSCAR: One-Step Diffusion Codec Across Multiple Bit-rates",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NTjr79xpX0": {
    "title": "Attention Mechanism, Max-Affine Partition, and Universal Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mjZi8cV18P": {
    "title": "Uni-RL: Unifying Online and Offline RL via Implicit Value Regularization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QvV8oF08HA": {
    "title": "ShiQ: Bringing back Bellman to LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HUJXOQkLex": {
    "title": "Mysteries of the Deep: Role of Intermediate Representations in Out of Distribution Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qu7swcInlt": {
    "title": "Online Segment Any 3D Thing as Instance Tracking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CA1xVSvn72": {
    "title": "Lua-LLM: Learning Unstructured-Sparsity Allocation for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TlYLJFTTX7": {
    "title": "PASS: Path-selective State Space Model for Event-based Recognition",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bHCB8H90aZ": {
    "title": "Each Complexity Deserves a Pruning Policy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VSir0FzFnP": {
    "title": "BitMark: Watermarking Bitwise Autoregressive Image Generative Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jAJv3CaS4f": {
    "title": "Memorization in Graph Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XTTbzC7O2T": {
    "title": "Learning 3D Persistent Embodied World Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RWkLsoLeuj": {
    "title": "Stochastic Forward-Forward Learning through Representational Dimensionality Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2xS4VtpApy": {
    "title": "FastVID: Dynamic Density Pruning for Fast Video Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EakfENFVPT": {
    "title": "HumanCrafter: Synergizing Generalizable Human Reconstruction and Semantic 3D Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n4IviFDeoU": {
    "title": "ShoeFit: A New Dataset and Dual-image-stream DiT Framework for Virtual Footwear Try-On",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y1bt0YIS6Y": {
    "title": "VeriThinker: Learning to Verify Makes Reasoning Model Efficient",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fg9HufTI0K": {
    "title": "CogVLA: Cognition-Aligned Vision-Language-Action Models via Instruction-Driven Routing & Sparsification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RbGUML7YK6": {
    "title": "FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VZQSrNfNHd": {
    "title": "RobotSmith: Generative Robotic Tool Design for Acquisition of Complex Manipulation Skills",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=feAzLLT9to": {
    "title": "Edit Less, Achieve More: Dynamic Sparse Neuron Masking for Lifelong Knowledge Editing in LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gxgPjFjGmd": {
    "title": "Orientation-anchored Hyper-Gaussian for 4D Reconstruction from Casual Videos",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=um4aiicz3L": {
    "title": "Mellow: a small audio language model for reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bulTwq5kNK": {
    "title": "Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=btZm6DUaDO": {
    "title": "Quadratic Coreset Selection: Certifying and Reconciling Sequence and Token Mining for Efficient Instruction Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gA3fFAEXNT": {
    "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6tOKqqiyWE": {
    "title": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Xn8h68mP3": {
    "title": "Bootstrapping Hierarchical Autoregressive Formal Reasoner with Chain-of-Proxy-Autoformalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XPLf9H27aO": {
    "title": "JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo Retouching Agent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lMU2kaMANl": {
    "title": "Don't be lazy: CompleteP enables compute-efficient deep transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UAc6RL9Tt4": {
    "title": "Neural-Driven Image Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=quY3zRPalR": {
    "title": "Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4MKIaHbmGO": {
    "title": "Looking Into the Water by Unsupervised Learning of the Surface Shape",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NJoDGAnhWB": {
    "title": "PMQ-VE: Progressive Multi-Frame Quantization for Video Enhancement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4n7IifN7yr": {
    "title": "LiveStar: Live Streaming Assistant for Real-World Online Video Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WJnqeSPkQe": {
    "title": "DiffE2E: Rethinking End-to-End Driving with a Hybrid Diffusion-Regression-Classification Policy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ckq032sarb": {
    "title": "Relieving the Over-Aggregating Effect in Graph Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QclFsekj9B": {
    "title": "X-Scene: Large-Scale Driving Scene Generation with High Fidelity and Flexible Controllability",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=icoV59tH6D": {
    "title": "Vid-SME: Membership Inference Attacks against Large Video Understanding Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=49EjZytlus": {
    "title": "Reinforced Active Learning for Large-Scale Virtual Screening with Learnable Policy Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ICgOzZc10r": {
    "title": "More Than Generation: Unifying Generation and Depth Estimation via Text-to-Image Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DkJImu7t3A": {
    "title": "DOVE: Efficient One-Step Diffusion Model for Real-World Video Super-Resolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Z5pd8iHP2q": {
    "title": "GOATex: Geometry & Occlusion-Aware Texturing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WlrmpjocNe": {
    "title": "RadarQA: Multi-modal Quality Analysis of Weather Radar Forecasts",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bTRH1O5Bp2": {
    "title": "Counterfactual Evolution of Multimodal Datasets via Visual Programming",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BCpDJWshVV": {
    "title": "3DOT: Texture Transfer for 3DGS Objects from a Single Reference Image",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PZqII8EoFG": {
    "title": "Prioritizing Perception-Guided Self-Supervision: A New Paradigm for Causal Modeling in End-to-End Autonomous Driving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=T1gVXvbkB1": {
    "title": "RAPID Hand: Robust, Affordable, Perception-Integrated, Dexterous Manipulation Platfrom for Embodied Intelligence",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qy5vFFeCZW": {
    "title": "Knowledge Distillation Detection for Open-weights Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PvEnRUWSfn": {
    "title": "Dynamic Focused Masking for Autoregressive Embodied Occupancy Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=de07K7kreI": {
    "title": "LoRO: Real-Time on-Device Secure Inference for LLMs via TEE-Based Low Rank Obfuscation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WBEknRZBpT": {
    "title": "Skrull: Towards Efficient Long Context Fine-tuning through Dynamic Data Scheduling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJiu7nvLxA": {
    "title": "Pixel-Perfect Depth with Semantics-Prompted Diffusion Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=i9BjNoVjub": {
    "title": "Learning Urban Climate Dynamics via Physics-Guided Urban Surface–Atmosphere Interactions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3Sxby0hH1q": {
    "title": "Stop Summation: Min-Form Credit Assignment Is All Process Reward Model Needs for Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lHW93LKaUk": {
    "title": "Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EAbuXJPKQz": {
    "title": "Towards Reliable and Holistic Visual In-Context Learning Prompt Selection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PJOwQ77Mul": {
    "title": "SAMPO: Scale-wise Autoregression with Motion Prompt for Generative World Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KxcysQw6Ma": {
    "title": "Continuous Domain Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Y4ZMHNhrPT": {
    "title": "SaFiRe: Saccade-Fixation Reiteration with Mamba for Referring Image Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EZAotKVWfk": {
    "title": "Align-DA: Align Score-based Atmospheric Data Assimilation with Multiple Preferences",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7JjS2cdBYN": {
    "title": "LOMIA: Label-Only Membership Inference Attacks against Pre-trained Large Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FDAI0PY9Qp": {
    "title": "AdaVideoRAG: Omni-Contextual Adaptive Retrieval-Augmented Efficient Long Video Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CrdrVdXCui": {
    "title": "Efficient Representativeness-Aware Coreset Selection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lMxuq0GNeC": {
    "title": "SmartCache: Context-aware Semantic Cache for Efficient Multi-turn LLM Inference",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UkBwyp3aXG": {
    "title": "GeGS-PCR: Fast and Robust Color 3D Point Cloud Registration with Two-Stage Geometric-3DGS Fusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2bbDg587uh": {
    "title": "Improving Formal Reasoning of Transformer with State Stack",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FVtu7yC7fY": {
    "title": "Can LLMs Reason Over Non-Text Modalities in a Training-Free Manner? A Case Study with In-Context Representation Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HWTdOSKK3n": {
    "title": "PAID: Pairwise Angular-Invariant Decomposition for Continual Test-Time Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iErudB83TM": {
    "title": "KaRF: Weakly-Supervised Kolmogorov-Arnold Networks-based Radiance Fields for Local Color Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ezOfR26pGQ": {
    "title": "Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kWZRLR7w52": {
    "title": "PreFM: Online Audio-Visual Event Parsing via Predictive Future Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=59TL8MpBzX": {
    "title": "Unraveling Metameric Dilemma for Spectral Reconstruction: A High-Fidelity Approach via Semi-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vN3ZRS7L3I": {
    "title": "Causal Spatio-Temporal Prediction: An Effective and Efficient Multi-Modal Approach",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NRqGpUAjV9": {
    "title": "MI-TRQR: Mutual Information-Based Temporal Redundancy Quantification and Reduction for Energy-Efficient Spiking Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SoPSI570Ap": {
    "title": "NopeRoomGS: Indoor 3D Gaussian Splatting Optimization without Camera Pose Input",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=px9GwMjloi": {
    "title": "Text-Aware Real-World Image Super-Resolution via Diffusion Model with Joint Segmentation Decoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EKGJl7IHRw": {
    "title": "ViewPoint: Panoramic Video Generation with Pretrained Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xkGxogC2mF": {
    "title": "Shortcutting Pre-trained Flow Matching Diffusion Models is Almost Free Lunch",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Fj2kyLs6G5": {
    "title": "NeuroH-TGL: Neuro-Heterogeneity Guided Temporal Graph Learning Strategy for Brain Disease Diagnosis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5mpQO0YpTv": {
    "title": "Shape-Informed Clustering of Multi-Dimensional Functional Data via Deep Functional Autoencoders",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SGnZhRp3al": {
    "title": "Normalized Attention Guidance: Universal Negative Guidance for Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O8g0PpUoa1": {
    "title": "WildCAT3D: Appearance-Aware Multi-View Diffusion in the Wild",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=r4dzaP61QH": {
    "title": "DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bxELEjg3VE": {
    "title": "GSAlign: Geometric and Semantic Alignment Network for Aerial-Ground Person Re-Identification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0Nq1GvAGR0": {
    "title": "A Driving-Style-Adaptive Framework for Vehicle Trajectory Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Lm4VIXVIuy": {
    "title": "DEGauss: Defending Against Malicious 3D Editing for Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7bP1wHsJgR": {
    "title": "COS3D: Collaborative Open-Vocabulary 3D Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4EkEL77k6O": {
    "title": "Compress Large Language Models via Collaboration Between Learning and Matrix Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=91GzT22Sef": {
    "title": "Linear Differential Vision Transformer: Learning Visual Contrasts via Pairwise Differentials",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xE09wJcjvc": {
    "title": "MODEM: A Morton-Order Degradation Estimation Mechanism for Adverse Weather Image Recovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q7hoTSbV1t": {
    "title": "CAT: Circular-Convolutional Attention for Sub-Quadratic Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5BfO9i4kj1": {
    "title": "TreeSplat: Mergeable Tree for Deformable Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=koG76YqOwo": {
    "title": "HiFlow: Training-free High-Resolution Image Generation with Flow-Aligned Guidance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LCFYj0R2rH": {
    "title": "LoRASuite: Efficient LoRA Adaptation Across Large Language Model Upgrades",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iG4UNfUn0r": {
    "title": "VideoMAR: Autoregressive Video Generation with Continuous Tokens",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sRLB4XjorC": {
    "title": "OmniSVG: A Unified Scalable Vector Graphics Generation Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1PvMSoKvZG": {
    "title": "PaceLLM: Brain-Inspired Large Language Models for Long-Context Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sbvLts2HqR": {
    "title": "MoFo: Empowering Long-term Time Series Forecasting with Periodic Pattern Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LfcfwlLCHM": {
    "title": "DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=5J4IpiMKkq": {
    "title": "In-Context Fully Decentralized Cooperative Multi-Agent Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xju2uBgTrB": {
    "title": "Unifying Reconstruction and Density Estimation via Invertible Contraction Mapping in One-Class Classification",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ypVW5jvguX": {
    "title": "Hierarchical Semantic-Augmented Navigation: Optimal Transport and Graph-Driven Reasoning for Vision-Language Navigation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dv6X6sv18b": {
    "title": "Normal-Abnormal Guided Generalist Anomaly Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kITPSoTBUo": {
    "title": "Exploring Semantic-constrained Adversarial Example with Instruction Uncertainty Reduction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=isil4sVqSQ": {
    "title": "Towards Realistic Earth-Observation Constellation Scheduling: Benchmark and Methodology",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=H1PLlbFvwW": {
    "title": "V2V: Scaling Event-Based Vision through Efficient Video-to-Voxel Simulation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fTkBZLxBzV": {
    "title": "Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rHt6tt5RNF": {
    "title": "Multi-Modal Interactive Agent Layer for Few-Shot Universal Cross-Domain Retrieval and Beyond",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KQTp6ljvlo": {
    "title": "Fourier Clouds: Fast Bias Correction for Imbalanced Semi-Supervised Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tejdm1EMXK": {
    "title": "ECO: Evolving Core Knowledge for Efficient Transfer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JzOZkxa8Wd": {
    "title": "Stable Cinemetrics : Structured Taxonomy and Evaluation for Professional Video Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RnOKrKVMfC": {
    "title": "OmniGaze: Reward-inspired Generalizable Gaze Estimation in the Wild",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tJhJYcCABr": {
    "title": "Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YYz4fumVed": {
    "title": "HetSyn: Versatile Timescale Integration in Spiking Neural Networks via Heterogeneous Synapses",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=y0SRR9XGlZ": {
    "title": "Hierarchical Fine-grained Preference Optimization for Physically Plausible Video Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AHEKhff4Oa": {
    "title": "PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L3aEdxJMHl": {
    "title": "NEED: Cross-Subject and Cross-Task Generalization for Video and Image Reconstruction from EEG Signals",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ub5QBBQ47S": {
    "title": "Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual Boosting Loop for FSCIL",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zQmXDUbZ5D": {
    "title": "OpenHype: Hyperbolic Embeddings for Hierarchical Open-Vocabulary Radiance Fields",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N2QBQh6HeN": {
    "title": "NoPo-Avatar: Generalizable and Animatable Avatars from Sparse Inputs without Human Poses",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rbIlWjTFKj": {
    "title": "Struct2D: A Perception-Guided Framework for Spatial Reasoning in MLLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=STsjfx2cee": {
    "title": "Atomic Diffusion Models for Small Molecule Structure Elucidation from NMR Spectra",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hymrUe6ATL": {
    "title": "PIPE: Physics-Informed Position Encoding for Alignment of Satellite Images and Time Series in Typhoon Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NFxA2Von7y": {
    "title": "Memory-Efficient Visual Autoregressive Modeling with Scale-Aware KV Cache Compression",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ariVQf0KZx": {
    "title": "Thinkless: LLM Learns When to Think",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6LIbPoVFWS": {
    "title": "OmniZoom: A Universal Plug-and-Play Paradigm for Cross-Device Smooth Zoom Interpolation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cRxzKxsKow": {
    "title": "Event-Driven Dynamic Scene Depth Completion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uUBQ96zs48": {
    "title": "Generalized Category Discovery under Domain Shift: A Frequency Domain Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=bmH6UgE1z7": {
    "title": "Advancing Wasserstein Convergence Analysis of Score-Based Models: Insights from Discretization and Second-Order Acceleration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=71e1UmCMQ9": {
    "title": "Covariances for Free: Exploiting Mean Distributions for Training-free Federated Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Dr06Wjh45k": {
    "title": "Frequency-Aware Token Reduction for Efficient Vision Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n63KgrgVHG": {
    "title": "Targeted Maximum Likelihood Learning: An Optimization Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p58mKXaeWC": {
    "title": "Exploring and Exploiting Model Uncertainty in Bayesian Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SbhBIkiRLT": {
    "title": "DBLoss: Decomposition-based Loss Function for Time Series Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Q2fU0JDHuW": {
    "title": "LabelAny3D: Label Any Object 3D in the Wild",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lovTDtbsdZ": {
    "title": "Towards a General Attention Framework on Gyrovector Spaces for Matrix Manifolds",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VzSjSUE0BZ": {
    "title": "CREA: A Collaborative Multi-Agent Framework for Creative Image Editing and Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8OvST1bejm": {
    "title": "Why Popular MOEAs are Popular: Proven Advantages in Approximating the Pareto Front",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=GLhLU6y1uK": {
    "title": "Adjacent Words, Divergent Intents: Jailbreaking Large Language Models via Task Concurrency",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eTmvwxohRx": {
    "title": "Feasibility-Aware Decision-Focused Learning for Predicting Parameters in the Constraints",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rqXEiXZT6C": {
    "title": "What do you know? Bayesian knowledge inference for navigating agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ACSOnSHiWe": {
    "title": "CAM: A Constructivist View of Agentic Memory for LLM-Based Reading Comprehension",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=K2Q4Jp4RbB": {
    "title": "Why and How LLMs Hallucinate: Connecting the Dots with Subsequence Associations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mjhCFB3HLQ": {
    "title": "Understanding Data Influence in Reinforcement Finetuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w2xl15ZbD3": {
    "title": "Beyond Higher Rank: Token-wise Input-Output Projections for Efficient Low-Rank Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1Sb0363f2y": {
    "title": "VL-SAE: Interpreting and Enhancing Vision-Language Alignment with a Unified Concept Set",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tebG8q5EeK": {
    "title": "Reinforcement Learning Teachers of Test Time Scaling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4X2UwlYsXt": {
    "title": "Uncovering the Spectral Bias in Diagonal State Space Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZfAwtFBR9K": {
    "title": "DeblurDiff: Real-Word Image Deblurring with Generative Diffusion Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Ekw6gjs5Y5": {
    "title": "PathVQ: Reforming Computational Pathology Foundation Model for Whole Slide Image Analysis via Vector Quantization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=JPoQca8CSg": {
    "title": "MoleBridge: Synthetic Space Projecting with Discrete Markov Bridges",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oVDAfLuRie": {
    "title": "VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal Reasoning by Iterative Perception",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=L0xZPXT3le": {
    "title": "Multi-Agent Collaboration via Evolving Orchestration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vRVfgcoeIl": {
    "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=q5QaTQcUbS": {
    "title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uUHNt5CoaL": {
    "title": "Auto-Connect: Connectivity-Preserving RigFormer with Direct Preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=wczmXLuLGd": {
    "title": "MMaDA: Multimodal Large Diffusion Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lC7q2cwpov": {
    "title": "iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hYovE4nHTt": {
    "title": "Radial Attention: O ( n log ⁡ n ) Sparse Attention for Long Video Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hFsCuVc1cB": {
    "title": "S 2 NN: Sub-bit Spiking Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HXFvNkNt0n": {
    "title": "Geometry Aware Operator Transformer as an efficient and accurate neural surrogate for PDEs on arbitrary domains",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KQuVaVUPL6": {
    "title": "MLLMs Need 3D-Aware Representation Supervision for Scene Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FS3FzdrFZ7": {
    "title": "Universal Video Temporal Grounding with Generative Multi-modal Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uWMF3MuEk2": {
    "title": "Comparing Uniform Price and Discriminatory Multi-Unit Auctions through Regret Minimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kMPII4XbKC": {
    "title": "R1-ShareVL: Incentivizing Reasoning Capabilities of Multimodal Large Language Models via Share-GRPO",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g52NwTQj0Q": {
    "title": "StateSpaceDiffuser: Bringing Long Context to Diffusion World Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IWEc6kpy8O": {
    "title": "ELDET: Early-Learning Distillation with Noisy Labels for Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BLLixcuZgl": {
    "title": "Beyond Attention or Similarity: Maximizing Conditional Diversity for Token Pruning in MLLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uCaqkrEXEP": {
    "title": "Non-Stationary Lipschitz Bandits",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=n1NFpViAhp": {
    "title": "The Price of Opportunity Fairness in Matroid Allocation Problems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pZIeK0Xvph": {
    "title": "Fin3R: Fine-tuning Feed-forward 3D Reconstruction Models via Monocular Knowledge Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=O7311P8YgP": {
    "title": "T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WZS7sJiSUm": {
    "title": "All You Need is One: Capsule Prompt Tuning with a Single Vector",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=RoVS9vmpd2": {
    "title": "Codifying Character Logic in Role-Playing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gFFgCWiXWI": {
    "title": "Tapered Off-Policy REINFORCE - Stable and efficient reinforcement learning for large language models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6JIjL7kXzy": {
    "title": "Impact of Layer Norm on Memorization and Generalization in Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eQ79kT0QY1": {
    "title": "Negative Feedback Really Matters: Signed Dual-Channel Graph Contrastive Learning Framework for Recommendation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tkV3n52mQO": {
    "title": "Rare Text Semantics Were Always There in Your Diffusion Transformer",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ctsLGmMnCT": {
    "title": "MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition Query",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tY8ctrD4W2": {
    "title": "Compiler-R1: Towards Agentic Compiler Auto-tuning with Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=REAhulrjzR": {
    "title": "GUIDED: Granular Understanding via Identification, Detection, and Discrimination for Fine-Grained Open-Vocabulary Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BxoEDR2yQM": {
    "title": "GPSToken: Gaussian Parameterized Spatially-adaptive Tokenization for Image Representation and Generation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=VXygIRRHxz": {
    "title": "GaRA-SAM: Robustifying Segment Anything Model with Gated-Rank Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PxximqJil4": {
    "title": "StarTrail: Concentric Ring Sequence Parallelism for Efficient Near-Infinite-Context Transformer Model Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=g9sWQsqemL": {
    "title": "Boosting Resilience of Large Language Models through Causality-Driven Robust Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fV46JjrrOm": {
    "title": "Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uiuA0ixKLd": {
    "title": "Part-Aware Bottom-Up Group Reasoning for Fine-Grained Social Interaction Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=toI181TMAx": {
    "title": "Rethinking Scale-Aware Temporal Encoding for Event-based Object Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=plrg87MP0F": {
    "title": "PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gZjPllL9jM": {
    "title": "Efficient Multi-modal Large Language Models via Progressive Consistency Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=C2fJE8t0lH": {
    "title": "TopoPoint: Enhance Topology Reasoning via Endpoint Detection in Autonomous Driving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qI64pCXNXn": {
    "title": "WorldWeaver: Generating Long-Horizon Video Worlds via Rich Perception",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Y2m0DZBfL": {
    "title": "GTR-Loc: Geospatial Text Regularization Assisted Outdoor LiDAR Localization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=b98ODdeYq5": {
    "title": "Code Graph Model (CGM): A Graph-Integrated Large Language Model for Repository-Level Software Engineering Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AqHlcF0zK6": {
    "title": "Unveiling the Learning Mind of Language Models: A Cognitive Framework and Empirical Study",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=dw9H08UxJb": {
    "title": "Omni-Mol: Multitask Molecular Model for Any-to-any Modalities",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=N5V3dlIck9": {
    "title": "Erasing Conceptual Knowledge from Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=lRkeVFL85X": {
    "title": "GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8wWHCzsTxS": {
    "title": "Learning from Disjoint Views: A Contrastive Prototype Matching Network for Fully Incomplete Multi-View Clustering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7HVADbW8fh": {
    "title": "Coloring Learning for Heterophilic Graph Representation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=p3z75I8lVq": {
    "title": "Unifying Appearance Codes and Bilateral Grids for Driving Scene Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zjQLUiguRz": {
    "title": "TAMI: Taming Heterogeneity in Temporal Interactions for Temporal Graph Link Prediction",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yGOytgjurF": {
    "title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IXiCBWNTRr": {
    "title": "Pro3D-Editor: A Progressive Framework for Consistent and Precise 3D Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=diyDUH4NtD": {
    "title": "FlexEvent: Towards Flexible Event-Frame Object Detection at Varying Operational Frequencies",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qsYbytjmQK": {
    "title": "Evolutionary Prediction Games",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=hTGqC1h8Ig": {
    "title": "Structural Entropy Guided Agent for Detecting and Repairing Knowledge Deficiencies in LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SoqzNbcBjy": {
    "title": "Spiral: Semantic-Aware Progressive LiDAR Scene Generation and Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=To7Rs2wsTd": {
    "title": "VideoLucy: Deep Memory Backtracking for Long Video Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QynKUq9X3L": {
    "title": "Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=W5HnIf2jla": {
    "title": "Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Tf9eoTIIjh": {
    "title": "Preserving LLM Capabilities through Calibration Data Curation: From Analysis to Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f4pvPNf9ox": {
    "title": "MURKA: Multi-Reward Reinforcement Learning with Knowledge Alignment for Optimization Tasks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=YGIbwfNWot": {
    "title": "Tri-MARF: A Tri-Modal Multi-Agent Responsive Framework for Comprehensive 3D Object Annotation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2Dwp12HmYI": {
    "title": "Dr. RAW: Towards General High-Level Vision from RAW with Efficient Task Conditioning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yruGxKsZyH": {
    "title": "Model Inversion with Layer-Specific Modeling and Alignment for Data-Free Continual Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XR30K9zxFm": {
    "title": "A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iE0oCRx81h": {
    "title": "TRIM: Scalable 3D Gaussian Diffusion Inference with Temporal and Spatial Trimming",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=x7EjzpW5XC": {
    "title": "Rectifying Soft-Label Entangled Bias in Long-Tailed Dataset Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3naHyE5klE": {
    "title": "Revolutionizing Training-Free NAS: Towards Efficient Automatic Proxy Discovery via Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=F7y7JMaTvj": {
    "title": "Robust Policy Expansion for Offline-to-Online RL under Diverse Data Corruption",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Qr0YbuilFb": {
    "title": "MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust Physics-Based Dynamics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WgrVccLRzG": {
    "title": "ObCLIP: Oblivious CLoud-Device Hybrid Image Generation with Privacy Preservation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PjbpL4brUb": {
    "title": "DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in LLMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=f4Wd385vHi": {
    "title": "Non-Line-of-Sight 3D Reconstruction with Radar",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aEAbRPXV37": {
    "title": "Majority of the Bests: Improving Best-of-N via Bootstrapping",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=A3ezGit0uB": {
    "title": "PLEIADES: Building Temporal Kernels with Orthogonal Polynomials",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3i53Z6g6j4": {
    "title": "4D-LRM: Large Space-Time Reconstruction Model From and To Any View at Any Time",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=HbTxc6U1fO": {
    "title": "Video World Models with Long-term Spatial Memory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ib0aV2hphN": {
    "title": "High-Order Flow Matching: Unified Framework and Sharp Statistical Rates",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1v0ULVJOZ9": {
    "title": "RHYTHM: Reasoning with Hierarchical Temporal Tokenization for Human Mobility",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BggfTUtZOM": {
    "title": "Video Diffusion Models Excel at Tracking Similar-Looking Objects Without Supervision",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=cy6MGBwToV": {
    "title": "FedRW: Efficient Privacy-Preserving Data Reweighting for Enhancing Federated Learning of Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=MoS4P8zieM": {
    "title": "Learning to Zoom with Anatomical Relations for Medical Structure Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PAYYkls0WD": {
    "title": "Decoupling Contrastive Decoding: Robust Hallucination Mitigation in Multimodal Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zKoeRtye8o": {
    "title": "Don't Just Chase \"Highlighted Tokens\" in MLLMs: Revisiting Visual Holistic Context Retention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=FoVF3iL6o3": {
    "title": "You Only Communicate Once: One-shot Federated Low-Rank Adaptation of MLLM",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=eCElREEUsr": {
    "title": "GoT: Unleashing Reasoning Capability of MLLM for Visual Generation and Editing",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TfCXLp0Jgo": {
    "title": "Plenodium: Underwater 3D Scene Reconstruction with Plenoptic Medium Representation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e7jNIna1eP": {
    "title": "NaDRO: Leveraging Dual-Reward Strategies for LLMs Training on Noisy Data",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vE98S8BmzP": {
    "title": "Intermediate Domain Alignment and Morphology Analogy for Patent-Product Image Retrieval",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=unMwI4JLrP": {
    "title": "FineRS: Fine-grained Reasoning and Segmentation of Small Objects with Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7nTWoceJGK": {
    "title": "Guard Me If You Know Me: Protecting Specific Face-Identity from Deepfakes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=32JgYdTfT9": {
    "title": "DAWP: A framework for global observation forecasting via Data Assimilation and Weather Prediction in satellite observation space",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LkA1yLshF8": {
    "title": "Audio Super-Resolution with Latent Bridge Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UCloKhbOvP": {
    "title": "Attention on the Sphere",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=626icWM1xd": {
    "title": "Bit-swapping Oriented Twin-memory Multi-view Clustering in Lifelong Incomplete Scenarios",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WI8rrwYJdT": {
    "title": "STAIR: Addressing Stage Misalignment through Temporal-Aligned Preference Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=iCvueZ8KaN": {
    "title": "VCM: Vision Concept Modeling with Adaptive Vision Token Compression via Instruction Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BeXcXrXetA": {
    "title": "Perception-R1: Pioneering Perception Policy with Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pXXKKBx8G0": {
    "title": "CoIDO: Efficient Data Selection for Visual Instruction Tuning via Coupled Importance-Diversity Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=c0c943R6ZJ": {
    "title": "Surface-Aware Feed-Forward Quadratic Gaussian for Frame Interpolation with Large Motion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0VDmWjW456": {
    "title": "L-MTP: Leap Multi-Token Prediction Beyond Adjacent Context for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2lNCktd2c3": {
    "title": "A Finite Sample Analysis of Distributional TD Learning with Linear Function Approximation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZnrM5RGrgR": {
    "title": "OptiScene: LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qeivJh0G2j": {
    "title": "Photography Perspective Composition: Towards Aesthetic Perspective Recommendation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tyERwC5520": {
    "title": "GRAVER: Generative Graph Vocabularies for Robust Graph Foundation Models Fine-tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=juROy8NYRD": {
    "title": "Continual Multimodal Contrastive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=E8adS5srds": {
    "title": "Anchored Diffusion Language Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=idtZwmjakN": {
    "title": "GEM: Empowering MLLM for Grounded ECG Understanding with Time Series and Images",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7Q1ApHpX31": {
    "title": "Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System Collaboration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9yG7LGYfHS": {
    "title": "GSPN-2: Efficient Parallel Sequence Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qqEfm8tlCM": {
    "title": "FAST: Foreground‑aware Diffusion with Accelerated Sampling Trajectory for Segmentation‑oriented Anomaly Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=crPlJvwHhS": {
    "title": "Do LVLMs Truly Understand Video Anomalies? Revealing Hallucination via Co-Occurrence Patterns",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=LujP8dao7o": {
    "title": "Smooth and Flexible Camera Movement Synthesis via Temporal Masked Generative Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=4ULtNYHc5T": {
    "title": "Exploring Tradeoffs through Mode Connectivity for Multi-Task Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uc1js1lBlB": {
    "title": "Learning Provably Improves the Convergence of Gradient Descent",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QzE4SDwcCr": {
    "title": "Curriculum Abductive Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nf8PKQKtl2": {
    "title": "NavBench: Probing Multimodal Large Language Models for Embodied Navigation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=weU95NZePs": {
    "title": "TITAN: A Trajectory-Informed Technique for Adaptive Parameter Freezing in Large-Scale VQE",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tXqLxHlb8Z": {
    "title": "AgentNet: Decentralized Evolutionary Coordination for LLM-based Multi-Agent Systems",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=37b23mxKH8": {
    "title": "Resounding Acoustic Fields with Reciprocity",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nqpbbvEZwF": {
    "title": "Beyond Masked and Unmasked: Discrete Diffusion Models via Partial Masking",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h60y6zlPyl": {
    "title": "TwinMarket: A Scalable Behavioral and Social Simulation for Financial Markets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NYuvz5YUzy": {
    "title": "Generative Perception of Shape and Material from Differential Motion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=w5euaFgNWm": {
    "title": "Latent Harmony: Synergistic Unified UHD Image Restoration via Latent Space Regularization and Controllable Refinement",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IbzDaIDyt6": {
    "title": "SE-GUI: Enhancing Visual Grounding for GUI Agents via Self-Evolutionary Reinforcement Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=NdScoAix25": {
    "title": "Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning of Vision Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zft0zTOFkN": {
    "title": "Noisy Multi-Label Learning through Co-Occurrence-Aware Diffusion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=P4KjfMrVNU": {
    "title": "Hierarchical Information Aggregation for Incomplete Multimodal Alzheimer's Disease Diagnosis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=M6zQNbCaLl": {
    "title": "FlowCut: Rethinking Redundancy via Information Flow for Efficient Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=86enCXORIV": {
    "title": "VideoTitans: Scalable Video Prediction with Integrated Short- and Long-term Memory",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vMpvtSmtXY": {
    "title": "MINT-CoT: Enabling Interleaved Visual Tokens in Mathematical Chain-of-Thought Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=vqXWCC0aUG": {
    "title": "EverybodyDance: Bipartite Graph–Based Identity Correspondence for Multi-Character Animation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sVSVgWIkb0": {
    "title": "Breaking Latent Prior Bias in Detectors for Generalizable AIGC Image Detection",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Uipk3FzI09": {
    "title": "Defining and Discovering Hyper-meta-paths for Heterogeneous Hypergraphs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=DUlZTgLkeh": {
    "title": "FreqExit: Enabling Early-Exit Inference for Visual Autoregressive Models via Frequency-Aware Guidance",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TLlFnoPUdB": {
    "title": "From Pixels to Views: Learning Angular-Aware and Physics-Consistent Representations for Light Field Microscopy",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ycPVp0577R": {
    "title": "Segment then Splat: Unified 3D Open-Vocabulary Segmentation via Gaussian Splatting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=quKHZ3fcgx": {
    "title": "Poison as Cure: Visual Noise for Mitigating Object Hallucinations in LVMs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kGlrPZuHPq": {
    "title": "Non-Asymptotic Guarantees for Average-Reward Q-Learning with Adaptive Stepsizes",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=swf3Hbc3Qe": {
    "title": "Learning Crossmodal Interaction Patterns via Attributed Bipartite Graphs for Single-Cell Omics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=nfxTpNiSMH": {
    "title": "SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D Visual Grounding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=6hvaQTKkpF": {
    "title": "HoliTom: Holistic Token Merging for Fast Video Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gXZlZAeqay": {
    "title": "Real-DRL: Teach and Learn in Reality",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fmQFCXAe4v": {
    "title": "Structured Initialization for Vision Transformers",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QaZxGWlbgO": {
    "title": "Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=efrFbKYobs": {
    "title": "Learning Robust Spectral Dynamics for Temporal Domain Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BOwPpmRgmW": {
    "title": "HoloScene: Simulation‑Ready Interactive 3D Worlds from a Single Video",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=uwUkETPIJN": {
    "title": "Improving Data Efficiency for LLM Reinforcement Fine-tuning Through Difficulty-targeted Online Data Selection and Rollout Replay",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=qup6v4WnYX": {
    "title": "GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tYnJC5ba6j": {
    "title": "Unveiling the Spatial-temporal Effective Receptive Fields of Spiking Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gqfQfqDQhx": {
    "title": "WKV-sharing embraced random shuffle RWKV high-order modeling for pan-sharpening",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=TS128AgTRw": {
    "title": "Quantile Reward Policy Optimization: Alignment with Pointwise Regression and Exact Partition Functions",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tBQEPRFT60": {
    "title": "UtilGen: Utility-Centric Generative Data Augmentation with Dual-Level Task Adaptation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ktvr5cyMdu": {
    "title": "Where Does It Exist from the Low-Altitude: Spatial Aerial Video Grounding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=CdkFnJSG4G": {
    "title": "Training-Free Efficient Video Generation via Dynamic Token Carving",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=9lASMulFUj": {
    "title": "Towards Pre-trained Graph Condensation via Optimal Transport",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=BxRsXqjWft": {
    "title": "ChartSketcher: Reasoning with Multimodal Feedback and Reflection for Chart Understanding",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jb4EvgxOXD": {
    "title": "Optimizing Distributional Geometry Alignment with Optimal Transport for Generative Dataset Distillation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3fDypdR4VN": {
    "title": "Provable Ordering and Continuity in Vision-Language Pretraining for Generalizable Embodied Agents",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2xPoZhOO23": {
    "title": "Enhancing Text-to-Image Diffusion Transformer via Split-Text Conditioning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Uon41HfqR3": {
    "title": "Many Minds, One Goal: Time Series Forecasting via Sub-task Specialization and Inter-agent Cooperation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=At8OUlyTOu": {
    "title": "Afterburner: Reinforcement Learning Facilitates Self-Improving Code Efficiency Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=84dnGT4ajt": {
    "title": "CoreGuard: Safeguarding Foundational Capabilities of LLMs Against Model Stealing in Edge Deployment",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=aLGgz4SOyu": {
    "title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=QXEhBMNrCW": {
    "title": "Group-in-Group Policy Optimization for LLM Agent Training",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V56unBiTHP": {
    "title": "HoliGS: Holistic Gaussian Splatting for Embodied View Synthesis",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jCGwSLwOt9": {
    "title": "Less but More: Linear Adaptive Graph Learning Empowering Spatiotemporal Forecasting",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=ZC2rbIYWfy": {
    "title": "Few-Shot Learning from Gigapixel Images via Hierarchical Vision-Language Alignment and Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=h6xQClTm4W": {
    "title": "Exploring the Limits of Vision-Language-Action Manipulation in Cross-task Generalization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3901vVCfo3": {
    "title": "UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=yAf2Akj1Wm": {
    "title": "Coarse-to-Fine 3D Part Assembly via Semantic Super-Parts and Symmetry-Aware Pose Estimation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=40M1uJl2GX": {
    "title": "Trust Region Reward Optimization and Proximal Inverse Reward Optimization Algorithm",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fgfftCWPrn": {
    "title": "Self-alignment of Large Video Language Models with Refined Regularized Preference Optimization",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8Ounc8L4F7": {
    "title": "RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=2aotKzkOCm": {
    "title": "FP64 is All You Need: Rethinking Failure Modes in Physics-Informed Neural Networks",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OfIUAlo2hJ": {
    "title": "Enhancing Sample Selection Against Label Noise by Cutting Mislabeled Easy Examples",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=IPxOoU8aqt": {
    "title": "Doctor Approved: Generating Medically Accurate Skin Disease Images through AI-Expert Feedback",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=3ln8F2nOuA": {
    "title": "Fast Data Attribution for Text-to-Image Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V8sDJ1oMEy": {
    "title": "VT-FSL: Bridging Vision and Text with LLMs for Few-Shot Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=e50L5Mx93u": {
    "title": "Restage4D: Reanimating Deformable 3D Reconstruction from a Single Video",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=kND7h1kD53": {
    "title": "DualEqui: A Dual-Space Hierarchical Equivariant Network for Large Biomolecules",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=1BAiQmAFsx": {
    "title": "Walking the Tightrope: Autonomous Disentangling Beneficial and Detrimental Drifts in Non-Stationary Custom-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=fJRuMulPkc": {
    "title": "Systematic Reward Gap Optimization for Mitigating VLM Hallucinations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=oEgybA04dY": {
    "title": "Open Vision Reasoner: Transferring Linguistic Cognitive Behavior for Visual Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=8Flpo0zaaO": {
    "title": "SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=PK07eretkF": {
    "title": "DreamVLA: A Vision-Language-Action Model Dreamed with Comprehensive World Knowledge",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=90qt3kVYaE": {
    "title": "Hierarchical Implicit Neural Emulators",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=XKnOA7MhCz": {
    "title": "Pay Attention to Small Weights",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=AA0qGRIfPn": {
    "title": "Why 1 + 1 < 1 in Visual Token Pruning: Beyond Naive Integration via Multi-Objective Balanced Covering",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zIFuLxUAu9": {
    "title": "Semi-off-Policy Reinforcement Learning for Vision-Language Slow-Thinking Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=EKJhU5ioSo": {
    "title": "GAM-Agent: Game-Theoretic and Uncertainty-Aware Collaboration for Complex Visual Reasoning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=zrBucj9BwG": {
    "title": "PhySwin: An Efficient and Physically-Informed Foundation Model for Multispectral Earth Observation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=SFsGKZU61H": {
    "title": "Learning to Integrate Diffusion ODEs by Averaging the Derivatives",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=xLFYd1owiP": {
    "title": "Spot the Fake: Large Multimodal Model-Based Synthetic Image Detection with Artifact Explanation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=0lNwIIHWhZ": {
    "title": "ComPO: Preference Alignment via Comparison Oracles",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rldWzb0E7G": {
    "title": "PolyJuice Makes It Real: Black-Box, Universal Red Teaming for Synthetic Image Detectors",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mLVqiNH0aA": {
    "title": "ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jYAdfwVGZw": {
    "title": "How Does Topology Bias Distort Message Passing in Graph Recommender? A Dirichlet Energy Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KWNabnuuct": {
    "title": "Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=epZTfZF7JC": {
    "title": "VITA-Audio: Fast Interleaved Audio-Text Token Generation for Efficient Large Speech-Language Model",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=V8HhrBSsEn": {
    "title": "The quest for the GRAph Level autoEncoder (GRALE)",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=gY9yOGYB48": {
    "title": "VORTA: Efficient Video Diffusion via Routing Sparse Attention",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=Hhc5McwASX": {
    "title": "Fractional Diffusion Bridge Models",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pOLpyGGOq8": {
    "title": "Kinaema: a recurrent sequence model for memory and pose in motion",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=KM2XzHq2Rm": {
    "title": "Integral Imprecise Probability Metrics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=tjXtcZjIgQ": {
    "title": "EvolvedGRPO: Unlocking Reasoning in LVLMs via Progressive Instruction Evolution",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B1nCWzpnE4": {
    "title": "DynamicVerse: A Physically-Aware Multimodal Framework for 4D World Modeling",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=B7lygdSDii": {
    "title": "SEAL: Semantic-Aware Hierarchical Learning for Generalized Category Discovery",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=OGxalNUHbJ": {
    "title": "RoboRefer: Towards Spatial Referring with Reasoning in Vision-Language Models for Robotics",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=mnIox70nhO": {
    "title": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=sPbsnUCyNt": {
    "title": "On the rankability of visual embeddings",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=WIiDiRqKaP": {
    "title": "TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=7kQjbCQwtT": {
    "title": "Discovering Important Experts for Mixture-of-Experts Models Pruning Through a Theoretical Perspective",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=UhvD08xpBO": {
    "title": "Bio-Inspired Image Restoration",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=pe92NSPLE8": {
    "title": "Machine Unlearning via Task Simplex Arithmetic",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=rJ5ky9C3ue": {
    "title": "CrossSpectra: Exploiting Cross-Layer Smoothness for Parameter-Efficient Fine-Tuning",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  },
  "https://openreview.net/forum?id=jNQ40aw5qL": {
    "title": "Amortized Active Generation of Pareto Sets",
    "volume": "poster",
    "abstract": "",
    "checked": false,
    "id": null,
    "semantic_title": "",
    "citation_count": 0,
    "authors": []
  }
}